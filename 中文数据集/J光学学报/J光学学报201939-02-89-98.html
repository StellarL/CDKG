

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135525173693750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201902011%26RESULT%3d1%26SIGN%3dl1Xh70X6yzo5wZFHq3ZjFyUIsTk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201902011&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201902011&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902011&amp;v=MTc2NjVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVUw3SklqWFRiTEc0SDlqTXJZOUVaWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#59" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="2 相关工作 ">2 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#64" data-title="&lt;b&gt;2.1&lt;/b&gt;&lt;b&gt;SRCNN算法&lt;/b&gt;"><b>2.1</b><b>SRCNN算法</b></a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;2.2&lt;/b&gt;&lt;b&gt;ESPCN算法&lt;/b&gt;"><b>2.2</b><b>ESPCN算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#79" data-title="3 DRSR算法 ">3 DRSR算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#81" data-title="&lt;b&gt;3.1&lt;/b&gt;&lt;b&gt;深层神经网络结构&lt;/b&gt;"><b>3.1</b><b>深层神经网络结构</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;3.2&lt;/b&gt;&lt;b&gt;残差网络模型&lt;/b&gt;"><b>3.2</b><b>残差网络模型</b></a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;3.3&lt;/b&gt;&lt;b&gt;参数优化&lt;/b&gt;"><b>3.3</b><b>参数优化</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#114" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#116" data-title="&lt;b&gt;4.1&lt;/b&gt;&lt;b&gt;数据集&lt;/b&gt;"><b>4.1</b><b>数据集</b></a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;4.2&lt;/b&gt;&lt;b&gt;层数设置&lt;/b&gt;"><b>4.2</b><b>层数设置</b></a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;4.3&lt;/b&gt;&lt;b&gt;激活函数比较&lt;/b&gt;"><b>4.3</b><b>激活函数比较</b></a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;4.4&lt;/b&gt;&lt;b&gt;速度比较&lt;/b&gt;"><b>4.4</b><b>速度比较</b></a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;4.5&lt;/b&gt;&lt;b&gt;优化方法比较&lt;/b&gt;"><b>4.5</b><b>优化方法比较</b></a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;4.6&lt;/b&gt;&lt;b&gt;滤波器数目比较&lt;/b&gt;"><b>4.6</b><b>滤波器数目比较</b></a></li>
                                                <li><a href="#134" data-title="&lt;b&gt;4.7&lt;/b&gt;&lt;b&gt;残差网络设置&lt;/b&gt;"><b>4.7</b><b>残差网络设置</b></a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;4.8&lt;/b&gt;&lt;b&gt;实验结果&lt;/b&gt;"><b>4.8</b><b>实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#158" data-title="5 结 论 ">5 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="图1 SRCNN结构图">图1 SRCNN结构图</a></li>
                                                <li><a href="#75" data-title="图2 ESPCN结构图">图2 ESPCN结构图</a></li>
                                                <li><a href="#83" data-title="图3 本文算法网络结构图">图3 本文算法网络结构图</a></li>
                                                <li><a href="#90" data-title="图4 残差网络结构图">图4 残差网络结构图</a></li>
                                                <li><a href="#120" data-title="图5 (a) 12层网络的损失函数随迭代次数的变化; (b) set 5的PSNR平均值在不同层数下随迭代次数的变化">图5 (a) 12层网络的损失函数随迭代次数的变化; (b) set 5的PSNR平均值在不同层数下......</a></li>
                                                <li><a href="#121" data-title="表1 不同深度下各测试集的PSNR/SSIM均值">表1 不同深度下各测试集的PSNR/SSIM均值</a></li>
                                                <li><a href="#126" data-title="图6 set 5的PSNR平均值在不同激活函数下随迭代次数的变化">图6 set 5的PSNR平均值在不同激活函数下随迭代次数的变化</a></li>
                                                <li><a href="#127" data-title="图7 测试集set 5在不同算法下的运行时间与PSNR均值的关系图">图7 测试集set 5在不同算法下的运行时间与PSNR均值的关系图</a></li>
                                                <li><a href="#132" data-title="图8 set 5的PSNR平均值在不同优化方法下随迭代次数的变化">图8 set 5的PSNR平均值在不同优化方法下随迭代次数的变化</a></li>
                                                <li><a href="#133" data-title="图9 set 5的PSNR平均值在不同滤波器数目下随迭代次数的变化">图9 set 5的PSNR平均值在不同滤波器数目下随迭代次数的变化</a></li>
                                                <li><a href="#138" data-title="图10 set 5的PSNR平均值在不同网络模型下随迭代次数的变化。 (a) 6层和8层网络; (b) 10层和12层网络">图10 set 5的PSNR平均值在不同网络模型下随迭代次数的变化。 (a) 6层和8层网络; (b......</a></li>
                                                <li><a href="#139" data-title="图11 Monarch在不同算法下的效果图">图11 Monarch在不同算法下的效果图</a></li>
                                                <li><a href="#140" data-title="图12 Comic在不同算法下的效果图">图12 Comic在不同算法下的效果图</a></li>
                                                <li><a href="#156" data-title="表2 测试集&lt;i&gt;set&lt;/i&gt; 5, &lt;i&gt;set&lt;/i&gt; 14, &lt;i&gt;BSD&lt;/i&gt;100在不同算法下的&lt;i&gt;PSNR&lt;/i&gt;平均值">表2 测试集<i>set</i> 5, <i>set</i> 14, <i>BSD</i>100在不同算法下的<i>PSNR</i>平均值</a></li>
                                                <li><a href="#157" data-title="表3 测试集&lt;i&gt;set&lt;/i&gt; 5, &lt;i&gt;set&lt;/i&gt; 14, &lt;i&gt;BSD&lt;/i&gt;100在不同算法下的&lt;i&gt;SSIM&lt;/i&gt;平均值">表3 测试集<i>set</i> 5, <i>set</i> 14, <i>BSD</i>100在不同算法下的<i>SSIM</i>平均值</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" &lt;i&gt;Zhang H Y&lt;/i&gt;, &lt;i&gt;Zhang L P&lt;/i&gt;, &lt;i&gt;Shen H F&lt;/i&gt;. &lt;i&gt;A blind super&lt;/i&gt;-&lt;i&gt;resolution reconstruction method considering image registration errors&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;International Journal of Fuzzy Systems&lt;/i&gt;, 2015, 17 (2) : 353-364." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A blind super-resolution reconstruction method considering image registration errors">
                                        <b>[1]</b>
                                         &lt;i&gt;Zhang H Y&lt;/i&gt;, &lt;i&gt;Zhang L P&lt;/i&gt;, &lt;i&gt;Shen H F&lt;/i&gt;. &lt;i&gt;A blind super&lt;/i&gt;-&lt;i&gt;resolution reconstruction method considering image registration errors&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;International Journal of Fuzzy Systems&lt;/i&gt;, 2015, 17 (2) : 353-364.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" &lt;i&gt;Chen H G&lt;/i&gt;, &lt;i&gt;He X H&lt;/i&gt;, &lt;i&gt;Teng Q Z&lt;/i&gt;, et al. &lt;i&gt;Single image super resolution using local smoothness and nonlocal self&lt;/i&gt;-&lt;i&gt;similarity priors&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;Signal Processing&lt;/i&gt;: &lt;i&gt;Image Communication&lt;/i&gt;, 2016, 43: 68-81." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES56BE354FB2B92978E52A6C8F7643372A&amp;v=MjQxMzlxMmhVemZiR1hRcmp1Q09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHRnekxtOHdhaz1OaWZPZmJhK2JLVFBxb3N6RnVsOUJYNHd5QjVtN3owTVRneg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         &lt;i&gt;Chen H G&lt;/i&gt;, &lt;i&gt;He X H&lt;/i&gt;, &lt;i&gt;Teng Q Z&lt;/i&gt;, et al. &lt;i&gt;Single image super resolution using local smoothness and nonlocal self&lt;/i&gt;-&lt;i&gt;similarity priors&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;Signal Processing&lt;/i&gt;: &lt;i&gt;Image Communication&lt;/i&gt;, 2016, 43: 68-81.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" &lt;i&gt;Li S M&lt;/i&gt;, &lt;i&gt;Lei G Q&lt;/i&gt;, &lt;i&gt;Fan R&lt;/i&gt;. &lt;i&gt;Depth map super&lt;/i&gt;-&lt;i&gt;resolution reconstruction based on convolutional neural networks&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;Acta Optica Sinica&lt;/i&gt;, 2017, 37 (12) : 1210002.  李素梅, 雷国庆, 范如. 基于卷积神经网络的深度图超分辨率重建[&lt;i&gt;J&lt;/i&gt;]. 光学学报, 2017, 37 (12) : 1210002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201712015&amp;v=MTk5NzdMN0pJalhUYkxHNEg5Yk5yWTlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEa1U=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         &lt;i&gt;Li S M&lt;/i&gt;, &lt;i&gt;Lei G Q&lt;/i&gt;, &lt;i&gt;Fan R&lt;/i&gt;. &lt;i&gt;Depth map super&lt;/i&gt;-&lt;i&gt;resolution reconstruction based on convolutional neural networks&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;Acta Optica Sinica&lt;/i&gt;, 2017, 37 (12) : 1210002.  李素梅, 雷国庆, 范如. 基于卷积神经网络的深度图超分辨率重建[&lt;i&gt;J&lt;/i&gt;]. 光学学报, 2017, 37 (12) : 1210002.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" &lt;i&gt;Batz M&lt;/i&gt;, &lt;i&gt;Eichenseer A&lt;/i&gt;, &lt;i&gt;Seiler J&lt;/i&gt;, et al. &lt;i&gt;Hybrid super&lt;/i&gt;-&lt;i&gt;resolution combining example&lt;/i&gt;-&lt;i&gt;based single&lt;/i&gt;-&lt;i&gt;image and interpolation&lt;/i&gt;-&lt;i&gt;based multi&lt;/i&gt;-&lt;i&gt;image reconstruction approaches&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;IEEE International Conference on Image Processing&lt;/i&gt; (&lt;i&gt;ICIP&lt;/i&gt;) , 2015: 58-62." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hybrid super-resolution combining example-based single-image and interpolationbased multi-image reconstruction approaches">
                                        <b>[4]</b>
                                         &lt;i&gt;Batz M&lt;/i&gt;, &lt;i&gt;Eichenseer A&lt;/i&gt;, &lt;i&gt;Seiler J&lt;/i&gt;, et al. &lt;i&gt;Hybrid super&lt;/i&gt;-&lt;i&gt;resolution combining example&lt;/i&gt;-&lt;i&gt;based single&lt;/i&gt;-&lt;i&gt;image and interpolation&lt;/i&gt;-&lt;i&gt;based multi&lt;/i&gt;-&lt;i&gt;image reconstruction approaches&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;IEEE International Conference on Image Processing&lt;/i&gt; (&lt;i&gt;ICIP&lt;/i&gt;) , 2015: 58-62.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" &lt;i&gt;Kim K I&lt;/i&gt;, &lt;i&gt;Kwon Y&lt;/i&gt;. &lt;i&gt;Single&lt;/i&gt;-&lt;i&gt;image super&lt;/i&gt;-&lt;i&gt;resolution using sparse regression and natural image prior&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/i&gt;, 2010, 32 (6) : 1127-1133." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-image super-resolution using sparse regression and natural image prior">
                                        <b>[5]</b>
                                         &lt;i&gt;Kim K I&lt;/i&gt;, &lt;i&gt;Kwon Y&lt;/i&gt;. &lt;i&gt;Single&lt;/i&gt;-&lt;i&gt;image super&lt;/i&gt;-&lt;i&gt;resolution using sparse regression and natural image prior&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/i&gt;, 2010, 32 (6) : 1127-1133.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" &lt;i&gt;Lian Q S&lt;/i&gt;, &lt;i&gt;Zhang W&lt;/i&gt;. &lt;i&gt;Image super&lt;/i&gt;-&lt;i&gt;resolution algorithms based on sparse representation of classified image patches&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;Acta Electronica Sinica&lt;/i&gt;, 2012, 40 (5) : 920-925. 练秋生, 张伟. 基于图像块分类稀疏表示的超分辨率重构算法[&lt;i&gt;J&lt;/i&gt;]. 电子学报, 2012, 40 (5) : 920-925." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201205010&amp;v=MTIwMTBQTXFvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVUw3SklUZlRlN0c0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         &lt;i&gt;Lian Q S&lt;/i&gt;, &lt;i&gt;Zhang W&lt;/i&gt;. &lt;i&gt;Image super&lt;/i&gt;-&lt;i&gt;resolution algorithms based on sparse representation of classified image patches&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;Acta Electronica Sinica&lt;/i&gt;, 2012, 40 (5) : 920-925. 练秋生, 张伟. 基于图像块分类稀疏表示的超分辨率重构算法[&lt;i&gt;J&lt;/i&gt;]. 电子学报, 2012, 40 (5) : 920-925.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" &lt;i&gt;Xiao J S&lt;/i&gt;, &lt;i&gt;Liu E Y&lt;/i&gt;, &lt;i&gt;Zhu L&lt;/i&gt;, et al. &lt;i&gt;Improved image super&lt;/i&gt;-&lt;i&gt;resolution algorithm based on convolutional neural network&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;Acta Optica Sinica&lt;/i&gt;, 2017, 37 (3) : 0318011.  肖进胜, 刘恩雨, 朱力, 等. 改进的基于卷积神经网络的图像超分辨率算法[&lt;i&gt;J&lt;/i&gt;]. 光学学报, 2017, 37 (3) : 0318011." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201703012&amp;v=MDQ4OTlHRnJDVVI3cWZadVp0RmlEa1VMN0pJalhUYkxHNEg5Yk1ySTlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         &lt;i&gt;Xiao J S&lt;/i&gt;, &lt;i&gt;Liu E Y&lt;/i&gt;, &lt;i&gt;Zhu L&lt;/i&gt;, et al. &lt;i&gt;Improved image super&lt;/i&gt;-&lt;i&gt;resolution algorithm based on convolutional neural network&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;Acta Optica Sinica&lt;/i&gt;, 2017, 37 (3) : 0318011.  肖进胜, 刘恩雨, 朱力, 等. 改进的基于卷积神经网络的图像超分辨率算法[&lt;i&gt;J&lt;/i&gt;]. 光学学报, 2017, 37 (3) : 0318011.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" &lt;i&gt;Irani M&lt;/i&gt;, &lt;i&gt;Peleg S&lt;/i&gt;. &lt;i&gt;Improving resolution by image registration&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;CVGIP&lt;/i&gt;: &lt;i&gt;Graphical Models and Image Processing&lt;/i&gt;, 1991, 53 (3) : 231-239." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving resolution by image registration">
                                        <b>[8]</b>
                                         &lt;i&gt;Irani M&lt;/i&gt;, &lt;i&gt;Peleg S&lt;/i&gt;. &lt;i&gt;Improving resolution by image registration&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;CVGIP&lt;/i&gt;: &lt;i&gt;Graphical Models and Image Processing&lt;/i&gt;, 1991, 53 (3) : 231-239.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" &lt;i&gt;Stark H&lt;/i&gt;, &lt;i&gt;Oskoui P&lt;/i&gt;. &lt;i&gt;High&lt;/i&gt;-&lt;i&gt;resolution image recovery from image&lt;/i&gt;-&lt;i&gt;plane arrays&lt;/i&gt;, &lt;i&gt;using convex projections&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;Journal of the Optical Society of America A&lt;/i&gt;, 1989, 6 (11) : 1715-1726." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-resolution image recovery from image-plane arrays, using convex projections">
                                        <b>[9]</b>
                                         &lt;i&gt;Stark H&lt;/i&gt;, &lt;i&gt;Oskoui P&lt;/i&gt;. &lt;i&gt;High&lt;/i&gt;-&lt;i&gt;resolution image recovery from image&lt;/i&gt;-&lt;i&gt;plane arrays&lt;/i&gt;, &lt;i&gt;using convex projections&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;Journal of the Optical Society of America A&lt;/i&gt;, 1989, 6 (11) : 1715-1726.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" &lt;i&gt;Chang H&lt;/i&gt;, &lt;i&gt;Yeung D Y&lt;/i&gt;, &lt;i&gt;Xiong Y M&lt;/i&gt;. &lt;i&gt;Super&lt;/i&gt;-&lt;i&gt;resolution through neighbor embedding&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;Proceedings of the&lt;/i&gt; 2004 &lt;i&gt;IEEE Computer Society Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 2004: 275-282." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Super-resolution through neighbor embedding">
                                        <b>[10]</b>
                                         &lt;i&gt;Chang H&lt;/i&gt;, &lt;i&gt;Yeung D Y&lt;/i&gt;, &lt;i&gt;Xiong Y M&lt;/i&gt;. &lt;i&gt;Super&lt;/i&gt;-&lt;i&gt;resolution through neighbor embedding&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;Proceedings of the&lt;/i&gt; 2004 &lt;i&gt;IEEE Computer Society Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 2004: 275-282.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" &lt;i&gt;Yang J C&lt;/i&gt;, &lt;i&gt;Wright J&lt;/i&gt;, &lt;i&gt;Huang T S&lt;/i&gt;, et al. &lt;i&gt;Image super&lt;/i&gt;-&lt;i&gt;resolution via sparse representation&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;IEEE Transactions on Image Processing&lt;/i&gt;, 2010, 19 (11) : 2861-2873." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">
                                        <b>[11]</b>
                                         &lt;i&gt;Yang J C&lt;/i&gt;, &lt;i&gt;Wright J&lt;/i&gt;, &lt;i&gt;Huang T S&lt;/i&gt;, et al. &lt;i&gt;Image super&lt;/i&gt;-&lt;i&gt;resolution via sparse representation&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;IEEE Transactions on Image Processing&lt;/i&gt;, 2010, 19 (11) : 2861-2873.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" &lt;i&gt;Timofte R&lt;/i&gt;, &lt;i&gt;de Smet V&lt;/i&gt;, &lt;i&gt;van Gool L&lt;/i&gt;. &lt;i&gt;Anchored neighborhood regression for fast example&lt;/i&gt;-&lt;i&gt;based super&lt;/i&gt;-&lt;i&gt;resolution&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;IEEE International Conference on Computer Vision&lt;/i&gt;, 2013: 1920-1927." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Anchored Neighborhood Regression for Fast Example-Based Super-Resolution">
                                        <b>[12]</b>
                                         &lt;i&gt;Timofte R&lt;/i&gt;, &lt;i&gt;de Smet V&lt;/i&gt;, &lt;i&gt;van Gool L&lt;/i&gt;. &lt;i&gt;Anchored neighborhood regression for fast example&lt;/i&gt;-&lt;i&gt;based super&lt;/i&gt;-&lt;i&gt;resolution&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;IEEE International Conference on Computer Vision&lt;/i&gt;, 2013: 1920-1927.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" &lt;i&gt;Timofte R&lt;/i&gt;, &lt;i&gt;de Smet V&lt;/i&gt;, &lt;i&gt;van Gool L&lt;/i&gt;. &lt;i&gt;A&lt;/i&gt;+: &lt;i&gt;adjusted anchored neighborhood regression for fast super&lt;/i&gt;-&lt;i&gt;resolution&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;Asian Conference on Computer Vision&lt;/i&gt;, 2015: 111-126." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A+:Adjusted Anchored Neighborhood Regression for Fast Super-Resolution">
                                        <b>[13]</b>
                                         &lt;i&gt;Timofte R&lt;/i&gt;, &lt;i&gt;de Smet V&lt;/i&gt;, &lt;i&gt;van Gool L&lt;/i&gt;. &lt;i&gt;A&lt;/i&gt;+: &lt;i&gt;adjusted anchored neighborhood regression for fast super&lt;/i&gt;-&lt;i&gt;resolution&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;Asian Conference on Computer Vision&lt;/i&gt;, 2015: 111-126.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" &lt;i&gt;Dong C&lt;/i&gt;, &lt;i&gt;Loy C C&lt;/i&gt;, &lt;i&gt;He K M&lt;/i&gt;, et al. &lt;i&gt;Image super&lt;/i&gt;-&lt;i&gt;resolution using deep convolutional networks&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/i&gt;, 2016, 38 (2) : 295-307." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using deep convolutional networks">
                                        <b>[14]</b>
                                         &lt;i&gt;Dong C&lt;/i&gt;, &lt;i&gt;Loy C C&lt;/i&gt;, &lt;i&gt;He K M&lt;/i&gt;, et al. &lt;i&gt;Image super&lt;/i&gt;-&lt;i&gt;resolution using deep convolutional networks&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/i&gt;, 2016, 38 (2) : 295-307.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" &lt;i&gt;Dong C&lt;/i&gt;, &lt;i&gt;Chen C L&lt;/i&gt;, &lt;i&gt;Tang X&lt;/i&gt;. &lt;i&gt;Accelerating the super&lt;/i&gt;-&lt;i&gt;resolution convolutional neural network&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;European Conference on Computer Vision&lt;/i&gt;, 2016: 391-407." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accelerating the super-resolution convolutional neural network">
                                        <b>[15]</b>
                                         &lt;i&gt;Dong C&lt;/i&gt;, &lt;i&gt;Chen C L&lt;/i&gt;, &lt;i&gt;Tang X&lt;/i&gt;. &lt;i&gt;Accelerating the super&lt;/i&gt;-&lt;i&gt;resolution convolutional neural network&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;European Conference on Computer Vision&lt;/i&gt;, 2016: 391-407.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" &lt;i&gt;Shi W Z&lt;/i&gt;, &lt;i&gt;Caballero J&lt;/i&gt;, &lt;i&gt;Husz&lt;/i&gt;&#225;&lt;i&gt;r F&lt;/i&gt;, et al. &lt;i&gt;Real&lt;/i&gt;-&lt;i&gt;time single image and video super&lt;/i&gt;-&lt;i&gt;resolution using an efficient sub&lt;/i&gt;-&lt;i&gt;pixel convolutional neural network&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 2016: 1874-1883." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network">
                                        <b>[16]</b>
                                         &lt;i&gt;Shi W Z&lt;/i&gt;, &lt;i&gt;Caballero J&lt;/i&gt;, &lt;i&gt;Husz&lt;/i&gt;&#225;&lt;i&gt;r F&lt;/i&gt;, et al. &lt;i&gt;Real&lt;/i&gt;-&lt;i&gt;time single image and video super&lt;/i&gt;-&lt;i&gt;resolution using an efficient sub&lt;/i&gt;-&lt;i&gt;pixel convolutional neural network&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 2016: 1874-1883.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_17" title=" &lt;i&gt;Kim J&lt;/i&gt;, &lt;i&gt;Lee J K&lt;/i&gt;, &lt;i&gt;Lee K M&lt;/i&gt;. &lt;i&gt;Accurate image super&lt;/i&gt;-&lt;i&gt;resolution using very deep convolutional networks&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 2016: 1646-1654." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate image super-resolution using very deep convolutional networks">
                                        <b>[17]</b>
                                         &lt;i&gt;Kim J&lt;/i&gt;, &lt;i&gt;Lee J K&lt;/i&gt;, &lt;i&gt;Lee K M&lt;/i&gt;. &lt;i&gt;Accurate image super&lt;/i&gt;-&lt;i&gt;resolution using very deep convolutional networks&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 2016: 1646-1654.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_18" title=" &lt;i&gt;Sun C&lt;/i&gt;, &lt;i&gt;L&lt;/i&gt;&#252; &lt;i&gt;J W&lt;/i&gt;, &lt;i&gt;Li J W&lt;/i&gt;, et al. &lt;i&gt;Fast image super&lt;/i&gt;-&lt;i&gt;resolution method based on deconvolution&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;Acta Optica Sinica&lt;/i&gt;, 2017, 37 (12) : 1210004.  孙超, 吕俊伟, 李健伟, 等. 基于去卷积的快速图像超分辨率方法[&lt;i&gt;J&lt;/i&gt;]. 光学学报, 2017, 37 (12) : 1210004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201712017&amp;v=MTk1NDFyQ1VSN3FmWnVadEZpRGtVTDdKSWpYVGJMRzRIOWJOclk5RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         &lt;i&gt;Sun C&lt;/i&gt;, &lt;i&gt;L&lt;/i&gt;&#252; &lt;i&gt;J W&lt;/i&gt;, &lt;i&gt;Li J W&lt;/i&gt;, et al. &lt;i&gt;Fast image super&lt;/i&gt;-&lt;i&gt;resolution method based on deconvolution&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;Acta Optica Sinica&lt;/i&gt;, 2017, 37 (12) : 1210004.  孙超, 吕俊伟, 李健伟, 等. 基于去卷积的快速图像超分辨率方法[&lt;i&gt;J&lt;/i&gt;]. 光学学报, 2017, 37 (12) : 1210004.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_19" title=" &lt;i&gt;Gong C&lt;/i&gt;, &lt;i&gt;Tao D C&lt;/i&gt;, &lt;i&gt;Liu W&lt;/i&gt;, et al. &lt;i&gt;Label propagation via teaching&lt;/i&gt;-&lt;i&gt;to&lt;/i&gt;-&lt;i&gt;learn and learning&lt;/i&gt;-&lt;i&gt;to&lt;/i&gt;-&lt;i&gt;teach&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;IEEE Transactions on Neural Networks and Learning Systems&lt;/i&gt;, 2017, 28 (6) : 1452-1465." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Label propagation via teaching-to-learn and learning-to-teach">
                                        <b>[19]</b>
                                         &lt;i&gt;Gong C&lt;/i&gt;, &lt;i&gt;Tao D C&lt;/i&gt;, &lt;i&gt;Liu W&lt;/i&gt;, et al. &lt;i&gt;Label propagation via teaching&lt;/i&gt;-&lt;i&gt;to&lt;/i&gt;-&lt;i&gt;learn and learning&lt;/i&gt;-&lt;i&gt;to&lt;/i&gt;-&lt;i&gt;teach&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;IEEE Transactions on Neural Networks and Learning Systems&lt;/i&gt;, 2017, 28 (6) : 1452-1465.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_20" >
                                        <b>[20]</b>
                                     &lt;i&gt;Jia Y Q&lt;/i&gt;, &lt;i&gt;Shelhamer E&lt;/i&gt;, &lt;i&gt;Donahue J&lt;/i&gt;, et al. &lt;i&gt;Caffe&lt;/i&gt;: &lt;i&gt;convolutional architecture for fast feature embedding&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//&lt;i&gt;Proceedings of the&lt;/i&gt; 22&lt;i&gt;nd ACM International&lt;/i&gt;&lt;i&gt;Conference on Multimedia&lt;/i&gt;- (&lt;i&gt;MM&lt;/i&gt;′14) , 2014: 675-678.</a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_21" title=" &lt;i&gt;Huang J B&lt;/i&gt;, &lt;i&gt;Singh A&lt;/i&gt;, &lt;i&gt;Ahuja N&lt;/i&gt;. &lt;i&gt;Single image super&lt;/i&gt;-&lt;i&gt;resolution from transformed self&lt;/i&gt;-&lt;i&gt;exemplars&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 2015: 5197-5206." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single image super-resolution from transformed self-exemplars">
                                        <b>[21]</b>
                                         &lt;i&gt;Huang J B&lt;/i&gt;, &lt;i&gt;Singh A&lt;/i&gt;, &lt;i&gt;Ahuja N&lt;/i&gt;. &lt;i&gt;Single image super&lt;/i&gt;-&lt;i&gt;resolution from transformed self&lt;/i&gt;-&lt;i&gt;exemplars&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]. &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 2015: 5197-5206.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_22" title=" &lt;i&gt;Du B&lt;/i&gt;, &lt;i&gt;Wang Z&lt;/i&gt;, &lt;i&gt;Zhang L&lt;/i&gt;, et al. &lt;i&gt;Robust and discriminative labeling for multi&lt;/i&gt;-&lt;i&gt;label active learning based on maximum correntropy criterion&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;IEEE Transactions on Image Processing&lt;/i&gt;, 2017, 26 (4) : 1694-1707." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust and discriminative labeling for multi-label active learning based on maximum correntropy criterion">
                                        <b>[22]</b>
                                         &lt;i&gt;Du B&lt;/i&gt;, &lt;i&gt;Wang Z&lt;/i&gt;, &lt;i&gt;Zhang L&lt;/i&gt;, et al. &lt;i&gt;Robust and discriminative labeling for multi&lt;/i&gt;-&lt;i&gt;label active learning based on maximum correntropy criterion&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;IEEE Transactions on Image Processing&lt;/i&gt;, 2017, 26 (4) : 1694-1707.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_23" title=" &lt;i&gt;Du B&lt;/i&gt;, &lt;i&gt;Xiong W&lt;/i&gt;, &lt;i&gt;Wu J&lt;/i&gt;, et al. &lt;i&gt;Stacked convolutional denoising auto&lt;/i&gt;-&lt;i&gt;encoders for feature representation&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;IEEE Transactions on Cybernetics&lt;/i&gt;, 2017, 47 (4) : 1017-1027." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked Convolutional Denoising Auto-Encoders for Feature Representation">
                                        <b>[23]</b>
                                         &lt;i&gt;Du B&lt;/i&gt;, &lt;i&gt;Xiong W&lt;/i&gt;, &lt;i&gt;Wu J&lt;/i&gt;, et al. &lt;i&gt;Stacked convolutional denoising auto&lt;/i&gt;-&lt;i&gt;encoders for feature representation&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;IEEE Transactions on Cybernetics&lt;/i&gt;, 2017, 47 (4) : 1017-1027.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_24" title=" &lt;i&gt;Ye Y X&lt;/i&gt;, &lt;i&gt;Shan J&lt;/i&gt;, &lt;i&gt;Bruzzone L&lt;/i&gt;, et al. &lt;i&gt;Robust registration of multimodal remote sensing images based on structural similarity&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;IEEE Transactions on Geoscience and Remote Sensing&lt;/i&gt;, 2017, 55 (5) : 2941-2958." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust Registration of Multimodal Remote Sensing Images Based on Structural Similarity">
                                        <b>[24]</b>
                                         &lt;i&gt;Ye Y X&lt;/i&gt;, &lt;i&gt;Shan J&lt;/i&gt;, &lt;i&gt;Bruzzone L&lt;/i&gt;, et al. &lt;i&gt;Robust registration of multimodal remote sensing images based on structural similarity&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;]. &lt;i&gt;IEEE Transactions on Geoscience and Remote Sensing&lt;/i&gt;, 2017, 55 (5) : 2941-2958.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(02),89-98 DOI:10.3788/AOS201939.0210003            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于深层残差网络的加速图像超分辨率重建</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B8%AD%E5%BF%97%E7%BA%A2&amp;code=05981488&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">席志红</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BE%AF%E5%BD%A9%E7%87%95&amp;code=40593718&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">侯彩燕</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A2%81%E6%98%86%E9%B9%8F&amp;code=40593719&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">袁昆鹏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%96%9B%E5%8D%93%E7%BE%A4&amp;code=41273197&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">薛卓群</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%93%88%E5%B0%94%E6%BB%A8%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E4%B8%8E%E9%80%9A%E4%BF%A1%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0119964&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">哈尔滨工程大学信息与通信工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对目前卷积神经网络的超分辨率算法存在卷积层数少、模型简单、计算量大、收敛速度慢以及图像纹理模糊等问题, 提出了一种基于深层残差网络的加速图像超分辨率重建方法, 该方法在提高图像分辨率的同时加快收敛速度。设计更深的卷积神经网络模型来提高精确度, 通过残差学习并且使用Adam优化方法使网络模型加速收敛。在原始低分辨率图像上直接进行特征映射, 只在网络的末端引入子像素卷积层, 将像素进行重新排列, 得到高分辨率图像。实验结果表明, 在set 5, set 14, BSD100测试集上, 所提算法的峰值信噪比与结构相似性指数均高于现有的几种算法, 能够恢复更多的图像细节, 图像边缘也更加完整且收敛速度更快。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">超分辨率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">残差网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AD%90%E5%83%8F%E7%B4%A0%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">子像素卷积;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *席志红, E-mail:xizhihong@hrbeu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-05-03</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (60875025);</span>
                    </p>
            </div>
                    <h1>Super-Resolution Reconstruction of Accelerated Image Based on Deep Residual Network</h1>
                    <h2>
                    <span>Xi Zhihong</span>
                    <span>Hou Caiyan</span>
                    <span>Yuan Kunpeng</span>
                    <span>Xue Zhuoqun</span>
            </h2>
                    <h2>
                    <span>College of Information and Communication Engineering, Harbin Engineering University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>An accelerated image super-resolution reconstruction algorithm based on deep residual network is proposed to solve some existing problems, such as few convolutional layers, simple model, large amount of calculation, slow convergence speed and fuzzy image texture. This method improves image resolution and accelerates convergence speed at the same time. First, a deep convolutional neural network model is proposed to improve accuracy, and accelerate convergence of network models is achieved by residual learning and Adam optimization method. Second, feature mapping is performed directly on the original low-resolution image, and the sub-pixel convolutional layer is introduced at the end of the network to rearrange the pixels, so a high-resolution image is obtained. Experimental results show that the proposed algorithm has higher peak signal-to-noise ratio and structural similarity index than those of existing algorithms on set 5, set 14 and BSD100 test sets, and can recover more image details. The image edges are complete and the convergence speed is fast.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=super-resolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">super-resolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolution%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolution neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=residual%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">residual network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sub-pixel%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sub-pixel convolution;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-05-03</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="59" name="59" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="60">超分辨率图像重建技术作为图像处理领域的研究热点, 是指利用一幅或者多幅低分辨率图像重建出一幅高分辨率图像<citation id="170" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。作为一种无需改善硬件设备即可显著提高图像质量的方法, 图像超分辨率重建技术在视频监控、高清晰数字电视、医学成像、遥感图像等领域中得到了广泛应用。图像超分辨率重建技术可以分为基于插值<citation id="160" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、基于重建<citation id="161" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>和基于学习<citation id="171" type="reference"><link href="21" rel="bibliography" /><link href="23" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>的方法。通过基于插值的方法得到的图像容易产生模糊、锯齿现象, 常见的插值方法有最近邻插值和双三次插值方法等。基于重建的方法在利用低分辨率作为约束的前提下, 结合图像的先验知识进行还原, 主要包括配准与重建两个部分, 常见的算法有迭代反向投影法<citation id="162" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和凸集投影法<citation id="163" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>等。现在主流的方法是基于学习的方法, 该方法通过提取高分辨率和低分辨率图像的特征学习两者之间的映射关系, 从而重建高分辨率图像, Chang等<citation id="164" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出邻域嵌入方法, 之后, Yang等<citation id="165" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>研究了基于稀疏编码的方法, Timofte等<citation id="172" type="reference"><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>提出了固定邻域回归 (ANR) 和调整的固定邻域回归的方法。近几年, 基于深度学习的超分辨率方法成为研究热点。2016年, Dong等<citation id="166" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>率先提出基于超分辨率卷积神经网络 (SRCNN) 的方法, 使用了三个卷积层, 将图像特征从低分辨率空间非线性映射到高分辨率空间, 重建效果优于其他传统方法。为了提高速度, Dong等<citation id="167" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>对SRCNN算法进行改进, 以低分辨率图像作为输入, 在网络最后用反卷积层进行相应比例放大, 得到高分辨率图像, 提出了基于反卷积的快速图像超分辨率重建 (FSRCNN) 。之后, Shi等<citation id="168" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>继续对SRCNN进行改进, 同样是直接在低分辨率图像上进行卷积, 通过子像素卷积层将特征图像进行重新排列, 得到高分辨率图像, 提出了高效子像素卷积神经网络 (ESPCN) 。为了提取更多的图像特征, Kim等<citation id="169" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>加深了卷积层数, 提出了一个20层的深度卷积神经网络模型 (VDSR) , 提高了图像的重建质量。</p>
                </div>
                <div class="p1">
                    <p id="61">但上述方法仍然存在一些问题, 虽然FSRCNN和ESPCN提高了网络训练速度, 但是卷积层数相对较少, ESPCN只有三层卷积操作, 无法充分提取图像更多的细节特征, 并且网络模型没有对边界使用零填充, 使输出图像在经过卷积操作后尺寸减小, 无法充分利用图像的边缘信息。VDSR虽然加深了网络深度, 但是它的输入图像需要进行双三次插值放大的预处理, 使得网络计算量加大、计算速度减慢且网络难以收敛。</p>
                </div>
                <div class="p1">
                    <p id="62">针对以上问题, 本文提出一种基于深层残差网络的加速图像超分辨率重建方法 (DRSR) , 使用深层网络结构, 直接在低分辨率图像上提取特征, 由于不需要上采样插值, 可以使用较小的卷积核级联更多的卷积层, 为了加快网络收敛, 避免训练过程中产生梯度弥散和梯度爆炸, 使用多路径模式的局部残差学习与多权重的递归学习相结合的模型, 并且使用Adam优化方法, 最后在网络的末端通过子像素卷积层将像素进行重新排列, 得到高分辨率图像。为了充分利用图像边缘信息, DRSR对图像边缘采用零填充, 使输出图像与输入图像具有相同的尺寸, 相比于其他方法, DRSR的超分辨率重建效果更好, 并且大大加快了收敛速度。</p>
                </div>
                <h3 id="63" name="63" class="anchor-tag">2 相关工作</h3>
                <h4 class="anchor-tag" id="64" name="64"><b>2.1</b><b>SRCNN算法</b></h4>
                <div class="p1">
                    <p id="65">SRCNN算法使用双三次插值将低分辨率图像升级到所需的大小, 然后通过特征提取、非线性映射和重建, 最终输出高分辨率图像。SRCNN结构图如图1所示, 图中Conv为卷积运算, ReLU为线性整流函数。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902011_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 SRCNN结构图" src="Detail/GetImg?filename=images/GXXB201902011_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 SRCNN结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902011_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Diagram of SRCNN structure</p>

                </div>
                <div class="p1">
                    <p id="67">特征提取的目的是从低分辨率图像中有重叠地提取图像块并用高维向量表示, 相当于采用一组滤波器对图像进行卷积操作, 非线性映射阶段是将上一层的特征向量从低分辨率空间变换至高分辨率空间, 图像重建是将重叠的高分辨率图像块利用平均化操作产生最终的图像, 该过程可表示为</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>F</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>*</mo><mi>Ι</mi><msup><mrow></mrow><mrow><mtext>L</mtext><mtext>R</mtext></mrow></msup><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>F</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>f</mi><mo stretchy="false">[</mo><mi>W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>*</mo><mi>F</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">]</mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Ι</mi><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>R</mtext></mrow></msup><mo>=</mo><mi>F</mi><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>W</mi><msub><mrow></mrow><mn>3</mn></msub><mo>*</mo><mi>F</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>3</mn></msub><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">式中<i>I</i><sup>LR</sup>为输入的低分辨率图像, <i>W</i><sub><i>k</i></sub> (<i>k</i>=1, 2, 3) 为滤波器, *表示卷积运算, <i>b</i><sub><i>k</i></sub>为偏差, <i>f</i> (<i>x</i>) 为激活函数, <i>F</i><sub>3</sub> (<i>Y</i>) 为最终的高分辨率图像, 即<i>I</i><sup>SR</sup>。</p>
                </div>
                <div class="p1">
                    <p id="70">上述三个操作结合在一起, 形成一个卷积神经网络。参数<i>θ</i>={<i>W</i><sub>1</sub>, <i>W</i><sub>2</sub>, <i>W</i><sub>3</sub>, <i>b</i><sub>1</sub>, <i>b</i><sub>2</sub>, <i>b</i><sub>3</sub>}, 该模型对所有的权重和偏重都进行了优化, 给定高分辨率数据集{<i>X</i><sub><i>i</i></sub>}及其对应的低分辨率数据集{<i>Y</i><sub><i>i</i></sub>}, 使用均方误差作为其损失函数, 即</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>s</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi>F</mi><mo stretchy="false"> (</mo><mi>Y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>;</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>-</mo><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">式中<i>n</i>为训练的样本数, {<i>Y</i><sub><i>i</i></sub>}需要{<i>X</i><sub><i>i</i></sub>}经过下采样后再通过双三次插值放大到与原图像相同的尺寸。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73"><b>2.2</b><b>ESPCN算法</b></h4>
                <div class="p1">
                    <p id="74">ESPCN直接在低分辨率图像上提取特征, 不需要通过上采样插值, 通过插值放大的图像本身会带来额外的干扰信息, 在之后的特征提取阶段需要用相对较大的卷积核尺寸, 使复杂度增加, 而且会受到插值放大带来的额外干扰信息影响, 通过在网络的末端用子像素卷积层将像素进行重新排列来得到高分辨率图像, 从而大大减小计算复杂度, 提高计算速度。ESPCN结构图如图2所示。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902011_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 ESPCN结构图" src="Detail/GetImg?filename=images/GXXB201902011_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 ESPCN结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902011_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Diagram of ESPCN structure</p>

                </div>
                <div class="p1">
                    <p id="76">卷积层表达式分别为</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>F</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>*</mo><mi>Y</mi><msup><mrow></mrow><mrow><mtext>L</mtext><mtext>R</mtext></mrow></msup><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>F</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>f</mi><mo stretchy="false">[</mo><mi>W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>*</mo><mi>F</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">]</mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Y</mi><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>R</mtext></mrow></msup><mo>=</mo><mi>F</mi><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false">[</mo><mi>W</mi><msub><mrow></mrow><mn>3</mn></msub><mo>*</mo><mi>F</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">]</mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">式中:<i>P</i> (<i>x</i>) 为亚像素卷积函数, 即将低分辨率图像特征按照特定位置周期性地插入到高分辨率图像中;<i>Y</i><sup>LR</sup>为原始低分辨率图像;<i>Y</i><sup>SR</sup>为最终的高分辨率图像。输入、输出图像尺寸分别为25×25和17×17。卷积层的网络参数设置为:<i>n</i><sub>1</sub>=64, <i>k</i><sub>1</sub>=5, <i>n</i><sub>2</sub>=32, <i>k</i><sub>2</sub>=3, <i>n</i><sub>3</sub>=9, <i>k</i><sub>3</sub>=3。</p>
                </div>
                <h3 id="79" name="79" class="anchor-tag">3 DRSR算法</h3>
                <div class="p1">
                    <p id="80">本文算法 (DRSR算法) 使用了较深的卷积网络, 用较小的卷积核进行级联, 直接在低分辨率图像上提取特征, 由于增加了网络深度, 使用局部残差的网络结构来加快模型收敛, 提高精度, 最后用子像素卷积层将像素进行重新排列, 得到高分辨率图像, 本文算法的网络结构图如图3所示。与VDSR的全局残差学习不同, 本文算法使用的是多路径模式的局部残差学习与多权重的递归学习相结合的模式, 其中每个残差单元都共同拥有一个相同的输入, 即递归块中的第一个卷积层的输出, 每个残差单元都包含2个卷积层。在一个递归块内, 每个残差单元内对应位置相同的卷积层参数都是共享的。为避免卷积后图像信息丢失, 在卷积过程中使用零填充保证输出图像与输入图像尺寸相等<citation id="173" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。在网络设计中, 池化层常用于图像分类的神经网络中, 通过对卷积后的结果进行下采样, 可减少部分参数, 获得旋转、平移、伸缩等不变性, 超分辨率算法的目标是增加图像的细节信息, 与池化层减少细节信息的目标相悖, 因此本文算法不使用池化层。</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>3.1</b><b>深层神经网络结构</b></h4>
                <div class="p1">
                    <p id="82">在未经过插值放大的情况下直接对原始低分辨率图像进行特征提取, 由于输入分辨率降低, 可以使用较小的卷积核来提取相同的信息, 一个9×9的卷积核等同于4个3×3的卷积核依次级联卷积, 但是由于额外引入了卷积层, 可以有效地提取更多的图像特征, 借鉴牛津大学VGG小组提出的基于卷积神经网络的视觉识别算法, 该算法采用VGG-ILSVRC-16-layers模型, 网络中所有卷积层均选取大小为3×3、数量为32的滤波器, 通道数量<i>c</i>仍然为1, 卷积层表示为Conv (32, 3, 1) , 即</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902011_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 本文算法网络结构图" src="Detail/GetImg?filename=images/GXXB201902011_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 本文算法网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902011_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Diagram of network structure of the proposed algorithm</p>

                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>F</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>*</mo><mi>Y</mi><msup><mrow></mrow><mrow><mtext>L</mtext><mtext>R</mtext></mrow></msup><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd columnalign="left"><mi>F</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>f</mi><mo stretchy="false">[</mo><mi>W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>*</mo><mi>F</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd columnalign="left"><mtext> </mtext><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mo>⋮</mo></mtd></mtr><mtr><mtd columnalign="left"><mi>F</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>f</mi><mo stretchy="false">[</mo><mi>W</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>*</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">]</mo></mtd></mtr></mtable></mrow></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Y</mi><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>R</mtext></mrow></msup><mo>=</mo><mi>F</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false">[</mo><mi>W</mi><msub><mrow></mrow><mi>n</mi></msub><mo>*</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">]</mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">式中<i>W</i><sub>1</sub>=32×3×3×1, <i>W</i><sub><i>i</i></sub>=32×3×3×32 (<i>i</i>=2, 3, …, <i>n</i>-1) , <i>W</i><sub><i>n</i></sub>=<i>r</i><sup>2</sup>×3×3×32。</p>
                </div>
                <div class="p1">
                    <p id="87"><i>P</i> (<i>x</i>) 的作用是将大小为<i>r</i><sup>2</sup>×<i>H</i>×<i>W</i>×<i>C</i> (<i>H</i>、<i>W</i>分别为图像的高度与宽度, <i>C</i>代表通道数) 的图像重新排列为大小为<i>rH</i>×<i>rW</i>×<i>C</i>的高分辨率图像<i>Y</i><sup>SR</sup>, 即将低分辨率图像特征按照特定位置周期性地插入到高分辨率图像中。通过使用子像素卷积层对图像进行重新排列, 在图像从低分辨率到高分辨率的放大过程中, 插值函数被隐含地包含在前面的卷积层中, 实现自动学习。只在最后一层对图像大小做变换, 由于前面的卷积运算在低分辨率图像上进行, 因此计算效率得到提高。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88"><b>3.2</b><b>残差网络模型</b></h4>
                <div class="p1">
                    <p id="89">在计算机视觉里, 特征的等级随着网络深度的加深而变高, 增加网络深度可以有效地提取更多的特征, 然而梯度弥散/爆炸成为训练深层次网络的障碍, 导致网络难以收敛。为了加快网络收敛, 使用多路径模式的局部残差学习与多权重的递归学习相结合的模式, 其结构图如图4所示。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902011_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 残差网络结构图" src="Detail/GetImg?filename=images/GXXB201902011_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 残差网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902011_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Residual network structure</p>

                </div>
                <div class="p1">
                    <p id="91">局部残差网络单元如图4中虚线框所示, 如果设第一个残差单元的输入为<i>x</i><sub><i>l</i></sub>, 输出为<i>x</i><sub><i>l</i>+1</sub>, 则残差单元的结构可以表示为</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>x</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>x</mi><msub><mrow></mrow><mi>l</mi></msub><mo>+</mo><mi>F</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>l</mi></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>F</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>l</mi></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>w</mi><msub><mrow></mrow><mi>l</mi></msub><mi>σ</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msub><mi>x</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">式中<i>F</i> (<i>x</i>) 为卷积函数, <i>σ</i> (·) 为激活函数, <i>w</i><sub><i>l</i></sub>、<i>w</i><sub><i>l</i>-1</sub>为权重。任意单元<i>x</i><sub><i>L</i></sub>可表示为</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msub><mrow></mrow><mi>L</mi></msub><mo>=</mo><mi>x</mi><msub><mrow></mrow><mi>l</mi></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>l</mi></mrow><mrow><mi>L</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>F</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">对于标准普通网络, <i>x</i><sub><i>L</i></sub>可表示为</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msub><mrow></mrow><mi>L</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mi>l</mi></mrow><mrow><mi>L</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">由于求和的计算量远小于求积的计算量, 因此残差网络可以加快收敛速度。</p>
                </div>
                <div class="p1">
                    <p id="98">假设损失函数为<i>C</i>, 根据反向传播链式求导法则可以得到:</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>C</mi></mrow><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mi>l</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>C</mi></mrow><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mi>L</mi></msub></mrow></mfrac><mfrac><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mi>L</mi></msub></mrow><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mi>l</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>C</mi></mrow><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mi>L</mi></msub></mrow></mfrac><mrow><mo>[</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mo>∂</mo><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mi>l</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>l</mi></mrow><mrow><mi>L</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>F</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100"> (14) 式中∂<i>C</i>/∂<i>x</i><sub><i>L</i></sub>保证信息能直接传回任意层<mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msub><mrow></mrow><mi>l</mi></msub><mo>‚</mo><mn>1</mn><mo>+</mo><mfrac><mo>∂</mo><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mi>l</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>l</mi></mrow><mrow><mi>L</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>F</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>保证网络不会出现梯度消失的现象。</p>
                </div>
                <div class="p1">
                    <p id="102">局部残差网络结构是将局部残差单元顺序堆叠, 不同的残差单元使用不同的输入, 在此基础上使用了多路径结构, 所有的残差单元共享相同的输入。与链式模式相比, 这种多路径模式更利于学习, 并且不容易过度拟合<citation id="174" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。残差单元可表示为</p>
                </div>
                <div class="p1">
                    <p id="103" class="code-formula">
                        <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>x</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>g</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>F</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>l</mi></msub><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">) </mo><mo>+</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>=</mo><mi>f</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="104">式中<i>g</i> (<i>x</i><sub><i>l</i></sub>) 为残差单元的函数, <i>x</i><sub>0</sub>为递归块中第一层卷积的结果, <i>f</i><sub>0</sub> (<i>x</i>) 为激活函数。由于残差单元是递归学习的, 权重集合<b><i>w</i></b>在递归块内的残差单元之间共享。之后, 通过叠加几个递归块得到最终的输出, 再通过子像素卷积层得到高分辨率图像。</p>
                </div>
                <h4 class="anchor-tag" id="105" name="105"><b>3.3</b><b>参数优化</b></h4>
                <div class="p1">
                    <p id="106">深层卷积神经网络是一个端对端的映射, 整个网络在训练的过程中, 会通过反向传播算法优化网络, 使其得到最优的参数。本文算法依旧使用均方误差作为损失函数, 使用Adam代替随机梯度下降法来优化参数, 使损失最小化。Adam利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率, 其优点在于经过偏置校正后, 每一次迭代学习率都有一个确定范围, 使得参数比较平稳。梯度的一阶矩估计<i>m</i><sub><i>t</i></sub>和二阶矩估计<i>n</i><sub><i>t</i></sub>可分别表示为</p>
                </div>
                <div class="p1">
                    <p id="107" class="code-formula">
                        <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>m</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>β</mi><msub><mrow></mrow><mn>1</mn></msub><mo>⋅</mo><mi>m</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>β</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>⋅</mo><mi>g</mi><msub><mrow></mrow><mi>t</mi></msub><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>n</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>β</mi><msub><mrow></mrow><mn>2</mn></msub><mo>⋅</mo><mi>n</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>β</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>⋅</mo><mi>g</mi><msubsup><mrow></mrow><mi>t</mi><mn>2</mn></msubsup><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="108">式中<i>β</i><sub>1</sub>=0.9, <i>β</i><sub>2</sub>=0.999, 下标<i>t</i>表示当前时刻, 下标<i>t</i>-1表示前一时刻。<mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>m</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>、<mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>n</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>是对m<sub>t</sub>、n<sub>t</sub>的校正, <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover accent="true"><mi>m</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo></mrow><mfrac><mrow><mi>m</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mn>1</mn><mo>-</mo><mi>β</mi><msubsup><mrow></mrow><mn>1</mn><mi>t</mi></msubsup></mrow></mfrac><mo>‚</mo><mover accent="true"><mi>n</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mn>1</mn><mo>-</mo><mi>β</mi><msubsup><mrow></mrow><mn>2</mn><mi>t</mi></msubsup></mrow></mfrac></mrow></math></mathml>;<mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>-</mo><mfrac><mi>η</mi><mrow><msqrt><mrow><mover accent="true"><mi>n</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></msqrt><mo>+</mo><mi>ε</mi></mrow></mfrac></mrow></math></mathml>对学习率形成一个动态约束, β<sub>1</sub>和β<sub>2</sub>为矩估计指数衰减速率;θ为模型参数;η为步长。<mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>θ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mfrac><mi>η</mi><mrow><msqrt><mrow><mover accent="true"><mi>n</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></msqrt><mo>+</mo><mi>ε</mi></mrow></mfrac><mover accent="true"><mi>m</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>‚</mo><mi>ε</mi><mo>=</mo><mrow><mn>1</mn><mn>0</mn></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>8</mn></mrow></msup></mrow></math></mathml>。</p>
                </div>
                <h3 id="114" name="114" class="anchor-tag">4 实验结果与分析</h3>
                <div class="p1">
                    <p id="115">本文算法使用的系统为<i>Ubuntu</i> 16.04, 在该系统下搭建了<i>caffe</i><citation id="175" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>框架进行训练, 在<i>MatlabR</i>2014下进行实验测试。电脑硬件配置为<i>Inter Core i</i>5-4460@3.2 <i>GHz</i>, <i>Nvidia GeForce GTX</i> 750<i>Ti</i>。通过实验验证本文算法的优势, 首先验证了深层网络的较浅层网络有更好的性能, 然后通过对激活函数、运行速度、优化方法及滤波器数目的比较得到了最优的设置, 最后通过实验表明局部残差网络可以使深层网络更快、更好地收敛, 充分证明了本文算法无论在速度还是精度上都有明显优势。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116"><b>4.1</b><b>数据集</b></h4>
                <div class="p1">
                    <p id="117">在SRCNN、ESPCN等方法中, 选择基于学习的超分辨率算法中广泛使用的91幅数据集作为训练数据。由于加深了网络深度, 为了避免过拟合, 需要扩大图像训练集, 将这91幅图像先分别旋转90°、180°和270°之后, 再将图像缩放为原来的50%、60%、70%、80%和90%, 此时得到的训练数据是之前的24倍。将其中1820 (91×4×5) 幅图像作为训练集, 将364 (91×4×10) 幅图像作为训练过程中的验证集, 将set 5、set 14、BSD100作为测试集。将1820幅原始图像进行<i>r</i>倍下采样得到<i>Y</i><sup>LR</sup>, 然后按照步幅<i>s</i>=14将图像裁剪成大小为25×25的子像素块, 并将其与相应的裁剪后的高分辨率像素块组成训练数据, 学习率设置为0.0001, 为了抑制振荡, 动量设置为0.9, 优化方法选择Adam, 在确定范围内动态调整学习率。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118"><b>4.2</b><b>层数设置</b></h4>
                <div class="p1">
                    <p id="119">文献<citation id="177" type="reference">[<a class="sup">21</a>,<a class="sup">22</a>]</citation>表明, 网络的深度是实现高分辨率图像的重要因素, 增加网络深度可以提取更多的特征, 然而深层网络会出现难以收敛和过拟合的情况, 使网络训练的准确率下降<citation id="176" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。本文算法增加了数据集并且采用局部残差网络结构, 可以加速网络收敛以及有效地避免过拟合现象。当放大倍数为3时, 分别训练和测试了层数为6, 10, 12的网络模型, 即分别有2, 4, 5个残差单元。为了确保网络完全收敛, 共迭代了10<sup>6</sup>次。图5 (a) 为10层和12层网络的训练集与验证集随迭代次数变化的关系曲线, 从图中可以看出, 最终train loss和validation loss都趋于平稳, 近似于平行, 说明网络已经完全收敛, 没有出现过拟合, 并且最终的12层网络的损失函数值更低, 说明12层网络的效果更好。为进一步验证模型的有效性, 选取set 5作为测试集对网络进行测试, 并使用峰值信噪比 (PSNR) 与结构相似性指数 (SSIM) 的平均值作为客观评价指标, 图5 (b) 为测试集set 5随迭代次数的增长曲线, 从图中可以看出, 12层网络的PSNR平均值虽然是最大的, 但是相比于10层网络, 二者差距很小, 又对比了两者的SSIM平均值, 最终结果说明了12层网络的PSNR与SSIM平均值均为最佳, 因此最终选择了12层的网络模型。为了选取合适的层数, 在表1中对比了不同层数的SSIM平均值。</p>
                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902011_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 (a) 12层网络的损失函数随迭代次数的变化; (b) set 5的PSNR平均值在不同层数下随迭代次数的变化" src="Detail/GetImg?filename=images/GXXB201902011_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 (a) 12层网络的损失函数随迭代次数的变化; (b) set 5的PSNR平均值在不同层数下随迭代次数的变化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902011_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 (a) Variation of loss function of 12-layer network with number of iterations; (b) variation of PSNR average value of set 5 with number of iterations under different layers</p>

                </div>
                <div class="area_img" id="121">
                    <p class="img_tit">表1 不同深度下各测试集的PSNR/SSIM均值 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 PSNR/SSIM average values of test sets at different depths</p>
                    <p class="img_note"></p>
                    <table id="121" border="1"><tr><td rowspan="2"><br />Depth</td><td colspan="2"><br />PSNR average value</td><td rowspan="2"></td><td colspan="2"><br />SSIM average value</td></tr><tr><td><br />set 5</td><td>set 14</td><td><br />set 5</td><td>set 14</td></tr><tr><td>6</td><td>33.48</td><td>29.65</td><td></td><td>0.9249</td><td>0.8936</td></tr><tr><td><br />10</td><td>33.58</td><td>29.67</td><td></td><td>0.9265</td><td>0.8941</td></tr><tr><td><br />12</td><td>33.60</td><td>29.69</td><td></td><td>0.9268</td><td>0.8944</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="122" name="122"><b>4.3</b><b>激活函数比较</b></h4>
                <div class="p1">
                    <p id="123">在网络训练过程中, 激活函数的选择会影响模型的性能与速度。Sigmoid函数的计算量较大, 收敛缓慢且容易发生梯度消失的情况, 已经逐渐不再使用;ReLU的表达式为<i>f</i> (<i>x</i>) =max (0, <i>x</i>) , 其计算复杂度低, 但其在训练时很脆弱, 极有可能会导致神经元坏死;tanh函数的表达式为<i>f</i> (<i>x</i>) = (1-e<sup>-2<i>x</i></sup>) / (1+e<sup>-2<i>x</i></sup>) , 虽然其计算复杂度较高, 但是其输出均值为0, 很好地解决了收敛速度慢与梯度容易消失的问题。如图6所示, 网络层数为12时, 在测试集set 5下将tanh与ReLU进行对比, 可以看出tanh表现出的性能优于ReLU, 并且比ReLU更快地达到收敛。因此, 最终选用tanh作为激活函数。</p>
                </div>
                <h4 class="anchor-tag" id="124" name="124"><b>4.4</b><b>速度比较</b></h4>
                <div class="p1">
                    <p id="125">网络结构的加深使模型复杂度增加, 导致速度减慢。残差网络结构使得收敛速度加快, 而且直接在低分辨率图像上进行卷积操作, 减小了计算复杂度, 使得计算速度有所提升。图7所示为在set 5测试集下, 不同算法处理图像所用的平均时间, 可以看出, 虽然本文算法使用了较深的网络模型, 但是处理图像的速度得到提升, 并且PSNR平均值明显增大, 因此最终选取深度为12的网络模型。</p>
                </div>
                <div class="area_img" id="126">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902011_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 set 5的PSNR平均值在不同激活函数下随迭代次数的变化" src="Detail/GetImg?filename=images/GXXB201902011_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 set 5的PSNR平均值在不同激活函数下随迭代次数的变化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902011_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Variation of PSNR average value of set 5 under different activation functions with number of iterations</p>

                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902011_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 测试集set 5在不同算法下的运行时间与PSNR均值的关系图" src="Detail/GetImg?filename=images/GXXB201902011_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 测试集set 5在不同算法下的运行时间与PSNR均值的关系图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902011_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Relationship between running time and PSNR average value of set 5 under different algorithms</p>

                </div>
                <h4 class="anchor-tag" id="128" name="128"><b>4.5</b><b>优化方法比较</b></h4>
                <div class="p1">
                    <p id="129">在12层 (包含5个残差单元的深度) 分别训练了优化方法为随机梯度下降法 (SGD) 、加速梯度下降法 (NAG) 和Adam的网络模型。从图8可以看出, Adam优化方法和NAG明显优于SGD, 并且Adam优化方法也优于NAG。因此, 选择Adam优化方法。</p>
                </div>
                <h4 class="anchor-tag" id="130" name="130"><b>4.6</b><b>滤波器数目比较</b></h4>
                <div class="p1">
                    <p id="131">为了评估不同滤波器数目对超分辨率性能的影响, 在网络层数为12时分别训练了滤波器数目为16, 32, 64的网络模型, 选择放大倍数为3, 并在set 5上进行测试, 结果如图9所示。可以看出, 增加滤波器数目虽然可以提高滤波性能, 但是由于卷积层数较多, 增大滤波器数目使得模型复杂度和计算量大大增加, 导致发生了过拟合现象, 并且降低了计算速度, 综合考虑超分辨率的性能与计算速度, 选用滤波器数目为32的网络模型, 以达到更好的效果。</p>
                </div>
                <div class="area_img" id="132">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902011_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 set 5的PSNR平均值在不同优化方法下随迭代次数的变化" src="Detail/GetImg?filename=images/GXXB201902011_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 set 5的PSNR平均值在不同优化方法下随迭代次数的变化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902011_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Variation of PSNR average value of set 5 under different optimization methods with number of iterations</p>

                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902011_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 set 5的PSNR平均值在不同滤波器数目下随迭代次数的变化" src="Detail/GetImg?filename=images/GXXB201902011_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 set 5的PSNR平均值在不同滤波器数目下随迭代次数的变化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902011_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Variation of PSNR average value of set 5 under different filter numbers with number of iterations</p>

                </div>
                <h4 class="anchor-tag" id="134" name="134"><b>4.7</b><b>残差网络设置</b></h4>
                <div class="p1">
                    <p id="135">残差网络特殊的网络结构可以加快深度网络模型的收敛, 为了验证其特性, 分别将残差网络与标准网络进行实验比较, 选取放大倍数为3, 测试集为set 5, 网络深度分别为6, 8, 10, 12 (分别有2, 3, 4, 5个残差单元) , 激活函数为tanh以及优化方法为Adam进行验证, 其收敛曲线如图10所示。可以看出, 残差网络的收敛速度要明显高于普通网络, 而且10层和12层网络的收敛速度比6层和8层的更快, PSNR平均值更高。由此可知, 增加残差单元的个数可以使深层网络更快地收敛, 更好地提升超分辨率重建的性能与速度。</p>
                </div>
                <h4 class="anchor-tag" id="136" name="136"><b>4.8</b><b>实验结果</b></h4>
                <div class="p1">
                    <p id="137">将本文算法分别与HE、Bicubic、ScSR<citation id="178" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation> (Image Super Resolution via Sparse Representation) 、NE+LLE<citation id="179" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation> (Super-Resolution Through Neighbor Embedding) 、ANR<citation id="180" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、SRCNN<citation id="181" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>和ESPCN<citation id="182" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>进行比较。由于受实验环境以及迭代次数的影响, 实验结果与SRCNN<citation id="183" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、ESPCN<citation id="184" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>等原文献略有不同, 所有算法均在放大倍数为2, 3, 4的比例下进行实验, 图11、12为放大倍数为3的情况下不同方法重建的效果图, 为了进一步观察, 对图像某些部分进行了局部放大, 可以看出, 双三次插值的效果最为模糊。图像经ScSR与NE+LLE处理后, 虽然清晰度有所提升, 但是图像细节效果仍有待提高, ANR, SRCNN, ESPCN算法的整体视觉效果较清晰, 边缘处也有很好的连续性, 但易产生伪影现象。对比以上算法, 本文算法可以恢复更多图像的细节, 消除伪影现象, 得到的图像边缘也更加锐利, 视觉效果更好。</p>
                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902011_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 set 5的PSNR平均值在不同网络模型下随迭代次数的变化。 (a) 6层和8层网络; (b) 10层和12层网络" src="Detail/GetImg?filename=images/GXXB201902011_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 set 5的PSNR平均值在不同网络模型下随迭代次数的变化。 (a) 6层和8层网络; (b) 10层和12层网络  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902011_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Variation of PSNR average value of set 5 under different network models with number of iterations. (a) Networks of 6-layer and 8-layer; (b) networks of 10-layer and 12-layer</p>

                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902011_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 Monarch在不同算法下的效果图" src="Detail/GetImg?filename=images/GXXB201902011_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 Monarch在不同算法下的效果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902011_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Effect of Monarch under different algorithms</p>

                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902011_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 Comic在不同算法下的效果图" src="Detail/GetImg?filename=images/GXXB201902011_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 Comic在不同算法下的效果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902011_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Effect of Comic under different algorithms</p>

                </div>
                <div class="p1">
                    <p id="142">客观评价标准使用PSNR和SSIM<citation id="185" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>两个指标对算法进行评估。PSNR用来衡量两幅图像之间的强度差, 其计算式为</p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>Ν</mtext><mtext>R</mtext></mrow></msub><mo>=</mo><mn>1</mn><mn>0</mn><mi>lg</mi><mfrac><mrow><mrow><mn>2</mn><mn>5</mn><mn>5</mn></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>Μ</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>E</mtext></mrow></msub></mrow></mfrac><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144">式中<i>M</i><sub>SE</sub>表示均方误差。<i>P</i><sub>SNR</sub>越大, 图像的重建效果越好。</p>
                </div>
                <div class="p1">
                    <p id="145">SSIM能更好地表达恢复图像与参考图像之间的结构相似性, 结构相似度的值越接近1, 说明原始标准图像块<i>x</i>和重建图像块<mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>x</mi><mo>^</mo></mover></math></mathml>之间越相似、重建图像质量效果越好。<i>SSIM</i>计算公式为</p>
                </div>
                <div class="p1">
                    <p id="147" class="code-formula">
                        <mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>Ι</mtext><mtext>Μ</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mn>2</mn><mi>u</mi><msub><mrow></mrow><mi>x</mi></msub><mi>u</mi><msub><mrow></mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover></msub><mo>+</mo><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mn>2</mn><mi>σ</mi><msub><mrow></mrow><mrow><mi>x</mi><mover accent="true"><mi>x</mi><mo>^</mo></mover></mrow></msub><mo>+</mo><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow><mrow><mo stretchy="false"> (</mo><mi>u</mi><msubsup><mrow></mrow><mi>x</mi><mn>2</mn></msubsup><mo>+</mo><mi>u</mi><msubsup><mrow></mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><mn>2</mn></msubsup><mo>+</mo><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>σ</mi><msubsup><mrow></mrow><mi>x</mi><mn>2</mn></msubsup><mo>+</mo><mi>σ</mi><msubsup><mrow></mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><mn>2</mn></msubsup><mo>+</mo><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="148">式中u<sub>x</sub>、<mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>u</mi><msub><mrow></mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover></msub></mrow></math></mathml>分别表示图像x和<mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>x</mi><mo>^</mo></mover></math></mathml>的均值, σ<sub>x</sub>、<mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><msub><mrow></mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover></msub></mrow></math></mathml>分别表示图像x和<mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>x</mi><mo>^</mo></mover></math></mathml>的方差, <mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><msub><mrow></mrow><mrow><mi>x</mi><mover accent="true"><mi>x</mi><mo>^</mo></mover></mrow></msub></mrow></math></mathml>表示图像<i>x</i>和<mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>x</mi><mo>^</mo></mover></math></mathml>的协方差, C<sub>1</sub>、C<sub>2</sub>为常数。</p>
                </div>
                <div class="p1">
                    <p id="155">不同测试集分别在放大倍数为2, 3, 4时的<i>PSNR</i>平均值和<i>SSIM</i>平均值如表2和表3所示。可以看出, 本文算法的<i>PSNR</i>平均值与<i>SSIM</i>平均值较传统的双三次插值算法有明显提高, 相比其他算法也均有所提高, 在放大倍数为2时, <i>set</i> 5测试集的<i>PSNR</i>平均值可以达到37.41 <i>dB</i>, <i>SSIM</i>平均值达到0.9621, 相比于<i>Bicubic</i>, <i>PSNR</i>平均值提高了约4 <i>dB</i>, <i>SSIM</i>提高了0.03。</p>
                </div>
                <div class="area_img" id="156">
                    <p class="img_tit">表2 测试集<i>set</i> 5, <i>set</i> 14, <i>BSD</i>100在不同算法下的<i>PSNR</i>平均值 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Table</i> 2 <i>PSNR average value of set</i> 5, <i>set</i> 14, <i>and BSD</i>100 <i>under different algorithms</i></p>
                    <p class="img_note"></p>
                    <table id="156" border="1"><tr><td><i>Data set</i></td><td><i>Scale</i></td><td><i>Bicubic</i></td><td><i>ScSR</i></td><td><i>NE</i>+<i>LLE</i></td><td><i>ANR</i></td><td><i>SRCNN</i></td><td><i>ESPCN</i></td><td><i>DRSR</i></td></tr><tr><td></td><td>×2</td><td>33.65</td><td>35.13</td><td>35.76</td><td>35.83</td><td>36.36</td><td>36.39</td><td>37.41</td></tr><tr><td><br /><i>set</i> 5</td><td>×3</td><td>30.42</td><td>31.54</td><td>31.91</td><td>32.00</td><td>32.52</td><td>32.78</td><td>33.60</td></tr><tr><td><br /></td><td>×4</td><td>28.44</td><td>28.24</td><td>29.66</td><td>29.74</td><td>30.15</td><td>30.21</td><td>31.18</td></tr><tr><td><br /></td><td>×2</td><td>30.21</td><td>31.36</td><td>31.78</td><td>31.81</td><td>32.21</td><td>32.21</td><td>32.95</td></tr><tr><td><br /><i>set</i> 14</td><td>×3</td><td>27.51</td><td>28.36</td><td>28.59</td><td>28.64</td><td>29.03</td><td>29.13</td><td>29.69</td></tr><tr><td><br /></td><td>×4</td><td>25.97</td><td>25.97</td><td>26.78</td><td>26.83</td><td>27.23</td><td>27.17</td><td>27.83</td></tr><tr><td><br /></td><td>×2</td><td>29.41</td><td>29.41</td><td>30.39</td><td>30.43</td><td>30.89</td><td>30.93</td><td>31.55</td></tr><tr><td><br /><i>BSD</i>100</td><td>×3</td><td>27.07</td><td>27.67</td><td>27.78</td><td>27.81</td><td>28.11</td><td>28.27</td><td>28.54</td></tr><tr><td><br /></td><td>×4</td><td>25.84</td><td>25.84</td><td>26.39</td><td>26.41</td><td>26.63</td><td>26.59</td><td>27.00</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="157">
                    <p class="img_tit">表3 测试集<i>set</i> 5, <i>set</i> 14, <i>BSD</i>100在不同算法下的<i>SSIM</i>平均值 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Table</i> 3 <i>SSIM average value of set</i> 5, <i>set</i> 14, <i>and BSD</i>100 <i>under different algorithms</i></p>
                    <p class="img_note"></p>
                    <table id="157" border="1"><tr><td><i>Data set</i></td><td><i>Scale</i></td><td><i>Bicubic</i></td><td><i>ScSR</i></td><td><i>NE</i>+<i>LLE</i></td><td><i>ANR</i></td><td><i>SRCNN</i></td><td><i>ESPCN</i></td><td><i>DRSR</i></td></tr><tr><td></td><td>×2</td><td>0.9355</td><td>0.9428</td><td>0.9537</td><td>0.9546</td><td>0.9566</td><td>0.9568</td><td>0.9621</td></tr><tr><td><br /><i>set</i> 5</td><td>×3</td><td>0.8779</td><td>0.8851</td><td>0.9053</td><td>0.9062</td><td>0.9129</td><td>0.9162</td><td>0.9268</td></tr><tr><td><br /></td><td>×4</td><td>0.8185</td><td>0.8025</td><td>0.8516</td><td>0.8533</td><td>0.8621</td><td>0.8578</td><td>0.8846</td></tr><tr><td><br /></td><td>×2</td><td>0.9348</td><td>0.9564</td><td>0.9575</td><td>0.9578</td><td>0.9591</td><td>0.9598</td><td>0.9630</td></tr><tr><td><br /><i>set</i> 14</td><td>×3</td><td>0.8494</td><td>0.8649</td><td>0.8802</td><td>0.8777</td><td>0.8846</td><td>0.8874</td><td>0.8941</td></tr><tr><td><br /></td><td>×4</td><td>0.7799</td><td>0.7800</td><td>0.8152</td><td>0.8173</td><td>0.8206</td><td>0.8225</td><td>0.8361</td></tr><tr><td><br /></td><td>×2</td><td>0.8401</td><td>0.8702</td><td>0.8729</td><td>0.8742</td><td>0.8827</td><td>0.8832</td><td>0.8924</td></tr><tr><td><br /><i>BSD</i>100</td><td>×3</td><td>0.7301</td><td>0.7514</td><td>0.7702</td><td>0.7718</td><td>0.7778</td><td>0.7816</td><td>0.7917</td></tr><tr><td><br /></td><td>×4</td><td>0.6470</td><td>0.6470</td><td>0.6875</td><td>0.6896</td><td>0.6919</td><td>0.6942</td><td>0.7092</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="158" name="158" class="anchor-tag">5 结 论</h3>
                <div class="p1">
                    <p id="159">针对现有算法网络结构简单、收敛速度慢以及图像纹理模糊等问题, 提出了一种深层残差卷积神经网络模型, 使用较小的卷积核直接在低分辨率图像上提取特征, 在网络的末端通过子像素卷积层将像素进行重新排列, 从而得到高分辨率图像, 使用多路径模式的局部残差结构可加快模型收敛并提高训练速度。为避免卷积后图像信息丢失, 在卷积过程中使用零填充以保证输出图像与输入图像尺寸相等, 实验中使用<i>Adam</i>优化方法。实验结果表明, 本文算法在主观视觉效果和客观评价指标上均优于现有的其他几种算法, 无论在速度还是精度上都有所提升。接下来的工作可以在保证速度的同时进一步优化网络模型, 以提高图像超分辨率的精度, 并且可以将算法应用于其他领域, 如医学图像的超分辨率重建中。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A blind super-resolution reconstruction method considering image registration errors">

                                <b>[1]</b> <i>Zhang H Y</i>, <i>Zhang L P</i>, <i>Shen H F</i>. <i>A blind super</i>-<i>resolution reconstruction method considering image registration errors</i>[<i>J</i>]. <i>International Journal of Fuzzy Systems</i>, 2015, 17 (2) : 353-364.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES56BE354FB2B92978E52A6C8F7643372A&amp;v=MTYzNDhhQnVIWWZPR1FsZkJyTFUwNXR0Z3pMbTh3YWs9TmlmT2ZiYStiS1RQcW9zekZ1bDlCWDR3eUI1bTd6ME1UZ3pxMmhVemZiR1hRcmp1Q09OdkZTaVdXcjdKSUZwbQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> <i>Chen H G</i>, <i>He X H</i>, <i>Teng Q Z</i>, et al. <i>Single image super resolution using local smoothness and nonlocal self</i>-<i>similarity priors</i>[<i>J</i>]. <i>Signal Processing</i>: <i>Image Communication</i>, 2016, 43: 68-81.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201712015&amp;v=MjE5OTA1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEa1VMN0pJalhUYkxHNEg5Yk5yWTlFWVlRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> <i>Li S M</i>, <i>Lei G Q</i>, <i>Fan R</i>. <i>Depth map super</i>-<i>resolution reconstruction based on convolutional neural networks</i>[<i>J</i>]. <i>Acta Optica Sinica</i>, 2017, 37 (12) : 1210002.  李素梅, 雷国庆, 范如. 基于卷积神经网络的深度图超分辨率重建[<i>J</i>]. 光学学报, 2017, 37 (12) : 1210002.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hybrid super-resolution combining example-based single-image and interpolationbased multi-image reconstruction approaches">

                                <b>[4]</b> <i>Batz M</i>, <i>Eichenseer A</i>, <i>Seiler J</i>, et al. <i>Hybrid super</i>-<i>resolution combining example</i>-<i>based single</i>-<i>image and interpolation</i>-<i>based multi</i>-<i>image reconstruction approaches</i>[<i>C</i>]. <i>IEEE International Conference on Image Processing</i> (<i>ICIP</i>) , 2015: 58-62.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-image super-resolution using sparse regression and natural image prior">

                                <b>[5]</b> <i>Kim K I</i>, <i>Kwon Y</i>. <i>Single</i>-<i>image super</i>-<i>resolution using sparse regression and natural image prior</i>[<i>J</i>]. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 2010, 32 (6) : 1127-1133.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201205010&amp;v=MDYxNjhadVp0RmlEa1VMN0pJVGZUZTdHNEg5UE1xbzlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> <i>Lian Q S</i>, <i>Zhang W</i>. <i>Image super</i>-<i>resolution algorithms based on sparse representation of classified image patches</i>[<i>J</i>]. <i>Acta Electronica Sinica</i>, 2012, 40 (5) : 920-925. 练秋生, 张伟. 基于图像块分类稀疏表示的超分辨率重构算法[<i>J</i>]. 电子学报, 2012, 40 (5) : 920-925.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201703012&amp;v=MDA1MTMzenFxQnRHRnJDVVI3cWZadVp0RmlEa1VMN0pJalhUYkxHNEg5Yk1ySTlFWm9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> <i>Xiao J S</i>, <i>Liu E Y</i>, <i>Zhu L</i>, et al. <i>Improved image super</i>-<i>resolution algorithm based on convolutional neural network</i>[<i>J</i>]. <i>Acta Optica Sinica</i>, 2017, 37 (3) : 0318011.  肖进胜, 刘恩雨, 朱力, 等. 改进的基于卷积神经网络的图像超分辨率算法[<i>J</i>]. 光学学报, 2017, 37 (3) : 0318011.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving resolution by image registration">

                                <b>[8]</b> <i>Irani M</i>, <i>Peleg S</i>. <i>Improving resolution by image registration</i>[<i>J</i>]. <i>CVGIP</i>: <i>Graphical Models and Image Processing</i>, 1991, 53 (3) : 231-239.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-resolution image recovery from image-plane arrays, using convex projections">

                                <b>[9]</b> <i>Stark H</i>, <i>Oskoui P</i>. <i>High</i>-<i>resolution image recovery from image</i>-<i>plane arrays</i>, <i>using convex projections</i>[<i>J</i>]. <i>Journal of the Optical Society of America A</i>, 1989, 6 (11) : 1715-1726.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Super-resolution through neighbor embedding">

                                <b>[10]</b> <i>Chang H</i>, <i>Yeung D Y</i>, <i>Xiong Y M</i>. <i>Super</i>-<i>resolution through neighbor embedding</i>[<i>C</i>]. <i>Proceedings of the</i> 2004 <i>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</i>, 2004: 275-282.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">

                                <b>[11]</b> <i>Yang J C</i>, <i>Wright J</i>, <i>Huang T S</i>, et al. <i>Image super</i>-<i>resolution via sparse representation</i>[<i>J</i>]. <i>IEEE Transactions on Image Processing</i>, 2010, 19 (11) : 2861-2873.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Anchored Neighborhood Regression for Fast Example-Based Super-Resolution">

                                <b>[12]</b> <i>Timofte R</i>, <i>de Smet V</i>, <i>van Gool L</i>. <i>Anchored neighborhood regression for fast example</i>-<i>based super</i>-<i>resolution</i>[<i>C</i>]. <i>IEEE International Conference on Computer Vision</i>, 2013: 1920-1927.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A+:Adjusted Anchored Neighborhood Regression for Fast Super-Resolution">

                                <b>[13]</b> <i>Timofte R</i>, <i>de Smet V</i>, <i>van Gool L</i>. <i>A</i>+: <i>adjusted anchored neighborhood regression for fast super</i>-<i>resolution</i>[<i>C</i>]. <i>Asian Conference on Computer Vision</i>, 2015: 111-126.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using deep convolutional networks">

                                <b>[14]</b> <i>Dong C</i>, <i>Loy C C</i>, <i>He K M</i>, et al. <i>Image super</i>-<i>resolution using deep convolutional networks</i>[<i>J</i>]. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 2016, 38 (2) : 295-307.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accelerating the super-resolution convolutional neural network">

                                <b>[15]</b> <i>Dong C</i>, <i>Chen C L</i>, <i>Tang X</i>. <i>Accelerating the super</i>-<i>resolution convolutional neural network</i>[<i>C</i>]. <i>European Conference on Computer Vision</i>, 2016: 391-407.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network">

                                <b>[16]</b> <i>Shi W Z</i>, <i>Caballero J</i>, <i>Husz</i>á<i>r F</i>, et al. <i>Real</i>-<i>time single image and video super</i>-<i>resolution using an efficient sub</i>-<i>pixel convolutional neural network</i>[<i>C</i>]. <i>IEEE Conference on Computer Vision and Pattern Recognition</i>, 2016: 1874-1883.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate image super-resolution using very deep convolutional networks">

                                <b>[17]</b> <i>Kim J</i>, <i>Lee J K</i>, <i>Lee K M</i>. <i>Accurate image super</i>-<i>resolution using very deep convolutional networks</i>[<i>C</i>]. <i>IEEE Conference on Computer Vision and Pattern Recognition</i>, 2016: 1646-1654.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201712017&amp;v=MTQ5NzJPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRGtVTDdKSWpYVGJMRzRIOWJOclk5RVk0UUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> <i>Sun C</i>, <i>L</i>ü <i>J W</i>, <i>Li J W</i>, et al. <i>Fast image super</i>-<i>resolution method based on deconvolution</i>[<i>J</i>]. <i>Acta Optica Sinica</i>, 2017, 37 (12) : 1210004.  孙超, 吕俊伟, 李健伟, 等. 基于去卷积的快速图像超分辨率方法[<i>J</i>]. 光学学报, 2017, 37 (12) : 1210004.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Label propagation via teaching-to-learn and learning-to-teach">

                                <b>[19]</b> <i>Gong C</i>, <i>Tao D C</i>, <i>Liu W</i>, et al. <i>Label propagation via teaching</i>-<i>to</i>-<i>learn and learning</i>-<i>to</i>-<i>teach</i>[<i>J</i>]. <i>IEEE Transactions on Neural Networks and Learning Systems</i>, 2017, 28 (6) : 1452-1465.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_20" >
                                    <b>[20]</b>
                                 <i>Jia Y Q</i>, <i>Shelhamer E</i>, <i>Donahue J</i>, et al. <i>Caffe</i>: <i>convolutional architecture for fast feature embedding</i>[<i>C</i>]//<i>Proceedings of the</i> 22<i>nd ACM International</i><i>Conference on Multimedia</i>- (<i>MM</i>′14) , 2014: 675-678.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single image super-resolution from transformed self-exemplars">

                                <b>[21]</b> <i>Huang J B</i>, <i>Singh A</i>, <i>Ahuja N</i>. <i>Single image super</i>-<i>resolution from transformed self</i>-<i>exemplars</i>[<i>C</i>]. <i>IEEE Conference on Computer Vision and Pattern Recognition</i>, 2015: 5197-5206.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust and discriminative labeling for multi-label active learning based on maximum correntropy criterion">

                                <b>[22]</b> <i>Du B</i>, <i>Wang Z</i>, <i>Zhang L</i>, et al. <i>Robust and discriminative labeling for multi</i>-<i>label active learning based on maximum correntropy criterion</i>[<i>J</i>]. <i>IEEE Transactions on Image Processing</i>, 2017, 26 (4) : 1694-1707.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked Convolutional Denoising Auto-Encoders for Feature Representation">

                                <b>[23]</b> <i>Du B</i>, <i>Xiong W</i>, <i>Wu J</i>, et al. <i>Stacked convolutional denoising auto</i>-<i>encoders for feature representation</i>[<i>J</i>]. <i>IEEE Transactions on Cybernetics</i>, 2017, 47 (4) : 1017-1027.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust Registration of Multimodal Remote Sensing Images Based on Structural Similarity">

                                <b>[24]</b> <i>Ye Y X</i>, <i>Shan J</i>, <i>Bruzzone L</i>, et al. <i>Robust registration of multimodal remote sensing images based on structural similarity</i>[<i>J</i>]. <i>IEEE Transactions on Geoscience and Remote Sensing</i>, 2017, 55 (5) : 2941-2958.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201902011" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902011&amp;v=MTc2NjVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVUw3SklqWFRiTEc0SDlqTXJZOUVaWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="3" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

