

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133859874658750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201907009%26RESULT%3d1%26SIGN%3dLonTXV%252fL0eo%252btMZAmvfxphzgVA4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201907009&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201907009&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201907009&amp;v=MDg4NjJDVVJMT2VaZVZ1RnluZ1ZMelBJalhUYkxHNEg5ak1xSTlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#60" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="2 理论分析 ">2 理论分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#69" data-title="&lt;b&gt;2.1 传播理论&lt;/b&gt;"><b>2.1 传播理论</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;2.2 实验结构&lt;/b&gt;"><b>2.2 实验结构</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;2.3 URNet网络结构&lt;/b&gt;"><b>2.3 URNet网络结构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#103" data-title="3 结果与分析 ">3 结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#104" data-title="&lt;b&gt;3.1 模型训练&lt;/b&gt;"><b>3.1 模型训练</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;3.2 模型恢复结果及分析&lt;/b&gt;"><b>3.2 模型恢复结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#116" data-title="4 结  论 ">4 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="图1 非相干光传播图">图1 非相干光传播图</a></li>
                                                <li><a href="#95" data-title="图2 非视域成像实验结构示意图">图2 非视域成像实验结构示意图</a></li>
                                                <li><a href="#96" data-title="图3 URNet网络结构图">图3 URNet网络结构图</a></li>
                                                <li><a href="#98" data-title="图4 瓶颈层结构图。">图4 瓶颈层结构图。</a></li>
                                                <li><a href="#108" data-title="图5 不同瓶颈层模型恢复结果。">图5 不同瓶颈层模型恢复结果。</a></li>
                                                <li><a href="#112" data-title="图6 手写英文字母恢复结果。">图6 手写英文字母恢复结果。</a></li>
                                                <li><a href="#113" data-title="图7 受外界影响下的模型恢复结果。">图7 受外界影响下的模型恢复结果。</a></li>
                                                <li><a href="#115" data-title="图8 不同模型恢复结果。">图8 不同模型恢复结果。</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" title=" Repasi E, Lutzmann P, Steinvall O, &lt;i&gt;et al&lt;/i&gt;.Advanced short-wavelength infrared range-gated imaging for ground applications in monostatic and bistatic configurations[J].Applied Optics, 2009, 48 (31) :5956-5969." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Advanced short-wavelength infrared range-gated imaging for ground applications in monostatic and bistatic configurations">
                                        <b>[1]</b>
                                         Repasi E, Lutzmann P, Steinvall O, &lt;i&gt;et al&lt;/i&gt;.Advanced short-wavelength infrared range-gated imaging for ground applications in monostatic and bistatic configurations[J].Applied Optics, 2009, 48 (31) :5956-5969.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" Laurenzis M, Christnacher F, Velten A.Study of a dual mode SWIR active imaging system for direct imaging and non-line-of-sight vision[J].Proceedings of SPIE, 2015, 9465:946509." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Study of a dual mode SWIR active imaging system for direct imaging and non-line-of-sight vision">
                                        <b>[2]</b>
                                         Laurenzis M, Christnacher F, Velten A.Study of a dual mode SWIR active imaging system for direct imaging and non-line-of-sight vision[J].Proceedings of SPIE, 2015, 9465:946509.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" title=" Pandharkar R, Velten A, Bardagjy A, &lt;i&gt;et al&lt;/i&gt;.Estimating motion and size of moving non-line-of-sight objects in cluttered environments[C]//2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 20-25, 2011, Colorado Springs, CO, USA.New York:IEEE, 2011:265-272." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Estimating Motion and size of moving non-line-of-sight objects in cluttered environments">
                                        <b>[3]</b>
                                         Pandharkar R, Velten A, Bardagjy A, &lt;i&gt;et al&lt;/i&gt;.Estimating motion and size of moving non-line-of-sight objects in cluttered environments[C]//2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 20-25, 2011, Colorado Springs, CO, USA.New York:IEEE, 2011:265-272.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" title=" Naik N, Zhao S, Velten A, &lt;i&gt;et al&lt;/i&gt;.Single view reflectance capture using multiplexed scattering and time-of-flight imaging[J].ACM Transactions on Graphics, 2011, 30 (6) :171." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000000996&amp;v=MjQ3NDlSVT1OaWZJWTdLN0h0ak5yNDlGWk9zUEJYVS9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lJVnNUYQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Naik N, Zhao S, Velten A, &lt;i&gt;et al&lt;/i&gt;.Single view reflectance capture using multiplexed scattering and time-of-flight imaging[J].ACM Transactions on Graphics, 2011, 30 (6) :171.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" title=" Heide F, Xiao L, Heidrich W, &lt;i&gt;et al&lt;/i&gt;.Diffuse mirrors:3D reconstruction from diffuse indirect illumination using inexpensive time-of-flight sensors[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:3222-3229." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Diffuse Mirrors:3D Reconstruction from Diffuse Indirect Illumination Using Inexpensive Time-of-Flight Sensors">
                                        <b>[5]</b>
                                         Heide F, Xiao L, Heidrich W, &lt;i&gt;et al&lt;/i&gt;.Diffuse mirrors:3D reconstruction from diffuse indirect illumination using inexpensive time-of-flight sensors[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:3222-3229.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" title=" Wu D, Velten A, O′Toole M, &lt;i&gt;et al&lt;/i&gt;.Decomposing global light transport using time of flight imaging[J].International Journal of Computer Vision, 2014, 107 (2) :123-138." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14032100000891&amp;v=Mjg0ODIvSUlWc1RhUlU9Tmo3QmFySzhIdExPcm85RlpPc1BCSFU0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Wu D, Velten A, O′Toole M, &lt;i&gt;et al&lt;/i&gt;.Decomposing global light transport using time of flight imaging[J].International Journal of Computer Vision, 2014, 107 (2) :123-138.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" Kirmani A, Hutchison T, Davis J, &lt;i&gt;et al&lt;/i&gt;.Looking around the corner using ultrafast transient imaging[J].International Journal of Computer Vision, 2011, 95 (1) :13-28." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Looking Around the Corner using Ultrafast Transient Imaging">
                                        <b>[7]</b>
                                         Kirmani A, Hutchison T, Davis J, &lt;i&gt;et al&lt;/i&gt;.Looking around the corner using ultrafast transient imaging[J].International Journal of Computer Vision, 2011, 95 (1) :13-28.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" title=" Gupta O, Willwacher T, Velten A, &lt;i&gt;et al&lt;/i&gt;.Reconstruction of hidden 3D shapes using diffuse reflections[J].Optics Express, 2012, 20 (17) :19096-19108." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reconstruction of hidden 3D shapes using diffuse reflections">
                                        <b>[8]</b>
                                         Gupta O, Willwacher T, Velten A, &lt;i&gt;et al&lt;/i&gt;.Reconstruction of hidden 3D shapes using diffuse reflections[J].Optics Express, 2012, 20 (17) :19096-19108.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" Velten A, Willwacher T, Gupta O, &lt;i&gt;et al&lt;/i&gt;.Recovering three-dimensional shape around a corner using ultrafast time-of-flight imaging[J].Nature Communications, 2012, 3:745." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recovering three-dimensional shape around a corner using ultrafast time-of-flight imaging">
                                        <b>[9]</b>
                                         Velten A, Willwacher T, Gupta O, &lt;i&gt;et al&lt;/i&gt;.Recovering three-dimensional shape around a corner using ultrafast time-of-flight imaging[J].Nature Communications, 2012, 3:745.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" title=" Gariepy G, Tonolini F, Henderson R, &lt;i&gt;et al&lt;/i&gt;.Detection and tracking of moving objects hidden from view[J].Nature Photonics, 2016, 10 (1) :23-26." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detection and tracking of moving objects hidden from view">
                                        <b>[10]</b>
                                         Gariepy G, Tonolini F, Henderson R, &lt;i&gt;et al&lt;/i&gt;.Detection and tracking of moving objects hidden from view[J].Nature Photonics, 2016, 10 (1) :23-26.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" title=" Singh A K, Naik D N, Pedrini G, &lt;i&gt;et al&lt;/i&gt;.Looking through a diffuser and around an opaque surface:a holographic approach[J].Optics Express, 2014, 22 (7) :7694-7701." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Looking through a diffuser and around an opaque surface:a holographic approach">
                                        <b>[11]</b>
                                         Singh A K, Naik D N, Pedrini G, &lt;i&gt;et al&lt;/i&gt;.Looking through a diffuser and around an opaque surface:a holographic approach[J].Optics Express, 2014, 22 (7) :7694-7701.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" title=" Vinu R V, Kim K, Somkuwar A S, &lt;i&gt;et al&lt;/i&gt;.Single-shot optical imaging through scattering medium using digital in-line holography[J/OL]. (2016-03-24) [2019-01-01].https://arxiv.org/abs/1603.07430." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-shot optical imaging through scattering medium using digital in-line holography">
                                        <b>[12]</b>
                                         Vinu R V, Kim K, Somkuwar A S, &lt;i&gt;et al&lt;/i&gt;.Single-shot optical imaging through scattering medium using digital in-line holography[J/OL]. (2016-03-24) [2019-01-01].https://arxiv.org/abs/1603.07430.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" title=" Yaqoob Z, Psaltis D, Feld M S, &lt;i&gt;et al&lt;/i&gt;.Optical phase conjugation for turbidity suppression in biological samples[J].Nature Photonics, 2008, 2 (2) :110-115." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=OPTICAL PHASE CONJUGATION FOR TURBIDITY SUPPRESSION IN BIOLOGICAL SAMPLES">
                                        <b>[13]</b>
                                         Yaqoob Z, Psaltis D, Feld M S, &lt;i&gt;et al&lt;/i&gt;.Optical phase conjugation for turbidity suppression in biological samples[J].Nature Photonics, 2008, 2 (2) :110-115.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" Vellekoop I M, Lagendijk A, Mosk A P.Exploiting disorder for perfect focusing[J].Nature Photonics, 2010, 4 (5) :320-322." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting disorder for perfect focusing">
                                        <b>[14]</b>
                                         Vellekoop I M, Lagendijk A, Mosk A P.Exploiting disorder for perfect focusing[J].Nature Photonics, 2010, 4 (5) :320-322.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" Liu Y, Ma C, Shen Y C, &lt;i&gt;et al&lt;/i&gt;.Focusing light inside dynamic scattering media with millisecond digital optical phase conjugation[J].Optica, 2017, 4 (2) :280-288." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Focusing light inside dynamic scattering media with millisecond digital optical phase conjugation">
                                        <b>[15]</b>
                                         Liu Y, Ma C, Shen Y C, &lt;i&gt;et al&lt;/i&gt;.Focusing light inside dynamic scattering media with millisecond digital optical phase conjugation[J].Optica, 2017, 4 (2) :280-288.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" title=" Bertolotti J, van Putten E G, Blum C, &lt;i&gt;et al&lt;/i&gt;.Non-invasive imaging through opaque scattering layers[J].Nature, 2012, 491 (7423) :232-234." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Non-invasive imaging through opaque scattering layers">
                                        <b>[16]</b>
                                         Bertolotti J, van Putten E G, Blum C, &lt;i&gt;et al&lt;/i&gt;.Non-invasive imaging through opaque scattering layers[J].Nature, 2012, 491 (7423) :232-234.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" title=" Katz O, Heidmann P, Fink M, &lt;i&gt;et al&lt;/i&gt;.Non-invasive single-shot imaging through scattering layers and around corners via speckle correlations[J].Nature Photonics, 2014, 8 (10) :784-790." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Non-invasive single-shot imaging through scattering layers and around corners via speckle correlations">
                                        <b>[17]</b>
                                         Katz O, Heidmann P, Fink M, &lt;i&gt;et al&lt;/i&gt;.Non-invasive single-shot imaging through scattering layers and around corners via speckle correlations[J].Nature Photonics, 2014, 8 (10) :784-790.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" Judkewitz B, Horstmeyer R, Vellekoop I M, &lt;i&gt;et al&lt;/i&gt;.Translation correlations in anisotropically scattering media[J].Nature Physics, 2015, 11 (8) :684-689." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Translation correlations in anisotropically scattering media">
                                        <b>[18]</b>
                                         Judkewitz B, Horstmeyer R, Vellekoop I M, &lt;i&gt;et al&lt;/i&gt;.Translation correlations in anisotropically scattering media[J].Nature Physics, 2015, 11 (8) :684-689.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_19" title=" Li L, Li Q, Sun S, &lt;i&gt;et al&lt;/i&gt;.Imaging through scattering layers exceeding memory effect range with spatial-correlation-achieved point-spread-function[J].Optics Letters, 2018, 43 (8) :1670-1673." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imaging through scattering layers exceeding memory effect range with spatial-correlation-achieved point-spread-function">
                                        <b>[19]</b>
                                         Li L, Li Q, Sun S, &lt;i&gt;et al&lt;/i&gt;.Imaging through scattering layers exceeding memory effect range with spatial-correlation-achieved point-spread-function[J].Optics Letters, 2018, 43 (8) :1670-1673.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_20" title=" Li S, Deng M, Lee J, &lt;i&gt;et al&lt;/i&gt;.Imaging through glass diffusers using densely connected convolutional networks[J].Optica, 2018, 5 (7) :803-812." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imaging through glass diffusers using densely connected convolutional networks">
                                        <b>[20]</b>
                                         Li S, Deng M, Lee J, &lt;i&gt;et al&lt;/i&gt;.Imaging through glass diffusers using densely connected convolutional networks[J].Optica, 2018, 5 (7) :803-812.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_21" title=" Lyu M, Wang H, Li G, &lt;i&gt;et al&lt;/i&gt;.Exploit imaging through opaque wall via deep learning[J/OL]. (2017-08-09) [2019-01-01].https://arxiv.org/abs/1708.07881." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploit imaging through opaque wall via deep learning">
                                        <b>[21]</b>
                                         Lyu M, Wang H, Li G, &lt;i&gt;et al&lt;/i&gt;.Exploit imaging through opaque wall via deep learning[J/OL]. (2017-08-09) [2019-01-01].https://arxiv.org/abs/1708.07881.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_22" title=" Li Y Z, Xue Y J, Tian L.Deep speckle correlation:a deep learning approach toward scalable imaging through scattering media[J].Optica, 2018, 5 (10) :1181-1190." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep speckle correlation:a deep learning approach toward scalable imaging through scattering media">
                                        <b>[22]</b>
                                         Li Y Z, Xue Y J, Tian L.Deep speckle correlation:a deep learning approach toward scalable imaging through scattering media[J].Optica, 2018, 5 (10) :1181-1190.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_23" title=" Ronneberger O, Fischer P, Brox T.U-Net:convolutional networks for biomedical image segmentation[M]//Navab N, Hornegger J, Wells W, &lt;i&gt;et al&lt;/i&gt;.Medical image computing and computer-assisted intervention-MICCAI 2015.Lecture notes in computer science.Cham:Springer, 2015, 9351:234-241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-Net:convolutional networks for biomedical image segmentation">
                                        <b>[23]</b>
                                         Ronneberger O, Fischer P, Brox T.U-Net:convolutional networks for biomedical image segmentation[M]//Navab N, Hornegger J, Wells W, &lt;i&gt;et al&lt;/i&gt;.Medical image computing and computer-assisted intervention-MICCAI 2015.Lecture notes in computer science.Cham:Springer, 2015, 9351:234-241.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_24" title=" He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.Deep residual learning for image recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[24]</b>
                                         He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.Deep residual learning for image recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_25" title=" Peng C, Zhang X Y, Yu G, &lt;i&gt;et al&lt;/i&gt;.Large kernel matters—improve semantic segmentation by global convolutional network[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:1743-1751." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large Kernel Matters—Improve Semantic Segmentation by Global Convolutional Network">
                                        <b>[25]</b>
                                         Peng C, Zhang X Y, Yu G, &lt;i&gt;et al&lt;/i&gt;.Large kernel matters—improve semantic segmentation by global convolutional network[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:1743-1751.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-02 15:14</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(07),87-93 DOI:10.3788/AOS201939.0711002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度学习的非视域成像</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E4%BA%AD%E4%B9%89&amp;code=42412958&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">于亭义</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B9%94%E6%9C%A8&amp;code=35461116&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">乔木</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E7%BA%A2%E6%9E%97&amp;code=20755184&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘红林</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%9F%A9%E7%94%B3%E7%94%9F&amp;code=20600737&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">韩申生</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E4%B8%8A%E6%B5%B7%E5%85%89%E5%AD%A6%E7%B2%BE%E5%AF%86%E6%9C%BA%E6%A2%B0%E7%A0%94%E7%A9%B6%E6%89%80%E9%87%8F%E5%AD%90%E5%85%89%E5%AD%A6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=1039176&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院上海光学精密机械研究所量子光学重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6%E6%9D%90%E6%96%99%E4%B8%8E%E5%85%89%E7%94%B5%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学材料与光电研究中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对非相干光照明下的非视域成像问题, 提出一种基于深度学习的解决方法。结合计算机视觉领域中经典的语义分割及残差模型, 构造一种URNet网络结构, 并改进了经典瓶颈层结构。实验结果表明, 改进的网络可以恢复更多的图像细节, 并具有一定泛化性, 相比于基于非相干光照明的散斑自相关成像技术, 该网络恢复性能有较大提升。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%88%90%E5%83%8F%E7%B3%BB%E7%BB%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">成像系统;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9D%9E%E8%A7%86%E5%9F%9F%E6%88%90%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">非视域成像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AE%8B%E5%B7%AE%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">残差模型;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *刘红林, E-mail:hlliu4@hotmail.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-29</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划 (2016YFC0100600);</span>
                    </p>
            </div>
                    <h1><b>Non-Line-of-Sight Imaging Through Deep Learning</b></h1>
                    <h2>
                    <span>Yu Tingyi</span>
                    <span>Qiao Mu</span>
                    <span>Liu Honglin</span>
                    <span>Han Shensheng</span>
            </h2>
                    <h2>
                    <span>Key Laboratory for Quantum Optics, Shanghai Institute of Optics and Fine Mechanics, Chinese Academy of Sciences</span>
                    <span>Center of Materials Science and Optoelectronics Engineering, University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problem of non-line-of-sight imaging under incoherent illumination, we propose a solution based on deep learning. Combining the classical semantic segmentation and residual model in the field of computer vision, a URNet network structure is constructed and the classical bottleneck layer structure is improved. The experimental results show that the improved model has more details of recovery images and generalization ability. Compared with speckle autocorrelation imaging method under incoherent illumination, the recovery performance of this method is greatly improved.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=imaging%20systems&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">imaging systems;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=non-line-of-sight%20imaging&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">non-line-of-sight imaging;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semantic%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semantic segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=residual%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">residual model;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-29</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="60" name="60" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="61">传统非视域成像技术采用激光主动照明, 光信号经过中介面反射或散射, 之后照射到物体上, 经过物体反射的光再次通过中介面, 最后被接收器接收。目前, 该技术主要有几种形式:</p>
                </div>
                <div class="p1">
                    <p id="62">1) 基于激光距离的选通成像技术<citation id="118" type="reference"><link href="10" rel="bibliography" /><link href="12" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。该技术根据物体与接收器之间的距离估计光信号传播时间, 在两次反射期间选通门打开, 其余时间关闭, 从而消除其他一次或多次反射的干扰。</p>
                </div>
                <div class="p1">
                    <p id="63">2) 基于超快激光的瞬态成像技术<citation id="119" type="reference"><link href="14" rel="bibliography" /><link href="16" rel="bibliography" /><link href="18" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。该技术将超快激光照射到中介面上的不同位置, 利用高分辨的条纹相机接收多个图像, 根据图像反演算法, 最终构建出物体的三维图像<citation id="120" type="reference"><link href="20" rel="bibliography" /><link href="22" rel="bibliography" /><link href="24" rel="bibliography" /><link href="26" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="64">3) 基于光子计数的探测成像技术<citation id="121" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。该技术利用单光子雪崩二极管 (SPAD) 获取物体反射回的单光子信号并构建光子计数直方图, 通过反投影算法进行图像重构。</p>
                </div>
                <div class="p1">
                    <p id="65">4) 基于数字全息的成像技术<citation id="122" type="reference"><link href="30" rel="bibliography" /><link href="32" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>。该技术将激光分为参考光和物光, 物光照射到物体后被反射, 与参考光在毛玻璃表面发生干涉, 干涉结果通过毛玻璃并经过透镜成像到接收面, 最后采用全息成像技术将接收到的信号进行恢复。</p>
                </div>
                <div class="p1">
                    <p id="66">以上方法均是基于相干光源照明采集弹道光信号, 过滤散射光信号。在散射较强的情况下, 弹道光衰减迅速, 无法成像。利用非相干光照明的波前整形<citation id="123" type="reference"><link href="34" rel="bibliography" /><link href="36" rel="bibliography" /><link href="38" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>和基于散射介质记忆效应的散斑自相关成像技术<citation id="124" type="reference"><link href="40" rel="bibliography" /><link href="42" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>, 克服了弹道光成像技术的缺陷, 并可以实现散射光成像。但波前整形需要目标先验信息, 调节速度慢, 过程复杂;基于散射介质记忆效应的成像方法受限于记忆效应范围, 成像视场角小, 之后虽然出现了扩大记忆效应范围的方法<citation id="125" type="reference"><link href="44" rel="bibliography" /><link href="46" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>, 却仍然无法满足实际需求。</p>
                </div>
                <div class="p1">
                    <p id="67">深度学习 (DL) 方法被应用到非视域成像技术中<citation id="127" type="reference"><link href="48" rel="bibliography" /><link href="50" rel="bibliography" /><link href="52" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>。该技术采用数据驱动方法而并非建立确定的模型, 通过向数据集学习的方式解决问题。此方法有较好的适应性与灵活性, 性能往往能够超越基于模型的方法。Li等<citation id="128" type="reference"><link href="48" rel="bibliography" /><link href="50" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>利用神经网络实现对单一散射体的非视域成像;Li等<citation id="126" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>对同一网络进行多次训练, 实现了对不同散射体的成像。但以上方法均采用相干光源照明、透射方式成像, 而在实际应用中, 存在许多仅适用于非相干光照明的场景。针对该问题, 提出一种基于深度学习的非相干光照明的非视域成像技术, 利用计算机视觉领域中的经典模型, 构造出一种新的卷积神经网络结构, 并能改进传统瓶颈层结构。结果表明, 该方法可实现端对端的图像恢复, 恢复性能相比于传统的散斑自相关成像技术有较大提升。</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag">2 理论分析</h3>
                <h4 class="anchor-tag" id="69" name="69"><b>2.1 传播理论</b></h4>
                <div class="p1">
                    <p id="70">在非相干光源照明下的非视域成像模型中, 物体反射的非相干光经空间自由传播, 之后被中介面散射或反射, 再经空间自由传播后被探测器接收。与相干光传播方式不同, 非相干光传播采用互强度传播公式。如图1所示, 物面<i>σ</i>经过自由空间传播, 之后得到中介面<i>τ</i>, 根据范希特-泽尼克定理, 有</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>J</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mo>〈</mo><mi>E</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mi>E</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>〉</mo><mo>=</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><mrow><munder><mo>∫</mo><mi>σ</mi></munder><mi>Ι</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mfrac><mrow><mi>exp</mi><mo stretchy="false">[</mo><mtext>i</mtext><mi>k</mi><mo stretchy="false"> (</mo><mi>R</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi>R</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow><mrow><mi>R</mi><msub><mrow></mrow><mn>1</mn></msub><mi>R</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mtext>d</mtext><mi>s</mi><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">式中:<i>p</i><sub>1</sub>和<i>p</i><sub>2</sub>为中介面<i>τ</i>上两点;<i>E</i> (<i>p</i><sub>2</sub>) 和<i>E</i><sup>*</sup> (<i>p</i><sub>1</sub>) 分别为<i>p</i><sub>2</sub>和<i>p</i><sub>1</sub>的光场和光场共轭;<i>I</i> (<i>s</i>) 为物面<i>σ</i>中<i>s</i>点的光强;<i>k</i>为波数;<i>R</i><sub>1</sub>和<i>R</i><sub>2</sub>为<i>s</i>到<i>p</i><sub>1</sub>和<i>p</i><sub>2</sub>的距离;<i>J</i> (<i>p</i><sub>1</sub>, <i>p</i><sub>2</sub>) 为被中介面调制前的互强度。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907009_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 非相干光传播图" src="Detail/GetImg?filename=images/GXXB201907009_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 非相干光传播图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907009_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Incoherent light propagation</p>

                </div>
                <div class="p1">
                    <p id="74">物光经过空间自由传播到中介面<i>τ</i>后, 被中介面调制。由于中介面为一个粗糙面, 实验中采用毛玻璃代替中介面, 用调制函数<i>T</i> (·) 表示该作用, 由此推出调制后的光场为</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Τ</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mi>E</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">式中:<i>E</i><sub>mod</sub> (<i>p</i>) 为经过中介面调制后的光场。由 (1) 式可推出调制后的互强度为</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>J</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mo>〈</mo><mi>E</mi><msubsup><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext></mrow><mo>*</mo></msubsup><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mi>E</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>〉</mo><mo>=</mo></mtd></mtr><mtr><mtd><mo>〈</mo><mi>E</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mi>Τ</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mi>E</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mi>Τ</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>〉</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">式中:<i>J</i><sub>mod</sub> (<i>p</i><sub>1</sub>, <i>p</i><sub>2</sub>) 为经过调制后的互强度。光场经中介面调制后, 最终经过空间自由传播被接收。利用范希特-泽尼克定理可推出接收像面<i>∈</i>中的点<i>Q</i><sub>1</sub>和点<i>Q</i><sub>2</sub>的互强度<i>J</i> (<i>Q</i><sub>1</sub>, <i>Q</i><sub>2</sub>) , 可得</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>J</mi><mo stretchy="false"> (</mo><mi>Q</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>Q</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>λ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mstyle displaystyle="true"><mrow><munder><mo>∬</mo><mi>τ</mi></munder><mi>J</mi></mrow></mstyle><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>⋅</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mi>exp</mi><mo stretchy="false">[</mo><mo>-</mo><mtext>i</mtext><mi>k</mi><mo stretchy="false"> (</mo><msup><mi>R</mi><mo>′</mo></msup><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><msup><mi>R</mi><mo>′</mo></msup><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow><mrow><msup><mi>R</mi><mo>′</mo></msup><msub><mrow></mrow><mn>1</mn></msub><msup><mi>R</mi><mo>′</mo></msup><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mtext>d</mtext><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mtext>d</mtext><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">式中:<i>λ</i>为光波长;<i>R</i>′<sub>1</sub>和<i>R</i>′<sub>2</sub>分别为<i>p</i><sub>1</sub>到<i>Q</i><sub>1</sub>以及<i>p</i><sub>2</sub>到<i>Q</i><sub>2</sub>的距离。接收像面<i>∈</i>上任意一点<i>Q</i>的光强<i>I</i> (<i>Q</i>) =<i>J</i> (<i>Q</i>, <i>Q</i>) , 由此得</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ι</mi><mo stretchy="false"> (</mo><mi>Q</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>λ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mstyle displaystyle="true"><mrow><munder><mo>∬</mo><mi>τ</mi></munder><mo>〈</mo></mrow></mstyle><mi>E</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mi>Τ</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mi>E</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mi>Τ</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>〉</mo><mo>⋅</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mi>exp</mi><mo stretchy="false">[</mo><mo>-</mo><mtext>i</mtext><mi>k</mi><mo stretchy="false"> (</mo><msup><mi>R</mi><mo>′</mo></msup><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><msup><mi>R</mi><mo>′</mo></msup><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow><mrow><msup><mi>R</mi><mo>′</mo></msup><msub><mrow></mrow><mn>1</mn></msub><msup><mi>R</mi><mo>′</mo></msup><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mtext>d</mtext><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mtext>d</mtext><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo>。</mo><mspace width="0.25em" /><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">由于计算像面的公式过于复杂, 用一个点扩展函数<font face="EU-HT">F</font>表示该系统响应, 即</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mi mathvariant="script">F</mi><mo stretchy="false">{</mo><mi>Ο</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">式中:<i>O</i> (<i>x</i><sub>0</sub>, <i>y</i><sub>0</sub>) 为物面<i>σ</i>上的物体信息;<i>I</i> (<i>x</i>, <i>y</i>) 为接收像面<i>∈</i>上的接收信息。对于非视域成像技术, 通过<i>I</i> (<i>x</i>, <i>y</i>) 反解出<i>O</i> (<i>x</i><sub>0</sub>, <i>y</i><sub>0</sub>) :</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mi mathvariant="script">R</mi><mo stretchy="false">{</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">式中:<font face="EU-HT">R</font>为点扩展函数<font face="EU-HT">F</font>的逆函数。理论上无法解出<font face="EU-HT">R</font>的解析表达式, 因此采用深度学习中的卷积神经网络, 通过学习的方式求得该函数的近似解。</p>
                </div>
                <div class="p1">
                    <p id="87">卷积神经网络使用包含物体<i>O</i><sub><i>n</i></sub>及对应像<i>I</i><sub><i>n</i></sub>的数据集, 其中<i>n</i>=1, 2, …, <i>N</i>。采用最小化损失函数的方法, 得到反向映射函数的近似解<font face="EU-HT">R</font><sub>learn</sub>为</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="script">R</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>e</mtext><mtext>a</mtext><mtext>r</mtext><mtext>n</mtext></mrow></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mrow><mi mathvariant="script">R</mi><msub><mrow></mrow><mi>θ</mi></msub><mo>, </mo><mi>θ</mi><mo>∈</mo><mi>Θ</mi></mrow></munder><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>f</mi></mstyle><mo stretchy="false"> (</mo><mi>Ο</mi><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mi mathvariant="script">R</mi><msub><mrow></mrow><mi>θ</mi></msub><mo stretchy="false">{</mo><mi>Ι</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">}</mo><mo stretchy="false">) </mo><mo>+</mo><mi>Ψ</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">式中:<i>Θ</i>为网络的参数集合;, <i>θ</i>为集合<i>Θ</i>中的元素;<i>f</i>为损失函数;<i>Ψ</i> (<i>θ</i>) 为防止过拟合的正则惩罚。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90"><b>2.2 实验结构</b></h4>
                <div class="p1">
                    <p id="91">实验装置如图2所示。准直LED光源 (M530L3, Thorlabs, America) 和透镜L1 (焦距<i>f</i><sub>1</sub>=5 cm) 、L2 (焦距<i>f</i><sub>2</sub>=5 cm) 构成科勒照明系统, 产生均匀非相干光。由科勒照明系统出射的光被数字微反射镜器件 (DMD, DLP Discovery 4100, Texas Instruments, America) 反射, DMD反射的光经过空间自由传播后被中介面反射, 反射后的光场最终被相机 (Stingray F-504B, Allied Vision Technologies, Germany) 接收。DMD与中介面的距离为<i>z</i>。当DMD上加载灰度图像时, 可将DMD看作是一个反射型的物体。该DMD的像素数为1024 pixel×768 pixel, 像元尺寸为13.6 μm×13.6 μm, 物体与中介面距离<i>z</i>=15 cm, 中介面采用粗糙的毛玻璃片, 相机的像素数为2452 pixel×2056 pixel, 相机的像元尺寸为3.45 μm×3.45 μm。DMD上的图像加载和相机快门通过信号源产生的方波脉冲信号同步控制。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92"><b>2.3 URNet网络结构</b></h4>
                <div class="p1">
                    <p id="93">为解决非相干光照明下的非视域成像问题, 提出一种URNet网络, 该网络结构主要包含U-Net网络结构<citation id="129" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>和残差神经 (ResNet) 网络结构<citation id="130" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>。U-Net网络结构利用下采样提取特征和上采样恢复图像, 实现端对端的语义分割;ResNet网络结构通过跳跃连接解决反向传播时的梯度消失问题, 提高训练速度。</p>
                </div>
                <div class="p1">
                    <p id="94">URNet网络结构如图3所示。输入图像首先经过4个瓶颈层, 每个瓶颈层后有一个最大池化层, 之后经过一个瓶颈层和4个反卷积层, 每个反卷积层后有一个瓶颈层, 最后通过一个普通卷积层输出。为了最大限度地保留信息, 在网络结构中添加跳跃连接, 连接的块具有相同的图像尺寸。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907009_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 非视域成像实验结构示意图" src="Detail/GetImg?filename=images/GXXB201907009_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 非视域成像实验结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907009_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Experiment structure of non-line-of-sight imaging</p>

                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907009_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 URNet网络结构图" src="Detail/GetImg?filename=images/GXXB201907009_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 URNet网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907009_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Structural diagram of URNet network</p>

                </div>
                <div class="p1">
                    <p id="97">对于经典的瓶颈层结构<citation id="131" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>, 针对该应用场景进行了改进。如图4所示, 图4 (a) 为经典的3层瓶颈层结构, 包含两个1×1卷积核、两个非线性激活函数和一个3×3卷积核。图4 (b) 为改进的瓶颈层结构, 在该结构中采用较大的7×7卷积核代替较小的3×3卷积核, 用3×3的卷积核代替1×1的卷积核, 相比于较小的卷积核, 较大的卷积核可以恢复更多图像细节信息<citation id="132" type="reference"><link href="58" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>。在激活函数后添加批量归一化 (BN) 层, 可以防止过拟合。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907009_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 瓶颈层结构图。" src="Detail/GetImg?filename=images/GXXB201907009_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 瓶颈层结构图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907009_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Structures of bottleneck layer.</p>
                                <p class="img_note"> (a) 经典瓶颈层结构; (b) 改进的瓶颈层结构</p>
                                <p class="img_note"> (a) Classical structure of bottleneck layer. (b) improved structure of bottleneck layer</p>

                </div>
                <div class="p1">
                    <p id="99">训练过程中的均方误差损失函数定义为</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>S</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>w</mi><mi>h</mi></mrow></mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>w</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>h</mi></munderover><mo stretchy="false"> (</mo></mstyle></mrow></mstyle><mi>y</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>y</mi></mstyle><mo>∼</mo></mover><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mspace width="0.25em" /><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">式中:<i>w</i>和<i>h</i>分别为图像的宽度和高度;<i>y</i> (<i>i</i>, <i>j</i>) 为原始物体图像;<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>y</mi></mstyle><mo>∼</mo></mover><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo></mrow></math></mathml>为系统恢复的物体图像。</p>
                </div>
                <h3 id="103" name="103" class="anchor-tag">3 结果与分析</h3>
                <h4 class="anchor-tag" id="104" name="104"><b>3.1 模型训练</b></h4>
                <div class="p1">
                    <p id="105">URNet网络恢复模型主要包含两个部分:训练部分和验证部分。在训练部分, 将原始图像和相机采集图像调整大小后作为网络输入。原始图像尺寸为28 pixel×28 pixel, 考虑到DMD加载图像尺寸要求为1024 pixel×768 pixel, 将原始图片缩放为512 pixel×512 pixel, 并将其放在中心位置, 对空白部分填充0, 相机采集图像也选取中心512 pixel×512 pixel部分。为了提高训练速度, 将两种输入图像尺寸缩放为256 pixel×256 pixel。考虑到信噪比和相机的帧频限制, 输入信号频率为2 Hz, 即DMD每秒加载两张图像, 相机对应地采集两张图像。数据集选自经典人工智能数据集MNIST, 约6万张手写阿拉伯数字图像, 将数据集分为训练集和验证集两部分, 二者比例为9\:1, 两部分的数据完全分开, 没有重合, 训练时长为15个Epoch, 每个Epoch表示将所有数据训练一遍, 使用装有Intel i7CPU和GTX 1070显卡的计算机进行数据处理, 一次训练时间为6～8 h。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106"><b>3.2 模型恢复结果及分析</b></h4>
                <div class="p1">
                    <p id="107">经典瓶颈层结构与改进的瓶颈层结构模型恢复结果对比图如图5所示。图5 (a) 为原始物体图像, 图5 (b) 为经过中介面反射后被相机接收的图像, 图5 (c) 为经典瓶颈层模型恢复结果, 图5 (d) 为改进的瓶颈层模型恢复结果。两个模型的训练时间分别为6.7 h和8.4 h, 改进的瓶颈层模型训练时间略长。对比两组恢复结果, 改进的瓶颈层模型具有较好的恢复结果, 尤其在图像细节方面, 恢复结果更加丰富。采用平均余弦距离度量恢复结果与真实结果之间的相似度, 得到的结果分别为0.8138和0.8397。该结果同样说明, 相比于经典瓶颈层模型, 改进的瓶颈层模型具有更好的恢复效果。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907009_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同瓶颈层模型恢复结果。" src="Detail/GetImg?filename=images/GXXB201907009_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同瓶颈层模型恢复结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907009_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Retrieving results of different bottleneck layer models. </p>
                                <p class="img_note"> (a) 原始物体图像; (b) 相机接收的光斑信号; (c) 经典瓶颈层模型恢复结果; (d) 改进的瓶颈层模型恢复结果. (i) ～ (iv) 分 别代表不同手写字符 3, 4, 5, 8</p>
                                <p class="img_note"> (a) Ground truths; (b) speckle images; (c) retrieving results of classical bottleneck layer model; (d) retrieving results of improved bottleneck layer model; (i) - (iv) represent different handwriting characters 3, 4, 5, and 8, respectively</p>

                </div>
                <div class="p1">
                    <p id="110">为了检验模型的泛化性, 对非MNIST数据集中的图像进行恢复。图6为手写英文字母的恢复结果, 图6 (a) 为原始图像;图6 (b) 为相机采集到的光斑信号;图6 (c) 为URNet网络恢复结果。对比原始图像与恢复图像可知, URNet网络对手写英文字母具有较好的恢复结果。此结果进一步说明, 当恢复目标与训练集中的图像具有相似的结构时, URNet网络对该目标适用。</p>
                </div>
                <div class="p1">
                    <p id="111">为了进一步检验模型的可靠性, 对模型采集信号受到外界影响的情况进行讨论。实验中对采集到的光斑进行部分遮挡, 被遮挡的部分灰度值置为0, 再将光斑信号输入到模型中。结果如图7所示, 图7 (a) 为被遮挡后的光斑信号图;图7 (b) 为模型恢复结果;图像尺寸为256 pixel×256 pixel。分析结果发现, 当被遮挡部分占比为0.0625和0.1250时, 恢复图像开始发生变形, 但基本形状保持完整;当被遮挡部分占比为0.1250和0.1875时, 恢复图像发生较大变形, 但仍然可以分辨出该图形为9;当被遮挡部分占比为0.3125时, 恢复图像已无法辨认。由上述结果可得, 当物体被遮挡的像素在一定范围内时, 模型依然有效。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907009_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 手写英文字母恢复结果。" src="Detail/GetImg?filename=images/GXXB201907009_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 手写英文字母恢复结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907009_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Retrieving results of handwriting English letters. </p>
                                <p class="img_note"> (a) 原始物体图像; (b) 相机接收的光斑信号; (c) URNet模型恢复结果. (i) ～ (iv) 分别代表不同手写英文字母 U, S, H, C</p>
                                <p class="img_note"> (a) Ground truths; (b) speckle images; (c) retrieving results of URNet model; (i) - (iv) represent different handwriting English letters U, S, H, and C, respectively</p>

                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907009_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 受外界影响下的模型恢复结果。" src="Detail/GetImg?filename=images/GXXB201907009_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 受外界影响下的模型恢复结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907009_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Retrieving results under external interference. </p>
                                <p class="img_note"> (a) 光斑信号被遮挡结果; (b) URNet模型恢复结果. (i) ～ (vi) 分别代表遮挡比例0, 0.0625, 0.1250, 0.1875, 0.2500, 0.3125</p>
                                <p class="img_note"> (a) Results of occlusion of speckle signal; (b) retrieving results of URNet model. (i) - (vi) represent occlusion proportion of 0, 0.0625, 0.1250, 0.1875, 0.2500, and 0.3125, respectively</p>

                </div>
                <div class="p1">
                    <p id="114">散斑自相关成像技术利用散射介质记忆效应, 可以在非相干光照明情况下实现非视域成像。图8所示为URNet网络与散斑自相关恢复结果对比图, 图8 (a) 为原始图像;图8 (b) 为URNet网络恢复结果;图8 (c) 为散斑自相关恢复结果; (i) ～ (iv) 对应不同的手写字符。由于散斑自相关成像技术在实验中无法对较大目标成像, 因此采用仿真模拟结果, 在仿真过程中添加了10 dB的高斯随机噪声, 并对URNet网络也添加了相同噪声, 计算所得两种结果的平均相似度分别为0.7455和0.8366。由两种结果的对比可得, 相比于散斑自相关成像技术, URNet网络恢复结果具有较大提升, 说明该网络对噪声干扰更加稳健。</p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907009_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 不同模型恢复结果。" src="Detail/GetImg?filename=images/GXXB201907009_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 不同模型恢复结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907009_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Retrieving results of different models.</p>
                                <p class="img_note"> (a) 原始物体图像; (b) URNet网络恢复结果; (c) 散斑自相关恢复结果。 (i) ～ (iv) 代表不同手写字符 9, 2, 6, 0</p>
                                <p class="img_note"> (a) Ground truths; (b) retrieving results of URNet model; (c) retrieving results of speckle autocorrelation model; (i) - (iv) different handwriting characters 9, 2, 6, and 0, respectively</p>

                </div>
                <h3 id="116" name="116" class="anchor-tag">4 结  论</h3>
                <div class="p1">
                    <p id="117">针对非相干光照明情况下的非视域成像问题, 提出了基于深度学习的解决方案。结合经典的U-Net网络和ResNet网络特点, 构建出URNet网络。在此基础上, 对经典的瓶颈层结构进行改进, 对比改进前后的网络恢复结果发现, 改进的网络可以恢复更多的图像细节。利用手写英文字母数据, 对模型的泛化性进行检验。通过遮挡采集到的光斑信号, 验证模型在受到外界影响下的恢复情况。实验结果表明, URNet网络具有较好的泛化性, 并在外界影响下仍能得到较好的恢复结果。相比于散斑自相关成像技术, URNet网络恢复结果质量更高。相比于传统方法, 基于深度学习的成像方法具有较好的成像效果, 但是该方法实际上是一种统计学习方法, 并不具有普适性, 对与学习库没有相似分类特征的对象, 该方法成像效果差, 甚至无法成像。因此, 以后的研究工作重点将为不同种类复杂目标的非视域成像。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Advanced short-wavelength infrared range-gated imaging for ground applications in monostatic and bistatic configurations">

                                <b>[1]</b> Repasi E, Lutzmann P, Steinvall O, <i>et al</i>.Advanced short-wavelength infrared range-gated imaging for ground applications in monostatic and bistatic configurations[J].Applied Optics, 2009, 48 (31) :5956-5969.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Study of a dual mode SWIR active imaging system for direct imaging and non-line-of-sight vision">

                                <b>[2]</b> Laurenzis M, Christnacher F, Velten A.Study of a dual mode SWIR active imaging system for direct imaging and non-line-of-sight vision[J].Proceedings of SPIE, 2015, 9465:946509.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Estimating Motion and size of moving non-line-of-sight objects in cluttered environments">

                                <b>[3]</b> Pandharkar R, Velten A, Bardagjy A, <i>et al</i>.Estimating motion and size of moving non-line-of-sight objects in cluttered environments[C]//2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 20-25, 2011, Colorado Springs, CO, USA.New York:IEEE, 2011:265-272.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000000996&amp;v=MjU2NDRIdGpOcjQ5RlpPc1BCWFUvb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JSVZzVGFSVT1OaWZJWTdLNw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Naik N, Zhao S, Velten A, <i>et al</i>.Single view reflectance capture using multiplexed scattering and time-of-flight imaging[J].ACM Transactions on Graphics, 2011, 30 (6) :171.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Diffuse Mirrors:3D Reconstruction from Diffuse Indirect Illumination Using Inexpensive Time-of-Flight Sensors">

                                <b>[5]</b> Heide F, Xiao L, Heidrich W, <i>et al</i>.Diffuse mirrors:3D reconstruction from diffuse indirect illumination using inexpensive time-of-flight sensors[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:3222-3229.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14032100000891&amp;v=MjkzNDIvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUlWc1RhUlU9Tmo3QmFySzhIdExPcm85RlpPc1BCSFU0b0JNVDZUNFBRSA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Wu D, Velten A, O′Toole M, <i>et al</i>.Decomposing global light transport using time of flight imaging[J].International Journal of Computer Vision, 2014, 107 (2) :123-138.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Looking Around the Corner using Ultrafast Transient Imaging">

                                <b>[7]</b> Kirmani A, Hutchison T, Davis J, <i>et al</i>.Looking around the corner using ultrafast transient imaging[J].International Journal of Computer Vision, 2011, 95 (1) :13-28.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reconstruction of hidden 3D shapes using diffuse reflections">

                                <b>[8]</b> Gupta O, Willwacher T, Velten A, <i>et al</i>.Reconstruction of hidden 3D shapes using diffuse reflections[J].Optics Express, 2012, 20 (17) :19096-19108.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recovering three-dimensional shape around a corner using ultrafast time-of-flight imaging">

                                <b>[9]</b> Velten A, Willwacher T, Gupta O, <i>et al</i>.Recovering three-dimensional shape around a corner using ultrafast time-of-flight imaging[J].Nature Communications, 2012, 3:745.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detection and tracking of moving objects hidden from view">

                                <b>[10]</b> Gariepy G, Tonolini F, Henderson R, <i>et al</i>.Detection and tracking of moving objects hidden from view[J].Nature Photonics, 2016, 10 (1) :23-26.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Looking through a diffuser and around an opaque surface:a holographic approach">

                                <b>[11]</b> Singh A K, Naik D N, Pedrini G, <i>et al</i>.Looking through a diffuser and around an opaque surface:a holographic approach[J].Optics Express, 2014, 22 (7) :7694-7701.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-shot optical imaging through scattering medium using digital in-line holography">

                                <b>[12]</b> Vinu R V, Kim K, Somkuwar A S, <i>et al</i>.Single-shot optical imaging through scattering medium using digital in-line holography[J/OL]. (2016-03-24) [2019-01-01].https://arxiv.org/abs/1603.07430.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=OPTICAL PHASE CONJUGATION FOR TURBIDITY SUPPRESSION IN BIOLOGICAL SAMPLES">

                                <b>[13]</b> Yaqoob Z, Psaltis D, Feld M S, <i>et al</i>.Optical phase conjugation for turbidity suppression in biological samples[J].Nature Photonics, 2008, 2 (2) :110-115.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting disorder for perfect focusing">

                                <b>[14]</b> Vellekoop I M, Lagendijk A, Mosk A P.Exploiting disorder for perfect focusing[J].Nature Photonics, 2010, 4 (5) :320-322.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Focusing light inside dynamic scattering media with millisecond digital optical phase conjugation">

                                <b>[15]</b> Liu Y, Ma C, Shen Y C, <i>et al</i>.Focusing light inside dynamic scattering media with millisecond digital optical phase conjugation[J].Optica, 2017, 4 (2) :280-288.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Non-invasive imaging through opaque scattering layers">

                                <b>[16]</b> Bertolotti J, van Putten E G, Blum C, <i>et al</i>.Non-invasive imaging through opaque scattering layers[J].Nature, 2012, 491 (7423) :232-234.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Non-invasive single-shot imaging through scattering layers and around corners via speckle correlations">

                                <b>[17]</b> Katz O, Heidmann P, Fink M, <i>et al</i>.Non-invasive single-shot imaging through scattering layers and around corners via speckle correlations[J].Nature Photonics, 2014, 8 (10) :784-790.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Translation correlations in anisotropically scattering media">

                                <b>[18]</b> Judkewitz B, Horstmeyer R, Vellekoop I M, <i>et al</i>.Translation correlations in anisotropically scattering media[J].Nature Physics, 2015, 11 (8) :684-689.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imaging through scattering layers exceeding memory effect range with spatial-correlation-achieved point-spread-function">

                                <b>[19]</b> Li L, Li Q, Sun S, <i>et al</i>.Imaging through scattering layers exceeding memory effect range with spatial-correlation-achieved point-spread-function[J].Optics Letters, 2018, 43 (8) :1670-1673.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imaging through glass diffusers using densely connected convolutional networks">

                                <b>[20]</b> Li S, Deng M, Lee J, <i>et al</i>.Imaging through glass diffusers using densely connected convolutional networks[J].Optica, 2018, 5 (7) :803-812.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploit imaging through opaque wall via deep learning">

                                <b>[21]</b> Lyu M, Wang H, Li G, <i>et al</i>.Exploit imaging through opaque wall via deep learning[J/OL]. (2017-08-09) [2019-01-01].https://arxiv.org/abs/1708.07881.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep speckle correlation:a deep learning approach toward scalable imaging through scattering media">

                                <b>[22]</b> Li Y Z, Xue Y J, Tian L.Deep speckle correlation:a deep learning approach toward scalable imaging through scattering media[J].Optica, 2018, 5 (10) :1181-1190.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-Net:convolutional networks for biomedical image segmentation">

                                <b>[23]</b> Ronneberger O, Fischer P, Brox T.U-Net:convolutional networks for biomedical image segmentation[M]//Navab N, Hornegger J, Wells W, <i>et al</i>.Medical image computing and computer-assisted intervention-MICCAI 2015.Lecture notes in computer science.Cham:Springer, 2015, 9351:234-241.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[24]</b> He K M, Zhang X Y, Ren S Q, <i>et al</i>.Deep residual learning for image recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large Kernel Matters—Improve Semantic Segmentation by Global Convolutional Network">

                                <b>[25]</b> Peng C, Zhang X Y, Yu G, <i>et al</i>.Large kernel matters—improve semantic segmentation by global convolutional network[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:1743-1751.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201907009" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201907009&amp;v=MDg4NjJDVVJMT2VaZVZ1RnluZ1ZMelBJalhUYkxHNEg5ak1xSTlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

