

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133864343565000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201907030%26RESULT%3d1%26SIGN%3d01KS4xddmLN9nMjBA1DcT4gJXsI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201907030&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201907030&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201907030&amp;v=MDg1OTNJOUdaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5oVWJ2TElqWFRiTEc0SDlqTXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#56" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#60" data-title="2 基本原理 ">2 基本原理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#68" data-title="&lt;b&gt;2.1 多维权重能量函数&lt;/b&gt;"><b>2.1 多维权重能量函数</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;2.2 局部优化窗口模型&lt;/b&gt;"><b>2.2 局部优化窗口模型</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;2.3 基于像素类型的迭代优化&lt;/b&gt;"><b>2.3 基于像素类型的迭代优化</b></a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;2.4 算法流程&lt;/b&gt;"><b>2.4 算法流程</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#157" data-title="3 实验结果及分析 ">3 实验结果及分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#164" data-title="4 结  论 ">4 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#67" data-title="图1 算法内容细节对比">图1 算法内容细节对比</a></li>
                                                <li><a href="#80" data-title="图2 &lt;i&gt;Geman&lt;/i&gt;-&lt;i&gt;McClure&lt;/i&gt;函数不同参数对比曲线">图2 <i>Geman</i>-<i>McClure</i>函数不同参数对比曲线</a></li>
                                                <li><a href="#98" data-title="图3 分组索引。">图3 分组索引。</a></li>
                                                <li><a href="#101" data-title="图4 交叉窗口。">图4 交叉窗口。</a></li>
                                                <li><a href="#102" data-title="图5 交叉窗口与原窗口方案在测试图像上的错误匹配率">图5 交叉窗口与原窗口方案在测试图像上的错误匹配率</a></li>
                                                <li><a href="#132" data-title="图6 多分类信息图。">图6 多分类信息图。</a></li>
                                                <li><a href="#144" data-title="图7 改进的局部扩张运动算法">图7 改进的局部扩张运动算法</a></li>
                                                <li><a href="#145" data-title="图8 对比改进标签生成机制和原方法在一次迭代后的错误率">图8 对比改进标签生成机制和原方法在一次迭代后的错误率</a></li>
                                                <li><a href="#150" data-title="图9 填充方法。">图9 填充方法。</a></li>
                                                <li><a href="#151" data-title="图10 迭代优化过程。">图10 迭代优化过程。</a></li>
                                                <li><a href="#155" data-title="图11 本文算法流程图">图11 本文算法流程图</a></li>
                                                <li><a href="#156" data-title="图12 本文算法优化进程">图12 本文算法优化进程</a></li>
                                                <li><a href="#160" data-title="图13 几种全局PatchMatch算法的视差图 (匹配错误率大于1 pixel的点用红色显示) 。">图13 几种全局PatchMatch算法的视差图 (匹配错误率大于1 pixel的点用红色显示) 。</a></li>
                                                <li><a href="#163" data-title="表1 几种基于PatchMatch优化的匹配算法在Midd2006数据集中非遮挡区域的匹配结果 (阈值为1 pixel, 最好的实验结果用粗体显示) ">表1 几种基于PatchMatch优化的匹配算法在Midd2006数据集中非遮挡区域的匹配结果 (阈......</a></li>
                                                <li><a href="#200" data-title="表2 本文算法与LocalExp在Midd2003数据集的错误率对比 (阈值为0.5pixel) ">表2 本文算法与LocalExp在Midd2003数据集的错误率对比 (阈值为0.5pixel) </a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" title=" Ma R H, Zhu F, Wu Q X, &lt;i&gt;et al&lt;/i&gt;.Dense stereo matching based on image segmentation[J].Acta Optica Sinica, 2019, 39 (3) :0315001.马瑞浩, 朱枫, 吴清潇, 等.基于图像分割的稠密立体匹配算法[J].光学学报, 2019, 39 (3) :0315001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201903028&amp;v=MjIxNzR6cXFCdEdGckNVUkxPZVplVnVGeW5oVWJ2S0lqWFRiTEc0SDlqTXJJOUhiSVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Ma R H, Zhu F, Wu Q X, &lt;i&gt;et al&lt;/i&gt;.Dense stereo matching based on image segmentation[J].Acta Optica Sinica, 2019, 39 (3) :0315001.马瑞浩, 朱枫, 吴清潇, 等.基于图像分割的稠密立体匹配算法[J].光学学报, 2019, 39 (3) :0315001.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" Bleyer M, Rhemann C, Rother C.PatchMatch stereo-stereo matching with slanted support windows[C]//Procedings of the British Machine Vision Conference 2011, August 29-September 1, 2011, Dundee.Durham:BMVA Press, 2011:14." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Patchmatch stereo-stereo matching with slanted support windows">
                                        <b>[2]</b>
                                         Bleyer M, Rhemann C, Rother C.PatchMatch stereo-stereo matching with slanted support windows[C]//Procedings of the British Machine Vision Conference 2011, August 29-September 1, 2011, Dundee.Durham:BMVA Press, 2011:14.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" title=" Barnes C, Shechtman E, Finkelstein A, &lt;i&gt;et al&lt;/i&gt;.PatchMatch:a randomized correspondence algorithm for structural image editing[J].ACM Transactions on Graphics, 2009, 28 (3) :24." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007574&amp;v=MTA0ODdGWk9zSUNYczlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lJVm9XYmhBPU5pZklZN0s3SHRqTnI0OQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Barnes C, Shechtman E, Finkelstein A, &lt;i&gt;et al&lt;/i&gt;.PatchMatch:a randomized correspondence algorithm for structural image editing[J].ACM Transactions on Graphics, 2009, 28 (3) :24.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" title=" Luus R, Jaakola T H I.Optimization by direct search and systematic reduction of the size of search region[J].AIChE Journal, 1973, 19 (4) :760-766." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=OPTIMIZATION BY DIRECT SEARCH AND SYSTEMATIC REDUCTION OF THE SIZE OF SEARCH REGION">
                                        <b>[4]</b>
                                         Luus R, Jaakola T H I.Optimization by direct search and systematic reduction of the size of search region[J].AIChE Journal, 1973, 19 (4) :760-766.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" title=" Besse F, Rother C, Fitzgibbon A, &lt;i&gt;et al&lt;/i&gt;.PMBP:PatchMatch belief propagation for correspondence field estimation[J].International Journal of Computer Vision, 2014, 110 (1) :2-13." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14101300030946&amp;v=MTg1ODZnL29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUlWb1diaEE9Tmo3QmFySzhIOUhOckk5RlpPZ1BCWA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Besse F, Rother C, Fitzgibbon A, &lt;i&gt;et al&lt;/i&gt;.PMBP:PatchMatch belief propagation for correspondence field estimation[J].International Journal of Computer Vision, 2014, 110 (1) :2-13.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" title=" Lu J B, Yang H S, Min D B, &lt;i&gt;et al&lt;/i&gt;.Patch match filter:efficient edge-aware filtering meets randomized search for fast correspondence field estimation[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2013, Portland, OR, USA.New York:IEEE, 2013:1854-1861." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Patchmatch filter:Efficient edge-aware filtering meets randomized search for fast correspondence field estimation">
                                        <b>[6]</b>
                                         Lu J B, Yang H S, Min D B, &lt;i&gt;et al&lt;/i&gt;.Patch match filter:efficient edge-aware filtering meets randomized search for fast correspondence field estimation[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2013, Portland, OR, USA.New York:IEEE, 2013:1854-1861.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" Heise P, Klose S, Jensen B, &lt;i&gt;et al&lt;/i&gt;.PM-huber:PatchMatch with huber regularization for stereo matching[C]//2013 IEEE International Conference on Computer Vision (ICCV) , December 1-8, 2013, Sydney, NSW, Australia.New York:IEEE, 2013:2360-2367." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;PatchMatch with Huber regularization for stereo matching&amp;quot;">
                                        <b>[7]</b>
                                         Heise P, Klose S, Jensen B, &lt;i&gt;et al&lt;/i&gt;.PM-huber:PatchMatch with huber regularization for stereo matching[C]//2013 IEEE International Conference on Computer Vision (ICCV) , December 1-8, 2013, Sydney, NSW, Australia.New York:IEEE, 2013:2360-2367.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" title=" Li L C, Zhang S L, Yu X, &lt;i&gt;et al&lt;/i&gt;.PMSC:PatchMatch-based superpixel cut for accurate stereo matching[J].IEEE Transactions on Circuits and Systems for Video Technology, 2018, 28 (3) :679-692." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PMSC:PatchMatch-based superpixel cut for accurate stereo matching">
                                        <b>[8]</b>
                                         Li L C, Zhang S L, Yu X, &lt;i&gt;et al&lt;/i&gt;.PMSC:PatchMatch-based superpixel cut for accurate stereo matching[J].IEEE Transactions on Circuits and Systems for Video Technology, 2018, 28 (3) :679-692.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" Žbontar J, LeCun Y.Stereo matching by training a convolutional neural network to compare image patches[J].Journal of Machine Learning Research, 2016, 17:1-32." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stereo matching by training a convolutional neural network to compare image patches">
                                        <b>[9]</b>
                                         Žbontar J, LeCun Y.Stereo matching by training a convolutional neural network to compare image patches[J].Journal of Machine Learning Research, 2016, 17:1-32.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" title=" Boykov Y, Kolmogorov V.An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2004, 26 (9) :1124-1137." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision">
                                        <b>[10]</b>
                                         Boykov Y, Kolmogorov V.An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2004, 26 (9) :1124-1137.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" title=" Li L C, Yu X, Zhang S L, &lt;i&gt;et al&lt;/i&gt;.3D cost aggregation with multiple minimum spanning trees for stereo matching[J].Applied Optics, 2017, 56 (12) :3411-3420." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3Dcost aggregation with multiple minimum spanning trees for stereo matching">
                                        <b>[11]</b>
                                         Li L C, Yu X, Zhang S L, &lt;i&gt;et al&lt;/i&gt;.3D cost aggregation with multiple minimum spanning trees for stereo matching[J].Applied Optics, 2017, 56 (12) :3411-3420.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" title=" Felzenszwalb P F, Huttenlocher D P.Efficient graph-based image segmentation[J].International Journal of Computer Vision, 2004, 59 (2) :167-181." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830890&amp;v=MDkyNDNoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RlNqbFVydktKRnc9Tmo3QmFyTzRIdEhPcDR4RmJPSVBZM2s1ekJk&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Felzenszwalb P F, Huttenlocher D P.Efficient graph-based image segmentation[J].International Journal of Computer Vision, 2004, 59 (2) :167-181.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" title=" Yang Q X.A non-local cost aggregation method for stereo matching[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:1402-1409." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A non-local cost aggregation method for stereo matc-hing">
                                        <b>[13]</b>
                                         Yang Q X.A non-local cost aggregation method for stereo matching[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:1402-1409.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" Taniai T, Matsushita Y, Sato Y, &lt;i&gt;et al&lt;/i&gt;.Continuous 3D label stereo matching using local expansion moves[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (11) :2725-2739." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Continuous 3D label stereo matching using local expansion moves">
                                        <b>[14]</b>
                                         Taniai T, Matsushita Y, Sato Y, &lt;i&gt;et al&lt;/i&gt;.Continuous 3D label stereo matching using local expansion moves[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (11) :2725-2739.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" Chipot M, March R, Rosati M, &lt;i&gt;et al&lt;/i&gt;.Analysis of a nonconvex problem related to signal selective smoothing[J].Mathematical Models and Methods in Applied Sciences, 1997, 7 (3) :313-328." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Analysis of a Nonconvex Problem Related to Signal Selective Smoothing">
                                        <b>[15]</b>
                                         Chipot M, March R, Rosati M, &lt;i&gt;et al&lt;/i&gt;.Analysis of a nonconvex problem related to signal selective smoothing[J].Mathematical Models and Methods in Applied Sciences, 1997, 7 (3) :313-328.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" title=" He K M, Sun J, Tang X O.Guided image filtering[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (6) :1397-1409." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Guided Image Filtering">
                                        <b>[16]</b>
                                         He K M, Sun J, Tang X O.Guided image filtering[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (6) :1397-1409.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" title=" Olsson C, Ul&#233;n J, Boykov Y.In defense of 3D-label stereo[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2013, Portland, OR, USA.New York:IEEE, 2013:1730-1737." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=In defense of 3D-label stereo">
                                        <b>[17]</b>
                                         Olsson C, Ul&#233;n J, Boykov Y.In defense of 3D-label stereo[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2013, Portland, OR, USA.New York:IEEE, 2013:1730-1737.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" Ahmed S, Hansard M, Cavallaro A.Constrained optimization for plane-based stereo[J].IEEE Transactions on Image Processing, 2018, 27 (8) :3870-3882." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Constrained optimization for plane-based stereo">
                                        <b>[18]</b>
                                         Ahmed S, Hansard M, Cavallaro A.Constrained optimization for plane-based stereo[J].IEEE Transactions on Image Processing, 2018, 27 (8) :3870-3882.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_19" title=" Comaniciu D, Meer P.Mean shift:a robust approach toward feature space analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002, 24 (5) :603-619." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mean shift: A robust approach toward feature space analysis">
                                        <b>[19]</b>
                                         Comaniciu D, Meer P.Mean shift:a robust approach toward feature space analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002, 24 (5) :603-619.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_20" title=" Achanta R, S&#252;sstrunk S.Superpixels and polygons using simple non-iterative clustering[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4895-4904." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Superpixels and polygons using simple non-iterative clustering">
                                        <b>[20]</b>
                                         Achanta R, S&#252;sstrunk S.Superpixels and polygons using simple non-iterative clustering[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4895-4904.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_21" title=" Chum O, Matas J, Kittler J.Locally optimized RANSAC[M]//Michaelis B, Krell G.Lecture notes in computer science.Berlin, Heidelberg:Springer, 2003, 2781:236-243." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locally optimized RANSAC">
                                        <b>[21]</b>
                                         Chum O, Matas J, Kittler J.Locally optimized RANSAC[M]//Michaelis B, Krell G.Lecture notes in computer science.Berlin, Heidelberg:Springer, 2003, 2781:236-243.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_22" title=" Li Y, Min D B, Brown M S, &lt;i&gt;et al&lt;/i&gt;.SPM-BP:sped-up PatchMatch belief propagation for continuous MRFs[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:4006-4014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spm-bp: Sped-up patchmatchbelief propagation for continuous mrfs">
                                        <b>[22]</b>
                                         Li Y, Min D B, Brown M S, &lt;i&gt;et al&lt;/i&gt;.SPM-BP:sped-up PatchMatch belief propagation for continuous MRFs[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:4006-4014.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_23" title=" Taniai T, Matsushita Y, Naemura T.Graph cut based continuous stereo matching using locally shared labels[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:1613-1620." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Graph cut based continuous stereo matching using locally shared labels">
                                        <b>[23]</b>
                                         Taniai T, Matsushita Y, Naemura T.Graph cut based continuous stereo matching using locally shared labels[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:1613-1620.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-16 17:32</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(07),269-281 DOI:10.3788/AOS201939.0715006            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于像素类别优化的PatchMatch立体匹配算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E9%9B%85%E6%98%86&amp;code=39208319&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高雅昆</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%B6%9B&amp;code=09314982&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘涛</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%B5%B7%E6%BB%A8&amp;code=09314894&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李海滨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%96%87%E6%98%8E&amp;code=09264027&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张文明</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%87%95%E5%B1%B1%E5%A4%A7%E5%AD%A6%E5%B7%A5%E4%B8%9A%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%8E%A7%E5%88%B6%E5%B7%A5%E7%A8%8B%E6%B2%B3%E5%8C%97%E7%9C%81%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0022354&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">燕山大学工业计算机控制工程河北省重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于PatchMatch (同时估计像素点的视差和法向量的3D标签) 的方案已经在立体匹配中取得高精度的亚像素视差, 但该类方法无法有效解决图像无纹理区域的错误匹配。针对这一问题, 对LocalExp (local expansion move) 算法进行了改进, 并提出一种融合多维信息的自适应像素类别优化的立体匹配算法。该方法设计了一种交叉窗口, 在窗口内基于颜色与颜色的自相关信息构建相关权重, 并利用约束函数剔除匹配代价中的离群值;在PatchMatch的标签初始化阶段增加约束机制, 改进视差标签的建议生成机制, 并利用基于局部扩张运动的优化方法求解标签值;利用基于像素类别的填充策略进行视差优化。实验结果表明所提算法能够在Middlebury数据集上取得较低的匹配误差。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">立体匹配算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=PatchMatch&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">PatchMatch;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%83%8F%E7%B4%A0%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">像素分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E5%B7%AE%E4%BC%98%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视差优化;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *李海滨, E-mail:hbli@ysu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-03</p>

            </div>
                    <h1><b>Stereo Matching Algorithm Based on Pixel Category Optimized Patch Match</b></h1>
                    <h2>
                    <span>Gao Yakun</span>
                    <span>Liu Tao</span>
                    <span>Li Haibin</span>
                    <span>Zhang Wenming</span>
            </h2>
                    <h2>
                    <span>Key Laboratory of Industrial Computer Control Engineering of Hebei Province, Yanshan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>PatchMatch-based algorithms that simultaneously estimate the disparities and normal unit of a disparity plane have achieved highly accurate sub-pixel disparities in the stereo matching problem; however, this kind of methods can not effectively deal with error matching in the non-texture regions of image. To solve this problem, we improve the LocalExp (local expansion move) algorithm and present a new stereo matching algorithm integrating multidimensional information for adaptive pixel category optimization. First, a crossover window is designed, the color and color self-correlation information in the window are used to establish the weight, and the restrained function is utilized to eliminate the outliers in the matching cost. Second, the constraint mechanism is added to the label initialization procedure, the proposal generation mechanism is modified, and the local expansion movement algorithm is used to optimize the label values. Finally, the pixel category information-based filling strategy is used to refine the disparity. The experimental results show that the proposed method can obtain a low matching error on the Middlebury dataset.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=stereo%20matching%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">stereo matching algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=PatchMatch&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">PatchMatch;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=pixel%20category&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">pixel category;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=disparity%20refinement&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">disparity refinement;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-03</p>
                            </div>


        <!--brief start-->
                        <h3 id="56" name="56" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="57">双目立体匹配技术作为三维重建研究领域的核心技术之一, 已广泛应用于无人驾驶、机器人导航定位、增强/虚拟现实、物体识别与跟踪、地理信息系统 (GIS) 等领域<citation id="169" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。在正向平行表面的视差一致性假设下 (窗口内视差相等) , 对于在斜面及曲面区域的视差计算, 传统匹配算法会产生阶梯效应。针对这一问题, Bleyer等<citation id="170" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出了基于3D标签的倾斜窗口假设 (窗口内视差值存在渐变) , 利用PatchMatch (patch match) <citation id="171" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>算法中图像修复的思想来解决立体匹配中视差问题, 通过对像素点3D标签分别进行随机初始化、邻域传播、左右视图传播、随机搜索以及平面细化等来估计视差, 并将该方法称为PMS (patch match stereo) 。3D标签相对于单一视差值, 增加了像素点的切平面法向量信息, 更能表征像素点所在平面的空间信息。但采用PatchMatch算法进行立体匹配时存在以下问题:1) 随机对每个像素点进行参数初始化, 初始化过程并不能保证每个像素都有一个几何意义下可行的视差平面参数;2) 平面细化过程使用Luus和Jaakola<citation id="172" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出的优化方法来最小化代价函数, 并不能保证找到代价的局部最小值;3) 受代价函数或能量函数的限制, 该方法不能有效解决弱或无纹理区域的错误匹配。</p>
                </div>
                <div class="p1">
                    <p id="58">不同学者针对这些问题提出了不同的立体匹配解决方案。Besse等<citation id="173" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>将PMS算法和粒子置信传播算法整合到统一的结构下, 提出了一种基于置信传播BP (belief propagation) 的加速全局立体匹配算法 (PMBP) 。该方法在BP框架下将粒子采样标签更新和PatchMatch算法的邻域传播相融合解决了连续性标签处理问题。同时, 该方法在PMS算法的代价函数基础上, 考虑邻域平面的平滑项约束构造能量函数, 得到了比PMS算法更准确的视差, 但BP作为优化算法会陷入局部极小值。Lu等<citation id="174" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出的PMF (patch match filter) 算法采用超像素作为匹配单元代替PMS算法中的固定尺寸窗口, 在代价函数中引入由粗到精的尺度一致性约束关系以缓解无纹理区域中的错误匹配, 并通过在PMS标签搜索策略中增加一种有效的超像素相似性计算方案来搜索候选标签。该方法比PMS算法的运行速度快, 但在能量函数中无正则化约束视差平滑变化。Heise等<citation id="175" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出的PM-huber算法将PatchMatch随机搜索环节同Huber算子约束的显式变分平滑方法相结合, 构建能量函数的视差约束平滑项, 并在PMS视差标签初始化时, 根据经验限定标签取值范围, 该方案相对于PMF能获得较好的精度。Li等<citation id="176" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出的PMSC (PatchMatch-based superpixel cut) 算法步骤为:1) 利用超像素单元构建顶层匹配代价, 以Žbontar等<citation id="177" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出的卷积神经网络 (CNN) 方法计算得到的像素块相似度作为底层代价, 并将两层代价进行组合以构建匹配能量函数, 同时将基于曲率的二阶平滑项用于视差平滑;2) 在视差优化过程中, 在超像素间使用PatchMatch算法更新当前超像素区域标签, 在超像素内利用GC (graph cut) <citation id="178" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>优化每一像素点的最优标签值;3) 融合超像素块标签和像素点标签以获得最终视差图。由于利用GC算法在节点间的优化可同步改善连接节点状态, 因此该方案可有效地减少图像在弱/无纹理区域的误匹配。不同于PMSC算法, Li等<citation id="179" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出的3DMST (3D minimum spanning tree) 匹配方案的步骤为:1) 采用分割树分割<citation id="180" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>代替超像素分割, 将图像分成多个最小生成树图结构;2) 使用Yang<citation id="181" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出的代价聚合方法计算每个MST上像素点的匹配代价值;3) 基于PatchMatch算法优化每个MST标签状态, 并求解像素点标签。该方案进一步降低了图像在无纹理区域的匹配错误率。Taniai等<citation id="182" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出的局部扩展移动优化算法LocalExp与PMSC、3DMST算法的整体思想相似, 但存在几点区别:1) 构造由小到大的多尺寸规则窗口金字塔模型, 在不同尺寸的窗口内计算匹配代价, 进行标签传播;2) 针对每一个窗口单元构造扩散区域用于PatchMatch的空间传播, 构造滤波区域用于基于快速代价滤波的代价聚合方法中, 求取粗略视差值;3) 利用GC优化更新细化局部窗口内像素点的标签。目前该方法的结果在Middlebury网站上排名前列。</p>
                </div>
                <div class="p1">
                    <p id="59">虽然上述匹配算法针对PatchMatch算法的局限性给出了不同的解决方法, 但这些方法仍不能较好地解决图像中无纹理和弱纹理区域的误匹配问题。针对这一问题, 本文基于LocalExp算法提出了一种新的基于像素分类的立体匹配解决方案。</p>
                </div>
                <h3 id="60" name="60" class="anchor-tag">2 基本原理</h3>
                <div class="p1">
                    <p id="61">LocalExp算法的流程如下:1) 随机初始化图像的3D标签图, 为每个像素点<i>p</i> (<i>p</i><sub><i>x</i></sub>, <i>p</i><sub><i>y</i></sub>) 随机分配单位表面法向量<b><i>n</i></b><sub><i>p</i></sub> (<i>n</i><sub><i>x</i></sub>, <i>n</i><sub><i>y</i></sub>, <i>n</i><sub><i>z</i></sub>) 和随机视差<i>z</i>, 用于像素点所在参数平面<i>L</i><sub><i>p</i></sub> (<i>a</i><sub><i>p</i></sub>, <i>b</i><sub><i>p</i></sub>, <i>c</i><sub><i>p</i></sub>) 的初始化, 其转换关系为<i>a</i><sub><i>p</i></sub>=-<i>n</i><sub><i>x</i></sub>/<i>n</i><sub><i>z</i></sub>, <i>b</i><sub><i>p</i></sub>=-<i>n</i><sub><i>y</i></sub>/<i>n</i><sub><i>z</i></sub>, <i>c</i><sub><i>p</i></sub>=- (<i>n</i><sub><i>x</i></sub><i>p</i><sub><i>x</i></sub>+<i>n</i><sub><i>y</i></sub><i>p</i><sub><i>y</i></sub>+<i>n</i><sub><i>z</i></sub><i>z</i>) /<i>n</i><sub><i>z</i></sub>, 其中<i>p</i><sub><i>x</i></sub>, <i>p</i><sub><i>y</i></sub>为像素点<i>p</i>的坐标分量, <i>n</i><sub><i>x</i></sub>, <i>n</i><sub><i>y</i></sub>, <i>n</i><sub><i>z</i></sub>为像素点<i>p</i>的3D单位法向量分量, <i>a</i><sub><i>p</i></sub>, <i>b</i><sub><i>p</i></sub>, <i>c</i><sub><i>p</i></sub>为点<i>p</i>所在视差参数平面系数;2) 使用3种不同尺寸的窗口构建局部窗口, 并将其作为优化单元, 利用颜色和梯度信息计算各窗口中像素点在当前标签下的匹配代价;3) 利用PatchMatch算法在不同窗口之间进行3D视差标签传播以搜寻最优标签;4) 基于GC算法优化窗口内像素点的标签。</p>
                </div>
                <div class="p1">
                    <p id="62">图1给出LocalExp算法和本文改进的算法内容, 其中黑色字体为LocalExp算法的结构图, 而黑色加粗字体为本文改进的环节, 图中<i>K</i><sub>s</sub>、<i>K</i><sub>b</sub>为窗口分组数量。具体改进内容如下。</p>
                </div>
                <div class="p1">
                    <p id="63">1) 提出一种能够提取更多局部窗口用于计算像素点匹配代价和共享标签的交叉窗口模型, 将颜色内相关性信息作为权重构建因素, 在局部窗口内对匹配代价进行多维权重滤波运算, 构建能量函数;同时, 用Geman-McClure函数<citation id="183" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>代替传统的截断函数处理异常值, 以保证初始代价值的稳定与平滑。</p>
                </div>
                <div class="p1">
                    <p id="64">2) 采用具有约束信息的视差标签初始化方法 (IPMS) , 使初始化视差标签值更合理有效。</p>
                </div>
                <div class="p1">
                    <p id="65">3) 根据像素分类信息改进LocalExp算法标签建议机制, 即在迭代过程中通过多种机制生成候选标签集合用于迭代更新视差标签。</p>
                </div>
                <div class="p1">
                    <p id="66">4) 在像素点视差值优化处理方面, 设计了一种基于像素分类的视差优化方法, 利用交叉线搜索同标签下可靠的视差标签对图像弱纹理和无纹理区域中的视差误匹配点进行错误填充;采用最近邻域填充方法对边缘、纹理区域以及遮挡区域中的错误点进行填充。</p>
                </div>
                <div class="area_img" id="67">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907030_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 算法内容细节对比" src="Detail/GetImg?filename=images/GXXB201907030_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 算法内容细节对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907030_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Detail comparison of algorithm content</p>

                </div>
                <h4 class="anchor-tag" id="68" name="68"><b>2.1 多维权重能量函数</b></h4>
                <div class="p1">
                    <p id="69">利用在PairwiseMRF模型下定义的最小化能量函数来估计像素点的3D视差标签。能量函数表达式为</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false"> (</mo><mi>L</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>p</mi><mo>∈</mo><mi>Ω</mi></mrow></munder><mtext>ϕ</mtext></mstyle><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false"> (</mo><mi>L</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>q</mi><mo stretchy="false">) </mo><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mn>4</mn></msub></mrow></munder><mi>φ</mi></mstyle><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mo stretchy="false"> (</mo><mi>L</mi><msub><mrow></mrow><mi>p</mi></msub><mo>, </mo><mi>L</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">式中:<i>Ω</i>为像素点集合;<i>λ</i>为平滑系数;<i>N</i><sub>4</sub>为四邻域关系;<i>L</i>为3D标签量集合;ϕ<sub><i>p</i></sub> (<i>L</i><sub><i>p</i></sub>) 为像素点<i>p</i>的数据项;<i>φ</i><sub><i>pq</i></sub> (<i>L</i><sub><i>p</i></sub>, <i>L</i><sub><i>q</i></sub>) 为像素点<i>p</i>、<i>q</i>间的平滑项, 其中<i>L</i><sub><i>p</i></sub>为像素点<i>p</i>的3D标签值, <i>L</i><sub><i>q</i></sub>为像素点<i>q</i>的3D标签值, <i>p</i>为窗口中心像素点, <i>q</i>代表窗口中的其他像素点。ϕ<sub><i>p</i></sub> (<i>L</i><sub><i>p</i></sub>) 用于计算匹配像素间相似度的数据项, 可表示为</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>ϕ</mtext><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false"> (</mo><mi>L</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ζ</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>q</mi><mo>∈</mo><mi>W</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></munder><mi>ω</mi></mstyle><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mi>Γ</mi><mo stretchy="false"> (</mo><mi>q</mi><mo>, </mo><mi>L</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">式中:<i>Γ</i> (<i>q</i>, <i>L</i><sub><i>p</i></sub>) 为像素点<i>q</i>的标签值为<i>L</i><sub><i>p</i></sub> (<i>a</i><sub><i>p</i></sub>, <i>b</i><sub><i>p</i></sub>, <i>c</i><sub><i>p</i></sub>) 时, 以<i>p</i>为中心的窗口内的像素点<i>q</i>与待匹配图对应点<i>q</i><sup>*</sup>的匹配代价值;<i>W</i><sub><i>p</i></sub>为以点<i>p</i>为中心的滤波窗口;<i>Z</i><sub><i>p</i></sub>为权重归一化参数;<i>ω</i><sub><i>pq</i></sub>为权重, 可表示为</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>W</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">|</mo></mrow></mfrac><mo>⋅</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>q</mi><mo stretchy="false">) </mo><mo>∈</mo><mi>W</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></munder><mo stretchy="false">[</mo></mstyle><mi mathvariant="bold-italic">Ι</mi><mo>+</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>⋅</mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mi mathvariant="bold-italic">e</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mi>q</mi></msub><mo>-</mo><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中<i>μ</i><sub><i>k</i></sub>、<i>Σ</i><sub><i>k</i></sub>、<b><i>e</i></b>分别为局部窗口<i>W</i><sub><i>k</i></sub>内的颜色均值、协方差和避免过拟合的矩阵<citation id="184" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>;<i>W</i>为窗口尺寸;<b><i>I</i></b><sub><i>p</i></sub>为<i>p</i>的颜色向量;<b><i>I</i></b><sub><i>q</i></sub>为<i>q</i>的颜色向量。</p>
                </div>
                <div class="p1">
                    <p id="76">利用由颜色分量、图像水平和垂直梯度分量组合的相似度函数<i>Γ</i> (<i>q</i>, <i>L</i><sub><i>p</i></sub>) 计算像素点间的匹配代价。多代价值融合的策略对一些变化区域的匹配精度有小幅度提升, 并且能够在边缘处得到更加准确和平滑的视差图。</p>
                </div>
                <div class="p1">
                    <p id="77">由颜色分量、图像水平和垂直梯度分量组合的相似度函数<i>Γ</i> (<i>q</i>, <i>L</i><sub><i>p</i></sub>) 可表示为</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Γ</mi><mo stretchy="false"> (</mo><mi>q</mi><mo>, </mo><mi>L</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>α</mi><mo stretchy="false">) </mo><mi>G</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false"> (</mo><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mi>q</mi></msub><mo>-</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mrow><mi>q</mi><msup><mrow></mrow><mo>*</mo></msup></mrow></msub><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>τ</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mn>0</mn><mo>.</mo><mn>5</mn><mi>α</mi><mo stretchy="false">[</mo><mi>G</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>r</mtext><mtext>a</mtext><mtext>d</mtext><mi>x</mi></mrow></msub><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mo>∇</mo><msub><mrow></mrow><mi>x</mi></msub><mi>Ι</mi><msub><mrow></mrow><mi>q</mi></msub><mo>-</mo><mo>∇</mo><msub><mrow></mrow><mi>x</mi></msub><mi>Ι</mi><msub><mrow></mrow><mrow><mi>q</mi><msup><mrow></mrow><mo>*</mo></msup></mrow></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>τ</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>r</mtext><mtext>a</mtext><mtext>d</mtext><mi>x</mi></mrow></msub><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>G</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>r</mtext><mtext>a</mtext><mtext>d</mtext><mi>y</mi></mrow></msub><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mo>∇</mo><msub><mrow></mrow><mi>y</mi></msub><mi>Ι</mi><msub><mrow></mrow><mi>q</mi></msub><mo>-</mo><mo>∇</mo><msub><mrow></mrow><mi>y</mi></msub><mi>Ι</mi><msub><mrow></mrow><mrow><mi>q</mi><msup><mrow></mrow><mo>*</mo></msup></mrow></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>τ</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>r</mtext><mtext>a</mtext><mtext>d</mtext><mi>y</mi></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">式中:∇<sub><i>x</i></sub><i>I</i>表示灰度图像在梯度算子滤波处理后水平方向灰度梯度;∇<sub><i>y</i></sub><i>I</i>为垂直方向灰度梯度;<i>G</i><sub>col</sub>为像素点颜色信息的约束函数;<i>G</i><sub>grad<i>x</i></sub>为水平方向梯度约束函数;<i>G</i><sub>grad<i>y</i></sub>为垂直方向梯度约束函数;<i>τ</i><sub>col</sub>、<i>τ</i><sub>grad<i>x</i></sub>、<i>τ</i><sub>grad<i>y</i></sub>分别表示匹配代价的不同分量的调节参数;<i>α</i>为比例因子。在 (4) 式中, 可以通过预定义参数<i>τ</i>调整异常值控制范围, 如图2所示, 原始代价<i>c</i>经过变化后趋于稳定、平滑。LocalExp在匹配代价计算中用线性截断处理异常点代价值, 而本文用</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907030_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Geman-McClure函数不同参数对比曲线" src="Detail/GetImg?filename=images/GXXB201907030_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 <i>Geman</i>-<i>McClure</i>函数不同参数对比曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907030_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Comparison curves for different parameters of</i><i>Geman</i>-<i>McClure function</i></p>

                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mo stretchy="false"> (</mo><mi>c</mi><mo>, </mo><mi>τ</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>c</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>c</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>τ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo><mspace width="0.25em" /></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">表示的约束函数替代线性截断函数, 当代价值c超过某值时, 函数的输出将平滑下降。</p>
                </div>
                <div class="p1">
                    <p id="84">能量函数中用于约束视差变化的平滑项φ<sub>pq</sub> (L<sub>p</sub>, L<sub>q</sub>) 由<i>Olsson</i>等<citation id="185" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出的二阶曲率正则化约束关系表示为</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>φ</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mo stretchy="false"> (</mo><mi>L</mi><msub><mrow></mrow><mi>p</mi></msub><mo>, </mo><mi>L</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>max</mi><mo stretchy="false"> (</mo><mi>ω</mi><msubsup><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow><mo>*</mo></msubsup><mo>, </mo><mi>ε</mi><mo stretchy="false">) </mo><mi>min</mi><mo stretchy="false">[</mo><mi>ψ</mi><mo stretchy="false"> (</mo><mi>L</mi><msub><mrow></mrow><mi>p</mi></msub><mo>, </mo><mi>L</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mi>ε</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>i</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">式中:ω<sup>*</sup><sub>pq</sub>代表平滑项的权重;ψ (L<sub>p</sub>, L<sub>q</sub>) 为视差惩罚函数;ε为下限值, 能够减小权重受噪声的影响;ε<sub><i>dis</i></sub>为最大视差跳变量。</p>
                </div>
                <div class="p1">
                    <p id="87">相对于<i>LocalExp</i>, 本文将颜色内相关性分量<b><i>I</i></b><sub>Cn</sub>和颜色分量作为平滑项的权重<i>ω</i><sup>*</sup><sub><i>pq</i></sub>组合, 组合后的权重关系反映了色彩内部的一些特征, 更能表征出相邻像素颜色的一致性和差异性。<i>I</i><sub>Cn</sub>和<i>ω</i><sup>*</sup><sub><i>pq</i></sub>可分别表示为</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mrow><mtext>C</mtext><mtext>n</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">[</mo><mi>Ι</mi><msubsup><mrow></mrow><mtext>n</mtext><mtext>r</mtext></msubsup><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>-</mo><mi>Ι</mi><msubsup><mrow></mrow><mtext>n</mtext><mtext>g</mtext></msubsup><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>, </mo><mi>Ι</mi><msubsup><mrow></mrow><mtext>n</mtext><mtext>g</mtext></msubsup><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>-</mo></mtd></mtr><mtr><mtd><mi>Ι</mi><msubsup><mrow></mrow><mtext>n</mtext><mtext>b</mtext></msubsup><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>, </mo><mi>Ι</mi><msubsup><mrow></mrow><mtext>n</mtext><mtext>b</mtext></msubsup><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>-</mo><mi>Ι</mi><msubsup><mrow></mrow><mtext>n</mtext><mtext>r</mtext></msubsup><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>ω</mi><msubsup><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow><mo>*</mo></msubsup><mo>=</mo><mi>exp</mi><mo stretchy="false">{</mo><mo>-</mo><mo stretchy="false">{</mo><mi>β</mi><mo stretchy="false">[</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mtext>n</mtext></msub><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mtext>n</mtext></msub><mo stretchy="false"> (</mo><mi>q</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mo>/</mo><mi>γ</mi><msub><mrow></mrow><mtext>n</mtext></msub><mo stretchy="false">]</mo><mo>-</mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>β</mi><mo stretchy="false">) </mo><mo stretchy="false">[</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mrow><mtext>C</mtext><mtext>n</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mrow><mtext>C</mtext><mtext>n</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>q</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mo>/</mo><mi>γ</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>n</mtext></mrow></msub><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo stretchy="false">}</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">式中:<i>I</i><sup>r</sup><sub>n</sub> (<i>p</i>) 、<i>I</i><sup>g</sup><sub>n</sub> (<i>p</i>) 、<i>I</i><sup>b</sup><sub>n</sub> (<i>p</i>) 为像素点<i>p</i>的RGB (red-green-blue) 强度值; (<i>p</i>, <i>q</i>) ∈<i>N</i><sub>4</sub>, <i>N</i><sub>4</sub>为4邻域;<i>β</i>为组合因子;<i>γ</i><sub>n</sub>, <i>γ</i><sub>cn</sub>为颜色相似计算的参数。平滑项中的视差惩罚函数表达式为</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>ψ</mi><mo stretchy="false"> (</mo><mi>L</mi><msub><mrow></mrow><mi>p</mi></msub><mo>, </mo><mi>L</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">|</mo><mi>d</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false"> (</mo><mi>L</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false"> (</mo><mi>L</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mo>-</mo></mtd></mtr><mtr><mtd><mo stretchy="false">|</mo><mi>d</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false"> (</mo><mi>L</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false"> (</mo><mi>L</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">式中:<i>d</i><sub><i>q</i></sub> (<i>L</i><sub><i>p</i></sub>) =<i>a</i><sub><i>p</i></sub><i>q</i><sub><i>x</i></sub>+<i>b</i><sub><i>p</i></sub><i>q</i><sub><i>y</i></sub>+<i>c</i><sub><i>p</i></sub>为像素点<i>q</i>在<i>L</i><sub><i>p</i></sub>标签下的视差值。函数<i>ψ</i> (<i>L</i><sub><i>p</i></sub>, <i>L</i><sub><i>q</i></sub>) 用于惩罚邻域像素点 (<i>p</i>, <i>q</i>) ∈<i>N</i><sub>4</sub>分别在标签<i>L</i><sub><i>p</i></sub>, <i>L</i><sub><i>q</i></sub>下视差的不连续性。由于平滑项函数满足GC算法中子模块能量优化条件, 即</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ψ</mi><mo stretchy="false"> (</mo><mi>α</mi><msup><mrow></mrow><mn>1</mn></msup><mo>, </mo><mi>α</mi><msup><mrow></mrow><mn>1</mn></msup><mo stretchy="false">) </mo><mo>+</mo><mi>ψ</mi><mo stretchy="false"> (</mo><mi>β</mi><msup><mrow></mrow><mn>1</mn></msup><mo>, </mo><mi>γ</mi><msup><mrow></mrow><mn>1</mn></msup><mo stretchy="false">) </mo><mo>≤</mo><mi>ψ</mi><mo stretchy="false"> (</mo><mi>α</mi><msup><mrow></mrow><mn>1</mn></msup><mo>, </mo><mi>β</mi><msup><mrow></mrow><mn>1</mn></msup><mo stretchy="false">) </mo><mo>+</mo><mi>ψ</mi><mo stretchy="false"> (</mo><mi>α</mi><msup><mrow></mrow><mn>1</mn></msup><mo>, </mo><mi>γ</mi><msup><mrow></mrow><mn>1</mn></msup><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">式中:<i>α</i><sup>1</sup>, <i>β</i><sup>1</sup>, <i>γ</i><sup>1</sup>为3D视差平面参数, 因此可根据GC算法最小化能量函数, 获取标签值, (10) 式的详细证明可参考文献<citation id="186" type="reference">[<a class="sup">14</a>]</citation>。</p>
                </div>
                <h4 class="anchor-tag" id="94" name="94"><b>2.2 局部优化窗口模型</b></h4>
                <h4 class="anchor-tag" id="95" name="95">2.2.1 交叉窗口</h4>
                <div class="p1">
                    <p id="96">在LocalExp中设定网格大小将图像分割成网格区域<i>C</i><sub><i>ij</i></sub>∈<i>Ω</i>, 下标 (<i>i</i>, <i>j</i>) ∈<b>Z</b><sup>2</sup>代表其网格单元坐标, <b>Z</b>为整数集。LocalExp中采用3种不同尺寸 (5×5, 10×10, 25×25) 的网格大小构建图像金字塔模型来平衡PM算法中的随机搜索和局部传播的互相影响。为了实现算法加速, 对多匹配单元进行并行优化处理, 规定了一种互不相交的局部区域窗口分组方法, 图像的网格分组数<i>K</i>=6× (<i>j</i>mod 6) + (<i>i</i>mod 6) , 如图3 (a) 所示为<i>K</i>=36分组后的图像细胞索引图。针对每个网格单元定义三个区域用于视差计算, 图3 (b) 中标号12的白色网格单元为中心区域<i>C</i><sub><i>ij</i></sub>, 相连的灰色单元为扩张区域<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mi>C</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mstyle displaystyle="true"><munder><mo>∪</mo><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mn>4</mn></msub><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo></mrow></munder><mi>C</mi></mstyle><msub><mrow></mrow><mrow><mi>m</mi><mi>n</mi></mrow></msub></mrow></mrow></math></mathml>, 其中<i>m</i>、<i>n</i>表示细胞单元<i>C</i>的横轴坐标。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907030_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 分组索引。" src="Detail/GetImg?filename=images/GXXB201907030_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 分组索引。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907030_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Grouping index. </p>
                                <p class="img_note"> (a) 图像细胞索引分组; (b) 局部扩张区域</p>
                                <p class="img_note"> (a) Image cell index groups; (b) locally extended region</p>

                </div>
                <div class="p1">
                    <p id="99">受局部立体匹配中多窗口和自适应窗口思想的启发, 提出一种交叉窗口模型以获取更多局部特性窗口。相对于LocalExp的窗口模型, 本文模型中心窗口的滑动距离小于细胞单元尺寸大小, 即保留相连细胞单元的部分区域作为新细胞的中心区域。新中心区域与原细胞区域存在局部信息交叉, 故称交叉窗口。图4 (a) 中给出了同分组下的两种窗口模型 (交叉窗口为半个细胞单元移动长度) 单元标号结果。交叉窗口结构能够产生更多的窗口去捕捉局部信息, 每一个像素能够被分配更多的标签求取代价, 因此更有利于获取最优标签。图4 (b) 中<mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mstyle displaystyle="true"><munder><mo>∪</mo><mrow><mi>p</mi><mo>∈</mo><mi>R</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></munder><mi>W</mi></mstyle><msub><mrow></mrow><mi>p</mi></msub></mrow></mrow></math></mathml>, 表示为中心区域<i>C</i><sub><i>ij</i></sub>的滤波区域。图5给出两种窗口模型在5次实验后的平均错误率结果对比, 相比于LocalExp, 本文方法的平均匹配准确率明显提高。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907030_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 交叉窗口。" src="Detail/GetImg?filename=images/GXXB201907030_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 交叉窗口。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907030_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Cross windows. </p>
                                <p class="img_note"> (a) 不同分组的扩展区域; (b) 局部图像区域</p>
                                <p class="img_note"> (a) Expansion regions for different groups; (b) local image region</p>

                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907030_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 交叉窗口与原窗口方案在测试图像上的错误匹配率" src="Detail/GetImg?filename=images/GXXB201907030_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 交叉窗口与原窗口方案在测试图像上的错误匹配率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907030_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Error matching rates of cross window and original method on test images</p>

                </div>
                <h4 class="anchor-tag" id="104" name="104">2.2.2 窗口标签初始化</h4>
                <div class="p1">
                    <p id="105">基于PatchMatch算法的匹配算法流程如下:1) 初始化窗口内像素点标签;2) 在每次迭代过程中, 在窗口间对标签进行空间传播和随机搜索, 通过判断窗口在当前标签下的能量函数增减性来改变窗口内像素的视差分布。算法迭代收敛速度取决于初始化过程的随机3D标签, 随机数值越可靠, 算法收敛速度越快。所以, 初始化阶段是重要组成部分。本文替换LocalExp中视差标签初始化方式, 采用Ahmed等<citation id="187" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出的增加约束原则的PatchMatch推理方法IPMS, 该方法建立窗口内的像素点所在视差平面法向量、视差范围以及窗口内视差分布关系, 给出可靠的初始化标签以加速收敛速度。</p>
                </div>
                <div class="p1">
                    <p id="106">IPMS中给出了两种初始化约束手段:</p>
                </div>
                <div class="p1">
                    <p id="107">1) 左右视图下视差可见性约束原则。根据射影变换关系</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi mathvariant="bold-italic">n</mi><mo>′</mo></msup><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">Μ</mi><msup><mrow></mrow><mrow><mo>-</mo><mtext>Τ</mtext></mrow></msup><mi mathvariant="bold-italic">n</mi></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Μ</mi><msup><mrow></mrow><mrow><mo>-</mo><mtext>Τ</mtext></mrow></msup><mi mathvariant="bold-italic">n</mi><mo stretchy="false">∥</mo></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">建立左视图<i>I</i><sub>0</sub>法向量<b><i>n</i></b>= (<i>n</i><sub><i>x</i></sub>, <i>n</i><sub><i>y</i></sub>, <i>n</i><sub><i>z</i></sub>) 和右视图<i>I</i><sub>1</sub>法向量<b><i>n</i></b>′= (<i>n</i>′<sub><i>x</i></sub>, <i>n</i>′<sub><i>y</i></sub>, <i>n</i>′<sub><i>z</i></sub>) 的转换关系<image id="199" type="formula" href="images/GXXB201907030_19900.jpg" display="inline" placement="inline"><alt></alt></image><image id="199" type="formula" href="images/GXXB201907030_19901.jpg" display="inline" placement="inline"><alt></alt></image>, 得到任意像素点<i>p</i>的<b><i>n</i></b>= (<i>n</i><sub><i>x</i></sub>, <i>n</i><sub><i>y</i></sub>, <i>n</i><sub><i>z</i></sub>) 中变量的限制关系为<i>n</i><sub><i>z</i></sub>&gt;0, <i>n</i><sub><i>x</i></sub>&lt;<i>n</i><sub><i>z</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="112">2) 在窗口内的视差约束。基于中心点<i>p</i> (<i>x</i><sub><i>p</i></sub>, <i>y</i><sub><i>p</i></sub>) 的半径为<i>r</i>的窗口内<i>q</i> (<i>x</i><sub><i>q</i></sub>, <i>y</i><sub><i>q</i></sub>) 的视差<i>d</i><sub><i>q</i></sub>满足</p>
                </div>
                <div class="p1">
                    <p id="113" class="code-formula">
                        <mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mi>q</mi></msub><mo>=</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mi>x</mi></msub></mrow><mrow><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub></mrow></mfrac><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mi>y</mi></msub></mrow><mrow><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub></mrow></mfrac><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>d</mi><msub><mrow></mrow><mi>p</mi></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="114">式中:<i>d</i><sub><i>p</i></sub>为像素点<i>p</i>的视差值。窗口内任意像素点<i>q</i>的视差范围为[<i>d</i><sub>min</sub>, <i>d</i><sub>max</sub>]及其与中心像素<i>p</i>的坐标差绝对值范围可分别表示为</p>
                </div>
                <div class="p1">
                    <p id="115" class="code-formula">
                        <mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>d</mi><msub><mrow></mrow><mrow><mi>q</mi><mo>∈</mo><mi>w</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></msub><mo>=</mo><mo stretchy="false">{</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>, </mo><mi>d</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false">}</mo></mtd></mtr><mtr><mtd columnalign="left"><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>q</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">|</mo><mo>≤</mo><mi>r</mi><mo>, </mo><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>q</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">|</mo><mo>≤</mo><mi>r</mi></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="116">将 (13) 式代入 (12) 式中可得</p>
                </div>
                <div class="p1">
                    <p id="117" class="code-formula">
                        <mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo>-</mo><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false"> (</mo><mi>d</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false">) </mo><mo>≤</mo><mi>n</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>n</mi><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">) </mo><mo>≤</mo></mtd></mtr><mtr><mtd><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false"> (</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="118">式中:<i>d</i><sub>min</sub>为最小视差值;<i>d</i><sub>max</sub>为最大视差值。<i>d</i><sup>*</sup>为<i>d</i><sub><i>p</i></sub>与<i>d</i><sub>min</sub>和<i>d</i><sub>max</sub>差值中的最小值, 满足</p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mi>min</mi><mo stretchy="false"> (</mo><mi>d</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>, </mo><mi>d</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>-</mo><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub><mo>⋅</mo><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup><mo>≤</mo><mi>n</mi><msub><mrow></mrow><mi>x</mi></msub><mo>⋅</mo><mi>r</mi><mo>±</mo><mi>n</mi><msub><mrow></mrow><mi>y</mi></msub><mo>⋅</mo><mi>r</mi><mo>≤</mo><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub><mo>⋅</mo><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">求解 (12) ～ (16) 式可得</p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula">
                        <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo>-</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub></mrow><mrow><mi>r</mi><mo>+</mo><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup></mrow></mfrac><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup><mo>≤</mo><mi>n</mi><msub><mrow></mrow><mi>x</mi></msub><mo>≤</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub></mrow><mi>r</mi></mfrac><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup><mo>, </mo><mspace width="0.25em" /><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mi>p</mi><mo>∈</mo><mi>Ι</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>-</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub></mrow><mi>r</mi></mfrac><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup><mo>≤</mo><mi>n</mi><msub><mrow></mrow><mi>x</mi></msub><mo>≤</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub></mrow><mrow><mi>r</mi><mo>+</mo><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup></mrow></mfrac><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup><mo>, </mo><mspace width="0.25em" /><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mi>p</mi><mo>∈</mo><mi>Ι</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="122">即获取左视图像素点<i>n</i><sub><i>x</i></sub>的上下边界。最后利用法向量在窗口内部的视差平面关系求解不等式[ (16) 式], 得到左视图<i>n</i><sub><i>y</i></sub>的取值范围为</p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>n</mi><msub><mrow></mrow><mi>y</mi></msub><mo>∈</mo><mfrac><mi>s</mi><mi>r</mi></mfrac><mo stretchy="false">[</mo><mo>-</mo><mn>1</mn><mo>, </mo><mn>1</mn><mo stretchy="false">]</mo><mo>, </mo><mspace width="0.25em" /><mtext>w</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>e</mtext><mspace width="0.25em" /><mspace width="0.25em" /><mi>s</mi><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>min</mi><mo stretchy="false">[</mo><mo stretchy="false">|</mo><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup><mo>+</mo><mi>n</mi><msub><mrow></mrow><mi>x</mi></msub><mi>r</mi><mo stretchy="false">|</mo><mo>, </mo><mo stretchy="false">|</mo><mo stretchy="false"> (</mo><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub><mo>+</mo><mi>n</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false">) </mo><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup><mo>+</mo><mi>n</mi><msub><mrow></mrow><mi>x</mi></msub><mi>r</mi><mo stretchy="false">|</mo><mo stretchy="false">]</mo><mo>, </mo><mspace width="0.25em" /><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mi>p</mi><mo>∈</mo><mi>Ι</mi><msub><mrow></mrow><mn>0</mn></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>min</mi><mo stretchy="false">[</mo><mo stretchy="false">|</mo><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup><mo>+</mo><mi>n</mi><msub><mrow></mrow><mi>x</mi></msub><mi>r</mi><mo stretchy="false">|</mo><mo>, </mo><mo stretchy="false">|</mo><mo stretchy="false"> (</mo><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub><mo>-</mo><mi>n</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false">) </mo><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup><mo>+</mo><mi>n</mi><msub><mrow></mrow><mi>x</mi></msub><mi>r</mi><mo stretchy="false">|</mo><mo stretchy="false">]</mo><mo>, </mo><mspace width="0.25em" /><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mi>p</mi><mo>∈</mo><mi>Ι</mi><msub><mrow></mrow><mn>1</mn></msub></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="124">式中:<i>I</i><sub>0</sub>为参考图;<i>I</i><sub>1</sub>为待匹配图;<i>s</i>为约束因子。通过上述约束条件得到的单位法向量 (<i>n</i><sub><i>x</i></sub>, <i>n</i><sub><i>y</i></sub>, <i>n</i><sub><i>z</i></sub>) 取值范围可在初始化3D标签时控制异常3D标签的产生, 只保留满足 (17) ～ (19) 式以及<i>n</i><sub><i>z</i></sub>&gt;0, <i>n</i><sub><i>x</i></sub>&lt;<i>n</i><sub><i>z</i></sub>的标签值。该方案对单位法向量进行多次采样, 直到满足约束条件, 增加了正确估计像素点3D平面参数的概率。</p>
                </div>
                <h4 class="anchor-tag" id="125" name="125"><b>2.3 基于像素类型的迭代优化</b></h4>
                <h4 class="anchor-tag" id="126" name="126">2.3.1 像素分类器</h4>
                <div class="p1">
                    <p id="127">像素分类器由Meanshift分类器<citation id="188" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、SNIC (simple non-iterative clustering) <citation id="189" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>分类器、LRC (left right consistency) 分类器以及纹理分类器4部分构成。其中Meanshift分类器利用颜色概率密度分布聚类颜色相似的像素点, 有很好的全局特性, 但颜色渐变小的区域分离效果不佳, 如图6 (b) 方框中局部分割标签图所示, SNIC分类器对分割区域具有更好的局部特性, 能够弥补Meanshift分割的单纯聚类方法缺点, 如图6 (c) 中方框区域所示。纹理分类器可表示为</p>
                </div>
                <div class="p1">
                    <p id="128" class="code-formula">
                        <mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>0</mn><mo>, </mo></mtd><mtd columnalign="left"><mi>n</mi><mo>≥</mo><mn>0</mn><mo>.</mo><mn>7</mn><mn>5</mn><mo>×</mo><mi>Ν</mi><mo>×</mo><mi>Ν</mi></mtd></mtr><mtr><mtd columnalign="left"><mn>1</mn><mo>, </mo></mtd><mtd columnalign="left"><mtext>e</mtext><mtext>l</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="129">式中:<i>n</i>为像素点个数;<i>N</i>为窗口半径。其原理是依据权重计算公式[ (8) 式]计算中心像素点与在半径为<i>N</i>的窗口内其他点的相似度大小, 后累计相似度值满足大于0.8的像素点个数<i>n</i>, 将大于设定阈值个数的像素点记作无纹理区域点, 反之为弱纹理点。图6 (d) 给出输入图像的像素纹理性质分类结果, 黑色代表有纹理点, 白色代表无纹理点。LRC分类器可表示为</p>
                </div>
                <div class="p1">
                    <p id="130" class="code-formula">
                        <mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>L</mtext><mtext>R</mtext><mtext>C</mtext><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>, </mo></mtd><mtd columnalign="left"><mspace width="0.25em" /><mi>D</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>-</mo><mi>D</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>, </mo></mtd><mtd columnalign="left"><mtext>e</mtext><mtext>l</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="131">式中:<i>D</i><sub>0</sub>为参考图视差图;<i>D</i><sub>1</sub>为待匹配图的视差图。 (21) 式用于分离视差图中的稳定点 (1) 和不稳定点 (0) 。以上几种像素点的分类方法所得结果可作为本文构建PatchMatch优化算法所需候选标签集生成和错误点视差优化的先验知识。</p>
                </div>
                <div class="area_img" id="132">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907030_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 多分类信息图。" src="Detail/GetImg?filename=images/GXXB201907030_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 多分类信息图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907030_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Multi-category graphs. </p>
                                <p class="img_note"> (a) Plastic; (b) Meanshift; (c) SNIC; (d) 纹理结构图</p>
                                <p class="img_note"> (a) Plastic; (b) Meanshift; (c) SNIC; (d) texture structure graph</p>

                </div>
                <h4 class="anchor-tag" id="133" name="133">2.3.2 基于像素分割类的标签生成机制</h4>
                <div class="p1">
                    <p id="134">利用2.3.1节的Meanshift和SNIC像素分类先验知识和PatchMatch算法优化原理, 改进了LocalExp算法的标签生成机制:</p>
                </div>
                <div class="p1">
                    <p id="135">1) 传播机制。在局部窗口的中心区域的对角线上选取1～2个像素点的3D标签, 加入更新候选标签集合。</p>
                </div>
                <div class="p1">
                    <p id="136">2) 局部最优标签机制。利用局部随机一致性算法 (LO-RANSAC) <citation id="190" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>拟合局部最优视差平面参数, 并将该参数加入到候选标签集合中。</p>
                </div>
                <div class="p1">
                    <p id="137">3) 基于像素分类的标签机制。在像素分类集合<i>S</i>中构造同名像素点集合, 并在其中随机选取一定数量的样本点用于LO-RANSAC拟合新标签, 并加入候选标签集合中。</p>
                </div>
                <div class="p1">
                    <p id="138">4) 视图传播机制。选取在其他视图下对应区域的有效视差标签加入到候选标签集合中。</p>
                </div>
                <div class="p1">
                    <p id="139">5) 随机扰动细化机制。对当前窗口内选取的一个标签量增加归一化的扰动量, 细化标签值, 直至扰动量足够小, 并将扰动标签加入候选标签集合中。</p>
                </div>
                <div class="p1">
                    <p id="140">图7是采用上述标签生成机制后, 基于PatchMatch和GC优化算法的求解视差的算法流程。针对每个图像细胞单元, 首先构建候选标签集合<i>S</i><sub>et</sub> (<i>α</i><sub><i>ij</i></sub>) , 选取集合中最小化能量函数的标签<i>L</i>′<sub><i>p</i></sub>作为更新像素点的标签值。最小化能量函数表达式为</p>
                </div>
                <div class="p1">
                    <p id="141" class="code-formula">
                        <mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><msup><mi>L</mi><mo>′</mo></msup></munder><mspace width="0.25em" /><mi>E</mi><mo stretchy="false">[</mo><msup><mi>L</mi><mo>′</mo></msup><mo stretchy="false">|</mo><msup><mi>L</mi><mo>′</mo></msup><msub><mrow></mrow><mi>p</mi></msub><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="142">式中:<i>l</i>为迭代次数;<i>L</i>′为最小化能量函数的标签集合;<i>E</i>为能量函数。</p>
                </div>
                <div class="p1">
                    <p id="143">图8对比了改进标签生成机制和原方法在一次迭代后的错误率, 结果表明, 相对于LocalExp, 本文方法的错误率更低。</p>
                </div>
                <div class="area_img" id="144">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907030_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 改进的局部扩张运动算法" src="Detail/GetImg?filename=images/GXXB201907030_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 改进的局部扩张运动算法  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907030_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Improved local expansion movement algorithm</p>

                </div>
                <div class="area_img" id="145">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907030_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 对比改进标签生成机制和原方法在一次迭代后的错误率" src="Detail/GetImg?filename=images/GXXB201907030_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 对比改进标签生成机制和原方法在一次迭代后的错误率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907030_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Comparison of error rates between improved generation mechanism and the original method in one iteration</p>

                </div>
                <h4 class="anchor-tag" id="146" name="146">2.3.3 基于像素分割、纹理及一致性分类的视差优化</h4>
                <div class="p1">
                    <p id="147">从图8对比实验结果可知, 虽然误匹配率相对于原算法有所降低, 但仍较高。为此, 采用一种基于像素分类信息的填充方法进一步优化视差。首先通过LRC分类器分离不稳定点, 其次, 依据纹理分类器对有纹理和无纹理区域的不稳定点分别进行处理。</p>
                </div>
                <div class="p1">
                    <p id="148">图9 (a) 表示位于纹理区域像素点的最近邻域填充方法。以图中标号5的不稳定更新为例, 分别向左和向右找到第1个位于像素分类器中同分割区域的稳定点。为了避免前景膨胀, 将稳定点中使错误点视差更小的标签值赋予该点。图像中无/弱纹理区域一般结构简单且对称, 为了有效扩大搜集范围并获得更多的稳定视差点, 如图9 (b) 所示, 采用横竖方向的交叉线法提取一定数目的稳定点作为权重中值滤波像素点集合, 用于优化错误点。该方法在当前错误像素点上下左右衍生一定范围以提取稳定点, 在各方向选取平均化个数的稳定点以防止选取过多的不同区域点, 通过累计稳定点个数自适应控制衍生搜索的臂长大小。最后采用权重中值选取标签用于更新无/弱纹理区域的不稳定点视差标签值。</p>
                </div>
                <div class="p1">
                    <p id="149">图10是所提方法和LocalExp算法经过4次迭代视差优化的视差变化过程图。图10 (a) 为LocalExp运行结果图, 其中方框1, 2, 3代表三个不同区域, 图中的黑色点代表视差匹配错误点。在迭代过程中, 方框1中部分点可在LocalExp算法迭代优化后得到正确的视差值, 对于方框2, 3中的像素点, 由于能量函数陷入局部最小值且传统的视差优化手段对于陷入局部最小值的区域无法有效进行滤波处理, 因此得到的视差标签图失效。图10 (b) 是所提方法的迭代过程图。从图中黑色点数目可知, 在初始状态下和迭代过程中, 错误区域误匹配率不断降低, 最终获得较准确的视差图。</p>
                </div>
                <div class="area_img" id="150">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907030_150.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 填充方法。" src="Detail/GetImg?filename=images/GXXB201907030_150.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 填充方法。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907030_150.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Filling methods. </p>
                                <p class="img_note"> (a) 纹理区域; (b) 弱纹理分区域。红色为稳定点, 黑色为不稳定点, 白色为剔除点</p>
                                <p class="img_note"> (a) Texture region; (b) texture-less region. Red regions represent stable points, black regions represent unstable points, and white regions represent culling points</p>

                </div>
                <div class="area_img" id="151">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907030_151.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 迭代优化过程。" src="Detail/GetImg?filename=images/GXXB201907030_151.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 迭代优化过程。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907030_151.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Iterative optimization process. </p>
                                <p class="img_note"> (a) LocalExp; (b) 提出的算法</p>
                                <p class="img_note"> (a) LocalExp; (b) proposed algorithm</p>

                </div>
                <h4 class="anchor-tag" id="152" name="152"><b>2.4 算法流程</b></h4>
                <div class="p1">
                    <p id="153">图11是本文算法流程图, 其中圆1代表算法初始化环节, 即完成IPMS算法中的视差标签初始化和构建像素分类器用于算法迭代优化与视差优化;圆2为算法迭代优化环节, 即对于每个局部窗口单元采用基于本文改进的标签生成机制的局部扩张运动算法来估计像素点视差;圆3代表满足迭代终止后的输出视差图, 左下角为算法中数据流动图例说明。</p>
                </div>
                <div class="p1">
                    <p id="154">图12给出本文算法优化流程说明。1) 算法初始化:行1～5分别是2.2～2.3节中阐述的多层窗口模型尺寸大小、利用meanshift、SNIC和纹理分类器<i>T</i> (<i>p</i>) 构建原始像素点分类集合<i>S</i>、迭代优化算法1中的propagation、optional和refinement更新次数以及其他的实验参数设定。2) 算法迭代循环:行8～9表示从窗口尺寸集合<i>G</i>={10×10, 20×20, 35×35}从小到大选取窗口大小执行2.2.1节中阐述的交叉窗口模型和2.2.2节的IPMS算法初始化 (该标签初始化算法只在首次迭代中执行, 其余迭代过程中视差标签从上次迭代结果选取) 。行11代表同组号窗口并行执行Algorithm 1-局部扩张运动算法迭代更新各自窗口内像素点的标签值, 其中<i>Δ</i>={<i>Δ</i>′<sub>d</sub>, <i>Δ</i>′<sub>n</sub>}为视差和法向量的扰动量, 用于细化视差标签<citation id="191" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。行14表示在一次迭代后, 采用2.3.3节提出的方法进行错误点视差填充;行16处表示在一次窗口尺寸迭代完成后, 缩小扰动范围<i>Δ</i>来改变视差标签细化精度。行17为迭代优化算法停止。行18执行后处理方法<citation id="192" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 以进一步提高视差精度。</p>
                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907030_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 本文算法流程图" src="Detail/GetImg?filename=images/GXXB201907030_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 本文算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907030_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Flow chart of proposed algorithm</p>

                </div>
                <div class="area_img" id="156">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907030_156.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 本文算法优化进程" src="Detail/GetImg?filename=images/GXXB201907030_156.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 本文算法优化进程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907030_156.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Optimization procedure of our algorithm</p>

                </div>
                <h3 id="157" name="157" class="anchor-tag">3 实验结果及分析</h3>
                <div class="p1">
                    <p id="158">实验环境为:Windows 7, 64位系统, Intel (R) Core (TM) i5-3210M, CPU主频为2.50 GHz, 4核, 内存为8 GB。为了验证本文算法对无/弱纹理区域的适用性与准确性以及纹理丰富区域的通用性, 实验数据采用Middlebury大学的立体视觉数据库中Midd2003 Datasets、Midd2006 Datasets。该数据集含有真实的视差数据, 可用于衡量算法的误差精度, 以及有效验证算法的正确性。本文实验参数为<i>ε</i>=0.01, <i>ε</i><sub>dis</sub>=0.8, <i>N</i>=5, <i>γ</i><sub>n</sub>=<i>γ</i><sub>cn</sub>=10, <i>τ</i><sub>col</sub>=0.2, <i>τ</i><sub>grad<i>x</i></sub>=<i>τ</i><sub>grad<i>y</i></sub>=0.7, <i>λ</i>=7, <i>α</i>=0.7, <i>R</i>=20, <i>β</i>=0.4。</p>
                </div>
                <div class="p1">
                    <p id="159">图13中列出基于PatchMatch优化的匹配算法<citation id="198" type="reference"><link href="18" rel="bibliography" /><link href="36" rel="bibliography" /><link href="52" rel="bibliography" /><link href="54" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">14</a>,<a class="sup">22</a>,<a class="sup">23</a>]</sup></citation>和本文提出的算法在Midd2006数据集中存在大片无纹理区域的4组测试图像Flowerpots、Bowling2、Midd1、Plastic的视差图可视化结果, 其中红色点代表视差错误大于1 pixel的点。从图13可知, 在采用没有结合CNN代价作为匹配代价的PatchMatch优化的全局匹配算法PMBP<citation id="193" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、LocalExp<citation id="194" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、SPM-BP<citation id="195" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>和GCLSL<citation id="196" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>得到的视差图中, 出现了大量错误点。得益于可靠的初始化标签值和基于像素分类的迭代计算和视差优化, 本文算法在任何一个测试图像上的实验效果都不存在较大区域的错误点。PMSC<citation id="197" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>由于利用CNN作为匹配代价进行计算, 获得了较好的实验效果, 但其需要通过数据集驱动来训练深度网络结构并计算初始匹配代价值。相比之下, 本文算法未采用CNN网格模型却取得与PMSC接近的视觉效果, 在弱纹理区域的结果甚至更优。</p>
                </div>
                <div class="area_img" id="160">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907030_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 几种全局PatchMatch算法的视差图 (匹配错误率大于1 pixel的点用红色显示) 。" src="Detail/GetImg?filename=images/GXXB201907030_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 几种全局PatchMatch算法的视差图 (匹配错误率大于1 pixel的点用红色显示) 。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907030_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 13 Disparities of some global PatchMatch algorithms (points with error matching rate greater than 1 pixel are shown in red) .</p>
                                <p class="img_note"> (a) Image; (b) PMBP; (c) SPM-BP; (d) GCLSL; (e) PMSC; (f) LocalExp; (g) proposed</p>
                                <p class="img_note"> (a) Image; (b) PMBP; (c) SPM-BP; (d) GCLSL; (e) PMSC; (f) LocalExp; (g) proposed</p>

                </div>
                <div class="p1">
                    <p id="161">表1中给出几种基于PatchMatch优化思想的全局匹配算法在Midd2006数据集典型图像中评价结果, 其数值为图像中非遮挡区域的像素点视差错误值大于1 pixel的误匹配率结果。从表1中可知, 本文算法取得了最低的平均误匹配率2.59%, 相对LocalExp算法的平均误匹配率 (11.66%) , 在所有测试的图像中, 只有Rocks图像的误匹配率高于LocalExp。本文算法在纹理较为丰富的测试图像Cloth1、Cloth4中获得的最低错误匹配率分别为0.59%和0.89%, 且在有无/弱纹理区域的Midd1、Plastic图像中, 取得最低的误匹配率, 在所有存在无纹理区域的图像中都取得较低的误匹配率。</p>
                </div>
                <div class="p1">
                    <p id="162">表2是本文算法与LocalExp在测试图像Midd2003中的实验测试结果对比, Rate代表视差图在非遮挡区域 (nonocc) 、全部区域 (all) 以及视差不连续区域 (disc) 的误匹配率。本文算法相对于LocalExp算法在Teddy、Cones上取得更低的误匹配率, 由原来的8.153%降低到本文的7.143%。在表2中的测试图像的视差图上框出视差细节对比区域, 两种方法中获取视差效果更好的用虚线方框圈出, 反之用实线方框圈出。对比方框内的细节可以发现, 本文方法在图像Teddy和Cones的disc区域上获得更好的边缘精度, 且在非遮挡区域获得更好的平滑视差效果。但对于第1行Tsukuba图像匹配效果较差, 因为其局部场景结构性过于复杂、图中存在阴影覆盖的影响以及图像视差变化范围小, 所以在三个区域的匹配错误率有小幅度上升。尽管本文算法在图像Tsukuba、Venus上的匹配精度降低, 但整体上仍取得与LocalExp接近的平均错误率, LocalExp和本文算法的平均错误率分别为5.97%和6.01%。</p>
                </div>
                <div class="area_img" id="163">
                    <p class="img_tit">表1 几种基于PatchMatch优化的匹配算法在Midd2006数据集中非遮挡区域的匹配结果 (阈值为1 pixel, 最好的实验结果用粗体显示)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Matching results of some PatchMatch based stereo algorithms in Midd2006 datasets with nonocc regions (threshold is 1 pixel, and the best results are shown in bold) </p>
                    <p class="img_note"></p>
                    <table id="163" border="1"><tr><td><br />Image</td><td>PMBP</td><td>SPMBP</td><td>GCLSL</td><td>PMSC</td><td>LocalExp</td><td>Proposed</td></tr><tr><td><br />Aloe</td><td>4.51</td><td>6.75</td><td>3.21</td><td>3.06</td><td>3.92</td><td>3.25</td></tr><tr><td><br />Baby1</td><td>4.10</td><td>3.27</td><td>2.21</td><td>1.98</td><td>2.74</td><td>1.34</td></tr><tr><td><br />Baby2</td><td>4.77</td><td>3.97</td><td>2.08</td><td>1.05</td><td>5.48</td><td>1.41</td></tr><tr><td><br />Baby3</td><td>4.77</td><td>3.92</td><td>3.07</td><td>3.12</td><td>6.56</td><td>2.73</td></tr><tr><td><br />Bowling1</td><td>14.10</td><td>12.10</td><td>4.14</td><td>2.06</td><td>5.37</td><td>2.46</td></tr><tr><td><br />Bowling2</td><td>4.64</td><td>5.27</td><td>2.19</td><td>1.45</td><td>6.44</td><td>1.99</td></tr><tr><td><br />Cloth1</td><td>1.68</td><td>1.17</td><td>0.71</td><td>0.60</td><td>0.78</td><td>0.59</td></tr><tr><td><br />Cloth4</td><td>3.10</td><td>2.20</td><td>1.75</td><td>1.87</td><td>0.99</td><td>0.98</td></tr><tr><td><br />Flowerpots</td><td>9.28</td><td>8.80</td><td>4.60</td><td>2.49</td><td>10.89</td><td>4.01</td></tr><tr><td><br />Lampshade1</td><td>13.50</td><td>8.67</td><td>12.60</td><td>1.50</td><td>5.96</td><td>2.14</td></tr><tr><td><br />Lampshade2</td><td>16.50</td><td>17.20</td><td>10.00</td><td>0.99</td><td>21.70</td><td>1.08</td></tr><tr><td><br />Midd1</td><td>37.40</td><td>37.40</td><td>34.90</td><td>12.80</td><td>30.80</td><td>6.07</td></tr><tr><td><br />Midd2</td><td>38.40</td><td>33.20</td><td>32.90</td><td>4.37</td><td>25.60</td><td>4.71</td></tr><tr><td><br />Monopoly</td><td>42.40</td><td>32.70</td><td>21.10</td><td>3.46</td><td>28.04</td><td>4.97</td></tr><tr><td><br />Plastic</td><td>44.80</td><td>35.20</td><td>43.90</td><td>4.40</td><td>40.10</td><td>3.34</td></tr><tr><td><br />Rocks1</td><td>4.15</td><td>2.60</td><td>2.19</td><td>1.80</td><td>1.60</td><td>2.34</td></tr><tr><td><br />Wood1</td><td>1.52</td><td>4.19</td><td>0.48</td><td>0.73</td><td>1.30</td><td>0.71</td></tr><tr><td><br />Average</td><td>14.68</td><td>12.85</td><td>10.70</td><td>2.80</td><td>11.66</td><td>2.59</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="164" name="164" class="anchor-tag">4 结  论</h3>
                <div class="p1">
                    <p id="165">提出一种基于像素类别优化的全局匹配算法。与多种相近算法比较, 本文算法在Midd2006图像数据集上取得了最低的平均错误匹配率, 相对于LocalExp算法降低了9.07%, 而且在存在大量无纹理区域图像Midd1、Plastic上取得了最低的误匹配率, 实验验证以下结论:本文算法能够有效改善图像中弱纹理和无纹理区域的误匹配问题。但本文算法还存在一些不足:1) 利用不同分类方法构建图像中像素点的类别信息, 其作为一种人工设计的方法, 对不同分割方法的准确性依赖较大;2) 由于迭代过程中加入视差优化算法, 运行比LocalExp耗时。在接下来的工作中, 将研究以深度学习和传统方法相结合的方式获取可靠的全局像素信息, 以提升匹配算法精确度并进一步缩短算法的运行时间。</p>
                </div>
                <div class="area_img" id="200">
                                            <p class="img_tit">
                                                表2 本文算法与LocalExp在Midd2003数据集的错误率对比 (阈值为0.5pixel) 
                                                    <br />
                                                Table 2 Comparison of error rates between proposed method and LocalExp on Midd2003datasets (threshold is 0.5pixel
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907030_20000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/GXXB201907030_20000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907030_20000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 本文算法与LocalExp在Midd2003数据集的错误率对比 (阈值为0.5pixel)" src="Detail/GetImg?filename=images/GXXB201907030_20000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201903028&amp;v=MDk5OTZxQnRHRnJDVVJMT2VaZVZ1RnluaFVidktJalhUYkxHNEg5ak1ySTlIYklRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Ma R H, Zhu F, Wu Q X, <i>et al</i>.Dense stereo matching based on image segmentation[J].Acta Optica Sinica, 2019, 39 (3) :0315001.马瑞浩, 朱枫, 吴清潇, 等.基于图像分割的稠密立体匹配算法[J].光学学报, 2019, 39 (3) :0315001.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Patchmatch stereo-stereo matching with slanted support windows">

                                <b>[2]</b> Bleyer M, Rhemann C, Rother C.PatchMatch stereo-stereo matching with slanted support windows[C]//Procedings of the British Machine Vision Conference 2011, August 29-September 1, 2011, Dundee.Durham:BMVA Press, 2011:14.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007574&amp;v=MjA0MzA0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lJVm9XYmhBPU5pZklZN0s3SHRqTnI0OUZaT3NJQ1hzOW9CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Barnes C, Shechtman E, Finkelstein A, <i>et al</i>.PatchMatch:a randomized correspondence algorithm for structural image editing[J].ACM Transactions on Graphics, 2009, 28 (3) :24.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=OPTIMIZATION BY DIRECT SEARCH AND SYSTEMATIC REDUCTION OF THE SIZE OF SEARCH REGION">

                                <b>[4]</b> Luus R, Jaakola T H I.Optimization by direct search and systematic reduction of the size of search region[J].AIChE Journal, 1973, 19 (4) :760-766.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14101300030946&amp;v=MTEwMDF5am1VYi9JSVZvV2JoQT1OajdCYXJLOEg5SE5ySTlGWk9nUEJYZy9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Besse F, Rother C, Fitzgibbon A, <i>et al</i>.PMBP:PatchMatch belief propagation for correspondence field estimation[J].International Journal of Computer Vision, 2014, 110 (1) :2-13.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Patchmatch filter:Efficient edge-aware filtering meets randomized search for fast correspondence field estimation">

                                <b>[6]</b> Lu J B, Yang H S, Min D B, <i>et al</i>.Patch match filter:efficient edge-aware filtering meets randomized search for fast correspondence field estimation[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2013, Portland, OR, USA.New York:IEEE, 2013:1854-1861.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;PatchMatch with Huber regularization for stereo matching&amp;quot;">

                                <b>[7]</b> Heise P, Klose S, Jensen B, <i>et al</i>.PM-huber:PatchMatch with huber regularization for stereo matching[C]//2013 IEEE International Conference on Computer Vision (ICCV) , December 1-8, 2013, Sydney, NSW, Australia.New York:IEEE, 2013:2360-2367.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PMSC:PatchMatch-based superpixel cut for accurate stereo matching">

                                <b>[8]</b> Li L C, Zhang S L, Yu X, <i>et al</i>.PMSC:PatchMatch-based superpixel cut for accurate stereo matching[J].IEEE Transactions on Circuits and Systems for Video Technology, 2018, 28 (3) :679-692.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stereo matching by training a convolutional neural network to compare image patches">

                                <b>[9]</b> Žbontar J, LeCun Y.Stereo matching by training a convolutional neural network to compare image patches[J].Journal of Machine Learning Research, 2016, 17:1-32.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision">

                                <b>[10]</b> Boykov Y, Kolmogorov V.An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2004, 26 (9) :1124-1137.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3Dcost aggregation with multiple minimum spanning trees for stereo matching">

                                <b>[11]</b> Li L C, Yu X, Zhang S L, <i>et al</i>.3D cost aggregation with multiple minimum spanning trees for stereo matching[J].Applied Optics, 2017, 56 (12) :3411-3420.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830890&amp;v=MDY4MDJJUFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RlNqbFVydktKRnc9Tmo3QmFyTzRIdEhPcDR4RmJP&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Felzenszwalb P F, Huttenlocher D P.Efficient graph-based image segmentation[J].International Journal of Computer Vision, 2004, 59 (2) :167-181.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A non-local cost aggregation method for stereo matc-hing">

                                <b>[13]</b> Yang Q X.A non-local cost aggregation method for stereo matching[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:1402-1409.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Continuous 3D label stereo matching using local expansion moves">

                                <b>[14]</b> Taniai T, Matsushita Y, Sato Y, <i>et al</i>.Continuous 3D label stereo matching using local expansion moves[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (11) :2725-2739.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Analysis of a Nonconvex Problem Related to Signal Selective Smoothing">

                                <b>[15]</b> Chipot M, March R, Rosati M, <i>et al</i>.Analysis of a nonconvex problem related to signal selective smoothing[J].Mathematical Models and Methods in Applied Sciences, 1997, 7 (3) :313-328.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Guided Image Filtering">

                                <b>[16]</b> He K M, Sun J, Tang X O.Guided image filtering[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (6) :1397-1409.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=In defense of 3D-label stereo">

                                <b>[17]</b> Olsson C, Ulén J, Boykov Y.In defense of 3D-label stereo[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2013, Portland, OR, USA.New York:IEEE, 2013:1730-1737.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Constrained optimization for plane-based stereo">

                                <b>[18]</b> Ahmed S, Hansard M, Cavallaro A.Constrained optimization for plane-based stereo[J].IEEE Transactions on Image Processing, 2018, 27 (8) :3870-3882.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mean shift: A robust approach toward feature space analysis">

                                <b>[19]</b> Comaniciu D, Meer P.Mean shift:a robust approach toward feature space analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002, 24 (5) :603-619.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Superpixels and polygons using simple non-iterative clustering">

                                <b>[20]</b> Achanta R, Süsstrunk S.Superpixels and polygons using simple non-iterative clustering[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4895-4904.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locally optimized RANSAC">

                                <b>[21]</b> Chum O, Matas J, Kittler J.Locally optimized RANSAC[M]//Michaelis B, Krell G.Lecture notes in computer science.Berlin, Heidelberg:Springer, 2003, 2781:236-243.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spm-bp: Sped-up patchmatchbelief propagation for continuous mrfs">

                                <b>[22]</b> Li Y, Min D B, Brown M S, <i>et al</i>.SPM-BP:sped-up PatchMatch belief propagation for continuous MRFs[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:4006-4014.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Graph cut based continuous stereo matching using locally shared labels">

                                <b>[23]</b> Taniai T, Matsushita Y, Naemura T.Graph cut based continuous stereo matching using locally shared labels[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:1613-1620.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201907030" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201907030&amp;v=MDg1OTNJOUdaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5oVWJ2TElqWFRiTEc0SDlqTXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

