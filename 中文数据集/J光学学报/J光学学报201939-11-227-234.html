

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133066001065000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dGXXB201911028%26RESULT%3d1%26SIGN%3d5Et4ngOcyImNX1gAsZ2x2xeD0oI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201911028&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201911028&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201911028&amp;v=Mjg5MzFOcm85SGJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5dmdVYi9BSWpYVGJMRzRIOWo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#64" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="2 相关工作 ">2 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#73" data-title="3 基于3DCNN的立体匹配算法 ">3 基于3DCNN的立体匹配算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#75" data-title="&lt;b&gt;3.1 网络结构的改进&lt;/b&gt;"><b>3.1 网络结构的改进</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;3.2 损失函数的改进&lt;/b&gt;"><b>3.2 损失函数的改进</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#99" data-title="4 实  验 ">4 实  验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#101" data-title="&lt;b&gt;4.1 实验细节&lt;/b&gt;"><b>4.1 实验细节</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;4.2 超参数分析&lt;/b&gt;"><b>4.2 超参数分析</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;4.3 几种典型算法的性能对比&lt;/b&gt;"><b>4.3 几种典型算法的性能对比</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#126" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="图1 算法的网络结构">图1 算法的网络结构</a></li>
                                                <li><a href="#83" data-title="图2 在视差维度上对损失进行采样的可视化描述。">图2 在视差维度上对损失进行采样的可视化描述。</a></li>
                                                <li><a href="#110" data-title="表1 不同&lt;i&gt;S&lt;/i&gt;值设置下算法的性能评价(&lt;i&gt;C&lt;/i&gt;=1)">表1 不同<i>S</i>值设置下算法的性能评价(<i>C</i>=1)</a></li>
                                                <li><a href="#113" data-title="表2 不同&lt;i&gt;C&lt;/i&gt;值设置下算法的性能评价">表2 不同<i>C</i>值设置下算法的性能评价</a></li>
                                                <li><a href="#116" data-title="表3 不同模型设置下算法的性能评价">表3 不同模型设置下算法的性能评价</a></li>
                                                <li><a href="#119" data-title="表4 在K-val上不同参数设置下算法的性能评价">表4 在K-val上不同参数设置下算法的性能评价</a></li>
                                                <li><a href="#123" data-title="表5 K15-test上不同算法的性能评价">表5 K15-test上不同算法的性能评价</a></li>
                                                <li><a href="#124" data-title="表6 K12-test上不同算法的性能评价">表6 K12-test上不同算法的性能评价</a></li>
                                                <li><a href="#125" data-title="图3 算法预测的视差结果。">图3 算法预测的视差结果。</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" title=" Nguyen V D,Nguyen D D,Lee S J,&lt;i&gt;et al&lt;/i&gt;.Local density encoding for robust stereo matching[J].IEEE Transactions on Circuits and Systems for Video Technology,2014,24(12):2049-2062." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Local density encoding for robust stereo matching">
                                        <b>[1]</b>
                                         Nguyen V D,Nguyen D D,Lee S J,&lt;i&gt;et al&lt;/i&gt;.Local density encoding for robust stereo matching[J].IEEE Transactions on Circuits and Systems for Video Technology,2014,24(12):2049-2062.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" Heise P,Jensen B,Klose S,&lt;i&gt;et al&lt;/i&gt;.Fast dense stereo correspondences by binary locality sensitive hashing[C]//2015 IEEE International Conference on Robotics and Automation (ICRA),May 26-30,2015,Seattle,WA,USA.New York:IEEE,2015:105-110." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast dense stereo correspondences by binary locality sensitive hashing">
                                        <b>[2]</b>
                                         Heise P,Jensen B,Klose S,&lt;i&gt;et al&lt;/i&gt;.Fast dense stereo correspondences by binary locality sensitive hashing[C]//2015 IEEE International Conference on Robotics and Automation (ICRA),May 26-30,2015,Seattle,WA,USA.New York:IEEE,2015:105-110.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" title=" Taniai T,Matsushita Y,Sato Y,&lt;i&gt;et al&lt;/i&gt;.Continuous 3D label stereo matching using local expansion moves[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(11):2725-2739." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Continuous 3D label stereo matching using local expansion moves">
                                        <b>[3]</b>
                                         Taniai T,Matsushita Y,Sato Y,&lt;i&gt;et al&lt;/i&gt;.Continuous 3D label stereo matching using local expansion moves[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(11):2725-2739.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" title=" Hamzah R A,Ibrahim H,Hassan A H A.Stereo matching algorithm based on per pixel difference adjustment,iterative guided filter and graph segmentation[J].Journal of Visual Communication and Image Representation,2017,42:145-160." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESDA94C517D724BE25F6DF4053D5A87285&amp;v=Mjc0ODhRbGZDcGJRMzVkbGh4NzI5d0tBPU5pZk9mY2ZKRjlXL3FvNUNFT3dOQ0E1TXpSTmw3RXNMVEgvbnIyWXdDTHFUUjdLYUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPRw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Hamzah R A,Ibrahim H,Hassan A H A.Stereo matching algorithm based on per pixel difference adjustment,iterative guided filter and graph segmentation[J].Journal of Visual Communication and Image Representation,2017,42:145-160.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" title=" He K M,Zhang X Y,Ren S Q,&lt;i&gt;et al&lt;/i&gt;.Identity mappings in deep residual networks[M]//Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.European conference on computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9908:630-645." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identity Mappings in Deep Residual Networks">
                                        <b>[5]</b>
                                         He K M,Zhang X Y,Ren S Q,&lt;i&gt;et al&lt;/i&gt;.Identity mappings in deep residual networks[M]//Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.European conference on computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9908:630-645.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                     Ren S Q,He K M,Girshick R,&lt;i&gt;et al&lt;/i&gt;.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(6):1137-1149.</a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" Shelhamer E,Long J,Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(4):640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[7]</b>
                                         Shelhamer E,Long J,Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(4):640-651.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" title=" Xiao J S,Tian H,Zou W T,&lt;i&gt;et al&lt;/i&gt;.Stereo matching based on convolutional neural network[J].Acta Optica Sinica,2018,38(8):0815017.肖进胜,田红,邹文涛,等.基于深度卷积神经网络的双目立体视觉匹配算法[J].光学学报,2018,38(8):0815017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201808018&amp;v=MTk0MDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5dmdVYi9BSWpYVGJMRzRIOW5NcDQ5RWJJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Xiao J S,Tian H,Zou W T,&lt;i&gt;et al&lt;/i&gt;.Stereo matching based on convolutional neural network[J].Acta Optica Sinica,2018,38(8):0815017.肖进胜,田红,邹文涛,等.基于深度卷积神经网络的双目立体视觉匹配算法[J].光学学报,2018,38(8):0815017.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" Žbontar J,LeCun Y.Computing the stereo matching cost with a convolutional neural network[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:1592-1599." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Computing the stereo matching cost with a convolutional neural network">
                                        <b>[9]</b>
                                         Žbontar J,LeCun Y.Computing the stereo matching cost with a convolutional neural network[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:1592-1599.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" title=" Žbontar J,LeCun Y.Stereo matching by training a convolutional neural network to compare image patches[J].Journal of Machine Learning Research,2016,17(1):2287-2318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stereo matching by training a convolutional neural network to compare image patches">
                                        <b>[10]</b>
                                         Žbontar J,LeCun Y.Stereo matching by training a convolutional neural network to compare image patches[J].Journal of Machine Learning Research,2016,17(1):2287-2318.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" title=" Mayer N,Ilg E,H&#228;usser P,&lt;i&gt;et al&lt;/i&gt;.A large dataset to train convolutional networks for disparity,optical flow,and scene flow estimation[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:4040-4048." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Large Dataset to Train Convolutional Networks for Disparity,Optical Flow,and Scene Flow Estimation">
                                        <b>[11]</b>
                                         Mayer N,Ilg E,H&#228;usser P,&lt;i&gt;et al&lt;/i&gt;.A large dataset to train convolutional networks for disparity,optical flow,and scene flow estimation[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:4040-4048.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" title=" Pang J H,Sun W X,Ren J S,&lt;i&gt;et al&lt;/i&gt;.Cascade residual learning:a two-stage convolutional neural network for stereo matching[C]//2017 IEEE International Conference on Computer Vision Workshops (ICCVW),October 22-29,2017,Venice,Italy.New York:IEEE,2017:878-886." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cascade Residual Learning A Two-Stage Convolutional Neural Network for Stereo Matching">
                                        <b>[12]</b>
                                         Pang J H,Sun W X,Ren J S,&lt;i&gt;et al&lt;/i&gt;.Cascade residual learning:a two-stage convolutional neural network for stereo matching[C]//2017 IEEE International Conference on Computer Vision Workshops (ICCVW),October 22-29,2017,Venice,Italy.New York:IEEE,2017:878-886.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" title=" Kendall A,Martirosyan H,Dasgupta S,&lt;i&gt;et al&lt;/i&gt;.End-to-end learning of geometry and context for deep stereo regression[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice,Italy.New York:IEEE,2017:66-75." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Endto-end learning of geometry and context for deep stereo regression">
                                        <b>[13]</b>
                                         Kendall A,Martirosyan H,Dasgupta S,&lt;i&gt;et al&lt;/i&gt;.End-to-end learning of geometry and context for deep stereo regression[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice,Italy.New York:IEEE,2017:66-75.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" Chang J R,Chen Y S.Pyramid stereo matching network[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:5410-5418." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Pyramid stereo matching network,&amp;quot;">
                                        <b>[14]</b>
                                         Chang J R,Chen Y S.Pyramid stereo matching network[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:5410-5418.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" Scharstein D,Szeliski R.A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J].International Journal of Computer Vision,2002,47(1/2/3):7-42." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830724&amp;v=MzIxNjZveGNNSDdSN3FlYnVkdEZTbmxVTHJLSUZZPU5qN0Jhck80SHRIT3A0eEZZK2tMWTNrNXpCZGg0ajk5U1hxUnJ4&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Scharstein D,Szeliski R.A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J].International Journal of Computer Vision,2002,47(1/2/3):7-42.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" title=" Scharstein D,Hirschm&#252;ller H,Kitajima Y,&lt;i&gt;et al&lt;/i&gt;.High-resolution stereo datasets with subpixel-accurate ground truth[M]//Jiang X,Hornegger J,Koch R.German conference on pattern recognition.GCPR 2014.Lecture notes in computer science.Cham:Springer,2014,8753:31-42." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-resolution stereo datasets with subpixel-accurate ground truth">
                                        <b>[16]</b>
                                         Scharstein D,Hirschm&#252;ller H,Kitajima Y,&lt;i&gt;et al&lt;/i&gt;.High-resolution stereo datasets with subpixel-accurate ground truth[M]//Jiang X,Hornegger J,Koch R.German conference on pattern recognition.GCPR 2014.Lecture notes in computer science.Cham:Springer,2014,8753:31-42.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" title=" Geiger A,Lenz P,Urtasun R.Are we ready for autonomous driving?The KITTI vision benchmark suite[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition,June 16-21,2012,Providence,RI,USA.New York:IEEE,2012:3354-3361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Are we ready for autonomous driving?The KITTI vision benchmark suite,&amp;quot;">
                                        <b>[17]</b>
                                         Geiger A,Lenz P,Urtasun R.Are we ready for autonomous driving?The KITTI vision benchmark suite[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition,June 16-21,2012,Providence,RI,USA.New York:IEEE,2012:3354-3361.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" Menze M,Geiger A.Object scene flow for autonomous vehicles[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:3061-3070." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object scene flow for autonomous vehicles">
                                        <b>[18]</b>
                                         Menze M,Geiger A.Object scene flow for autonomous vehicles[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:3061-3070.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_19" title=" Luo W J,Schwing A G,Urtasun R.Efficient deep learning for stereo matching[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:5695-5703." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient Deep Learning for Stereo Matching">
                                        <b>[19]</b>
                                         Luo W J,Schwing A G,Urtasun R.Efficient deep learning for stereo matching[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:5695-5703.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_20" title=" Liang Z F,Feng Y L,Guo Y L,&lt;i&gt;et al&lt;/i&gt;.Learning for disparity estimation through feature constancy[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:2811-2820." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning for disparity estimation through feature constancy">
                                        <b>[20]</b>
                                         Liang Z F,Feng Y L,Guo Y L,&lt;i&gt;et al&lt;/i&gt;.Learning for disparity estimation through feature constancy[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:2811-2820.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_21" title=" Jie Z Q,Wang P F,Ling Y G,&lt;i&gt;et al&lt;/i&gt;.Left-right comparative recurrent model for stereo matching[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:3838-3846." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Left-right comparative recurrent model for stereo matching">
                                        <b>[21]</b>
                                         Jie Z Q,Wang P F,Ling Y G,&lt;i&gt;et al&lt;/i&gt;.Left-right comparative recurrent model for stereo matching[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:3838-3846.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_22" title=" Yu L D,Wang Y C,Wu Y W,&lt;i&gt;et al&lt;/i&gt;.Deep stereo matching with explicit cost aggregation sub-architecture[J/OL].(2018-01-12)[2019-04-15].http://cn.arxiv.org/abs/1801.04065." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep stereo matching with explicit cost aggregation sub-architecture">
                                        <b>[22]</b>
                                         Yu L D,Wang Y C,Wu Y W,&lt;i&gt;et al&lt;/i&gt;.Deep stereo matching with explicit cost aggregation sub-architecture[J/OL].(2018-01-12)[2019-04-15].http://cn.arxiv.org/abs/1801.04065.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_23" title=" Smolyanskiy N,Kamenev A,Birchfield S.On the importance of stereo for accurate depth estimation:an efficient semi-supervised deep neural network approach[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),June 18-22,2018,Salt Lake City,UT,USA.New York:IEEE,2018:1120-1128." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the importance of stereo for accurate depth estimation:an efficient semi-supervised deep neural network approach">
                                        <b>[23]</b>
                                         Smolyanskiy N,Kamenev A,Birchfield S.On the importance of stereo for accurate depth estimation:an efficient semi-supervised deep neural network approach[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),June 18-22,2018,Salt Lake City,UT,USA.New York:IEEE,2018:1120-1128.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_24" title=" Zhong Y R,Dai Y C,Li H D.Self-supervised learning for stereo matching with self-improving ability[J/OL].(2017-09-04)[2019-04-15].http://cn.arxiv.org/abs/1709.00930." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Self-supervised learning for stereo matching with self-improving ability">
                                        <b>[24]</b>
                                         Zhong Y R,Dai Y C,Li H D.Self-supervised learning for stereo matching with self-improving ability[J/OL].(2017-09-04)[2019-04-15].http://cn.arxiv.org/abs/1709.00930.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_25" title=" Wang Y F,Wang H W,Wu C,&lt;i&gt;et al&lt;/i&gt;.Self-supervised stereo matching algorithm based on common view[J].Acta Optica Sinica,2019,39(2):0215004.王玉锋,王宏伟,吴晨,等.基于共同视域的自监督立体匹配算法[J].光学学报,2019,39(2):0215004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902035&amp;v=MjcwNDhaZVZ2Rnl2Z1ViL0FJalhUYkxHNEg5ak1yWTlHWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         Wang Y F,Wang H W,Wu C,&lt;i&gt;et al&lt;/i&gt;.Self-supervised stereo matching algorithm based on common view[J].Acta Optica Sinica,2019,39(2):0215004.王玉锋,王宏伟,吴晨,等.基于共同视域的自监督立体匹配算法[J].光学学报,2019,39(2):0215004.
                                    </a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_26" >
                                        <b>[26]</b>
                                     Girshick R.Fast R-CNN[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:1440-1448.</a>
                                </li>
                                <li id="62">


                                    <a id="bibliography_27" title=" Kingma D P,Ba J.Adam:a method for stochastic optimization[J/OL].(2017-01-30)[2019-04-15].http://cn.arxiv.org/abs/1412.6980." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">
                                        <b>[27]</b>
                                         Kingma D P,Ba J.Adam:a method for stochastic optimization[J/OL].(2017-01-30)[2019-04-15].http://cn.arxiv.org/abs/1412.6980.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-07-17 08:56</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(11),227-234 DOI:10.3788/AOS201939.1115001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于三维卷积神经网络的立体匹配算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E7%8E%89%E9%94%8B&amp;code=41275592&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王玉锋</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%AE%8F%E4%BC%9F&amp;code=20899251&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王宏伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E5%85%89&amp;code=22174856&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">于光</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%98%8E%E6%9D%83&amp;code=21934607&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨明权</a>
                                <a href="javascript:;">袁昱纬</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%85%A8%E5%90%89%E6%88%90&amp;code=41275594&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全吉成</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%B7%E5%86%9B%E8%88%AA%E7%A9%BA%E5%A4%A7%E5%AD%A6&amp;code=1701745&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">海军航空大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A9%BA%E5%86%9B%E8%88%AA%E7%A9%BA%E5%A4%A7%E5%AD%A6&amp;code=1020414&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空军航空大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6&amp;code=0199248&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息工程大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=491977%E9%83%A8%E9%98%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">491977部队</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>对于基于深度学习的立体匹配而言,模型的网络结构对算法精度的影响很大,而算法运行效率也是实际应用中需要考虑的重要因素。提出一种在视差维度上使用稀疏损失体进行立体匹配的方法。采用宽步长平移右视角特征图构建稀疏的三维损失体,使三维卷积模块所需的显存和计算资源均降低数倍。采用多类别输出的方式对匹配损失在视差维度上进行非线性上采样,并结合两种损失函数训练模型,在保证运行效率的同时提高算法精度。在KITTI测试集上,与基准算法相比,所提算法不仅提高了精度,而且运行时间缩短了约40%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">立体匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E7%9B%AE%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双目视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    全吉成,E-mail:jicheng_quan@126.com;
                                </span>
                                <span>
                                    王宏伟;E-mail:alex19820911@126.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-05</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家杰出青年科学基金(61301233);</span>
                    </p>
            </div>
                    <h1><b>Stereo Matching Algorithm Based on Three-Dimensional Convolutional Neural Network</b></h1>
                    <h2>
                    <span>Wang Yufeng</span>
                    <span>Wang Hongwei</span>
                    <span>Yu Guang</span>
                    <span>Yang Mingquan</span>
                    <span>Yuan Yuwei</span>
                    <span>Quan Jicheng</span>
            </h2>
                    <h2>
                    <span>Naval Aviation University</span>
                    <span>Aviation University of Air Force</span>
                    <span>Information Engineering University</span>
                    <span>The 91977 Troops</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>For a stereo matching method based on deep learning, network architecture is critical to ensuring the algorithm′s accuracy; efficiency is also an important factor to consider in practical applications. A stereo matching method with a sparse cost volume in the disparity dimension is proposed herein. The three-dimensional sparse cost volume is created by shifting right-view features with a large step to substantially reduce the memory and computational resources in a three-dimensional convolution module. The matching cost is nonlinearly sampled via multiclass output in the disparity dimension, and the model is trained by merging two types of loss functions, such that the proposed method′s accuracy is improved without any notable reduction in efficiency. The proposed algorithm reduces running time by about 40% while improving accuracy compared with the benchmark algorithm on the KITTI test dataset.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=stereo%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">stereo matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=binocular%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">binocular vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-05-05</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="64" name="64" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="65">立体匹配是计算机视觉中的一个基础性研究,广泛应用于三维重构、无人驾驶及机器人导航等多种领域。传统的立体匹配算法多围绕损失计算和视差优化进行研究:一方面,设计良好的度量函数来计算匹配损失<citation id="131" type="reference"><link href="10" rel="bibliography" /><link href="12" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>;另一方面,使用局部或全局的方法为每个像素分配视差值<citation id="132" type="reference"><link href="14" rel="bibliography" /><link href="16" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。这些算法均采用人工设计的浅函数,对于病态区域(如纹理少的区域等)往往不能得到正确的结果。</p>
                </div>
                <div class="p1">
                    <p id="66">近年来,深度学习表现出了强大的图像理解能力,在目标分类、目标检测和语义分割等任务中具有优异的性能<citation id="133" type="reference"><link href="18" rel="bibliography" /><link href="20" rel="bibliography" /><link href="22" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>,基于深度学习的立体匹配算法也越来越受关注<citation id="134" type="reference"><link href="24" rel="bibliography" /><link href="26" rel="bibliography" /><link href="28" rel="bibliography" /><link href="30" rel="bibliography" /><link href="32" rel="bibliography" /><link href="34" rel="bibliography" /><link href="36" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>。卷积神经网络(CNN)可从图像中提取稳健特征,很适于学习图像块之间的相似度<citation id="135" type="reference"><link href="24" rel="bibliography" /><link href="26" rel="bibliography" /><link href="28" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>。在匹配性模糊的区域,为进一步提高模型的全局优化能力,端到端的立体匹配方法<citation id="136" type="reference"><link href="30" rel="bibliography" /><link href="32" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>将视差预测的全过程整合到CNN模型中。然而,这种算法多采用一种沿视差方向的一维相关算法,损失了在视差维度的特征。在立体匹配中引入三维卷积神经网络(3DCNN)<citation id="137" type="reference"><link href="34" rel="bibliography" /><link href="36" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>,使模型可以在3个维度上去理解全局语义信息,从而能更好地理解场景的上下文信息。</p>
                </div>
                <div class="p1">
                    <p id="67">在立体匹配算法中引入3DCNN,对匹配过程建模效果很好,但也使显存开销和计算量增加了数十倍。这些资源负载主要来自3DCNN,因此,所提算法主要从降低这些负载入手,主要贡献包含3个方面:1)构建视差维度上稀疏损失体作为3DCNN的输入,降低显存开销和计算量;2)在单个平移步长内增加3DCNN的输出类别数,在视差维度上对匹配损失进行更精细的采样;3)在最大概率邻域内进行视差回归,并结合亚像素的交叉熵损失和平滑的L1损失进行训练,使模型不仅能对视差图进行更准确的亚像素估计,而且在直接扩展视差范围时对精度影响较小。</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag">2 相关工作</h3>
                <div class="p1">
                    <p id="69">典型的立体匹配算法通常包含4个步骤<citation id="138" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>:匹配损失计算、损失聚合、初始视差计算和视差细化。大量公开的提供高质量视差真值的立体匹配数据集<citation id="140" type="reference"><link href="30" rel="bibliography" /><link href="40" rel="bibliography" /><link href="42" rel="bibliography" /><link href="44" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>,不仅为各种立体匹配算法提供了定量的对比,而且为深度学习以各种方式改进立体匹配算法提供了可能。最初,CNN被用于计算图像块之间的相似度,Žbontar等<citation id="141" type="reference"><link href="26" rel="bibliography" /><link href="28" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>训练了一个孪生网络来提取稳健的图像特征和计算匹配损失,与传统方法相比其性能取得了较大的提升。Luo等<citation id="139" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>将视差预测转化为多标签分类任务,使用点积来计算相似度打分,显著提高了算法速度。</p>
                </div>
                <div class="p1">
                    <p id="70">端到端的学习在算法的整体优化上往往能获得更好的性能,Mayer等<citation id="142" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出一种“编码-解码”网络结构,并创建了一个大型的合成数据集来进行视差的端到端学习。以此视差预测网络为基础,Pang等<citation id="143" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>通过级联另一个网络进行视差微调以提高精度。Liang等<citation id="144" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>将CNN与贝叶斯推理相结合,通过学习先验和后验的不变特征来进行视差预测和微调。Jie等<citation id="145" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>引入循环神经网络,通过不断对比左右视角来逐渐改善视差预测结果。</p>
                </div>
                <div class="p1">
                    <p id="71">为更好地利用语义的上下文信息,Kendall等<citation id="146" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>使用3DCNN学习进行匹配损失的计算,再以可差分的回归函数进行亚像素的视差预测,使模型可以从3个维度上理解全局语义信息。在此基础上,Yu等<citation id="147" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>引入一个明确的损失聚合子模块进行匹配损失优化。Smolyanskiy等<citation id="148" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>根据双目图像的几何关系构建半监督的损失函数,以稀疏的视差真值来提供密集的误差反馈信号。Chang等<citation id="149" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>采用平均池化模块进行多尺度特征融合,提高了特征抽象能力,并以深层监督的方式学习匹配损失计算。这些算法增加了新的视差维度,显存开销和计算量增加了数十倍,且场景变化需要扩展视差范围时仍需对模型进行重新训练或微调。</p>
                </div>
                <div class="p1">
                    <p id="72">虽然现有研究的重点是有监督的立体匹配算法,但无监督的立体匹配算法<citation id="150" type="reference"><link href="56" rel="bibliography" /><link href="58" rel="bibliography" /><sup>[<a class="sup">24</a>,<a class="sup">25</a>]</sup></citation>也是非常值得关注的内容,这种算法在训练过程中不需要大范围高质量的视差数据,根据左右图像的几何约束关系就能学习如何预测视差,从而大大减少采集训练数据所需的工作量。</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag">3 基于3DCNN的立体匹配算法</h3>
                <div class="p1">
                    <p id="74">立体匹配是从双目图像中找到同名对应点,输出密集视差图的过程,可以将其转化为端到端的值回归任务,以双目图像为输入,直接输出预测的视差图。所提算法的网络结构主要由4个部分组成,分别为特征提取、空间金字塔特征融合、匹配损失计算和视差回归,如图1所示。基本流程如下:1)使用CNN分别对左右视角图像进行特征提取,并融合多尺度特征;2)连接左视角特征和平移的右视角特征,构建视差维度上稀疏的损失体,再使用3DCNN学习并根据几何上下文信息计算匹配损失;3)重采样损失体到原始图片尺寸,用Softmax函数将损失值转化为视差概率分布,并通过视差回归函数输出亚像素的预测视差。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>3.1 网络结构的改进</b></h4>
                <div class="p1">
                    <p id="76">连接左视角特征和平移的右视角特征构建损失体的过程如图1下方的虚线框所示,其中,矩形表示左右视角特征,矩形的叠加表示对特征进行连接。通常,平移右视角特征图的步长(记为<i>S</i>)为1,如图1下方虚线框的上半部分所示,在最大视差(记为<i>D</i>)范围内构建损失体,其数据量变为特征图的<i>D</i>倍,单个三维卷积层的计算量将是单个二维卷积层的3<i>D</i>倍(卷积核大小为3)。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911028_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 算法的网络结构" src="Detail/GetImg?filename=images/GXXB201911028_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 算法的网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911028_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Architecture overview of proposed method</p>

                </div>
                <div class="p1">
                    <p id="79">对图像特征在视差维度上进行小范围(<i>S</i>=1)的平移和堆叠,显然存在大量的信息冗余。采用相对较宽的平移步长(<i>S</i>&gt;1),在视差维度稀疏的损失体上计算匹配损失,相对于平移步长为1的情况,三维卷积模块的显存开销和计算量均能降低为约1/<i>S</i>。与<i>S</i>=1时相比,三维损失体在视差维度上只有原来的1/<i>S</i>,而每个三维卷积层的特征维度不变,其输入输出也只有原来的1/<i>S</i>,因此,所需要的显存和计算资源均为原来的1/<i>S</i>。</p>
                </div>
                <div class="p1">
                    <p id="80">卷积神经网络往往可以综合一定像素范围内的信息,同时也能对其进行分解。平移步长的增加使模型在视差维度上的细化能力变弱。为减弱这种影响,在每个平移步长内,对匹配损失进行多类别预测(其数目记为<i>C</i>)。这相当于将匹配损失进行了非线性的上采样,使模型可以学习视差概率分布的细化函数,从而改善算法精度。由于只在匹配损失的输出层增加了权重,因此其对算法运行效率的影响较小。</p>
                </div>
                <div class="p1">
                    <p id="81">不同的<i>S</i>值和<i>C</i>值决定了匹配损失在视差维度上的采样数,如图2所示。图2(a)为基准方法的情况,在每个视差值都对应一个采样点;图2(b)为宽步长平移的情况,每个平移步长产生一个采样点,虚线部分是由于宽步长平移而减少的采样点;图2(c)为宽步长平移、多类别预测的情况,宽步长平移造成了采样点的减少,通过多类别预测进行采样点的补充,相当于在视差维度上对匹配损失进行了非线性上采样。</p>
                </div>
                <div class="p1">
                    <p id="82">设置的<i>S</i>值越大,效率越高,而精度越低,因此需要权衡精度和效率,选择合适的<i>S</i>值。设置的<i>C</i>值越大,非线性上采样的因子就越大,潜在的模型细化能力就越强,但也会增加模型训练的难度,因此<i>C</i>值的选择不仅会影响模型的收敛精度,而且会影响模型的收敛速度。</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911028_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 在视差维度上对损失进行采样的可视化描述。" src="Detail/GetImg?filename=images/GXXB201911028_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 在视差维度上对损失进行采样的可视化描述。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911028_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Graphical depiction of sampling cost in disparity dimension. </p>
                                <p class="img_note">(a) S=1, C=1;(b) S=2, C=1;(c) S=2, C=4</p>
                                <p class="img_note">(a) S=1, C=1; (b) S=2, C=1; (c) S=2, C=4</p>

                </div>
                <h4 class="anchor-tag" id="85" name="85"><b>3.2 损失函数的改进</b></h4>
                <div class="p1">
                    <p id="86">文献<citation id="151" type="reference">[<a class="sup">14</a>]</citation>中使用可差分的视差回归模块预测视差值,结合平滑的L1(绝对值误差)损失<citation id="152" type="reference"><link href="60" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>对模型进行训练。可差分的视差回归模块使用Softmax函数<i>σ</i>(·),根据匹配损失<i>C</i>计算视差的概率分布,再以加权的方式对视差值进行亚像素估计,其表达式为</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>d</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>A</mtext></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mtext>d</mtext></msub></mrow></munderover><mi>d</mi></mstyle><msub><mrow></mrow><mi>n</mi></msub><mi>σ</mi><mo stretchy="false">(</mo><mo>-</mo><mi>C</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">式中:<mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>d</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>A</mtext></msub></mrow></math></mathml>为视差值的亚像素估计;<i>d</i><sub><i>n</i></sub>=<i>D</i><sub>max</sub><i>n</i>/<i>N</i><sub>d</sub>,<i>D</i><sub>max</sub>为设置的最大视差,<i>N</i><sub>d</sub>为视差维度的采样数,<i>n</i>为视差维度的索引。</p>
                </div>
                <div class="p1">
                    <p id="89">当视差概率分布是单峰且对称时, (1) 式可以得到较好的亚像素估计,当视差分布存在多峰值时,预测值将会远离峰值。Kendall等<citation id="153" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>阐述了CNN学习可以对输出值进行预尺度处理,并使其分布具有单峰性,但这种预尺度仅适合训练阶段使用的视差范围,当在测试阶段改变视差范围时,需要对模型参数进行重新训练或微调。若只在最大概率的视差值邻域内进行加权平均,可以有效解决上述问题,其表达式为</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>d</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>Μ</mtext></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">|</mo><mi>d</mi><msub><mrow></mrow><mi>n</mi></msub><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">|</mo><mo>≤</mo><mi>δ</mi></mrow></munder><mi>d</mi></mstyle><msub><mrow></mrow><mi>n</mi></msub><mi>σ</mi><mo stretchy="false">(</mo><mo>-</mo><mi>C</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">式中:<mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>d</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>Μ</mtext></msub></mrow></math></mathml>为视差预测值;<mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mi>m</mi></msub><mo>=</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mi>m</mi><mo>/</mo><mi>Ν</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>,</mo><mi>m</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mn>0</mn><mo>≤</mo><mi>n</mi><mo>≤</mo><mi>Ν</mi><msub><mrow></mrow><mtext>d</mtext></msub></mrow></munder><mo stretchy="false">(</mo><mo>-</mo><mi>C</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>,实验中取<i>δ</i>=2。</p>
                </div>
                <div class="p1">
                    <p id="92">为使(2) 式得到更好的视差估计,结合亚像素的交叉熵损失<i>L</i><sup>CE</sup>和平滑的L1损失<i>L</i><sup>s</sup><sub>1</sub>训练模型,损失函数为</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mi>L</mi><msup><mrow></mrow><mrow><mtext>C</mtext><mtext>E</mtext></mrow></msup><mo>+</mo><mi>w</mi><mi>L</mi><msubsup><mrow></mrow><mn>1</mn><mtext>s</mtext></msubsup><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">式中:<i>w</i>为<i>L</i><sup>s</sup><sub>1</sub>的权重,用于平衡两种损失函数的重要性,实验中取0.1;<i>L</i><sup>CE</sup>和<i>L</i><sup>s</sup><sub>1</sub>的具体公式为</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msup><mrow></mrow><mrow><mtext>C</mtext><mtext>E</mtext></mrow></msup><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mtext>d</mtext></msub></mrow></munderover><mo>-</mo></mstyle></mrow></mstyle><mi>Q</mi><mo stretchy="false">(</mo><mi>d</mi><msup><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msup><mo>,</mo><mi>d</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo><mi>ln</mi><mo stretchy="false">[</mo><mi>σ</mi><mo stretchy="false">(</mo><mo>-</mo><mi>C</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>L</mi><msubsup><mrow></mrow><mn>1</mn><mtext>s</mtext></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>Ν</mi></munderover><mi>f</mi></mstyle><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>L</mtext></mrow></msub><mo stretchy="false">(</mo><mo stretchy="false">|</mo><mi>d</mi><msup><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msup><mo>-</mo><mover accent="true"><mi>d</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>A</mtext></msub><mo stretchy="false">|</mo><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">式中:<i>N</i>为具有视差标签值的像素数;<i>i</i>为像素索引; <i>Q</i>(<i>d</i><sup>gt</sup>,<i>d</i><sub><i>n</i></sub>)=exp(-|<i>d</i><sub><i>n</i></sub>-<i>d</i><sup>gt</sup>|/<i>b</i>),为目标概率分布,是以视差标签值<i>d</i><sup>gt</sup>为中心、散度为<i>b</i>的拉普拉斯分布,实验中取<i>b</i>=2;</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>L</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>0</mn><mo>.</mo><mn>5</mn><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>,</mo></mtd><mtd columnalign="left"><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">|</mo><mo>&lt;</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left"><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">|</mo><mo>-</mo><mn>0</mn><mo>.</mo><mn>5</mn><mo>,</mo></mtd><mtd columnalign="left"><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow></mrow></mrow></math></mathml>
                    </p>
                </div>
                <h3 id="99" name="99" class="anchor-tag">4 实  验</h3>
                <div class="p1">
                    <p id="100">在<i>SceneFlow</i>数据集<citation id="154" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、<i>KITTI</i>2015数据集<citation id="155" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>和<i>KITTI</i>2012数据集<citation id="156" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>上,使用评价指标E<sub><i>EP</i></sub>和E<sub><i>D</i></sub><sub>1</sub>对算法进行评价。其中,E<sub><i>EP</i></sub>表示预测视差与真值之间的差值绝对值;E<sub><i>D</i></sub><sub>1</sub>表示每组图像对评价区域的错误像素百分比,其中E<sub><i>EP</i></sub>小于3 <i>pixel</i>或E<sub><i>EP</i></sub>小于真值5%时,认为是正确像素,否则为错误像素。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101"><b>4.1 实验细节</b></h4>
                <div class="p1">
                    <p id="102">所提算法使用PyTorch实现,源代码见https:www.github.com/Wyf_2017/WSMCnet,所有训练和测试均在2个Nvidia 1070ti显卡上运行。使用小批量随机梯度下降的方式进行训练,单次迭代的样本大小取2,4,8(保证训练时不会出现内存溢出的情况下取最大值),梯度单次更新的迭代次数取8,4,2,使单次梯度更新的样本大小扩展为16。模型的训练均使用Adam优化器<citation id="157" type="reference"><link href="62" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>,延迟率参数取(0.9, 0.999),图片随机裁剪的大小为256 pixel×512 pixel,最大视差设为192 pixel。使用的数据集如下:</p>
                </div>
                <div class="p1">
                    <p id="103">1) SceneFlow数据集:一个大型的合成数据集,包含35454对训练图像(记为SF-train)和4370对测试图像(记为SF-test)。每对图像的像素大小为540 pixel×960 pixel,可以提供密集精细的视差图真值。实验中计算损失和评价指标时,排除视差大于192 pixel的图像。</p>
                </div>
                <div class="p1">
                    <p id="104">2) KITTI2015数据集和KITTI2012数据集:均为在不同天气条件下对街区真实场景记录的数据集,KITTI2015数据集包含200对训练图像(记为K15-train)和200对测试图像(记为K15-test),KITTI2012数据集包含194对训练图像(记为K12-train)和195对测试图像(记为K12-test),只使用其中的彩色图像对。每对图像的像素大小为375 pixel×1242 pixel,可以提供稀疏的激光雷达数据作为视差图的真值。为了对不同设置进行分析,将两个数据集中的训练图像的前160对图像作为训练集(记为K-train),其余的作为验证集(记为K-val)。</p>
                </div>
                <div class="p1">
                    <p id="105">为提高模型的泛化能力,对训练数据进行颜色增强和空间变换增强。其中,颜色增强包括色调增强、对比度增强、亮度增强和随机灰度化;为保证双目图像的核线几何特性,空间变换增强只包括随机裁剪和随机水平翻转。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106"><b>4.2 超参数分析</b></h4>
                <div class="p1">
                    <p id="107">对超参数的分析主要分为3组,前2组分别对3.1节中的<i>S</i>值和<i>C</i>值的作用进行分析,仅使用SF-train训练模型;第3组对两种损失函数进行对比分析,使用SF-train和K-train训练模型。其中,在SF-train上的实验,均先以学习率为0.001训练15个epoch(1个epoch就是将模型在数据集的所有样本上训练一次),再以学习率为0.0001训练5个epoch;在K-train上的实验,均先以学习率为0.0005训练450个epoch,再以学习率为0.0001训练150个epoch。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">4.2.1 <i>S</i>值对算法性能的影响</h4>
                <div class="p1">
                    <p id="109">分析<i>S</i>值的同时,为与基准算法进行直接对比,本组实验在文献<citation id="158" type="reference">[<a class="sup">14</a>]</citation>的模型设置(其中<i>C</i>=1)基础上,设定<i>S</i>的取值范围为<citation id="161" type="reference">[<a class="sup">1</a>,<a class="sup">8</a>]</citation>,其性能对比如表1所示。表中GPU表示对于256 pixel×256 pixel的图片在训练阶段占用的GPU显存,<i>E</i><sub>EP</sub>和<i>E</i><sub>D1</sub>均为训练完成时在验证集上的测试结果,<i>t</i><sub>Run</sub>为20个随机样本(每个样本的图片大小为540 pixel×960 pixel)上测试的平均值。可以看出,随着<i>S</i>值增大,算法误差逐渐增大,其计算负载逐渐降低。与文献<citation id="159" type="reference">[<a class="sup">14</a>]</citation>相比,同样模型设置的情况下,本实验中<i>E</i><sub>EP</sub>误差减小了约6%,这主要得益于训练周期的延长和学习率的适当调整。与<i>S</i>=1相比,<i>S</i>=2, 3时<i>E</i><sub>EP</sub>增加了约8%, 14%,<i>E</i><sub>D1</sub>增加了0.58%, 0.93%,<i>t</i><sub>Run</sub>降低了约40%, 53%;而与文献<citation id="160" type="reference">[<a class="sup">14</a>]</citation>相比,<i>S</i>=2, 3时<i>E</i><sub>EP</sub>仅增加了约1%, 6%,误差的增加较小,而运行时间显著缩短。</p>
                </div>
                <div class="area_img" id="110">
                    <p class="img_tit">表1 不同<i>S</i>值设置下算法的性能评价(<i>C</i>=1) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Performance evaluation of proposed method with different <i>S</i> (<i>C</i>=1)</p>
                    <p class="img_note"></p>
                    <table id="110" border="1"><tr><td>Method</td><td><i>S</i></td><td><i>E</i><sub>EP</sub> /pixel</td><td><i>E</i><sub>D1</sub> /%</td><td><i>t</i><sub>Run</sub> /s</td><td>GPU /GB</td></tr><tr><td><br />PSMNet<sup>[14]</sup></td><td>1</td><td>1.09</td><td>-</td><td>-</td><td>-</td></tr><tr><td><br /></td><td>1</td><td><b>1.02</b></td><td><b>3.41</b></td><td>0.75</td><td>2.16</td></tr><tr><td><br /></td><td>2</td><td>1.10</td><td>3.89</td><td>0.45</td><td>1.51</td></tr><tr><td><br /></td><td>3</td><td>1.16</td><td>4.34</td><td>0.35</td><td>1.32</td></tr><tr><td><br />Proposed</td><td>4</td><td>1.22</td><td>4.81</td><td>0.30</td><td>1.20</td></tr><tr><td><br /></td><td>5</td><td>1.30</td><td>5.25</td><td>0.25</td><td>1.09</td></tr><tr><td><br /></td><td>6</td><td>1.34</td><td>5.62</td><td>0.25</td><td>1.08</td></tr><tr><td><br /></td><td>7</td><td>1.41</td><td>6.07</td><td>0.22</td><td>1.01</td></tr><tr><td><br /></td><td>8</td><td>1.42</td><td>6.23</td><td><b>0.22</b></td><td><b>1.01</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">4.2.2 <i>C</i>值对算法性能的影响</h4>
                <div class="p1">
                    <p id="112">从上述分析可知,当<i>S</i>=2, 3时算法性能并不会有明显地降低,而运行效率得到了显著提高。因此,设定<i>S</i>=2, 3时,<i>C</i>的取值范围为<citation id="163" type="reference">[<a class="sup">1</a>,<a class="sup">6</a>]</citation>,算法的性能对比如表2所示。可以看出,当<i>S</i>值固定时,算法的计算负载随<i>C</i>值的增大而略有增加,<i>C</i>值的范围为<citation id="164" type="reference">[<a class="sup">1</a>,<a class="sup">3</a>]</citation>时,误差随<i>C</i>值的增大而减小,当<i>C</i>值的范围为<citation id="165" type="reference">[<a class="sup">4</a>,<a class="sup">6</a>]</citation>时,并不能维持这种变化趋势,这主要是因为较大的<i>C</i>值会使模型的训练难度加大。与<i>S</i>=1, <i>C</i>=1相比,当<i>S</i>=2, 3, <i>C</i>=3时,<i>E</i><sub>EP</sub>增加了约3%, 9%,<i>E</i><sub>D1</sub>增加了0.22%, 0.58,<i>t</i><sub>Run</sub>降低了约33%, 48%;而与文献<citation id="162" type="reference">[<a class="sup">14</a>]</citation>相比,当<i>S</i>=2, <i>C</i>=3时,<i>E</i><sub>EP</sub>降低了约4%,<i>t</i><sub>Run</sub>降低了约33%,算法精度和效率均得到提升;当<i>S</i>=3, <i>C</i>=3时,<i>E</i><sub>EP</sub>仅增加了约2%,<i>t</i><sub>Run</sub>降低了约48%,损失较小精度的情况下效率得到了显著提高。</p>
                </div>
                <div class="area_img" id="113">
                    <p class="img_tit">表2 不同<i>C</i>值设置下算法的性能评价 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Performance evaluation of proposed method with different <i>C</i></p>
                    <p class="img_note"></p>
                    <table id="113" border="1"><tr><td><i>S</i></td><td><i>C</i></td><td><i>E</i><sub>EP</sub> /pixel</td><td><i>E</i><sub>D1</sub> /%</td><td><i>t</i><sub>Run</sub> /s</td><td>GPU /GB</td></tr><tr><td><br />1</td><td>1</td><td><b>1.02</b></td><td><b>3.41</b></td><td>0.75</td><td>2.16</td></tr><tr><td><br /></td><td>1</td><td>1.10</td><td>3.89</td><td>0.45</td><td>1.51</td></tr><tr><td><br /></td><td>2</td><td>1.07</td><td>3.71</td><td>0.48</td><td>1.68</td></tr><tr><td><br />2</td><td>3</td><td>1.05</td><td>3.63</td><td>0.51</td><td>1.85</td></tr><tr><td><br /></td><td>4</td><td>1.08</td><td>3.81</td><td>0.53</td><td>2.02</td></tr><tr><td><br /></td><td>5</td><td>1.08</td><td>3.79</td><td>0.54</td><td>2.20</td></tr><tr><td><br /></td><td>6</td><td>1.07</td><td>3.69</td><td>0.56</td><td>2.37</td></tr><tr><td><br /></td><td>1</td><td>1.16</td><td>4.34</td><td><b>0.35</b></td><td><b>1.32</b></td></tr><tr><td><br /></td><td>2</td><td>1.13</td><td>4.06</td><td>0.37</td><td>1.42</td></tr><tr><td><br />3</td><td>3</td><td>1.11</td><td>3.99</td><td>0.39</td><td>1.55</td></tr><tr><td><br /></td><td>4</td><td>1.14</td><td>4.06</td><td>0.41</td><td>1.66</td></tr><tr><td><br /></td><td>5</td><td>1.14</td><td>4.03</td><td>0.42</td><td>1.78</td></tr><tr><td><br /></td><td>6</td><td>1.09</td><td>3.89</td><td>0.43</td><td>1.89</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="114" name="114">4.2.3 视差回归函数对算法性能的影响</h4>
                <div class="p1">
                    <p id="115">从上述分析可知,与<i>S</i>=1, <i>C</i>=1相比,当<i>S</i>=2, 3, <i>C</i>=3时,误差增加了约3%, 9%,而效率显著提高了约33%, 48%。因此,设定<i>S</i>=2, 3, <i>C</i>=3,研究两种损失函数对算法的影响,其性能对比如表3所示。表中L1表示使用 (1)式和平滑的L1损失来训练模型,CE表示使用(2)式和 (3)式中的交叉熵损失来训练模型,CE+L1表示使用(2)式和(3)式来训练模型,Tri/Bi表示对匹配损失在3/2个维度上进行线性上采样。可以看出,在训练模型时,与使用平滑的L1损失相比,采用交叉熵损失对评价指标<i>E</i><sub>D1</sub>的改善较为明显,且当直接扩展视差范围时算法精度的降幅更小。与参数设置为{1, 1, L1, Tri}相比,参数设置为{2, 3, CE+L1, Bi}和{3, 3, CE+L1, Bi}时,<i>E</i><sub>EP</sub>分别增加了约2%和10%,<i>E</i><sub>D1</sub>分别降低了0.72%和0.66%,<i>t</i><sub>Run</sub>分别降低了约40%和52%,这不仅减少了异常值的像素数,而且大幅度提高了运行效率。</p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit">表3 不同模型设置下算法的性能评价 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Performance evaluation of proposed method with different settings</p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td colspan="4"><br />Setting</td><td colspan="4">Max disparity of 192</td><td colspan="2">Max disparity of 384</td><td rowspan="2" colspan="4"></td><td rowspan="2" colspan="2"></td></tr><tr><td><i>S</i></td><td><i>C</i></td><td>Loss</td><td>Dimensionality</td><td><i>E</i><sub>EP</sub> /pixel</td><td><i>E</i><sub>D1</sub> /%</td><td><i>t</i><sub>Run</sub> /s</td><td>GPU /GB</td><td><i>E</i><sub>EP</sub> /pixel</td><td><i>E</i><sub>D1</sub> /%</td></tr><tr><td>1</td><td>1</td><td>L1</td><td>Tri</td><td><b>1.02</b></td><td>3.41</td><td>0.75</td><td>2.16</td><td>1.33</td><td>3.64</td></tr><tr><td><br />2</td><td>3</td><td>L1</td><td>Tri</td><td>1.05</td><td>3.63</td><td>0.51</td><td>1.85</td><td>1.38</td><td>3.90</td></tr><tr><td><br />2</td><td>3</td><td>CE</td><td>Bi</td><td>1.04</td><td>2.71</td><td>0.45</td><td>1.50</td><td>1.29</td><td>2.92</td></tr><tr><td><br />2</td><td>3</td><td>CE+L1</td><td>Bi</td><td>1.04</td><td><b>2.69</b></td><td>0.45</td><td>1.56</td><td><b>1.28</b></td><td><b>2.87</b></td></tr><tr><td><br />3</td><td>3</td><td>L1</td><td>Tri</td><td>1.11</td><td>3.99</td><td>0.39</td><td>1.55</td><td>1.49</td><td>4.30</td></tr><tr><td><br />3</td><td>3</td><td>CE</td><td>Bi</td><td>1.13</td><td>2.73</td><td>0.36</td><td>1.30</td><td>1.39</td><td>3.40</td></tr><tr><td><br />3</td><td>3</td><td>CE+L1</td><td>Bi</td><td>1.12</td><td>2.75</td><td><b>0.36</b></td><td><b>1.28</b></td><td>1.37</td><td>3.00</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="118">选择表3中的三组模型设置<i>S</i><sub>1</sub>={1, 1, L1, Tri},<i>S</i><sub>2</sub>={2, 3, CE+L1, Bi},<i>S</i><sub>3</sub>={3, 3, CE+L1, Bi},在K-train上进行微调,在K-val上进行测试的性能对比如表4所示,表中K15-val和K12-val分别对应K-val中的两个子集。可以看出,与基准算法相比,所提算法不仅提高了算法运行效率,而且以<i>E</i><sub>D1</sub>为评价指标时具有一定优势。</p>
                </div>
                <div class="area_img" id="119">
                    <p class="img_tit">表4 在K-val上不同参数设置下算法的性能评价 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Performance evaluation of proposed method with different settings on K-val</p>
                    <p class="img_note"></p>
                    <table id="119" border="1"><tr><td rowspan="2"><br />Setting</td><td colspan="2"><br />K15-val</td><td colspan="2">K12-val</td><td rowspan="2" colspan="2"></td></tr><tr><td><br /><i>E</i><sub>EP</sub> /pixel</td><td><i>E</i><sub>D1</sub> /%</td><td><i>E</i><sub>EP</sub> /pixel</td><td><i>E</i><sub>D1</sub> /%</td></tr><tr><td><br /><i>S</i><sub>1</sub></td><td><b>0.74</b></td><td>2.23</td><td><b>0.62</b></td><td>2.05</td></tr><tr><td><br /><i>S</i><sub>2</sub></td><td>0.75</td><td><b>2.02</b></td><td>0.63</td><td><b>1.78</b></td></tr><tr><td><br /><i>S</i><sub>3</sub></td><td>0.81</td><td>2.23</td><td>0.70</td><td>1.98</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="120" name="120"><b>4.3 几种典型算法的性能对比</b></h4>
                <div class="p1">
                    <p id="121">对K15-test和K12-test中的图像对使用所提算法(表4中参数设置<i>S</i><sub>2</sub>)进行视差预测,并将其结果提交到KITTI数据集用于在线评价服务。基于深度学习的几种典型算法的性能对比如表5和表6所示,其中“All”表示评价时包含所有像素,“Noc”表示只考虑非遮挡区域内的像素。表5中<i>E</i><sub>D1-bg</sub>、<i>E</i><sub>D1-fg</sub>和<i>E</i><sub>D1-all</sub>分别表示在背景区域、前景区域和所有区域内计算评价指标<i>E</i><sub>D1</sub>;表6中<i>γ</i><sub><i>n</i></sub>表示评价区域内,<i>E</i><sub>EP</sub>大于<i>n</i>的像素百分比。可以看出,与几种典型算法相比,所提算法在精度和运行效率上均具有一定优势,且在K15-test和K12-test的非遮挡区域取得了最高的精度。</p>
                </div>
                <div class="p1">
                    <p id="122">为分析所提算法存在的主要问题,在K15-test和K12-test上,分别选择误差较大的2组图像进行主观评价,结果如图3所示。其中,误差图为预测视差图与基准值之间的差值绝对值,左侧2组来自K15-test(<i>E</i><sub>D1-all</sub>分别为3.80%和3.55%),右侧2组来自K12-test(<i>γ</i><sub>3</sub>分别为3.80%和3.55%)。可以看出,在4种场景下所提算法均能得到合理且稠密的视差图,算法能够很好地处理纹理重复区域、弱/无纹理区域,如场景中道路、天空和车辆等所在的区域。但对于不规则表面、被遮挡和光线特别暗的区域仍存在较大的误差,如草地、物体边缘和栅栏等所在区域。</p>
                </div>
                <div class="area_img" id="123">
                    <p class="img_tit">表5 K15-test上不同算法的性能评价 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Performance evaluation of different methods on K15-test</p>
                    <p class="img_note"></p>
                    <table id="123" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="3"><br />All</td><td colspan="3">Noc</td><td rowspan="2" colspan="3"></td><td rowspan="2"><i>t</i><sub>Run</sub> /s</td></tr><tr><td><br /><i>E</i><sub>D1-bg</sub> /%</td><td><i>E</i><sub>D1-fg</sub> /%</td><td><i>E</i><sub>D1-all</sub> /%</td><td><i>E</i><sub>D1-bg</sub> /%</td><td><i>E</i><sub>D1-fg</sub> /%</td><td><i>E</i><sub>D1-all</sub> /%</td></tr><tr><td>MC-CNN-arct<sup>[10]</sup></td><td>2.89</td><td>8.88</td><td>3.89</td><td>2.48</td><td>7.64</td><td>3.33</td><td>67.00</td></tr><tr><td><br />DispNetC<sup>[11]</sup></td><td>4.32</td><td>4.41</td><td>4.34</td><td>4.11</td><td>3.72</td><td>4.05</td><td><b>0.06</b></td></tr><tr><td><br />iResNet-i2<sup>[12]</sup></td><td>2.25</td><td>3.40</td><td>2.44</td><td>2.07</td><td>2.76</td><td>2.19</td><td>0.12</td></tr><tr><td><br />GC-net<sup>[13]</sup></td><td>2.21</td><td>6.16</td><td>2.87</td><td>2.02</td><td>5.58</td><td>2.61</td><td>0.90</td></tr><tr><td><br />PSMNet<sup>[14]</sup></td><td>1.86</td><td>4.62</td><td>2.32</td><td>1.71</td><td>4.31</td><td>2.14</td><td>0.41</td></tr><tr><td><br />Proposed</td><td><b>1.72</b></td><td><b>4.19</b></td><td><b>2.13</b></td><td><b>1.51</b></td><td><b>3.57</b></td><td><b>1.85</b></td><td>0.39</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="124">
                    <p class="img_tit">表6 K12-test上不同算法的性能评价 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 6 Performance evaluation of different methods on K12-test</p>
                    <p class="img_note"></p>
                    <table id="124" border="1"><tr><td rowspan="2">Method</td><td colspan="2"><br /><i>γ</i><sub>2</sub> /%</td><td colspan="2"><i>γ</i><sub>3</sub> /%</td><td colspan="2"><i>γ</i><sub>5</sub> /%</td><td colspan="2">Mean <i>E</i><sub>EP</sub> /pixel</td><td rowspan="2" colspan="2"></td><td rowspan="2" colspan="2"></td><td rowspan="2" colspan="2"></td></tr><tr><td>Noc</td><td>All</td><td>Noc</td><td>All</td><td>Noc</td><td>All</td><td>Noc</td><td>All</td></tr><tr><td>MC-CNN-arct<sup>[10]</sup></td><td>3.90</td><td>5.45</td><td>2.43</td><td>3.63</td><td>1.64</td><td>2.39</td><td>0.7</td><td>0.9</td></tr><tr><td><br />DispNetC<sup>[11]</sup></td><td>7.38</td><td>8.11</td><td>4.11</td><td>4.65</td><td>2.05</td><td>2.39</td><td>0.9</td><td>1.0</td></tr><tr><td><br />iResNet-i2<sup>[12]</sup></td><td>2.69</td><td>3.34</td><td>1.71</td><td>2.16</td><td>1.06</td><td>1.32</td><td>0.5</td><td>0.6</td></tr><tr><td><br />GC-net<sup>[13]</sup></td><td>2.71</td><td>3.46</td><td>1.77</td><td>2.30</td><td>1.12</td><td>1.46</td><td>0.6</td><td>0.7</td></tr><tr><td><br />PSMNet<sup>[14]</sup></td><td>2.44</td><td><b>3.01</b></td><td>1.49</td><td><b>1.89</b></td><td>0.90</td><td><b>1.15</b></td><td><b>0.5</b></td><td><b>0.6</b></td></tr><tr><td><br />Proposed</td><td><b>2.35</b></td><td>3.04</td><td><b>1.42</b></td><td>1.90</td><td><b>0.89</b></td><td>1.19</td><td>0.6</td><td>0.6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911028_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 算法预测的视差结果。" src="Detail/GetImg?filename=images/GXXB201911028_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 算法预测的视差结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911028_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Disparity predicted by proposed method.</p>
                                <p class="img_note">(a)左视角图像;(b)视差图;(c)误差图;(d)局部细节</p>
                                <p class="img_note"> (a) Left image; (b) disparity map; (c) error map; (d) local details</p>

                </div>
                <h3 id="126" name="126" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="127">提出一种在视差维度上使用稀疏损失体进行立体匹配的方法,在K15-test和K12-test上,与几种典型的基于深度学习的方法相比,所提算法在整体精度上取得了最优性能,特别是与基准方法相比,不仅提高了算法精度,而且运行时间也大幅度缩短。但对于小型设备算法开销较大,仍无法满足实时性要求,为得到精度高、运行效率高的立体匹配算法,仍需要对算法的网络结构进行更深入的研究。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Local density encoding for robust stereo matching">

                                <b>[1]</b> Nguyen V D,Nguyen D D,Lee S J,<i>et al</i>.Local density encoding for robust stereo matching[J].IEEE Transactions on Circuits and Systems for Video Technology,2014,24(12):2049-2062.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast dense stereo correspondences by binary locality sensitive hashing">

                                <b>[2]</b> Heise P,Jensen B,Klose S,<i>et al</i>.Fast dense stereo correspondences by binary locality sensitive hashing[C]//2015 IEEE International Conference on Robotics and Automation (ICRA),May 26-30,2015,Seattle,WA,USA.New York:IEEE,2015:105-110.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Continuous 3D label stereo matching using local expansion moves">

                                <b>[3]</b> Taniai T,Matsushita Y,Sato Y,<i>et al</i>.Continuous 3D label stereo matching using local expansion moves[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(11):2725-2739.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESDA94C517D724BE25F6DF4053D5A87285&amp;v=Mjg3MjNOQ0E1TXpSTmw3RXNMVEgvbnIyWXdDTHFUUjdLYUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRsaHg3Mjl3S0E9TmlmT2ZjZkpGOVcvcW81Q0VPdw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Hamzah R A,Ibrahim H,Hassan A H A.Stereo matching algorithm based on per pixel difference adjustment,iterative guided filter and graph segmentation[J].Journal of Visual Communication and Image Representation,2017,42:145-160.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identity Mappings in Deep Residual Networks">

                                <b>[5]</b> He K M,Zhang X Y,Ren S Q,<i>et al</i>.Identity mappings in deep residual networks[M]//Leibe B,Matas J,Sebe N,<i>et al</i>.European conference on computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9908:630-645.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                 Ren S Q,He K M,Girshick R,<i>et al</i>.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(6):1137-1149.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[7]</b> Shelhamer E,Long J,Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(4):640-651.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201808018&amp;v=MjAzMDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl2Z1ViL0FJalhUYkxHNEg5bk1wNDlFYklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Xiao J S,Tian H,Zou W T,<i>et al</i>.Stereo matching based on convolutional neural network[J].Acta Optica Sinica,2018,38(8):0815017.肖进胜,田红,邹文涛,等.基于深度卷积神经网络的双目立体视觉匹配算法[J].光学学报,2018,38(8):0815017.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Computing the stereo matching cost with a convolutional neural network">

                                <b>[9]</b> Žbontar J,LeCun Y.Computing the stereo matching cost with a convolutional neural network[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:1592-1599.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stereo matching by training a convolutional neural network to compare image patches">

                                <b>[10]</b> Žbontar J,LeCun Y.Stereo matching by training a convolutional neural network to compare image patches[J].Journal of Machine Learning Research,2016,17(1):2287-2318.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Large Dataset to Train Convolutional Networks for Disparity,Optical Flow,and Scene Flow Estimation">

                                <b>[11]</b> Mayer N,Ilg E,Häusser P,<i>et al</i>.A large dataset to train convolutional networks for disparity,optical flow,and scene flow estimation[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:4040-4048.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cascade Residual Learning A Two-Stage Convolutional Neural Network for Stereo Matching">

                                <b>[12]</b> Pang J H,Sun W X,Ren J S,<i>et al</i>.Cascade residual learning:a two-stage convolutional neural network for stereo matching[C]//2017 IEEE International Conference on Computer Vision Workshops (ICCVW),October 22-29,2017,Venice,Italy.New York:IEEE,2017:878-886.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Endto-end learning of geometry and context for deep stereo regression">

                                <b>[13]</b> Kendall A,Martirosyan H,Dasgupta S,<i>et al</i>.End-to-end learning of geometry and context for deep stereo regression[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice,Italy.New York:IEEE,2017:66-75.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Pyramid stereo matching network,&amp;quot;">

                                <b>[14]</b> Chang J R,Chen Y S.Pyramid stereo matching network[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:5410-5418.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830724&amp;v=MDg4MzE9Tmo3QmFyTzRIdEhPcDR4Rlkra0xZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTbmxVTHJLSUZZ&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Scharstein D,Szeliski R.A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J].International Journal of Computer Vision,2002,47(1/2/3):7-42.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-resolution stereo datasets with subpixel-accurate ground truth">

                                <b>[16]</b> Scharstein D,Hirschmüller H,Kitajima Y,<i>et al</i>.High-resolution stereo datasets with subpixel-accurate ground truth[M]//Jiang X,Hornegger J,Koch R.German conference on pattern recognition.GCPR 2014.Lecture notes in computer science.Cham:Springer,2014,8753:31-42.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Are we ready for autonomous driving?The KITTI vision benchmark suite,&amp;quot;">

                                <b>[17]</b> Geiger A,Lenz P,Urtasun R.Are we ready for autonomous driving?The KITTI vision benchmark suite[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition,June 16-21,2012,Providence,RI,USA.New York:IEEE,2012:3354-3361.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object scene flow for autonomous vehicles">

                                <b>[18]</b> Menze M,Geiger A.Object scene flow for autonomous vehicles[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:3061-3070.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient Deep Learning for Stereo Matching">

                                <b>[19]</b> Luo W J,Schwing A G,Urtasun R.Efficient deep learning for stereo matching[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:5695-5703.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning for disparity estimation through feature constancy">

                                <b>[20]</b> Liang Z F,Feng Y L,Guo Y L,<i>et al</i>.Learning for disparity estimation through feature constancy[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:2811-2820.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Left-right comparative recurrent model for stereo matching">

                                <b>[21]</b> Jie Z Q,Wang P F,Ling Y G,<i>et al</i>.Left-right comparative recurrent model for stereo matching[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:3838-3846.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep stereo matching with explicit cost aggregation sub-architecture">

                                <b>[22]</b> Yu L D,Wang Y C,Wu Y W,<i>et al</i>.Deep stereo matching with explicit cost aggregation sub-architecture[J/OL].(2018-01-12)[2019-04-15].http://cn.arxiv.org/abs/1801.04065.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the importance of stereo for accurate depth estimation:an efficient semi-supervised deep neural network approach">

                                <b>[23]</b> Smolyanskiy N,Kamenev A,Birchfield S.On the importance of stereo for accurate depth estimation:an efficient semi-supervised deep neural network approach[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),June 18-22,2018,Salt Lake City,UT,USA.New York:IEEE,2018:1120-1128.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Self-supervised learning for stereo matching with self-improving ability">

                                <b>[24]</b> Zhong Y R,Dai Y C,Li H D.Self-supervised learning for stereo matching with self-improving ability[J/OL].(2017-09-04)[2019-04-15].http://cn.arxiv.org/abs/1709.00930.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902035&amp;v=MTU3MDNVUkxPZVplVnZGeXZnVWIvQUlqWFRiTEc0SDlqTXJZOUdZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> Wang Y F,Wang H W,Wu C,<i>et al</i>.Self-supervised stereo matching algorithm based on common view[J].Acta Optica Sinica,2019,39(2):0215004.王玉锋,王宏伟,吴晨,等.基于共同视域的自监督立体匹配算法[J].光学学报,2019,39(2):0215004.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_26" >
                                    <b>[26]</b>
                                 Girshick R.Fast R-CNN[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:1440-1448.
                            </a>
                        </p>
                        <p id="62">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">

                                <b>[27]</b> Kingma D P,Ba J.Adam:a method for stochastic optimization[J/OL].(2017-01-30)[2019-04-15].http://cn.arxiv.org/abs/1412.6980.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201911028" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201911028&amp;v=Mjg5MzFOcm85SGJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5dmdVYi9BSWpYVGJMRzRIOWo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

