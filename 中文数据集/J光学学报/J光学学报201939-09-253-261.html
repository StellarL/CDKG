

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133152268565000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201909030%26RESULT%3d1%26SIGN%3d4tiTvbpRQAhl2q0ZI0W%252fmT9vQdk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201909030&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201909030&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201909030&amp;v=Mjc5MTNvOUdaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXprVmI3UElqWFRiTEc0SDlqTXA=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="2 本文方法 ">2 本文方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="&lt;b&gt;2.1 通用特征学习&lt;/b&gt;"><b>2.1 通用特征学习</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;2.2 特定特征学习&lt;/b&gt;"><b>2.2 特定特征学习</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;2.3 网络在线更新&lt;/b&gt;"><b>2.3 网络在线更新</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="3 实验分析 ">3 实验分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#94" data-title="&lt;b&gt;3.1 参数设置标准&lt;/b&gt;"><b>3.1 参数设置标准</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;3.2 OTB数据库结果分析&lt;/b&gt;"><b>3.2 OTB数据库结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#121" data-title="4 结  论 ">4 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="图1 网络在线更新流程图。">图1 网络在线更新流程图。</a></li>
                                                <li><a href="#91" data-title="图2 相似干扰序列中跟踪目标响应图。">图2 相似干扰序列中跟踪目标响应图。</a></li>
                                                <li><a href="#97" data-title="图3 不同更新策略下的精确率与成功率对比图。">图3 不同更新策略下的精确率与成功率对比图。</a></li>
                                                <li><a href="#108" data-title="图4 不同算法在OTB50视频库中的精确率与成功率对比图。">图4 不同算法在OTB50视频库中的精确率与成功率对比图。</a></li>
                                                <li><a href="#109" data-title="图5 不同算法在OTB100视频库中的精确率与成功率对比图。">图5 不同算法在OTB100视频库中的精确率与成功率对比图。</a></li>
                                                <li><a href="#113" data-title="图6 不同难度属性下的各算法跟踪结果的精确率曲线图">图6 不同难度属性下的各算法跟踪结果的精确率曲线图</a></li>
                                                <li><a href="#116" data-title="图7 各算法在不同难度属性的视频中的实际追踪效果图">图7 各算法在不同难度属性的视频中的实际追踪效果图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="9">


                                    <a id="bibliography_1" title=" Li S S,Zhao G P,Wang J Y.Distractor-aware object tracking based on multi-feature fusion and scale-adaption[J].Acta Optica Sinica,2017,37(5):0515005.李双双,赵高鹏,王建宇.基于特征融合和尺度自适应的干扰感知目标跟踪[J].光学学报,2017,37(5):0515005." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201705025&amp;v=MjQ2ODlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5emtWYjdQSWpYVGJMRzRIOWJNcW85SFlZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Li S S,Zhao G P,Wang J Y.Distractor-aware object tracking based on multi-feature fusion and scale-adaption[J].Acta Optica Sinica,2017,37(5):0515005.李双双,赵高鹏,王建宇.基于特征融合和尺度自适应的干扰感知目标跟踪[J].光学学报,2017,37(5):0515005.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_2" title=" Li Z D,Zhong Y,Chen M,&lt;i&gt;et al&lt;/i&gt;.Fast face image retrieval based on depth feature[J].Acta Optica Sinica,2018,38(10):1010004.李振东,钟勇,陈蔓,等.基于深度特征的快速人脸图像检索方法[J].光学学报,2018,38(10):1010004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201810020&amp;v=MjQ2ODlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5emtWYjdQSWpYVGJMRzRIOW5OcjQ5SFpJUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Li Z D,Zhong Y,Chen M,&lt;i&gt;et al&lt;/i&gt;.Fast face image retrieval based on depth feature[J].Acta Optica Sinica,2018,38(10):1010004.李振东,钟勇,陈蔓,等.基于深度特征的快速人脸图像检索方法[J].光学学报,2018,38(10):1010004.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_3" title=" Bertinetto L,Valmadre J,Henriques J F,&lt;i&gt;et al&lt;/i&gt;.Fully-convolutional Siamese networks for object tracking[M]//Hua G,J&#233;gou H.Computer vision-ECCV 2016 workshops.Lecture notes in computer science.Cham:Springer,2016,9914:850-865." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully-convolutional siamese networks for object tracking">
                                        <b>[3]</b>
                                         Bertinetto L,Valmadre J,Henriques J F,&lt;i&gt;et al&lt;/i&gt;.Fully-convolutional Siamese networks for object tracking[M]//Hua G,J&#233;gou H.Computer vision-ECCV 2016 workshops.Lecture notes in computer science.Cham:Springer,2016,9914:850-865.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_4" title=" Valmadre J,Bertinetto L,Henriques J,&lt;i&gt;et al&lt;/i&gt;.End-to-end representation learning for correlation filter based tracking[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI.New York:IEEE,2017:2805-2813." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-End Representation Learning for Correlation Filter Based Tracking">
                                        <b>[4]</b>
                                         Valmadre J,Bertinetto L,Henriques J,&lt;i&gt;et al&lt;/i&gt;.End-to-end representation learning for correlation filter based tracking[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI.New York:IEEE,2017:2805-2813.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_5" title=" Guo Q,Feng W,Zhou C,&lt;i&gt;et al&lt;/i&gt;.Learning dynamic Siamese network for visual object tracking[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice.New York:IEEE,2017:1763-1771." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Dynamic Siamese Network for Visual Object Tracking">
                                        <b>[5]</b>
                                         Guo Q,Feng W,Zhou C,&lt;i&gt;et al&lt;/i&gt;.Learning dynamic Siamese network for visual object tracking[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice.New York:IEEE,2017:1763-1771.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_6" title=" Wang Q,Teng Z,Xing J L,&lt;i&gt;et al&lt;/i&gt;.Learning attentions:residual attentional Siamese network for high performance online visual tracking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT.New York:IEEE,2018:4854-4863." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning attentions:residual attentional siamese network for high performance online visual tracking">
                                        <b>[6]</b>
                                         Wang Q,Teng Z,Xing J L,&lt;i&gt;et al&lt;/i&gt;.Learning attentions:residual attentional Siamese network for high performance online visual tracking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT.New York:IEEE,2018:4854-4863.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_7" title=" He A F,Luo C,Tian X M,&lt;i&gt;et al&lt;/i&gt;.A twofold Siamese network for real-time object tracking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:4834-4843." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A twofold siamese network for real-time object tracking">
                                        <b>[7]</b>
                                         He A F,Luo C,Tian X M,&lt;i&gt;et al&lt;/i&gt;.A twofold Siamese network for real-time object tracking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:4834-4843.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_8" title=" Krizhevsky A,Sutskever I,Hinton G E.ImageNet classification with deep convolutional neural networks[J].Communications of the ACM,2017,60(6):84-90." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMD71033EF8EE149F894A1F173A0F662B0&amp;v=MjIxMzNTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxod0xtNXdhOD1OaWZJWThlL0g5SFByUG96Yko1NkRYZ3d1UjRhN2s1OFBuN2xyMk0xRDdTU1I4aWZDT052Rg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Krizhevsky A,Sutskever I,Hinton G E.ImageNet classification with deep convolutional neural networks[J].Communications of the ACM,2017,60(6):84-90.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     He K M,Zhang X Y,Ren S Q,&lt;i&gt;et al&lt;/i&gt;.Deep residual learning for image recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:770-778.</a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_10" title=" Wu Y,Lim J,Yang M H.Online object tracking:a benchmark[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:2411-2418." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online object tracking:A benchmark">
                                        <b>[10]</b>
                                         Wu Y,Lim J,Yang M H.Online object tracking:a benchmark[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:2411-2418.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_11" title=" Wu Y,Lim J,Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(9):1834-1848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">
                                        <b>[11]</b>
                                         Wu Y,Lim J,Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(9):1834-1848.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_12" title=" Wang Q,Gao J,Xing J,&lt;i&gt;et al&lt;/i&gt;.DCFNet:discriminant correlation filters network for visual tracking[J/OL].(2017-04-13)[2019-03-15].https://arxiv.org/abs/1704.04057." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DCFNet:discriminant correlation filters network for visual tracking">
                                        <b>[12]</b>
                                         Wang Q,Gao J,Xing J,&lt;i&gt;et al&lt;/i&gt;.DCFNet:discriminant correlation filters network for visual tracking[J/OL].(2017-04-13)[2019-03-15].https://arxiv.org/abs/1704.04057.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_13" title=" Hong Z B,Chen Z,Wang C H,&lt;i&gt;et al&lt;/i&gt;.MUlti-store tracker (MUSTer):a cognitive psychology inspired approach to object tracking[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:749-758." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-Store Tracker (MUSTer):A cognitive psychology inspired approach to object tracking">
                                        <b>[13]</b>
                                         Hong Z B,Chen Z,Wang C H,&lt;i&gt;et al&lt;/i&gt;.MUlti-store tracker (MUSTer):a cognitive psychology inspired approach to object tracking[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:749-758.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_14" title=" Wang L J,Ouyang W L,Wang X G,&lt;i&gt;et al&lt;/i&gt;.Visual tracking with fully convolutional networks[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:3119-3127." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual Tracking with Fully Convolutional Networks">
                                        <b>[14]</b>
                                         Wang L J,Ouyang W L,Wang X G,&lt;i&gt;et al&lt;/i&gt;.Visual tracking with fully convolutional networks[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:3119-3127.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_15" title=" Girshick R,Donahue J,Darrell T,&lt;i&gt;et al&lt;/i&gt;.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[15]</b>
                                         Girshick R,Donahue J,Darrell T,&lt;i&gt;et al&lt;/i&gt;.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:580-587.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_16" title=" Tao R,Gavves E,Smeulders A W M.Siamese instance search for tracking[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1420-1429." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Siamese Instance Search for Tracking">
                                        <b>[16]</b>
                                         Tao R,Gavves E,Smeulders A W M.Siamese instance search for tracking[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1420-1429.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_17" title=" Danelljan M,Hager G,Khan F S,&lt;i&gt;et al&lt;/i&gt;.Learning spatially regularized correlation filters for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:4310-4318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">
                                        <b>[17]</b>
                                         Danelljan M,Hager G,Khan F S,&lt;i&gt;et al&lt;/i&gt;.Learning spatially regularized correlation filters for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:4310-4318.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_18" title=" Zhang J M,Ma S G,Sclaroff S.MEEM:robust tracking via multiple experts using entropy minimization[M]//Fleet D,Pajdla T,Schiele B,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8694:188-203." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MEEM:Robust tracking via multiple experts using entropy minimization">
                                        <b>[18]</b>
                                         Zhang J M,Ma S G,Sclaroff S.MEEM:robust tracking via multiple experts using entropy minimization[M]//Fleet D,Pajdla T,Schiele B,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8694:188-203.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_19" title=" Danelljan M,Hager G,Khan F S,&lt;i&gt;et al&lt;/i&gt;.Convolutional features for correlation filter based visual tracking[C]//2015 IEEE International Conference on Computer Vision Workshop (ICCVW),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:58-66." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional features for correlation filter based visual tracking">
                                        <b>[19]</b>
                                         Danelljan M,Hager G,Khan F S,&lt;i&gt;et al&lt;/i&gt;.Convolutional features for correlation filter based visual tracking[C]//2015 IEEE International Conference on Computer Vision Workshop (ICCVW),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:58-66.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_20" title=" Qi Y K,Zhang S P,Qin L,&lt;i&gt;et al&lt;/i&gt;.Hedged deep tracking[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:4303-4311." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hedged Deep Tracking">
                                        <b>[20]</b>
                                         Qi Y K,Zhang S P,Qin L,&lt;i&gt;et al&lt;/i&gt;.Hedged deep tracking[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:4303-4311.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_21" title=" Ma C,Huang J B,Yang X K,&lt;i&gt;et al&lt;/i&gt;.Hierarchical convolutional features for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:3074-3082." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical Convolutional Features for Visual Tracking">
                                        <b>[21]</b>
                                         Ma C,Huang J B,Yang X K,&lt;i&gt;et al&lt;/i&gt;.Hierarchical convolutional features for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:3074-3082.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(09),253-261 DOI:10.3788/AOS201939.0915003            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于孪生神经网络在线判别特征的视觉跟踪算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%87%E7%A5%9D%E4%BB%A4&amp;code=42914570&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">仇祝令</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9F%A5%E5%AE%87%E9%A3%9E&amp;code=20011812&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">查宇飞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B1%E9%B9%8F&amp;code=41927086&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朱鹏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E6%95%8F&amp;code=38154387&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴敏</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A9%BA%E5%86%9B%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E8%88%AA%E7%A9%BA%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0274788&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空军工程大学航空工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于孪生神经网络的跟踪算法是利用离线训练的网络提取目标的特征并进行匹配,从而实现跟踪。在离线训练过程中,网络学到的是相似目标的通用特征,因此当有相似目标干扰时,用这种通用特征表达特定目标将会导致跟踪性能下降,甚至丢失目标。为提高对相似目标的判别能力,通过在线更新网络参数,使网络能够在通用特征的基础上,进一步学到当前目标的特定特征,这样不仅能有效地区分目标与背景,还能消除相似目标的干扰。实验在OTB50和OTB100数据库上进行,结果表明该算法可以提高对网络提取特征的判别力,实现对目标的稳健性跟踪。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E8%BF%BD%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉追踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A6%BB%E7%BA%BF%E8%AE%AD%E7%BB%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">离线训练;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%A8%E7%BA%BF%E6%9B%B4%E6%96%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">在线更新;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    查宇飞,E-mail:2530858535@qq.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-13</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61773397,61703423,61701524);</span>
                    </p>
            </div>
                    <h1><b>Visual Tracking Algorithm Based on Online Feature Discrimination with Siamese Network</b></h1>
                    <h2>
                    <span>Qiu Zhuling</span>
                    <span>Zha Yufei</span>
                    <span>Zhu Peng</span>
                    <span>Wu Min</span>
            </h2>
                    <h2>
                    <span>Aeronautics Engineering College, Air Force Engineering University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Tracking algorithms with Siamese network use the offline training network to extract features from the target for matching and tracking. In the offline training process, the network learns the common features of similar goals. In the case of interference from similar targets, using common features to express specific targets will lead to degradation of tracking performance and even loss of targets. To improve the feature discriminative ability for similar targets, we update the parameters of network online, and make the network further learn the specific characteristics of the current target based on the common features. The proposed method can not only effectively distinguish the target and background, but also eliminate interference from similar targets. We conduct a large number of experiments on the OTB50 and OTB100 databases. The results show that the proposed algorithm can improve the discriminative ability to features extracted by the network and achieve robust tracking of the target.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visual%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visual tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=offline%20training&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">offline training;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=online%20update&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">online update;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-13</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="52">作为计算机视觉研究领域的重要课题,目标跟踪具有广泛的应用前景,在过去几十年一直备受关注,并取得长足的发展。卷积神经网络因学习获得的深度特征具有强大的目标表示能力,逐渐取代传统的手工特征,在图像识别和目标检测等任务中取得突破性进展,近几年被引入到目标跟踪任务中,用于提高跟踪的稳健性和准确性<citation id="123" type="reference"><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="53">最近,基于孪生神经网络的目标跟踪方法在一些流行的跟踪数据库和竞赛中展现了优秀的性能。这些方法通过一个Y型网络<citation id="124" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>,基于超大的视频目标检测(VID)<citation id="125" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>数据库和离线训练网络,学习目标的通用特征表示,这种表示同时具有度量图像间相似程度的作用,在后续的图像帧中通过判别与初始帧目标最相似的区域,从而稳健地估计目标的状态。但是,在跟踪过程中,因网络参数巨大和训练数据缺乏,无法在线更新网络以适应目标在时域上的变化,在目标周围出现相似目标时,跟踪性能下降,甚至会丢失目标。</p>
                </div>
                <div class="p1">
                    <p id="54">导致上述现象的一个主要原因在于网络所学到的特征对时域变化目标判别力不够。相似性匹配算法大都通过相似性匹配训练网络,利用最后一层卷积层来提取目标的语义信息,使相似目标具有类似的语义信息。然而此类算法只采用离线训练网络,并没有实现网络的在线更新。因此,网络只学到了相似目标的通用特征,当目标周围出现相似的干扰项时,离线训练所学到的通用特征无法将其进行区分,即无法实现对特定目标的表达。</p>
                </div>
                <div class="p1">
                    <p id="55">因此,人们尝试利用在线训练网络来提升目标跟踪的准确性。与基于端到端学习型相关滤波器(CFNet)<citation id="126" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>直接对在跟踪过程中网络提取的特征进行多项式拟合来表示目标不同,Guo等<citation id="127" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出动态孪生网络(Dsiam),通过在线学习目标外观变化和背景抑制,提高跟踪性能。Wang等<citation id="128" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>通过U形网络(RASNet)保留目标更多细节信息。双重孪生网络的视觉物体跟踪方法<citation id="129" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>是利用图像识别模型来弥补相似模型的不足,从而提高所学特征的判别力。但是,这些方法都是利用训练好的深度特征来描述语义目标,而训练样本中并不包含跟踪目标,导致特征对当前跟踪目标的表达力不够,特别是当背景中含有相似语义目标时,跟踪器将无法鉴别,导致跟踪失败。为提高网络对特定目标的判别能力,本文直接利用跟踪结果对深度特征进行增量学习,在跟踪过程中直接增加训练步骤,以获得表示当前特定目标判别力更强的深度特征,从而能够区分跟踪目标和相似背景。</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag">2 本文方法</h3>
                <div class="p1">
                    <p id="57">基于全卷积孪生网络的目标跟踪(SiamFC)<citation id="130" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>算法采用大量离线数据训练网络参数,学习相似目标之间的通用特征。在线跟踪中,采用此种通用特征来实现对目标的表示,其损失函数为</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">{</mo></mstyle><mi>L</mi><mo stretchy="false">[</mo><mi>v</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo>,</mo><mspace width="0.25em" /><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">式中:<i>N</i>为正样本个数;<i>v</i>为由目标模板<i>x</i>和搜索区域<i>z</i>得到的响应,即<i>v</i>=<i>f</i>(<i>x</i><sub><i>i</i></sub>)*<i>f</i>(<i>z</i><sub><i>i</i></sub>),<i>f</i>(<i>x</i><sub><i>i</i></sub>)为目标模板<i>x</i>经最后一层卷积层得到的特征,<i>f</i>(<i>z</i><sub><i>i</i></sub>)为搜索区域<i>z</i>经最后一层卷积层得到的特征,*为卷积;<i>y</i><sub><i>i</i></sub>为样本所对应的标签,<i>y</i><sub><i>i</i></sub>∈{+1,-1}。</p>
                </div>
                <div class="p1">
                    <p id="60">通过离线训练,SiamFC算法学到了相似目标之间的共性。但是,目标跟踪任务是对当前特定目标进行跟踪,单独的通用特征只能反映当前目标的共性部分,对当前目标的特定部分有所忽视。</p>
                </div>
                <div class="area_img" id="61">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909030_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 网络在线更新流程图。" src="Detail/GetImg?filename=images/GXXB201909030_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 网络在线更新流程图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909030_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Flow chart of network online update.</p>
                                <p class="img_note">(a）网络训练模型；（b）通用特征响应；（c）特定特征响应</p>
                                <p class="img_note">(a)Network training model;(b)response of common features;(c)response of special features</p>

                </div>
                <div class="p1">
                    <p id="62">SiamFC算法通过离线训练目标模型,当目标周围出现干扰选项时无法区分。本文算法以SiamFC算法为基础算法,结合离线训练的通用特征和在线更新的特定特征,以提高对特定目标的表达能力。离线训练是通过大量数据训练网络参数,学习相似目标之间的通用特征;在线更新则是根据当前目标训练网络,提取当前目标的特定特征,进而更新目标模型,提高目标跟踪的准确度。图1为在线更新网络结构图。其中,图1(a)表示网络训练模型,<i>x</i>通过首帧目标框确定,输入大小为125×125×3;<i>z</i>为不同帧数所保存的正样本,大小为255×255×3。将<i>x</i>和<i>z</i>输入网络中,通过在目标模板与搜索区域的网络C1层、C2层和C3层中提取相似目标之间的通用特征得到<i>f</i>(<i>x</i>)和<i>f</i>(<i>z</i>)。利用卷积层C4层和C5层提取相似目标的特定特征得到<i>g</i>(<i>x</i>)和<i>g</i>(<i>z</i>),将所获得的特定特征进行卷积得到模板在搜索区域上的响应图,利用正样本来提取深度层次特征,基于得到的响应与样本标签可实现对网络参数的更新。图1(b)表示对相似目标的通用特征进行相关卷积得到的响应图,图1(c)表示对特定特征进行相关卷积得到的响应图。</p>
                </div>
                <h4 class="anchor-tag" id="63" name="63"><b>2.1 通用特征学习</b></h4>
                <div class="p1">
                    <p id="64">本文网络为拥有五层卷积层的深度卷积图像分类神经网络(AlexNet)<citation id="131" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>,在离线训练过程中,采用与SiamFC算法类似的思路,利用VID数据库实现对网络参数的训练,学到相似目标间的通用特征。损失函数<i>L</i>为逻辑回归函数。在跟踪过程中,保持前三层卷积层参数不变,用来提取目标的共性特征<i>f</i>(<i>x</i>)和<i>f</i>(<i>z</i>)。</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>v</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo>=</mo><mi>R</mi><mo stretchy="false">[</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>,</mo><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">式中:<i>f</i>(<i>x</i>)为目标模板<i>x</i>经过前三层卷积层得到的通用特征;<i>f</i>(<i>z</i>)为搜索区域<i>z</i>经过前三层卷积层得到的通用特征;<i>v</i><sub>c</sub>为目标模板与搜索区域经过<i>R</i>(·)相关得到的响应。</p>
                </div>
                <div class="p1">
                    <p id="67">由图1可知,通用特征可以提取目标模板<i>x</i>与搜索区域<i>z</i>中相似的部分,实现跟踪目标与背景的初步区分。从图1(b)可知:根据对通用特征进行相关卷积运算得到的响应图可以区分目标与背景,但不能有效区分有相似干扰时的特定目标。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68"><b>2.2 特定特征学习</b></h4>
                <div class="p1">
                    <p id="69">离线训练学习的是相似目标间的通用特征,但通用特征会对特定目标和相似干扰项同时产生较大的响应,无法进行区分。本文以离线训练好的网络为基础,根据样本在线训练卷积层C4、C5的参数,通过离线训练和在线更新,可以同时表达目标的共性与个性。</p>
                </div>
                <div class="p1">
                    <p id="70">输入首帧的目标模板<i>x</i>,算法的搜索区域<i>z</i>。先将<i>x</i>和<i>z</i>输入网络中,经过固定参数的前三层卷积层提取通用特征<i>f</i>(<i>x</i>)和<i>f</i>(<i>z</i>),将通用特征<i>f</i>(<i>x</i>)和<i>f</i>(<i>z</i>)经后两层卷积层学习当前目标的个性,得到特定特征<i>g</i>(<i>x</i>)和<i>g</i>(<i>z</i>)。</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>v</mi><msub><mrow></mrow><mtext>p</mtext></msub><mo>=</mo><mi>R</mi><mo stretchy="false">[</mo><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>,</mo><mi>g</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">式中,<i>v</i><sub>p</sub>为根据目标模板与搜索区域得到的响应值。将响应值与标签同时代入损失函数<i>L</i>中,实现对后两层卷积层参数的更新。</p>
                </div>
                <div class="p1">
                    <p id="73">在线跟踪中,在通用特征的基础上利用正样本来学习特定目标的特定特征,进行相关卷积得到其响应图,实现对特定目标的有效区分。其损失函数为</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">{</mo></mstyle><mi>L</mi><mo stretchy="false">(</mo><mi>v</mi><msub><mrow></mrow><mtext>p</mtext></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">}</mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">由图1(c)可知,经过最后一层卷积层得到的特征可以实现目标模板与背景的区分,目标模板只对搜索区域中属于目标的部分进行响应,所得到的响应图只包含特定目标,从而实现对目标的区分。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76"><b>2.3 网络在线更新</b></h4>
                <div class="p1">
                    <p id="77">在跟踪的初始阶段,根据首帧目标框绝对准确的特点,在首帧样本中提取正负样本,可以确定训练样本的准确性。但是在跟踪阶段,仅采用首帧样本远远满足不了更新的要求,因为首帧样本数量太少,并且没有目标和背景的变化。而高分样本是通过设定阈值,在跟踪精确度的帧中提取目标样本并保存,这样可以有效提高样本的数目,并对样本的准确性也有一定的保证。</p>
                </div>
                <div class="p1">
                    <p id="78">考虑到跟踪的效率,每帧更新在很大程度上会对跟踪速度产生较大的影响。为减小在线更新对跟踪速度的影响,本文主要采用首帧更新、间隔更新和失败更新三种方式。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79">1) 首帧更新。</h4>
                <div class="p1">
                    <p id="80">在首帧利用首帧模板<i>x</i><sub>f</sub>和搜索区域<i>z</i><sub>f</sub>实现对网络参数<i>w</i><sub>f</sub>的更新,可使网络初步学习当前目标的个性,对后续更快地学习目标个性具有较大作用。</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mtext>f</mtext></msub><mo>=</mo><mi>U</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mtext>f</mtext></msub><mo>,</mo><mi>z</mi><msub><mrow></mrow><mtext>f</mtext></msub><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">式中,<i>w</i><sub>f</sub>为首帧更新后的网络参数,<i>U</i>(·)是更新网络操作。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83">2）间隔更新。</h4>
                <div class="p1">
                    <p id="145">利用高分样本并采用间隔若干帧的方式来更新网络参数：</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>U</mi></mstyle><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mtext>f</mtext></msub><mo>,</mo><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mspace width="0.25em" /><mspace width="0.25em" /><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mspace width="0.25em" /><mspace width="0.25em" /><mi>Τ</mi><mo>=</mo><mi>Μ</mi><mo>×</mo><mi>Κ</mi><mspace width="0.25em" /><mspace width="0.25em" /></mtd></mtr><mtr><mtd><mo stretchy="false">(</mo><mi>Κ</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mo>⋯</mo><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">式中:<i>w</i><sub><i>i</i></sub>为间隔更新后的网络参数;<i>z</i><sub><i>i</i></sub>为第<i>i</i>个样本的搜索区域;<i>T</i>为当前帧数;<i>M</i>为更新间隔;<i>K</i>为更新次数。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86">3) 失败更新。</h4>
                <div class="p1">
                    <p id="87">当跟踪失败时,说明当前网络对目标的表达能力不够,无法将目标与背景进行有效区分,在此时利用高分样本进行更新,可以提高网络对目标的表达力。</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mtext>a</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>U</mi></mstyle><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mtext>f</mtext></msub><mo>,</mo><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mspace width="0.25em" /><mspace width="0.25em" /><mspace width="0.25em" /><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mspace width="0.25em" /><mi>v</mi><msub><mrow></mrow><mtext>p</mtext></msub><mo>&lt;</mo><mi>Q</mi><mo>,</mo><mspace width="0.25em" /><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">式中:<i>w</i><sub>a</sub>为失败更新后的网络参数;<i>Q</i>为判断跟踪失败的阈值。通过在线更新网络参数,在浅层网络提取的通用特征基础上,精调深层网络以适应目标变化,进一步消除相似目标的干扰。</p>
                </div>
                <div class="p1">
                    <p id="90">图2为本文算法与SiamFC算法的对比图,选取basketball、girl2、liquor 3个具有相似干扰的序列。第二行是SiamFC算法对三个序列的目标响应图,当目标周围出现相似干扰项时,SiamFC算法中所学到的特征无法很好地区分目标与干扰项。第三行是本文算法的目标响应图,从中可以看出通过增加网络的在线更新,利用通用特征与特定特征之间的互补,能够有效提高网络对特定目标的判别力,从而可以很好地区分目标与背景。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909030_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 相似干扰序列中跟踪目标响应图。" src="Detail/GetImg?filename=images/GXXB201909030_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 相似干扰序列中跟踪目标响应图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909030_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Tracking target responses in similar interference sequence.</p>
                                <p class="img_note">(a）篮球；（b）女孩；（c）酒</p>
                                <p class="img_note">(a)Basketball;(b)girl2;(c)liquor</p>

                </div>
                <h3 id="92" name="92" class="anchor-tag">3 实验分析</h3>
                <div class="p1">
                    <p id="93">实验的硬件环境为multi-core CPU、NVIDIA 1080TI GPU,软件环境为MATLAB 2017a。离线训练的数据是根据用于检测任务的视频序列ILSVRC2015<citation id="132" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>进行改造得到的专门用于训练孪生网络的数据。训练网络为拥有五层卷积层的AlexNet,学习率<i>η</i>=0.001。在第一帧初始化时,采用较多的迭代次数(150次)训练模型,使其能够收敛;在跟踪过程中,由于目标变化较小,采用较少的迭代次数(5～10次)就能够使模型收敛;离线训练过程中,实现对五层卷积层参数的更新,在线更新过程中,固定前三层卷积层的参数,实现对后两层卷积层参数的更新。测试视频来自于当前目标跟踪领域最常用的OTB50<citation id="133" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和OTB100<citation id="134" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>数据库。</p>
                </div>
                <h4 class="anchor-tag" id="94" name="94"><b>3.1 参数设置标准</b></h4>
                <div class="p1">
                    <p id="95">为证明增加模型更新对算法的提升,在OTB100数据库中进行实验,并分别就不同的更新条件(首帧更新、间隔更新、失败更新和整体性能)对算法进行说明。其中,首帧更新是采用首帧的目标模板和搜索区域对网络参数进行训练;间隔更新是每间隔30帧利用保存的样本对网络参数进行更新;失败更新是当出现跟踪失败的情况时对网络参数进行更新;整体性能是同时采用首帧更新、间隔更新和失败更新后得到的实验结果。经过多次实验得到失败更新中的阈值<i>Q</i>=0.3。</p>
                </div>
                <div class="p1">
                    <p id="96">图3中,基准算法为SiamFC算法,OSFC_F是在SiamFC算法的基础上增加首帧更新,OSFC_FI是在SiamFC算法的基础上增加首帧更新和间隔更新策略,OSFC_FF是在SiamFC算法的基础上增加首帧更新和失败更新,OSFC_OP是在SiamFC算法的基础上增加首帧更新、间隔更新和失败更新。从图中可知,只采用首帧更新的实验结果得分较低,精确率和成功率只能达到0.669和0.491。当增加间隔更新后,通过30帧间隔对网络参数进行更新,实验结果有了很大的提升,精确率和成功率分别达到了0.810和0.587,说明使用时间信息更新网络参数可以使网络模型更加适应目标的时域变化。在首帧更新的基础上增加失败更新,利用之前保存的正样本对网络模型进行更新,提高网络模型对正负样本的判别能力,精确率和成功率分别达到了0.795和0.579。当同时使用首帧更新、间隔更新和失败更新时,算法的精确度和成功率最高,分别为0.833和0.609,在SiamFC算法的基础上分别提高了8.5%和5%。在线更新方式的引进使得跟踪性能得到提升,但是更新的方法需要采用梯度下降法对新收集到的样本进行迭代训练和网络模型在线更新,这将会导致更新时间与解析解相比所需要的时间更长,使得跟踪速度有所下降。</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909030_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同更新策略下的精确率与成功率对比图。" src="Detail/GetImg?filename=images/GXXB201909030_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 不同更新策略下的精确率与成功率对比图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909030_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Comparison of precision and success rates under different update strategies.</p>
                                <p class="img_note">(a）精确率图；（b）成功率图</p>
                                <p class="img_note">(a)Plot of precision rate;(b)plot of success rate</p>

                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>3.2 OTB数据库结果分析</b></h4>
                <h4 class="anchor-tag" id="100" name="100">3.2.1 总体性能分析</h4>
                <div class="p1">
                    <p id="101">选取OTB50和OTB100数据库作为测试数据库,分别与其他算法进行比较与分析。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">1) OTB50基准数据库。</h4>
                <div class="p1">
                    <p id="103">选取基于判别式相关滤波网络的视觉跟踪(DCFNet)<citation id="135" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、 基于认知心理学的目标跟踪(MUSTer)<citation id="136" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、基于完全卷积网络的视觉跟踪(FCNT)<citation id="137" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、基于卷积神经网络学习判别显著图的在线跟踪(CNN-SVM)<citation id="138" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、 孪生网络实例跟踪研究(SINT)<citation id="139" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、基于学习型空间正则化相关滤波的视觉跟踪(SRDCF)<citation id="140" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、多专家跟踪(MEEM)<citation id="141" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、SiamFC算法与本文算法进行对比。其中,DCFNet、SINT、FCNT是基于孪生网络的算法,SiamFC3s是在3尺度条件下SiamFC算法公布的实验结果,SiamFC是根据公布的代码在5尺度条件下得到的实验结果,同时也是本文的基准算法。由图4可知,本文算法在SiamFC的基础上实现了精确度为11%,成功率为7.1%的提高。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">2) OTB100基准数据库。</h4>
                <div class="p1">
                    <p id="105">OTB100数据库是在OTB50数据库上改进的数据库,为证明本文算法的有效性,在OTB100数据库上也进行了对比实验,对比算法有基于深度学习型空间正则化相关滤波的视觉跟踪(DeepSRDCF)<citation id="142" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、 对冲深度跟踪(HDT)<citation id="143" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、 基于分层卷积特征的视觉跟踪(HCF)<citation id="144" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、 DCFNet、CNN-SVM、SRDCF、 MEEM、 SiamFC算法。由图5表明,本文算法的精确度为0.833,成功率为0.609,在SiamFC算法的基础上分别提高了8.5%和5%。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106">3.2.2 属性分析</h4>
                <div class="p1">
                    <p id="107">为全面评估跟踪算法在不同难点属性上的性能,图6给出了各个跟踪器在各个难点属性下精确率的分析曲线。通过分析曲线可以看出,在8个难点属性里,本文算法在平面旋转、低分辨率、运动模糊、遮挡、超出视野、平面外旋转、光照变化、尺度变化条件下取得较好的成绩,分别达到了0.829,0.957,0.790,0.834,0.798,0.860,0.830,0.898,并且相比于SiamFC算法有了较大的提高。由图6可知,通过在线模型更新,提高模型的特征表达能力,本文算法在大部分环境下的稳健性均有所提高。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909030_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同算法在OTB50视频库中的精确率与成功率对比图。" src="Detail/GetImg?filename=images/GXXB201909030_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同算法在OTB50视频库中的精确率与成功率对比图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909030_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Comparison of precision and success rates of different algorithms in the OTB50video library.</p>
                                <p class="img_note">(a）精确率图；（b）成功率图</p>
                                <p class="img_note">(a)Plot of precision rate;(b)plot of success rate</p>

                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909030_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同算法在OTB100视频库中的精确率与成功率对比图。" src="Detail/GetImg?filename=images/GXXB201909030_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同算法在OTB100视频库中的精确率与成功率对比图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909030_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Comparison of precision and success rates of different algorithms in the OTB100video library.</p>
                                <p class="img_note">(a）精确率图；（b）成功率图</p>
                                <p class="img_note">(a)Plot of precision rate;(b)plot of success rate</p>

                </div>
                <h4 class="anchor-tag" id="111" name="111">3.2.3 定性分析</h4>
                <div class="p1">
                    <p id="112">离线训练学习相似目标之间的共性特征,在线更新学习当前目标的个性特征,通过结合目标的共性特征和个性特征,可以提高对当前目标的判别能力。为了证明这种判定能力,从OTB100数据库中选取6个具有各种跟踪难点的视频序列,与DCFNet、 HCF、CNN-SVM、SiamFC3s、 SiamFC进行对比验证。</p>
                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909030_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同难度属性下的各算法跟踪结果的精确率曲线图" src="Detail/GetImg?filename=images/GXXB201909030_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同难度属性下的各算法跟踪结果的精确率曲线图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909030_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Precision rate graph of each algorithm under different difficulty attributes</p>

                </div>
                <div class="p1">
                    <p id="115">由图6可知,通过与各算法进行对比,发现本文算法对各跟踪难点都比较稳健,特别是在处理相似目标的问题方面。为了可视化显示跟踪效果,本文结合部分测试视频的难点属性和部分视频的部分跟踪结果进行针对性分析,如图7所示。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909030_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 各算法在不同难度属性的视频中的实际追踪效果图" src="Detail/GetImg?filename=images/GXXB201909030_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 各算法在不同难度属性的视频中的实际追踪效果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909030_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Actual tracking effect of each algorithm for vidios with different difficulty attributes</p>

                </div>
                <div class="p1">
                    <p id="117">对于basketball测试视频的跟踪,其主要难点在于相似目标的干扰。参与测评的跟踪器在第723帧开始产生分化,此时目标周围有较为密集的相似干扰,并且有部分相似的背景对目标有了部分遮盖。CNN-SVM和SiamFC3s都直接跟丢了目标,其他跟踪器都有不同程度地偏离目标或者是有跟踪效果变差的情况出现。而本文算法由于模型更新方式的改进,在面对较多相似的干扰目标时依旧能达到优异的效果。</p>
                </div>
                <div class="p1">
                    <p id="118">对于girl2测试视频的跟踪,其主要难点在于背景中的人物对于目标的遮挡和相似干扰。由于相似的遮挡出现,除了本算法和CNN-SVM之外的算法都同时跟丢了目标,CNN-SVM的目标框大小并不能精准地定位目标。在后续帧中除HCF和DCFNet之外的其他算法都找回了目标,但是跟踪效果如第640帧中所显示,效果都受到了不同程度的影响。但是本文算法跟踪效果一直较好,在较大遮挡的情况下依旧能够有非常出色的跟踪效果。</p>
                </div>
                <div class="p1">
                    <p id="119">对于liquor测试视频的跟踪,其主要难点是场景中其他背景对目标产生的相似干扰以及目标本身的快速移动,由于难度属性的叠加,本测试视频对于跟踪器的要求相对较高,在第388帧中由于目标发生快速移动并且超出了边框,SiamFC算法跟踪器直接跟丢目标,并且各个算法都出现了跟踪效果变差的情况。在第893帧中,背景中出现相似干扰,只有本文算法和CNN-SVM还有较为优异的跟踪效果,其他的算法都产生了跟丢或是效果变差的现象。说明本文算法对于快速移动也有很好的跟踪效果。</p>
                </div>
                <div class="p1">
                    <p id="120">对于twinnings测试视频的跟踪,其主要难点是场景中复杂的背景对目标跟踪的较大干扰。在第15帧中所有的跟踪算法都有较好的效果,但是在第232帧中,由于目标开始移动,此时背景对于目标的干扰开始加强,各算法的跟踪效果开始分化,DCFNet和CNN-SVM的跟踪效果开始变差,并开始脱离目标。在第484帧中,除了本文算法,其他算法的跟踪效果都有不同程度地变差,并且其他算法的目标框都发生偏移或是尺度与目标不符合的情况。说明本文算法在复杂背景下也能有很好的跟踪效果。</p>
                </div>
                <h3 id="121" name="121" class="anchor-tag">4 结  论</h3>
                <div class="p1">
                    <p id="122">目前,基于相似性匹配算法仅使用离线训练网络参数来学习相似目标之间的一般相似性。在在线跟踪过程中,由于跟踪目标是特定目标,因此一般特征无法实现对当前特定目标的表达,会受到周围相似目标的干扰。基于这些问题,增加基于相似性匹配算法的网络在线更新,利用跟踪结果对网络的最后两层进行增量学习,网络基于通用特征学习当前特定目标的特定特征,并将通用特征与特定特征相结合,实现对特定目标的表达。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="9">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201705025&amp;v=MTc0ODVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXprVmI3UElqWFRiTEc0SDliTXFvOUhZWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Li S S,Zhao G P,Wang J Y.Distractor-aware object tracking based on multi-feature fusion and scale-adaption[J].Acta Optica Sinica,2017,37(5):0515005.李双双,赵高鹏,王建宇.基于特征融合和尺度自适应的干扰感知目标跟踪[J].光学学报,2017,37(5):0515005.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201810020&amp;v=MTM2MzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXprVmI3UElqWFRiTEc0SDluTnI0OUhaSVE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Li Z D,Zhong Y,Chen M,<i>et al</i>.Fast face image retrieval based on depth feature[J].Acta Optica Sinica,2018,38(10):1010004.李振东,钟勇,陈蔓,等.基于深度特征的快速人脸图像检索方法[J].光学学报,2018,38(10):1010004.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully-convolutional siamese networks for object tracking">

                                <b>[3]</b> Bertinetto L,Valmadre J,Henriques J F,<i>et al</i>.Fully-convolutional Siamese networks for object tracking[M]//Hua G,Jégou H.Computer vision-ECCV 2016 workshops.Lecture notes in computer science.Cham:Springer,2016,9914:850-865.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-End Representation Learning for Correlation Filter Based Tracking">

                                <b>[4]</b> Valmadre J,Bertinetto L,Henriques J,<i>et al</i>.End-to-end representation learning for correlation filter based tracking[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI.New York:IEEE,2017:2805-2813.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Dynamic Siamese Network for Visual Object Tracking">

                                <b>[5]</b> Guo Q,Feng W,Zhou C,<i>et al</i>.Learning dynamic Siamese network for visual object tracking[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice.New York:IEEE,2017:1763-1771.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning attentions:residual attentional siamese network for high performance online visual tracking">

                                <b>[6]</b> Wang Q,Teng Z,Xing J L,<i>et al</i>.Learning attentions:residual attentional Siamese network for high performance online visual tracking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT.New York:IEEE,2018:4854-4863.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A twofold siamese network for real-time object tracking">

                                <b>[7]</b> He A F,Luo C,Tian X M,<i>et al</i>.A twofold Siamese network for real-time object tracking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:4834-4843.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMD71033EF8EE149F894A1F173A0F662B0&amp;v=Mjk3NjZOdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxod0xtNXdhOD1OaWZJWThlL0g5SFByUG96Yko1NkRYZ3d1UjRhN2s1OFBuN2xyMk0xRDdTU1I4aWZDTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Krizhevsky A,Sutskever I,Hinton G E.ImageNet classification with deep convolutional neural networks[J].Communications of the ACM,2017,60(6):84-90.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 He K M,Zhang X Y,Ren S Q,<i>et al</i>.Deep residual learning for image recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:770-778.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online object tracking:A benchmark">

                                <b>[10]</b> Wu Y,Lim J,Yang M H.Online object tracking:a benchmark[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:2411-2418.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">

                                <b>[11]</b> Wu Y,Lim J,Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(9):1834-1848.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DCFNet:discriminant correlation filters network for visual tracking">

                                <b>[12]</b> Wang Q,Gao J,Xing J,<i>et al</i>.DCFNet:discriminant correlation filters network for visual tracking[J/OL].(2017-04-13)[2019-03-15].https://arxiv.org/abs/1704.04057.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-Store Tracker (MUSTer):A cognitive psychology inspired approach to object tracking">

                                <b>[13]</b> Hong Z B,Chen Z,Wang C H,<i>et al</i>.MUlti-store tracker (MUSTer):a cognitive psychology inspired approach to object tracking[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:749-758.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual Tracking with Fully Convolutional Networks">

                                <b>[14]</b> Wang L J,Ouyang W L,Wang X G,<i>et al</i>.Visual tracking with fully convolutional networks[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:3119-3127.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[15]</b> Girshick R,Donahue J,Darrell T,<i>et al</i>.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:580-587.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Siamese Instance Search for Tracking">

                                <b>[16]</b> Tao R,Gavves E,Smeulders A W M.Siamese instance search for tracking[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1420-1429.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">

                                <b>[17]</b> Danelljan M,Hager G,Khan F S,<i>et al</i>.Learning spatially regularized correlation filters for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:4310-4318.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MEEM:Robust tracking via multiple experts using entropy minimization">

                                <b>[18]</b> Zhang J M,Ma S G,Sclaroff S.MEEM:robust tracking via multiple experts using entropy minimization[M]//Fleet D,Pajdla T,Schiele B,<i>et al</i>.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8694:188-203.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional features for correlation filter based visual tracking">

                                <b>[19]</b> Danelljan M,Hager G,Khan F S,<i>et al</i>.Convolutional features for correlation filter based visual tracking[C]//2015 IEEE International Conference on Computer Vision Workshop (ICCVW),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:58-66.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hedged Deep Tracking">

                                <b>[20]</b> Qi Y K,Zhang S P,Qin L,<i>et al</i>.Hedged deep tracking[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:4303-4311.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical Convolutional Features for Visual Tracking">

                                <b>[21]</b> Ma C,Huang J B,Yang X K,<i>et al</i>.Hierarchical convolutional features for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:3074-3082.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201909030" />
        <input id="dpi" type="hidden" value="200" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201909030&amp;v=Mjc5MTNvOUdaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXprVmI3UElqWFRiTEc0SDlqTXA=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

