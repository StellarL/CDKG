

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133152727158750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201909032%26RESULT%3d1%26SIGN%3dvVvRNImFuBqhMmnQy%252f8V1f0BZ9g%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201909032&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201909032&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201909032&amp;v=MDgwMjRSTE9lWmVWdkZ5emtWYnZMSWpYVGJMRzRIOWpNcG85R1pvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#69" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#74" data-title="2 基本原理 ">2 基本原理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="&lt;b&gt;2.1 基于U-Net的全卷积骨干网络&lt;/b&gt;"><b>2.1 基于U-Net的全卷积骨干网络</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;2.2 双注意力模块&lt;/b&gt;"><b>2.2 双注意力模块</b></a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;2.3 循环卷积模块&lt;/b&gt;"><b>2.3 循环卷积模块</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#115" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#116" data-title="&lt;b&gt;3.1 数据集与评价准则&lt;/b&gt;"><b>3.1 数据集与评价准则</b></a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;3.2 算法实现细节&lt;/b&gt;"><b>3.2 算法实现细节</b></a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;3.3 实验结果比较与分析&lt;/b&gt;"><b>3.3 实验结果比较与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#153" data-title="4 结  论 ">4 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#76" data-title="图1 算法总体结构图">图1 算法总体结构图</a></li>
                                                <li><a href="#82" data-title="图2 双注意力模块内部结构图">图2 双注意力模块内部结构图</a></li>
                                                <li><a href="#90" data-title="图3 多尺度感受野示意图">图3 多尺度感受野示意图</a></li>
                                                <li><a href="#106" data-title="图4 多特征循环卷积模块内部结构图">图4 多特征循环卷积模块内部结构图</a></li>
                                                <li><a href="#121" data-title="图5 多特征循环卷积模块连接示意图">图5 多特征循环卷积模块连接示意图</a></li>
                                                <li><a href="#142" data-title="图6 主观视觉效果对比图">图6 主观视觉效果对比图</a></li>
                                                <li><a href="#143" data-title="表1 客观量化指标对比">表1 客观量化指标对比</a></li>
                                                <li><a href="#145" data-title="图7 不同算法在ECSSD-250测试集下的特征曲线。">图7 不同算法在ECSSD-250测试集下的特征曲线。</a></li>
                                                <li><a href="#146" data-title="图8 不同算法在HKU-IS-1447测试集下的特征曲线。">图8 不同算法在HKU-IS-1447测试集下的特征曲线。</a></li>
                                                <li><a href="#147" data-title="图9 7种算法在DUT-OMRON-1500测试集下的特征曲线。">图9 7种算法在DUT-OMRON-1500测试集下的特征曲线。</a></li>
                                                <li><a href="#151" data-title="表2 各算法的运行时间">表2 各算法的运行时间</a></li>
                                                <li><a href="#152" data-title="表3 算法参数量统计">表3 算法参数量统计</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Gao Y,Wang M,Zha Z J,&lt;i&gt;et al&lt;/i&gt;.Visual-textual joint relevance learning for tag-based social image search[J].IEEE Transactions on Image Processing,2013,22(1):363-376." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual-Textual Joint Relevance Learning for Tag-Based Social Image Search">
                                        <b>[1]</b>
                                         Gao Y,Wang M,Zha Z J,&lt;i&gt;et al&lt;/i&gt;.Visual-textual joint relevance learning for tag-based social image search[J].IEEE Transactions on Image Processing,2013,22(1):363-376.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Guo C L,Zhang L M.A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression[J].IEEE Transactions on Image Processing,2010,19(1):185-198." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression">
                                        <b>[2]</b>
                                         Guo C L,Zhang L M.A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression[J].IEEE Transactions on Image Processing,2010,19(1):185-198.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" Guo C L,Ma Q,Zhang L M.Spatio-temporal saliency detection using phase spectrum of quaternion Fourier transform[C]//2008 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),June 23-28,2008,Anchorage,AK,USA.New York:IEEE,2008:4587715." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatio-temporal saliency detection using phase spectrum of quater-nion fourier transform">
                                        <b>[3]</b>
                                         Guo C L,Ma Q,Zhang L M.Spatio-temporal saliency detection using phase spectrum of quaternion Fourier transform[C]//2008 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),June 23-28,2008,Anchorage,AK,USA.New York:IEEE,2008:4587715.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Courty N,Marchand E.Visual perception based on salient features[C]//Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003),October 27-31,2003,Las Vegas,Nevada,USA.New York:IEEE,2003:1024-1029." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual perception based on salient features">
                                        <b>[4]</b>
                                         Courty N,Marchand E.Visual perception based on salient features[C]//Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003),October 27-31,2003,Las Vegas,Nevada,USA.New York:IEEE,2003:1024-1029.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" Itti L,Koch C,Niebur E.A model of saliency-based visual attention for rapid scene analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1998,20(11):1254-1259." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A model of saliency-based visual attention for rapid scene analysis">
                                        <b>[5]</b>
                                         Itti L,Koch C,Niebur E.A model of saliency-based visual attention for rapid scene analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1998,20(11):1254-1259.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Liu D M,Chang F L.Coarse-to-fine saliency detection based on non-subsampled contourlet transform enhancement[J].Acta Optica Sinica,2019,39(1):0115003.刘冬梅,常发亮.基于非下采样轮廓小波变换增强的从粗到精的显著性检测[J].光学学报,2019,39(1):0115003." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201901034&amp;v=MzI0NzFNcm85R1lJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5emtWYnZMSWpYVGJMRzRIOWo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Liu D M,Chang F L.Coarse-to-fine saliency detection based on non-subsampled contourlet transform enhancement[J].Acta Optica Sinica,2019,39(1):0115003.刘冬梅,常发亮.基于非下采样轮廓小波变换增强的从粗到精的显著性检测[J].光学学报,2019,39(1):0115003.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" Li D F,Liu S T.Image target saliency detection method based on feature fusion[J].Semiconductor Optoelectronics,2018,39(6):898-902,908.李德峰,刘松涛.基于特征融合的图像目标显著性检测方法[J].半导体光电,2018,39(6):898-902,908." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BDTG201806028&amp;v=MjkwMDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5emtWYnZMSnluZmFiRzRIOW5NcVk5SGJJUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Li D F,Liu S T.Image target saliency detection method based on feature fusion[J].Semiconductor Optoelectronics,2018,39(6):898-902,908.李德峰,刘松涛.基于特征融合的图像目标显著性检测方法[J].半导体光电,2018,39(6):898-902,908.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" Liu F,Shen T S,Lou S L,&lt;i&gt;et al&lt;/i&gt;.Deep network saliency detection based on global model and local optimization[J].Acta Optica Sinica,2017,37(12):1215005.刘峰,沈同圣,娄树理,等.全局模型和局部优化的深度网络显著性检测[J].光学学报,2017,37(12):1215005." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201712032&amp;v=MTUzMjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5emtWYnZMSWpYVGJMRzRIOWJOclk5R1pvUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Liu F,Shen T S,Lou S L,&lt;i&gt;et al&lt;/i&gt;.Deep network saliency detection based on global model and local optimization[J].Acta Optica Sinica,2017,37(12):1215005.刘峰,沈同圣,娄树理,等.全局模型和局部优化的深度网络显著性检测[J].光学学报,2017,37(12):1215005.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" Zhang S L,Xie L B.Salient detection based on all convolutional feature combination[J].Laser &amp;amp; Optoelectronics Progress,2018,55(10):101502.张松龙,谢林柏.基于全部卷积特征融合的显著性检测[J].激光与光电子学进展,2018,55(10):101502." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201810031&amp;v=MTQ0MjY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5emtWYnZMTHlyUFpMRzRIOW5OcjQ5R1pZUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Zhang S L,Xie L B.Salient detection based on all convolutional feature combination[J].Laser &amp;amp; Optoelectronics Progress,2018,55(10):101502.张松龙,谢林柏.基于全部卷积特征融合的显著性检测[J].激光与光电子学进展,2018,55(10):101502.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" Hou Q B,Cheng M M,Hu X W,&lt;i&gt;et al&lt;/i&gt;.Deeply supervised salient object detection with short connections[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI.New York:IEEE,2017:5300-5309." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deeply supervised salient object detection with short connections">
                                        <b>[10]</b>
                                         Hou Q B,Cheng M M,Hu X W,&lt;i&gt;et al&lt;/i&gt;.Deeply supervised salient object detection with short connections[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI.New York:IEEE,2017:5300-5309.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" Xie S N,Tu Z W.Holistically-nested edge detection[C]∥2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:1395-1403." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Holistically-nested edge detection">
                                        <b>[11]</b>
                                         Xie S N,Tu Z W.Holistically-nested edge detection[C]∥2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:1395-1403.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" Ronneberger O,Fischer P,Brox T.U-net:convolutional networks for biomedical image segmentation[M]//Navab N,Hornegger J,Wells W,&lt;i&gt;et al&lt;/i&gt;.Medical image computing and computer-assisted intervention-MICCAI 2015.Lecture notes in computer science.Cham:Springer,2015,9351:234-241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-Net:convolutional networks for biomedical image segmentation">
                                        <b>[12]</b>
                                         Ronneberger O,Fischer P,Brox T.U-net:convolutional networks for biomedical image segmentation[M]//Navab N,Hornegger J,Wells W,&lt;i&gt;et al&lt;/i&gt;.Medical image computing and computer-assisted intervention-MICCAI 2015.Lecture notes in computer science.Cham:Springer,2015,9351:234-241.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional netw orks for semantic segmentation">
                                        <b>[13]</b>
                                         Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:3431-3440.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" Zhang X N,Wang T T,Qi J Q,&lt;i&gt;et al&lt;/i&gt;.Progressive attention guided recurrent network for salient object detection[C]∥2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT.New York:IEEE,2018:714-722." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Progressive attention guided recurrent network for salient object detection">
                                        <b>[14]</b>
                                         Zhang X N,Wang T T,Qi J Q,&lt;i&gt;et al&lt;/i&gt;.Progressive attention guided recurrent network for salient object detection[C]∥2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT.New York:IEEE,2018:714-722.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" Kim H J,Dunn E,Frahm J M.Learned contextual feature reweighting for image geo-localization[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI.New York:IEEE,2017:3251-3260." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learned contextual feature reweighting for image geolocalization">
                                        <b>[15]</b>
                                         Kim H J,Dunn E,Frahm J M.Learned contextual feature reweighting for image geo-localization[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI.New York:IEEE,2017:3251-3260.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" Liu S T,Huang D,Wang Y H.Receptive field block net for accurate and fast object detection[M]//Ferrari V,Hebert M,Sminchisescu C,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2018.Lecture notes in computer science.Cham:Springer,2018,11215:404-419." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Receptive field block net for accurate and fast object detection">
                                        <b>[16]</b>
                                         Liu S T,Huang D,Wang Y H.Receptive field block net for accurate and fast object detection[M]//Ferrari V,Hebert M,Sminchisescu C,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2018.Lecture notes in computer science.Cham:Springer,2018,11215:404-419.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_17" title=" Szegedy C,Ioffe S,Vanhoucke V,&lt;i&gt;et al&lt;/i&gt;.Inception-v4,inception-resnet and the impact of residual connections on learning[C]//31st AAAI Conference on Artificial Intelligence,February 4-10,2017,San Francisco.California:AAAI Press,2017:4278-4284." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inception-v4 Inception-ResNet and the Impact of Residual Connections on Learning">
                                        <b>[17]</b>
                                         Szegedy C,Ioffe S,Vanhoucke V,&lt;i&gt;et al&lt;/i&gt;.Inception-v4,inception-resnet and the impact of residual connections on learning[C]//31st AAAI Conference on Artificial Intelligence,February 4-10,2017,San Francisco.California:AAAI Press,2017:4278-4284.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_18" >
                                        <b>[18]</b>
                                     Hu J,Shen L,Sun G.Squeeze-and-excitation networks[C]∥2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT.New York:IEEE,2018:7132-7141.</a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_19" title=" Liang M,Hu X L.Recurrent convolutional neural network for object recognition[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:3367-3375." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recurrent convolutional neural network for object recognition">
                                        <b>[19]</b>
                                         Liang M,Hu X L.Recurrent convolutional neural network for object recognition[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:3367-3375.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_20" title=" Liu N,Han J W.DHSNet:deep hierarchical saliency network for salient object detection[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:678-686." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DHSNet:Deep Hierarchical Saliency Network for Salient Object Detection">
                                        <b>[20]</b>
                                         Liu N,Han J W.DHSNet:deep hierarchical saliency network for salient object detection[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:678-686.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_21" title=" Tang Y B,Wu X Q,Bu W.Deeply-supervised recurrent convolutional neural network for saliency detection[C]∥The 24th ACM International Conference on Multimedia,October 15-19,2016,Amsterdam,Netherlands.USA:ACM,2016:397-401." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deeply-supervised recurrent convolutional neural network for saliency detection">
                                        <b>[21]</b>
                                         Tang Y B,Wu X Q,Bu W.Deeply-supervised recurrent convolutional neural network for saliency detection[C]∥The 24th ACM International Conference on Multimedia,October 15-19,2016,Amsterdam,Netherlands.USA:ACM,2016:397-401.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_22" title=" Yan Q,Xu L,Shi J P,&lt;i&gt;et al&lt;/i&gt;.Hierarchical saliency detection[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:1155-1162." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical saliency detection">
                                        <b>[22]</b>
                                         Yan Q,Xu L,Shi J P,&lt;i&gt;et al&lt;/i&gt;.Hierarchical saliency detection[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:1155-1162.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_23" title=" Li G B,Yu Y Z.Visual saliency based on multiscale deep features[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:5455-5463." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual saliency based on multiscale deep features">
                                        <b>[23]</b>
                                         Li G B,Yu Y Z.Visual saliency based on multiscale deep features[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:5455-5463.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_24" title=" Yang C,Zhang L H,Lu H C,&lt;i&gt;et al&lt;/i&gt;.Saliency detection via graph-based manifold ranking[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3166-3173." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Saliency detection via graph-based manifold ranking">
                                        <b>[24]</b>
                                         Yang C,Zhang L H,Lu H C,&lt;i&gt;et al&lt;/i&gt;.Saliency detection via graph-based manifold ranking[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3166-3173.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_25" title=" Achanta R,Hemami S,Estrada F,&lt;i&gt;et al&lt;/i&gt;.Frequency-tuned salient region detection[C]∥2009 IEEE Conference on Computer Vision and Pattern Recognition,June 20-25,2009,Miami,FL,USA.New York:IEEE,2009:1597-1604." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Frequency-tuned salient region detection">
                                        <b>[25]</b>
                                         Achanta R,Hemami S,Estrada F,&lt;i&gt;et al&lt;/i&gt;.Frequency-tuned salient region detection[C]∥2009 IEEE Conference on Computer Vision and Pattern Recognition,June 20-25,2009,Miami,FL,USA.New York:IEEE,2009:1597-1604.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_26" title=" Wang T T,Borji A,Zhang L H,&lt;i&gt;et al&lt;/i&gt;.A stagewise refinement model for detecting salient objects in images[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice.New York:IEEE,2017:4019-4028." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Stagewise Refinement Model for Detecting Salient Objects in Images">
                                        <b>[26]</b>
                                         Wang T T,Borji A,Zhang L H,&lt;i&gt;et al&lt;/i&gt;.A stagewise refinement model for detecting salient objects in images[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice.New York:IEEE,2017:4019-4028.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_27" title=" Zhang P P,Wang D,Lu H C,&lt;i&gt;et al&lt;/i&gt;.Amulet:aggregating multi-level convolutional features for salient object detection[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice.New York:IEEE,2017:202-211." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Amulet:Aggregating Multi-level Convolutional Features for Salient Object Detection">
                                        <b>[27]</b>
                                         Zhang P P,Wang D,Lu H C,&lt;i&gt;et al&lt;/i&gt;.Amulet:aggregating multi-level convolutional features for salient object detection[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice.New York:IEEE,2017:202-211.
                                    </a>
                                </li>
                                <li id="65">


                                    <a id="bibliography_28" title=" Zhang P P,Wang D,Lu H C,&lt;i&gt;et al&lt;/i&gt;.Learning uncertain convolutional features for accurate saliency detection[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice.New York:IEEE,2017:212-221." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Uncertain Convolutional Features for Accurate Saliency Detection">
                                        <b>[28]</b>
                                         Zhang P P,Wang D,Lu H C,&lt;i&gt;et al&lt;/i&gt;.Learning uncertain convolutional features for accurate saliency detection[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice.New York:IEEE,2017:212-221.
                                    </a>
                                </li>
                                <li id="67">


                                    <a id="bibliography_29" title=" Wang T T,Zhang L H,Lu H C,&lt;i&gt;et al&lt;/i&gt;.Kernelized subspace ranking for saliency detection[M]//Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9912:450-466." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Kernelized subspace ranking for saliency detection">
                                        <b>[29]</b>
                                         Wang T T,Zhang L H,Lu H C,&lt;i&gt;et al&lt;/i&gt;.Kernelized subspace ranking for saliency detection[M]//Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9912:450-466.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-06-04 15:25</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(09),268-278 DOI:10.3788/AOS201939.0915005            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>双注意力循环卷积显著性目标检测算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%A2%E5%AD%A6%E7%AB%8B&amp;code=39780946&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谢学立</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E4%BC%A0%E7%A5%A5&amp;code=39181774&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李传祥</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E5%B0%8F%E5%86%88&amp;code=35497392&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨小冈</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B8%AD%E5%BB%BA%E7%A5%A5&amp;code=38378577&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">席建祥</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%81%AB%E7%AE%AD%E5%86%9B%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E5%AF%BC%E5%BC%B9%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=1699750&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">火箭军工程大学导弹工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>显著性目标检测是机器视觉领域的研究热点,具有广泛的应用前景。针对现有显著性目标检测算法存在的显著区域检测不均匀、边缘表示模糊等问题,提出一种双注意力循环卷积显著性目标检测算法。在U-Net全卷积骨干网络中添加像素间-通道间双注意力模块,在跨层连接前对底层特征进行预处理,减小噪声和杂波干扰,提高显著区域检测性能。在骨干网络后端使用循环卷积模块,将最后的预测图与底层卷积层特征进一步结合,增强预测区域边缘的表示效果。在三个公开数据集上进行实验评测,并与相关算法进行对比,结果表明所提算法能更好地均匀突显显著区域和细化区域边缘。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%98%BE%E8%91%97%E6%80%A7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">显著性目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力机制;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BE%AA%E7%8E%AF%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">循环卷积网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    席建祥,E-mail:xijx07@mails.tsinghua.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61503012,61703411,61503009);</span>
                    </p>
            </div>
                    <h1><b>Salient Object Detection Algorithm Based on Dual-Attention Recurrent Convolution</b></h1>
                    <h2>
                    <span>Xie Xueli</span>
                    <span>Li Chuanxiang</span>
                    <span>Yang Xiaogang</span>
                    <span>Xi Jianxiang</span>
            </h2>
                    <h2>
                    <span>College of Missile Engineering, Rocket Force University of Engineering</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Salient object detection has attracted considerable attention in the field of the machine vision, with a wide range of applications. This study proposes a salient object detection algorithm based on the dual-attention recurrent convolution to overcome the limitations associated with the existing algorithms, i.e., uneven salient region detection and fuzzy edge representations. A dual-attention module consisting of pixel-and channel-wise attentions is added to a backbone U-Net fully convolutional network to preprocess the shallow convolutional features before skip-layer connection, and reduce noise and clutter interference. This improves its salient region detection performance. Then, following the backbone network, a recurrent convolutional module enhances the edge representation of the prediction region by combining the final prediction map with the shallow convolutional features. The results of experiments on three open datasets show that the proposed algorithm is better able to highlight salient regions and refine their edges than other correlation algorithms.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=salient%20object%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">salient object detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fully%20convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fully convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attention%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attention mechanism;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=recurrent%20convolutional%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">recurrent convolutional network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-11</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="69" name="69" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="70">人类的视觉注意力机制能帮助人眼在复杂环境中快速定位最感兴趣的目标或区域。在机器视觉领域,常使用显著性目标检测算法模拟人类这一视觉特性:使用显著性目标检测算法对图像像素点进行显著度计算,突出图像前景区域像素,抑制背景区域像素,最终达到数据降维和减少背景干扰的目的。生成的显著图将有助于合理分配有限的计算资源,为后续复杂的视觉任务提供先验信息。显著性目标检测被广泛应用于图像检索<citation id="166" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、图像/视频压缩<citation id="167" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、图像质量评估<citation id="168" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和虚拟视觉<citation id="169" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等领域。</p>
                </div>
                <div class="p1">
                    <p id="71">传统的显著性目标检测算法主要依靠人工提取的特征来计算区域显著性。Itti等<citation id="170" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>最先通过融合图像的颜色、方向、强度等浅层特征来度量图像显著区域;刘冬梅等<citation id="171" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>利用非下采样轮廓小波变换分解的高低频分量进行“由粗到精”的显著性检测;李德峰等<citation id="172" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>利用区域协方差理论融合多尺度空间特征,再结合全局核密度估计实现全局和局部特征融合估计目标显著性。上述算法在背景简单、对比度高的情况下能有效反映出目标的显著性,但由于未能利用深层语义的特征,检测结果的稳健性较差,显著区域突显不充分,算法泛化能力较弱。</p>
                </div>
                <div class="p1">
                    <p id="72">近年来,卷积神经网络(CNN)被广泛应用于机器视觉领域,其具有自动提取图像特征的能力,通过堆叠卷积块,可以获得丰富的高层次特征。刘峰等<citation id="173" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>利用线性拟合将卷积网络全局预测模型和区域特征描述子优化模型的结果进行融合,生成高质量显著图;张松龙等<citation id="174" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>首先密集地融合尺度内卷积特征输出各尺度显著图,再融合多尺度显著图生成最终显著图;Hou等<citation id="175" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>在整体嵌套边缘检测(HED) 网络<citation id="176" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>基础上提出了一种特征“自顶向下”传播的全卷积神经网络架构。卷积神经网络能自动提取丰富的图像特征,包括浅层的结构特征和高层的语义特征,采用监督学习的方式对特征进行筛选、组合,增强了特征表示能力。特别是全卷积神经网络的使用,极大提高了显著性目标检测的性能。但仍存在特征利用效益不足,显著目标区域突出不均匀,背景错误检测和目标轮廓模糊等问题。</p>
                </div>
                <div class="p1">
                    <p id="73">本文提出一种双注意力循环卷积网络显著性目标检测算法,选取U-Net<citation id="177" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>作为骨干网络,设计像素间-通道间双注意力模块(DAM),增强特征的利用效益,减弱噪声和背景像素干扰;设计一种循环卷积模块(RCM),通过循环迭代细化显著区域边缘轮廓;使用旁路输出策略对预测结果进行多阶段约束。</p>
                </div>
                <h3 id="74" name="74" class="anchor-tag">2 基本原理</h3>
                <div class="p1">
                    <p id="75">所提检测算法模型主要包括全卷积骨干网络、双注意力模块和循环卷积模块。采用跨层连接策略融合深层语义特征和浅层结构特征。算法总体结构如图1所示。</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909032_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 算法总体结构图" src="Detail/GetImg?filename=images/GXXB201909032_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 算法总体结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909032_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Overall structure of algorithm</p>

                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>2.1 基于U-Net的全卷积骨干网络</b></h4>
                <div class="p1">
                    <p id="78">文献<citation id="178" type="reference">[<a class="sup">13</a>]</citation>的实验结果表明只利用最后一层语义特征进行解码生成的图像具有严重“马赛克”现象,这是因为编码器中最大池化层会使图像信息损失,解码器不能完整还原细节信息。卷积神经网络中深层特征学习更多的语义信息,浅层特征描述图像局部结构特征,通过跨层连接将浅层细节特征与深层语义特征进行融合,能提高编码器预测图的细节表征能力。U-Net是图像分割领域的经典网络模型,其采用对称 “金字塔形”的编码器-解码器结构进行逐像素点预测,编码器与解码器之间采用跨层连接的方式进行多层级、多尺度特征融合,用于实现目标区域的准确分割。</p>
                </div>
                <div class="p1">
                    <p id="79">本研究采用基于U-Net的改进全卷积神经网络作为特征提取骨干网络,用于实现显著性目标检测。编码器采用五层卷积块提取输入图像的高层语义特征,用于描述图像目标的显著性。其中卷积块由两个包含3×3卷积层、BN(batch normalization)层和ReLU激活层的子卷积模块组成。卷积块之间使用滑步为2的最大池化层进行下采样,增大深层特征感受野,确保特征描述具有一定的全局性。对每次下采样后的特征通道进行翻倍,最后的下采样通道不进行加倍。解码器采用反卷积块对高层语义特征进行上采样。反卷积块具有卷积块相同对卷积子模块,只是模块之间采用反卷积层进行上采样。上采样后通道数减半,并与对应的具有相同分辨率的编码器特征进行通道堆叠(channel concatenate)。</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80"><b>2.2 双注意力模块</b></h4>
                <div class="p1">
                    <p id="81">前景区域和背景区域对显著性目标检测的影响不同,各特征通道对预测结果的贡献度存在差异。U-Net直接采用跨层连接将编码端特征与解码端特征进行concatenate融合,该操作会引入大量的杂波,干扰最后预测图生成。针对此问题,引入注意力机制对编码器特征进行预加权处理,过滤有害杂波干扰。以往的注意力机制只片面考虑通道间或像素间特征的影响,未能全面地对整个特征图中各像素点的重要度进行衡量。而显著性目标检测是通过逐像素点预测来对显著性目标区域进行分割,对各像素点的质量要求较高。基于此设计中的双注意力模块,在融合编码端特征与解码端特征之前,先将编码端特征接入设计的通道间-像素间双注意力模块,标定通道间响应权重,突出前景区域像素响应强度,增强任务相关特征表征能力,弱化背景和噪声影响。为避免由于多级注意力模块串联造成的特征信息弱化与损失,双注意力模块采用并联方式融合两类注意力。双注意力模块内部结构如图2所示。</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909032_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 双注意力模块内部结构图" src="Detail/GetImg?filename=images/GXXB201909032_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 双注意力模块内部结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909032_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Internal structure of DAM</p>

                </div>
                <h4 class="anchor-tag" id="83" name="83">2.2.1 像素间注意力模块</h4>
                <div class="p1">
                    <p id="84">显著性目标一般存在于图像某个区域,该区域像素点对目标显著值预测的影响大于其他像素点<citation id="179" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。引入像素间注意力模块,为图像各像素点赋予权值,更多地关注前景区域像素,有助于生成有效的特征,减弱非显著区域的干扰。</p>
                </div>
                <div class="p1">
                    <p id="85">文献<citation id="180" type="reference">[<a class="sup">15</a>]</citation>使用多尺度卷积滤波器设计上下文重加权网络,对输入特征图进行像素间加权处理,增强地标建筑区域像素响应,提高了图像地理定位精度。文献<citation id="181" type="reference">[<a class="sup">16</a>]</citation>利用感受野模块(RFB)模拟人类视觉感受野特性。聚合了多尺度特征信息,提升了目标检测准确率。本研究基于文献<citation id="182" type="reference">[<a class="sup">15</a>]</citation>,参考RFB设计思想,设计一种多尺度上下文重加权网络(MSCRN),作为像素间注意力模块来实现特征图像素间注意力预测,如图2中间虚线椭圆区域所示。</p>
                </div>
                <div class="p1">
                    <p id="86">基于Inception-ResNet<citation id="183" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>结构设计一种多尺度上下文信息聚合模块,构建三路分支,分别为3×3卷积、3×3卷积加比率为3的3×3空洞卷积、3×3卷积加比率为3的5×5空洞卷积,实现3×3,9×9和15×15三个尺度特征信息的提取,并采用1×1卷积进行多通道特征融合。由于各级卷积特征分辨率存在差异,多尺度信息整合模块需自适应地进行调整。Conv1和Conv2的特征图分辨率较大,采用三支路提取多尺度特征;由于Conv3特征图经过两次下采样后分辨率较小,只采用3×3、9×9尺度特征提取分支;对于Conv4特征图则只取3×3尺度特征进行分支提取。各支路感受野如图3所示,圆点代表卷积核采样点,三个不同颜色的区域分别表示三路分支感受野区域。</p>
                </div>
                <div class="p1">
                    <p id="87">输入特征图通过多尺度信息整合模块和激励函数,生成像素间注意力分布图。分布图分别与各通道特征图像素对应相乘,得到加权后的特征图。设输入特征图<i><b>X</b></i>∈<b>R</b><sup><i>W</i></sup><sup>×</sup><sup><i>H</i></sup><sup>×</sup><sup><i>C</i></sup>,下采样特征图<mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mrow><msup><mi>W</mi><mo>′</mo></msup><mo>×</mo><msup><mi>Η</mi><mo>′</mo></msup><mo>×</mo><mi>C</mi></mrow></msup></mrow></math></mathml>,注意力模块输出<mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">X</mi></mstyle><mo>∼</mo></mover><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mrow><mi>W</mi><mo>×</mo><mi>Η</mi><mo>×</mo><mi>C</mi></mrow></msup></mrow></math></mathml>,该模块可以表述为</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><mo>=</mo><mi>f</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>,</mo><mspace width="0.25em" /><msup><mi mathvariant="bold-italic">X</mi><mo>′</mo></msup><mo>=</mo><mi>f</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>1</mn><mo>×</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><msup><mi mathvariant="bold-italic">X</mi><mo>″</mo></msup><mo>=</mo><mi>f</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo stretchy="false">[</mo><mi>f</mi><msub><mrow></mrow><mrow><mtext>b</mtext><mtext>r</mtext><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">X</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mo>,</mo><mi>f</mi><msub><mrow></mrow><mrow><mtext>b</mtext><mtext>r</mtext><mn>2</mn></mrow></msub><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">X</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mo>,</mo><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mrow><mtext>b</mtext><mtext>r</mtext><mn>3</mn></mrow></msub><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">X</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><msup><mi mathvariant="bold-italic">Y</mi><mo>′</mo></msup><mo>=</mo><mi>f</mi><msub><mrow></mrow><mtext>S</mtext></msub><mo stretchy="false">{</mo><msup><mi>f</mi><mo>″</mo></msup><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>1</mn><mo>×</mo><mn>1</mn></mrow></msub><mo stretchy="false">[</mo><msup><mi mathvariant="bold-italic">X</mi><mo>′</mo></msup><mo>+</mo><msup><mi>f</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>1</mn><mo>×</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">X</mi><mo>″</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo>,</mo></mtd></mtr><mtr><mtd><msup><mi mathvariant="bold-italic">Y</mi><mo>′</mo></msup><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mrow><msup><mi>W</mi><mo>′</mo></msup><mo>×</mo><msup><mi>Η</mi><mo>′</mo></msup><mo>×</mo><mn>1</mn></mrow></msup><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">X</mi></mstyle><mo>∼</mo></mover><mo>=</mo><mi>f</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo stretchy="false">[</mo><msup><mi mathvariant="bold-italic">X</mi><mo>′</mo></msup><mo>⋅</mo><mi>f</mi><msub><mrow></mrow><mtext>e</mtext></msub><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">Y</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">式中:<i>f</i><sub>conv1×1</sub>表示1×1卷积;<i>f</i><sub>d</sub>表示下采样操作;<i>f</i><sub>c</sub>表示全连接层;<i>f</i><sub>br1</sub>,<i>f</i><sub>br2</sub>,<i>f</i><sub>br3</sub>分别表示三个尺度特征提取分支的卷积操作;<i>f</i><sub>S</sub>表示Sigmoid激活函数;<i>f</i><sub>u</sub>表示进行上采样操作;<i>f</i><sub>e</sub>表示复制扩展<i><b>Y</b></i>′维度,使其与<i><b>X</b></i>′一致;<i>f</i>′<sub>conv1×1</sub>和<i>f</i>″<sub>conv1×1</sub>分别表示具有不同卷积核参数的1×1卷积。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909032_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 多尺度感受野示意图" src="Detail/GetImg?filename=images/GXXB201909032_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 多尺度感受野示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909032_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Schematic of multi-scale receptive field</p>

                </div>
                <h4 class="anchor-tag" id="91" name="91">2.2.2 通道间注意力模块</h4>
                <div class="p1">
                    <p id="92">不同的特征通道往往具有不同的特征响应,不同的特征响应对于目标任务的影响存在差异。而卷积算子对输入特征图各通道的处理是等权重的,这与实际情况不相符。引入通道间注意力模块,实现通道间差异化处理,可以增大任务有利的特征通道权值,降低无用的特征通道权值,从而减弱无关信息干扰。本研究选用文献<citation id="184" type="reference">[<a class="sup">18</a>]</citation>提出的挤压-激励(SE)模块作为通道间注意力模块,如图2左上角椭圆区域所示。其采用挤压-激励操作提取通道间注意力,在监督学习下,计算预测值与真值之间的偏差,以偏差量为导向,给输入特征图各特征通道赋予权重。设输入张量为<i><b>X</b></i>∈<b>R</b><sup><i>W</i></sup><sup>×</sup><sup><i>H</i></sup><sup>×</sup><sup><i>C</i></sup> 。挤压操作是对特征图中各个通道进行全局平均池化,得到一个1×1×<i>C</i>的全局信息描述符,该描述符表示各通道特征整体响应的相对强弱,可表示为</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ζ</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mrow><mrow><mo>[</mo><mrow><mfrac><mn>1</mn><mrow><mi>W</mi><mo>×</mo><mi>Η</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Η</mi></munderover><mi mathvariant="bold-italic">X</mi></mstyle></mrow></mstyle><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mo>]</mo></mrow></mrow></mstyle><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">式中:<i><b>Z</b></i>为全局特征描述符。</p>
                </div>
                <div class="p1">
                    <p id="95">激励操作是采用全连接对全局信息描述符进行仿射变换,全面捕获通道依赖性,过程可表示为</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><mo>=</mo><mi>f</mi><msub><mrow></mrow><mtext>S</mtext></msub><mo stretchy="false">{</mo><msup><mi>f</mi><mo>′</mo></msup><msub><mrow></mrow><mtext>c</mtext></msub><mo stretchy="false">[</mo><mi>δ</mi><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">式中:<i>δ</i>表示ReLU激活函数,<i>f</i>′c表示含有<i>r</i>个单元的全连接层,<i><b>S</b></i>为描述各通道权重赋值的张量,<i><b>S</b></i>∈<b>R</b><sup>1×1×</sup><sup><i>C</i></sup>。</p>
                </div>
                <div class="p1">
                    <p id="98">最后,将得到的通道间权向量与各通道对应相乘,输出为</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">X</mi></mstyle><mo>∼</mo></mover><mo>=</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mtext>a</mtext></msub><mo>⋅</mo><mi>f</mi><msub><mrow></mrow><mtext>e</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mtext>a</mtext></msub><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">式中:<i><b>X</b></i><sub>a</sub>为输入特征图的单个通道特征,<i><b>S</b></i><sub>a</sub>为对应通道特征的权重,<i>f</i><sub>e</sub>表示复制扩展操作。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101">2.2.3 双注意力模块融合</h4>
                <div class="p1">
                    <p id="102">由于两类注意力模块输出的权值均在0到1之间,直接采用串联方式将结果进行融合,会减小像素间灰度值差异,弱化特征响应,造成信息损失。为避免这个问题,本研究采用并联方式融合两类注意力模块。首先,让两类注意力模块分别对输入特征图进行处理,得到两类注意力加权特征图,然后采用concatenate操作对结果进行通道堆叠,最后用1×1卷积将两类结果进行整合,并使用L2 Normalize对输出层进行标准化。</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103"><b>2.3 循环卷积模块</b></h4>
                <div class="p1">
                    <p id="104">对高层语义特征直接进行反卷积预测得到的显著图中会存在模糊现象。采取跨层连接,引入底层特征能明显改善显著图细节表示能力,但仍存在边界模糊现象。为进一步提升预测图质量,受人类“凝视”视觉机制启发,在模型后端引入循环卷积模块,以时间迭代来增强空间分辨能力,细化目标区域边缘。</p>
                </div>
                <div class="p1">
                    <p id="105">文献<citation id="185" type="reference">[<a class="sup">19</a>]</citation>首先提出循环卷积层(RCL),文献<citation id="186" type="reference">[<a class="sup">20</a>,<a class="sup">21</a>]</citation>将其引入显著性目标检测,用于分层细化预测图,提升了显著性目标检测分割效果。基于RCL,设计多特征循环卷积模块(MFRCM),遵循Inception思想改造卷积循环单元,聚合多尺度感受野特征;同时添加外部循环机制,结合多卷积层特征,将解码器最后一层输出与编码器底层卷积特征作为输入,输出边缘效果更好的预测图。模块内部结构如图4所示。</p>
                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909032_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 多特征循环卷积模块内部结构图" src="Detail/GetImg?filename=images/GXXB201909032_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 多特征循环卷积模块内部结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909032_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Internal structure of MFRCM</p>

                </div>
                <div class="p1">
                    <p id="107">MFRCM将循环连接引至卷积层之间,以Inception卷积层为循环单位,逐步提取特征信息。随循环步骤<i>T</i>的增加,输出图单个像素的感受野增大,特征提取范围增大,信息描述更全面。输入图与循环单元输出之间使用捷径(shortcut)连接。设输入特征图为<i><b>U</b></i>∈<b>R</b><sup><i>W</i></sup><sup>×</sup><sup><i>H</i></sup><sup>×</sup><sup><i>C</i></sup>, <i>u</i><mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>为第k通道位于(m,n)的像素,循环卷积过程可描述为</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>z</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>W</mi><msubsup><mrow></mrow><mi>k</mi><mtext>f</mtext></msubsup><mo>⋅</mo><mi>u</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>k</mi></msub><mo>,</mo><mspace width="0.25em" /><mi>t</mi><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left"><mi>z</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>W</mi><msubsup><mrow></mrow><mi>k</mi><mtext>f</mtext></msubsup><mo>⋅</mo><mi>u</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>W</mi><msubsup><mrow></mrow><mi>k</mi><mtext>r</mtext></msubsup><mo>⋅</mo></mtd></mtr><mtr><mtd columnalign="left"><mtext> </mtext><mtext> </mtext><mi>x</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>k</mi></msub><mo>,</mo><mspace width="0.25em" /><mn>0</mn><mo>&lt;</mo><mi>t</mi><mo>&lt;</mo><mi>Τ</mi></mtd></mtr></mtable></mrow></mrow><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">式中:W<sup><i>f</i></sup><sub>k</sub>,W<sup><i>r</i></sup><sub>k</sub>分别表示前向输入与循环输入的权值;u<mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>表示前向输入,不随循环步长变化;z<mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>表示卷积输出;T表示内循环次数;x<sup>(m,n)</sup>(t-1)表示循环输入,即上一个循环单元输出。循环单元输出x<mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>(t)由z<mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>(t)变化而来,即</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo><mo>=</mo><mtext>R</mtext><mtext>e</mtext><mtext>L</mtext><mtext>U</mtext><mo stretchy="false">(</mo><mi>z</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd columnalign="left"><mi>g</mi><mo stretchy="false">(</mo><mi>f</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>R</mtext><mtext>Ν</mtext></mrow></msub><mo stretchy="false">(</mo><mi>f</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd columnalign="left"><mi>x</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>g</mi><mo stretchy="false">{</mo><mi>f</mi><mo stretchy="false">[</mo><mi>z</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo></mtd></mtr></mtable></mrow></mrow><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">式中f<sub><i>LRN</i></sub>表示局部响应归一化(<i>LRN</i>)函数, 用于模拟人脑侧抑制机制。</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>R</mtext><mtext>Ν</mtext></mrow></msub><mo stretchy="false">(</mo><mi>f</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>/</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mi>α</mi><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>L</mi><mo>=</mo><mi>max</mi><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>k</mi><mo>-</mo><mi>Ν</mi><mo>/</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><mrow><mi>min</mi><mo stretchy="false">(</mo><msup><mi>C</mi><mo>′</mo></msup><mo>,</mo><mi>k</mi><mo>+</mo><mi>Ν</mi><mo>/</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></munderover><mo stretchy="false">(</mo></mstyle><mi>f</mi><msubsup><mrow></mrow><mi>L</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>]</mo></mrow><msup><mrow></mrow><mi>β</mi></msup><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">式中:N表示参与归一化的邻近通道数;α和β是用来调整标准化幅度;C′表示总通道数,f<mathml id="162"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>表示一个计算中间值,这里表示<i>LRN</i>的输入特征图;L表示表示特征图的第L个通道特征。</p>
                </div>
                <div class="p1">
                    <p id="114">在原<i>RCL</i>基础上,添加外部循环机制,将上一轮<i>MFRCM</i>的输出值作为下一轮外部循环的输入值。连接示意图如图5所示。由于采用<i>concatenate</i>操作结合输入图和特征图,输入图通道占比重较小,故采用<i>Repeat</i>方案扩充输入图通道。</p>
                </div>
                <h3 id="115" name="115" class="anchor-tag">3 实验与结果分析</h3>
                <h4 class="anchor-tag" id="116" name="116"><b>3.1 数据集与评价准则</b></h4>
                <h4 class="anchor-tag" id="117" name="117">3.1.1 数据集介绍</h4>
                <div class="p1">
                    <p id="118">在常用公开数据集ESSCD<citation id="187" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、HKU-IS<citation id="188" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>和DUT-OMRON<citation id="189" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>上进行算法训练与验证。ESSCD数据集包含1000张语义结构复杂的图像。HKU-IS数据集包含4447张含有多个显著目标的图像。DUT-OMRON数据集包含对比度低、尺寸变化大的自然图像5168张,是一个十分具有挑战性的数据集。分别从ECSSD数据集、HKU-IS数据集和DUT-OMRON数据集随机抽取532张、2000张、2668张图像构成算法的训练集,共5200张。从剩余图像中抽取218张、1000张、1000张构成验证集,共2218张。其余图像用作测试集:ECSSD-250,HKU-IS-1447,DUT-OMRON-1500。</p>
                </div>
                <h4 class="anchor-tag" id="119" name="119">3.1.2 评价准则</h4>
                <div class="p1">
                    <p id="120">选取PR(precision-recall)曲线、ROC(receiveroperating characteristic)曲线、<i>F</i>-measure值和MAE(mean absolute error)值作为客观评价指标。PR曲线是显著性目标检测常用的评价指标之一。逐渐增加阈值<i>H</i>∈[0,255],将显著图二值化与真值图进行逐像素对比,计算出每个阈值对应下的准确率<i>P</i>(precision)与召回率<i>R</i>(recall),再以召回率为横坐标,准确率为纵坐标,画出PR曲线。一般来说PR曲线越接近右上方,算法性能越好。ROC曲线是逐渐增加阈值下以假阳率(FPR)为横坐标,真阳率(TPR)为纵坐标画出的曲线,一般来说曲线越靠近左上方说明算法性能越好。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909032_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 多特征循环卷积模块连接示意图" src="Detail/GetImg?filename=images/GXXB201909032_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 多特征循环卷积模块连接示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909032_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Connectionschematic of MFRCM</p>

                </div>
                <div class="p1">
                    <p id="122">单独使用准确率或召回率不能判断算法的优劣,<i>F</i>-measure值综合考虑了准确率和召回率,以加权平均和的形式给出一个度量值,是一个全面的衡量指标,该值越大算法性能越好。<i>F</i>-measure值的计算式为</p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mi>β</mi></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><mo>⋅</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>i</mtext><mtext>s</mtext><mtext>o</mtext><mtext>n</mtext></mrow></msub><mo>⋅</mo><mi>R</mi><msub><mrow></mrow><mrow><mtext>R</mtext><mtext>e</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext></mrow></msub></mrow><mrow><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup><mo>⋅</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>i</mtext><mtext>s</mtext><mtext>o</mtext><mtext>n</mtext></mrow></msub><mo>+</mo><mi>R</mi><msub><mrow></mrow><mrow><mtext>R</mtext><mtext>e</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext></mrow></msub></mrow></mfrac><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="124">式中<i>σ</i><sup>2</sup>一般取0.3,认为准确率比查全率更重要。在计算<i>F</i>-measure值前,需要对预测的显著图进行二值化处理,采用广泛使用的自适应阈值方案<citation id="190" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>,以图像像素灰度平均值的2倍为阈值进行二值化分割,可由(12)式计算,即</p>
                </div>
                <div class="p1">
                    <p id="125" class="code-formula">
                        <mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mo>=</mo><mfrac><mn>2</mn><mrow><mover accent="true"><mi>W</mi><mo>^</mo></mover><mo>×</mo><mover accent="true"><mi>Η</mi><mo>^</mo></mover></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow></munder><mi>S</mi></mstyle></mrow></mstyle><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="126">式中:<mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>W</mi><mo>^</mo></mover></math></mathml>表示图像宽度;<mathml id="164"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Η</mi><mo>^</mo></mover></math></mathml>表示图像高度;S(x,y)表示显著图的像素点灰度值。<i>MAE</i>值表示二值化后显著图<i><b>S</b></i>与真值图<i><b>G</b></i>对应的像素点绝对误差,即</p>
                </div>
                <div class="p1">
                    <p id="127" class="code-formula">
                        <mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>A</mtext><mtext>E</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mover accent="true"><mi>W</mi><mo>^</mo></mover><mo>×</mo><mover accent="true"><mi>Η</mi><mo>^</mo></mover></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow></munder><mo stretchy="false">|</mo></mstyle></mrow></mstyle><mi>S</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>-</mo><mi>G</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="128">式中:<i>G</i>(<i>x</i>,<i>y</i>)表示真值图像素点的灰度值。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129"><b>3.2 算法实现细节</b></h4>
                <div class="p1">
                    <p id="130">采用深度学习框架pytorch-0.4进行神经网络搭建,实验环境为Windows 10,使用NVIDA TITAN X GPU进行神经网络训练与测试。采用二值交叉熵(BCE)损失函数作为预测值与目标之间误差的度量准则。设<i><b>W</b></i>表示网络参数,<i>y</i><sub><i>i</i></sub><sub>′</sub>表示真值,<mathml id="165"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><msup><mi>i</mi><mo>′</mo></msup></msub></mrow></math></mathml>表示预测值,N′表示图像总像素数,则<i>BCE</i>损失函数表示为</p>
                </div>
                <div class="p1">
                    <p id="131" class="code-formula">
                        <mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><msup><mi>i</mi><mo>′</mo></msup><mo>=</mo><mn>1</mn></mrow><msup><mi>Ν</mi><mo>′</mo></msup></munderover><mo stretchy="false">[</mo></mstyle><mi>y</mi><msub><mrow></mrow><msup><mi>i</mi><mo>′</mo></msup></msub><mo>⋅</mo><mi>ln</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><msup><mi>i</mi><mo>′</mo></msup></msub><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><msup><mi>i</mi><mo>′</mo></msup></msub><mo stretchy="false">)</mo><mo>⋅</mo><mi>ln</mi><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><msup><mi>i</mi><mo>′</mo></msup></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="132">网络训练时使用多阶段约束方案,约束输出值域范围,总损失函数L<sub><i>total</i></sub>采用各阶段损失函数加权和表示,即</p>
                </div>
                <div class="p1">
                    <p id="133" class="code-formula">
                        <mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>o</mtext><mtext>t</mtext><mtext>a</mtext><mtext>l</mtext></mrow></msub><mo>=</mo><mi>w</mi><msub><mrow></mrow><mtext>d</mtext></msub><mi>L</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>+</mo><mi>w</mi><msub><mrow></mrow><mtext>e</mtext></msub><mi>L</mi><msub><mrow></mrow><mtext>e</mtext></msub><mo>+</mo><mi>w</mi><msub><mrow></mrow><mtext>f</mtext></msub><mi>L</mi><msub><mrow></mrow><mtext>f</mtext></msub><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="134">式中:L<sub><i>d</i></sub>表示最后反卷积块的输出图与真值之间的<i>BCE</i>损失;L<sub><i>e</i></sub>表示循环卷积块第一次外部循环的输出图与真值之间的<i>BCE</i>损失;L<sub><i>f</i></sub>表示最终输出图与真值之间的<i>BCE</i>损失,w<sub><i>d</i></sub>,w<sub><i>e</i></sub>,w<sub><i>f</i></sub>分别表示各阶段损失的权重,训练时分别设置为0.3,1.0,1.0。</p>
                </div>
                <div class="p1">
                    <p id="135">所提算法使用<i>U</i>-<i>Net</i>语义分割模型卷积层权重作为预训练参数进行训练,使用随机梯度下降优化器进行优化,其中初始学习率设置为0.001,动量设置为0.9,权重衰减设置为0.0001,学习率衰减采用基于轮次下降的策略,每200个轮次降为原来的0.1。循环卷积模块中<i>LRN</i>的超参数设置α=0.001,β=0.75,N=C/8,内循环次数设置为T=3,外循环次数设置为S=2。在训练阶段,保持长宽比例将图像放缩至短边为256 <i>pixel</i>,并随机裁剪使图像输入尺寸固定为256 <i>pixel</i>×256 <i>pixel</i>。批量训练样本数量(<i>batch size</i>)设为4,训练轮次设为1000,取最佳指标对应轮次的模型参数为最终参数。测试帧率约为9 <i>frame</i>/<i>s</i>。</p>
                </div>
                <h4 class="anchor-tag" id="136" name="136"><b>3.3 实验结果比较与分析</b></h4>
                <div class="p1">
                    <p id="137">将所提出的算法与6种流行的基于卷积神经网络的显著性目标检测算法进行对比实验:SRM<citation id="191" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>,AMU<citation id="192" type="reference"><link href="63" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>,UCF<citation id="193" type="reference"><link href="65" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>,KSR<citation id="194" type="reference"><link href="67" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>,MDF<citation id="195" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>,DSS<citation id="196" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。对比算法的评测数据,均来自算法作者公布的显著结果图和相关的模型参数。</p>
                </div>
                <h4 class="anchor-tag" id="138" name="138">3.3.1 主观视觉对比</h4>
                <div class="p1">
                    <p id="139">为比较所提出的算法与对比算法在不同情况下的目标检测效果,选择6组算法效果图作为主观视觉对比(见图6)。其中,图6(a)和(b)反映了7种算法在低对比度环境下对目标区域的检测与分割,可以看出所提算法无论是对显著性目标位置的检测,还是对目标边缘分割,都优于其他6种算法;图6(c)和(d)反映对多显著目标的检测效果,虽然所提算法在检测左边啤酒杯时出现把手漏检情况,但是从总体的分割细节来看,优于其他算法。尤其在图6(d)上所提算法效果明显优于其他算法。图6(e)和(f)反映算法对细节检测的能力,可以看出所提算法的优势。</p>
                </div>
                <h4 class="anchor-tag" id="140" name="140">3.3.2 客观量化对比</h4>
                <div class="p1">
                    <p id="141">根据选择的客观评测准则对7种算法进行量化对比。表1为各算法在三个测试集下<i>F</i>-measure值和MAE值,其中,算法性能同<i>F</i>-measure值呈正相关,同MAE值呈负相关。根据表1数据可以看出在三个测试集上所提算法性能均优于其他算法。尤其是在DUT-OMRON-1500数据集上,所提算法优势明显。</p>
                </div>
                <div class="area_img" id="142">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909032_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 主观视觉效果对比图" src="Detail/GetImg?filename=images/GXXB201909032_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 主观视觉效果对比图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909032_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Comparison of subjective visual effects</p>

                </div>
                <div class="area_img" id="143">
                    <p class="img_tit">表1 客观量化指标对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Comparison of objective quantitative indicators</p>
                    <p class="img_note"></p>
                    <table id="143" border="1"><tr><td rowspan="2"><br />Dataset</td><td rowspan="2">Evaluating <br />indicator</td><td colspan="7"><br />Value</td></tr><tr><td>Ours</td><td>AMU</td><td>UCF</td><td>SRM</td><td>KSR</td><td>MDF</td><td>DSS</td></tr><tr><td><br />ECSSD-250</td><td><i>F</i>-measure</td><td>0.928</td><td>0.918</td><td>0.912</td><td>0.916</td><td>0.827</td><td>0.834</td><td>0.917</td></tr><tr><td><br /></td><td>MAE</td><td>0.043</td><td>0.054</td><td>0.071</td><td>0.054</td><td>0.117</td><td>0.104</td><td>0.056</td></tr><tr><td><br />HKU-IS-1447</td><td><i>F</i>-measure</td><td>0.923</td><td>0.907</td><td>0.893</td><td>0.912</td><td>0.784</td><td>0.850</td><td>0.910</td></tr><tr><td><br /></td><td>MAE</td><td>0.038</td><td>0.049</td><td>0.058</td><td>0.042</td><td>0.127</td><td>0.118</td><td>0.040</td></tr><tr><td><br />DUT-OMRON-1500</td><td><i>F</i>-measure</td><td>0.782</td><td>0.748</td><td>0.737</td><td>0.763</td><td>0.634</td><td>0.647</td><td>0.758</td></tr><tr><td><br /></td><td>MAE</td><td>0.056</td><td>0.071</td><td>0.131</td><td>0.068</td><td>0.131</td><td>0.087</td><td>0.074</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="144">图7、8、9中图(a)为7种算法在三个测试集上的PR曲线,可以明显看出所提算法曲线右上部更接近(1,1)点,说明通过调整阈值所提算法能取得高准确率的同时保持高召回率,对显著目标的检测效果也最好。图(b)为7种算法在三个测试集上的ROC曲线,在ECSSD- 250和HKU-IS-1447测试集上所提算法具有明显优势,在DUT-OMRON-1500测试集上ROC曲线曲率出现突变,这是因为曲线是使用点对直接画出,当算法结果具有较低假阳率和较高的真阳率时,点对会集中出现在低假阳率区域,因此可以说所提算法优于其他算法。</p>
                </div>
                <div class="area_img" id="145">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909032_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同算法在ECSSD-250测试集下的特征曲线。" src="Detail/GetImg?filename=images/GXXB201909032_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同算法在ECSSD-250测试集下的特征曲线。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909032_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Characteristic curves of seven algorithms on ECSSD-250testset.</p>
                                <p class="img_note">(a)PR曲线；（b)ROC曲线</p>
                                <p class="img_note">(a)PR curves;(b)ROC curves</p>

                </div>
                <div class="area_img" id="146">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909032_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 不同算法在HKU-IS-1447测试集下的特征曲线。" src="Detail/GetImg?filename=images/GXXB201909032_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 不同算法在HKU-IS-1447测试集下的特征曲线。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909032_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Characteristic curves of seven algorithms on HKU-IS-1447testset.</p>
                                <p class="img_note">(a)PR曲线；（b)ROC曲线</p>
                                <p class="img_note">(a)PR curves;(b)ROC curves</p>

                </div>
                <div class="area_img" id="147">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909032_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 7种算法在DUT-OMRON-1500测试集下的特征曲线。" src="Detail/GetImg?filename=images/GXXB201909032_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 7种算法在DUT-OMRON-1500测试集下的特征曲线。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909032_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Characteristic curves of seven algorithms on DUT-OMRON-1500testset.</p>
                                <p class="img_note">(a)PR曲线；（b)ROC曲线</p>
                                <p class="img_note">(a)PR curves;(b)ROC curves</p>

                </div>
                <h4 class="anchor-tag" id="148" name="148">3.3.3 算法运行时间及参数量对比</h4>
                <div class="p1">
                    <p id="149">表2对比了7种算法的运行时间。实验平台配置如下:CPU:Xeon E5-1650 v4,3.6 GHz;GPU:NVIDIA TITAN-X,12 GB;运行内存为32 GB;固态硬盘容量为512 GB。所提算法处理速度约为9 frame/s,达到次实时要求。</p>
                </div>
                <div class="p1">
                    <p id="150">表3对AMU、UCF、SRM、DSS和所提算法模型参数量进行统计,并给出U-Net骨干网络模型参数量作为对照。由于MDF和KSR与其他算法性能相差较大,参数量对比无实际意义,故未进行统计。从表3可以看出所提算法参数量相对较小,一定程度说明在中小规模数据集上不易出现过拟合,更容易训练。</p>
                </div>
                <div class="area_img" id="151">
                    <p class="img_tit">表2 各算法的运行时间 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Running time of different algorithms</p>
                    <p class="img_note"></p>
                    <table id="151" border="1"><tr><td>Algorithm</td><td>AMU</td><td>UCF</td><td>KSR</td><td>SRM</td><td>MDF</td><td>DSS</td><td>Ours</td></tr><tr><td><br />Time /s</td><td>0.08</td><td>0.08</td><td>47.20</td><td>0.10</td><td>7.64</td><td>0.48</td><td>0.11</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="152">
                    <p class="img_tit">表3 算法参数量统计 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Parameters statistics of different algorithms</p>
                    <p class="img_note"></p>
                    <table id="152" border="1"><tr><td>Algorithm</td><td>U-Net</td><td>Ours</td><td>AMU</td><td>UCF</td><td>SRM</td><td>DSS</td></tr><tr><td><br />Number of parameters</td><td>13395329</td><td>19062256</td><td>17740720</td><td>30406400</td><td>45484608</td><td>19130619</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="153" name="153" class="anchor-tag">4 结  论</h3>
                <div class="p1">
                    <p id="154">本研究提出一种双注意力循环卷积显著性目标检测算法。算法在U-Net骨干网络中引入像素间-通道间双注意力模块,对跨层连接的特征进行重加权处理,提高特征利用的有效性。其中使用空洞卷积设计像素间注意力模块,采用三路特征提取分支模拟感受野机制,有效整合多尺度上下文信息,实现像素间加权。采用多特征循环卷积层将U-Net解码端输出与编码端卷积特征进行结合处理,增强输出显著图边缘表示效果。实验表明,所提算法在多显著性目标检测和细节检测能力上表现良好,在三个测试集上的客观量化指标性能优于其他6类对比算法。但由于骨干网络的最大池化层会造成信息量丢失,影响预测图重构质量,下一步工作是改进编码器网络,提取适合显著性检测的特征,同时优化模型结构与参数,进一步改善运行速度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual-Textual Joint Relevance Learning for Tag-Based Social Image Search">

                                <b>[1]</b> Gao Y,Wang M,Zha Z J,<i>et al</i>.Visual-textual joint relevance learning for tag-based social image search[J].IEEE Transactions on Image Processing,2013,22(1):363-376.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression">

                                <b>[2]</b> Guo C L,Zhang L M.A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression[J].IEEE Transactions on Image Processing,2010,19(1):185-198.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatio-temporal saliency detection using phase spectrum of quater-nion fourier transform">

                                <b>[3]</b> Guo C L,Ma Q,Zhang L M.Spatio-temporal saliency detection using phase spectrum of quaternion Fourier transform[C]//2008 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),June 23-28,2008,Anchorage,AK,USA.New York:IEEE,2008:4587715.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual perception based on salient features">

                                <b>[4]</b> Courty N,Marchand E.Visual perception based on salient features[C]//Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003),October 27-31,2003,Las Vegas,Nevada,USA.New York:IEEE,2003:1024-1029.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A model of saliency-based visual attention for rapid scene analysis">

                                <b>[5]</b> Itti L,Koch C,Niebur E.A model of saliency-based visual attention for rapid scene analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1998,20(11):1254-1259.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201901034&amp;v=MTAzMzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXprVmJ2TElqWFRiTEc0SDlqTXJvOUdZSVE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Liu D M,Chang F L.Coarse-to-fine saliency detection based on non-subsampled contourlet transform enhancement[J].Acta Optica Sinica,2019,39(1):0115003.刘冬梅,常发亮.基于非下采样轮廓小波变换增强的从粗到精的显著性检测[J].光学学报,2019,39(1):0115003.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BDTG201806028&amp;v=MzEwMTdidkxKeW5mYWJHNEg5bk1xWTlIYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl6a1Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Li D F,Liu S T.Image target saliency detection method based on feature fusion[J].Semiconductor Optoelectronics,2018,39(6):898-902,908.李德峰,刘松涛.基于特征融合的图像目标显著性检测方法[J].半导体光电,2018,39(6):898-902,908.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201712032&amp;v=Mjg2NTA1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl6a1ZidkxJalhUYkxHNEg5Yk5yWTlHWm9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Liu F,Shen T S,Lou S L,<i>et al</i>.Deep network saliency detection based on global model and local optimization[J].Acta Optica Sinica,2017,37(12):1215005.刘峰,沈同圣,娄树理,等.全局模型和局部优化的深度网络显著性检测[J].光学学报,2017,37(12):1215005.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201810031&amp;v=MTE2MTVxcUJ0R0ZyQ1VSTE9lWmVWdkZ5emtWYnZMTHlyUFpMRzRIOW5OcjQ5R1pZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Zhang S L,Xie L B.Salient detection based on all convolutional feature combination[J].Laser &amp; Optoelectronics Progress,2018,55(10):101502.张松龙,谢林柏.基于全部卷积特征融合的显著性检测[J].激光与光电子学进展,2018,55(10):101502.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deeply supervised salient object detection with short connections">

                                <b>[10]</b> Hou Q B,Cheng M M,Hu X W,<i>et al</i>.Deeply supervised salient object detection with short connections[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI.New York:IEEE,2017:5300-5309.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Holistically-nested edge detection">

                                <b>[11]</b> Xie S N,Tu Z W.Holistically-nested edge detection[C]∥2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:1395-1403.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-Net:convolutional networks for biomedical image segmentation">

                                <b>[12]</b> Ronneberger O,Fischer P,Brox T.U-net:convolutional networks for biomedical image segmentation[M]//Navab N,Hornegger J,Wells W,<i>et al</i>.Medical image computing and computer-assisted intervention-MICCAI 2015.Lecture notes in computer science.Cham:Springer,2015,9351:234-241.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional netw orks for semantic segmentation">

                                <b>[13]</b> Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:3431-3440.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Progressive attention guided recurrent network for salient object detection">

                                <b>[14]</b> Zhang X N,Wang T T,Qi J Q,<i>et al</i>.Progressive attention guided recurrent network for salient object detection[C]∥2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT.New York:IEEE,2018:714-722.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learned contextual feature reweighting for image geolocalization">

                                <b>[15]</b> Kim H J,Dunn E,Frahm J M.Learned contextual feature reweighting for image geo-localization[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI.New York:IEEE,2017:3251-3260.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Receptive field block net for accurate and fast object detection">

                                <b>[16]</b> Liu S T,Huang D,Wang Y H.Receptive field block net for accurate and fast object detection[M]//Ferrari V,Hebert M,Sminchisescu C,<i>et al</i>.Computer vision-ECCV 2018.Lecture notes in computer science.Cham:Springer,2018,11215:404-419.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inception-v4 Inception-ResNet and the Impact of Residual Connections on Learning">

                                <b>[17]</b> Szegedy C,Ioffe S,Vanhoucke V,<i>et al</i>.Inception-v4,inception-resnet and the impact of residual connections on learning[C]//31st AAAI Conference on Artificial Intelligence,February 4-10,2017,San Francisco.California:AAAI Press,2017:4278-4284.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_18" >
                                    <b>[18]</b>
                                 Hu J,Shen L,Sun G.Squeeze-and-excitation networks[C]∥2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT.New York:IEEE,2018:7132-7141.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recurrent convolutional neural network for object recognition">

                                <b>[19]</b> Liang M,Hu X L.Recurrent convolutional neural network for object recognition[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:3367-3375.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DHSNet:Deep Hierarchical Saliency Network for Salient Object Detection">

                                <b>[20]</b> Liu N,Han J W.DHSNet:deep hierarchical saliency network for salient object detection[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:678-686.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deeply-supervised recurrent convolutional neural network for saliency detection">

                                <b>[21]</b> Tang Y B,Wu X Q,Bu W.Deeply-supervised recurrent convolutional neural network for saliency detection[C]∥The 24th ACM International Conference on Multimedia,October 15-19,2016,Amsterdam,Netherlands.USA:ACM,2016:397-401.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical saliency detection">

                                <b>[22]</b> Yan Q,Xu L,Shi J P,<i>et al</i>.Hierarchical saliency detection[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:1155-1162.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual saliency based on multiscale deep features">

                                <b>[23]</b> Li G B,Yu Y Z.Visual saliency based on multiscale deep features[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:5455-5463.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Saliency detection via graph-based manifold ranking">

                                <b>[24]</b> Yang C,Zhang L H,Lu H C,<i>et al</i>.Saliency detection via graph-based manifold ranking[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3166-3173.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Frequency-tuned salient region detection">

                                <b>[25]</b> Achanta R,Hemami S,Estrada F,<i>et al</i>.Frequency-tuned salient region detection[C]∥2009 IEEE Conference on Computer Vision and Pattern Recognition,June 20-25,2009,Miami,FL,USA.New York:IEEE,2009:1597-1604.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Stagewise Refinement Model for Detecting Salient Objects in Images">

                                <b>[26]</b> Wang T T,Borji A,Zhang L H,<i>et al</i>.A stagewise refinement model for detecting salient objects in images[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice.New York:IEEE,2017:4019-4028.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Amulet:Aggregating Multi-level Convolutional Features for Salient Object Detection">

                                <b>[27]</b> Zhang P P,Wang D,Lu H C,<i>et al</i>.Amulet:aggregating multi-level convolutional features for salient object detection[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice.New York:IEEE,2017:202-211.
                            </a>
                        </p>
                        <p id="65">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Uncertain Convolutional Features for Accurate Saliency Detection">

                                <b>[28]</b> Zhang P P,Wang D,Lu H C,<i>et al</i>.Learning uncertain convolutional features for accurate saliency detection[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice.New York:IEEE,2017:212-221.
                            </a>
                        </p>
                        <p id="67">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Kernelized subspace ranking for saliency detection">

                                <b>[29]</b> Wang T T,Zhang L H,Lu H C,<i>et al</i>.Kernelized subspace ranking for saliency detection[M]//Leibe B,Matas J,Sebe N,<i>et al</i>.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9912:450-466.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201909032" />
        <input id="dpi" type="hidden" value="200" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201909032&amp;v=MDgwMjRSTE9lWmVWdkZ5emtWYnZMSWpYVGJMRzRIOWpNcG85R1pvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

