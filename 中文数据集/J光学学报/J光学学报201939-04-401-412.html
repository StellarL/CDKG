

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134126219971250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201904047%26RESULT%3d1%26SIGN%3dzpKwjwXkf6DUyPPCe7SzxOFV0%252bo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201904047&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201904047&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201904047&amp;v=MDE5OTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhsVXJ6SklqWFRiTEc0SDlqTXE0OUJZNFE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#88" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="2 遥感图像语义分割方法 ">2 遥感图像语义分割方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="&lt;b&gt;2.1 基于简单分类的方法&lt;/b&gt;"><b>2.1 基于简单分类的方法</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;2.2 基于CNNs的方法&lt;/b&gt;"><b>2.2 基于CNNs的方法</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;2.3 基于FCN的方法&lt;/b&gt;"><b>2.3 基于FCN的方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#102" data-title="3 基于FCN的非均衡遥感图像语义分割 ">3 基于FCN的非均衡遥感图像语义分割</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#103" data-title="&lt;b&gt;3.1 模型体系结构&lt;/b&gt;"><b>3.1 模型体系结构</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;3.2 数据预处理&lt;/b&gt;"><b>3.2 数据预处理</b></a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;3.3 模型训练&lt;/b&gt;"><b>3.3 模型训练</b></a></li>
                                                <li><a href="#139" data-title="&lt;b&gt;3.4 自适应阈值&lt;/b&gt;"><b>3.4 自适应阈值</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#145" data-title="4 语义分割实验与结果 ">4 语义分割实验与结果</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#146" data-title="&lt;b&gt;4.1 软硬件环境&lt;/b&gt;"><b>4.1 软硬件环境</b></a></li>
                                                <li><a href="#148" data-title="&lt;b&gt;4.2 数据集&lt;/b&gt;"><b>4.2 数据集</b></a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;4.3 实验结果&lt;/b&gt;"><b>4.3 实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#166" data-title="5 结 论 ">5 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#99" data-title="图1 基于图像块的CNN与FCN的对比。 (a) CNN; (b) FCN">图1 基于图像块的CNN与FCN的对比。 (a) CNN; (b) FCN</a></li>
                                                <li><a href="#105" data-title="图2 模型体系结构">图2 模型体系结构</a></li>
                                                <li><a href="#150" data-title="表1 DSTL数据集在各波段的具体参数">表1 DSTL数据集在各波段的具体参数</a></li>
                                                <li><a href="#154" data-title="图3 DSTL数据集中各类别的分布">图3 DSTL数据集中各类别的分布</a></li>
                                                <li><a href="#155" data-title="图4 DSTL数据集的图像标注">图4 DSTL数据集的图像标注</a></li>
                                                <li><a href="#157" data-title="图5 训练过程。 (a) 精度; (b) 损失函数; (c) Jaccard_coef; (d) Jaccard_coef_int">图5 训练过程。 (a) 精度; (b) 损失函数; (c) Jaccard_coef; (d) J......</a></li>
                                                <li><a href="#158" data-title="图6 模型预测的结果">图6 模型预测的结果</a></li>
                                                <li><a href="#162" data-title="图7 各类阈值和评价值之间的关系">图7 各类阈值和评价值之间的关系</a></li>
                                                <li><a href="#163" data-title="图8 所提方法的实验结果。 (a) ～ (c) 真实值; (d) ～ (f) 经自适应阈值处理的结果; (g) ～ (i) 未经自适应阈值处理的结果">图8 所提方法的实验结果。 (a) ～ (c) 真实值; (d) ～ (f) 经自适应阈值处理的结果......</a></li>
                                                <li><a href="#165" data-title="表2 各类别的最佳阈值">表2 各类别的最佳阈值</a></li>
                                                <li><a href="#168" data-title="图9 所提方法的实验结果。 (a) ～ (c) 基于图像块的CNN模型结果; (d) ～ (f) 采用自适应阈值和未采用数据增强的结果; (g) ～ (i) 未采用自适应阈值和数据增强的结果">图9 所提方法的实验结果。 (a) ～ (c) 基于图像块的CNN模型结果; (d) ～ (f) 采......</a></li>
                                                <li><a href="#169" data-title="图10 小类的实验结果。 (a) (b) 原图; (c) (d) 真实值; (e) (f) 所提方法的结果; (g) (h) 基本U-Net模型的结果">图10 小类的实验结果。 (a) (b) 原图; (c) (d) 真实值; (e) (f) 所提方法......</a></li>
                                                <li><a href="#170" data-title="图11 算法性能对比">图11 算法性能对比</a></li>
                                                <li><a href="#171" data-title="表3 算法性能对比 (Jaccard指数) ">表3 算法性能对比 (Jaccard指数) </a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" title=" Kaiser P, Wegner J D, Lucchi A, &lt;i&gt;et al&lt;/i&gt;.Learning aerial image segmentation from online maps[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (11) :6054-6068." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning aerial image segmentation from online maps">
                                        <b>[1]</b>
                                         Kaiser P, Wegner J D, Lucchi A, &lt;i&gt;et al&lt;/i&gt;.Learning aerial image segmentation from online maps[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (11) :6054-6068.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" Liu F, Li L.Evaluation of information acquisition capability of optical remote sensing satellites[J].Optics and Precision Engineering, 2017, 25 (9) :2454-2460.刘锋, 李琳.光学遥感卫星信息获取能力指数的评估[J].光学精密工程, 2017, 25 (9) :2454-2460." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201709024&amp;v=MjY0MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIbFVyekpJalhCWTdHNEg5Yk1wbzlIWUk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Liu F, Li L.Evaluation of information acquisition capability of optical remote sensing satellites[J].Optics and Precision Engineering, 2017, 25 (9) :2454-2460.刘锋, 李琳.光学遥感卫星信息获取能力指数的评估[J].光学精密工程, 2017, 25 (9) :2454-2460.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" title=" Trivedi M M.Object detection in multispectral high resolution images[J].Proceedings of SPIE, 1988, 0933:8-15." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object detection in multispectral high resolution images">
                                        <b>[3]</b>
                                         Trivedi M M.Object detection in multispectral high resolution images[J].Proceedings of SPIE, 1988, 0933:8-15.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" title=" Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[J].Communications of the ACM, 2017, 60 (6) :84-90." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMD71033EF8EE149F894A1F173A0F662B0&amp;v=Mjc5MzhwbWFCdUhZZk9HUWxmQ3BiUTM1ZGhoemJpK3c2az1OaWZJWThlL0g5SFByUG96Yko1NkRYZ3d1UjRhN2s1OFBuN2xyMk0xRDdTU1I4aWZDT052RlNpV1dyN0pJRg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[J].Communications of the ACM, 2017, 60 (6) :84-90.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" title=" Liu F, Shen T S, Ma X X, &lt;i&gt;et al&lt;/i&gt;.Ship recognition based on multi-band deep neural network[J].Optics and Precision Engineering, 2017, 25 (11) :2939-2946.刘峰, 沈同圣, 马新星, 等.基于多波段深度神经网络的舰船目标识别[J].光学精密工程, 2017, 25 (11) :2939-2946." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201711022&amp;v=MDY5MDBJalhCWTdHNEg5Yk5ybzlIWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIbFVyeko=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Liu F, Shen T S, Ma X X, &lt;i&gt;et al&lt;/i&gt;.Ship recognition based on multi-band deep neural network[J].Optics and Precision Engineering, 2017, 25 (11) :2939-2946.刘峰, 沈同圣, 马新星, 等.基于多波段深度神经网络的舰船目标识别[J].光学精密工程, 2017, 25 (11) :2939-2946.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" title=" Penatti O A B, Nogueira K, dos Santos J A.Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?[C]//IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) , 2015:44-51." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?">
                                        <b>[6]</b>
                                         Penatti O A B, Nogueira K, dos Santos J A.Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?[C]//IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) , 2015:44-51.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" Zhong Y F, Fei F, Liu Y F, &lt;i&gt;et al&lt;/i&gt;.SatCNN:satellite image dataset classification using agile convolutional neural networks[J].Remote Sensing Letters, 2017, 8 (2) :136-145." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD4E1F73FD33C38F6EB304F9595132EA65&amp;v=MDM3ODhaK2g4RDNSUHlXTmg2VDk1UG5ibnBSYzBlckRoTkx5YUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRoaHpiaSt3Nms9TmpuQmFyZk5INmZMclBreA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Zhong Y F, Fei F, Liu Y F, &lt;i&gt;et al&lt;/i&gt;.SatCNN:satellite image dataset classification using agile convolutional neural networks[J].Remote Sensing Letters, 2017, 8 (2) :136-145.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" title=" Marmanis D, Datcu M, Esch T, &lt;i&gt;et al&lt;/i&gt;.Deep learning earth observation classification using ImageNet pretrained networks[J].IEEE Geoscience and Remote Sensing Letters, 2016, 13 (1) :105-109." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning earth observation classification using image Net pretrained networks">
                                        <b>[8]</b>
                                         Marmanis D, Datcu M, Esch T, &lt;i&gt;et al&lt;/i&gt;.Deep learning earth observation classification using ImageNet pretrained networks[J].IEEE Geoscience and Remote Sensing Letters, 2016, 13 (1) :105-109.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" Garcia-Garcia A, Orts-Escolano S, Oprea S, &lt;i&gt;et al&lt;/i&gt;.A survey on deep learning techniques for image and video semantic segmentation[J].Applied Soft Computing, 2018, 70:41-65." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3D5B9FF840C5784BE15B377C0EBD6BF8&amp;v=MTgxMjdPZmJETUc2UEYyZmxOWU90OENYc3h5MlJtNnpvUFMzamwzeEpBQzhhU044eVhDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh6YmkrdzZrPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Garcia-Garcia A, Orts-Escolano S, Oprea S, &lt;i&gt;et al&lt;/i&gt;.A survey on deep learning techniques for image and video semantic segmentation[J].Applied Soft Computing, 2018, 70:41-65.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" title=" 2D semantic labeling contest[EB/OL]. (2018-09-05) [2018-10-15].http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=2D semantic labeling contest">
                                        <b>[10]</b>
                                         2D semantic labeling contest[EB/OL]. (2018-09-05) [2018-10-15].http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" title=" Marmanis D, Wegner J D, Galliani S, &lt;i&gt;et al&lt;/i&gt;.Semantic segmentation of aerial images with an ensemble of CNNs[J].ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, 2016, III-3:473-480." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic segmentation of aerial images with an ensemble of CNSS">
                                        <b>[11]</b>
                                         Marmanis D, Wegner J D, Galliani S, &lt;i&gt;et al&lt;/i&gt;.Semantic segmentation of aerial images with an ensemble of CNNs[J].ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, 2016, III-3:473-480.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" title=" Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic seg-mentation">
                                        <b>[12]</b>
                                         Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" title=" Chen X.Relationships between evaluation criteria of feature selection and analysis on class imbalance problem over VHR remote sensing imagery[D].Shanghai:Shanghai Jiao Tong University, 2011.陈曦.特征选择准则间的关联及高分辨率遥感影像类别不平衡问题研究[D].上海:上海交通大学, 2011." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1012016720.nh&amp;v=MDA0NjdlWmVWdUZ5SGxVcnpKVkYyNkhMTzVHTmJPcjVFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Chen X.Relationships between evaluation criteria of feature selection and analysis on class imbalance problem over VHR remote sensing imagery[D].Shanghai:Shanghai Jiao Tong University, 2011.陈曦.特征选择准则间的关联及高分辨率遥感影像类别不平衡问题研究[D].上海:上海交通大学, 2011.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" Decatur.Application of neural networks to terrain classification[C]//International 1989 Joint Conference on Neural Networks, 1989:283-288." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Application of neural networks to terrain classification">
                                        <b>[14]</b>
                                         Decatur.Application of neural networks to terrain classification[C]//International 1989 Joint Conference on Neural Networks, 1989:283-288.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" Cai Q, Hao J Y, Cao J, &lt;i&gt;et al&lt;/i&gt;.Salient detection via local and global feature[J].Optics and Precision Engineering, 2017, 25 (3) :772-778.蔡强, 郝佳云, 曹健, 等.结合局部特征及全局特征的显著性检测[J].光学精密工程, 2017, 25 (3) :772-778." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201703031&amp;v=MDM3MjhIOWJNckk5R1pZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGxVcnpKSWpYQlk3RzQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Cai Q, Hao J Y, Cao J, &lt;i&gt;et al&lt;/i&gt;.Salient detection via local and global feature[J].Optics and Precision Engineering, 2017, 25 (3) :772-778.蔡强, 郝佳云, 曹健, 等.结合局部特征及全局特征的显著性检测[J].光学精密工程, 2017, 25 (3) :772-778.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" title=" Bischof H, Schneider W, Pinz A J.Multispectral classification of Landsat-images using neural networks[J].IEEE Transactions on Geoscience and Remote Sensing, 1992, 30 (3) :482-490." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multispectral classification of landsat-images using neural networks">
                                        <b>[16]</b>
                                         Bischof H, Schneider W, Pinz A J.Multispectral classification of Landsat-images using neural networks[J].IEEE Transactions on Geoscience and Remote Sensing, 1992, 30 (3) :482-490.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" title=" Mountrakis G, Im J, Ogole C.Support vector machines in remote sensing:a review[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2011, 66 (3) :247-259." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501968436&amp;v=MTQ4OTNpZk9mYks3SHRETnFvOUViZTBIQ0g4L29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUtWNFZhUk09Tg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         Mountrakis G, Im J, Ogole C.Support vector machines in remote sensing:a review[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2011, 66 (3) :247-259.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" Pal M.Random forest classifier for remote sensing classification[J].International Journal of Remote Sensing, 2005, 26 (1) :217-222." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD713863214&amp;v=MjQ2NDhaZVZ1RnlIbFVyekpOam5CYXJTNUhkbktySTFFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Pal M.Random forest classifier for remote sensing classification[J].International Journal of Remote Sensing, 2005, 26 (1) :217-222.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_19" title=" Atkinson P M, Lewis P.Geostatistical classification for remote sensing:an introduction[J].Computers &amp;amp; Geosciences, 2000, 26 (4) :361-371." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600161209&amp;v=MjI0ODNRVE1ud1plWnVIeWptVWIvSUtWNFZhUk09TmlmT2ZiSzdIdEROcVk5RlplME9Ebnd3b0JNVDZUNFBRSC9pclJkR2VycQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         Atkinson P M, Lewis P.Geostatistical classification for remote sensing:an introduction[J].Computers &amp;amp; Geosciences, 2000, 26 (4) :361-371.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_20" title=" Han J W, Zhang D W, Cheng G, &lt;i&gt;et al&lt;/i&gt;.Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (6) :3325-3337." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object detection in optical remote sensing images based on weakly supervised learning and high-level feature Learning">
                                        <b>[20]</b>
                                         Han J W, Zhang D W, Cheng G, &lt;i&gt;et al&lt;/i&gt;.Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (6) :3325-3337.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_21" title=" Zhang Q C, Tong G F, Li Y, &lt;i&gt;et al&lt;/i&gt;.River detection in remote sensing images based on multi-feature fusion and soft voting[J].Acta Optica Sinica, 2018, 38 (6) :0628002.张庆春, 佟国峰, 李勇, 等.基于多特征融合和软投票的遥感图像河流检测[J].光学学报, 2018, 38 (6) :0628002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806043&amp;v=MTc4MDdlWmVWdUZ5SGxVcnpKSWpYVGJMRzRIOW5NcVk5Qlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         Zhang Q C, Tong G F, Li Y, &lt;i&gt;et al&lt;/i&gt;.River detection in remote sensing images based on multi-feature fusion and soft voting[J].Acta Optica Sinica, 2018, 38 (6) :0628002.张庆春, 佟国峰, 李勇, 等.基于多特征融合和软投票的遥感图像河流检测[J].光学学报, 2018, 38 (6) :0628002.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_22" title=" Gao X J, Zheng X D, Liu Z X, &lt;i&gt;et al&lt;/i&gt;.Automatic building extraction from high resolution visible images based on shifted shadow analysis[J].Acta Optica Sinica, 2017, 37 (4) :0428002.高贤君, 郑学东, 刘子潇, 等.基于偏移阴影分析的高分辨率可见光影像建筑物自动提取[J].光学学报, 2017, 37 (4) :0428002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201704037&amp;v=MDI0MjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIbFVyekpJalhUYkxHNEg5Yk1xNDlHWTRRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         Gao X J, Zheng X D, Liu Z X, &lt;i&gt;et al&lt;/i&gt;.Automatic building extraction from high resolution visible images based on shifted shadow analysis[J].Acta Optica Sinica, 2017, 37 (4) :0428002.高贤君, 郑学东, 刘子潇, 等.基于偏移阴影分析的高分辨率可见光影像建筑物自动提取[J].光学学报, 2017, 37 (4) :0428002.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_23" title=" Mnih V.Machine learning for aerial image labeling[D].Toronto:University of Toronto, 2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Machine learning for aerial image labeling">
                                        <b>[23]</b>
                                         Mnih V.Machine learning for aerial image labeling[D].Toronto:University of Toronto, 2013.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_24" title=" Sharma A, Liu X W, Yang X J, &lt;i&gt;et al&lt;/i&gt;.A patch-based convolutional neural network for remote sensing image classification[J].Neural Networks, 2017, 95:19-28." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1F9E01344C238F05C7C64E9D96B0B9BC&amp;v=MTQzMTBOdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGhoemJpK3c2az1OaWZPZmJMT0Y2VE1yb3hCWUpnTkQzUlB6eE5nN1V4N1RBcnIyQnN6QzdMbVRNanNDTw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                         Sharma A, Liu X W, Yang X J, &lt;i&gt;et al&lt;/i&gt;.A patch-based convolutional neural network for remote sensing image classification[J].Neural Networks, 2017, 95:19-28.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_25" title=" Badrinarayanan V, Kendall A, Cipolla R.SegNet:A deep convolutional encoder-decoder architecture for image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (12) :2481-2495." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SegNet:A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation">
                                        <b>[25]</b>
                                         Badrinarayanan V, Kendall A, Cipolla R.SegNet:A deep convolutional encoder-decoder architecture for image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (12) :2481-2495.
                                    </a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_26" title=" Ronneberger O, Fischer P, Brox T.U-Net:Convolutional Networks for Biomedical Image Segmentation[M]//Ronneberger O, Fischer P, Brox T.eds.Lecture Notes in Computer Science.Cham:Springer International Publishing, 2015:234-241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-net:Convolutional networks for biomedical image segmentation">
                                        <b>[26]</b>
                                         Ronneberger O, Fischer P, Brox T.U-Net:Convolutional Networks for Biomedical Image Segmentation[M]//Ronneberger O, Fischer P, Brox T.eds.Lecture Notes in Computer Science.Cham:Springer International Publishing, 2015:234-241.
                                    </a>
                                </li>
                                <li id="62">


                                    <a id="bibliography_27" >
                                        <b>[27]</b>
                                     Noh H, Hong S, Han B.Learning deconvolution network for semantic segmentation[C]//2015 IEEE International Conference on Computer Vision (ICCV) , 2015:1520-1528.</a>
                                </li>
                                <li id="64">


                                    <a id="bibliography_28" title=" Chen L C, Papandreou G, Kokkinos I, &lt;i&gt;et al&lt;/i&gt;.DeepLab:semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (4) :834-848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Lab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs">
                                        <b>[28]</b>
                                         Chen L C, Papandreou G, Kokkinos I, &lt;i&gt;et al&lt;/i&gt;.DeepLab:semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (4) :834-848.
                                    </a>
                                </li>
                                <li id="66">


                                    <a id="bibliography_29" title=" Muruganandham S.Semantic segmentation of satellite images using deep learning[D].Lulea:Lulea University of Technology, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic segmentation of satellite images using deep learning">
                                        <b>[29]</b>
                                         Muruganandham S.Semantic segmentation of satellite images using deep learning[D].Lulea:Lulea University of Technology, 2016.
                                    </a>
                                </li>
                                <li id="68">


                                    <a id="bibliography_30" title=" Wei Y N, Wang Z L, Xu M.Road structure refined CNN for road extraction in aerial image[J].IEEE Geoscience and Remote Sensing Letters, 2017, 14 (5) :709-713." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Road structure refined CNN for road extraction in aerial image">
                                        <b>[30]</b>
                                         Wei Y N, Wang Z L, Xu M.Road structure refined CNN for road extraction in aerial image[J].IEEE Geoscience and Remote Sensing Letters, 2017, 14 (5) :709-713.
                                    </a>
                                </li>
                                <li id="70">


                                    <a id="bibliography_31" title=" Kaiser P.Learning city structures from online maps[D].Zurich:ETH Zurich, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning city structures from online maps">
                                        <b>[31]</b>
                                         Kaiser P.Learning city structures from online maps[D].Zurich:ETH Zurich, 2016.
                                    </a>
                                </li>
                                <li id="72">


                                    <a id="bibliography_32" title=" Kampffmeyer M, Salberg A B, Jenssen R.Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks[C]//IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) , 2016:680-688." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic Segmentation of Small Objects and Modeling of Uncertainty in Urban Remote Sensing Images Using Deep Convolutional Neural Networks">
                                        <b>[32]</b>
                                         Kampffmeyer M, Salberg A B, Jenssen R.Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks[C]//IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) , 2016:680-688.
                                    </a>
                                </li>
                                <li id="74">


                                    <a id="bibliography_33" title=" Kemker R, Salvaggio C, Kanan C.Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2018, 145:60-77." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES5B7962BC3BA907C7D62A85F38063B243&amp;v=MDY2MTdiaSt3Nms9TmlmT2ZiYktHZGpLcmYwMlo1bCtCWHcrdkJGbjdEME1RSHFVcnhvMWY3SG1SNzZjQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGhoeg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[33]</b>
                                         Kemker R, Salvaggio C, Kanan C.Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2018, 145:60-77.
                                    </a>
                                </li>
                                <li id="76">


                                    <a id="bibliography_34" title=" Maggiori E, Tarabalka Y, Charpiat G, &lt;i&gt;et al&lt;/i&gt;.Can semantic labeling methods generalize to any city?the inria aerial image labeling benchmark[C]//IEEE International Geoscience and Remote Sensing Symposium (IGARSS) , 2017:3226-3229." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Can semantic labeling methods generalize to any city? The inria aerial image labeling benchmark">
                                        <b>[34]</b>
                                         Maggiori E, Tarabalka Y, Charpiat G, &lt;i&gt;et al&lt;/i&gt;.Can semantic labeling methods generalize to any city?the inria aerial image labeling benchmark[C]//IEEE International Geoscience and Remote Sensing Symposium (IGARSS) , 2017:3226-3229.
                                    </a>
                                </li>
                                <li id="78">


                                    <a id="bibliography_35" title=" Tuia D, Moser G, Le Saux B, &lt;i&gt;et al&lt;/i&gt;.IEEE GRSS data fusion contest:open data for global multimodal land use classification [technical committees][J].IEEE Geoscience and Remote Sensing Magazine, 2017, 5 (1) :70-73." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=IEEE GRSS data fusion contest:open data for global multimodal land use classification [technical committees]">
                                        <b>[35]</b>
                                         Tuia D, Moser G, Le Saux B, &lt;i&gt;et al&lt;/i&gt;.IEEE GRSS data fusion contest:open data for global multimodal land use classification [technical committees][J].IEEE Geoscience and Remote Sensing Magazine, 2017, 5 (1) :70-73.
                                    </a>
                                </li>
                                <li id="80">


                                    <a id="bibliography_36" title=" Dstl satellite imagery feature detection[EB/OL]. (2018-09-05) [2018-10-15].https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dstl satellite imagery feature detection">
                                        <b>[36]</b>
                                         Dstl satellite imagery feature detection[EB/OL]. (2018-09-05) [2018-10-15].https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection.
                                    </a>
                                </li>
                                <li id="82">


                                    <a id="bibliography_37" title=" Kingma D P, Ba J.Adam:a method for stochastic optimization[EB/OL]. (2014-12-22) [2018-10-15].https://arxiv.org/abs/1412.6980 ." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">
                                        <b>[37]</b>
                                         Kingma D P, Ba J.Adam:a method for stochastic optimization[EB/OL]. (2014-12-22) [2018-10-15].https://arxiv.org/abs/1412.6980 .
                                    </a>
                                </li>
                                <li id="84">


                                    <a id="bibliography_38" title=" Amin H H, Deabes W, Bouazza K.Clustering of user activities based on adaptive threshold spiking neural networks[C]//Ninth International Conference on Ubiquitous and Future Networks (ICUFN) , 2017:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering of user activities based on adaptive threshold spiking neural networks">
                                        <b>[38]</b>
                                         Amin H H, Deabes W, Bouazza K.Clustering of user activities based on adaptive threshold spiking neural networks[C]//Ninth International Conference on Ubiquitous and Future Networks (ICUFN) , 2017:1-6.
                                    </a>
                                </li>
                                <li id="86">


                                    <a id="bibliography_39" title=" Parvat A, Chavan J, Kadam S, &lt;i&gt;et al&lt;/i&gt;.A survey of deep-learning frameworks[C]//International Conference on Inventive Systems and Control (ICISC) , 2017:1-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A survey of deep-learning frameworks">
                                        <b>[39]</b>
                                         Parvat A, Chavan J, Kadam S, &lt;i&gt;et al&lt;/i&gt;.A survey of deep-learning frameworks[C]//International Conference on Inventive Systems and Control (ICISC) , 2017:1-7.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-08 10:05</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(04),401-412 DOI:10.3788/AOS201939.0428004            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>类别非均衡遥感图像语义分割的全卷积网络方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E6%AD%A2%E9%94%BE&amp;code=39208736&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴止锾</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E6%B0%B8%E6%98%8E&amp;code=39208735&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高永明</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E7%A3%8A&amp;code=39208734&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李磊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%96%9B%E4%BF%8A%E8%AF%97&amp;code=38963292&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">薛俊诗</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%88%AA%E5%A4%A9%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E7%A0%94%E7%A9%B6%E7%94%9F%E9%99%A2&amp;code=1702146&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">航天工程大学研究生院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=63883%E9%83%A8%E9%98%9F&amp;code=0046167&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">63883部队</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%88%AA%E5%A4%A9%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E8%88%AA%E5%A4%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">航天工程大学航天信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%88%AA%E5%A4%A9%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%B8%8E%E5%85%89%E5%AD%A6%E5%B7%A5%E7%A8%8B%E7%B3%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">航天工程大学电子与光学工程系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于U-Net模型, 提出了一个全卷积网络 (FCN) 模型, 用于高分辨率遥感图像语义分割, 其中数据预处理采用了数据标准化和数据增强, 模型训练过程采用Adam优化器, 模型性能评估采用平均Jaccard指数。为提高小类预测的准确率, 模型中采用了加权交叉熵损失函数和自适应阈值方法。在DSTL数据集上进行了实验, 结果表明所提方法将预测结果的平均Jaccard指数从0.611提升到0.636, 可实现对高分辨率遥感图像端到端的精确分类。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%A5%E6%84%9F%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遥感图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%B1%BB%E5%88%AB%E9%9D%9E%E5%9D%87%E8%A1%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">类别非均衡;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全卷积网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *吴止锾, E-mail:wuzhihuan@hotmail.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-22</p>

                    <p>

                            <b>基金：</b>
                                                        <span>军内科研项目 (16A804);</span>
                    </p>
            </div>
                    <h1><b>Fully Convolutional Network Method of Semantic Segmentation of Class Imbalance Remote Sensing Images</b></h1>
                    <h2>
                    <span>Wu Zhihuan</span>
                    <span>Gao Yongming</span>
                    <span>Li Lei</span>
                    <span>Xue Junshi</span>
            </h2>
                    <h2>
                    <span>Graduate School, Space Engineering University</span>
                    <span>63883 Troops</span>
                    <span>School of Space Information, Space Engineering University</span>
                    <span>Department of Electronic and Optical Engineering, Space Engineering University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A fully convolutional network (FCN) model based on U-Net is proposed to implement the semantic segmentation of remote sensing images with high resolution, in which the data standardization and data augmentation are adopted for data preprocessing. In addition, the Adam optimizer is used for the model training and the average Jaccard index is used as the evaluation metric. A weighted cross entropy loss function and an adaptive threshold algorithm are employed to improve the classification accuracy of small classes. The experimental results on the DSTL dataset show that the proposed method can increase the average Jaccard index of prediction results from 0.611 to 0.636, and produces an accurate end-to-end classification for high-resolution remote sensing images.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=remote%20sensing%20images&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">remote sensing images;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semantic%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semantic segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=class%20imbalance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">class imbalance;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fully%20convolutional%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fully convolutional network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-22</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="88" name="88" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="89">遥感图像的语义分割是图像的像素级分类, 是遥感图像目标识别应用的一个重要研究方向<citation id="173" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。随着遥感技术的快速发展, 高分辨率遥感卫星 (如IKONOS, SPOT-5, WorldView和Quickbird) 产生的遥感影像能够表现丰富的地物信息, 进而有利于提取地物的复杂特征及识别过去难以识别的人造目标<citation id="174" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。语义分割在计算机视觉 (CV) 和遥感领域得到了广泛的研究, 传统的方法主要是用特征提取方法将图像的像素信息转化到特征空间, 这类方法的共性是特征提取方法由领域专家人工设计, 虽然对于某些特定任务能够取得良好性能, 但是泛化能力不佳, 且任务条件变化时重新设计特征提取方法需要大量的时间和丰富的经验<citation id="175" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。这些不足促使研究人员开始寻找一种稳定且效率更高的方法。</p>
                </div>
                <div class="p1">
                    <p id="90">2017年Krizhevsky等<citation id="176" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>用一个7层卷积神经网络 (CNNs) AlexNet赢得了ILSVRC竞赛。随着数据资源的不断丰富和计算机性能特别是GPU技术的快速发展, 深度学习在计算机视觉领域的应用成为研究热点。近年来, CNNs广泛应用于各种图像识别问题, 并取得了显著成果<citation id="177" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。CNNs通过将卷积层的概念引入神经网络, 利用卷积操作的两个特点大大缩减了参数数量, 这两大特点分别是局部感受野和参数共享。与人类视觉聚集在局部小区域类似, 图像中局部像素之间的空间联系较紧密, 因而网络中每个神经元不必对整个网络进行感知。此外, 一般认为图像的统计特征与位置无关, 故可以把卷积核作为每一层的特征提取方法, 在卷积层之间采用最大池化层表征图像变换中隐含的不变性。在遥感领域, 一系列基于CNNs的分类研究在UCMerced数据集上获得的state-of-the-art结果<citation id="182" type="reference"><link href="20" rel="bibliography" /><link href="22" rel="bibliography" /><link href="24" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>, 证明了CNNs能够很好地泛化到遥感图像领域, 并取得比传统方法更好的结果。目前CNNs已经成为计算机视觉中语义分割的主要建模方法, 并且在遥感中发挥着越来越重要的作用<citation id="178" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。在ISPRS语义分割竞赛<citation id="179" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>中, 基于CNNs的方法占据着主导地位, 并获得了最佳性能<citation id="180" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。其中全卷积网络 (FCN) <citation id="181" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>通过用卷积层代替全连接的CNNs层, 以端对端的方式直接学习从图像像素到类别标签的映射, 是当前语义分割领域研究的热点。</p>
                </div>
                <div class="p1">
                    <p id="91">高分辨率遥感影像样本数据因地物的偏斜分布, 导致出现类别不均衡问题。不同类别数据量大小差异较大, 其中样本数量较少的称为小类, 样本数量较多的称为大类<citation id="183" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。类别不均衡问题限制了小类的分类精度, 进而降低了语义分割的平均分类精度。针对这一问题, 本文提出一个基于FCN的框架实现对高分辨率遥感图像端对端的语义分割。模型基于U-Net网络结构设计, 通过加权交叉熵损失函数提高小类的权重, 并通过自适应阈值算法调整各类别预测的置信度阈值, 从而改善了样本数据较少的小类语义分割结果, 最后使用英国国防科技实验室 (DSTL) 数据集进行实验验证。</p>
                </div>
                <h3 id="92" name="92" class="anchor-tag">2 遥感图像语义分割方法</h3>
                <h4 class="anchor-tag" id="93" name="93"><b>2.1 基于简单分类的方法</b></h4>
                <div class="p1">
                    <p id="94">早在1989年, 研究人员就开始使用机器学习方法解决遥感图像像素级分类问题<citation id="184" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。简单分类法分为特征提取和分类两个步骤。特征提取方法从直接使用不同光谱波段的亮度值作为特征, 发展到提取局部光谱和纹理特征, 如颜色直方图、方向梯度直方图 (HOG) 、局部二值模式 (LBP) 和尺度不变特征变换 (SIFT) 等<citation id="185" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。分类器早期多采用简单贝叶斯分类<citation id="186" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 为了明确学习复杂非线性的决策边界, 高分辨率遥感图像像元分类开始使用更为复杂的分类器, 如支持向量机 (SVM) <citation id="187" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、随机森林<citation id="188" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>和多种boosting算法<citation id="189" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。随着遥感图像分辨率的逐渐提高, 使用亮度值和浅层特征值 (如局部光谱和纹理特征) 的方法已无法为分类器提供足够的信息。最近的研究中多采用无监督或有监督的特征学习方法, 如稀疏编码, 受限波尔兹曼机<citation id="190" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>和CNNs, 从而显著地提高了分类精度。虽然这类方法目前在多个领域的研究也取得了优异的结果<citation id="191" type="reference"><link href="50" rel="bibliography" /><link href="52" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>, 但是往往仅针对某些特定类别或特定任务时, 还存在泛化能力不强的问题。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95"><b>2.2 基于CNNs的方法</b></h4>
                <div class="p1">
                    <p id="96">经典CNNs模型是一种图像-标记模型, 输出的是不同类别的概率分布, 因此语义分割任务不仅需要对整幅图像进行分类, 还要获得像素级分类结果。为此, Mnih<citation id="192" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>提出了基于图像块算法的CNNs, 并实现了对固定尺寸遥感图像像素级的训练和预测。图像块分类算法在每个像素点附近取一个固定尺寸的图像块, 作为一幅图像输入神经网络进行训练和预测, 该算法比简单的分类法具有更高的分类精度<citation id="193" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>。遥感图像中相邻两个像素的图像块相似度很高, 每个像素提取图像会产生很大的冗余信息, 导致图像块算法存在存储开销大、计算效率较低等问题。同时, 图像块尺寸直接决定了感受野的大小, 进而影响分类精度。如图1所示, 经典CNNs在卷积层之后采用全连接层将卷积层生成的特征图映射到固定长度的特征向量。最后一层通过sigmoid或softmax激活函数计算最终分类概率。与经典CNNs使用连接层来获得用于在卷积层 (完全连接层和softmax输出) 之后进行分类的固定长度特征向量不同的是, FCN用卷积层替换最后的全连接层<citation id="194" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 通过反卷积层操作对特征图进行上采样来产生与原图像相同空间维度的输出标记的图片。</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97"><b>2.3 基于FCN的方法</b></h4>
                <div class="p1">
                    <p id="98">针对图像块分类算法存在的问题, Shelhamer等<citation id="195" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出FCN模型用于图像分割。FCN将传统分类网络进行卷积化, 即将其中的用于输出分类标记的全连接层改为卷积层 (可将全连接层视作卷积核与上一层维度一致的卷积层) , 实现了输入和输出都是图像的端对端语义分割。经池化操作降低特征图的分辨率, 通过反卷积层学习一个用于上采样的滤波器, 提高特征图的分辨率。FCN中较浅的高分辨率层用来解决像素定位问题, 较深的层用来解决像素分类问题。该FCN具有3个优势:1) 有效解决了边缘图像不连续性;2) 简化了学习过程, 通过较少的参数训练出较高的精度;3) 充分利用GPU在卷积运算中的优势降低模型预测时间开销。FCN 模型具有可以生产任意尺寸的图像分割图且比图像块方法速度更快等优势, 成为当前图像语义分割领域最新研究的热点。近年来涌现出的大量基于FCN的语义分割模型 (如SegNet<citation id="196" type="reference"><link href="58" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、U-Net<citation id="197" type="reference"><link href="60" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、DeconvNet<citation id="198" type="reference"><link href="62" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>和Deeplab<citation id="199" type="reference"><link href="64" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>等) 和不断丰富的基准数据集 (PASCAL VOC、MS-COCO、 ADE20K和Cityscapes等) , 为研究更好的语义分割机器学习系统提供了重要支撑<citation id="200" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904047_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于图像块的CNN与FCN的对比。 (a) CNN; (b) FCN" src="Detail/GetImg?filename=images/GXXB201904047_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基于图像块的CNN与FCN的对比。 (a) CNN; (b) FCN  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904047_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Comparison of FCN and CNN based on image blocks. (a) CNN; (b) FCN</p>

                </div>
                <div class="p1">
                    <p id="101">近年来越来越多的研究开始关注FCN模型用于遥感图像的语义分割任务。Muruganandham等<citation id="209" type="reference"><link href="66" rel="bibliography" /><link href="68" rel="bibliography" /><link href="70" rel="bibliography" /><sup>[<a class="sup">29</a>,<a class="sup">30</a>,<a class="sup">31</a>]</sup></citation>利用FCN模型实现了道路和建筑的提取。Kampffmeyer等<citation id="201" type="reference"><link href="72" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>结合图像块分类算法和FCN方法实现了小目标分割。Kemker等<citation id="202" type="reference"><link href="74" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>利用合成图像进行了参数的初始化, 以避免模型过拟合。多个研究机构开始研究并提供遥感图像语义分割基准数据集, 如建筑和道路数据集<citation id="203" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>, 包括法国国家信息与自动化研究所Inria Aerial Image Labeling数据集<citation id="204" type="reference"><link href="76" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>、IEEE地球科学与遥感学会 (GRSS) 数据集<citation id="205" type="reference"><link href="78" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>、英国国防科技实验室 (DSTL) 数据集<citation id="206" type="reference"><link href="80" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>、国际摄影测量与遥感学会 (ISPRS) 数据集<citation id="207" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>以及罗彻斯特理工学院 (RIT) 的RIT-18数据集<citation id="208" type="reference"><link href="74" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>等。</p>
                </div>
                <h3 id="102" name="102" class="anchor-tag">3 基于FCN的非均衡遥感图像语义分割</h3>
                <h4 class="anchor-tag" id="103" name="103"><b>3.1 模型体系结构</b></h4>
                <div class="p1">
                    <p id="104">在ISBI 2015中赢得EM图像分割竞赛的U-Net模型<citation id="210" type="reference"><link href="60" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>是一种高度可扩展的FCN模型, U-Net模型对训练数据规模的要求较低, 分割精度较高, 在多个领域的语义分割任务中表现出了独特的潜力。所设计的模型采用了基于U-Net的对称编码器-解码器网络结构, 其模型体系结构如图2所示。图中Conv2D表示二维卷积, 编码器用来降低空间维度, 通过解码器上采样逐渐恢复对象的细节和空间维度。模型分为左侧收缩路径和右侧扩展路径, 收缩路径和传统卷积网络一致, 每次下采样过程可降低特征图的分辨率, 同时将特征通道数量翻倍。扩展路径包含一系列特征图的上采样过程, 采用2×2卷积核进行反卷积, 同时将对应的收缩路径上的特征图相连接, 进行两次3×3卷积, 进而弥补传统CNN在池化的过程中丢失位置信息的不足。在下采样路径中进行标准化 (batch-normalization) 操作后, 对上采样进行Dropout操作。模型中激活层采用ReLU激活函数, 网络训练过程中的损失函数为加权交叉熵损失函数, 优化函数为Adam优化器<citation id="211" type="reference"><link href="82" rel="bibliography" /><sup>[<a class="sup">37</a>]</sup></citation>。模型训练结束后, 采用自适应阈值算法动态调整各类别阈值以达到最佳的Jaccard指数。</p>
                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904047_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 模型体系结构" src="Detail/GetImg?filename=images/GXXB201904047_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 模型体系结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904047_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Architecture of model system</p>

                </div>
                <h4 class="anchor-tag" id="106" name="106"><b>3.2 数据预处理</b></h4>
                <h4 class="anchor-tag" id="107" name="107">1) 图像和特征标准化</h4>
                <div class="p1">
                    <p id="108">遥感图像数据类型是整型, 而神经网络的参数和激活函数通常初始化为[0, 1]之间的随机数, 需要采用标准化方法避免出现异常梯度。<i>z</i>-score标准化算法将输入图像的像素值调整为近似呈正态分布, 有利于优化梯度下降法的收敛。<i>η</i>、<i>σ</i>分别为<i>X</i>/Max (<i>X</i>) 的均值和标准差, 则标准化公式为</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>X</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mfrac><mi>X</mi><mrow><mtext>Μ</mtext><mtext>a</mtext><mtext>x</mtext><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>-</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">式中:<i>X</i>为输入值;Max (<i>X</i>) 为输入最大值。</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">2) 数据增强</h4>
                <div class="p1">
                    <p id="112">数据增强的目的是生成新的样本实例。当训练样本较少时, 数据增强对于提高网络的稳定性十分有益。对于遥感图像, 采用旋转45°、缩放15%～25%、剪切、切换波段、垂直和水平翻转等图像操作, 能够显著扩充数据集的规模, 从而增加其网络泛化能力。对验证集和测试集不进行数据增强。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113"><b>3.3 模型训练</b></h4>
                <h4 class="anchor-tag" id="114" name="114">1) 优化器</h4>
                <div class="p1">
                    <p id="115">神经网络训练最常见的优化方法是随机梯度下降 (SGD) 。SGD随机选取一个样本点做梯度下降, 则新权重为</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>w</mtext></mrow></msub><mo>=</mo><mi>θ</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>l</mtext><mtext>d</mtext></mrow></msub><mo>-</mo><mi>α</mi><mo>∇</mo><mi>J</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">式中:<i>θ</i><sub>new</sub>为新权重;<i>θ</i><sub>old</sub>为旧权重;<i>α</i>为学习率;<i>J</i><sub><i>t</i></sub> (<i>θ</i>) 为样本<i>t</i>的损失;∇表示梯度。SGD波动的特点使得优化方向从当前的局部极小值点跳到另一个更好的局部极小值点。对于非凸函数, SGD可使其最终收敛于一个较好的局部极值点, 甚至全局极值点。</p>
                </div>
                <div class="p1">
                    <p id="118">SGD存在寻找合适学习率困难和容易收敛到局部最优的问题。采用自适应矩估计方法 (Adam优化器) 来计算每个参数的自适应学习率, 该方法根据损失函数对每个参数梯度的一阶矩估计和二阶矩估计动态调整对于每个参数的学习率。该优化方法收敛速度快, 且能纠正学习率消失、收敛过慢或损失函数波动较大的问题。计算得到一阶矩偏差<mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>m</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>、二阶矩偏差<mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>v</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>分别为</p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula">
                        <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi>m</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>m</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mn>1</mn><mo>-</mo><mi>β</mi><msubsup><mrow></mrow><mn>1</mn><mi>t</mi></msubsup></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mover accent="true"><mi>v</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>v</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mn>1</mn><mo>-</mo><mi>β</mi><msubsup><mrow></mrow><mn>2</mn><mi>t</mi></msubsup></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="122">式中:m<sub>t</sub>为梯度均值;v<sub>t</sub>为梯度方差;β<mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>t</mi></msubsup></mrow></math></mathml>为一阶矩估计的指数衰减率;β<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>t</mi></msubsup></mrow></math></mathml>为二阶矩估计的指数衰减率。<i>Adam</i>优化公式可表示为</p>
                </div>
                <div class="p1">
                    <p id="125" class="code-formula">
                        <mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>θ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mfrac><mi>η</mi><mrow><msqrt><mrow><mover accent="true"><mi>v</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></msqrt><mo>+</mo><mi>ε</mi></mrow></mfrac><mover accent="true"><mi>m</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="126">式中:θ<sub>t+1</sub>为调整后参数;θ<sub>t</sub>为调整前参数;η为步长;ε为小常数 (默认为10<sup>-8</sup>) 。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127">2) 加权交叉熵损失函数</h4>
                <div class="p1">
                    <p id="128">交叉熵损失函数是深度神经网络常用的损失函数, 该函数在误差较大时权重更新快, 误差小时权重更新慢。经典的二元交叉熵损失函数为</p>
                </div>
                <div class="p1">
                    <p id="129" class="code-formula">
                        <mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><mo>∑</mo><mo stretchy="false">[</mo></mstyle><mi>y</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>u</mtext><mtext>e</mtext></mrow></msub><mi>ln</mi><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>u</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false">) </mo><mi>ln</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="130">式中:y<sub><i>true</i></sub>为真实值;y<sub><i>pred</i></sub>为预测值。</p>
                </div>
                <div class="p1">
                    <p id="131">由于遥感图像数据集中存在显著的类别非均衡问题, 对于小类目标, 其正样本相对负样本的数量较少。如果采用相同的权重, 网络会倾向于将多数像素预测为负样本, 极端情况下甚至全部预测为负样本也能得到较高的分类精度。因此, 采用加权交叉熵损失函数, 给正样本加上一定的权重。对于n类目标, 权重向量为{w<sub>1</sub>, w<sub>2</sub>, …, w<sub>n</sub>}, 则类别i的损失函数为</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><mo>∑</mo><mo stretchy="false">[</mo></mstyle><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mi>y</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>u</mtext><mtext>e</mtext></mrow></msub><mi>ln</mi><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>u</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false">) </mo><mi>ln</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133">式中:w<sub>i</sub>为该类别的权重。加权交叉熵损失函数中权重与该类正样本所占比例β=Y<sub>+</sub>/Y (其中Y<sub>+</sub>为正样本个数, Y为总样本数) 呈负相关。这里<mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>β</mi><mo stretchy="false">) </mo></mrow><mo>=</mo><mroot><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>β</mi><mo stretchy="false">) </mo><mo>/</mo><mi>β</mi></mrow><mi>α</mi></mroot></mrow></math></mathml>, 其中<i>α</i>为权重调节因子, <i>α</i>=2。</p>
                </div>
                <h4 class="anchor-tag" id="135" name="135">3) 评价方法</h4>
                <div class="p1">
                    <p id="136">由于在非均衡遥感图像语义分割中小类的负样本较多, 即使全部预测为多数类也可以达到较高的正确率, 直接使用分类正确率或错误率作为评价方法, 难以真实评价模型预测结果和地物真实情况的差距。因此所提算法的效能评价采用预测值和真实值的平均Jaccard指数, 即</p>
                </div>
                <div class="p1">
                    <p id="137" class="code-formula">
                        <mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mfrac><mrow><mi>q</mi><msubsup><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ρ</mtext></mrow><mi>i</mi></msubsup></mrow><mrow><mi>q</mi><msubsup><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ρ</mtext></mrow><mi>i</mi></msubsup><mo>+</mo><mi>q</mi><msubsup><mrow></mrow><mrow><mtext>F</mtext><mtext>Ρ</mtext></mrow><mi>i</mi></msubsup><mo>+</mo><mi>q</mi><msubsup><mrow></mrow><mrow><mtext>F</mtext><mtext>Ν</mtext></mrow><mi>i</mi></msubsup></mrow></mfrac></mrow></mstyle><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="138">式中:<i>N</i>为识别的类别数量;下标TP表示正确检测区域, FP表示误检区域, FN表示漏检区域;<i>q</i><sup><i>i</i></sup>为第<i>i</i>类的像元数量。</p>
                </div>
                <h4 class="anchor-tag" id="139" name="139"><b>3.4 自适应阈值</b></h4>
                <div class="p1">
                    <p id="140">FCN的输出是一个4维张量, 包含<i>m</i>幅<i>x</i>×<i>y</i>图像中每个像素属于<i>n</i>个类别的置信度。模型训练完成后, 需要设置每个类别的阈值<i>τ</i>以决定像素是否属于该类, 当<i>p</i>&gt;<i>τ</i> (<i>p</i>为分类概率) 时认为像素属于该类别。训练数据中存在的类别不均衡问题导致在训练数据较少的类别中预测准确率较低, 需降低阈值以提高该类别的查全率。自适应阈值方法<citation id="212" type="reference"><link href="84" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>利用验证集将每类阈值调整为具有最佳Jaccard指数的值。类别数为<i>n</i>时各类的阈值<i>τ</i>={<i>τ</i><sub>1</sub>, <i>τ</i><sub>2</sub>, …, <i>τ</i><sub><i>n</i></sub>}, <i>J</i><sup><i>i</i></sup> (<i>t</i>) 为类别<i>i</i> 在阈值<i>τ</i>下分割结果的Jaccard指数, 则类别<i>i</i>的阈值<i>τ</i><sub><i>i</i></sub>为</p>
                </div>
                <div class="p1">
                    <p id="141" class="code-formula">
                        <mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>τ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext><mo stretchy="false">[</mo><mi>J</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false"> (</mo><mi>τ</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="142">采用梯度上升算法寻找最佳阈值, 阈值取值空间为[0, 1]。求解类别<i>i</i>阈值的梯度上升算法可表示为</p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>τ</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>w</mtext></mrow></msub><mo>=</mo><mi>τ</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>l</mtext><mtext>d</mtext></mrow></msub><mo>+</mo><msup><mi>η</mi><mo>′</mo></msup><mo>∇</mo><mi>J</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false"> (</mo><mi>τ</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144">式中:<i>τ</i><sub>new</sub>为新阈值;<i>τ</i><sub>old</sub>为旧阈值;<i>η</i>′为学习率;∇<i>J</i><sup><i>i</i></sup> (<i>τ</i>) 表示Jaccard指数在<i>τ</i>处的梯度。</p>
                </div>
                <h3 id="145" name="145" class="anchor-tag">4 语义分割实验与结果</h3>
                <h4 class="anchor-tag" id="146" name="146"><b>4.1 软硬件环境</b></h4>
                <div class="p1">
                    <p id="147">采用后端为Tensorflow的Keras作为深度学习开发平台。Google的Tensorflow是业界最流行的深度学习开发框架之一<citation id="213" type="reference"><link href="86" rel="bibliography" /><sup>[<a class="sup">39</a>]</sup></citation>, Keras是一个易用的高层抽象的深度学习框架, 后端可以支持在CNTK、Tensorflow和Theano中切换。实验中用到的第三方库包括读取遥感图像数据的Tifffle、进行基本图像处理的OpenCV、处理多边形数据的shapely、作为可视化工具的matplotlib以及提供基础机器学习方法的scikit-learn等。服务器采用曙光W560-G20工作站, CPU为E5 2650v3, 运行内存32 GB, GPU为Nvidia GTX 1080 Ti。</p>
                </div>
                <h4 class="anchor-tag" id="148" name="148"><b>4.2 数据集</b></h4>
                <div class="p1">
                    <p id="149">采用DSTL数据集, 该数据集包含10类标记的样本数据。数据来自WorldView-3卫星, 包含25幅1 km<sup>2</sup>大小地区的高分辨率遥感图像, 包含全色、可见光、多光谱和短波红外等共20个波段。DSTL数据集在各波段的具体参数如表1所示。</p>
                </div>
                <div class="area_img" id="150">
                    <p class="img_tit">表1 DSTL数据集在各波段的具体参数 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Specifications of DSTL dataset at different bands</p>
                    <p class="img_note"></p>
                    <table id="150" border="1"><tr><td><br />Band</td><td>Radiometric resolution /bit</td><td>Spatial resolution /m</td><td>Size / (pixel×pixel) </td></tr><tr><td><br />RGB+P (450-690 nm) </td><td>11</td><td>0.31</td><td>3348×3392</td></tr><tr><td><br />M band (400-1040 nm) </td><td>11</td><td>1.24</td><td>837×848</td></tr><tr><td><br />A band (1195-2365 nm) </td><td>14</td><td>7.50</td><td>134×136</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="151">DSTL数据集包含10类地物目标:房屋、混杂人工建筑、道路、铁路、树木、农作物、河流、积水区、大型车辆和小型车辆。图3显示了数据集中各类别目标面积占数据总量比例的分布情况, 可以看出数据集中存在显著的类别非均衡问题。数据集中原始图像和标记地物目标的掩码图像如图4所示。实验部分采用了数据集中高分辨率的12个波段, 主要有分辨率为1.24 m的8个多光谱波段和分辨率为0.31 m的可见光波段, 对于较低分辨率的多光谱数据部分, 重采样至与RGB和全色部分相同的分辨率, 组合为12通道的数据集。</p>
                </div>
                <h4 class="anchor-tag" id="152" name="152"><b>4.3 实验结果</b></h4>
                <div class="p1">
                    <p id="153">模型在DSTL数据集上训练时, 数据集中20幅图像作为训练验证集, 5幅图像作为测试集, 训练验证集采用5-flod交叉验证, 所有对比实验均采用相同的数据集划分。将图像集归一化为具有零均值和单位方差的数据集, 将图像转换为一系列160 pixel×160 pixel的图像块作为输入, 并对输入数据进行数据增强。批大小设为32, 进行100个epoch的训练, 训练过程中采用三个性能评价指标跟踪:精度和Jaccard指数的两个估计值[Jaccard_coef (预测值为连续概率) 和Jaccard_coef_int (预测值为二元变量) ]。训练完成后用score函数对最终的模型性能进行评估。图5表示了三个性能评价函数以及损失函数随训练过程变化的情况, 其中Train表示训练数据上的变化, Val表示验证数据上的变化。在训练过程中用scikit-learn 库中的“Jaccard_similarity_score”评价方法得到验证集的最后得分为0.8752。图6表示训练完成后模型的预测效果, 将每类目标预测结果可视化为对应的掩码图像, 在实验的硬件条件下, 预测尺寸为3348 pixel×3392 pixel图像的平均时间约为3.42 s (运行时间统计仅为模型预测时间, 不考虑数据的读取和预处理时间) 。参数分别为<i>β</i><sub>1</sub>=0.9, <i>β</i><sub>2</sub>=0.999, <i>ε</i>=10<sup>-8</sup>, <i>η</i>′=0.001。此外, 在不使用数据增强的情况下进行实验, 同时与经典的patch-based CNN和二元逻辑回归方法进行了比较。</p>
                </div>
                <div class="area_img" id="154">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904047_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 DSTL数据集中各类别的分布" src="Detail/GetImg?filename=images/GXXB201904047_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 DSTL数据集中各类别的分布  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904047_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Class distribution of DSTL dataset</p>

                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904047_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 DSTL数据集的图像标注" src="Detail/GetImg?filename=images/GXXB201904047_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 DSTL数据集的图像标注  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904047_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Labels of image in DSTL dataset</p>

                </div>
                <div class="area_img" id="157">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904047_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 训练过程。 (a) 精度; (b) 损失函数; (c) Jaccard_coef; (d) Jaccard_coef_int" src="Detail/GetImg?filename=images/GXXB201904047_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 训练过程。 (a) 精度; (b) 损失函数; (c) Jaccard_coef; (d) Jaccard_coef_int  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904047_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Training process. (a) Accuracy; (b) loss function; (c) Jaccard_coef; (d) Jaccard_coef_int</p>

                </div>
                <div class="area_img" id="158">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904047_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 模型预测的结果" src="Detail/GetImg?filename=images/GXXB201904047_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 模型预测的结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904047_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Results predicted by model</p>

                </div>
                <div class="p1">
                    <p id="160">模型训练完成后, 采用自适应阈值方法获取各类别Jaccard指数最大的阈值。图7显示了各类别阈值及其Jaccard指数的关系, 表2为各类别最佳阈值的计算结果。实验过程中对真实值以及所提方法和基本FCN方法 (基本方法采用了相同的数据增强方法, 未采用自适应阈值算法和加权交叉熵损失函数) 的结果进行比较, 语义分割结果对比如图8和图9所示。其中, 未采用自适应阈值的方法采用类别1 (building类) 的Jaccard Index进行快速阈值选择, 选取0.4作为其固定阈值下的阈值。图10为图像中存在小类“vehicle small”和“vehicle large”的部分区域语义分割结果的掩码图, 可以看出, 所提出的方法显著改善了小类的语义分割结果。图11和表3均为算法性能的比较结果, 其中WCE表示加权交叉熵损失, AT表示自适应阈值, DA表示数据增强。所提方法将平均Jaccard指数从0.611提高到了0.636, 将小类“vehicle small”的Jaccard指数从0.166 提高到了0.238。通过对训练数据集和实验过程及结果的分析, 可以看出对于小类, 由于数据过少, 数据增强对于精度的改善较大。在采用同样数据增强方法的情况下, 自适应阈值和加权交叉熵损失能够进一步改善分类效果。</p>
                </div>
                <div class="p1">
                    <p id="161">实验结果证明, 所提出的模型显著优于二元逻辑回归和图像块分类CNN方法。通过数据增强, 显著提高了平均Jaccard指数。模型的有效性主要存在三方面原因:1) U-Net模型通过将编码器特征图直接连接到对应的解码器的上采样特征图, 形成梯形结构;2) 通过其跳过级联连接的架构允许每个级的解码器学习在编码器中汇集时丢失的相关特征;3) 通过自适应阈值算法调整每个类的阈值以帮助模型获得更好的平均Jaccard指数。</p>
                </div>
                <div class="area_img" id="162">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904047_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 各类阈值和评价值之间的关系" src="Detail/GetImg?filename=images/GXXB201904047_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 各类阈值和评价值之间的关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904047_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Relationship between evaluation value and threshold of each class</p>

                </div>
                <div class="area_img" id="163">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904047_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 所提方法的实验结果。 (a) ～ (c) 真实值; (d) ～ (f) 经自适应阈值处理的结果; (g) ～ (i) 未经自适应阈值处理的结果" src="Detail/GetImg?filename=images/GXXB201904047_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 所提方法的实验结果。 (a) ～ (c) 真实值; (d) ～ (f) 经自适应阈值处理的结果; (g) ～ (i) 未经自适应阈值处理的结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904047_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Experimental results by proposed method. (a) - (c) Ground truths; (d) - (f) results with adaptive threshold; (g) - (i) results without adaptive threshold</p>

                </div>
                <div class="area_img" id="165">
                    <p class="img_tit">表2 各类别的最佳阈值 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Best threshold of each class</p>
                    <p class="img_note"></p>
                    <table id="165" border="1"><tr><td>Class</td><td>buildings</td><td>misc</td><td>road</td><td>track</td><td>trees</td><td>crops</td><td>waterway</td><td>standing <br />water</td><td>vehicle <br />large</td><td>vehicle <br />small</td></tr><tr><td><br />Threshold</td><td>0.36</td><td>0.22</td><td>0.51</td><td>0.26</td><td>0.45</td><td>0.39</td><td>0.57</td><td>0.61</td><td>0.31</td><td>0.18</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="166" name="166" class="anchor-tag">5 结 论</h3>
                <div class="p1">
                    <p id="167">为了实现高分辨率遥感图像语义分割任务和对遥感数据中存在的类别非均衡问题开展研究, 介绍了遥感图像语义分割相关研究及当前研究热点, 提出了一个基于FCN的语义分割模型。模型中采用U-Net网络结构实现了端对端的语义分割, 通过加权交叉熵损失函数和自适应阈值方法提高了小类的分割精度。对DSTL数据集语义分割的实验结果证明:所提的方法能够有效用于遥感图像语义分割并显著提高小类的分割精度, 进而将平均Jaccard指数从0.611提升至0.636。所提的模型具有以下优点:1) 实现端到端的训练及预测;2) 通过卷积化充分利用GPU性能;3) 简化了学习流程, 能够用较少的参数得到较好的结果。该方法存在的局限性主要是FCN需要大量高质量地物真实标记的数据集用于模型训练, 数据集的获取需要图像判读专家的经验和大量人工作业, 而在很多实际应用中难以获得这些条件。因此下一步研究的重要方向是在弱监督情况下训练模型以增强模型的应用范围。</p>
                </div>
                <div class="area_img" id="168">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904047_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 所提方法的实验结果。 (a) ～ (c) 基于图像块的CNN模型结果; (d) ～ (f) 采用自适应阈值和未采用数据增强的结果; (g) ～ (i) 未采用自适应阈值和数据增强的结果" src="Detail/GetImg?filename=images/GXXB201904047_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 所提方法的实验结果。 (a) ～ (c) 基于图像块的CNN模型结果; (d) ～ (f) 采用自适应阈值和未采用数据增强的结果; (g) ～ (i) 未采用自适应阈值和数据增强的结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904047_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Experimental results by proposed method. (a) - (c) Results of Patch-based CNN model; (d) - (f) results with adaptive threshold and without data augmentation; (g) - (i) results without adaptive threshold and without data augmentation</p>

                </div>
                <div class="area_img" id="169">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904047_169.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 小类的实验结果。 (a) (b) 原图; (c) (d) 真实值; (e) (f) 所提方法的结果; (g) (h) 基本U-Net模型的结果" src="Detail/GetImg?filename=images/GXXB201904047_169.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 小类的实验结果。 (a) (b) 原图; (c) (d) 真实值; (e) (f) 所提方法的结果; (g) (h) 基本U-Net模型的结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904047_169.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Experimental results of small class. (a) (b) Original images; (c) (d) ground truths; (e) (f) results of proposed method; (g) (h) results of basic U-Net model</p>

                </div>
                <div class="area_img" id="170">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904047_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 算法性能对比" src="Detail/GetImg?filename=images/GXXB201904047_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 算法性能对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904047_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Comparison of algorithm performance</p>

                </div>
                <div class="area_img" id="171">
                    <p class="img_tit">表3 算法性能对比 (Jaccard指数)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Comparison of algorithm performance (Jaccard index) </p>
                    <p class="img_note"></p>
                    <table id="171" border="1"><tr><td>Algorithm</td><td>buildings</td><td>misc</td><td>road</td><td>track</td><td>trees</td><td>crops</td><td>waterway</td><td>standing <br />water</td><td>vehicle <br />large</td><td>vehicle <br />small</td><td>Average</td></tr><tr><td><br />Binary logistic</td><td>0.484</td><td>0.015</td><td>0.500</td><td>0.201</td><td>0.393</td><td>0.539</td><td>0.575</td><td>0</td><td>0</td><td>0</td><td>0.271</td></tr><tr><td><br />Patch-based CNN</td><td>0.623</td><td>0.146</td><td>0.756</td><td>0.258</td><td>0.671</td><td>0.958</td><td>0.873</td><td>0.800</td><td>0.001</td><td>0.030</td><td>0.512</td></tr><tr><td><br />FCN+DA</td><td>0.698</td><td>0.152</td><td>0.863</td><td>0.455</td><td>0.728</td><td>0.969</td><td>0.917</td><td>0.830</td><td>0.359</td><td>0.166</td><td>0.614</td></tr><tr><td><br />FCN+WCE+DA</td><td>0.701</td><td>0.181</td><td>0.863</td><td>0.467</td><td>0.728</td><td>0.969</td><td>0.918</td><td>0.834</td><td>0.366</td><td>0.183</td><td>0.621</td></tr><tr><td><br />FCN+AT+DA</td><td>0.702</td><td>0.228</td><td>0.863</td><td>0.465</td><td>0.728</td><td>0.969</td><td>0.918</td><td>0.832</td><td>0.362</td><td>0.226</td><td>0.629</td></tr><tr><td><br />FCN+WCE+AT</td><td>0.599</td><td>0.138</td><td>0.568</td><td>0.219</td><td>0.451</td><td>0.878</td><td>0.829</td><td>0.707</td><td>0.042</td><td>0.047</td><td>0.448</td></tr><tr><td><br />Proposed method</td><td>0.705</td><td>0.258</td><td>0.864</td><td>0.472</td><td>0.728</td><td>0.969</td><td>0.919</td><td>0.835</td><td>0.369</td><td>0.238</td><td>0.636</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning aerial image segmentation from online maps">

                                <b>[1]</b> Kaiser P, Wegner J D, Lucchi A, <i>et al</i>.Learning aerial image segmentation from online maps[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (11) :6054-6068.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201709024&amp;v=MjI4MTNvOUhZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhsVXJ6SklqWEJZN0c0SDliTXA=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Liu F, Li L.Evaluation of information acquisition capability of optical remote sensing satellites[J].Optics and Precision Engineering, 2017, 25 (9) :2454-2460.刘锋, 李琳.光学遥感卫星信息获取能力指数的评估[J].光学精密工程, 2017, 25 (9) :2454-2460.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object detection in multispectral high resolution images">

                                <b>[3]</b> Trivedi M M.Object detection in multispectral high resolution images[J].Proceedings of SPIE, 1988, 0933:8-15.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMD71033EF8EE149F894A1F173A0F662B0&amp;v=MDk1MTRNMUQ3U1NSOGlmQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGhoemJpK3c2az1OaWZJWThlL0g5SFByUG96Yko1NkRYZ3d1UjRhN2s1OFBuN2xyMg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[J].Communications of the ACM, 2017, 60 (6) :84-90.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201711022&amp;v=MTQzMDlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGxVcnpKSWpYQlk3RzRIOWJOcm85SFpvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Liu F, Shen T S, Ma X X, <i>et al</i>.Ship recognition based on multi-band deep neural network[J].Optics and Precision Engineering, 2017, 25 (11) :2939-2946.刘峰, 沈同圣, 马新星, 等.基于多波段深度神经网络的舰船目标识别[J].光学精密工程, 2017, 25 (11) :2939-2946.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?">

                                <b>[6]</b> Penatti O A B, Nogueira K, dos Santos J A.Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?[C]//IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) , 2015:44-51.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD4E1F73FD33C38F6EB304F9595132EA65&amp;v=MDc2MTZRbGZDcGJRMzVkaGh6YmkrdzZrPU5qbkJhcmZOSDZmTHJQa3haK2g4RDNSUHlXTmg2VDk1UG5ibnBSYzBlckRoTkx5YUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPRw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Zhong Y F, Fei F, Liu Y F, <i>et al</i>.SatCNN:satellite image dataset classification using agile convolutional neural networks[J].Remote Sensing Letters, 2017, 8 (2) :136-145.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning earth observation classification using image Net pretrained networks">

                                <b>[8]</b> Marmanis D, Datcu M, Esch T, <i>et al</i>.Deep learning earth observation classification using ImageNet pretrained networks[J].IEEE Geoscience and Remote Sensing Letters, 2016, 13 (1) :105-109.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3D5B9FF840C5784BE15B377C0EBD6BF8&amp;v=MTg5MzVDcGJRMzVkaGh6YmkrdzZrPU5pZk9mYkRNRzZQRjJmbE5ZT3Q4Q1hzeHkyUm02em9QUzNqbDN4SkFDOGFTTjh5WENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Garcia-Garcia A, Orts-Escolano S, Oprea S, <i>et al</i>.A survey on deep learning techniques for image and video semantic segmentation[J].Applied Soft Computing, 2018, 70:41-65.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=2D semantic labeling contest">

                                <b>[10]</b> 2D semantic labeling contest[EB/OL]. (2018-09-05) [2018-10-15].http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic segmentation of aerial images with an ensemble of CNSS">

                                <b>[11]</b> Marmanis D, Wegner J D, Galliani S, <i>et al</i>.Semantic segmentation of aerial images with an ensemble of CNNs[J].ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, 2016, III-3:473-480.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic seg-mentation">

                                <b>[12]</b> Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1012016720.nh&amp;v=MTc0NTF1RnlIbFVyekpWRjI2SExPNUdOYk9yNUViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Chen X.Relationships between evaluation criteria of feature selection and analysis on class imbalance problem over VHR remote sensing imagery[D].Shanghai:Shanghai Jiao Tong University, 2011.陈曦.特征选择准则间的关联及高分辨率遥感影像类别不平衡问题研究[D].上海:上海交通大学, 2011.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Application of neural networks to terrain classification">

                                <b>[14]</b> Decatur.Application of neural networks to terrain classification[C]//International 1989 Joint Conference on Neural Networks, 1989:283-288.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201703031&amp;v=MjA2MDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGxVcnpKSWpYQlk3RzRIOWJNckk5R1pZUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Cai Q, Hao J Y, Cao J, <i>et al</i>.Salient detection via local and global feature[J].Optics and Precision Engineering, 2017, 25 (3) :772-778.蔡强, 郝佳云, 曹健, 等.结合局部特征及全局特征的显著性检测[J].光学精密工程, 2017, 25 (3) :772-778.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multispectral classification of landsat-images using neural networks">

                                <b>[16]</b> Bischof H, Schneider W, Pinz A J.Multispectral classification of Landsat-images using neural networks[J].IEEE Transactions on Geoscience and Remote Sensing, 1992, 30 (3) :482-490.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501968436&amp;v=MDUyNDlvOUViZTBIQ0g4L29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUtWNFZhUk09TmlmT2ZiSzdIdEROcQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> Mountrakis G, Im J, Ogole C.Support vector machines in remote sensing:a review[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2011, 66 (3) :247-259.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD713863214&amp;v=MTY1MTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhsVXJ6Sk5qbkJhclM1SGRuS3JJMUVZSVE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Pal M.Random forest classifier for remote sensing classification[J].International Journal of Remote Sensing, 2005, 26 (1) :217-222.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600161209&amp;v=MzE1NTZmYks3SHRETnFZOUZaZTBPRG53d29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUtWNFZhUk09TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> Atkinson P M, Lewis P.Geostatistical classification for remote sensing:an introduction[J].Computers &amp; Geosciences, 2000, 26 (4) :361-371.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object detection in optical remote sensing images based on weakly supervised learning and high-level feature Learning">

                                <b>[20]</b> Han J W, Zhang D W, Cheng G, <i>et al</i>.Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (6) :3325-3337.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806043&amp;v=MjI3OTh6SklqWFRiTEc0SDluTXFZOUJaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhsVXI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> Zhang Q C, Tong G F, Li Y, <i>et al</i>.River detection in remote sensing images based on multi-feature fusion and soft voting[J].Acta Optica Sinica, 2018, 38 (6) :0628002.张庆春, 佟国峰, 李勇, 等.基于多特征融合和软投票的遥感图像河流检测[J].光学学报, 2018, 38 (6) :0628002.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201704037&amp;v=MzAzMDNUYkxHNEg5Yk1xNDlHWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIbFVyekpJalg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> Gao X J, Zheng X D, Liu Z X, <i>et al</i>.Automatic building extraction from high resolution visible images based on shifted shadow analysis[J].Acta Optica Sinica, 2017, 37 (4) :0428002.高贤君, 郑学东, 刘子潇, 等.基于偏移阴影分析的高分辨率可见光影像建筑物自动提取[J].光学学报, 2017, 37 (4) :0428002.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Machine learning for aerial image labeling">

                                <b>[23]</b> Mnih V.Machine learning for aerial image labeling[D].Toronto:University of Toronto, 2013.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1F9E01344C238F05C7C64E9D96B0B9BC&amp;v=MDYyMjRJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh6YmkrdzZrPU5pZk9mYkxPRjZUTXJveEJZSmdORDNSUHp4Tmc3VXg3VEFycjJCc3pDN0xtVE1qc0NPTnZGU2lXV3I3Sg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b> Sharma A, Liu X W, Yang X J, <i>et al</i>.A patch-based convolutional neural network for remote sensing image classification[J].Neural Networks, 2017, 95:19-28.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SegNet:A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation">

                                <b>[25]</b> Badrinarayanan V, Kendall A, Cipolla R.SegNet:A deep convolutional encoder-decoder architecture for image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (12) :2481-2495.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-net:Convolutional networks for biomedical image segmentation">

                                <b>[26]</b> Ronneberger O, Fischer P, Brox T.U-Net:Convolutional Networks for Biomedical Image Segmentation[M]//Ronneberger O, Fischer P, Brox T.eds.Lecture Notes in Computer Science.Cham:Springer International Publishing, 2015:234-241.
                            </a>
                        </p>
                        <p id="62">
                            <a id="bibliography_27" >
                                    <b>[27]</b>
                                 Noh H, Hong S, Han B.Learning deconvolution network for semantic segmentation[C]//2015 IEEE International Conference on Computer Vision (ICCV) , 2015:1520-1528.
                            </a>
                        </p>
                        <p id="64">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Lab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs">

                                <b>[28]</b> Chen L C, Papandreou G, Kokkinos I, <i>et al</i>.DeepLab:semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (4) :834-848.
                            </a>
                        </p>
                        <p id="66">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic segmentation of satellite images using deep learning">

                                <b>[29]</b> Muruganandham S.Semantic segmentation of satellite images using deep learning[D].Lulea:Lulea University of Technology, 2016.
                            </a>
                        </p>
                        <p id="68">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Road structure refined CNN for road extraction in aerial image">

                                <b>[30]</b> Wei Y N, Wang Z L, Xu M.Road structure refined CNN for road extraction in aerial image[J].IEEE Geoscience and Remote Sensing Letters, 2017, 14 (5) :709-713.
                            </a>
                        </p>
                        <p id="70">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning city structures from online maps">

                                <b>[31]</b> Kaiser P.Learning city structures from online maps[D].Zurich:ETH Zurich, 2016.
                            </a>
                        </p>
                        <p id="72">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic Segmentation of Small Objects and Modeling of Uncertainty in Urban Remote Sensing Images Using Deep Convolutional Neural Networks">

                                <b>[32]</b> Kampffmeyer M, Salberg A B, Jenssen R.Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks[C]//IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) , 2016:680-688.
                            </a>
                        </p>
                        <p id="74">
                            <a id="bibliography_33" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES5B7962BC3BA907C7D62A85F38063B243&amp;v=MTEwMDhJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh6YmkrdzZrPU5pZk9mYmJLR2RqS3JmMDJaNWwrQlh3K3ZCRm43RDBNUUhxVXJ4bzFmN0htUjc2Y0NPTnZGU2lXV3I3Sg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[33]</b> Kemker R, Salvaggio C, Kanan C.Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2018, 145:60-77.
                            </a>
                        </p>
                        <p id="76">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Can semantic labeling methods generalize to any city? The inria aerial image labeling benchmark">

                                <b>[34]</b> Maggiori E, Tarabalka Y, Charpiat G, <i>et al</i>.Can semantic labeling methods generalize to any city?the inria aerial image labeling benchmark[C]//IEEE International Geoscience and Remote Sensing Symposium (IGARSS) , 2017:3226-3229.
                            </a>
                        </p>
                        <p id="78">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=IEEE GRSS data fusion contest:open data for global multimodal land use classification [technical committees]">

                                <b>[35]</b> Tuia D, Moser G, Le Saux B, <i>et al</i>.IEEE GRSS data fusion contest:open data for global multimodal land use classification [technical committees][J].IEEE Geoscience and Remote Sensing Magazine, 2017, 5 (1) :70-73.
                            </a>
                        </p>
                        <p id="80">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dstl satellite imagery feature detection">

                                <b>[36]</b> Dstl satellite imagery feature detection[EB/OL]. (2018-09-05) [2018-10-15].https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection.
                            </a>
                        </p>
                        <p id="82">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">

                                <b>[37]</b> Kingma D P, Ba J.Adam:a method for stochastic optimization[EB/OL]. (2014-12-22) [2018-10-15].https://arxiv.org/abs/1412.6980 .
                            </a>
                        </p>
                        <p id="84">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering of user activities based on adaptive threshold spiking neural networks">

                                <b>[38]</b> Amin H H, Deabes W, Bouazza K.Clustering of user activities based on adaptive threshold spiking neural networks[C]//Ninth International Conference on Ubiquitous and Future Networks (ICUFN) , 2017:1-6.
                            </a>
                        </p>
                        <p id="86">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A survey of deep-learning frameworks">

                                <b>[39]</b> Parvat A, Chavan J, Kadam S, <i>et al</i>.A survey of deep-learning frameworks[C]//International Conference on Inventive Systems and Control (ICISC) , 2017:1-7.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201904047" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201904047&amp;v=MDE5OTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhsVXJ6SklqWFRiTEc0SDlqTXE0OUJZNFE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

