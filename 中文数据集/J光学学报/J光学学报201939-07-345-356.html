

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133869286846250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201907039%26RESULT%3d1%26SIGN%3dLUMZZewTUARycmg8Uxt81K5CRBE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201907039&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201907039&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201907039&amp;v=MjEwMjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlqa1VMN09JalhUYkxHNEg5ak1xSTlHYllRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#70" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#74" data-title="2 解析字典学习 ">2 解析字典学习</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#78" data-title="3 具体算法 ">3 具体算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#80" data-title="&lt;b&gt;3.1 源域遥感场景的局部保持&lt;/b&gt;"><b>3.1 源域遥感场景的局部保持</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;3.2 目标域遥感场景的局部保持&lt;/b&gt;"><b>3.2 目标域遥感场景的局部保持</b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;3.3 目标域遥感场景的&lt;i&gt;k&lt;/i&gt;-means聚类&lt;/b&gt;"><b>3.3 目标域遥感场景的<i>k</i>-means聚类</b></a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;3.4 算法流程步骤&lt;/b&gt;"><b>3.4 算法流程步骤</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="4 实验及结果分析 ">4 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#135" data-title="&lt;b&gt;4.1 数据集及实验设置&lt;/b&gt;"><b>4.1 数据集及实验设置</b></a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;4.2 鉴别性分析&lt;/b&gt;"><b>4.2 鉴别性分析</b></a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;4.3 收敛性分析&lt;/b&gt;"><b>4.3 收敛性分析</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;4.4 分类效果分析&lt;/b&gt;"><b>4.4 分类效果分析</b></a></li>
                                                <li><a href="#158" data-title="&lt;b&gt;4.5 计算效率分析&lt;/b&gt;"><b>4.5 计算效率分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#160" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#82" data-title="图1 所提算法的整体框架图">图1 所提算法的整体框架图</a></li>
                                                <li><a href="#137" data-title="图2 UCM数据集若干类的样本。">图2 UCM数据集若干类的样本。</a></li>
                                                <li><a href="#138" data-title="图3 RSSCN7数据集各类样本。">图3 RSSCN7数据集各类样本。</a></li>
                                                <li><a href="#142" data-title="图4 GoogLeNet特征下&lt;b&gt;&lt;i&gt;X&lt;/i&gt;&lt;/b&gt;&lt;sub&gt;u&lt;/sub&gt;及不同情形&lt;b&gt;&lt;i&gt;Z&lt;/i&gt;&lt;/b&gt;&lt;sub&gt;u&lt;/sub&gt;的t-SNE图。">图4 GoogLeNet特征下<b><i>X</i></b><sub>u</sub>及不同情形<b><i>Z</i></b><sub>u</sub>的t-SNE图。</a></li>
                                                <li><a href="#143" data-title="图5 VGGNet特征下&lt;b&gt;&lt;i&gt;X&lt;/i&gt;&lt;/b&gt;&lt;sub&gt;u&lt;/sub&gt;及不同情形&lt;b&gt;&lt;i&gt;Z&lt;/i&gt;&lt;/b&gt;&lt;sub&gt;u&lt;/sub&gt;的t-SNE图。">图5 VGGNet特征下<b><i>X</i></b><sub>u</sub>及不同情形<b><i>Z</i></b><sub>u</sub>的t-SNE图。</a></li>
                                                <li><a href="#144" data-title="图6 本文方法在不同&lt;i&gt;λ&lt;/i&gt;, &lt;i&gt;μ&lt;/i&gt;取值下的OA值">图6 本文方法在不同<i>λ</i>, <i>μ</i>取值下的OA值</a></li>
                                                <li><a href="#147" data-title="图7 GoogLeNet特征上的算法&lt;i&gt;ζ&lt;/i&gt;&lt;sub&gt;sum&lt;/sub&gt;/OA曲线">图7 GoogLeNet特征上的算法<i>ζ</i><sub>sum</sub>/OA曲线</a></li>
                                                <li><a href="#148" data-title="图8 VGGNet特征上的算法&lt;i&gt;ζ&lt;/i&gt;&lt;sub&gt;sum&lt;/sub&gt;/OA曲线">图8 VGGNet特征上的算法<i>ζ</i><sub>sum</sub>/OA曲线</a></li>
                                                <li><a href="#154" data-title="表1 各ZSC方法在对RSSCN7上的总体分类准确度">表1 各ZSC方法在对RSSCN7上的总体分类准确度</a></li>
                                                <li><a href="#155" data-title="图9 I型ZSC方法对RSSCN7各类别的分类准确度。">图9 I型ZSC方法对RSSCN7各类别的分类准确度。</a></li>
                                                <li><a href="#156" data-title="图10 T型ZSC方法对RSSCN7各类别的分类准确度。">图10 T型ZSC方法对RSSCN7各类别的分类准确度。</a></li>
                                                <li><a href="#162" data-title="表2 ZSC算法在GoogLeNet特征上的运算耗时">表2 ZSC算法在GoogLeNet特征上的运算耗时</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="18">


                                    <a id="bibliography_1" title=" Liu D W, Han L, Han X Y.High spatial resolution remote sensing image classification based on deep learning[J].Acta Optica Sinica, 2016, 36 (4) :0428001.刘大伟, 韩玲, 韩晓勇.基于深度学习的高分辨率遥感影像分类研究[J].光学学报, 2016, 36 (4) :0428001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604039&amp;v=MDQ3NjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlqa1VMN09JalhUYkxHNEg5Zk1xNDlHYllRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Liu D W, Han L, Han X Y.High spatial resolution remote sensing image classification based on deep learning[J].Acta Optica Sinica, 2016, 36 (4) :0428001.刘大伟, 韩玲, 韩晓勇.基于深度学习的高分辨率遥感影像分类研究[J].光学学报, 2016, 36 (4) :0428001.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_2" title=" Chen S Z, Tian Y L.Pyramid of spatial relatons for scene-level land use classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (4) :1947-1957." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pyramid of Spatial Relatons for Scene-Level Land Use Classification">
                                        <b>[2]</b>
                                         Chen S Z, Tian Y L.Pyramid of spatial relatons for scene-level land use classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (4) :1947-1957.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_3" title=" Zhang L P, Zhang L F, Du B.Deep learning for remote sensing data:a technical tutorial on the state of the art[J].IEEE Geoscience and Remote Sensing Magazine, 2016, 4 (2) :22-40." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning for remote sensing data:A technical tutorial on the state of the art">
                                        <b>[3]</b>
                                         Zhang L P, Zhang L F, Du B.Deep learning for remote sensing data:a technical tutorial on the state of the art[J].IEEE Geoscience and Remote Sensing Magazine, 2016, 4 (2) :22-40.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_4" title=" Li A X, Lu Z W, Wang L W, &lt;i&gt;et al&lt;/i&gt;.Zero-shot scene classification for high spatial resolution remote sensing images[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :4157-4167." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot scene classification for high spatial resolution remote sensing images">
                                        <b>[4]</b>
                                         Li A X, Lu Z W, Wang L W, &lt;i&gt;et al&lt;/i&gt;.Zero-shot scene classification for high spatial resolution remote sensing images[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :4157-4167.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_5" title=" Xian Y Q, Akata Z, Sharma G, &lt;i&gt;et al&lt;/i&gt;.Latent embeddings for zero-shot classification[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition, June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:69-77." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Latent embeddings for zero-shot classification">
                                        <b>[5]</b>
                                         Xian Y Q, Akata Z, Sharma G, &lt;i&gt;et al&lt;/i&gt;.Latent embeddings for zero-shot classification[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition, June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:69-77.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_6" title=" Wang D H, Li Y, Lin Y T, &lt;i&gt;et al&lt;/i&gt;.Relational knowledge transfer for zero-shot learning[C]//Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16) , February 12-17, 2016, Phoenix, Arizona, USA.California:AAAI Publications, 2016:2145-2151." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Relational knowledge transfer for zero-shot learning">
                                        <b>[6]</b>
                                         Wang D H, Li Y, Lin Y T, &lt;i&gt;et al&lt;/i&gt;.Relational knowledge transfer for zero-shot learning[C]//Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16) , February 12-17, 2016, Phoenix, Arizona, USA.California:AAAI Publications, 2016:2145-2151.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_7" title=" Zhang Z M, Saligrama V.Zero-shot learning via joint latent similarity embedding[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition, June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:6034-6042." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning via joint latent similarity embedding">
                                        <b>[7]</b>
                                         Zhang Z M, Saligrama V.Zero-shot learning via joint latent similarity embedding[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition, June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:6034-6042.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_8" title=" Zhang Z M, Saligrama V.Zero-shot learning via semantic similarity embedding[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:4166-4174." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning via semantic similarity embedding">
                                        <b>[8]</b>
                                         Zhang Z M, Saligrama V.Zero-shot learning via semantic similarity embedding[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:4166-4174.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_9" title=" Wang Q, Chen K.Zero-shot visual recognition via bidirectional latent embedding[J].International Journal of Computer Vision, 2017, 124 (3) :356-383." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDD31F20253A10A4B4D5AE16DA35AA17A9&amp;v=Mjg3MDROajdCYXNlN0g2Zk9yNDFBWjVvT0RBMDl2UkpuNzA0SVNYbVczUkV3Q01PVlFzdVdDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh4TG04d2E0PQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Wang Q, Chen K.Zero-shot visual recognition via bidirectional latent embedding[J].International Journal of Computer Vision, 2017, 124 (3) :356-383.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_10" title=" Li Y N, Wang D H, Hu H H, &lt;i&gt;et al&lt;/i&gt;.Zero-shot recognition using dual visual-semantic mapping paths[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition, July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:5207-5215." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot recognition using dual visual-semantic mapping paths">
                                        <b>[10]</b>
                                         Li Y N, Wang D H, Hu H H, &lt;i&gt;et al&lt;/i&gt;.Zero-shot recognition using dual visual-semantic mapping paths[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition, July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:5207-5215.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_11" title=" Kodirov E, Xiang T, Fu Z Y, &lt;i&gt;et al&lt;/i&gt;.Unsupervised domain adaptation for zero-shot learning[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:2452-2460." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised Domain Adaptation for Zero-Shot Learning">
                                        <b>[11]</b>
                                         Kodirov E, Xiang T, Fu Z Y, &lt;i&gt;et al&lt;/i&gt;.Unsupervised domain adaptation for zero-shot learning[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:2452-2460.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_12" title=" Guo Y C, Ding G G, Jing X M, &lt;i&gt;et al&lt;/i&gt;.Transductive zero-shot recognition via shared model space learning[C]//Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA.California:AAAI Publications, 2016:3494-3500." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transductive zero-shot recognition via shared model space learning">
                                        <b>[12]</b>
                                         Guo Y C, Ding G G, Jing X M, &lt;i&gt;et al&lt;/i&gt;.Transductive zero-shot recognition via shared model space learning[C]//Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA.California:AAAI Publications, 2016:3494-3500.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_13" title=" Fu Y W, Hospedales T M, Xiang T, &lt;i&gt;et al&lt;/i&gt;.Transductive multi-view embedding for zero-shot recognition and annotation[M]//Fleet D, Pajdla T, Schiele B, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science 2014.Cham:Springer, 2014, 8690:584-599." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transductive multi-view embedding for zero-shot recognition and annotation">
                                        <b>[13]</b>
                                         Fu Y W, Hospedales T M, Xiang T, &lt;i&gt;et al&lt;/i&gt;.Transductive multi-view embedding for zero-shot recognition and annotation[M]//Fleet D, Pajdla T, Schiele B, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science 2014.Cham:Springer, 2014, 8690:584-599.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_14" title=" Ji Z, Xie Y Z, Pang Y W.Zero-shot learning based on canonical correlation analysis and distance metric learning[J].Journal of Tianjin University, 2017, 50 (8) :813-820.冀中, 谢于中, 庞彦伟.基于典型相关分析和距离度量学习的零样本学习[J].天津大学学报, 2017, 50 (8) :813-820." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJDX201708005&amp;v=MjE4NTVGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlqa1VMN09NU2ZQZHJHNEg5Yk1wNDk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Ji Z, Xie Y Z, Pang Y W.Zero-shot learning based on canonical correlation analysis and distance metric learning[J].Journal of Tianjin University, 2017, 50 (8) :813-820.冀中, 谢于中, 庞彦伟.基于典型相关分析和距离度量学习的零样本学习[J].天津大学学报, 2017, 50 (8) :813-820.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_15" title=" Ji Z, Sun T, Yu Y L.Transductive discriminative dictionary learning approach for zero-shot classification[J].Journal of Software, 2017, 28 (11) :2961-2970.冀中, 孙涛, 于云龙.一种基于直推判别字典学习的零样本分类方法[J].软件学报, 2017, 28 (11) :2961-2970." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201711012&amp;v=MjM2MDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5amtVTDdPTnlmVGJMRzRIOWJOcm85RVpvUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Ji Z, Sun T, Yu Y L.Transductive discriminative dictionary learning approach for zero-shot classification[J].Journal of Software, 2017, 28 (11) :2961-2970.冀中, 孙涛, 于云龙.一种基于直推判别字典学习的零样本分类方法[J].软件学报, 2017, 28 (11) :2961-2970.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_16" title=" Bao C L, Ji H, Quan Y H, &lt;i&gt;et al&lt;/i&gt;.Dictionary learning for sparse coding:algorithms and convergence analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (7) :1356-1369." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dictionary learning for sparse coding:algorithms and convergence analysis">
                                        <b>[16]</b>
                                         Bao C L, Ji H, Quan Y H, &lt;i&gt;et al&lt;/i&gt;.Dictionary learning for sparse coding:algorithms and convergence analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (7) :1356-1369.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_17" title=" Yang M, Chang H Y, Luo W X.Discriminative analysis-synthesis dictionary learning for image classification[J].Neurocomputing, 2017, 219:404-411." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative analysis-synthesis dictionary learning for image classification">
                                        <b>[17]</b>
                                         Yang M, Chang H Y, Luo W X.Discriminative analysis-synthesis dictionary learning for image classification[J].Neurocomputing, 2017, 219:404-411.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_18" title=" Wang J J, Guo Y Q, Guo J, &lt;i&gt;et al&lt;/i&gt;.Synthesis linear classifier based analysis dictionary learning for pattern classification[J].Neurocomputing, 2017, 238:103-113." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES579AE40DE8831392A28EB8D29F9C9B3D&amp;v=MjE1NjhjTUdkTjduckNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRoaHhMbTh3YTQ9TmlmT2ZiYS9GNkM1cTQ4eEVlTUhEMzA2eGhSaTZEY0lPbmVXcmh0RA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Wang J J, Guo Y Q, Guo J, &lt;i&gt;et al&lt;/i&gt;.Synthesis linear classifier based analysis dictionary learning for pattern classification[J].Neurocomputing, 2017, 238:103-113.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_19" title=" Ravishankar S, Bresler Y.Sparsifying transform learning with efficient optimal updates and convergence guarantees[J].IEEE Transactions on Signal Processing, 2015, 63 (9) :2389-2404." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=L0 sparsifying transform learning with efficient optimal updates and convergence guarantees">
                                        <b>[19]</b>
                                         Ravishankar S, Bresler Y.Sparsifying transform learning with efficient optimal updates and convergence guarantees[J].IEEE Transactions on Signal Processing, 2015, 63 (9) :2389-2404.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_20" title=" Kreutz-Delgado K, Murray J F, Rao B D, &lt;i&gt;et al&lt;/i&gt;.Dictionary learning algorithms for sparse representation[J].Neural Computation, 2003, 15 (2) :349-396." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012156&amp;v=MzAxOTRmSlpiSzlIdGpNcW85RlpPb05EWGsvb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JSUY4WGF4UT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         Kreutz-Delgado K, Murray J F, Rao B D, &lt;i&gt;et al&lt;/i&gt;.Dictionary learning algorithms for sparse representation[J].Neural Computation, 2003, 15 (2) :349-396.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_21" title=" Fernando B, Fromont E, Muselet D, &lt;i&gt;et al&lt;/i&gt;.Discriminative feature fusion for image classification[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition, June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:3434-3441." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative feature fusion for image classific-ation">
                                        <b>[21]</b>
                                         Fernando B, Fromont E, Muselet D, &lt;i&gt;et al&lt;/i&gt;.Discriminative feature fusion for image classification[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition, June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:3434-3441.
                                    </a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_22" title=" Tenenbaum J B.A global geometric framework for nonlinear dimensionality reduction[J].Science, 2000, 290 (5500) :2319-2323." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Global Geometric Framework for Nonlinear Dimensionality Reduction">
                                        <b>[22]</b>
                                         Tenenbaum J B.A global geometric framework for nonlinear dimensionality reduction[J].Science, 2000, 290 (5500) :2319-2323.
                                    </a>
                                </li>
                                <li id="62">


                                    <a id="bibliography_23" title=" Yang Y, Newsam S.Bag-of-visual-words and spatial extensions for land-use classification[C]//Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems, November 2-5, 2010, San Jose, California.New York:ACM, 2010:270-279." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bag-of-visual-words and spatial extensions for land-use classification">
                                        <b>[23]</b>
                                         Yang Y, Newsam S.Bag-of-visual-words and spatial extensions for land-use classification[C]//Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems, November 2-5, 2010, San Jose, California.New York:ACM, 2010:270-279.
                                    </a>
                                </li>
                                <li id="64">


                                    <a id="bibliography_24" title=" Zou Q, Ni L H, Zhang T, &lt;i&gt;et al&lt;/i&gt;.Deep learning based feature selection for remote sensing scene classification[J].IEEE Geoscience and Remote Sensing Letters, 2015, 12 (11) :2321-2325." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Learning Based Feature Selection for Remote Sensing Scene Classification">
                                        <b>[24]</b>
                                         Zou Q, Ni L H, Zhang T, &lt;i&gt;et al&lt;/i&gt;.Deep learning based feature selection for remote sensing scene classification[J].IEEE Geoscience and Remote Sensing Letters, 2015, 12 (11) :2321-2325.
                                    </a>
                                </li>
                                <li id="66">


                                    <a id="bibliography_25" title=" Szegedy C, Liu W, Jia Y Q, &lt;i&gt;et al&lt;/i&gt;.Going deeper with convolutions[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition, June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:7298594." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[25]</b>
                                         Szegedy C, Liu W, Jia Y Q, &lt;i&gt;et al&lt;/i&gt;.Going deeper with convolutions[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition, June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:7298594.
                                    </a>
                                </li>
                                <li id="68">


                                    <a id="bibliography_26" title=" Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [ 2019-01-01].https://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[26]</b>
                                         Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [ 2019-01-01].https://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-02 15:13</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(07),345-356 DOI:10.3788/AOS201939.0728001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于局部保持的遥感场景零样本分类算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E6%99%A8&amp;code=41275593&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴晨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%AE%8F%E4%BC%9F&amp;code=20899251&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王宏伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%BF%97%E5%BC%BA&amp;code=20250775&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王志强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A2%81%E6%98%B1%E7%BA%AC&amp;code=42150725&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">袁昱纬</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%AE%87&amp;code=20481306&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A8%8B%E7%BA%A2&amp;code=20025334&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">程红</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%85%A8%E5%90%89%E6%88%90&amp;code=20534592&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全吉成</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%B7%E5%86%9B%E8%88%AA%E7%A9%BA%E5%A4%A7%E5%AD%A6&amp;code=1701745&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">海军航空大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A9%BA%E5%86%9B%E8%88%AA%E7%A9%BA%E5%A4%A7%E5%AD%A6&amp;code=1020414&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空军航空大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=91977%E9%83%A8%E9%98%9F&amp;code=1743710&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">91977部队</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>目标域遥感图像特征分布的变化, 导致遥感场景零样本分类性能下降, 针对该问题, 提出一种基于局部保持的遥感场景零样本分类算法。首先, 为减少冗余信息, 采用解析字典学习方法, 将源域中的场景图像特征和类别语义词向量嵌入到同一稀疏编码空间, 并实现两者稀疏系数的强制对齐, 以建立图像特征与词向量之间的关系;然后, 通过保留图像特征空间中场景图像间的局部近邻关系, 增强场景图像对应稀疏系数的鉴别性, 以有助于对稀疏系数进行聚类分析;最后, 为适应目标域图像特征分布变化, 采用<i>k</i>-means算法对目标域场景图像的稀疏系数进行聚类, 并以初始中心的类别标签作为对应的聚类簇中场景的类别标签。实验分别采用GoogLeNet和VGGNet图像特征, 以数据集UCM作为源域遥感场景集, 对目标域场景集RSSCN7进行零样本分类, 获得了最高50.67%和53.29%的总体分类准确度, 比现有算法各提升了8.06%和9.70%。实验结果表明:该算法能够适应目标域遥感场景图像特征分布的变化, 显著提升遥感场景零样本分类效果, 具有一定的优越性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%A5%E6%84%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遥感;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">零样本分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%3Ci%3Ek%3C%2Fi%3E-means%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank"><i>k</i>-means算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%A3%E6%9E%90%E5%AD%97%E5%85%B8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">解析字典学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像特征;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *全吉成, E-mail:jicheng_quan@126.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-21</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金青年基金 (61301233);</span>
                    </p>
            </div>
                    <h1><b>Zero-Shot Classification for Remote Sensing Scenes Based on Locality Preservation</b></h1>
                    <h2>
                    <span>Wu Chen</span>
                    <span>Wang Hongwei</span>
                    <span>Wang Zhiqiang</span>
                    <span>Yuan Yuwei</span>
                    <span>Liu Yu</span>
                    <span>Cheng Hong</span>
                    <span>Quan Jicheng</span>
            </h2>
                    <h2>
                    <span>University of Naval Aviation</span>
                    <span>Aviation University of Air Force</span>
                    <span>The 91977 of Peoples Liberation Army of China</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Due to the change of image feature distribution in target domain, the performance of zero-shot classification for remote sensing scenes degrades. To solve this problem, a zero-shot classification algorithm for remote sensing scenes based on locality preservation is proposed. Firstly, in order to reduce redundant information, the analysis dictionary learning method was exploited to embed the image features and word vectors of the source domain into the common sparse coefficient space, and the sparse coefficients were compulsively aligned for establishing the relationship between the image features and word vectors. Then, the discriminability of sparse coefficients of scene images was enhanced by preserving the local neighborhood relationship among scene images, which is helpful for clustering analysis on the sparse coefficients. Finally, in order to adapt to the change of image feature distribution, the <i>k</i>-means algorithm was utilized to cluster the sparse coefficients of scene images, and the class labels of the initial centers were used as the scene class labels. With the UCM remote sensing scene dataset as the source domain, zero-shot classification experiments were carried out on RSSCN7 scene dataset of the target domain via two type image features, i.e., GoogLeNet and VGGNet. The highest overall accuracies of 50.67% and 53.29% are obtained, which outperform the state-of-the-art algorithms by 8.06% and 9.70%, respectively. The experimental results show that this method can adapt to the feature distribution of remote sensing scenes, and significantly improve the zero-shot classification performance with certain advantages.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=remote%20sensing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">remote sensing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=zero-shot%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">zero-shot classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=k-means%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">k-means algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=analysis%20dictionary%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">analysis dictionary learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20features&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image features;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-21</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="70" name="70" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="71">近年来, 随着遥感图像空间分辨率不断提升, 传统的基于对象的图像分类方法难以满足遥感图像分析需要, 因此人们提出了基于场景的遥感图像分类方法<citation id="166" type="reference"><link href="18" rel="bibliography" /><link href="20" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。所谓“场景”是指遥感图像中具有主题地物含义的局部图像块, 目前其已成为场景分类的基本单元<citation id="164" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。然而现有遥感图像场景分类需要标注大量的场景训练样本, 而且学习得到的场景识别模型属于监督分类模型, 难以扩展识别其他场景类别。针对该问题, Li等<citation id="165" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出一种遥感场景零样本分类算法, 该算法通过将零样本分类与遥感场景结合, 实现对场景样本标注成本的降低以及对新场景类别的扩展。</p>
                </div>
                <div class="p1">
                    <p id="72">作为一种特殊的无监督分类方法, 零样本分类 (ZSC) 算法使用类别名称的语义词向量提供类别之间的距离关系, 通过迁移源域中的类别标注样本中的知识, 推断识别目标域中类别的样本, 且源域与目标域没有交叉重叠的类别。ZSC算法具有降低样本标注成本和灵活扩展识别能力的优势, 因此近年来受到广泛关注<citation id="167" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。依据学习过程中是否引入未标注的目标域类别样本, ZSC算法分为直推式 (T型) 和推导式 (I型) 两种类型。由于没有未标注的目标域类别样本参与学习, I型ZSC算法更符合ZSC算法的理念。Xian等<citation id="168" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>在细粒度ZSC研究中, Xian等<citation id="169" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>通过引入隐式变量模型到兼容函数学习过程中, 提出了隐式嵌入 (LatEm) 算法。Wang等<citation id="170" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>针对映射函数泛化能力不足的问题, 通过利用语义映射还原目标域中类别的流形结构, 提出了关系知识迁移 (RKT) 算法。Zhang等<citation id="171" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出联合隐式相似性嵌入 (JLSE) 算法。Zhang等<citation id="172" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出语义相似性嵌入 (SSE) 算法。Wang等<citation id="173" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出的双向隐式嵌入 (BiDiLEL) 算法, 将图像特征和语义特征分别映射到公共空间, 并在该公共空间中完成零样本分类。Li等<citation id="174" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出的双视觉语义映射 (DMaP) 算法, 利用了语义空间流形和视觉语义映射迁移能力之间的关系。ZSC算法的基本原理是将源域中监督学习的分类模型, 迁移到目标域中新类别的样本上。这导致I型ZSC算法存在一定的域偏移问题, 降低了分类效果。为减轻域偏移问题, 学习时引入未标注的目标域类别样本, 提出T型ZSC算法。Kodirov等<citation id="175" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出利用目标域词向量正则化映射函数的无监督域适配算法 (UDA) 。Guo等<citation id="176" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出基于共享模型空间学习的T型ZSC算法 (SMS) 。Fu等<citation id="177" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出基于多视角嵌入的T型ZSC算法 (TME) 。此外, 冀中等<citation id="178" type="reference"><link href="44" rel="bibliography" /><link href="46" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>对ZSC算法也进行了相关研究。</p>
                </div>
                <div class="p1">
                    <p id="73">不过, 现有ZSC算法都假定源域和目标域中的遥感场景图像的来源相同, 即光照、成像时间等属性未发生变化。但在实际应用中这些条件通常很难满足, 这导致由图像特征提取器获得的图像特征的分布发生显著变化, 降低遥感场景ZSC效果。针对该问题, 本文采用解析字典学习方法分别提取源域图像特征和语义词向量的稀疏系数, 并强制使两者的稀疏系数一致, 从而实现源域的图像特征与语义词向量对齐。同时, 为增强稀疏系数的鉴别性, 进而提高聚类效果, 将图像特征空间中的局部近邻关系, 保持到稀疏编码空间。最后, 对目标域场景的稀疏系数进行<i>k</i>-means聚类, 并以初始中心的类别标签, 作为对应的聚类簇的场景类别标签。</p>
                </div>
                <h3 id="74" name="74" class="anchor-tag">2 解析字典学习</h3>
                <div class="p1">
                    <p id="75">稀疏化表示是减少冗余信息的一种重要方法<citation id="179" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。字典学习 (DL) 是获取输入样本稀疏系数的典型方法。字典学习方法分为两类, 即合成字典学习 (SDL) 和解析字典学习 (ADL) 。虽然SDL方法应用广泛, 但计算复杂度较高。而ADL通常具有闭式解, 编码能力良好, 计算效率较高, 近年来受到广泛关注<citation id="180" type="reference"><link href="50" rel="bibliography" /><link href="52" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>。ADL的基本公式为</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>arg</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ζ</mi><mo>, </mo><mi mathvariant="bold-italic">Ω</mi></mrow></munder><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ζ</mi><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><mi mathvariant="bold-italic">X</mi><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>, </mo><mspace width="0.25em" /><mspace width="0.25em" /><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">Ω</mi><mo>∈</mo><mi mathvariant="bold-italic">Γ</mi><mo>, </mo><mspace width="0.25em" /></mtd></mtr><mtr><mtd><mo stretchy="false">∥</mo><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub><mo>≤</mo><mi>Τ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">式中, <b><i>X</i></b>=[<i>x</i><sub>1</sub>, …, <i>x</i><sub><i>n</i></sub>]∈ℝ<sup><i>m</i>×<i>n</i></sup>为<i>n</i>个输入样本的特征 (<i>m</i>维) 矩阵, <i>x</i><sub><i>i</i></sub>∈ℝ<sup><i>m</i></sup>为第<i>i</i>个样本特征, <i>z</i><sub><i>i</i></sub>∈ℝ<sup><i>m</i></sup>为<i>x</i><sub><i>i</i></sub>的稀疏系数, 其稀疏性采用L<sub>0</sub>范数及参数<i>T</i><sub>0</sub>实现, <b><i>Z</i></b>=[<i>z</i><sub>1</sub>, …, <i>z</i><sub><i>n</i></sub>]∈ℝ<sup><i>m</i>×<i>n</i></sup>为<b><i>X</i></b>的稀疏系数矩阵;<i>Ω</i>为解析字典, <i>Γ</i>是为避免出现平凡解, 对<i>Ω</i>限制的log-det条件<citation id="181" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。ADL的任务是给定输入特征矩阵<b><i>X</i></b>, 来寻找最佳稀疏系数矩阵<b><i>Z</i></b>及解析字典<i>Ω</i>。</p>
                </div>
                <h3 id="78" name="78" class="anchor-tag">3 具体算法</h3>
                <div class="p1">
                    <p id="79">算法的具体流程如图1所示。</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80"><b>3.1 源域遥感场景的局部保持</b></h4>
                <div class="p1">
                    <p id="81">现有研究证明, 稀疏表示可以减少冗余信息, 提升分类效果<citation id="182" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。将图像特征和语义词向量分别嵌入到中间的公共空间<citation id="183" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 是零样本分类研究中常用的方法。本文采用稀疏编码空间作为中间的公共空间, 运用稀疏编码效率较高的ADL方法, 提取图像特征和语义词向量的稀疏编码系数。虽然场景图像特征空间与语义词向量空间的物理意义及表现方式迥异, 但由于场景图像特征与类别语义词向量表达的是相同的场景对象, 因此嵌入同一稀疏编码空间后, 两者的稀疏系数应保持一致, 即实现场景图像特征与类别语义词向量的互相对齐。本文通过ADL方法将源域场景的图像特征与类别语义词向量嵌入到同一稀疏编码空间, 并强制两者的稀疏系数相同, 实现图像特征与类别词向量的对齐, 从而建立两者之间的联系。</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907039_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 所提算法的整体框架图" src="Detail/GetImg?filename=images/GXXB201907039_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 所提算法的整体框架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907039_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Whole framework of the proposed algorithm</p>

                </div>
                <div class="p1">
                    <p id="84">虽然图像特征与语义词向量在稀疏编码空间中进行了对齐, 但这种对齐是无监督式的, 因此稀疏系数之间的鉴别性不足, 即相同类别场景的稀疏系数没有互相靠近, 不同类别场景的稀疏系数没有互相远离, 使得分类效果下降。已有研究证明, 增强鉴别性可以显著提升分类效果<citation id="184" type="reference"><link href="58" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="85">近年来, 基于流形理论的局部保持算法, 在降维、分类等任务中得到广泛关注<citation id="185" type="reference"><link href="60" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。流形理论认为:高维图像特征空间中隐含着反映图像之间局部近邻关系的低维度空间 (即流形) 。在流形上, 不同类别的图像样本分别聚集在不同的位置, 彼此之间互相远离, 具有良好的鉴别性。通过将图像特征空间中场景样本间的局部近邻关系, 传递到稀疏编码空间, 可增强稀疏编码系数间的鉴别性, 从而提升分类效果。<i>K</i>最近邻 (KNN) 是一种常用的局部近邻表示, 记录了每个样本与其<i>K</i>个近邻样本间的距离关系。通过将每个场景图像的KNN局部关系保持到稀疏编码空间, 可使在图像特征空间流形中靠近或远离的样本, 在稀疏编码空间依然靠近或远离, 从而增强稀疏编码系数之间的鉴别性。源域中的场景图像具有类别标签, 因此采用监督式局部保持。源域遥感场景的局部保持的目标损失函数<i>ζ</i><sub>s</sub>可表示为</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>ζ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>, </mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>, </mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>c</mtext></msub></mrow></munder><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>s</mtext></msub><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>c</mtext></msub><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mo>⊗</mo><mtext>s</mtext></msubsup><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><mtext>t</mtext><mtext>r</mtext><mo stretchy="false">[</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">]</mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>∈</mo><mi mathvariant="bold-italic">Γ</mi><mo>, </mo><mspace width="0.25em" /><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">z</mi><msubsup><mrow></mrow><mi>i</mi><mtext>s</mtext></msubsup><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub><mo>≤</mo><mi>Τ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mspace width="0.25em" /><mo>∀</mo><mi>i</mi><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">式中:第一项和第二项为稀疏编码项, 分别将场景图像特征、语义词向量转换到稀疏编码空间, 且强制两者的稀疏系数相同;第三项为局部保持项, 将图像特征空间中的KNN局部近邻关系保持到稀疏编码空间, 以增强稀疏系数的鉴别性;<i>λ</i>≥0为局部保持项的重要性系数;<b><i>X</i></b><sub>s</sub>∈ℝ<sup><i>d</i><sub><i>x</i></sub>×<i>N</i><sub>s</sub></sup>为源域遥感场景的图像特征矩阵, <i>d</i><sub><i>x</i></sub>为图像特征维数, <i>N</i><sub>s</sub>为源域中遥感场景的个数;<b><i>Z</i></b><sub>s</sub>∈ℝ<sup><i>d</i><sub><i>z</i></sub>×<i>N</i><sub>s</sub></sup>为<b><i>X</i></b><sub>s</sub>对应的稀疏编码系数矩阵, <b><i>z</i></b><sup>s</sup><sub><i>i</i></sub>为<b><i>Z</i></b><sub>s</sub>的第<i>i</i>列向量, <i>d</i><sub><i>z</i></sub>为稀疏编码系数的维数;<b><i>C</i></b><sup>s</sup><sub>⨂</sub>∈ℝ<sup><i>d</i><sub><i>c</i></sub>×<i>N</i><sub>s</sub></sup>为源域场景对应的类别词向量组成的矩阵, <i>d</i><sub>c</sub>为语义词向量的维数;<i>Ω</i><sub>s</sub>∈ℝ<sup><i>d</i><sub><i>z</i></sub>×<i>d</i><sub><i>x</i></sub></sup>为源域场景图像特征的解析字典;<i>Ω</i><sub>c</sub>∈ℝ<sup><i>d</i><sub><i>z</i></sub>×<i>d</i><sub><i>c</i></sub></sup>为语义词向量的解析字典;<b><i>L</i></b><sub>s</sub>=<b><i>D</i></b><sub>s</sub>-<b><i>W</i></b><sub>s</sub>为拉普拉斯矩阵, <b><i>W</i></b><sub>s</sub>是反映源域场景之间局部近邻关系的仿射矩阵, 其元素<b><i>W</i></b><sub>s</sub> (<i>i</i>, <i>j</i>) 表示图像特征<i>x</i><sup>s</sup><sub><i>i</i></sub>和<i>x</i><sup>s</sup><sub><i>j</i></sub>之间的KNN近邻关系, 即</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mi>exp</mi><mo stretchy="false"> (</mo><mo>-</mo><mo stretchy="false">∥</mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mtext>s</mtext></msubsup><mo>-</mo><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mtext>s</mtext></msubsup><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo><mo>, </mo></mtd><mtd><mtable><mtr><mtd><mtable><mtr><mtd><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mtext>s</mtext></msubsup><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mtext>s</mtext></msubsup><mo stretchy="false">) </mo></mtd><mtd><mtext>o</mtext><mtext>r</mtext></mtd><mtd><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mtext>s</mtext></msubsup><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mtext>s</mtext></msubsup><mo stretchy="false">) </mo></mtd></mtr></mtable></mtd></mtr><mtr><mtd><mtable><mtr><mtd><mtext>a</mtext><mtext>n</mtext><mtext>d</mtext></mtd><mtd><mtext>l</mtext><mtext>a</mtext><mtext>b</mtext><mtext>e</mtext><mtext>l</mtext><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mtext>s</mtext></msubsup><mo stretchy="false">) </mo><mo>=</mo><mtext>l</mtext><mtext>a</mtext><mtext>b</mtext><mtext>e</mtext><mtext>l</mtext><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mtext>s</mtext></msubsup><mo stretchy="false">) </mo></mtd></mtr></mtable></mtd></mtr></mtable></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo></mtd><mtd><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow></mrow><mo>;</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89"><b><i>D</i></b><sub>s</sub>为对角矩阵且其对角元素为<b><i>W</i></b><sub>s</sub>中对应行的元素之和, 即<b><i>D</i></b><sub>s</sub> (<i>i</i>, <i>j</i>) =∑<sub><i>j</i></sub><b><i>W</i></b><sub>s</sub> (<i>i</i>, <i>j</i>) , <i>N</i><sub><i>K</i></sub> (<i>x</i><sup>s</sup><sub><i>j</i></sub>) 为<i>x</i><sup>s</sup><sub><i>j</i></sub>的<i>K</i>个近邻, label (<i>x</i><sup>s</sup><sub><i>j</i></sub>) 为<i>x</i><sup>s</sup><sub><i>j</i></sub>的类别标签。因此可知, (2) 式中第三项局部保持项的物理意义为<b><i>Z</i></b><sub>s</sub>的局部近邻关系与<b><i>X</i></b><sub>s</sub>的局部近邻关系 (<b><i>W</i></b><sub>s</sub>) 之间的差异性损失。通过降低这种损失, 使<b><i>Z</i></b><sub>s</sub>与<b><i>X</i></b><sub>s</sub>的局部近邻关系趋于一致, 从而在稀疏编码空间中实现源域场景的局部保持。 (2) 式对变量<b><i>Z</i></b><sub>s</sub>、<i>Ω</i><sub>s</sub>和<i>Ω</i><sub>c</sub>同时非凸, 可以通过固定其他变量方式, 逐个迭代求解。由于<b><i>Z</i></b><sub>s</sub>一般可初始化为样本标签one-hot向量, 而<i>Ω</i><sub>s</sub>和<i>Ω</i><sub>c</sub>的初始化较难, 因此迭代过程需先更新<i>Ω</i><sub>s</sub>和<i>Ω</i><sub>c</sub>, 最后更新<b><i>Z</i></b><sub>s</sub>, 具体过程如下:</p>
                </div>
                <div class="p1">
                    <p id="90">1) 固定<b><i>Z</i></b><sub>s</sub>、<i>Ω</i><sub>c</sub>不变, 更新<i>Ω</i><sub>s</sub>, 此时的目标函数为</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>arg</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>s</mtext></msub><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>, </mo><mspace width="0.25em" /><mspace width="0.25em" /><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>∈</mo><mi mathvariant="bold-italic">Γ</mi><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">为避免出现平凡解, 引入对解析字典<i>Ω</i><sub>s</sub>的log-det条件<i>R</i> (<i>Ω</i><sub>s</sub>) =‖<i>Ω</i><sub>s</sub>‖<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mrow></math></mathml>-lg|det[ (<i>Ω</i><sub>s</sub>) <sup>T</sup><i>Ω</i><sub>s</sub>]|, 其中lg|det[ (<i>Ω</i><sub>s</sub>) <sup>T</sup><i>Ω</i><sub>s</sub>]|用于抑制出现无效解, 避免<i>Ω</i><sub>s</sub>中的行重复或全0。因此, 更新<i>Ω</i><sub>s</sub>的目标函数为</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>arg</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>s</mtext></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>s</mtext></msub><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>R</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">) </mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95"> (5) 式的求解步骤<citation id="186" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>:首先, 将<b><i>X</i></b><sub>s</sub> (<b><i>X</i></b><sub>s</sub>) <sup>T</sup>+<b><i>I</i></b>矩阵Cholesky分解为<b><i>LL</i></b><sup>T</sup>;其次, 对<b><i>L</i></b><sup>-1</sup><b><i>X</i></b><sub>s</sub><b><i>Z</i></b><sub>s</sub>进行奇异值分解, 分解为<b><i>Q</i></b><i>Σ</i><b><i>R</i></b><sup>T</sup>;最后, 得到<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>=</mo><mn>0</mn><mo>.</mo><mn>5</mn><mi mathvariant="bold-italic">R</mi><mrow><mo>[</mo><mrow><mi>Σ</mi><mo>+</mo><mrow><mo> (</mo><mrow><mi>Σ</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mn>2</mn><mi mathvariant="bold-italic">Ι</mi></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow><mo>]</mo></mrow><mi mathvariant="bold-italic">Q</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">L</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="97">2) 固定<i>Ω</i><sub>s</sub>、<b><i>Z</i></b><sub>s</sub>不变, 更新<i>Ω</i><sub>c</sub>, 此时的目标函数为</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>arg</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>c</mtext></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>c</mtext></msub><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mo>⊗</mo><mtext>s</mtext></msubsup><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>, </mo><mspace width="0.25em" /><mspace width="0.25em" /><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mspace width="0.25em" /><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo>∈</mo><mi mathvariant="bold-italic">Γ</mi><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">同理, <i>Ω</i><sub>c</sub>的更新与<i>Ω</i><sub>s</sub>相同。</p>
                </div>
                <div class="p1">
                    <p id="100">3) 固定<i>Ω</i><sub>s</sub>、<i>Ω</i><sub>c</sub>不变, 更新<b><i>Z</i></b><sub>s</sub>, 此时的目标函数为</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>arg</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>s</mtext></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>s</mtext></msub><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>c</mtext></msub><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mo>⊗</mo><mtext>s</mtext></msubsup><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><mtext>t</mtext><mtext>r</mtext><mo stretchy="false">[</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">]</mo><mo>, </mo><mspace width="0.25em" /><mspace width="0.25em" /><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">z</mi><msubsup><mrow></mrow><mi>i</mi><mtext>s</mtext></msubsup><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub><mo>≤</mo><mi>Τ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mspace width="0.25em" /><mo>∀</mo><mi>i</mi><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">通过对<b><i>Z</i></b><sub>s</sub>求导并置<b>0</b>, 得到<b><i>Z</i></b><sub>s</sub>= (<i>Ω</i><sub>s</sub><b><i>X</i></b><sub>s</sub>+<i>Ω</i><sub>c</sub><b><i>C</i></b><sup>s</sup><sub>⨂</sub>) (2<b><i>I</i></b>+<i>λ</i><b><i>L</i></b><sub>s</sub>) <sup>-1</sup>, 并对其进行稀疏化处理。</p>
                </div>
                <div class="p1">
                    <p id="103">迭代循环步骤1) ～3) , 直至循环结束或总的损失函数值下降幅度小于门限值。对于门限值的设置, 除了考虑损失下降的幅度值外, 还应考虑其占总体损失的比例。由于本文实验过程中, 算法的总体损失可快速下降至收敛状态, 因此采用固定迭代次数的方式, 直至迭代循环结束。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104"><b>3.2 目标域遥感场景的局部保持</b></h4>
                <div class="p1">
                    <p id="105">对目标域遥感场景进行局部保持时, 需要利用解析字典将图像特征嵌入到稀疏编码空间, 提取稀疏编码系数。由于目标域与源域的图像特征分布不同, 因此由源域遥感场景学习到的解析字典<i>Ω</i><sub>s</sub>, 不能直接作为目标域图像特征的解析字典<i>Ω</i><sub>u</sub>。两个解析字典虽不相同, 但存在一定的相关性, 通过引入正则项‖<i>Ω</i><sub>s</sub>-<i>Ω</i><sub>u</sub>‖<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mrow></math></mathml>控制偏移量, 实现<i>Ω</i><sub>s</sub>到<i>Ω</i><sub>u</sub>的迁移。由于目标域中的场景图像没有对应的类别标签, 因此采用无监督式KNN局部关系保持。目标域遥感场景局部保持的目标损失函数<i>ζ</i><sub>u</sub>为</p>
                </div>
                <div class="p1">
                    <p id="107" class="code-formula">
                        <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>ζ</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo>, </mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>u</mtext></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>u</mtext></msub><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mi>μ</mi><mtext>t</mtext><mtext>r</mtext><mo stretchy="false">[</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>u</mtext></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">]</mo><mo>, </mo><mspace width="0.25em" /><mspace width="0.25em" /><mo stretchy="false">∥</mo><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mtext>u</mtext></msubsup><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub><mo>≤</mo><mi>Τ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mspace width="0.25em" /><mo>∀</mo><mi>i</mi><mo>, </mo><mspace width="0.25em" /></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mo stretchy="false">∥</mo><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mtext>u</mtext></msubsup><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub><mo>≤</mo><mi>Τ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mspace width="0.25em" /><mo>∀</mo><mi>i</mi><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="108">式中:第一项为稀疏编码项, 将目标域场景的图像特征嵌入到稀疏编码空间, 获取稀疏系数;第二项为解析字典<i>Ω</i><sub>u</sub>的正则项, 便于稳定求解<i>Ω</i><sub>u</sub>;第三项为局部保持项, <i>μ</i>≥0为其重要性系数;<b><i>X</i></b><sub>u</sub>∈ℝ<sup><i>d</i><sub><i>x</i></sub>×<i>N</i><sub>u</sub></sup>为目标域遥感场景的图像特征矩阵, <i>N</i><sub>u</sub>为目标域遥感场景个数;<b><i>Z</i></b><sub>u</sub>∈ℝ<sup><i>d</i><sub><i>z</i></sub>×<i>N</i><sub>u</sub></sup>为<b><i>X</i></b><sub>u</sub>对应的稀疏编码系数矩阵;<i>Ω</i><sub>u</sub>∈ℝ<sup><i>d</i><sub><i>z</i></sub>×<i>d</i><sub><i>x</i></sub></sup>为目标域场景图像特征的解析字典;<b><i>L</i></b><sub>u</sub>=<b><i>D</i></b><sub>u</sub>-<b><i>W</i></b><sub>u</sub>为目标域拉普拉斯矩阵;<b><i>W</i></b><sub>u</sub>是反映目标域场景之间局部近邻关系的仿射矩阵, 其元素<b><i>W</i></b><sub>u</sub> (<i>i</i>, <i>j</i>) 表示图像特征<i>x</i><sup>u</sup><sub><i>i</i></sub>和<i>x</i><sup>u</sup><sub><i>j</i></sub>之间的无监督式KNN近邻关系, 即</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mi>exp</mi><mo stretchy="false"> (</mo><mo>-</mo><mo stretchy="false">∥</mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mtext>u</mtext></msubsup><mo>-</mo><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mtext>u</mtext></msubsup><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo><mo>, </mo></mtd><mtd><mtable><mtr><mtd><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mtext>u</mtext></msubsup><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mtext>u</mtext></msubsup><mo stretchy="false">) </mo></mtd><mtd><mtext>o</mtext><mtext>r</mtext></mtd><mtd><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mtext>u</mtext></msubsup><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mtext>u</mtext></msubsup><mo stretchy="false">) </mo></mtd></mtr></mtable></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo></mtd><mtd><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow></mrow><mo>;</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110"><b><i>D</i></b><sub>u</sub>为对角矩阵且其对角元素为<b><i>W</i></b><sub>u</sub>中对应行的元素之和, 即<b><i>D</i></b><sub>u</sub> (<i>i</i>, <i>i</i>) =∑<sub><i>j</i></sub><b><i>W</i></b><sub>u</sub> (<i>i</i>, <i>j</i>) 。与源域局部保持相似, (8) 式第三项局部保持项的物理意义为<b><i>Z</i></b><sub>u</sub>的局部近邻关系与<b><i>X</i></b><sub>u</sub>的局部近邻关系 (<b><i>W</i></b><sub>u</sub>) 之间的差异性损失。通过降低这种损失, 使<b><i>Z</i></b><sub>u</sub>与<b><i>X</i></b><sub>u</sub>的局部近邻关系趋于一致, 从而在稀疏编码空间中实现目标域场景的局部保持。 (8) 式对变量<b><i>Z</i></b><sub>u</sub>和<i>Ω</i><sub>u</sub>同时非凸, 难以直接求解, 同样通过固定其他变量方式逐个求解, 此时<i>Ω</i><sub>u</sub>初始化为<i>Ω</i><sub>s</sub>, 因此可以先求解<b><i>Z</i></b><sub>u</sub>, 具体过程如下:</p>
                </div>
                <div class="p1">
                    <p id="111">1) 初始化<i>Ω</i><sub>u</sub>为<i>Ω</i><sub>s</sub>, </p>
                </div>
                <div class="p1">
                    <p id="112">2) 固定<i>Ω</i><sub>u</sub>不变, 更新<b><i>Z</i></b><sub>u</sub>, 此时的目标函数为</p>
                </div>
                <div class="p1">
                    <p id="113" class="code-formula">
                        <mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>arg</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mi>u</mi></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>u</mtext></msub><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><mtext>t</mtext><mtext>r</mtext><mo stretchy="false">[</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>u</mtext></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">]</mo><mo>, </mo><mspace width="0.25em" /><mspace width="0.25em" /></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mo stretchy="false">∥</mo><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mtext>u</mtext></msubsup><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub><mo>≤</mo><mi>Τ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mspace width="0.25em" /><mo>∀</mo><mi>i</mi><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="114">通过对<b><i>Z</i></b><sub>u</sub>求导并置<b>0</b>, 得到<b><i>Z</i></b><sub>u</sub>=<i>Ω</i><sub>u</sub><b><i>X</i></b><sub>u</sub> (<b><i>I</i></b>+<i>μ</i><b><i>L</i></b><sub>u</sub>) <sup>-1</sup>并对其进行稀疏化处理。</p>
                </div>
                <div class="p1">
                    <p id="115">3) 固定<b><i>Z</i></b><sub>u</sub>不变, 更新<i>Ω</i><sub>u</sub>, 此时目标函数为</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>arg</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>u</mtext></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>u</mtext></msub><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mtext>u</mtext></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">同样对<i>Ω</i><sub>u</sub>求导并置<b>0</b>, 得到<i>Ω</i><sub>u</sub>=[<b><i>Z</i></b><sub>u</sub> (<b><i>X</i></b><sub>u</sub>) <sup>T</sup>+<i>Ω</i><sub>s</sub>][<b><i>X</i></b><sub>u</sub> (<b><i>X</i></b><sub>u</sub>) <sup>T</sup>+<b><i>I</i></b>]<sup>-1</sup>。</p>
                </div>
                <div class="p1">
                    <p id="118">迭代循环步骤2) ～3) , 直至循环结束或损失函数值下降幅度小于门限值。</p>
                </div>
                <h4 class="anchor-tag" id="119" name="119"><b>3.3 目标域遥感场景的<i>k</i>-means聚类</b></h4>
                <div class="p1">
                    <p id="120">由于成像时间、光照等条件不同, 目标域的遥感图像的色调、纹理等属性与源域的遥感图像相比发生明显变化, 导致通过图像特征提取器获得的图像特征的分布也发生明显变化。虽然T型ZSC方法在学习过程中, 引入目标域图像特征, 在一定程度上减轻因源域与目标域的场景类别不同而导致的域偏移问题, 但这些ZSC方法均假定目标域与源域的遥感图像的来源相同, 图像特征分布未发生变化, 因此当目标域遥感图像特征分布变化时, 现有ZSC方法不能适应这种变化, 从而导致分类效果下降。</p>
                </div>
                <div class="p1">
                    <p id="121">目标域遥感图像特征的分布虽然发生变化, 但是流形分布特点依然存在, 即不同类别场景仍聚集在流形上的不同位置。利用该特点, 对增强鉴别性后的目标域稀疏系数进行无监督聚类, 可以发现场景样本间内在的类别结构关系, 从而减轻图像特征分布变化的影响。<i>k</i>-means因其简单、高效的特点, 成为目前使用最广泛的聚类算法。<i>k</i>-means旨在将样本集划分为<i>k</i>个聚类簇, 使聚类簇中的样本距离自己聚类簇中心的平均距离最小。因此, 为适应目标域图像特征分布变化, 本文以稀疏化后的<i>Ω</i><sub>c</sub><b><i>C</i></b><sub>u</sub>为初始中心 (<b><i>C</i></b><sub>u</sub>∈ℝ<sup><i>d</i><sub>c</sub>×<i>n</i><sub>u</sub></sup>为目标域场景类别的类别词向量组成的矩阵, <b><i>n</i></b><sub>u</sub>为目标域场景类别个数) , 对<b><i>Z</i></b><sub>u</sub>进行<i>k</i>-means聚类, 最后以<i>Ω</i><sub>c</sub><b><i>C</i></b><sub>u</sub>的类别标签, 作为各对应聚类簇的类别标签, 从而实现对目标域场景样本的分类。</p>
                </div>
                <h4 class="anchor-tag" id="122" name="122"><b>3.4 算法流程步骤</b></h4>
                <div class="p1">
                    <p id="123">基于局部保持的遥感场景零样本分类算法的具体步骤如下。</p>
                </div>
                <div class="p1">
                    <p id="124">输入:源域遥感场景的图像特征矩阵<b><i>X</i></b><sub>s</sub>∈ℝ<sup><i>d</i><sub><i>x</i></sub>×<i>N</i><sub>s</sub></sup>, 源域场景对应的类别词向量组成的矩阵<b><i>C</i></b><sup>s</sup><sub>⨂</sub>∈ℝ<sup><i>d</i><sub><i>c</i></sub>×<i>N</i><sub>s</sub></sup>, 目标域遥感场景的图像特征矩阵<b><i>X</i></b><sub>u</sub>∈ℝ<sup><i>d</i><sub><i>x</i></sub>×<i>N</i><sub>u</sub></sup>, 目标域遥感场景类别的类别词向量组成的矩阵<b><i>C</i></b><sub>u</sub>∈ℝ<sup><i>d</i><sub><i>c</i></sub>×<i>n</i><sub>u</sub></sup>, 最大迭代次数为Iter_N。</p>
                </div>
                <div class="p1">
                    <p id="125">输出:对<b><i>X</i></b><sub>u</sub>中样本推断类别标签。</p>
                </div>
                <div class="p1">
                    <p id="126">步骤1:初始化<b><i>Z</i></b><sub>s</sub>为one-hot向量矩阵, 根据 (3) 式计算<b><i>W</i></b><sub>s</sub>、<b><i>D</i></b><sub>s</sub>以及<b><i>L</i></b><sub>s</sub>=<b><i>D</i></b><sub>s</sub>-<b><i>W</i></b><sub>s</sub>, 根据 (9) 式计算<b><i>W</i></b><sub>u</sub>、<b><i>D</i></b><sub>u</sub>以及<b><i>L</i></b><sub>u</sub>=<b><i>D</i></b><sub>u</sub>-<b><i>W</i></b><sub>u</sub>。</p>
                </div>
                <div class="p1">
                    <p id="127">步骤2:固定<b><i>Z</i></b><sub>s</sub>、<i>Ω</i><sub>c</sub>不变, 根据 (5) 式更新<i>Ω</i><sub>s</sub>。</p>
                </div>
                <div class="p1">
                    <p id="128">步骤3:固定<i>Ω</i><sub>s</sub>、<b><i>Z</i></b><sub>s</sub>不变, 根据 (6) 式更新<i>Ω</i><sub>c</sub>。</p>
                </div>
                <div class="p1">
                    <p id="129">步骤4:固定<i>Ω</i><sub>s</sub>、<i>Ω</i><sub>c</sub>不变, 根据 (7) 式更新<b><i>Z</i></b><sub>s</sub>。</p>
                </div>
                <div class="p1">
                    <p id="130">步骤5:固定<i>Ω</i><sub>u</sub>不变, 根据 (10) 式更新<b><i>Z</i></b><sub>u</sub>。</p>
                </div>
                <div class="p1">
                    <p id="131">步骤6:固定<b><i>Z</i></b><sub>u</sub>不变, 根据 (11) 式更新<i>Ω</i><sub>u</sub>。</p>
                </div>
                <div class="p1">
                    <p id="132">步骤7:以稀疏化后的<i>Ω</i><sub>c</sub><b><i>C</i></b><sub>u</sub>为初始中心, 对<b><i>Z</i></b><sub>u</sub>进行<i>k</i>-means聚类, 最后以<i>Ω</i><sub>c</sub><b><i>C</i></b><sub>u</sub>的类别标签, 作为<b><i>Z</i></b><sub>u</sub>各对应聚类簇的类别标签, 从而实现对目标域场景样本的分类。</p>
                </div>
                <div class="p1">
                    <p id="133">步骤8:判断是否达到最大循环次数Iter_N。若是, 则终止执行;若否, 则循环执行步骤2～8。</p>
                </div>
                <h3 id="134" name="134" class="anchor-tag">4 实验及结果分析</h3>
                <h4 class="anchor-tag" id="135" name="135"><b>4.1 数据集及实验设置</b></h4>
                <div class="p1">
                    <p id="136">实验采用两种公开的遥感场景集, UC-Merced (UCM) 数据集<citation id="187" type="reference"><link href="62" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>和RSSCN7数据集<citation id="188" type="reference"><link href="64" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>。UCM是使用最广泛的遥感场景集, 有21类场景, 每类100张图像, 共2100张遥感场景图像, 图像尺寸为256 pixel×256 pixel, 空间分辨率为0.3 m, 其样本如图2所示。RSSCN7共2800张遥感场景图像, 分为7个类别 (1草地、2河湖、3工厂、4场地、5森林、6居民区和7停车场) , 每类400张图像, 图像尺寸为400 pixel×400 pixel, 均来自谷歌地球, 其样本如图3所示。UCM与RSSCN7来源不同, 色调区别明显, 满足本文对源域和目标域遥感场景图像的要求。</p>
                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907039_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 UCM数据集若干类的样本。" src="Detail/GetImg?filename=images/GXXB201907039_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 UCM数据集若干类的样本。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907039_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Images of several UCM classes. </p>
                                <p class="img_note"> (a) 农田; (b) 飞机; (c) 棒球场; (d) 密集住宅; (e) 高速公路; (f) 海港; (g) 储罐; (h) 网球场; (i) 立交桥; (j) 高尔夫球场</p>
                                <p class="img_note"> (a) Farmland; (b) airplane; (c) baseball diamond; (d) dense residential; (e) freeway; (f) harbor; (g) storage tanks; (h) tennis court; (i) overpass; (j) golf course</p>

                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907039_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 RSSCN7数据集各类样本。" src="Detail/GetImg?filename=images/GXXB201907039_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 RSSCN7数据集各类样本。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907039_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Images of RSSCN7 classes.</p>
                                <p class="img_note"> (a) 草地; (b) 河湖; (c) 工厂; (d) 场地; (e) 森林; (f) 居民区; (g) 停车场</p>
                                <p class="img_note"> (a) Grass; (b) river lake; (c) industry; (d) field; (e) forest; (f) residential; (g) parking</p>

                </div>
                <div class="p1">
                    <p id="139">由于深度特征具有更丰富的类别语义信息, 实验采用ImageNet大规模数据集上预训练的深度卷积网络模型GoogLeNet<citation id="189" type="reference"><link href="66" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>和VGGNet<citation id="190" type="reference"><link href="68" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>, 提取全连接层输出 (1024维、4096维) 作为场景的图像特征。采用在维基百科语料上训练的Word2Vec词向量作为场景类别的语义词向量, 维数为300。最大迭代次数Iter_N设置为30, KNN局部近邻关系中设置<i>K</i>=5, 稀疏系数中稀疏的比例设置为10%。实验以UCM作为源域遥感场景集, 对目标域遥感场景集RSSCN7进行零样本分类, 并采用总体分类准确度 (OA) 作为评价指标, OA值为分类正确的RSSCN7场景图像个数占RSSCN7场景总数的比例。由于算法初始化及求解过程均未有随机变量引入, 因此得到的OA值不需进行结果平均。超参数<i>λ</i>、<i>μ</i>的取值范围均为{0, 1}, 当<i>λ</i>=0时, 源域未进行局部保持;当<i>λ</i>=1时, 源域进行了局部保持;当<i>μ</i>=0时, 本文方法属于I型ZSC方法;当<i>μ</i>=1时, 本文方法属于T型ZSC方法。实验采用64位Windows 7操作系统, 内存32 GB, 计算平台使用MATLAB R2016a, CPU为Intel (R) Core (TM) i7-6700 3.40 GHz。</p>
                </div>
                <h4 class="anchor-tag" id="140" name="140"><b>4.2 鉴别性分析</b></h4>
                <div class="p1">
                    <p id="141">本文方法通过采用局部保持, 增强目标域场景的稀疏系数的鉴别性, 提升基于<i>k</i>-means聚类的ZSC效果。本节旨在分析局部保持项能否增强目标域场景的稀疏系数的鉴别性。图4和图5分别为GoogLeNet和VGGNet特征下, <b><i>X</i></b><sub>u</sub>及不同情形<b><i>Z</i></b><sub>u</sub>的t-SNE图。可以看出, 目标域遥感场景的图像特征矩阵<b><i>X</i></b><sub>u</sub>的不同类别的场景样本没有互相远离, 相同类别的场景样本没有互相聚集, 即其鉴别性明显不足。这主要因为训练GoogLeNet和VGGNet深度模型的ImageNet图像集不是遥感场景图像, 没有适应遥感图像本身特点, 因而提取的遥感场景GoogLeNet和VGGNet特征不具有良好的鉴别性。<b><i>Z</i></b><sub>u</sub>+I表示通过本文I型方法 (<i>μ</i>=0) 得到的<b><i>X</i></b><sub>u</sub>的稀疏系数<b><i>Z</i></b><sub>u</sub>。图4和5中的 (b) 、 (c) 为本文I型算法计算得到的<b><i>Z</i></b><sub>u</sub>的t-SNE图, (b) 和 (c) 的主要区别在于: (b) 中的<b><i>Z</i></b><sub>u</sub>未进行源域的局部保持 (<i>λ</i>=0) , 而 (c) 中的<b><i>Z</i></b><sub>u</sub>进行了源域的局部保持 (<i>λ</i>=1) 。图4和5中的 (b) 、 (c) 场景图像样本分布的疏密情况与 (a) 基本一致, 不同类别的场景图像没有互相远离, 而且仍有一定的重叠, 此外, 相同类别的样本也没有互相靠近聚集。上述结果说明:I型的两种情形 (<i>λ</i>=0, <i>λ</i>=1) 下<b><i>Z</i></b><sub>u</sub>的鉴别性依然不足。<b><i>Z</i></b><sub>u</sub>+T表示通过T型方法 (<i>μ</i>=1) 得到的<b><i>X</i></b><sub>u</sub>的稀疏系数<b><i>Z</i></b><sub>u</sub>。图4和5中的 (d) 、 (e) 为本文T型算法计算得到的<b><i>Z</i></b><sub>u</sub>的t-SNE图, (d) 和 (e) 的主要区别在于: (d) 中的<b><i>Z</i></b><sub>u</sub>未进行源域的局部保持 (<i>λ</i>=0) , 而 (e) 中的<b><i>Z</i></b><sub>u</sub>进行了源域的局部保持 (<i>λ</i>=1) 。从图4和5中的 (d) 、 (e) 可知<b><i>Z</i></b><sub>u</sub>场景图像样本的分布情况, 即不同类别的场景样本彼此互相远离, 同类别的场景样本互相靠近聚集, 这与 (a) (b) (c) 有很大不同。由 (d) 和 (e) 的t-SNE图可知:T型的两种情形 (<i>λ</i>=0, <i>λ</i>=1) 下<b><i>Z</i></b><sub>u</sub>的不同类别场景样本彼此互相远离, 同类场景样本彼此互相靠近, 即<b><i>Z</i></b><sub>u</sub>的鉴别性均得到大幅提升, 且明显优于<b><i>X</i></b><sub>u</sub>和<b><i>Z</i></b><sub>u</sub>+I。这表明:目标域的局部保持显著提升了<b><i>Z</i></b><sub>u</sub>的鉴别性, 其主要原因是图像特征空间中的场景局部近邻关系隐含了场景分布流形结构, 而场景分布流形结构本身具有同类样本互相靠近、异类样本互相远离的特性。因此, 通过局部保持将这种场景分布流形结构传递到稀疏系数空间, 可使<b><i>Z</i></b><sub>u</sub>具有良好的鉴别性。图6为本文方法在不同<i>λ</i>、 <i>μ</i>取值下的OA值。可知, 当<i>λ</i>=1且<i>μ</i>=1 (源域和目标域都进行局部保持) 时, 本文方法在GoogLeNet和VGGNet两种图像特征上, 均取得最高OA值。因此, 源域的局部保持 (<i>λ</i>=1) 和目标域的局部保持 (<i>μ</i>=1) 对提升鉴别性和OA值都具有必要性。</p>
                </div>
                <div class="area_img" id="142">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907039_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 GoogLeNet特征下Xu及不同情形Zu的t-SNE图。" src="Detail/GetImg?filename=images/GXXB201907039_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 GoogLeNet特征下<b><i>X</i></b><sub>u</sub>及不同情形<b><i>Z</i></b><sub>u</sub>的t-SNE图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907039_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 The t-SNE graphs of <b><i>X</i></b><sub>u</sub> and different <b><i>Z</i></b><sub>u</sub> on GoogLeNet feature. </p>
                                <p class="img_note"> (a) X<sub>u</sub>; (b) Z<sub>u</sub>+I (λ=0, μ=0) ; (c) Z<sub>u</sub>+I (λ=1, μ=0) ; (d) Z<sub>u</sub>+T (λ=0, μ=1) ; (e) Z<sub>u</sub>+T (λ=1, μ=1) </p>
                                <p class="img_note"> (a) X<sub>u</sub>; (b) Z<sub>u</sub>+I (λ=0, μ=0) ; (c) Z<sub>u</sub>+I (λ=1, μ=0) ; (d) Z<sub>u</sub>+T (λ=0, μ=1) ; (e) Z<sub>u</sub>+T (λ=1, μ=1) </p>

                </div>
                <div class="area_img" id="143">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907039_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 VGGNet特征下Xu及不同情形Zu的t-SNE图。" src="Detail/GetImg?filename=images/GXXB201907039_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 VGGNet特征下<b><i>X</i></b><sub>u</sub>及不同情形<b><i>Z</i></b><sub>u</sub>的t-SNE图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907039_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 The t-SNE graphs of <b><i>X</i></b><sub>u</sub> and different <b><i>Z</i></b><sub>u</sub> on VGGNet feature. </p>
                                <p class="img_note"> (a) X<sub>u</sub>; (b) Z<sub>u</sub>+I (λ=0, μ=0) ; (c) Z<sub>u</sub>+I (λ=1, μ=0) ; (d) Z<sub>u</sub>+T (λ=0, μ=1) ; (e) Z<sub>u</sub>+T (λ=1, μ=1) </p>
                                <p class="img_note"> (a) X<sub>u</sub>; (b) Z<sub>u</sub>+I (λ=0, μ=0) ; (c) Z<sub>u</sub>+I (λ=1, μ=0) ; (d) Z<sub>u</sub>+T (λ=0, μ=1) ; (e) Z<sub>u</sub>+T (λ=1, μ=1) </p>

                </div>
                <div class="area_img" id="144">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907039_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 本文方法在不同λ, μ取值下的OA值" src="Detail/GetImg?filename=images/GXXB201907039_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 本文方法在不同<i>λ</i>, <i>μ</i>取值下的OA值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907039_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 OA values of proposed algorithm under different <i>λ</i> and <i>μ</i></p>

                </div>
                <h4 class="anchor-tag" id="145" name="145"><b>4.3 收敛性分析</b></h4>
                <div class="p1">
                    <p id="146">图7和图8分别为GoogLeNet和VGGNet特征下, 本文算法的总体损失<i>ζ</i><sub>sum</sub>和OA值随迭代次数的变化情况。其中, <i>ζ</i><sub>sum</sub>=<i>ζ</i><sub>s</sub>+<i>ζ</i><sub>u</sub>, 即源域和目标域损失值之和。从图中看出, 随着迭代次数不断增加, <i>ζ</i><sub>sum</sub>快速下降, 并趋于稳定收敛, OA值折线逐步提升并趋于稳定。这表明算法具有良好的收敛性, 这主要得益于优化计算过程中的交替求解策略。</p>
                </div>
                <div class="area_img" id="147">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907039_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 GoogLeNet特征上的算法ζsum/OA曲线" src="Detail/GetImg?filename=images/GXXB201907039_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 GoogLeNet特征上的算法<i>ζ</i><sub>sum</sub>/OA曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907039_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 The <i>ζ</i><sub>sum</sub>/OA curves of algorithm under GoogLeNet feature</p>

                </div>
                <div class="area_img" id="148">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907039_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 VGGNet特征上的算法ζsum/OA曲线" src="Detail/GetImg?filename=images/GXXB201907039_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 VGGNet特征上的算法<i>ζ</i><sub>sum</sub>/OA曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907039_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 The <i>ζ</i><sub>sum</sub>/OA curves of algorithm under VGGNet feature</p>

                </div>
                <h4 class="anchor-tag" id="149" name="149"><b>4.4 分类效果分析</b></h4>
                <h4 class="anchor-tag" id="150" name="150">4.4.1 总体分类准确度</h4>
                <div class="p1">
                    <p id="151">表1是本文及现有典型ZSC方法, 以UCM为源域遥感场景集, 对目标场景集RSSCN7进行分类获取OA值。其中, LatEm、BiDiLEL、JLSE、SSE、DMaP、SAE和RKT及本文RSZSC_I (<i>μ</i>=0) 方法属于I型ZSC方法, 即目标域的场景图像特征未参与训练阶段。UDA、TME、SMS及本文RSZSC_T (<i>μ</i>=1) 方法属于T型ZSC方法, 即目标域的场景图像特征参与训练。RSZSC_I和RSZSC_T均进行了源域局部保持, 即<i>λ</i>=1。从表1可知, 在I型及T型两种情形中, 本文方法均取得了最高OA值。7种I型对比方法中, SSE方法在GoogLeNet、VGGNet特征上分别获得了39.86%和37.36%的最高OA值, 但仍分别低于本文I型方法RSZSC_I (GoogLeNet:43.46%, VGGNet:46.50%) OA值3.6%和9.14%。其主要原因是SSE方法是将目标域场景类别视为由源域各场景类别混合, 基于目标域中相同类别场景的混合比例具有一定的相似性, 将图像特征和词向量嵌入到同一语义空间中测量这种相似性, 实现目标域场景的零样本分类。因此, SSE方法中的相似性处理, 难以转换为语义空间中场景的鉴别性。而本文RSZSC_I方法将源域中场景间局部关系保持到稀疏编码空间, 并通过解析字典<i>Ω</i><sub>s</sub>和<i>Ω</i><sub>c</sub>, 将这种鉴别性间接保持到目标域, 从而提升了目标域场景稀疏系数的鉴别性。T型方法中, 本文RSZSC_T方法在GoogLeNet、VGGNet特征上分别获得50.67%和53.29%的最大OA值, 优于UDA (GoogLeNet:42.61%, VGGNet:43.59%) OA值8.06%和9.70%。其主要原因为UDA方法利用目标域语义词向量正则化映射函数来减轻域偏移问题, 而UDA正则化时需要事先利用源域数据对目标域场景的类别标签预测, 以作为正则化的目标项, 但源域和目标域图像特征分布不同, 引入了预测误差, 因此降低了目标域样本间鉴别性。</p>
                </div>
                <h4 class="anchor-tag" id="152" name="152">4.4.2 RSSCN7各类别的分类准确度</h4>
                <div class="p1">
                    <p id="153">图9和图10分别为I型和T型ZSC方法在GoogLeNet和VGGNet图像特征上, 对目标域各场景类别的分类准确度。可以看出, 本文RSZSC_I和RSZSC_T方法在目标域7种场景类别上, 均取得了最高的分类准确度, 优于典型的ZSC方法, 证明了本文“增强鉴别性+<i>k</i>-means聚类”方法能够适应目标域图像特征分布的变化, 取得最高的分类准确度。</p>
                </div>
                <div class="area_img" id="154">
                    <p class="img_tit">表1 各ZSC方法在对RSSCN7上的总体分类准确度 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 OA values of ZSC methods on RSSCN7</p>
                    <p class="img_note"></p>
                    <table id="154" border="1"><tr><td rowspan="2"><br />Method</td><td rowspan="2">I/T</td><td colspan="2"><br />OA /%</td></tr><tr><td><br />GoogLeNet</td><td>VGGNet</td></tr><tr><td><br />LatEm</td><td>I</td><td>18.79</td><td>14.15</td></tr><tr><td><br />BiDiLEL</td><td>I</td><td>14.51</td><td>14.47</td></tr><tr><td><br />JLSE</td><td>I</td><td>36.61</td><td>33.45</td></tr><tr><td><br />SSE</td><td>I</td><td>39.86</td><td>37.36</td></tr><tr><td><br />DMaP</td><td>I</td><td>34.00</td><td>32.07</td></tr><tr><td><br />SAE</td><td>I</td><td>34.20</td><td>35.60</td></tr><tr><td><br />RKT</td><td>I</td><td>35.47</td><td>32.07</td></tr><tr><td><br /><b>RSZSC_I</b></td><td>I</td><td><b>43.46</b></td><td><b>46.50</b></td></tr><tr><td><br />UDA</td><td>T</td><td>42.61</td><td>43.59</td></tr><tr><td><br />TME</td><td>T</td><td>41.96</td><td>42.44</td></tr><tr><td><br />SMS</td><td>T</td><td>37.15</td><td>39.18</td></tr><tr><td><br /><b>RSZSC_T</b></td><td>T</td><td><b>50.67</b></td><td><b>53.29</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907039_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 I型ZSC方法对RSSCN7各类别的分类准确度。" src="Detail/GetImg?filename=images/GXXB201907039_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 I型ZSC方法对RSSCN7各类别的分类准确度。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907039_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Accuracies of different classes of RSSCN7 produced by I type ZSC methods. </p>
                                <p class="img_note"> (a) GoogLeNet; (b) VGGNet</p>
                                <p class="img_note"> (a) GoogLeNet; (b) VGGNet</p>

                </div>
                <div class="area_img" id="156">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907039_156.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 T型ZSC方法对RSSCN7各类别的分类准确度。" src="Detail/GetImg?filename=images/GXXB201907039_156.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 T型ZSC方法对RSSCN7各类别的分类准确度。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907039_156.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Accuracies of different classes of RSSCN7 produced by T type ZSC methods.</p>
                                <p class="img_note"> (a) GoogLeNet; (b) VGGNet</p>
                                <p class="img_note"> (a) GoogLeNet; (b) VGGNet</p>

                </div>
                <h4 class="anchor-tag" id="158" name="158"><b>4.5 计算效率分析</b></h4>
                <div class="p1">
                    <p id="159">为比较本文与其他ZSC方法的计算效率, 测试各ZSC方法对图像特征GoogLeNet的运算耗时, 结果如表2所示。从表2可以看出, 在I型算法中, DMaP方法耗时最长, 为396.45 s, 其次是JLSE方法, 耗时为67.49 s, 而本文RSZSC_I方法耗时最短, 为11.68 s。在T型算法中, TME耗时最长, 为76.91 s, 而本文RSZSC_T方法耗时最短, 为14.70 s。这主要因为ADL算法的时间复杂度低, 且每个更新步骤都有闭式解, 使本文方法的运算效率优于典型的ZSC方法。</p>
                </div>
                <h3 id="160" name="160" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="161">成像时间等条件不同, 导致目标域与源域的遥感图像特征分布发生变化, 令遥感场景零分类效果下降。针对该问题, 提出一种基于局部保持的遥感场景零样本分类算法。该方法的主要特点是:1) 利用稀疏编码效率较高的解析字典学习方法, 提取图像特征和语义词向量的稀疏系数, 减少了冗余信息;2) 采用基于流形理论的局部近邻关系保持, 显著提升了目标域场景稀疏系数的鉴别性;3) 采用<i>k</i>-means方法对目标域稀疏系数进行聚类, 并推断各场景的类别标签, 有效减轻了图像特征分布变化对分类效果的影响。实验结果证明, 本文方法能够适应目标域图像特征分布变化, 分类效果优于典型的ZSC方法, 具有一定的优越性和适应性。但该方法仅利用了简单的KNN局部近邻关系, 下一步将考虑使用更复杂的近邻关系, 以进一步提升遥感场景零样本分类效果。</p>
                </div>
                <div class="area_img" id="162">
                    <p class="img_tit">表2 ZSC算法在GoogLeNet特征上的运算耗时 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Computing time of different ZSC algorithms on GoogLeNet feature</p>
                    <p class="img_note"></p>
                    <table id="162" border="1"><tr><td><br />Method</td><td>I/T</td><td>Time /s</td></tr><tr><td><br />LatEm</td><td>I</td><td>17.71</td></tr><tr><td><br />BiDiLEL</td><td>I</td><td>22.59</td></tr><tr><td><br />JLSE</td><td>I</td><td>67.49</td></tr><tr><td><br />SSE</td><td>I</td><td>14.35</td></tr><tr><td><br />DMaP</td><td>I</td><td>396.45</td></tr><tr><td><br />SAE</td><td>I</td><td>16.21</td></tr><tr><td><br />RKT</td><td>I</td><td>22.53</td></tr><tr><td><br /><b>RSZSC_I</b></td><td>I</td><td><b>11.68</b></td></tr><tr><td><br />UDA</td><td>T</td><td>56.37</td></tr><tr><td><br />TME</td><td>T</td><td>76.91</td></tr><tr><td><br />SMS</td><td>T</td><td>49.28</td></tr><tr><td><br /><b>RSZSC_T</b></td><td>T</td><td><b>14.70</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="18">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604039&amp;v=MDk0NTJPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5amtVTDdPSWpYVGJMRzRIOWZNcTQ5R2JZUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Liu D W, Han L, Han X Y.High spatial resolution remote sensing image classification based on deep learning[J].Acta Optica Sinica, 2016, 36 (4) :0428001.刘大伟, 韩玲, 韩晓勇.基于深度学习的高分辨率遥感影像分类研究[J].光学学报, 2016, 36 (4) :0428001.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pyramid of Spatial Relatons for Scene-Level Land Use Classification">

                                <b>[2]</b> Chen S Z, Tian Y L.Pyramid of spatial relatons for scene-level land use classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (4) :1947-1957.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning for remote sensing data:A technical tutorial on the state of the art">

                                <b>[3]</b> Zhang L P, Zhang L F, Du B.Deep learning for remote sensing data:a technical tutorial on the state of the art[J].IEEE Geoscience and Remote Sensing Magazine, 2016, 4 (2) :22-40.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot scene classification for high spatial resolution remote sensing images">

                                <b>[4]</b> Li A X, Lu Z W, Wang L W, <i>et al</i>.Zero-shot scene classification for high spatial resolution remote sensing images[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :4157-4167.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Latent embeddings for zero-shot classification">

                                <b>[5]</b> Xian Y Q, Akata Z, Sharma G, <i>et al</i>.Latent embeddings for zero-shot classification[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition, June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:69-77.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Relational knowledge transfer for zero-shot learning">

                                <b>[6]</b> Wang D H, Li Y, Lin Y T, <i>et al</i>.Relational knowledge transfer for zero-shot learning[C]//Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16) , February 12-17, 2016, Phoenix, Arizona, USA.California:AAAI Publications, 2016:2145-2151.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning via joint latent similarity embedding">

                                <b>[7]</b> Zhang Z M, Saligrama V.Zero-shot learning via joint latent similarity embedding[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition, June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:6034-6042.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning via semantic similarity embedding">

                                <b>[8]</b> Zhang Z M, Saligrama V.Zero-shot learning via semantic similarity embedding[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:4166-4174.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDD31F20253A10A4B4D5AE16DA35AA17A9&amp;v=MDE3MzJwYlEzNWRoaHhMbTh3YTQ9Tmo3QmFzZTdINmZPcjQxQVo1b09EQTA5dlJKbjcwNElTWG1XM1JFd0NNT1ZRc3VXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Wang Q, Chen K.Zero-shot visual recognition via bidirectional latent embedding[J].International Journal of Computer Vision, 2017, 124 (3) :356-383.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot recognition using dual visual-semantic mapping paths">

                                <b>[10]</b> Li Y N, Wang D H, Hu H H, <i>et al</i>.Zero-shot recognition using dual visual-semantic mapping paths[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition, July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:5207-5215.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised Domain Adaptation for Zero-Shot Learning">

                                <b>[11]</b> Kodirov E, Xiang T, Fu Z Y, <i>et al</i>.Unsupervised domain adaptation for zero-shot learning[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:2452-2460.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transductive zero-shot recognition via shared model space learning">

                                <b>[12]</b> Guo Y C, Ding G G, Jing X M, <i>et al</i>.Transductive zero-shot recognition via shared model space learning[C]//Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA.California:AAAI Publications, 2016:3494-3500.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transductive multi-view embedding for zero-shot recognition and annotation">

                                <b>[13]</b> Fu Y W, Hospedales T M, Xiang T, <i>et al</i>.Transductive multi-view embedding for zero-shot recognition and annotation[M]//Fleet D, Pajdla T, Schiele B, <i>et al</i>.Lecture notes in computer science 2014.Cham:Springer, 2014, 8690:584-599.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJDX201708005&amp;v=MjE1MjllVnVGeWprVUw3T01TZlBkckc0SDliTXA0OUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Ji Z, Xie Y Z, Pang Y W.Zero-shot learning based on canonical correlation analysis and distance metric learning[J].Journal of Tianjin University, 2017, 50 (8) :813-820.冀中, 谢于中, 庞彦伟.基于典型相关分析和距离度量学习的零样本学习[J].天津大学学报, 2017, 50 (8) :813-820.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201711012&amp;v=MDU2MTR6cXFCdEdGckNVUkxPZVplVnVGeWprVUw3T055ZlRiTEc0SDliTnJvOUVab1FLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Ji Z, Sun T, Yu Y L.Transductive discriminative dictionary learning approach for zero-shot classification[J].Journal of Software, 2017, 28 (11) :2961-2970.冀中, 孙涛, 于云龙.一种基于直推判别字典学习的零样本分类方法[J].软件学报, 2017, 28 (11) :2961-2970.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dictionary learning for sparse coding:algorithms and convergence analysis">

                                <b>[16]</b> Bao C L, Ji H, Quan Y H, <i>et al</i>.Dictionary learning for sparse coding:algorithms and convergence analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (7) :1356-1369.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative analysis-synthesis dictionary learning for image classification">

                                <b>[17]</b> Yang M, Chang H Y, Luo W X.Discriminative analysis-synthesis dictionary learning for image classification[J].Neurocomputing, 2017, 219:404-411.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES579AE40DE8831392A28EB8D29F9C9B3D&amp;v=MDI5NzI2RGNJT25lV3JodERjTUdkTjduckNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRoaHhMbTh3YTQ9TmlmT2ZiYS9GNkM1cTQ4eEVlTUhEMzA2eGhSaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Wang J J, Guo Y Q, Guo J, <i>et al</i>.Synthesis linear classifier based analysis dictionary learning for pattern classification[J].Neurocomputing, 2017, 238:103-113.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=L0 sparsifying transform learning with efficient optimal updates and convergence guarantees">

                                <b>[19]</b> Ravishankar S, Bresler Y.Sparsifying transform learning with efficient optimal updates and convergence guarantees[J].IEEE Transactions on Signal Processing, 2015, 63 (9) :2389-2404.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012156&amp;v=MDM4MzdUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JSUY4WGF4UT1OaWZKWmJLOUh0ak1xbzlGWk9vTkRYay9vQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> Kreutz-Delgado K, Murray J F, Rao B D, <i>et al</i>.Dictionary learning algorithms for sparse representation[J].Neural Computation, 2003, 15 (2) :349-396.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative feature fusion for image classific-ation">

                                <b>[21]</b> Fernando B, Fromont E, Muselet D, <i>et al</i>.Discriminative feature fusion for image classification[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition, June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:3434-3441.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Global Geometric Framework for Nonlinear Dimensionality Reduction">

                                <b>[22]</b> Tenenbaum J B.A global geometric framework for nonlinear dimensionality reduction[J].Science, 2000, 290 (5500) :2319-2323.
                            </a>
                        </p>
                        <p id="62">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bag-of-visual-words and spatial extensions for land-use classification">

                                <b>[23]</b> Yang Y, Newsam S.Bag-of-visual-words and spatial extensions for land-use classification[C]//Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems, November 2-5, 2010, San Jose, California.New York:ACM, 2010:270-279.
                            </a>
                        </p>
                        <p id="64">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Learning Based Feature Selection for Remote Sensing Scene Classification">

                                <b>[24]</b> Zou Q, Ni L H, Zhang T, <i>et al</i>.Deep learning based feature selection for remote sensing scene classification[J].IEEE Geoscience and Remote Sensing Letters, 2015, 12 (11) :2321-2325.
                            </a>
                        </p>
                        <p id="66">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[25]</b> Szegedy C, Liu W, Jia Y Q, <i>et al</i>.Going deeper with convolutions[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition, June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:7298594.
                            </a>
                        </p>
                        <p id="68">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[26]</b> Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [ 2019-01-01].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201907039" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201907039&amp;v=MjEwMjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlqa1VMN09JalhUYkxHNEg5ak1xSTlHYllRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

