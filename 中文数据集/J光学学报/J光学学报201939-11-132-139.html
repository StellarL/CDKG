

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133060370440000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dGXXB201911016%26RESULT%3d1%26SIGN%3daWRfX6PEh4AMIQX4Gc6Etm1TnJI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201911016&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201911016&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201911016&amp;v=MTQ2NTQ5RVlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5dm5VYnZQSWpYVGJMRzRIOWpOcm8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#46" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#49" data-title="2 PCNN模型的改进 ">2 PCNN模型的改进</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="3 PCNN参数设置 ">3 PCNN参数设置</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="&lt;b&gt;3.1 耦合连接矩阵&lt;/b&gt;&lt;i&gt;&lt;b&gt;W&lt;/b&gt;&lt;/i&gt;&lt;b&gt;的设置&lt;/b&gt;"><b>3.1 耦合连接矩阵</b><i><b>W</b></i><b>的设置</b></a></li>
                                                <li><a href="#63" data-title="&lt;b&gt;3.2 参数&lt;/b&gt;&lt;i&gt;α&lt;/i&gt;&lt;sub&gt;&lt;i&gt;&lt;b&gt;L&lt;/b&gt;&lt;/i&gt;&lt;/sub&gt;、&lt;i&gt;&lt;b&gt;V&lt;/b&gt;&lt;/i&gt;&lt;sub&gt;&lt;i&gt;&lt;b&gt;L&lt;/b&gt;&lt;/i&gt;&lt;/sub&gt;、&lt;i&gt;α&lt;/i&gt;&lt;sub&gt;&lt;i&gt;θ&lt;/i&gt;&lt;/sub&gt;、&lt;i&gt;&lt;b&gt;V&lt;/b&gt;&lt;/i&gt;&lt;sub&gt;&lt;i&gt;θ&lt;/i&gt;&lt;/sub&gt;&lt;b&gt;以及&lt;/b&gt;&lt;i&gt;&lt;b&gt;β&lt;/b&gt;&lt;/i&gt;&lt;b&gt;的设置&lt;/b&gt;"><b>3.2 参数</b><i>α</i><sub><i><b>L</b></i></sub>、<i><b>V</b></i><sub><i><b>L</b></i></sub>、<i>α</i><sub><i>θ</i></sub>、<i><b>V</b></i><sub><i>θ</i></sub><b>以及</b><i><b>β</b></i><b>的设置</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#77" data-title="4 融合框架 ">4 融合框架</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="5 实验结果及分析 ">5 实验结果及分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="&lt;b&gt;5.1 实验相关设置&lt;/b&gt;"><b>5.1 实验相关设置</b></a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;5.2 实验结果分析&lt;/b&gt;"><b>5.2 实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#105" data-title="6 结  论 ">6 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="图1 改进的PCNN神经元结构">图1 改进的PCNN神经元结构</a></li>
                                                <li><a href="#91" data-title="图2 本文融合算法框架">图2 本文融合算法框架</a></li>
                                                <li><a href="#98" data-title="图3 4组原始红外与可见光图像。">图3 4组原始红外与可见光图像。</a></li>
                                                <li><a href="#100" data-title="表1 不同算法融合结果的客观评价指标">表1 不同算法融合结果的客观评价指标</a></li>
                                                <li><a href="#101" data-title="图4 不同算法融合结果对比。">图4 不同算法融合结果对比。</a></li>
                                                <li><a href="#103" data-title="表2 不同融合算法平均运行时间对比">表2 不同融合算法平均运行时间对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="14">


                                    <a id="bibliography_1" title=" &lt;i&gt;Dogra A&lt;/i&gt;,&lt;i&gt;Goyal B&lt;/i&gt;,&lt;i&gt;Agrawal S&lt;/i&gt;.&lt;i&gt;From multi&lt;/i&gt;-&lt;i&gt;scale decomposition to non&lt;/i&gt;-&lt;i&gt;multi&lt;/i&gt;-&lt;i&gt;scale decomposition methods&lt;/i&gt;:&lt;i&gt;a comprehensive survey of image fusion techniques and its applications&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Access&lt;/i&gt;,2017,5:16040-16067." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=From multi-scale decomposition to non-multi-scale decomposition methods:a comprehensive survey of image fusion techniques and its applications">
                                        <b>[1]</b>
                                         &lt;i&gt;Dogra A&lt;/i&gt;,&lt;i&gt;Goyal B&lt;/i&gt;,&lt;i&gt;Agrawal S&lt;/i&gt;.&lt;i&gt;From multi&lt;/i&gt;-&lt;i&gt;scale decomposition to non&lt;/i&gt;-&lt;i&gt;multi&lt;/i&gt;-&lt;i&gt;scale decomposition methods&lt;/i&gt;:&lt;i&gt;a comprehensive survey of image fusion techniques and its applications&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Access&lt;/i&gt;,2017,5:16040-16067.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_2" title=" &lt;i&gt;Chen Z&lt;/i&gt;,&lt;i&gt;Yang X P&lt;/i&gt;,&lt;i&gt;Zhang C X&lt;/i&gt;,et al.&lt;i&gt;Infrared and visible image fusion based on the compensation mechanism in NSCT domain&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Chinese Journal of Scientific Instrument&lt;/i&gt;,2016,37(4):860-870.陈震,杨小平,张聪炫,等.基于补偿机制的&lt;i&gt;NSCT&lt;/i&gt;域红外与可见光图像融合[&lt;i&gt;J&lt;/i&gt;].仪器仪表学报,2016,37(4):860-870." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201604019&amp;v=MjcwMzVxcUJ0R0ZyQ1VSTE9lWmVWdkZ5dm5VYnZQUER6VGJMRzRIOWZNcTQ5RWJZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         &lt;i&gt;Chen Z&lt;/i&gt;,&lt;i&gt;Yang X P&lt;/i&gt;,&lt;i&gt;Zhang C X&lt;/i&gt;,et al.&lt;i&gt;Infrared and visible image fusion based on the compensation mechanism in NSCT domain&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Chinese Journal of Scientific Instrument&lt;/i&gt;,2016,37(4):860-870.陈震,杨小平,张聪炫,等.基于补偿机制的&lt;i&gt;NSCT&lt;/i&gt;域红外与可见光图像融合[&lt;i&gt;J&lt;/i&gt;].仪器仪表学报,2016,37(4):860-870.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_3" title=" &lt;i&gt;Zhang B H&lt;/i&gt;,&lt;i&gt;Lu X Q&lt;/i&gt;,&lt;i&gt;Pei H Q&lt;/i&gt;,et al.&lt;i&gt;A fusion algorithm for infrared and visible images based on saliency analysis and non&lt;/i&gt;-&lt;i&gt;subsampled shearlet transform&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Infrared Physics&lt;/i&gt; &amp;amp; &lt;i&gt;Technology&lt;/i&gt;,2015,73:286-297." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A fusion algorithm for infrared and visible images based on saliency analysis and non-subsampled shearlet transform">
                                        <b>[3]</b>
                                         &lt;i&gt;Zhang B H&lt;/i&gt;,&lt;i&gt;Lu X Q&lt;/i&gt;,&lt;i&gt;Pei H Q&lt;/i&gt;,et al.&lt;i&gt;A fusion algorithm for infrared and visible images based on saliency analysis and non&lt;/i&gt;-&lt;i&gt;subsampled shearlet transform&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Infrared Physics&lt;/i&gt; &amp;amp; &lt;i&gt;Technology&lt;/i&gt;,2015,73:286-297.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_4" title=" &lt;i&gt;Wu D P&lt;/i&gt;,&lt;i&gt;Bi D Y&lt;/i&gt;,&lt;i&gt;He L Y&lt;/i&gt;,et al.&lt;i&gt;A fusion algorithm of infrared and visible image based on NSSCT&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Optica Sinica&lt;/i&gt;,2017,37(7):0710003.吴冬鹏,毕笃彦,何林远,等.基于&lt;i&gt;NSSCT&lt;/i&gt;的红外与可见光图像融合[&lt;i&gt;J&lt;/i&gt;].光学学报,2017,37(7):0710003." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201707013&amp;v=MTI0NTJPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5dm5VYnZQSWpYVGJMRzRIOWJNcUk5RVo0UUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         &lt;i&gt;Wu D P&lt;/i&gt;,&lt;i&gt;Bi D Y&lt;/i&gt;,&lt;i&gt;He L Y&lt;/i&gt;,et al.&lt;i&gt;A fusion algorithm of infrared and visible image based on NSSCT&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Optica Sinica&lt;/i&gt;,2017,37(7):0710003.吴冬鹏,毕笃彦,何林远,等.基于&lt;i&gt;NSSCT&lt;/i&gt;的红外与可见光图像融合[&lt;i&gt;J&lt;/i&gt;].光学学报,2017,37(7):0710003.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_5" title=" &lt;i&gt;Yin M&lt;/i&gt;,&lt;i&gt;Duan P H&lt;/i&gt;,&lt;i&gt;Liu W&lt;/i&gt;,et al.&lt;i&gt;A novel infrared and visible image fusion algorithm based on shift&lt;/i&gt;-&lt;i&gt;invariant dual&lt;/i&gt;-&lt;i&gt;tree complex shearlet transform and sparse representation&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Neurocomputing&lt;/i&gt;,2017,226:182-191." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7A4A453866DB652BE03740A731660FA5&amp;v=MTczMDJUSkdxRElxb3hOWXUxN2Zubzh6V1JtNmp4NlRIK1RxeEUwZjdTVU04dWFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkbGh4N3E5eEs4PU5pZk9mYg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         &lt;i&gt;Yin M&lt;/i&gt;,&lt;i&gt;Duan P H&lt;/i&gt;,&lt;i&gt;Liu W&lt;/i&gt;,et al.&lt;i&gt;A novel infrared and visible image fusion algorithm based on shift&lt;/i&gt;-&lt;i&gt;invariant dual&lt;/i&gt;-&lt;i&gt;tree complex shearlet transform and sparse representation&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Neurocomputing&lt;/i&gt;,2017,226:182-191.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_6" title=" &lt;i&gt;Deng H&lt;/i&gt;,&lt;i&gt;Wang C L&lt;/i&gt;,&lt;i&gt;Hu Y J&lt;/i&gt;,et al.&lt;i&gt;Fusion of infrared and visible images based on non&lt;/i&gt;-&lt;i&gt;subsampled dual&lt;/i&gt;-&lt;i&gt;tree complex contourlet and adaptive block&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Photonica Sinica&lt;/i&gt;,2019,48(7):0710006.邓辉,王长龙,胡永江,等.基于非下采样双树复轮廓波与自适应分块的红外与可见光图像融合[&lt;i&gt;J&lt;/i&gt;].光子学报,2019,48(7):0710006." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GZXB201907015&amp;v=MTY3MTh0R0ZyQ1VSTE9lWmVWdkZ5dm5VYnZQSWpmVGJMRzRIOWpNcUk5RVlZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         &lt;i&gt;Deng H&lt;/i&gt;,&lt;i&gt;Wang C L&lt;/i&gt;,&lt;i&gt;Hu Y J&lt;/i&gt;,et al.&lt;i&gt;Fusion of infrared and visible images based on non&lt;/i&gt;-&lt;i&gt;subsampled dual&lt;/i&gt;-&lt;i&gt;tree complex contourlet and adaptive block&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Photonica Sinica&lt;/i&gt;,2019,48(7):0710006.邓辉,王长龙,胡永江,等.基于非下采样双树复轮廓波与自适应分块的红外与可见光图像融合[&lt;i&gt;J&lt;/i&gt;].光子学报,2019,48(7):0710006.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_7" title=" &lt;i&gt;Meher B&lt;/i&gt;,&lt;i&gt;Agrawal S&lt;/i&gt;,&lt;i&gt;Panda R&lt;/i&gt;,et al.&lt;i&gt;A survey on region based image fusion methods&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Information Fusion&lt;/i&gt;,2019,48:119-132." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC425305563F7DAD147D59EE71752A9DA&amp;v=MDYyNTdpZk9mY0M4SE5UUHI0cEFZdWg1Q3doSXV4Y1g3VXQ0UVFxWHF4TXlmTERsVE03dUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRsaHg3cTl4Szg9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         &lt;i&gt;Meher B&lt;/i&gt;,&lt;i&gt;Agrawal S&lt;/i&gt;,&lt;i&gt;Panda R&lt;/i&gt;,et al.&lt;i&gt;A survey on region based image fusion methods&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Information Fusion&lt;/i&gt;,2019,48:119-132.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_8" title=" &lt;i&gt;Zhu H R&lt;/i&gt;,&lt;i&gt;Liu Y Q&lt;/i&gt;,&lt;i&gt;Zhang W Y&lt;/i&gt;.&lt;i&gt;Infrared and visible image fusion based on iterative guided filtering and multi&lt;/i&gt;-&lt;i&gt;visual weight information&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Photonica Sinica&lt;/i&gt;,2019,48(3):0310002.朱浩然,刘云清,张文颖.基于迭代导向滤波与多视觉权重信息的红外与可见光图像融合[&lt;i&gt;J&lt;/i&gt;].光子学报,2019,48(3):0310002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GZXB201903024&amp;v=MDE2MjJmVGJMRzRIOWpNckk5SFlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5dm5VYnZQSWo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         &lt;i&gt;Zhu H R&lt;/i&gt;,&lt;i&gt;Liu Y Q&lt;/i&gt;,&lt;i&gt;Zhang W Y&lt;/i&gt;.&lt;i&gt;Infrared and visible image fusion based on iterative guided filtering and multi&lt;/i&gt;-&lt;i&gt;visual weight information&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Photonica Sinica&lt;/i&gt;,2019,48(3):0310002.朱浩然,刘云清,张文颖.基于迭代导向滤波与多视觉权重信息的红外与可见光图像融合[&lt;i&gt;J&lt;/i&gt;].光子学报,2019,48(3):0310002.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_9" title=" &lt;i&gt;Lu X Q&lt;/i&gt;,&lt;i&gt;Zhang B H&lt;/i&gt;,&lt;i&gt;Zhao Y&lt;/i&gt;,et al.&lt;i&gt;The infrared and visible image fusion algorithm based on target separation and sparse representation&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Infrared Physics&lt;/i&gt; &amp;amp; &lt;i&gt;Technology&lt;/i&gt;,2014,67:397-407." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700406695&amp;v=MzAyNTJJMXdXYmhVPU5pZk9mYks4SDlETXFJOUZZT3NKQ25VOG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWI3SQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         &lt;i&gt;Lu X Q&lt;/i&gt;,&lt;i&gt;Zhang B H&lt;/i&gt;,&lt;i&gt;Zhao Y&lt;/i&gt;,et al.&lt;i&gt;The infrared and visible image fusion algorithm based on target separation and sparse representation&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Infrared Physics&lt;/i&gt; &amp;amp; &lt;i&gt;Technology&lt;/i&gt;,2014,67:397-407.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_10" title=" &lt;i&gt;Lindblad T&lt;/i&gt;,&lt;i&gt;Kinser J M&lt;/i&gt;.&lt;i&gt;Image processing using pulse&lt;/i&gt;-&lt;i&gt;coupled neural networks&lt;/i&gt;[&lt;i&gt;M&lt;/i&gt;].&lt;i&gt;Ma Y D&lt;/i&gt;,&lt;i&gt;Zhan K&lt;/i&gt;,&lt;i&gt;Wang Z B&lt;/i&gt;,et al.,&lt;i&gt;Transl&lt;/i&gt;.&lt;i&gt;Beijing&lt;/i&gt;:&lt;i&gt;Higher Education Press&lt;/i&gt;,2008:10-20.&lt;i&gt;Lindblad T&lt;/i&gt;,&lt;i&gt;Kinser J M&lt;/i&gt;.脉冲耦合神经网络图像处理[&lt;i&gt;M&lt;/i&gt;].马义德,绽琨,王兆滨,等,译.北京:高等教育出版社,2008:10-20." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787040244632001&amp;v=MzI3MTRQSXE0bEdadXNQRFJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3JrVTd6S0lsb1RYRnF6R2JPOEh0&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         &lt;i&gt;Lindblad T&lt;/i&gt;,&lt;i&gt;Kinser J M&lt;/i&gt;.&lt;i&gt;Image processing using pulse&lt;/i&gt;-&lt;i&gt;coupled neural networks&lt;/i&gt;[&lt;i&gt;M&lt;/i&gt;].&lt;i&gt;Ma Y D&lt;/i&gt;,&lt;i&gt;Zhan K&lt;/i&gt;,&lt;i&gt;Wang Z B&lt;/i&gt;,et al.,&lt;i&gt;Transl&lt;/i&gt;.&lt;i&gt;Beijing&lt;/i&gt;:&lt;i&gt;Higher Education Press&lt;/i&gt;,2008:10-20.&lt;i&gt;Lindblad T&lt;/i&gt;,&lt;i&gt;Kinser J M&lt;/i&gt;.脉冲耦合神经网络图像处理[&lt;i&gt;M&lt;/i&gt;].马义德,绽琨,王兆滨,等,译.北京:高等教育出版社,2008:10-20.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_11" title=" &lt;i&gt;Ma Y D&lt;/i&gt;,&lt;i&gt;Dai R L&lt;/i&gt;,&lt;i&gt;Li L&lt;/i&gt;.&lt;i&gt;Automated image segmentation using pulse coupled neural networks and image&lt;/i&gt;&#39;&lt;i&gt;s entropy&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Journal of China Institute of Communications&lt;/i&gt;,2002,23(1):46-51.马义德,戴若兰,李廉.一种基于脉冲耦合神经网络和图像熵的自动图像分割方法[&lt;i&gt;J&lt;/i&gt;].通信学报,2002,23(1):46-51." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB200201006&amp;v=MDE2NjVMT2VaZVZ2Rnl2blVidlBNVFhUYkxHNEh0UE1ybzlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         &lt;i&gt;Ma Y D&lt;/i&gt;,&lt;i&gt;Dai R L&lt;/i&gt;,&lt;i&gt;Li L&lt;/i&gt;.&lt;i&gt;Automated image segmentation using pulse coupled neural networks and image&lt;/i&gt;&#39;&lt;i&gt;s entropy&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Journal of China Institute of Communications&lt;/i&gt;,2002,23(1):46-51.马义德,戴若兰,李廉.一种基于脉冲耦合神经网络和图像熵的自动图像分割方法[&lt;i&gt;J&lt;/i&gt;].通信学报,2002,23(1):46-51.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_12" title=" &lt;i&gt;Deng X Y&lt;/i&gt;,&lt;i&gt;Ma Y D&lt;/i&gt;.&lt;i&gt;PCNN model automatic parameters determination and its modified model&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Electronica Sinica&lt;/i&gt;,2012,40(5):955-964.邓翔宇,马义德.&lt;i&gt;PCNN&lt;/i&gt;参数自适应设定及其模型的改进[&lt;i&gt;J&lt;/i&gt;].电子学报,2012,40(5):955-964." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201205015&amp;v=MTUwNjBJVGZUZTdHNEg5UE1xbzlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl2blVidlA=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         &lt;i&gt;Deng X Y&lt;/i&gt;,&lt;i&gt;Ma Y D&lt;/i&gt;.&lt;i&gt;PCNN model automatic parameters determination and its modified model&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Electronica Sinica&lt;/i&gt;,2012,40(5):955-964.邓翔宇,马义德.&lt;i&gt;PCNN&lt;/i&gt;参数自适应设定及其模型的改进[&lt;i&gt;J&lt;/i&gt;].电子学报,2012,40(5):955-964.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_13" title=" &lt;i&gt;Wu Y Q&lt;/i&gt;,&lt;i&gt;Wang Z L&lt;/i&gt;.&lt;i&gt;Infrared and visible image fusion based on target extraction and guided filtering enhancement&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Optica Sinica&lt;/i&gt;,2017,37(8):0810001.吴一全,王志来.基于目标提取与引导滤波增强的红外与可见光图像融合[&lt;i&gt;J&lt;/i&gt;].光学学报,2017,37(8):0810001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201708011&amp;v=MjczNjZHNEg5Yk1wNDlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl2blVidlBJalhUYkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         &lt;i&gt;Wu Y Q&lt;/i&gt;,&lt;i&gt;Wang Z L&lt;/i&gt;.&lt;i&gt;Infrared and visible image fusion based on target extraction and guided filtering enhancement&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Optica Sinica&lt;/i&gt;,2017,37(8):0810001.吴一全,王志来.基于目标提取与引导滤波增强的红外与可见光图像融合[&lt;i&gt;J&lt;/i&gt;].光学学报,2017,37(8):0810001.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_14" title=" &lt;i&gt;Li S T&lt;/i&gt;,&lt;i&gt;Kang X D&lt;/i&gt;,&lt;i&gt;Hu J W&lt;/i&gt;.&lt;i&gt;Image fusion with guided filtering&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Image Processing&lt;/i&gt;,2013,22(7):2864-2875." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image fusion with guided filtering">
                                        <b>[14]</b>
                                         &lt;i&gt;Li S T&lt;/i&gt;,&lt;i&gt;Kang X D&lt;/i&gt;,&lt;i&gt;Hu J W&lt;/i&gt;.&lt;i&gt;Image fusion with guided filtering&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Image Processing&lt;/i&gt;,2013,22(7):2864-2875.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_15" title=" &lt;i&gt;Zhang Y&lt;/i&gt;,&lt;i&gt;Zhang L J&lt;/i&gt;,&lt;i&gt;Bai X Z&lt;/i&gt;,et al.&lt;i&gt;Infrared and visual image fusion through infrared feature extraction and visual information preservation&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Infrared Physics&lt;/i&gt; &amp;amp; &lt;i&gt;Technology&lt;/i&gt;,2017,83:227-237." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES9430CCB0989C35D95CF68177E5B5C3D8&amp;v=MTY0NDVmT0dRbGZDcGJRMzVkbGh4N3E5eEs4PU5pZk9mYnE4SGRHLzNQMUZiZU1HZjM4OHV4OFdtVWw3UUg3bHEyY3dDN2ZuUnM2WENPTnZGU2lXV3I3SklGcG1hQnVIWQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         &lt;i&gt;Zhang Y&lt;/i&gt;,&lt;i&gt;Zhang L J&lt;/i&gt;,&lt;i&gt;Bai X Z&lt;/i&gt;,et al.&lt;i&gt;Infrared and visual image fusion through infrared feature extraction and visual information preservation&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Infrared Physics&lt;/i&gt; &amp;amp; &lt;i&gt;Technology&lt;/i&gt;,2017,83:227-237.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_16" title=" &lt;i&gt;Chen M S&lt;/i&gt;.&lt;i&gt;Image fusion of visual and infrared image based on NSCT and compressed sensing&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Journal of Image and Graphics&lt;/i&gt;,2016,21(1):39-44.陈木生.结合&lt;i&gt;NSCT&lt;/i&gt;和压缩感知的红外与可见光图像融合[&lt;i&gt;J&lt;/i&gt;].中国图象图形学报,2016,21(1):39-44." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201601005&amp;v=MjcwMDc0SDlmTXJvOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXZuVWJ2UFB5cmZiTEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         &lt;i&gt;Chen M S&lt;/i&gt;.&lt;i&gt;Image fusion of visual and infrared image based on NSCT and compressed sensing&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Journal of Image and Graphics&lt;/i&gt;,2016,21(1):39-44.陈木生.结合&lt;i&gt;NSCT&lt;/i&gt;和压缩感知的红外与可见光图像融合[&lt;i&gt;J&lt;/i&gt;].中国图象图形学报,2016,21(1):39-44.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(11),132-139 DOI:10.3788/AOS201939.1110003            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>结合脉冲耦合神经网络与引导滤波的红外与可见光图像融合</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E5%93%93%E7%8E%B2&amp;code=38866029&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周哓玲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%9F%E6%B3%BD%E6%B6%9B&amp;code=31495590&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江泽涛</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%A1%82%E6%9E%97%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E5%B9%BF%E8%A5%BF%E5%9B%BE%E5%83%8F%E5%9B%BE%E5%BD%A2%E5%A4%84%E7%90%86%E4%B8%8E%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0269119&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">桂林电子科技大学广西图像图形处理与智能处理重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%A1%82%E6%9E%97%E8%88%AA%E5%A4%A9%E5%B7%A5%E4%B8%9A%E5%AD%A6%E9%99%A2%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=1698417&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">桂林航天工业学院电子信息与自动化学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对红外与可见光图像融合细节信息不够丰富、易出现伪影等问题,提出了一种结合脉冲耦合神经网络(PCNN)与引导滤波的红外与可见光图像融合方法。改进传统PCNN模型结构,在脉冲产生单元加入抑制项,避免像素重复点火对点火时间矩阵带来噪声;以原图为引导图像对点火时间矩阵<i><b>T</b></i>进行引导滤波,得到兼具显著信息与边缘细节信息的多区域加权划分矩阵;基于该多区域加权划分矩阵,对红外与可见光图像进行加权融合。同时,根据PCNN数学模型点火行为分析,提出了一种包含约束的PCNN模型参数设置方法,可降低PCNN模型参数设置的复杂度。实验结果表明该融合方法具有较高的融合效率,同时融合图像细节信息丰富,无明显伪影,交叉熵、空间频率指标相对于当前常用融合方法均较优。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%84%89%E5%86%B2%E8%80%A6%E5%90%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">脉冲耦合神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%82%B9%E7%81%AB%E6%97%B6%E9%97%B4%E7%9F%A9%E9%98%B5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">点火时间矩阵;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%8C%BA%E5%9F%9F%E5%88%92%E5%88%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多区域划分;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%95%E5%AF%BC%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">引导滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%8A%A0%E6%9D%83%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像加权融合;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    江泽涛,E-mail:zetaojiang@126.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-06-17</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61876049,61762066,61572147);</span>
                                <span>广西科技计划项目(AC16380108);</span>
                                <span>广西图像图形智能处理重点实验项目(GIIP201701、GIIP201801、GIIP201802、GIIP201803);</span>
                                <span>广西研究生教育创新计划(YCBZ2018052,2019YCXS043);</span>
                    </p>
            </div>
                    <h1><b>Infrared and Visible Image Fusion Combining Pulse-Coupled Neural Network and Guided Filtering</b></h1>
                    <h2>
                    <span>Zhou XiaoLing</span>
                    <span>Jiang Zetao</span>
            </h2>
                    <h2>
                    <span>Guangxi Key Laboratory of Image and Graphic Intelligent Processing, Guilin University of Electronic Technology</span>
                    <span>College of Electronic Information and Automation, Guilin University of Aerospace Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>This study proposes a novel fusion method that combines a pulse-coupled neural network(PCNN) and guided filtering to solve the issues of lacking details and virtual shadows in infrared and visible images. First, to eliminate the additional noise caused by the multi-impulse of a pixel in firing time matrix <i><b>T</b></i>, the traditional PCNN model is improved by simplifying its structure and adding restraint items into the pulse generating unit. Second, using original images as input, guided filtering is utilized to improve <i><b>T</b></i> with more edge details and salient information. Finally, based on the modified <i><b>T</b></i>, the weight fusion rule is adopted to obtain the fusion image. Following the firing mechanism analysis of the PCNN model, a new parameter setting method combining constraints is proposed to reduce the model′s parameter setting complexity. Experimental results show that the proposed method provides efficient, satisfactory, and well-detailed fusion results, and obvious virtual shadows scarcely appear in the fusion image. Additionally, the cross entropy and space frequency indexes of the results are superior to those of other current fusion methods.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=pulse-coupled%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">pulse-coupled neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=firing%20time%20matrix&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">firing time matrix;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-regional%20division&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-regional division;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=guided%20filtering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">guided filtering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20weight%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image weight fusion;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-06-17</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="46" name="46" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="47">红外与可见光图像融合的目的是提取二者的优势信息,得到具有良好细节信息和目标显著性的融合图像。图像融合算法可分为两大类:基于变换域的融合和基于空间域的融合<citation id="107" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。基于变换域的融合方法,近年来其研究热点在于改善传统多尺度变换方法[如Laplacian Pyramid (LP)、Principal Component Analysis (PCA)、Wavelet Transform (WT)及Contourlet变换等]中存在的特征提取效率不高、信息冗余等问题,主要有Nonsubsampled Contourlet Transform (NSCT)变换<citation id="108" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、Nonsubsampled Shearlet Transform (NSST)变换<citation id="109" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、Nonsubsampled Shearlet Contrast Transform (NSSCT)变换<citation id="110" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、非下采样双树复轮廓波变换<citation id="113" type="reference"><link href="22" rel="bibliography" /><link href="24" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>等。因多尺度、多方向的分解及逆变换比较耗时,变换域的融合策略普遍存在融合效率不高的问题,基于空间域的融合策略由于直接在像素上进行操作,其执行效率具有一定的优势。基于空间域的融合方法,其中一个研究热点为基于区域的图像融合,融合的关键在于区域划分方法的选取及融合规则的制定<citation id="114" type="reference"><link href="26" rel="bibliography" /><link href="28" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>。考虑到红外和可见光图像目标区域与背景区域具有显著不同的特性,Zhang等<citation id="111" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>基于超像素显著度将原图像划分为目标区域和背景区域,其中目标区域取红外图像灰度值,背景区域经NSST变换,通过对比梯度信息进行系数加权融合;Lu等<citation id="112" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>根据区域统计特性对原图像进行区域划分,在目标区域保留红外图像信息,在背景区域采用核奇异值分解进行融合。上述基于区域的融合方法,将原图划分为背景和目标区域,分别采用不同的融合规则实现图像融合,更好地保留了红外目标区域信息及可见光背景细节。但是,由于其仅仅进行二值区域分割,容易丢失部分细节信息,且直接基于区域划分图进行融合,容易出现伪影,而这个问题也正是基于区域的融合方法普遍存在的。针对这个问题,本文将提出一种基于多区域分割的图像融合方法,并结合引导滤波改善伪影问题。</p>
                </div>
                <div class="p1">
                    <p id="48">图像经脉冲耦合神经网络(PCNN)处理得到的点火时间矩阵,可视为从空间纹理分布以及亮度分布上对图像的多区域划分,满足红外与可见光图像融合过程中充分保留红外目标亮度信息和可见光背景纹理特性的要求。本文提出一种结合PCNN与引导滤波的快速融合方法,解决了基于区域的图像融合方法所获得图像细节不够丰富、易出现伪影的问题。为提高算法执行效率,简化PCNN模型,并在脉冲产生单元加入抑制项,抑制已灭火像素的再次激活,同时采用信息熵判定准则及时停止迭代,在确保获得的点火时间矩阵<i><b>T</b></i>具有较丰富的多区域划分特性的同时,减少迭代次数,提高融合算法效率;红外与可见光图像分别经PCNN处理得到点火时间矩阵(<i><b>T</b></i><sub>IR</sub>)和原始的可反映纹理特性的多区域点火时间矩阵(<i><b>T</b></i><sub>VIS</sub>),为避免融合图像存在伪影,利用引导滤波修正<i><b>T</b></i><sub>IR</sub>和<i><b>T</b></i><sub>VIS</sub>得到兼具细节信息的多区域权重矩阵<i><b>G</b></i><sub>IR</sub>和<i><b>G</b></i><sub>VIS</sub>,其中所用的引导滤波以原图为引导图像,点火时间矩阵为输入图像;基于多区域权重矩阵<i><b>G</b></i><sub>IR</sub>和<i><b>G</b></i><sub>VIS</sub>,对红外与可见光图像进行加权融合。得到的融合图像细节丰富,且各区域之间灰度值连贯,无明显伪影。由于整个算法进行过程中所需的迭代次数较少,大部分为基于像素的直接操作,因此,该算法具有较高的执行效率,且所获得的融合图像具有较好的视觉效果。</p>
                </div>
                <h3 id="49" name="49" class="anchor-tag">2 PCNN模型的改进</h3>
                <div class="p1">
                    <p id="50">PCNN是由多个神经元以特定的结构互相连接的反馈型单层网络<citation id="115" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,具有三个功能单元:反馈输入域、连接输入域及脉冲产生域。该模型结构充分模拟了哺乳动物视觉系统对图像亮度响应的衰减特性以及脉冲同步发放特性。</p>
                </div>
                <div class="p1">
                    <p id="51">在保证能够具备模拟哺乳动物视觉特性的基础上,可省去反馈域的耦合输入部分,同时,为了避免已灭火像素的再次激活干扰点火矩阵的区域划分效果,修正其脉冲产生过程,增加了抑制项:即将上一次脉冲状态与本次脉冲产生单元进行调制。图1为改进的PCNN神经元结构示意图,具体数学模型可以用迭代差分方程表示</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>F</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mi>exp</mi><mo stretchy="false">(</mo><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">)</mo><mi>L</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mtext> </mtext><mtext> </mtext></mtd></mtr><mtr><mtd><mtext> </mtext><mi>V</mi><msub><mrow></mrow><mi>L</mi></msub><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mi>l</mi></mrow></munder><mi>W</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi><mi>k</mi><mi>l</mi></mrow></msub><mi>Y</mi><msub><mrow></mrow><mrow><mi>k</mi><mi>l</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>U</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">[</mo><mn>1</mn><mo>+</mo><mi>β</mi><mi>L</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>θ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mi>exp</mi><mo stretchy="false">(</mo><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>θ</mi></msub><mo stretchy="false">)</mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mi>V</mi><msub><mrow></mrow><mi>θ</mi></msub><mi>Y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>Y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mi>ε</mi><mo stretchy="false">[</mo><mi>U</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo>-</mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>Τ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Τ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mi>Y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="53">式中:<i>ε</i>表示阶跃函数;<i>U</i><sub><i>ij</i></sub>为内部活动项,<i>θ</i><sub><i>ij</i></sub>为阈值,下标<i>ij</i>表示当前PCNN神经元的位置,应用到图像处理中表示的是像素点位于第<i>i</i>行<i>j</i>列,内部活动项<i>U</i><sub><i>ij</i></sub>&lt;<i>θ</i><sub><i>ij</i></sub>(灭火条件),该函数输出值为0,否则为1;下标<i>kl</i>则表示邻域神经元的位置,在图像处理中对应为第<i>k</i>行<i>l</i>列像素点,且<i>kl</i>不与<i>ij</i>重合;<i><b>F</b></i>是神经元的反馈输入矩阵;<i><b>S</b></i>为归一化后的像素灰度值;<i>L</i>是连接输入域的输出,<i>L</i><sub><i>ij</i></sub>表示第<i>i</i>行<i>j</i>列像素点受周围像素点激活状态耦合影响;<i><b>W</b></i>为耦合连接权重矩阵;<i>U</i>表示内部活动项,由反馈输出和连接输入域调制得到;<i>α</i><sub><i>L</i></sub>和<i>V</i><sub><i>L</i></sub>,<i>α</i><sub><i>θ</i></sub>和<i>V</i><sub><i>θ</i></sub>分别表示连接输入域和脉冲产生单元阈值的迭代衰减时间常数和连接权重放大系数;<i>θ</i>为阈值输出;<i>β</i>为内部活动项的连接系数;<i><b>Y</b></i>表示脉冲输出矩阵;<i><b>T</b></i>表示点火时间矩阵;<i>n</i>为迭代次数,常取较大值,带来较大的计算开销。为了提高算法执行效率,本研究采用文献<citation id="116" type="reference">[<a class="sup">11</a>]</citation>中提出的基于信息熵最大的迭代停止准则。当迭代输出已经达到最大信息熵分割效果时,仍保持点火部分应该是红外或可见光图像的优势信息,无需再继续迭代更新点火时间矩阵。这是因为迭代会增加计算消耗成本,且对后续基于点火时间矩阵的融合图像质量的提升效果不大。(5)式为增加的抑制项。</p>
                </div>
                <div class="p1">
                    <p id="54">图1中linking area耦合连接部分反映的是邻域像素在空间上对当前像素激活状态的影响。Generate pulse脉冲产生单元则是产生点火输出的部分,利用内部活动项的调制,可使具有相近灰度值的区域同步点火。</p>
                </div>
                <div class="area_img" id="55">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911016_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 改进的PCNN神经元结构" src="Detail/GetImg?filename=images/GXXB201911016_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 改进的PCNN神经元结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911016_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Neuronal construction of the improved PCNN</p>

                </div>
                <div class="p1">
                    <p id="56">传统PCNN模型在step function模块获得的输出即为最终输出,随着模型迭代的进行,灭火的像素会周期地再次点火,且不同像素会存在点火周期混叠的情况,从而导致点火时间矩阵出现噪声和振荡。为了解决传统PCNN模型存在的上述问题,本研究在step function模块后面加入了点火抑制项(即图1中的红色框部分),采用上一次迭代的点火输出对当前脉冲输出进行调制,可抑制已灭火像素的再次点火,从而避免因像素的再次点火激活给PCNN输出的区域划分图像带来噪声。</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag">3 PCNN参数设置</h3>
                <div class="p1">
                    <p id="58">图1中改进PCNN模型需设置的参数包括<i><b>W</b></i>、<i>α</i><sub><i>L</i></sub>、<i>V</i><sub><i>L</i></sub>、<i>α</i><sub><i>θ</i></sub>、<i>V</i><sub><i>θ</i></sub>以及<i>β</i>。参数的合理设置直接影响同步脉冲发放效果,进而影响所得区域的分割效果。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59"><b>3.1 耦合连接矩阵</b><i><b>W</b></i><b>的设置</b></h4>
                <div class="p1">
                    <p id="60"><i><b>W</b></i>反映了邻域像素灰度值对本神经元的影响,距离越远其影响应越小,本研究选取3×3的窗口计算邻域耦合影响,权重结构采用欧氏距离的倒数,最终得到的耦合权重矩阵为</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>0</mn><mo>.</mo><mn>7</mn><mn>0</mn><mn>7</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>7</mn><mn>0</mn><mn>7</mn></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>.</mo><mn>7</mn><mn>0</mn><mn>7</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>7</mn><mn>0</mn><mn>7</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">从(7)式可以看出,中心位置权重为零,周围邻域的权值系数为该处位置与中心点位置的欧氏距离的倒数,距离越远其灰度值对中心像素点火状态的影响越小。</p>
                </div>
                <h4 class="anchor-tag" id="63" name="63"><b>3.2 参数</b><i>α</i><sub><i><b>L</b></i></sub>、<i><b>V</b></i><sub><i><b>L</b></i></sub>、<i>α</i><sub><i>θ</i></sub>、<i><b>V</b></i><sub><i>θ</i></sub><b>以及</b><i><b>β</b></i><b>的设置</b></h4>
                <div class="p1">
                    <p id="64">PCNN不同于常规神经网络及深度学习网络,不需要根据大量样本进行训练,其模型参数可以直接设定,故参数设置可直接决定所获得的点火时间矩阵分割效果。虽然本文对模型进行了简化,但是仍涉及到5个关键参数的设置,且各参数之间存在较强的耦合关系。当前PCNN参数设置分为三大类:基于经验设置、采用优化算法寻优、基于机理建模分析。其中,优化算法寻优容易出现不收敛、过拟合等问题,稳健性不佳;基于机理建模分析尚不成熟,适用性不高;基于经验的设置方法基本可以满足需求,即通过多次实验调整参数设置。但是PCNN模型待设置的参数过多,不同参数之间存在耦合影响,使得基于经验设置过程比较麻烦。因此,本文根据模型点火过程分析,为基于经验设置增加约束,从而减少需要设定参数的数量,降低设置参数的复杂度。</p>
                </div>
                <div class="p1">
                    <p id="65">参数<i>α</i><sub><i>L</i></sub>和<i>α</i><sub><i>θ</i></sub>反映的是耦合连接域和阈值的衰减速度,二者取值比较接近时可获得较好发放特性,为了降低参数设置复杂度,令二者取值一致,即取<i>α</i><sub><i>L</i></sub>=<i>α</i><sub><i>θ</i></sub>。根据文献<citation id="117" type="reference">[<a class="sup">12</a>]</citation>中对PCNN参数的机理分析可知,其值越小,同步脉冲发放过程越细腻,但是发放速度也越低,当数量级设定在10<sup>-3</sup>时,在发放速度和图像纹理分割细腻度上能折中并获得较满意效果。本文取<i>α</i><sub><i>L</i></sub>=<i>α</i><sub><i>θ</i></sub>=0.005,可在保证PCNN输出图像序列具有较好纹理分割特性的基础上,PCNN脉冲发放速度仍较快。</p>
                </div>
                <div class="p1">
                    <p id="66"><i>V</i><sub><i>L</i></sub>的作用是放大邻域耦合的影响,而(3)式中参数<i>β</i>也有类似作用,因此,<i>V</i><sub><i>L</i></sub>参数的设定意义并不大,故而可令<i>V</i><sub><i>L</i></sub>=1,使得邻域的耦合作用主要反映在耦合权重矩阵上。</p>
                </div>
                <div class="p1">
                    <p id="67"><i>V</i><sub><i>θ</i></sub>表示像素首次点火,阈值<i>θ</i>的抬升情况直接影响后续迭代过程像素点火情况。此外,根据(5)式可知,脉冲是通过比较内部活动项输出<i>U</i>和阈值<i>θ</i>的差异而产生的,<i>V</i><sub><i>θ</i></sub>的取值与<i>β</i>参数耦合性较强。因未考虑二者的约束关系,直接通过实验调参比较麻烦。为了避免上述情况,下面将通过分析(1)～(5)式所示的第1次迭代情况,寻找<i>V</i><sub><i>θ</i></sub>和<i>β</i>的关系。</p>
                </div>
                <div class="p1">
                    <p id="68">本文基于PCNN模型进行图像处理,是从像素点灭火角度出发考虑。因此,在设定模型初始条件时,假定图像最原始的状态是全部点火的,即脉冲输出矩阵<i><b>Y</b></i>的元素均为1,即</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn><mtable><mtr><mtd><mo>,</mo></mtd><mtd><mi>Y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn></mtd></mtr></mtable><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">第1次迭代时,相关变量状态为</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mo>=</mo><mi>exp</mi><mo stretchy="false">(</mo><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">)</mo><mi>L</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mi>l</mi></mrow></munder><mi>W</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi><mi>k</mi><mi>l</mi></mrow></msub><mi>Y</mi><msub><mrow></mrow><mrow><mi>k</mi><mi>l</mi></mrow></msub><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mn>6</mn><mo>.</mo><mn>8</mn><mn>2</mn><mn>8</mn><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>U</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mo>=</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>×</mo><mo stretchy="false">[</mo><mn>1</mn><mo>+</mo><mi>β</mi><mi>L</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>=</mo></mtd></mtr><mtr><mtd><mi>S</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>×</mo><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mn>6</mn><mo>.</mo><mn>8</mn><mn>2</mn><mn>8</mn><mo>×</mo><mi>β</mi><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>θ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mo>=</mo><mi>exp</mi><mo stretchy="false">(</mo><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>θ</mi></msub><mo stretchy="false">)</mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>+</mo><mi>V</mi><msub><mrow></mrow><mi>θ</mi></msub><mi>Y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mi>V</mi><msub><mrow></mrow><mi>θ</mi></msub><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">由(10)式和(11)式及像素灭火条件<i>U</i><sub><i>ij</i></sub>&lt;<i>θ</i><sub><i>ij</i></sub>可知,<i>S</i><sub><i>ij</i></sub>为客观存在的像素灰度,<i>β</i>和<i>V</i><sub><i>θ</i></sub>的合理设置,决定了低于某一灰度<i>S</i><sub>min</sub>的像素将在第二次迭代中灭火。输入为<i>S</i><sub>min</sub>的神经元应处于点火临界状态,即<i>β</i>和<i>V</i><sub><i>θ</i></sub>应满足</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>U</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo><mo>=</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>×</mo><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mn>6</mn><mo>.</mo><mn>8</mn><mn>2</mn><mn>8</mn><mi>β</mi><mo stretchy="false">)</mo><mo>=</mo><mi>V</mi><msub><mrow></mrow><mi>θ</mi></msub><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">图像经PCNN获得的脉冲输出矩阵序列,第一次迭代时灭火像素可看成是种子点,后续迭代可看作是基于种子点的灭火过程的扩散。由于本研究采用PCNN的目的是获得可反映纹理特性的点火时间矩阵输出,其脉冲发放过程不能过于粗糙。因此,第一次迭代时灭火像素不可过多,以保证点火时间矩阵后续迭代的有效性,即像素灰度值较暗的区域仍进行一定程度的纹理分割,本研究设定<i>S</i><sub>min</sub>=0.2,即限定灰度值低于最高亮度值20%左右的像素点在第一次迭代时灭火。因此,进一步细化得到<i>V</i><sub><i>θ</i></sub>和<i>β</i>的约束关系:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>0</mn><mo>.</mo><mn>2</mn><mo>×</mo><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mn>6</mn><mo>.</mo><mn>8</mn><mn>2</mn><mn>8</mn><mi>β</mi><mo stretchy="false">)</mo><mo>=</mo><mi>V</mi><msub><mrow></mrow><mi>θ</mi></msub><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">(13)式即为本文所增加的约束。可充分考虑<i>V</i><sub><i>θ</i></sub>和<i>β</i>的耦合关系,只需设置<i>β</i>即可得到最合适的<i>V</i><sub><i>θ</i></sub>。经多次红外与可见光图像融合实验发现,当取<i>β</i>=0.01时,根据(13)式可计算出<i>V</i><sub><i>θ</i></sub>=0.21,经过5次迭代后停止点火,所获得的点火时间矩阵的多区域分割效果较为细腻,可以满足融合需求。该参数设置方法中,<i>β</i>的取值若应用到其他图像处理场合,则需要针对不同图像处理要求进行实验设定,在0～1之间进行取值。</p>
                </div>
                <h3 id="77" name="77" class="anchor-tag">4 融合框架</h3>
                <div class="p1">
                    <p id="78">考虑PCNN输出的点火时间矩阵具有多区域分割效果,基于点火时间矩阵进行直接加权融合会带来严重伪影效应。因此,引入引导滤波对点火时间矩阵<i><b>T</b></i>进行修正,再基于修正的<i><b>T</b></i>实现红外与可见光图像的加权融合。</p>
                </div>
                <div class="p1">
                    <p id="79">引导滤波是一种具有保留边缘特性的滤波器,易于实现且执行速度快<citation id="118" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。假定输入图像表示为<i><b>P</b></i>,引导图像表示为<i><b>I</b></i>,输出图像表示为<i><b>O</b></i>,则引导滤波通过局部线性变换实现滤波处理。局部线性变换公式可表示为</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mi>α</mi><msub><mrow></mrow><mi>m</mi></msub><mi>Ι</mi><msub><mrow></mrow><mi>n</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>m</mi></msub><mo>,</mo><mo>∀</mo><mi>n</mi><mo>∈</mo><mi>ω</mi><msub><mrow></mrow><mi>m</mi></msub><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">式中:<i>ω</i><sub><i>m</i></sub>表示以像素<i>m</i>为中心的局部窗口,窗口半径为<i>r</i>;<i>a</i><sub><i>m</i></sub>和<i>b</i><sub><i>m</i></sub>是局部线性变换系数;<i>I</i><sub><i>n</i></sub>表示局部窗口内位置为<i>n</i>的像素点对应的灰度值;<i>O</i><sub><i>n</i></sub>表示位置<i>n</i>对应的像素点线性变换输出。通过最小化代价函数<i>E</i>寻优求解</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><mo>,</mo><mi>b</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>n</mi><mo>∈</mo><mi>ω</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></munder><mo stretchy="false">[</mo></mstyle><mo stretchy="false">(</mo><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><mi>Ι</mi><msub><mrow></mrow><mi>n</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi>Ρ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mi>a</mi><msubsup><mrow></mrow><mi>k</mi><mn>2</mn></msubsup><mo stretchy="false">]</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">式中:<i>λ</i>为正则化系数<citation id="119" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>;<i>E</i>为代价函数,在保证线性参数不过大的前提下,使得基于引导图像获得的输出图像与输入图像之间的差异尽可能减小。</p>
                </div>
                <div class="p1">
                    <p id="84">所提出的融合框架如图2所示。图2中<i><b>T</b></i><sub>IR</sub>和<i><b>T</b></i><sub>VIS</sub>为原始的点火时间矩阵,通过不同颜色标记可以看出,该多区域分割具有较好的纹理分割特性。经GFF处理后得到修正多区域权重矩阵<i><b>G</b></i><sub>IR</sub>和<i><b>G</b></i><sub>VIS</sub>,不同区域之间过渡更为平滑,基于该权重矩阵进行原红外图像(<i><b>H</b></i><sub>VIS</sub>)和可见光图像(<i><b>H</b></i><sub>VIS</sub>)加权融合,得到的融合图像<i><b>I</b></i><sub>fuse</sub>可较好地保留原红外与可见光图像中的优势细节信息,且基本不存在明显的伪影。</p>
                </div>
                <div class="p1">
                    <p id="85">融合步骤如下所述:</p>
                </div>
                <div class="p1">
                    <p id="86">1) 将已配准的红外图像(<i><b>H</b></i><sub>IR</sub>)与可见光图像(<i><b>H</b></i><sub>VIS</sub>)输入到改进的PCNN网络中,分别得到原始点火矩阵<i><b>T</b></i><sub>IR</sub>和<i><b>T</b></i><sub>VIS</sub>,为了更好地符合人眼视觉系统对暗区域像素变化值更为敏感的特点,对得到的<i><b>T</b></i><sub>IR</sub>和<i><b>T</b></i><sub>VIS</sub>分别取对数,拉伸暗区域(即点火矩阵中元素值较小区域);</p>
                </div>
                <div class="p1">
                    <p id="87">2) <i><b>T</b></i><sub>IR</sub>和<i><b>T</b></i><sub>VIS</sub>分别以各自对应的原图为引导图像进行引导滤波,设定引导滤波的窗口半径<i>r</i>=3,正则化系数<i>λ</i>=0.001,获得修正的多区域权重矩阵<i><b>G</b></i><sub>IR</sub>和<i><b>G</b></i><sub>VIS</sub>;这里采用引导滤波修正PCNN输出的点火时间矩阵,目的是获得兼具显著信息与边缘细节信息的多区域加权矩阵,基于该矩阵对红外与可见光图像进行加权融合,可更好地提高融合图像质量。</p>
                </div>
                <div class="p1">
                    <p id="88">3) 基于<i><b>G</b></i><sub>IR</sub>和<i><b>G</b></i><sub>VIS</sub>对红外和可见光图像进行加权平均,得到融合图像,并对其进行线性变换增强对比度,以获得更好的视觉效果。加权融合规则的表达式为</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>e</mtext></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">G</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>R</mtext></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>R</mtext></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">G</mi><msub><mrow></mrow><mrow><mtext>V</mtext><mtext>Ι</mtext><mtext>S</mtext></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mtext>V</mtext><mtext>Ι</mtext><mtext>S</mtext></mrow></msub><mo stretchy="false">)</mo><mo>/</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">G</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>R</mtext></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">G</mi><msub><mrow></mrow><mrow><mtext>V</mtext><mtext>Ι</mtext><mtext>S</mtext></mrow></msub><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">式中:·表示点乘运算。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911016_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 本文融合算法框架" src="Detail/GetImg?filename=images/GXXB201911016_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 本文融合算法框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911016_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Flow chart of the proposed fusion method</i></p>

                </div>
                <h3 id="92" name="92" class="anchor-tag">5 实验结果及分析</h3>
                <h4 class="anchor-tag" id="93" name="93"><b>5.1 实验相关设置</b></h4>
                <div class="p1">
                    <p id="94">为了验证所提出模型和算法的有效性,本文方法在<i>Intel Core i</i>5-4300<i>U CPU</i>, 内存为 4 <i>GB</i> 的计算机上采用<i>MATLAB</i> 2016<i>a</i>编程实现,选取4组典型红外与可见光图像(图3)进行融合实验,并与其他3种融合算法结果进行比较:基于引导滤波<i>Guided Filter Filtering</i>(<i>GFF</i>)的融合方法<citation id="120" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、基于特征提取的融合方法(本文简称为<i>FE</i>)<citation id="121" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>以及<i>NSCT</i>变换与<i>PCNN</i>的融合方法(简称<i>NSCT</i>_<i>PCNN</i>)<citation id="122" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。其中,对比方法的参数设置均与原文献保持一致。</p>
                </div>
                <div class="p1">
                    <p id="95">为了评价融合效果,本文将从主观视觉效果和客观评价指标两方面进行讨论。选用的客观评价指标为交叉熵(<i>Cerf</i>)、空间频率(<i>SF</i>)和信息熵(<i>sh</i>)。交叉熵越小,表明融合图像与原图像的差异越小,融合效果越好;空间频率越大,表明图像清晰度越高;信息熵越大,表明融合图像包含信息越丰富。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96"><b>5.2 实验结果分析</b></h4>
                <div class="p1">
                    <p id="97">4组红外与可见光图像的融合结果如图4所示。图4(<i>a</i>1)～(<i>a</i>4)为基于引导滤波<i>GFF</i>的融合结果,可以看出融合图像细节较为丰富,但是<i>dune</i>图像中红框部分看不到可见光图像中的道路,<i>kayak</i>图像中红框部分噪声过大,<i>road</i>融合结果中部分灯光信息丢失;图4(<i>b</i>1)～(<i>b</i>4)为基于特征提取方法的融合结果,可以看出可见光信息和红外目标得到了很好的保留,例如图4(<i>b</i>1)、(<i>b</i>2)的红框部分,但是<i>road</i>这类目标场景较丰富的融合情况,会弱化部分信息,如汽车轮廓、坐着的行人轮廓等;图4(<i>c</i>1)～(<i>c</i>4)为<i>NSCT</i>_<i>PCNN</i>的融合结果,图4(<i>c</i>1)、(<i>c</i>2)融合图像细节信息较为丰富,图4(<i>c</i>3)、(<i>c</i>4)在目标周边出现了伪影;图4(<i>d</i>1)～(<i>d</i>4)为本文所采用结合<i>PCNN</i>与<i>GFF</i>的融合结果,可以看出本文方法融合图像细节较为丰富,<i>camp</i>图像的栅栏和树木、<i>dune</i>图像的路面、<i>kayak</i>图像的路人以及<i>road</i>图像的建筑物、汽车轮廓灯都清晰可见。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911016_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 4组原始红外与可见光图像。" src="Detail/GetImg?filename=images/GXXB201911016_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 4组原始红外与可见光图像。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911016_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Four groups of the original infrared and visible images</i>. </p>
                                <p class="img_note">(a) Camp红外图像;(b) camp可见光图像;(c) dune红外图像;(d) dune可见光图像;
(e) kayak红外图像;(f) kayak可见光图像;(g) road红外图像;(h) road可见光图像</p>
                                <p class="img_note">(a) Camp infrared image; (b) camp visible image; (c) dune infrared image; (d) dune visible image; (e) kayak infrared image; (f) kayak visible image; (g) road infrared image; (h) road visible image</p>

                </div>
                <div class="p1">
                    <p id="99">为了客观评价不同算法的融合结果,选取交叉熵、空间频率和信息熵三个指标进行对比分析,具体评价结果如表1所示,表中黑体标注为最优指标。从表1可以看出,在4组红外与可见光图像的融合结果对比分析中,本文方法的交叉熵<i>Cerf</i>、空间频率<i>SF</i>两项指标均优于其他3种对比方法,信息熵指标<i>sh</i>部分占优,其中<i>dune</i>和<i>kayak</i>图像融合结果稍微处于劣势,但是相差也不大。主观和客观评价结果表明本文融合方法获得的融合图像,具有较丰富的细节信息,融合效果较好。</p>
                </div>
                <div class="area_img" id="100">
                    <p class="img_tit">表1 不同算法融合结果的客观评价指标 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Table</i> 1 <i>Objective evaluation indexes of different fusion methods</i></p>
                    <p class="img_note"></p>
                    <table id="100" border="1"><tr><td rowspan="2"><br /><i>Image groups</i></td><td rowspan="2"><i>Evaluation index</i></td><td colspan="4"><br /><i>Value</i></td></tr><tr><td><i>GFF</i></td><td><i>FE</i></td><td><i>NSCT</i>_<i>PCNN</i></td><td><i>PCNN</i>_<i>GFF</i> (<i>Proposed</i>)</td></tr><tr><td rowspan="3"><br /><i>Camp</i></td><td><i>Cerf</i></td><td>0.60</td><td>1.14</td><td>0.78</td><td><b>0.13</b></td></tr><tr><td><br /><i>SF</i></td><td>21.30</td><td>24.32</td><td>25.02</td><td><b>37.91</b></td></tr><tr><td><br /><i>sh</i></td><td>6.44</td><td>6.75</td><td>6.85</td><td><b>6.93</b></td></tr><tr><td rowspan="3"><br /><i>Dune</i></td><td><i>Cerf</i></td><td>0.26</td><td>0.59</td><td>0.24</td><td><b>0.10</b></td></tr><tr><td><br /><i>SF</i></td><td>9.55</td><td>15.01</td><td>11.56</td><td><b>30.79</b></td></tr><tr><td><br /><i>sh</i></td><td>5.79</td><td><b>6.36</b></td><td>6.01</td><td>5.90</td></tr><tr><td rowspan="3"><br /><i>Kayak</i></td><td><i>Cerf</i></td><td>3.89</td><td>0.47</td><td>0.59</td><td><b>0.56</b></td></tr><tr><td><br /><i>SF</i></td><td>15.4572</td><td>16.42</td><td>15.59</td><td><b>25.01</b></td></tr><tr><td><br /><i>sh</i></td><td>6.01</td><td><b>6.72</b></td><td>6.59</td><td>6.41</td></tr><tr><td rowspan="3"><br /><i>Road</i></td><td><i>Cerf</i></td><td>1.22</td><td>1.35</td><td>1.26</td><td><b>0.10</b></td></tr><tr><td><br /><i>SF</i></td><td>33.97</td><td>37.03</td><td>36.01</td><td><b>42.74</b></td></tr><tr><td><br /><i>sh</i></td><td>7.2</td><td>7.43</td><td>7.33</td><td><b>7.52</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911016_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同算法融合结果对比。" src="Detail/GetImg?filename=images/GXXB201911016_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同算法融合结果对比。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911016_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Fusion results of different fusion methods</i>. </p>
                                <p class="img_note">(a1)～(a4)引导滤波(GFF)的融合结果;(b1)～(b4)基于特征提取(FE)的融合结果;
(c1)～(c4) NSCT_PCNN的融合结果;(d1)～(d4)本文PCNN_GFF的融合结果</p>
                                <p class="img_note">(a1)-(a4) Fusion results of GFF; (b1)-(b4) fusion results of EF; 
(c1)-(c4) fusion results of NSCT_PCNN; (d1)-(d4) fusion results of the proposed PCNN_GFF</p>

                </div>
                <div class="p1">
                    <p id="102">为了进一步验证本文算法的快速性,在同一硬件平台上运行上述融合算法,列出其所需的执行时间,如表2所示。可以看出,文献<citation id="123" type="reference">[<a class="sup">16</a>]</citation>所选用的<i>NSCT</i>_<i>PCNN</i>融合方法所耗时间最长,因为<i>NSCT</i>分解计算量较大;基于引导滤波<i>GFF</i>的融合、特征提取融合及本文的结合<i>PCNN</i>与引导滤波的融合方法的运行速度在同一个级别,但是本文的融合图像细节更为丰富,进一步验证了本文所提出融合算法可在保证融合图像质量的基础上提高融合效率。</p>
                </div>
                <div class="area_img" id="103">
                    <p class="img_tit">表2 不同融合算法平均运行时间对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Table</i> 2 <i>Average running time of different</i><i>fusion methods</i></p>
                    <p class="img_note"></p>
                    <table id="103" border="1"><tr><td><i>Method</i><br /><i>value</i></td><td><i>GFF</i></td><td><i>FE</i></td><td><i>NSCT</i>_<i>PCNN</i></td><td><i>PCNN</i>_<i>GFF</i><br />(<i>proposed</i>)</td></tr><tr><td><br /><i>Time</i> /<i>s</i></td><td>0.35</td><td>0.38</td><td>32.79</td><td>0.21</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="105" name="105" class="anchor-tag">6 结  论</h3>
                <div class="p1">
                    <p id="106">针对红外与可见光图像融合过程中图像细节不够丰富,融合图像中目标边缘易出现模糊等问题,利用<i>PCNN</i>输出的点火时间矩阵具有多区域分割特性这一特点,提出了结合<i>PCNN</i>与引导滤波<i>GFF</i>的融合方法。通过引入<i>GFF</i>修正点火时间矩阵,进行红外与可见光图像的加权融合,可很好地将红外与可见光图像各自的优势信息保留到融合图像中,且不会出现由于区域分割而带来的伪影。同时,为了提高融合速度,简化了<i>PCNN</i>的模型结构,提出了一种带约束的参数设置方法,并基于信息熵最大原则确定迭代停止次数,在保证点火时间矩阵分割的区域可较好反映原图纹理特性的基础上,减少迭代次数,从而提高了算法的效率。多组实验结果表明,本文提出的算法具有较高的运行效率,同时融合图像具有较为丰富的细节纹理信息,主客观综合性能均优于其他几种对比融合方法。本研究采用简单加权的规则进行融合,未来可寻找更为合适的融合规则,突出目标信息,以便于后续基于融合图像的目标定位及识别应用。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="14">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=From multi-scale decomposition to non-multi-scale decomposition methods:a comprehensive survey of image fusion techniques and its applications">

                                <b>[1]</b> <i>Dogra A</i>,<i>Goyal B</i>,<i>Agrawal S</i>.<i>From multi</i>-<i>scale decomposition to non</i>-<i>multi</i>-<i>scale decomposition methods</i>:<i>a comprehensive survey of image fusion techniques and its applications</i>[<i>J</i>].<i>IEEE Access</i>,2017,5:16040-16067.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201604019&amp;v=MDkyMzh2UFBEelRiTEc0SDlmTXE0OUViWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXZuVWI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> <i>Chen Z</i>,<i>Yang X P</i>,<i>Zhang C X</i>,et al.<i>Infrared and visible image fusion based on the compensation mechanism in NSCT domain</i>[<i>J</i>].<i>Chinese Journal of Scientific Instrument</i>,2016,37(4):860-870.陈震,杨小平,张聪炫,等.基于补偿机制的<i>NSCT</i>域红外与可见光图像融合[<i>J</i>].仪器仪表学报,2016,37(4):860-870.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A fusion algorithm for infrared and visible images based on saliency analysis and non-subsampled shearlet transform">

                                <b>[3]</b> <i>Zhang B H</i>,<i>Lu X Q</i>,<i>Pei H Q</i>,et al.<i>A fusion algorithm for infrared and visible images based on saliency analysis and non</i>-<i>subsampled shearlet transform</i>[<i>J</i>].<i>Infrared Physics</i> &amp; <i>Technology</i>,2015,73:286-297.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201707013&amp;v=MzE0MDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl2blVidlBJalhUYkxHNEg5Yk1xSTlFWjRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> <i>Wu D P</i>,<i>Bi D Y</i>,<i>He L Y</i>,et al.<i>A fusion algorithm of infrared and visible image based on NSSCT</i>[<i>J</i>].<i>Acta Optica Sinica</i>,2017,37(7):0710003.吴冬鹏,毕笃彦,何林远,等.基于<i>NSSCT</i>的红外与可见光图像融合[<i>J</i>].光学学报,2017,37(7):0710003.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7A4A453866DB652BE03740A731660FA5&amp;v=MTAxMTU5eEs4PU5pZk9mYlRKR3FESXFveE5ZdTE3Zm5vOHpXUm02ang2VEgrVHF4RTBmN1NVTTh1YUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRsaHg3cQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> <i>Yin M</i>,<i>Duan P H</i>,<i>Liu W</i>,et al.<i>A novel infrared and visible image fusion algorithm based on shift</i>-<i>invariant dual</i>-<i>tree complex shearlet transform and sparse representation</i>[<i>J</i>].<i>Neurocomputing</i>,2017,226:182-191.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GZXB201907015&amp;v=MDEwNjJDVVJMT2VaZVZ2Rnl2blVidlBJamZUYkxHNEg5ak1xSTlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> <i>Deng H</i>,<i>Wang C L</i>,<i>Hu Y J</i>,et al.<i>Fusion of infrared and visible images based on non</i>-<i>subsampled dual</i>-<i>tree complex contourlet and adaptive block</i>[<i>J</i>].<i>Acta Photonica Sinica</i>,2019,48(7):0710006.邓辉,王长龙,胡永江,等.基于非下采样双树复轮廓波与自适应分块的红外与可见光图像融合[<i>J</i>].光子学报,2019,48(7):0710006.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC425305563F7DAD147D59EE71752A9DA&amp;v=MDk3MjJwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxoeDdxOXhLOD1OaWZPZmNDOEhOVFByNHBBWXVoNUN3aEl1eGNYN1V0NFFRcVhxeE15ZkxEbFRNN3VDT052RlNpV1dyN0pJRg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> <i>Meher B</i>,<i>Agrawal S</i>,<i>Panda R</i>,et al.<i>A survey on region based image fusion methods</i>[<i>J</i>].<i>Information Fusion</i>,2019,48:119-132.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GZXB201903024&amp;v=MTM3NTJySTlIWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl2blVidlBJamZUYkxHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> <i>Zhu H R</i>,<i>Liu Y Q</i>,<i>Zhang W Y</i>.<i>Infrared and visible image fusion based on iterative guided filtering and multi</i>-<i>visual weight information</i>[<i>J</i>].<i>Acta Photonica Sinica</i>,2019,48(3):0310002.朱浩然,刘云清,张文颖.基于迭代导向滤波与多视觉权重信息的红外与可见光图像融合[<i>J</i>].光子学报,2019,48(3):0310002.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700406695&amp;v=MDQ0Nzg3SUkxd1diaFU9TmlmT2ZiSzhIOURNcUk5RllPc0pDblU4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> <i>Lu X Q</i>,<i>Zhang B H</i>,<i>Zhao Y</i>,et al.<i>The infrared and visible image fusion algorithm based on target separation and sparse representation</i>[<i>J</i>].<i>Infrared Physics</i> &amp; <i>Technology</i>,2014,67:397-407.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787040244632001&amp;v=MjM4Mzk5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3JrVTd6S0lsb1RYRnF6R2JPOEh0UElxNGxHWnVzUERSTTh6eFVTbURk&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> <i>Lindblad T</i>,<i>Kinser J M</i>.<i>Image processing using pulse</i>-<i>coupled neural networks</i>[<i>M</i>].<i>Ma Y D</i>,<i>Zhan K</i>,<i>Wang Z B</i>,et al.,<i>Transl</i>.<i>Beijing</i>:<i>Higher Education Press</i>,2008:10-20.<i>Lindblad T</i>,<i>Kinser J M</i>.脉冲耦合神经网络图像处理[<i>M</i>].马义德,绽琨,王兆滨,等,译.北京:高等教育出版社,2008:10-20.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB200201006&amp;v=MDc1NzJybzlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl2blVidlBNVFhUYkxHNEh0UE0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> <i>Ma Y D</i>,<i>Dai R L</i>,<i>Li L</i>.<i>Automated image segmentation using pulse coupled neural networks and image</i>'<i>s entropy</i>[<i>J</i>].<i>Journal of China Institute of Communications</i>,2002,23(1):46-51.马义德,戴若兰,李廉.一种基于脉冲耦合神经网络和图像熵的自动图像分割方法[<i>J</i>].通信学报,2002,23(1):46-51.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201205015&amp;v=MjkwODJDVVJMT2VaZVZ2Rnl2blVidlBJVGZUZTdHNEg5UE1xbzlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> <i>Deng X Y</i>,<i>Ma Y D</i>.<i>PCNN model automatic parameters determination and its modified model</i>[<i>J</i>].<i>Acta Electronica Sinica</i>,2012,40(5):955-964.邓翔宇,马义德.<i>PCNN</i>参数自适应设定及其模型的改进[<i>J</i>].电子学报,2012,40(5):955-964.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201708011&amp;v=MzA0NzR2blVidlBJalhUYkxHNEg5Yk1wNDlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> <i>Wu Y Q</i>,<i>Wang Z L</i>.<i>Infrared and visible image fusion based on target extraction and guided filtering enhancement</i>[<i>J</i>].<i>Acta Optica Sinica</i>,2017,37(8):0810001.吴一全,王志来.基于目标提取与引导滤波增强的红外与可见光图像融合[<i>J</i>].光学学报,2017,37(8):0810001.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image fusion with guided filtering">

                                <b>[14]</b> <i>Li S T</i>,<i>Kang X D</i>,<i>Hu J W</i>.<i>Image fusion with guided filtering</i>[<i>J</i>].<i>IEEE Transactions on Image Processing</i>,2013,22(7):2864-2875.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES9430CCB0989C35D95CF68177E5B5C3D8&amp;v=MjM4NDVlTUdmMzg4dXg4V21VbDdRSDdscTJjd0M3Zm5SczZYQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxoeDdxOXhLOD1OaWZPZmJxOEhkRy8zUDFGYg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> <i>Zhang Y</i>,<i>Zhang L J</i>,<i>Bai X Z</i>,et al.<i>Infrared and visual image fusion through infrared feature extraction and visual information preservation</i>[<i>J</i>].<i>Infrared Physics</i> &amp; <i>Technology</i>,2017,83:227-237.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201601005&amp;v=MDU4Mjk5Zk1ybzlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl2blVidlBQeXJmYkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> <i>Chen M S</i>.<i>Image fusion of visual and infrared image based on NSCT and compressed sensing</i>[<i>J</i>].<i>Journal of Image and Graphics</i>,2016,21(1):39-44.陈木生.结合<i>NSCT</i>和压缩感知的红外与可见光图像融合[<i>J</i>].中国图象图形学报,2016,21(1):39-44.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201911016" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201911016&amp;v=MTQ2NTQ5RVlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5dm5VYnZQSWpYVGJMRzRIOWpOcm8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

