

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134139525440000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201903028%26RESULT%3d1%26SIGN%3dQLvgFGcJH%252b7EVeN%252ftG%252f6NEmth44%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201903028&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201903028&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201903028&amp;v=Mjc3NTMzenFxQnRHRnJDVVJMT2VaZVZ1RnlIblVMM0pJalhUYkxHNEg5ak1ySTlIYklRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="2 基于图像分割的立体匹配算法 ">2 基于图像分割的立体匹配算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="&lt;b&gt;2.1 匹配代价计算&lt;/b&gt;"><b>2.1 匹配代价计算</b></a></li>
                                                <li><a href="#78" data-title="&lt;b&gt;2.2 匹配代价聚合&lt;/b&gt;"><b>2.2 匹配代价聚合</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;2.3 视差后处理&lt;/b&gt;"><b>2.3 视差后处理</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#111" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#113" data-title="&lt;b&gt;3.1 参数设置&lt;/b&gt;"><b>3.1 参数设置</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;3.2 算法对比分析&lt;/b&gt;"><b>3.2 算法对比分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#127" data-title="4 结 论 ">4 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="图1 算法流程图。">图1 算法流程图。</a></li>
                                                <li><a href="#80" data-title="图2 SLIC算法结果。">图2 SLIC算法结果。</a></li>
                                                <li><a href="#100" data-title="图3 不同算法所得结果。">图3 不同算法所得结果。</a></li>
                                                <li><a href="#101" data-title="图4 支持域">图4 支持域</a></li>
                                                <li><a href="#122" data-title="图5 不同参数对平均误差的影响。">图5 不同参数对平均误差的影响。</a></li>
                                                <li><a href="#125" data-title="表1 不同算法下的平均误匹配率">表1 不同算法下的平均误匹配率</a></li>
                                                <li><a href="#130" data-title="图6 实验结果。">图6 实验结果。</a></li>
                                                <li><a href="#131" data-title="表2 不同算法下n-occ的平均误匹配率">表2 不同算法下n-occ的平均误匹配率</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="9">


                                    <a id="bibliography_1" title=" Fan H R, Yang F, Pan X R, &lt;i&gt;et al&lt;/i&gt;. Stereo matching algorithm for improved Census transform and gradient fusion[J]. Acta Optica Sinica, 2018, 38 (2) : 0215006.  范海瑞, 杨帆, 潘旭冉, 等. 一种改进Census变换与梯度融合的立体匹配算法[J]. 光学学报, 2018, 38 (2) : 0215006." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201802032&amp;v=MTAzODNUYkxHNEg5bk1yWTlHWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIblVMM0lJalg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Fan H R, Yang F, Pan X R, &lt;i&gt;et al&lt;/i&gt;. Stereo matching algorithm for improved Census transform and gradient fusion[J]. Acta Optica Sinica, 2018, 38 (2) : 0215006.  范海瑞, 杨帆, 潘旭冉, 等. 一种改进Census变换与梯度融合的立体匹配算法[J]. 光学学报, 2018, 38 (2) : 0215006.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_2" title=" Scharstein D, Szeliski R. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J]. International Journal of Computer Vision, 2002, 47 (1/2/3) : 7-42." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830724&amp;v=MTQ5NDVYcVJyeG94Y01IN1I3cWVidWR0RlNqbFdyM0xJbDQ9Tmo3QmFyTzRIdEhPcDR4Rlkra0xZM2s1ekJkaDRqOTlT&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Scharstein D, Szeliski R. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J]. International Journal of Computer Vision, 2002, 47 (1/2/3) : 7-42.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_3" title=" Taniai T, Matsushita Y, Sato Y, &lt;i&gt;et al&lt;/i&gt;. Continuous 3D label stereo matching using local expansion moves[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (11) : 2725-2739." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Continuous 3D label stereo matching using local expansion moves">
                                        <b>[3]</b>
                                         Taniai T, Matsushita Y, Sato Y, &lt;i&gt;et al&lt;/i&gt;. Continuous 3D label stereo matching using local expansion moves[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (11) : 2725-2739.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_4" title=" Li J J, Ma L, Wang A X, &lt;i&gt;et al&lt;/i&gt;. Stereo matching algorithm based on improved patchmatch and slice sampling particle belief propagation[J]. Journal of Northeastern University (Natural Science) , 2016, 37 (5) : 609-613.  李晶皎, 马利, 王爱侠, 等. 基于改进Patchmatch及切片采样粒子置信度传播的立体匹配算法[J]. 东北大学学报 (自然科学版) , 2016, 37 (5) : 609-613." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DBDX201605001&amp;v=MDg5MDlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SG5VTDNJSVMvUGRyRzRIOWZNcW85RlpZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Li J J, Ma L, Wang A X, &lt;i&gt;et al&lt;/i&gt;. Stereo matching algorithm based on improved patchmatch and slice sampling particle belief propagation[J]. Journal of Northeastern University (Natural Science) , 2016, 37 (5) : 609-613.  李晶皎, 马利, 王爱侠, 等. 基于改进Patchmatch及切片采样粒子置信度传播的立体匹配算法[J]. 东北大学学报 (自然科学版) , 2016, 37 (5) : 609-613.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_5" title=" Zhu S P, Li Z. A stereo matching algorithm using improved gradient and adaptive window[J]. Acta Optica Sinica, 2015, 35 (1) : 0110003.  祝世平, 李政. 基于改进梯度和自适应窗口的立体匹配算法[J]. 光学学报, 2015, 35 (1) : 0110003." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201501014&amp;v=MzIyMDllVnVGeUhuVUwzSUlqWFRiTEc0SDlUTXJvOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Zhu S P, Li Z. A stereo matching algorithm using improved gradient and adaptive window[J]. Acta Optica Sinica, 2015, 35 (1) : 0110003.  祝世平, 李政. 基于改进梯度和自适应窗口的立体匹配算法[J]. 光学学报, 2015, 35 (1) : 0110003.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_6" title=" Zhu S P, Yan L N, Li Z. Stereo matching algorithm based on improved Census transform and dynamic programming[J]. Acta Optica Sinica, 2016, 36 (4) : 0415001. 祝世平, 闫利那, 李政. 基于改进Census变换和动态规划的立体匹配算法[J]. 光学学报, 2016, 36 (4) : 0415001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604028&amp;v=MDk3NTh0R0ZyQ1VSTE9lWmVWdUZ5SG5VTDNJSWpYVGJMRzRIOWZNcTQ5SGJJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Zhu S P, Yan L N, Li Z. Stereo matching algorithm based on improved Census transform and dynamic programming[J]. Acta Optica Sinica, 2016, 36 (4) : 0415001. 祝世平, 闫利那, 李政. 基于改进Census变换和动态规划的立体匹配算法[J]. 光学学报, 2016, 36 (4) : 0415001.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_7" title=" Liang Z F, Feng Y L, Guo Y L, &lt;i&gt;et al&lt;/i&gt;. Learning deep correspondence through prior and posterior feature constancy[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2017: 2403-2411." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep correspondence through prior and posterior feature constancy">
                                        <b>[7]</b>
                                         Liang Z F, Feng Y L, Guo Y L, &lt;i&gt;et al&lt;/i&gt;. Learning deep correspondence through prior and posterior feature constancy[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2017: 2403-2411.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_8" title=" Xiao J P, Tian H, Zou W T, &lt;i&gt;et al&lt;/i&gt;. Stereo matching based on convolutional neural network[J]. Acta Optica Sinica. 2018, 38 (8) : 0815017. 肖进胜, 田红, 邹文涛, 等. 基于深度卷积神经网络的双目立体视觉匹配算法[J]. 光学学报, 2018, 38 (8) : 0815017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201808018&amp;v=MjQyMjFyQ1VSTE9lWmVWdUZ5SG5VTDNJSWpYVGJMRzRIOW5NcDQ5RWJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Xiao J P, Tian H, Zou W T, &lt;i&gt;et al&lt;/i&gt;. Stereo matching based on convolutional neural network[J]. Acta Optica Sinica. 2018, 38 (8) : 0815017. 肖进胜, 田红, 邹文涛, 等. 基于深度卷积神经网络的双目立体视觉匹配算法[J]. 光学学报, 2018, 38 (8) : 0815017.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_9" title=" Mayer N, Ilg E, H&#228;usser P, &lt;i&gt;et al&lt;/i&gt;. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 4040-4048." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A large dataset to train convolutional networks for disparity,optical flow,and scene flow estimation">
                                        <b>[9]</b>
                                         Mayer N, Ilg E, H&#228;usser P, &lt;i&gt;et al&lt;/i&gt;. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 4040-4048.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_10" title=" Pang J H, Sun W X, Ren J S, &lt;i&gt;et al&lt;/i&gt;. Cascade residual learning: a two-stage convolutional neural network for stereo matching[C]. IEEE International Conference on Computer Vision Workshops, 2017: 878-886." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cascade Residual Learning A Two-Stage Convolutional Neural Network for Stereo Matching">
                                        <b>[10]</b>
                                         Pang J H, Sun W X, Ren J S, &lt;i&gt;et al&lt;/i&gt;. Cascade residual learning: a two-stage convolutional neural network for stereo matching[C]. IEEE International Conference on Computer Vision Workshops, 2017: 878-886.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_11" title=" Rhemann C, Hosni A, Bleyer M, &lt;i&gt;et al&lt;/i&gt;. Fast cost-volume filtering for visual correspondence and beyond[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (2) : 504-511." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast Cost-Volume Filtering for Visual Correspondence and Beyond">
                                        <b>[11]</b>
                                         Rhemann C, Hosni A, Bleyer M, &lt;i&gt;et al&lt;/i&gt;. Fast cost-volume filtering for visual correspondence and beyond[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (2) : 504-511.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_12" title=" Yang Q X. A non-local cost aggregation method for stereo matching[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2012: 1402-1409." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A non-local cost aggregation method for stereo matching">
                                        <b>[12]</b>
                                         Yang Q X. A non-local cost aggregation method for stereo matching[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2012: 1402-1409.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_13" title=" Zhang K, Fang Y Q, Min D B, &lt;i&gt;et al&lt;/i&gt;. Cross-scale cost aggregation for stereo matching[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 1590-1597." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-Scale Cost Aggregation for Stereo Matching">
                                        <b>[13]</b>
                                         Zhang K, Fang Y Q, Min D B, &lt;i&gt;et al&lt;/i&gt;. Cross-scale cost aggregation for stereo matching[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 1590-1597.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_14" title=" Liu Y, Li Q W, Huo G Y, &lt;i&gt;et al&lt;/i&gt;. Local binary description combined with superpixel segmentation refinement for stereo matching[J]. Acta Optica Sinica, 2018, 38 (6) : 0615003.  刘艳, 李庆武, 霍冠英, 等. 结合局部二进制表示和超像素分割求精的立体匹配[J]. 光学学报, 2018, 38 (6) : 0615003." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806033&amp;v=MTg1ODhaZVZ1RnlIblVMM0lJalhUYkxHNEg5bk1xWTlHWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Liu Y, Li Q W, Huo G Y, &lt;i&gt;et al&lt;/i&gt;. Local binary description combined with superpixel segmentation refinement for stereo matching[J]. Acta Optica Sinica, 2018, 38 (6) : 0615003.  刘艳, 李庆武, 霍冠英, 等. 结合局部二进制表示和超像素分割求精的立体匹配[J]. 光学学报, 2018, 38 (6) : 0615003.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     Gong W B, Gu G H, Qian W X, &lt;i&gt;et al&lt;/i&gt;. Stereo matching algorithm based on image segmentation and adaptive support weight[J]. Acta Optica Sinica, 2015, 35 (s2) : s210002.  龚文彪, 顾国华, 钱惟贤, 等. 基于图像分割和自适应支撑权重的立体匹配算法[J]. 光学学报, 2015, 35 (s2) : s210002.</a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_16" title=" Achanta R, Shaji A, Smith K, &lt;i&gt;et al&lt;/i&gt;. SLIC superpixels compared to state-of-the-art superpixel methods[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (11) : 2274-2282." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SLIC Superpixels Compared to State-of-the-Art Superpixel Methods">
                                        <b>[16]</b>
                                         Achanta R, Shaji A, Smith K, &lt;i&gt;et al&lt;/i&gt;. SLIC superpixels compared to state-of-the-art superpixel methods[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (11) : 2274-2282.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_17" title=" Criminisi A, Perez P, Toyama K. Region filling and object removal by exemplar-based image inpainting[J]. IEEE Transactions on Image Processing, 2004, 13 (9) : 1200-1212." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Region filling and object removal by exemplar-based image inpainting">
                                        <b>[17]</b>
                                         Criminisi A, Perez P, Toyama K. Region filling and object removal by exemplar-based image inpainting[J]. IEEE Transactions on Image Processing, 2004, 13 (9) : 1200-1212.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_18" title=" Guo H, Ono N, Sagayama S. A structure-synthesis image inpainting algorithm based on morphological erosion operation[C]. Congress on Image and Signal Processing, 2008: 530-535." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A structure-synthesis image inpainting algorithm based on morphological erosion operation">
                                        <b>[18]</b>
                                         Guo H, Ono N, Sagayama S. A structure-synthesis image inpainting algorithm based on morphological erosion operation[C]. Congress on Image and Signal Processing, 2008: 530-535.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_19" title=" Jiao A S M, Tsang P W M, Poon T C. Restoration of digital off-axis Fresnel hologram by exemplar and search based image inpainting with enhanced computing speed[J]. Computer Physics Communications, 2015, 193: 30-37." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15120600573741&amp;v=MDY0MjI0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lLVndYYUJJPU5pZk9mYks5SDlQTXFZOUZZZXdNQzNnNG9CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         Jiao A S M, Tsang P W M, Poon T C. Restoration of digital off-axis Fresnel hologram by exemplar and search based image inpainting with enhanced computing speed[J]. Computer Physics Communications, 2015, 193: 30-37.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_20" title=" Mei X, Sun X, Zhou M C, &lt;i&gt;et al&lt;/i&gt;. On building an accurate stereo matching system on graphics hardware[C]. IEEE International Conference on Computer Vision Workshops, 2011: 467-474." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On building an accurate stereo match-ing system on graphics hardware">
                                        <b>[20]</b>
                                         Mei X, Sun X, Zhou M C, &lt;i&gt;et al&lt;/i&gt;. On building an accurate stereo matching system on graphics hardware[C]. IEEE International Conference on Computer Vision Workshops, 2011: 467-474.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_21" title=" Yoon K J, Kweon I S. Adaptive support-weight approach for correspondence search[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006, 28 (4) : 650-656." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive support-weight approach for correspondence search">
                                        <b>[21]</b>
                                         Yoon K J, Kweon I S. Adaptive support-weight approach for correspondence search[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006, 28 (4) : 650-656.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_22" title=" Scharstein D, Szeliski R. Middlebury stereo vision page[EB/OL]. (2017-11-15) [2018-12-03]. http://vision.middlebury.edu/stereo/." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The middle stereo vision page">
                                        <b>[22]</b>
                                         Scharstein D, Szeliski R. Middlebury stereo vision page[EB/OL]. (2017-11-15) [2018-12-03]. http://vision.middlebury.edu/stereo/.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-10-29 06:37</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(03),240-248 DOI:10.3788/AOS201939.0315001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于图像分割的稠密立体匹配算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E7%91%9E%E6%B5%A9&amp;code=41392360&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马瑞浩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B1%E6%9E%AB&amp;code=09588420&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朱枫</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E6%B8%85%E6%BD%87&amp;code=11224531&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴清潇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%B2%81%E8%8D%A3%E8%8D%A3&amp;code=41392362&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">鲁荣荣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AD%8F%E6%99%AF%E9%98%B3&amp;code=39838214&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">魏景阳</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E6%B2%88%E9%98%B3%E8%87%AA%E5%8A%A8%E5%8C%96%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0183762&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院沈阳自动化研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%9C%E5%8C%97%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0111402&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">东北大学信息科学与工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%85%89%E7%94%B5%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院光电信息处理重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出一种基于图像分割的稠密立体匹配算法, 该算法将灰度-梯度算法与零均值归一化互相关 (ZNCC) 算法相结合生成匹配代价, 利用SLIC (Simple Liner Iterative Cluster) 算法对图像进行分割, 基于视差图和超像素更新了匹配代价。在视差后处理阶段, 基于左右一致性检验 (LRC) 、孔洞填充和十字交叉自适应窗口加权中值滤波的方法减小视差图的误匹配率。利用Middlebury数据集的4组图像进行测试, 测试结果表明, 平均误匹配率为4.99%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">立体匹配算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8C%B9%E9%85%8D%E4%BB%A3%E4%BB%B7%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">匹配代价计算方法融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%81%E5%AD%97%E4%BA%A4%E5%8F%89%E8%87%AA%E9%80%82%E5%BA%94%E7%AA%97%E5%8F%A3%E5%8A%A0%E6%9D%83%E4%B8%AD%E5%80%BC%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">十字交叉自适应窗口加权中值滤波;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *朱枫 E-mail:fzhu@sia.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-13</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (U1713216);</span>
                    </p>
            </div>
                    <h1><b>Dense Stereo Matching Algorithm Based on Image Segmentation</b></h1>
                    <h2>
                    <span>Ma Ruihao</span>
                    <span>Zhu Feng</span>
                    <span>Wu Qingxiao</span>
                    <span>Lu Rongrong</span>
                    <span>Wei Jingyang</span>
            </h2>
                    <h2>
                    <span>Shenyang Institute of Automation, Chinese Academy of Sciences</span>
                    <span>College of Information Science and Engineering, Northeastern University</span>
                    <span>Key Laboratory of Opto-Electronic Information Processing, Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A dense stereo matching algorithm is proposed based on image segmentation. This algorithm combines the gray-gradient algorithm and the zero-mean normalized cross-correlation (ZNCC) algorithm to generate matching cost. The SLIC (Simple Liner Iterative Cluster) algorithm is used for image segmentation. A method based disparity map and superpixels is proposed to update the matching cost. At the disparity post-processing stage, the LRC (Left Right Check) , hole filling and cross adaptive window weighted median filtering methods are used to reduce the error matching rate of the disparity map. The performance evaluation experiments on four Middlebury stereo pairs demonstrate that the proposed algorithm achieves an average error matching rate of 4.99%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=stereo%20matching%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">stereo matching algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=matching%20cost%20computation%20method%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">matching cost computation method fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=cross%20adaptive%20window%20weighted%20median%20filtering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">cross adaptive window weighted median filtering;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-09-13</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="53" name="53" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="54">立体匹配是计算机视觉中的热门研究方向, 广泛应用于机器人导航、虚拟现实、三维重建等领域<citation id="132" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。Scharstein等<citation id="133" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>将立体匹配算法分为4个步骤:1) 匹配代价计算;2) 代价聚合;3) 视差计算或优化;4) 视差细化 (视差后处理) 。根据是否采用代价聚合步骤, 可以将传统的立体匹配算法分为全局、半全局和局部立体匹配算法。全局立体匹配算法通常跳过匹配代价聚合步骤, 直接进行视差的计算和优化, 如图割法<citation id="134" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、置信度传播法<citation id="135" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等。全局立体匹配算法虽然精度高, 但是计算效率低。半全局立体匹配算法中的动态规划算法<citation id="136" type="reference"><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>是最为典型的方法, 传统动态规划算法的结果会出现扫描线效应。局部立体匹配算法一般利用匹配点的局部信息计算匹配代价, 采用代价聚合方法来改善匹配代价, 然后利用WTA (Winner Take All) 算法得到视差图, 虽然精度低, 但是计算效率高。</p>
                </div>
                <div class="p1">
                    <p id="55">近几年, 以深度学习为代表的机器学习被用于解决立体匹配问题, 并且取得了非常好的效果。机器学习主要是将卷积神经网络 (CNN) 应用在立体匹配中。基于CNN解决立体匹配问题的方法也大致分为三类<citation id="137" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>:1) 利用CNN学习匹配代价, 然后利用传统方法进行视差后处理, 例如MC-CNN (Matching Cost-Convolutional Neural Network) 及其改进网络<citation id="138" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>;2) 从端到端训练CNN, 直接从图像对估计视差, 例如DispNets<citation id="139" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>网络;3) 利用多个网络得到视差图, 例如CRL (Cascade Residual Learning) <citation id="140" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>网络, 由DisFullNet和DisResNet两个网络组成。基于CNN网络的方法取得了非常好的效果, 但是也具有一定的局限性:首先遮挡区域的像素点不能用来训练, 这意味着很难在这些区域获得可靠的视差估计<citation id="141" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>;其次, 训练神经网络需要大量的数据, 在某些特定场合, 无法得到训练网络所需要的数据, 这使得基于CNN的方法受到限制。</p>
                </div>
                <div class="p1">
                    <p id="56">传统方法不需要训练数据, 且已取得了较好的结果。Rhemann等<citation id="142" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>将图像引导滤波器 (GF) 应用在立体匹配中, 不仅能够减轻视差图边缘模糊现象, 而且计算效率高。Yang<citation id="143" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>将最小生成树 (MST) 引入匹配代价聚合, 不仅速度快, 而且精度高, 但是在遮挡处容易误匹配。Zhang等<citation id="144" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出跨尺度代价聚合模型, 优化了GF、MST等算法的结果, 但是对遮挡处的误匹配问题优化效果不佳。刘艳等<citation id="145" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出结合局部二进制表示和超像素分割求精的方法, 改善了弱纹理区域的阶梯效应, 但是在平均误匹配率较高。龚文彪等<citation id="146" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>利用mean shift分割算法, 分割出不同深度区域的匹配点, 根据匹配点所在的深度区域进行匹配代价重定义, 在遮挡处的效果较好, 但是在其他区域的误匹配率较高。为降低所有区域的误匹配率, 本文提出基于图像分割的稠密立体匹配方法。</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag">2 基于图像分割的立体匹配算法</h3>
                <div class="p1">
                    <p id="58">所提算法的流程分为三个部分:1) 匹配代价融合算法和SLIC (Simple Liner Iterative Cluster) <citation id="147" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>算法;2) 匹配代价聚合方法, 包括基于视差图和超像素更新匹配代价和GF算法;3) 视差后处理方法, 包括左右一致性检验 (LRC) 、孔洞填充和十字交叉自适应窗口加权中值滤波。匹配代价聚合、视差后处理以及整个算法的流程图如图1所示。</p>
                </div>
                <div class="area_img" id="59">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903028_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 算法流程图。" src="Detail/GetImg?filename=images/GXXB201903028_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 算法流程图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903028_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Flow chart of algorithm. </p>
                                <p class="img_note"> (a) 匹配代价聚合; (b) 视差后处理; (c) 综合流程图</p>
                                <p class="img_note"> (a) Matching cost aggregation; (b) disparity post-processing; (c) integrated flow chart</p>

                </div>
                <h4 class="anchor-tag" id="60" name="60"><b>2.1 匹配代价计算</b></h4>
                <div class="p1">
                    <p id="61">匹配代价计算基于立体校正过的图像, 即同一物体在左右两幅图像的同一行。匹配代价计算就是计算左右图像中各个点之间的相似性程度。分别计算图像中每点的灰度-梯度算法<citation id="148" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和零均值归一化互相关 (ZNCC) 算法的匹配代价。灰度-梯度算法的输入为RGB图像, 而ZNCC算法的输入是由RGB图像转换成的灰度图像。</p>
                </div>
                <div class="p1">
                    <p id="62">灰度-梯度算法的计算公式为</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>r</mtext><mtext>a</mtext><mtext>y</mtext><mo>-</mo><mtext>g</mtext><mtext>r</mtext><mtext>a</mtext><mtext>d</mtext><mtext>i</mtext><mtext>e</mtext><mtext>n</mtext><mtext>t</mtext></mrow></msub><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>α</mi><mo stretchy="false">) </mo><mi>min</mi><mo stretchy="false">[</mo><mo stretchy="false">∥</mo><mi>Ι</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>-</mo><mi>Ι</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>d</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><mo>, </mo><mi>τ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">]</mo><mo>+</mo><mi>α</mi><mi>min</mi><mo stretchy="false">[</mo><mo stretchy="false">∥</mo><mo>∇</mo><msub><mrow></mrow><mi>x</mi></msub><mi>Ι</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>-</mo><mo>∇</mo><msub><mrow></mrow><mi>x</mi></msub><mi>Ι</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>d</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><mo>, </mo><mi>τ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">式中:<i>C</i><sub>gray-gradient</sub>为灰度-梯度算法匹配代价;<i>I</i><sub>L</sub>为左图像;<i>I</i><sub>R</sub> (<i>x</i>+<i>d</i>, <i>y</i>) 为右图像;<i>x</i>和<i>y</i>表示像素在图像中的坐标;<i>d</i>表示视差值;∇<sub><i>x</i></sub><i>I</i><sub>L</sub>和∇<sub><i>x</i></sub><i>I</i><sub>R</sub>为图像水平方向的梯度图像;<i>α</i>为梯度图像匹配代价的权重;<i>τ</i><sub>1</sub>、<i>τ</i><sub>2</sub>为常量。</p>
                </div>
                <div class="p1">
                    <p id="65">ZNCC算法的计算公式为</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mrow><mtext>Ζ</mtext><mtext>Ν</mtext><mtext>C</mtext><mtext>C</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>∈</mo><mi>W</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msub></mrow></munder><mo stretchy="false">[</mo></mstyle><mi>Ι</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>i</mi><mo>, </mo><mi>y</mi><mo>+</mo><mi>j</mi><mo stretchy="false">) </mo><mo>-</mo><mover accent="true"><mi>Ι</mi><mo>¯</mo></mover><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>×</mo><mo stretchy="false">[</mo><mi>Ι</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>i</mi><mo>+</mo><mi>d</mi><mo>, </mo><mi>y</mi><mo>+</mo><mi>j</mi><mo stretchy="false">) </mo><mo>-</mo><mover accent="true"><mi>Ι</mi><mo>¯</mo></mover><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>d</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow><mrow><msqrt><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>∈</mo><mi>W</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msub></mrow></munder><mo stretchy="false">[</mo></mstyle><mi>Ι</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>i</mi><mo>, </mo><mi>y</mi><mo>+</mo><mi>j</mi><mo stretchy="false">) </mo><mo>-</mo><mover accent="true"><mi>Ι</mi><mo>¯</mo></mover><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mo>×</mo><msqrt><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>∈</mo><mi>W</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msub></mrow></munder><mo stretchy="false">[</mo></mstyle><mi>Ι</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>i</mi><mo>+</mo><mi>d</mi><mo>, </mo><mi>y</mi><mo>+</mo><mi>j</mi><mo stretchy="false">) </mo><mo>-</mo><mover accent="true"><mi>Ι</mi><mo>¯</mo></mover><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>d</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">式中:<i>C</i><sub>ZNCC</sub>为ZNCC算法匹配代价;<i>W</i><sub>in</sub>是匹配窗口;<i>i</i>和<i>j</i>表示窗口中以中心元素为原点时, 窗口内元素的坐标值, 窗口大小为<i>N</i>×<i>N</i>;<mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Ι</mi><mo>¯</mo></mover><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></math></mathml>和<mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Ι</mi><mo>¯</mo></mover><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>d</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></math></mathml>分别是左图像和右图像窗口W<sub><i>in</i></sub>的平均值。</p>
                </div>
                <div class="p1">
                    <p id="70">一般在物体边缘处容易发生遮挡, 遮挡处容易发生误匹配。遮挡就是在相机的公共视场内, 物体在左图中可以看到而在右图中看不到, 或物体在左图中看不到而在右图中能看到。物体边缘处一般为图像边缘处。图像边缘和非边缘处具有不同的特征, 因此在边缘和非边缘处对两种匹配代价计算方法分配不同的权值。由于灰度-梯度算法对灰度变化敏感, 而物体边缘处一般灰度发生变化较大, 因此在物体边缘处, 给灰度-梯度算法分配的权值较大, 给<i>ZNCC</i>算法分配的权值较小。反之, 在非边缘区域, <i>ZNCC</i>算法对噪声具有较强的稳健性, 因此给<i>ZNCC</i>算法分配的权值较大, 给灰度-梯度算法分配的权值较小。</p>
                </div>
                <div class="p1">
                    <p id="71">匹配代价计算公式为</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mo stretchy="false">|</mo><mi>C</mi><msub><mrow></mrow><mrow><mtext>Ζ</mtext><mtext>Ν</mtext><mtext>C</mtext><mtext>C</mtext></mrow></msub><mo stretchy="false">|</mo><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mn>2</mn><mo>-</mo><mi>β</mi><mo stretchy="false">) </mo><mo>+</mo><mi>C</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>r</mtext><mtext>a</mtext><mtext>y</mtext><mo>-</mo><mtext>g</mtext><mtext>r</mtext><mtext>a</mtext><mtext>d</mtext><mtext>i</mtext><mtext>e</mtext><mtext>n</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false"> (</mo><mn>2</mn><mo>-</mo><mi>γ</mi><mo stretchy="false">) </mo><mo>/</mo><mn>2</mn><mo>, </mo><mspace width="0.25em" /><mtext>e</mtext><mtext>d</mtext><mtext>g</mtext><mtext>e</mtext></mtd></mtr><mtr><mtd columnalign="left"><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mo stretchy="false">|</mo><mi>C</mi><msub><mrow></mrow><mrow><mtext>Ζ</mtext><mtext>Ν</mtext><mtext>C</mtext><mtext>C</mtext></mrow></msub><mo stretchy="false">|</mo><mo stretchy="false">) </mo><mi>β</mi><mo>+</mo><mi>C</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>r</mtext><mtext>a</mtext><mtext>y</mtext><mo>-</mo><mtext>g</mtext><mtext>r</mtext><mtext>a</mtext><mtext>d</mtext><mtext>i</mtext><mtext>e</mtext><mtext>n</mtext><mtext>t</mtext></mrow></msub><mi>γ</mi><mo>/</mo><mn>2</mn><mo>, </mo><mspace width="0.25em" /><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>s</mtext></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">式中:β和γ分别为<i>ZNCC</i>算法和灰度-梯度算法在非边缘区域的权值, 都为常量。</p>
                </div>
                <div class="p1">
                    <p id="74">图像的边缘可以利用<i>SLIC</i>超像素分割算法得到。<i>SLIC</i>算法将图像从<i>RGB</i>颜色空间转换到<i>CIE</i>-<i>Lab</i>颜色空间, 每个像素的颜色 (l, a, b) 和坐标 (x, y) 组成一个5维向量<b><i>V</i></b>=[<i>l</i>, <i>a</i>, <i>b</i>, <i>x</i>, <i>y</i>], 两个像素的相似性由对应向量<b><i>V</i></b>的欧式距离来测量。然后在图像上生成<i>K</i>个种子点, 在每个种子点的周围空间搜索距离该种子最近的若干个像素, 将它们与该种子归为同一类, 直到所有像素点都归类完毕。然后计算<i>K</i>个超像素里所有像素点的平均向量值, 再以<i>K</i>个平均向量值为中心去搜索其周围与其最相似的若干像素, 所有像素归类完毕后重新得到<i>K</i>个超像素。更新聚类中心, 再次迭代, 如此反复直到收敛。种子点个数<i>K</i>可表示为</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><mo>=</mo><mfrac><mrow><mi>r</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>w</mtext></mrow></msub><mi>c</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>l</mtext></mrow></msub></mrow><mi>ρ</mi></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">式中:<i>r</i><sub>ow</sub>和<i>c</i><sub>ol</sub>表示图像的行和列;<i>ρ</i>为超像素内初始的像素个数, 为常量。</p>
                </div>
                <div class="p1">
                    <p id="77">对Tsukuba进行SLIC超像素分割, 根据分割结果提取边缘, 其结果如图2所示。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78"><b>2.2 匹配代价聚合</b></h4>
                <div class="p1">
                    <p id="79">匹配代价聚合分3个步骤:1) 通过视差图更新匹配代价;2) 利用超像素更新匹配代价;3) 利用GF对匹配代价进行滤波。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903028_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 SLIC算法结果。" src="Detail/GetImg?filename=images/GXXB201903028_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 SLIC算法结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903028_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Result of SLIC algorithm. </p>
                                <p class="img_note"> (a) Tsukuba图像; (b) 超像素分割图; (c) 边缘图像</p>
                                <p class="img_note"> (a) Tsukuba image; (b) superpixel segmentation image; (c) edge image</p>

                </div>
                <div class="p1">
                    <p id="82">首先利用视差图对匹配代价进行更新, 视差图的更新公式为</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>w</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">|</mo><mi>d</mi><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">式中:<i>C</i><sub>new</sub> (<i>x</i>, <i>y</i>, <i>d</i>) 为更新后的匹配代价;<i>D</i> (<i>x</i>, <i>y</i>) 表示视差图;视差<i>d</i>∈[0, <i>d</i><sub>max</sub>], <i>d</i><sub>max</sub>为视差最大值。</p>
                </div>
                <div class="p1">
                    <p id="85">利用超像素更新匹配代价, 超像素内大多数像素拥有正确的视差值, 用这些正确的视差更新超像素内像素的匹配代价。匹配代价更新公式为</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>w</mtext></mrow></msub><mo>=</mo><mi>C</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>l</mtext><mtext>d</mtext></mrow></msub><mo>×</mo><mi>exp</mi><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi>d</mi><mo>, </mo><mi>s</mi></mrow></msub></mrow><mrow><mi>n</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">式中:<i>n</i><sub><i>s</i></sub>表示第<i>s</i>块超像素中像素数目;<i>n</i><sub><i>d</i>, <i>s</i></sub>表示第<i>s</i>块超像素中视差值为<i>d</i>的像素数目;<i>C</i><sub>old</sub>为更新之前的匹配代价;<i>C</i><sub>new</sub>为更新之后的匹配代价。</p>
                </div>
                <div class="p1">
                    <p id="88">为了解决边缘处的误匹配问题, 使用GF对匹配代价进行滤波, GF滤波器定义为</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>g</mi><msub><mrow></mrow><mi>p</mi></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>q</mi><mo>∈</mo><mi>w</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></munder><mi>W</mi></mstyle><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mo stretchy="false"> (</mo><mi>Ι</mi><mo stretchy="false">) </mo><mi>f</mi><msub><mrow></mrow><mi>q</mi></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>W</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mo stretchy="false"> (</mo><mi>Ι</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>:</mo><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>q</mi><mo stretchy="false">) </mo><mo>∈</mo><mi>w</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></munder><mo stretchy="false">[</mo></mstyle><mn>1</mn><mo>+</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mi>ε</mi><mi mathvariant="bold-italic">E</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mi>q</mi></msub><mo>-</mo><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">式中:<i>I</i>为RGB引导图像;<i>f</i><sub><i>q</i></sub>为待滤波图像, 即匹配代价图像;<i>g</i><sub><i>q</i></sub>为滤波后的匹配代价图像;<i>w</i><sub><i>p</i></sub>为以像素<i>p</i>为中心的窗口;<i>W</i><sub><i>pq</i></sub> (<i>I</i>) 为像素点<i>p</i> 和<i>q</i> 之间的权值;<i>w</i><sub><i>k</i></sub>为以像素<i>k</i>为中心的窗口, 公式中窗口大小都为<i>r</i>×<i>r</i>;<b><i>I</i></b><sub><i>p</i></sub>和<b><i>I</i></b><sub><i>q</i></sub>为窗口<i>w</i><sub><i>k</i></sub>内像素的RGB颜色值, 为三维向量;|<i>w</i><sub><i>k</i></sub>|为窗口内像素的数目;<i>μ</i><sub><i>k</i></sub>为窗口<i>w</i><sub><i>k</i></sub>的均值, 为三维向量;<i>Σ</i><sub><i>k</i></sub>为3×3的协方差矩阵;<i>ε</i>为惩罚系数;<b><i>E</i></b>为3×3的单位矩阵。</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91"><b>2.3 视差后处理</b></h4>
                <div class="p1">
                    <p id="92">完成匹配代价聚合后, 使用WTA算法得到视差图。但是视差图上存在许多误匹配点, 视差后处理在一定程度上可以消除这些误匹配点。视差后处理包括LRC算法, 孔洞填充, 十字交叉自适应窗口加权中值滤波和3×3的中值滤波。</p>
                </div>
                <div class="p1">
                    <p id="93">LRC算法是检测误匹配点的重要且有效的方法, 其主要思想是:当以左图像为参考图时, 生成左视差图, 反之生成右视差图。左视差图和右视差图中匹配点的视差值相同, 视差值不同的点为误匹配点。LRC算法的计算公式为</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>D</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>-</mo><mi>d</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>-</mo><mi>D</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mo>≤</mo><mn>1</mn><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">式中:<i>D</i><sub>L</sub> (<i>x</i> -<i>d</i>, <i>y</i>) 表示左视差图;<i>D</i><sub>R</sub> (<i>x</i>, <i>y</i>) 表示右视差图。</p>
                </div>
                <div class="p1">
                    <p id="96">通过LRC算法标记出误匹配点, 在孔洞填充阶段对误匹配点进行填充。孔洞填充可以看作是一个图像修复问题, 经典的图像修复算法<citation id="149" type="reference"><link href="41" rel="bibliography" /><link href="43" rel="bibliography" /><link href="45" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>常利用偏微分方程、纹理合成、结构特征等方法修复图像, 虽然能取得很好的结果, 但是计算效率低, 一般在孔洞填充阶段很少采用。传统的视差图孔洞填充算法是分别寻找误匹配点所在行的左侧和右侧距离误匹配点最近的有效匹配点, 用这2个有效匹配点视差值中较小的视差值填充误匹配点, 虽然效果一般, 但是速度快。为了兼顾视差图修复过程的速度和精度, 改进传统的视差图孔洞填充算法过程, 不仅在误匹配点所在行寻找有效匹配点, 还在误匹配点所在行的上一行和下一行寻找有效匹配点, 此时一共有6个有效匹配点视差值, 取6个视差值中的最小值填充误匹配点。但当选择同一个视差值作为大量无效点的填充值时, 会在视差图上产生条纹效应, 如图3 (a) 所示。</p>
                </div>
                <div class="p1">
                    <p id="97">通过十字交叉自适应窗口加权中值滤波来减弱条纹效应, 其效果图如图3 (b) 所示。首先确定加权中值滤波的支持域, 其支持域示意图如图4所示。图4中<i>p</i>点为误匹配点, <i>q</i>是其邻域点。支持域的确定过程如下:首先在<i>p</i>点的竖直方向进行扩展, 然后在扩展竖直方向的基础上, 扩展水平方向, 得到加权中值滤波的支持域。为了加快计算速度, 只计算误匹配点的自适应窗口。用于判断<i>q</i>点是否在<i>p</i>点的支持域内<citation id="150" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>的计算公式为</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>D</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>q</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mspace width="0.25em" /><mspace width="0.25em" /><mtext>a</mtext><mtext>n</mtext><mtext>d</mtext><mspace width="0.25em" /><mspace width="0.25em" /><mi>D</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>q</mi><msup><mrow></mrow><mo>+</mo></msup><mo stretchy="false">) </mo><mo>&lt;</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>q</mi><msup><mrow></mrow><mo>+</mo></msup><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>q</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mspace width="0.25em" /><mspace width="0.25em" /><mspace width="0.25em" /><mtext>v</mtext><mtext>e</mtext><mtext>r</mtext><mtext>t</mtext><mtext>i</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext></mtd></mtr><mtr><mtd columnalign="left"><mi>q</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mn>1</mn><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mspace width="0.25em" /><mspace width="0.25em" /><mspace width="0.25em" /><mtext>h</mtext><mtext>o</mtext><mtext>r</mtext><mtext>i</mtext><mtext>z</mtext><mtext>o</mtext><mtext>n</mtext><mtext>a</mtext><mtext>l</mtext></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr><mtd columnalign="left"><mi>D</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>q</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mi>L</mi><msub><mrow></mrow><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>D</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>q</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mi>θ</mi><msub><mrow></mrow><mn>2</mn></msub><mspace width="0.25em" /><mspace width="0.25em" /><mspace width="0.25em" /><mspace width="0.25em" /><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mi>L</mi><msub><mrow></mrow><mn>2</mn></msub><mo>&lt;</mo><mi>D</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>q</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mi>L</mi><msub><mrow></mrow><mn>1</mn></msub></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">式中:<i>D</i><sub>c</sub> 表示点<i>p</i>与点<i>q</i>的RGB颜色差值;<i>D</i><sub>s</sub>表示两点之间的距离差值;<i>L</i><sub>1</sub>、<i>L</i><sub>2</sub>、<i>θ</i><sub>1</sub>、<i>θ</i><sub>2</sub>为常数。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903028_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同算法所得结果。" src="Detail/GetImg?filename=images/GXXB201903028_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 不同算法所得结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903028_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Results of different algorithms. </p>
                                <p class="img_note"> (a) 孔洞填充; (b) 十字交叉自适应窗口加权中值滤波</p>
                                <p class="img_note"> (a) Hole filling; (b) cross adaptive window weighted median filtering</p>

                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903028_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 支持域" src="Detail/GetImg?filename=images/GXXB201903028_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 支持域  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903028_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Support region</p>

                </div>
                <div class="p1">
                    <p id="102">在支持域内进行加权中值滤波, 权值通过RGB颜色值计算<citation id="151" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="103" class="code-formula">
                        <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mo>=</mo><mi>exp</mi><mrow><mo>[</mo><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>Ι</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>Ι</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>σ</mi><msubsup><mrow></mrow><mtext>c</mtext><mn>2</mn></msubsup></mrow></mfrac></mrow><mo>]</mo></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="104">式中:<i>I</i><sub><i>p</i></sub>和<i>I</i><sub><i>q</i></sub>分别为<i>p</i>点和<i>q</i>点的RGB颜色值;<i>σ</i><sub>c</sub>为常量。</p>
                </div>
                <div class="p1">
                    <p id="105">求出权值后, 再取中值。构造视差直方图:</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>h</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><msup><mi>x</mi><mo>′</mo></msup><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></msub><mi>w</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><msup><mi>x</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mtext>δ</mtext><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><msup><mi>x</mi><mo>′</mo></msup></msub><mo stretchy="false">) </mo><mo>-</mo><mi>d</mi><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">式中:<i>h</i> (<i>x</i>, <i>d</i>) 为<i>p</i>点的直方图;<i>x</i>为<i>p</i>点的坐标;<i>x</i>′为支持域内其他点的坐标;<i>N</i><sub>s</sub> (<i>x</i>) 表示点<i>x</i>的支持域;<i>V</i>表示取点的视差值操作;δ (·) 为狄拉克函数;<i>w</i> (<i>x</i>, <i>x</i>′) 表示权值。</p>
                </div>
                <div class="p1">
                    <p id="108">将权值按照视差值从小到大的顺序相加, 当权值之和大于等于总权值的一半时, 此时对应的视差为加权中值滤波的结果, 即</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mi>min</mi><mspace width="0.25em" /><mi>d</mi></mtd></mtr><mtr><mtd columnalign="left"><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mspace width="0.25em" /><mspace width="0.25em" /><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>j</mi><msub><mrow></mrow><mi>d</mi></msub><mo>=</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mi>d</mi></msubsup><mi>h</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>j</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo stretchy="false">) </mo><mo>≥</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>j</mi><msub><mrow></mrow><mi>d</mi></msub><mo>=</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub></mrow></msubsup><mi>h</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>j</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">式中:<i>d</i><sup>*</sup>为加权中值滤波的结果;<i>d</i><sub>min</sub>、<i>d</i><sub>max</sub>分别表示视差的最小值和最大值。</p>
                </div>
                <h3 id="111" name="111" class="anchor-tag">3 实验结果与分析</h3>
                <div class="p1">
                    <p id="112">实验环境为:Windows7 64位系统, Intel (R) Core (TM) i7-6700CPU主频3.4 GHz, 4核, 8 GB内存。使用Middlebury<citation id="152" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>平台提供的Tsukuba、Venus、Teddy和Cones 4组彩色图像进行测试。利用这4组图像确定算法参数, 并与其他算法进行比较。除这4组图像外, 再加上平台提供的27组图像, 共利用31组图像对所提算法进行测试。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113"><b>3.1 参数设置</b></h4>
                <div class="p1">
                    <p id="114">所提算法参数设置如下:灰度-梯度算法和GF的参数采用文献<citation id="153" type="reference">[<a class="sup">11</a>]</citation>的参数<i>α</i>=0.9, <i>τ</i><sub>1</sub>=10, <i>τ</i><sub>2</sub>=2, <i>r</i>=9, <i>ε</i>=0.0001;十字交叉自适应窗口的参数采用文献<citation id="154" type="reference">[<a class="sup">20</a>]</citation>的参数<i>L</i><sub>1</sub>=62, <i>L</i><sub>2</sub>=32, <i>θ</i><sub>1</sub>=32, <i>θ</i><sub>2</sub>=16;加权中值滤波的参数采用文献<citation id="155" type="reference">[<a class="sup">21</a>]</citation>的参数<i>σ</i><sub>c</sub>=5;剩下的参数ZNCC窗口大小<i>N</i>, 匹配代价融合系数<i>β</i>、<i>γ</i>, 超像素分割系数<i>ρ</i>和迭代次数<i>T</i>通过实验确定。</p>
                </div>
                <div class="p1">
                    <p id="115">首先, 确定ZNCC窗口大小<i>N</i>的值, 具体方法如下:1) 计算ZNCC的匹配代价, GF滤波得到新的匹配代价;2) 采用WTA算法得到视差图;3) 计算AvgPBM。AvgPBM表示4组图像在非遮挡区域、所有区域和深度不连续区域 (遮挡处) 三个区域误匹配率的平均误匹配率。选择误匹配极限误差<i>E</i><sub>rror</sub>&gt;1, 即得到的视差图与标准视差图的像素差值大于1时认为是误匹配。<i>N</i>与AvgPBM的关系如图5 (a) 所示, 当<i>N</i>=5时, AvgPBM值最小。</p>
                </div>
                <div class="p1">
                    <p id="116">下一步确定<i>β</i>和<i>γ</i>的值, 与确定<i>N</i>的方法相同。固定<i>N</i>=5、<i>ρ</i>=1000。<i>β</i>、<i>γ</i>取不同的值时与AvgPBM的关系如图5 (b) 所示。当<i>β</i>=0.9、<i>γ</i>=0.3时, AvgPBM值最小。</p>
                </div>
                <div class="p1">
                    <p id="117">确定<i>N</i>、<i>β</i>、<i>γ</i>的值之后, 选择迭代次数<i>T</i>=2, 固定这几个值。按照第2节描述的算法得到视差图。超像素分割系数<i>ρ</i>与AvgPBM的关系如图5 (c) 所示。当<i>ρ</i>=2300时, AvgPBM值最小。</p>
                </div>
                <div class="p1">
                    <p id="118">固定<i>N</i>=5、<i>β</i>=0.9、<i>γ</i>=0.3和<i>ρ</i>=2300, 确定迭代次数<i>T</i>的值。迭代次数<i>T</i>与AvgPBM的关系如图5 (d) 所示。两次迭代后, AvgPBM大幅度减小, 第三次迭代后AvgPBM减小幅度较小, 所以迭代两次后停止迭代, 即<i>T</i>=2。</p>
                </div>
                <div class="p1">
                    <p id="119">通过实验, 最终确定:<i>N</i>=5, <i>β</i>=0.9, <i>γ</i>=0.3, <i>ρ</i>=2300, <i>T</i>=2。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120"><b>3.2 算法对比分析</b></h4>
                <div class="p1">
                    <p id="121">确定参数之后, 在Middlebury提供的Tsukuba、Venus、Teddy和 Cones 4组图像上进行测试, 并将所提算法与MST<citation id="156" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、GF<citation id="157" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>和GA-DP (Gradient Adaptive-Dynamic Programming) <citation id="158" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>算法进行对比, 如图6所示。从图6中可以看出所提算法在Tsukuba上的视差图效果一般, 但是在其他三张视差图上效果较好。</p>
                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903028_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同参数对平均误差的影响。" src="Detail/GetImg?filename=images/GXXB201903028_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同参数对平均误差的影响。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903028_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Influences of parameters on AvgPBM. </p>
                                <p class="img_note"> (a) N; (b) β和γ; (c) ρ; (d) T</p>
                                <p class="img_note"> (a) N; (b) β and γ; (c) ρ; (d) T</p>

                </div>
                <div class="p1">
                    <p id="123">所提算法、MST、GF和GA-DP算法的误匹配率如表1所示。表1中“n-occ”表示非遮挡区域, “all” 表示所有区域, “disc”表示深度不连续区域 (遮挡处) , AvgDisc表示4组图像在遮挡区域的平均误匹配率。表1中, Gray表示采用所提算法, 但是输入图像为灰度图像。从表1中可以看出, 当灰度图像作为输入时, 比RGB图像作为输入在AvgPBM上高1.47%, 在AvgDisc上高2.85%。因此采用灰度图像作为输入时, 效果比RGB图像作为输入要差。所提算法的AvgPBM比MST、GF和GA-DP算法的分别低0.49%、0.56%和1.11%。但是所提算法在Tsukuba上的n-occ和all区域的误匹配率较高, Tsukuba图像匹配错误的地方集中在无纹理和弱纹理区域, 因此所提算法在无纹理和弱纹理区域容易出现误匹配。所提算法的AvgDisc比MST、GF和GA-DP算法的分别低0.72%、1.06%和1.44%。可见所提算法的AvgPBM和AvgDisc较低。</p>
                </div>
                <div class="p1">
                    <p id="124">为全面地测试算法性能, 对Middlebury提供的31组图像上进行测试, 测试结果如表2所示。表2中数字下标表示各个算法匹配精度的排名, AvgErr表示算法在n-occ区域的平均误匹配率, AvgRank表示算法的平均排名。在表2中, 灰度图像作为输入比直接输入RGB图像的AvgErr高1.32%, 但是比MST算法低0.49%, 因此得出所提算法可以用灰度图像作为输入的结论。所提算法在Midd1、Midd2、Monopoly、Plastic和Reindeer的误匹配率比较高, 是因为这5幅图像中有大量无纹理和弱纹理区域。但是所提算法的AvgErr分别比GF、CS-MST和MST低0.83%、1.02%和1.81%。在4种算法中, 所提算法的平均排名为1.87, 排名高于其他三种算法。</p>
                </div>
                <div class="area_img" id="125">
                    <p class="img_tit">表1 不同算法下的平均误匹配率 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 AvgPBM for different algorithms %</p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td rowspan="2">Algorithm</td><td colspan="3"><br />Tsukuba</td><td rowspan="2"></td><td colspan="3"><br />Venus</td><td rowspan="2"></td><td colspan="3"><br />Teddy</td><td rowspan="2"></td><td colspan="3"><br />Cones</td><td rowspan="2">Avg PBM</td><td rowspan="2">Avg Disc</td></tr><tr><td><br />n-occ</td><td>all</td><td>disc</td><td><br />n-occ</td><td>all</td><td>disc</td><td><br />n-occ</td><td>all</td><td>disc</td><td><br />n-occ</td><td>all</td><td>disc</td></tr><tr><td>Proposed</td><td>1.50</td><td>1.95</td><td><b>6.71</b></td><td><b></b></td><td><b>0.11</b></td><td><b>0.33</b></td><td><b>1.25</b></td><td><b></b></td><td><b>5.27</b></td><td><b>10.8</b></td><td>14.5</td><td></td><td><b>2.38</b></td><td><b>8.02</b></td><td><b>7.01</b></td><td><b>4.99</b></td><td><b>7.36</b></td></tr><tr><td><br />MST</td><td><b>1.47</b></td><td><b>1.85</b></td><td>7.88</td><td></td><td>0.25</td><td>0.42</td><td>2.60</td><td></td><td>6.01</td><td>11.6</td><td><b>14.3</b></td><td></td><td>2.87</td><td>8.45</td><td>8.10</td><td>5.48</td><td>8.08</td></tr><tr><td><br />GF</td><td>1.51</td><td><b>1.85</b></td><td>7.61</td><td></td><td>0.20</td><td>0.93</td><td>2.42</td><td></td><td>6.16</td><td>11.8</td><td>16.0</td><td></td><td>2.71</td><td>8.24</td><td>7.66</td><td>5.55</td><td>8.42</td></tr><tr><td><br />GA-DP</td><td>1.57</td><td>2.00</td><td>7.32</td><td></td><td>0.89</td><td>1.00</td><td>3.18</td><td></td><td>7.20</td><td>12.4</td><td>16.1</td><td></td><td>3.68</td><td>9.18</td><td>8.62</td><td>6.10</td><td>8.80</td></tr><tr><td><br />Gray</td><td>1.91</td><td>2.74</td><td>9.70</td><td></td><td>0.32</td><td>0.68</td><td>4.25</td><td></td><td>5.99</td><td>11.7</td><td>16.2</td><td></td><td>3.70</td><td>9.64</td><td>10.7</td><td>6.46</td><td>10.21</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="127" name="127" class="anchor-tag">4 结 论</h3>
                <div class="p1">
                    <p id="128">提出一种基于图像分割的稠密立体匹配算法。首先提出基于超像素边缘的匹配代价融合计算方法, 为后面迭代提供良好的初始视差图。经过基于视差图和超像素的匹配代价更新、GF滤波和视差后处理循环迭代后, 视差图的误匹配率有所降低。实验结果表明所提算法具有较低的平均误匹配率和较低的遮挡处误匹配率, 但是在无纹理和弱纹理区域的匹配精度有待提高。</p>
                </div>
                <div class="p1">
                    <p id="129">后续研究方向包括:1) 继续研究匹配代价计算方法的融合;2) 研究图像分割算法与立体匹配算法相结合的方法;3) 对于无纹理区域, 所提算法的错误率较高, 后续工作着重解决无纹理区域的误匹配问题。</p>
                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903028_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 实验结果。" src="Detail/GetImg?filename=images/GXXB201903028_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 实验结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903028_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Experimental results.</p>
                                <p class="img_note"> (a) 左图像; (b) 真实视差图; (c) 所提算法结果; (d) MST算法结果; (e) GF算法结果; (f) GA-DP算法结果</p>
                                <p class="img_note"> (a) Left image; (b) ground-truth disparity; (c) result of proposed algorithm; (d) result of MST algorithm; (e) result of GF algorithm; (f) result of GA-DP algorithm</p>

                </div>
                <div class="area_img" id="131">
                    <p class="img_tit">表2 不同算法下n-occ的平均误匹配率 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 AvgPBM for different algorithms on n-occ region %</p>
                    <p class="img_note"></p>
                    <table id="131" border="1"><tr><td><br />Stereo pair</td><td>Proposed</td><td>GF</td><td>CS-MST</td><td>Gray</td><td>MST</td></tr><tr><td>Tsukuba</td><td>1.50<sub>2</sub></td><td>1.51<sub>3</sub></td><td>2.12<sub>5</sub></td><td>1.91<sub>4</sub></td><td>1.47<sub>1</sub></td></tr><tr><td><br />Venus</td><td><b>0.11</b><sub>1</sub></td><td>0.20<sub>2</sub></td><td>0.84<sub>5</sub></td><td>0.32<sub>4</sub></td><td>0.25<sub>3</sub></td></tr><tr><td><br />Teddy</td><td><b>5.27</b><sub>1</sub></td><td>6.16<sub>5</sub></td><td>7.61<sub>4</sub></td><td>5.99<sub>3</sub></td><td>5.53<sub>2</sub></td></tr><tr><td><br />Cones</td><td><b>2.38</b><sub>1</sub></td><td>2.71<sub>2</sub></td><td>4.10<sub>4</sub></td><td>3.70<sub>3</sub></td><td>6.01<sub>5</sub></td></tr><tr><td><br />Alone</td><td>4.58<sub>2</sub></td><td>5.53<sub>4</sub></td><td><b>4.14</b><sub>1</sub></td><td>7.13<sub>5</sub></td><td>4.63<sub>3</sub></td></tr><tr><td><br />Art</td><td><b>7.08</b><sub>1</sub></td><td>9.03<sub>2</sub></td><td>9.79<sub>3</sub></td><td>9.88<sub>4</sub></td><td>10.79<sub>5</sub></td></tr><tr><td><br />Baby1</td><td><b>2.62</b><sub>1</sub></td><td>4.69<sub>3</sub></td><td>7.37<sub>4</sub></td><td>3.24<sub>2</sub></td><td>8.39<sub>5</sub></td></tr><tr><td><br />Baby2</td><td><b>3.30</b><sub>1</sub></td><td>6.08<sub>3</sub></td><td>11.95<sub>4</sub></td><td>4.91<sub>2</sub></td><td>13.37<sub>5</sub></td></tr><tr><td><br />Baby3</td><td><b>3.46</b><sub>1</sub></td><td>5.79<sub>4</sub></td><td>5.64<sub>3</sub></td><td>4.52<sub>2</sub></td><td>7.25<sub>5</sub></td></tr><tr><td><br />Books</td><td><b>8.29</b><sub>1</sub></td><td>10.22<sub>3</sub></td><td>9.56<sub>2</sub></td><td>10.64<sub>5</sub></td><td>10.26<sub>4</sub></td></tr><tr><td><br />Bowling1</td><td><b>6.48</b><sub>1</sub></td><td>14.52<sub>3</sub></td><td>16.81<sub>4</sub></td><td>9.77<sub>2</sub></td><td>20.89<sub>5</sub></td></tr><tr><td><br />Bowling2</td><td><b>4.87</b><sub>1</sub></td><td>7.08<sub>3</sub></td><td>9.31<sub>4</sub></td><td>6.82<sub>2</sub></td><td>10.15<sub>5</sub></td></tr><tr><td><br />Cloth1</td><td>1.01<sub>3</sub></td><td>1.08<sub>4</sub></td><td><b>0.51</b><sub>1</sub></td><td>1.12<sub>5</sub></td><td>0.61<sub>2</sub></td></tr><tr><td><br />Cloth2</td><td><b>2.31</b><sub>1</sub></td><td>3.46<sub>3</sub></td><td>2.85<sub>2</sub></td><td>3.57<sub>4</sub></td><td>4.13<sub>5</sub></td></tr><tr><td><br />Cloth3</td><td><b>1.46</b><sub>1</sub></td><td>2.15<sub>3</sub></td><td>1.77<sub>2</sub></td><td>2.20<sub>4</sub></td><td>2.66<sub>5</sub></td></tr><tr><td><br />Cloth4</td><td><b>3.20</b><sub>4</sub></td><td>1.62<sub>2</sub></td><td>1.30<sub>1</sub></td><td>3.74<sub>5</sub></td><td>1.87<sub>3</sub></td></tr><tr><td><br />Dolls</td><td><b>4.08</b><sub>1</sub></td><td>5.04<sub>3</sub></td><td>5.00<sub>2</sub></td><td>6.57<sub>5</sub></td><td>5.95<sub>4</sub></td></tr><tr><td><br />Flowerpots</td><td><b>9.80</b><sub>1</sub></td><td>12.79<sub>2</sub></td><td>16.67<sub>4</sub></td><td>12.88<sub>3</sub></td><td>19.41<sub>5</sub></td></tr><tr><td><br />Lampshade1</td><td><b>5.59</b><sub>1</sub></td><td>11.57<sub>4</sub></td><td>10.43<sub>3</sub></td><td>6.74<sub>2</sub></td><td>11.99<sub>5</sub></td></tr><tr><td><br />Lampshade2</td><td><b>13.88</b><sub>1</sub></td><td>21.13<sub>5</sub></td><td>20.88<sub>4</sub></td><td>15.04<sub>2</sub></td><td>18.20<sub>3</sub></td></tr><tr><td><br />Laundry</td><td>15.65<sub>3</sub></td><td>16.40<sub>4</sub></td><td>13.69<sub>2</sub></td><td>18.50<sub>5</sub></td><td><b>12.94</b><sub>1</sub></td></tr><tr><td><br />Midd1</td><td>40.10<sub>4</sub></td><td>40.11<sub>5</sub></td><td>32.32<sub>2</sub></td><td>36.67<sub>3</sub></td><td><b>27.85</b><sub>1</sub></td></tr><tr><td><br />Midd2</td><td>39.24<sub>5</sub></td><td>35.85<sub>3</sub></td><td>34.50<sub>2</sub></td><td>36.93<sub>4</sub></td><td><b>32.09</b><sub>1</sub></td></tr><tr><td><br />Moebius</td><td><b>7.44</b><sub>1</sub></td><td>9.25<sub>4</sub></td><td>7.67<sub>2</sub></td><td>9.33<sub>5</sub></td><td>8.69<sub>3</sub></td></tr><tr><td><br />Monopoly</td><td>25.40<sub>3</sub></td><td>27.99<sub>5</sub></td><td><b>22.51</b><sub>1</sub></td><td>27.71<sub>4</sub></td><td>24.21<sub>2</sub></td></tr><tr><td><br />Plastic</td><td><b>33.62</b><sub>1</sub></td><td>39.29<sub>2</sub></td><td>42.53<sub>4</sub></td><td>40.21<sub>3</sub></td><td>47.03<sub>5</sub></td></tr><tr><td><br />Reindeer</td><td>27.57<sub>4</sub></td><td><b>7.23</b><sub>1</sub></td><td>9.15<sub>2</sub></td><td>28.36<sub>5</sub></td><td>9.87<sub>3</sub></td></tr><tr><td><br />Rocks1</td><td>3.68<sub>4</sub></td><td>2.70<sub>2</sub></td><td><b>2.23</b><sub>1</sub></td><td>4.41<sub>5</sub></td><td>2.83<sub>3</sub></td></tr><tr><td><br />Rocks2</td><td>2.04<sub>3</sub></td><td>1.61<sub>2</sub></td><td><b>1.57</b><sub>1</sub></td><td>2.58<sub>5</sub></td><td>2.08<sub>4</sub></td></tr><tr><td><br />Wood1</td><td>3.77<sub>2</sub></td><td><b>2.83</b><sub>1</sub></td><td>8.68<sub>4</sub></td><td>4.73<sub>3</sub></td><td>11.06<sub>5</sub></td></tr><tr><td><br />Wood2</td><td>2.27<sub>2</sub></td><td>2.34<sub>3</sub></td><td><b>0.99</b><sub>1</sub></td><td>3.00<sub>4</sub></td><td>5.61<sub>5</sub></td></tr><tr><td><br />AvgErr</td><td><b>9.42</b></td><td>10.25</td><td>10.44</td><td>10.74</td><td>11.23</td></tr><tr><td><br />AvgRank</td><td><b>1.87</b></td><td>3.06</td><td>2.61</td><td>3.67</td><td>3.65</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="9">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201802032&amp;v=MjQ5NDZPZVplVnVGeUhuVUwzSUlqWFRiTEc0SDluTXJZOUdab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Fan H R, Yang F, Pan X R, <i>et al</i>. Stereo matching algorithm for improved Census transform and gradient fusion[J]. Acta Optica Sinica, 2018, 38 (2) : 0215006.  范海瑞, 杨帆, 潘旭冉, 等. 一种改进Census变换与梯度融合的立体匹配算法[J]. 光学学报, 2018, 38 (2) : 0215006.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830724&amp;v=Mjc1MDZkdEZTamxXcjNMSWw0PU5qN0Jhck80SHRIT3A0eEZZK2tMWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Scharstein D, Szeliski R. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J]. International Journal of Computer Vision, 2002, 47 (1/2/3) : 7-42.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Continuous 3D label stereo matching using local expansion moves">

                                <b>[3]</b> Taniai T, Matsushita Y, Sato Y, <i>et al</i>. Continuous 3D label stereo matching using local expansion moves[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (11) : 2725-2739.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DBDX201605001&amp;v=MTMwMzBmTXFvOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhuVUwzSUlTL1Bkckc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Li J J, Ma L, Wang A X, <i>et al</i>. Stereo matching algorithm based on improved patchmatch and slice sampling particle belief propagation[J]. Journal of Northeastern University (Natural Science) , 2016, 37 (5) : 609-613.  李晶皎, 马利, 王爱侠, 等. 基于改进Patchmatch及切片采样粒子置信度传播的立体匹配算法[J]. 东北大学学报 (自然科学版) , 2016, 37 (5) : 609-613.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201501014&amp;v=MTY0MTMzenFxQnRHRnJDVVJMT2VaZVZ1RnlIblVMM0lJalhUYkxHNEg5VE1ybzlFWUlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Zhu S P, Li Z. A stereo matching algorithm using improved gradient and adaptive window[J]. Acta Optica Sinica, 2015, 35 (1) : 0110003.  祝世平, 李政. 基于改进梯度和自适应窗口的立体匹配算法[J]. 光学学报, 2015, 35 (1) : 0110003.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604028&amp;v=MjkxNjhIOWZNcTQ5SGJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SG5VTDNJSWpYVGJMRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Zhu S P, Yan L N, Li Z. Stereo matching algorithm based on improved Census transform and dynamic programming[J]. Acta Optica Sinica, 2016, 36 (4) : 0415001. 祝世平, 闫利那, 李政. 基于改进Census变换和动态规划的立体匹配算法[J]. 光学学报, 2016, 36 (4) : 0415001.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep correspondence through prior and posterior feature constancy">

                                <b>[7]</b> Liang Z F, Feng Y L, Guo Y L, <i>et al</i>. Learning deep correspondence through prior and posterior feature constancy[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2017: 2403-2411.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201808018&amp;v=MDA5MDBJalhUYkxHNEg5bk1wNDlFYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIblVMM0k=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Xiao J P, Tian H, Zou W T, <i>et al</i>. Stereo matching based on convolutional neural network[J]. Acta Optica Sinica. 2018, 38 (8) : 0815017. 肖进胜, 田红, 邹文涛, 等. 基于深度卷积神经网络的双目立体视觉匹配算法[J]. 光学学报, 2018, 38 (8) : 0815017.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A large dataset to train convolutional networks for disparity,optical flow,and scene flow estimation">

                                <b>[9]</b> Mayer N, Ilg E, Häusser P, <i>et al</i>. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 4040-4048.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cascade Residual Learning A Two-Stage Convolutional Neural Network for Stereo Matching">

                                <b>[10]</b> Pang J H, Sun W X, Ren J S, <i>et al</i>. Cascade residual learning: a two-stage convolutional neural network for stereo matching[C]. IEEE International Conference on Computer Vision Workshops, 2017: 878-886.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast Cost-Volume Filtering for Visual Correspondence and Beyond">

                                <b>[11]</b> Rhemann C, Hosni A, Bleyer M, <i>et al</i>. Fast cost-volume filtering for visual correspondence and beyond[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (2) : 504-511.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A non-local cost aggregation method for stereo matching">

                                <b>[12]</b> Yang Q X. A non-local cost aggregation method for stereo matching[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2012: 1402-1409.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-Scale Cost Aggregation for Stereo Matching">

                                <b>[13]</b> Zhang K, Fang Y Q, Min D B, <i>et al</i>. Cross-scale cost aggregation for stereo matching[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 1590-1597.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806033&amp;v=MzI1OTFNcVk5R1o0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SG5VTDNJSWpYVGJMRzRIOW4=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Liu Y, Li Q W, Huo G Y, <i>et al</i>. Local binary description combined with superpixel segmentation refinement for stereo matching[J]. Acta Optica Sinica, 2018, 38 (6) : 0615003.  刘艳, 李庆武, 霍冠英, 等. 结合局部二进制表示和超像素分割求精的立体匹配[J]. 光学学报, 2018, 38 (6) : 0615003.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 Gong W B, Gu G H, Qian W X, <i>et al</i>. Stereo matching algorithm based on image segmentation and adaptive support weight[J]. Acta Optica Sinica, 2015, 35 (s2) : s210002.  龚文彪, 顾国华, 钱惟贤, 等. 基于图像分割和自适应支撑权重的立体匹配算法[J]. 光学学报, 2015, 35 (s2) : s210002.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SLIC Superpixels Compared to State-of-the-Art Superpixel Methods">

                                <b>[16]</b> Achanta R, Shaji A, Smith K, <i>et al</i>. SLIC superpixels compared to state-of-the-art superpixel methods[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (11) : 2274-2282.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Region filling and object removal by exemplar-based image inpainting">

                                <b>[17]</b> Criminisi A, Perez P, Toyama K. Region filling and object removal by exemplar-based image inpainting[J]. IEEE Transactions on Image Processing, 2004, 13 (9) : 1200-1212.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A structure-synthesis image inpainting algorithm based on morphological erosion operation">

                                <b>[18]</b> Guo H, Ono N, Sagayama S. A structure-synthesis image inpainting algorithm based on morphological erosion operation[C]. Congress on Image and Signal Processing, 2008: 530-535.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15120600573741&amp;v=MTg3NjJ3WGFCST1OaWZPZmJLOUg5UE1xWTlGWWV3TUMzZzRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lLVg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> Jiao A S M, Tsang P W M, Poon T C. Restoration of digital off-axis Fresnel hologram by exemplar and search based image inpainting with enhanced computing speed[J]. Computer Physics Communications, 2015, 193: 30-37.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On building an accurate stereo match-ing system on graphics hardware">

                                <b>[20]</b> Mei X, Sun X, Zhou M C, <i>et al</i>. On building an accurate stereo matching system on graphics hardware[C]. IEEE International Conference on Computer Vision Workshops, 2011: 467-474.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive support-weight approach for correspondence search">

                                <b>[21]</b> Yoon K J, Kweon I S. Adaptive support-weight approach for correspondence search[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006, 28 (4) : 650-656.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The middle stereo vision page">

                                <b>[22]</b> Scharstein D, Szeliski R. Middlebury stereo vision page[EB/OL]. (2017-11-15) [2018-12-03]. http://vision.middlebury.edu/stereo/.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201903028" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201903028&amp;v=Mjc3NTMzenFxQnRHRnJDVVJMT2VaZVZ1RnlIblVMM0pJalhUYkxHNEg5ak1ySTlIYklRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="4" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

