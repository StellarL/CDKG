

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133923144346250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201906030%26RESULT%3d1%26SIGN%3dOOi8MLc8Ca%252fxC%252bWLiLYfxLE6o1k%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201906030&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201906030&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201906030&amp;v=MTc5NThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnl2blU3dkxJalhUYkxHNEg5ak1xWTlHWkk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#58" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="2 总体框架设计 ">2 总体框架设计</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="1) 建模阶段。">1) 建模阶段。</a></li>
                                                <li><a href="#68" data-title="2) 跟踪阶段。">2) 跟踪阶段。</a></li>
                                                <li><a href="#72" data-title="3) 更新阶段。">3) 更新阶段。</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#74" data-title="3 自适应特征选择的跟踪算法 ">3 自适应特征选择的跟踪算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#86" data-title="&lt;b&gt;3.1 位置滤波器与颜色概率模型&lt;/b&gt;"><b>3.1 位置滤波器与颜色概率模型</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;3.2 基于GMS的目标检测器&lt;/b&gt;"><b>3.2 基于GMS的目标检测器</b></a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;3.3 多尺度滤波&lt;/b&gt;"><b>3.3 多尺度滤波</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#145" data-title="4 算法步骤 ">4 算法步骤</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#207" data-title="1) 建立模型。">1) 建立模型。</a></li>
                                                <li><a href="#213" data-title="2) 融合特征。">2) 融合特征。</a></li>
                                                <li><a href="#212" data-title="3) 可信度判断。">3) 可信度判断。</a></li>
                                                <li><a href="#211" data-title="4) 特征点匹配和重检测。">4) 特征点匹配和重检测。</a></li>
                                                <li><a href="#210" data-title="5) 尺度估计。">5) 尺度估计。</a></li>
                                                <li><a href="#209" data-title="6) 模型更新。">6) 模型更新。</a></li>
                                                <li><a href="#208" data-title="7) 输出跟踪结果。">7) 输出跟踪结果。</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#154" data-title="5 实验结果及分析 ">5 实验结果及分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#155" data-title="&lt;b&gt;5.1 实验参数设置及数据&lt;/b&gt;"><b>5.1 实验参数设置及数据</b></a></li>
                                                <li><a href="#160" data-title="&lt;b&gt;5.2 对比实验分析&lt;/b&gt;"><b>5.2 对比实验分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#178" data-title="6 结  论 ">6 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="图1 整体框架示意图">图1 整体框架示意图</a></li>
                                                <li><a href="#80" data-title="表1 融合特征多权重分配方式">表1 融合特征多权重分配方式</a></li>
                                                <li><a href="#84" data-title="图2 自适应选择检测跟踪示意图">图2 自适应选择检测跟踪示意图</a></li>
                                                <li><a href="#123" data-title="图3 重检测过程示意图">图3 重检测过程示意图</a></li>
                                                <li><a href="#133" data-title="图4 目标运动轨迹图">图4 目标运动轨迹图</a></li>
                                                <li><a href="#134" data-title="图5 预测区域构造示意图">图5 预测区域构造示意图</a></li>
                                                <li><a href="#137" data-title="表2 目标中心位置比较">表2 目标中心位置比较</a></li>
                                                <li><a href="#143" data-title="表3 估计尺度与实际尺度信息表">表3 估计尺度与实际尺度信息表</a></li>
                                                <li><a href="#164" data-title="表4 8种跟踪算法平均跟踪性能比较">表4 8种跟踪算法平均跟踪性能比较</a></li>
                                                <li><a href="#165" data-title="图6 8种跟踪算法在OTB50上的精确率和成功率">图6 8种跟踪算法在OTB50上的精确率和成功率</a></li>
                                                <li><a href="#166" data-title="图7 8种跟踪算法在OTB100上的精确率和成功率">图7 8种跟踪算法在OTB100上的精确率和成功率</a></li>
                                                <li><a href="#169" data-title="图8 8种跟踪方法在部分序列上的跟踪结果">图8 8种跟踪方法在部分序列上的跟踪结果</a></li>
                                                <li><a href="#177" data-title="表5 8种跟踪算法平均跟踪速度比较">表5 8种跟踪算法平均跟踪速度比较</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="12">


                                    <a id="bibliography_1" title=" Zhang T Z, Xu C S, Yang M H.Multi-task correlation particle filter for robust object tracking[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4819-4827." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-task Correlation Particle Filter for Robust Object Tracking">
                                        <b>[1]</b>
                                         Zhang T Z, Xu C S, Yang M H.Multi-task correlation particle filter for robust object tracking[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4819-4827.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_2" title=" Zhao G P, Shen Y P, Wang J Y.Adaptive feature fusion object tracking based on circulant structure with kernel[J].Acta Optica Sinica, 2017, 37 (8) :0815001.赵高鹏, 沈玉鹏, 王建宇.基于核循环结构的自适应特征融合目标跟踪[J].光学学报, 2017, 37 (8) :0815001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201708025&amp;v=MDczOTFNcDQ5SFlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5dm5VN3ZLSWpYVGJMRzRIOWI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Zhao G P, Shen Y P, Wang J Y.Adaptive feature fusion object tracking based on circulant structure with kernel[J].Acta Optica Sinica, 2017, 37 (8) :0815001.赵高鹏, 沈玉鹏, 王建宇.基于核循环结构的自适应特征融合目标跟踪[J].光学学报, 2017, 37 (8) :0815001.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_3" title=" Zhang Z, Sun J, Yang L T.Tracking algorithm based on correlation filter fusing and keypoint matching[J].Acta Optica Sinica, 2019, 39 (2) :0215001.张哲, 孙瑾, 杨刘涛.融合相关滤波与关键点匹配的跟踪算法[J].光学学报, 2019, 39 (2) :0215001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902032&amp;v=MTAyMDJYVGJMRzRIOWpNclk5R1pvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5dm5VN3ZLSWo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Zhang Z, Sun J, Yang L T.Tracking algorithm based on correlation filter fusing and keypoint matching[J].Acta Optica Sinica, 2019, 39 (2) :0215001.张哲, 孙瑾, 杨刘涛.融合相关滤波与关键点匹配的跟踪算法[J].光学学报, 2019, 39 (2) :0215001.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_4" title=" Ge B Y, Zuo X Z, Hu Y J.Long-term object tracking based on feature fusion[J].Acta Optica Sinica, 2018, 38 (11) :1115002.葛宝义, 左宪章, 胡永江.基于特征融合的长时目标跟踪算法[J].光学学报, 2018, 38 (11) :1115002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811025&amp;v=MDM2MjVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeXZuVTd2S0lqWFRiTEc0SDluTnJvOUhZWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Ge B Y, Zuo X Z, Hu Y J.Long-term object tracking based on feature fusion[J].Acta Optica Sinica, 2018, 38 (11) :1115002.葛宝义, 左宪章, 胡永江.基于特征融合的长时目标跟踪算法[J].光学学报, 2018, 38 (11) :1115002.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_5" title=" Lu H C, Li P X, Wang D.Visual object tracking:a survey[J].Pattern Recognition and Artificial Intelligence, 2018, 31 (1) :61-76.卢湖川, 李佩霞, 王栋.目标跟踪算法综述[J].模式识别与人工智能, 2018, 31 (1) :61-76." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201801008&amp;v=MjY0NzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5dm5VN3ZLS0Q3WWJMRzRIOW5Ncm85RmI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Lu H C, Li P X, Wang D.Visual object tracking:a survey[J].Pattern Recognition and Artificial Intelligence, 2018, 31 (1) :61-76.卢湖川, 李佩霞, 王栋.目标跟踪算法综述[J].模式识别与人工智能, 2018, 31 (1) :61-76.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_6" title=" Bolme D S, Beveridge J R, Draper B A, &lt;i&gt;et al&lt;/i&gt;.Visual object tracking using adaptive correlation filters[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:2544-2550." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">
                                        <b>[6]</b>
                                         Bolme D S, Beveridge J R, Draper B A, &lt;i&gt;et al&lt;/i&gt;.Visual object tracking using adaptive correlation filters[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:2544-2550.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_7" title=" Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;.Exploiting the circulant structure of tracking by detection with kernels[M]//Fitzgibbon A, Lazebnik S, Perona P, &lt;i&gt;et al&lt;/i&gt;.Computer Vision-ECCV 2012.Berlin, Heidelberg:Springer, 2012, 7575:702-715." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">
                                        <b>[7]</b>
                                         Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;.Exploiting the circulant structure of tracking by detection with kernels[M]//Fitzgibbon A, Lazebnik S, Perona P, &lt;i&gt;et al&lt;/i&gt;.Computer Vision-ECCV 2012.Berlin, Heidelberg:Springer, 2012, 7575:702-715.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                     Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596.</a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     Danelljan M, H&#228;ger G, Khan F, &lt;i&gt;et al&lt;/i&gt;.Accurate scale estimation for robust visual tracking[C]//British Machine Vision Conference 2014, September 1-5, 2014, Nottingham.Durham, England, UK:BMVA Press, 2014:65.</a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_10" title=" Bertinetto L, Valmadre J, Golodetz S, &lt;i&gt;et al&lt;/i&gt;.Staple:complementary learners for real-time tracking[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1401-1409." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">
                                        <b>[10]</b>
                                         Bertinetto L, Valmadre J, Golodetz S, &lt;i&gt;et al&lt;/i&gt;.Staple:complementary learners for real-time tracking[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1401-1409.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_11" title=" Tang M, Feng J Y.Multi-kernel correlation filter for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:3038-3046." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-kernel Correlation Filter for Visual Tracking">
                                        <b>[11]</b>
                                         Tang M, Feng J Y.Multi-kernel correlation filter for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:3038-3046.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_12" title=" Zhang K H, Zhang L, Yang M H, &lt;i&gt;et al&lt;/i&gt;.Fast tracking via spatio-temporal context learning[EB/OL]. (2013-11-08) [2018-12-15].https://arxiv.org/pdf/1311.1939.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast Tracking via Spatio-Temporal Context Learning">
                                        <b>[12]</b>
                                         Zhang K H, Zhang L, Yang M H, &lt;i&gt;et al&lt;/i&gt;.Fast tracking via spatio-temporal context learning[EB/OL]. (2013-11-08) [2018-12-15].https://arxiv.org/pdf/1311.1939.pdf.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Discriminative scale space tracking[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (8) :1561-1575.</a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_14" title=" Possegger H, Mauthner T, Bischof H.In defense of color-based model-free tracking[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:2113-2120." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=In defense of color-based model-free tracking">
                                        <b>[14]</b>
                                         Possegger H, Mauthner T, Bischof H.In defense of color-based model-free tracking[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:2113-2120.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_15" title=" Wang M M, Liu Y, Huang Z Y.Large margin object tracking with circulant feature maps[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4800-4808." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large Margin Object Tracking with Circulant Feature Maps">
                                        <b>[15]</b>
                                         Wang M M, Liu Y, Huang Z Y.Large margin object tracking with circulant feature maps[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4800-4808.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_16" title=" Bian J W, Lin W Y, Matsushita Y, &lt;i&gt;et al&lt;/i&gt;.GMS:grid-based motion statistics for fast, ultra-robust feature correspondence[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:2828-2837." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GMS:Grid-based Motion Statistics for Fast,UltraRobust Feature Correspondence">
                                        <b>[16]</b>
                                         Bian J W, Lin W Y, Matsushita Y, &lt;i&gt;et al&lt;/i&gt;.GMS:grid-based motion statistics for fast, ultra-robust feature correspondence[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:2828-2837.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_17" title=" Wu Y, Lim J, Yang M H.Online object tracking:a benchmark[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2013, Portland, OR, USA.New York:IEEE, 2013:2411-2418." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online Object Tracking:A Benchmark">
                                        <b>[17]</b>
                                         Wu Y, Lim J, Yang M H.Online object tracking:a benchmark[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2013, Portland, OR, USA.New York:IEEE, 2013:2411-2418.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_18" title=" Wu Y, Lim J, Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1834-1848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">
                                        <b>[18]</b>
                                         Wu Y, Lim J, Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1834-1848.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_19" title=" Li Y, Zhu J K.A scale adaptive kernel correlation filter tracker with feature integration[M]//Agapito L, Bronstein M, Rother C.Computer Vision-ECCV 2014 Workshops.Cham:Springer, 2015, 8693:254-265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;A scale adaptive kernel correlation filter tracker with feature integration &amp;quot;">
                                        <b>[19]</b>
                                         Li Y, Zhu J K.A scale adaptive kernel correlation filter tracker with feature integration[M]//Agapito L, Bronstein M, Rother C.Computer Vision-ECCV 2014 Workshops.Cham:Springer, 2015, 8693:254-265.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_20" >
                                        <b>[20]</b>
                                     Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Learning spatially regularized correlation filters for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:4310-4318.</a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_21" title=" Galoogahi H K, Fagg A, Lucey S.Learning background-aware correlation filters for visual tracking[C]//2017 IEEE International Conference on Computer Vision (ICCV) , October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:1144-1152." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Background-Aware Correlation Filters for Visual Tracking">
                                        <b>[21]</b>
                                         Galoogahi H K, Fagg A, Lucey S.Learning background-aware correlation filters for visual tracking[C]//2017 IEEE International Conference on Computer Vision (ICCV) , October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:1144-1152.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_22" title=" Li F, Tian C, Zuo W M, &lt;i&gt;et al&lt;/i&gt;.Learning spatial-temporal regularized correlation filters for visual tracking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:4904-4913." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatial-temporal regularized correlation filters for visual tracking">
                                        <b>[22]</b>
                                         Li F, Tian C, Zuo W M, &lt;i&gt;et al&lt;/i&gt;.Learning spatial-temporal regularized correlation filters for visual tracking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:4904-4913.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_23" >
                                        <b>[23]</b>
                                     Danelljan M, Bhat G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.ECO:efficient convolution operators for tracking[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:6931-6939.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-02-25 09:21</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(06),242-255 DOI:10.3788/AOS201939.0615004            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>自适应特征选择的相关滤波跟踪算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E4%B8%87%E5%86%9B&amp;code=07922295&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘万军</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E8%99%8E&amp;code=33426137&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙虎</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9C%E6%96%87%E6%B6%9B&amp;code=25583268&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姜文涛</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%BE%BD%E5%AE%81%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6%E8%BD%AF%E4%BB%B6%E5%AD%A6%E9%99%A2&amp;code=0034851&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">辽宁工程技术大学软件学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%BE%BD%E5%AE%81%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6%E7%A0%94%E7%A9%B6%E7%94%9F%E9%99%A2&amp;code=0034851&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">辽宁工程技术大学研究生院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对相关滤波方法对快速运动与快速变形的目标跟踪稳定性较差的问题, 提出一种自适应特征选择的相关滤波跟踪算法。利用位置滤波器和颜色概率模型提取候选区域中的基础特征, 对基础特征以不同的权重分配方式进行融合, 得到多个融合特征。对融合特征进行可信度判定, 选择可信度较高的融合特征作为当前帧的跟踪特征, 估计出目标的候选位置。若最高可信度低于可信度阈值, 启动检测器重新检测目标位置, 否则候选位置即为目标最终位置。与此同时, 对目标模型进行更新, 确保模型对目标描述的准确性。在标准数据集OTB50和OTB100上进行大量实验, 测试结果表明, 所提出的跟踪方法在运动模糊、光照变化、快速运动等条件下具有较高的跟踪准确率和较好的稳健性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标跟踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%85%B3%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相关滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%A2%9C%E8%89%B2%E7%BB%9F%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">颜色统计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%BA%E5%BA%A6%E5%8F%98%E6%8D%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">尺度变换;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征选择;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    孙虎 E-mail:shzxzx@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-09</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61172144);</span>
                                <span>辽宁省自然科学基金 (20170540426);</span>
                                <span>辽宁省教育厅基金 (LJYL049, LJ2017QL034, LJ2017ZL003);</span>
                    </p>
            </div>
                    <h1><b>Correlation Filter Tracking Algorithm for Adaptive Feature Selection</b></h1>
                    <h2>
                    <span>Liu Wanjun</span>
                    <span>Sun Hu</span>
                    <span>Jiang Wentao</span>
            </h2>
                    <h2>
                    <span>School of Software, Liaoning Technical University</span>
                    <span>Graduate School, Liaoning Technical University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The conventional correlation filtering methods are known to demonstrate poor tracking stability for fast moving and fast deforming targets. Therefore, this paper proposes a correlation filter tracking algorithm for adaptive feature selection. First, the basic features are extracted in the candidate regions using a position filter and a color probability model and fused in different weight combinations to obtain multiple fusion features. Then, the credibility of the fusion features is determined and the features with relatively high credibility are selected as the tracking features of the current frame to estimate the candidate position of the target. Finally, if the maximum credibility is less than the credibility threshold, the detector is activated to redetect the target position; otherwise, the candidate position is just the final position. Meanwhile, the target model is updated to ensure the accuracy of target description. The experimental results on the standard OTB50 and OTB100 datasets show that the proposed tracking method has relatively high tracking accuracy and good robustness under the conditions of motion blurring, illumination variation, and fast motion.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=target%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">target tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=correlation%20filter&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">correlation filter;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=color%20statistics&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">color statistics;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=scale%20transformation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">scale transformation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20selection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature selection;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-09</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="58" name="58" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="59">目标跟踪是国内外计算机视觉领域的一个研究热点<citation id="180" type="reference"><link href="12" rel="bibliography" /><link href="14" rel="bibliography" /><link href="16" rel="bibliography" /><link href="18" rel="bibliography" /><link href="20" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>, 广泛应用于军事领域、民用领域等。现实场景的复杂性 (光照变化、形状变化、运动模糊等) , 使得目标跟踪的效果不太理想, 因此如何在复杂背景下提高算法的跟踪准确率是目标跟踪领域的热点问题。</p>
                </div>
                <div class="p1">
                    <p id="60">根据目标建模方式的不同, 目前跟踪方法主要分为生成式方法和判别式方法两种。生成式方法主要是计算候选目标与目标模型的相似度, 以相似度最高的候选目标作为目标进行跟踪。生成式方法仅针对目标本身的外观特征, 导致其在处理目标外观快速变化和遇到遮挡时, 跟踪效果并不好。判别式方法则是把跟踪看作一个二分类问题, 旨在构建一个有效的分类器以区分目标及其背景, 从而实现对目标的跟踪。相比于生成式方法, 判别式方法在建模时将目标与背景进行区分, 因此在目标跟踪时的效果通常表现得更为稳健, 目前已成为目标跟踪的主流跟踪方式。</p>
                </div>
                <div class="p1">
                    <p id="61">近年来, 基于相关滤波的判别式方法已广泛应用于目标跟踪领域, 并取得了显著的效果。2010年Bolme等<citation id="181" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出将相关滤波 (CF) 应用于目标跟踪领域, 该算法基于输出结果的最小均方误差 (MOSSE) 训练相关滤波器, 通过响应图像的值判断候选位置与初始化目标的相关性。响应值越大, 则相关性越大, 从而提高了滤波器的准确度。同时该算法的计算从时域转换到频域, 跟踪速率超过了600 frame/s。此后, 很多算法都在其基础上进行改进。Henriques等<citation id="182" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>于2012年提出核循环结构 (CSK) 算法, 该算法针对相关滤波中样本数量不足对滤波器的影响, 使用循环密集采样的方法, 有效地利用了整张图片的特征。为了提升算法性能, Henriques等<citation id="183" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>在相关滤波中引入核空间, 提出了核相关滤波算法 (KCF) , 在算法中采用多通道梯度方向直方图 (HOG) 特征, 通过核函数将线性空间的岭回归问题映射到非线性空间, 提高了高维特征空间中样本分类的速度。Danelljan等<citation id="184" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>针对尺度变化问题, 基于MOSSE跟踪方法提出了多尺度空间滤波跟踪算法 (DSST) , 该算法分为位置滤波器 (TF) 和尺度滤波器 (SF) 两部分, 采用判别相关滤波器来确定位置, 同时在文中提出一种精准的尺度估计方法以确定尺度信息。利用该算法进行跟踪的过程中, 两个滤波器独立工作, 分别进行目标定位和尺度评估, 取得了优异的表现。Bertinetto等<citation id="185" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>从特征融合入手, 提出一种特征互补的跟踪方法, 将HOG特征与颜色特征相融合, 有效地解决了变形情况下HOG特征稳健性较差和光照变化下颜色特征表现效果较差的问题, 提高了跟踪的准确度, 但两种特征融合增加了计算复杂度。除融合特征外, 针对人为选择的核函数未必是最优函数的问题, Tang等<citation id="186" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出了多核相关滤波器 (MKCF) , 即在核化滤波器 (KCF) 基础上引入多个核函数, 并提出了优化求解的方法。Zhang等<citation id="187" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>将多个特征进行融合, 同时在计算时引入多个核函数, 再将多个滤波器进行整合, 并通过最后的跟踪结果调整核的权重, 从而使得核有自适应选择的能力。</p>
                </div>
                <div class="p1">
                    <p id="62">虽然相关滤波算法凭借其速度的优势和良好的精度得到广泛应用, 但也存在不足之处:1) 相关滤波方法属于模板类方法, 对于快速变形和快速运动的情况跟踪效果较差, 易发生跟踪漂移现象;2) 在提取目标特征的过程中, 提取单一特征或固定权重融合特征不能够全面地描述目标, 导致跟踪的准确率较低;3) 为了提高跟踪的准确率, 一些跟踪方法采用多个滤波器相互融合来确定目标位置, 时间复杂度较高。</p>
                </div>
                <div class="p1">
                    <p id="63">针对以上问题, 本文在相关滤波跟踪的基础上, 提出一种自适应特征选择的相关滤波跟踪算法。该方法对HOG特征相关滤波器的响应值与颜色统计特征的响应值分配不同的权重, 进行线性加权运算, 得到多个具有不同偏向性的融合特征, 通过对融合特征进行可信度判定, 选取得分最高的融合特征作为跟踪特征, 保证了对目标特征描述的准确性。然后对确定的目标位置进行尺度估计和模型更新, 实现对目标的稳定跟踪。在OTB50和OTB100的标准数据序列上进行实验, 实验结果验证了所提方法的有效性和稳定性。</p>
                </div>
                <h3 id="64" name="64" class="anchor-tag">2 总体框架设计</h3>
                <div class="p1">
                    <p id="65">快速多尺度空间滤波跟踪 (fDSST) 算法<citation id="188" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>在跟踪效果和速度上都表现得较好, 因此本文算法采用fDSST算法作为基本框架, 同时为了更有效、全面地描述目标以及方便后续与其余跟踪算法的比较, 采用判别能力较强的HOG特征与颜色统计特征作为基础特征进行融合。跟踪框架主要分为建模、跟踪和更新三个阶段, 整体框架如图1所示。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">1) 建模阶段。</h4>
                <div class="p1">
                    <p id="67">根据手动标定的第一帧目标位置信息, 训练用来估计目标位置的相关滤波器, 同时训练颜色概率模型和用来估计目标尺度的相关滤波器, 最终建立目标模型。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68">2) 跟踪阶段。</h4>
                <div class="p1">
                    <p id="69">首先在当前帧的候选区域提取HOG特征和颜色特征<citation id="189" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 使用前一帧确定的目标模型对两种基础特征进行计算。然后对计算结果分配不同的权重并进行融合, 对融合结果进行可信度判断, 选取可信度最高的融合特征作为当前帧的跟踪特征, 并根据该特征估计出目标的候选位置。对于最高可信度仍低于阈值的情况, 根据两帧之间特征点的匹配结果, 判断是否启动检测器。若匹配程度较低, 启动检测器重新检测和定位目标的位置, 否则候选位置即为目标最终位置。最后使用尺度滤波器对目标的尺度进行估计, 从而完成跟踪过程。</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906030_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 整体框架示意图" src="Detail/GetImg?filename=images/GXXB201906030_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 整体框架示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906030_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Schematic of overall frame</p>

                </div>
                <h4 class="anchor-tag" id="72" name="72">3) 更新阶段。</h4>
                <div class="p1">
                    <p id="73">利用每帧跟踪阶段确定的目标位置, 对位置滤波器、颜色概率模型和尺度滤波器进行更新。</p>
                </div>
                <h3 id="74" name="74" class="anchor-tag">3 自适应特征选择的跟踪算法</h3>
                <div class="p1">
                    <p id="75">相关滤波方法最早在文献<citation id="190" type="reference">[<a class="sup">6</a>]</citation>中提出, 可应用在跟踪领域, 是近年来跟踪领域的主流方法。其基本思想是:越是相似的两个目标, 其相关值越大, 即图片中的目标与初始化目标越相似, 滤波器得到的响应值越大, 因此将滤波器响应最大的位置作为目标的位置, 从而实现跟踪。该类方法属于模板类方法, 并受边界效应的影响, 对于快速运动与快速变形的目标, 效果并不好。文献<citation id="191" type="reference">[<a class="sup">10</a>]</citation>提出融合两种互补特征的方法, 一定程度上改善了相关滤波方法对快速运动和快速变形目标跟踪的情况, 同时对光照变化的情况也能表现出较好的跟踪效果。但是该方法采用固定等权重分配特征的方式融合互补的两个特征, 对不断变化的目标以及背景没有选择适应能力, 且未对跟踪结果进行可靠性判定。在文献<citation id="192" type="reference">[<a class="sup">6</a>]</citation>和文献<citation id="193" type="reference">[<a class="sup">10</a>]</citation>的基础上, 本文提出一种自适应特征选择的跟踪算法, 通过融合不同权重的特征, 不仅有效解决了相关滤波器对于快速运动和快速变形目标效果不好的问题, 而且增强了跟踪框架对于变化的目标与背景的适应能力。同时对跟踪效果进行可信度判断, 可信度较低时启动检测器进行重新检测, 重新定位目标的位置, 从而有效地提高了跟踪的准确率。</p>
                </div>
                <div class="p1">
                    <p id="76">在本文中, 位置滤波器和颜色概率模型用于提取样本特征, 得到相对应的响应图。然后通过对两个响应图以线性加权的方式进行组合, 得到最终的目标位置响应图。其组合公式为</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>e</mtext></mrow></msub><mo>=</mo><mi>ω</mi><mi>f</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>l</mtext><mtext>o</mtext><mtext>r</mtext></mrow></msub><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>ω</mi><mo stretchy="false">) </mo><mi>f</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>f</mtext></mrow></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">式中:<i>f</i><sub>fuse</sub>为融合特征的得分响应函数;<i>ω</i>为权重分配因子;<i>f</i><sub>color</sub>为颜色概率模型的得分响应函数;<i>f</i><sub>cf</sub>为位置滤波器的得分响应函数。通过对两个得分函数进行线性加权, 最终得到融合特征的响应函数。</p>
                </div>
                <div class="p1">
                    <p id="79">通过使用不同的权重分配因子<i>ω</i>, 使融合出的特征具有不同的特征偏向性, 以应对变化的目标及背景信息。大量实验证明:若权重分配的偏向性太大, 则会使跟踪效果和单一特征的跟踪效果相近, 不能体现出融合特征的必要性;若权重分配的偏向性太小, 则导致跟踪效果与等权重分配方式的跟踪效果相近, 不能体现本文选择特征的有效性。因此本文采用实验中表现最为合适的权重分配方式进行特征融合。融合特征的权重分配方式见表1。</p>
                </div>
                <div class="area_img" id="80">
                    <p class="img_tit">表1 融合特征多权重分配方式 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Multi-weight distribution of fusion features</p>
                    <p class="img_note"></p>
                    <table id="80" border="1"><tr><td><br />Number</td><td>Filter weight</td><td>Color weight</td><td>Total weight</td></tr><tr><td><br />1</td><td>0.7</td><td>0.3</td><td>1</td></tr><tr><td><br />2</td><td>0.3</td><td>0.7</td><td>1</td></tr><tr><td><br />3</td><td>0.5</td><td>0.5</td><td>1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="81">通过融合不同权重的特征得到不同的融合特征响应图, 对融合后得到的响应图的可信度进行判断, 取可信度最大的特征作为当前帧的跟踪特征。采用文献<citation id="194" type="reference">[<a class="sup">15</a>]</citation>提出的平均峰值相关能量 (APCE) 计算跟踪的可信度, 计算公式为</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">|</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>w</mi><mo>, </mo><mi>h</mi></mrow></munder><mo stretchy="false"> (</mo></mstyle><mi>F</mi><msub><mrow></mrow><mrow><mi>w</mi><mo>, </mo><mi>h</mi></mrow></msub><mo>-</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>]</mo></mrow></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">式中:<i>F</i><sub>max</sub>、<i>F</i><sub>min</sub>、<i>F</i><sub><i>w</i>, <i>h</i></sub>分别代表特征响应图中的最大值、最小值以及对应 (<i>w</i>, <i>h</i>) 位置的元素值;<i>E</i><sub>APCE</sub>值反映响应图的波动程度和检测目标的置信水平。<i>E</i><sub>APCE</sub>值越大, 表明响应图除峰值之外的区域波动较为平滑, 目标的峰值响应相对背景的响应越高, 目标的可信度越高;反之, 目标的可信度越低。实验中使用不同权重分配方式对特征进行融合, 计算每种方式的<i>E</i><sub>APCE</sub>值, 取最大值的融合特征作为当前帧的跟踪特征, 所选择的跟踪特征以及检测跟踪过程如图2所示。图中, <i>w</i><sub>hog</sub>表示位置滤波器的权重, <i>w</i><sub>color</sub>表示颜色概率模型的权重, <i>P</i><sub><i>t</i></sub>代表第<i>t</i>帧的目标的中心位置, <i>M</i><sub>1</sub>、<i>M</i><sub>2</sub>、 <i>M</i><sub>3</sub>表示在不同权重分配方式下的可信度。</p>
                </div>
                <div class="area_img" id="84">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906030_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 自适应选择检测跟踪示意图" src="Detail/GetImg?filename=images/GXXB201906030_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 自适应选择检测跟踪示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906030_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Schematic of adaptive selection detection tracking</p>

                </div>
                <div class="p1">
                    <p id="85">由于图2中的检测跟踪结果可能会因各种因素的影响出现跟踪漂移, 实验设定了一个重检测阈值来判断是否需要对检测跟踪结果进行重新检测。重检测阈值表示可信度历史均值的比例, 若重检测阈值设置过大, 则会导致目标可信度的可波动范围较小, 检测器会误判目标的正常变化, 因此需要激活重检测机制, 这样就增加了总体跟踪过程的时间复杂度;反之, 可信度的可波动范围较大, 检测器会漏判目标的异常变化, 此时不需要激活重检测机制, 这样就降低了整体跟踪的准确率。本文设定重检测阈值<i>θ</i><sub>APCE</sub>=0.6<i>F</i><sub>mean</sub>, 其中<i>F</i><sub>mean</sub>表示<i>E</i><sub>APCE</sub>值的历史均值。Δ<i>F</i><sub>APCE</sub>代表当前帧的历史均值与最大<i>E</i><sub>APCE</sub>值的差值, 表示当前帧目标候选区域与历史目标区域峰值的波动情况。当Δ<i>F</i><sub>APCE</sub>的值大于重检测阈值时, 表明融合特征定位的目标区域的峰值波动发生突变, 可能产生了漂移的情况, 启动网格运动统计 (GMS) 检测器对目标进行判断重检测。当Δ<i>F</i><sub>APCE</sub>的值小于重检测阈值时, 更新滤波器模型以及颜色概率模型。针对目标的尺度变化问题, 采用文献<citation id="195" type="reference">[<a class="sup">13</a>]</citation>中的方法, 在候选区域构建目标尺度金字塔, 使用尺度滤波器得到最佳的目标尺度。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86"><b>3.1 位置滤波器与颜色概率模型</b></h4>
                <div class="p1">
                    <p id="87">位置滤波器与颜色概率模型主要是用来提取特征, 以获取相应的响应图。其中位置滤波器主要用于提取HOG特征, 颜色概率模型主要用于统计搜索区域内的颜色特征。HOG特征用于快速运动和快速变形的情况时效果不好, 但用于运动模糊光照变化等情况时比较好;而颜色统计特征不属于相关滤波框架, 不受边界效应的影响, 用于快速运动和快速变形的情况时效果较好, 但光照变化对其影响较大。因此, 选择这两种互补的特征作为融合特征的基础特征。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88">3.1.1 位置滤波器</h4>
                <div class="p1">
                    <p id="89">位置滤波器主要通过使用fDSST算法提取搜索区域中的HOG特征, 并使用相关滤波器将其转化为响应图。在响应图中, 峰值代表该位置的响应分数最高, 表示该位置是目标位置的可能性最大。fDSST算法是对DSST算法的改进, 本文借鉴该算法对目标进行HOG特征的提取。针对目标区域提取<i>d</i>维特征, 记作<i>f</i><sub><i>l</i></sub>, <i>l</i>∈{1, 2, …, <i>d</i>}, <i>l</i>表示特征通道, 通过建立最小化代价函数, 构建最优的相关滤波器<i>h</i><sub><i>l</i></sub>。目标函数的公式为</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ε</mi><mo>=</mo><mspace width="0.25em" /><mo stretchy="false">∥</mo><mi>g</mi><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mi>h</mi></mstyle><msub><mrow></mrow><mi>l</mi></msub><mo>★</mo><mi>f</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi>h</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">式中:★表示循环相关;<i>g</i>表示期望的响应分数, 由高斯函数产生; (3) 式中的第二项表示权重参数为<i>λ</i>的正则化项, <i>f</i><sub><i>l</i></sub>、<i>h</i><sub><i>l</i></sub>与<i>g</i>的维数相同, 大小也相同。</p>
                </div>
                <div class="p1">
                    <p id="92">为减少卷积计算耗费的时间, 基于Parseval公式将 (3) 式转换到傅里叶域进行计算, 得到最优化滤波器为</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><msub><mrow></mrow><mi>l</mi></msub><mo>=</mo><mfrac><mrow><mover accent="true"><mi>G</mi><mo>¯</mo></mover><mi>F</mi><msub><mrow></mrow><mi>l</mi></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mover accent="true"><mi>F</mi><mo>¯</mo></mover></mstyle><msub><mrow></mrow><mi>k</mi></msub><mi>F</mi><mo>+</mo><mi>λ</mi></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><mi>l</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>d</mi><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">式中:<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>G</mi><mo>¯</mo></mover></math></mathml>表示g经傅里叶变换后的复共轭;<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>F</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>表示f<sub>l</sub>经傅里叶变换后的复共轭。 (4) 式中的大写字母对应 (3) 式中的相应变量的离散傅里叶变换 (<i>DFT</i>) , 在跟踪过程中, 根据每帧的跟踪结果决定是否进行模型更新。为确保模型能得到稳健的近似结果, 采用如下策略更新相关滤波器模型:</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>A</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mover accent="true"><mi>G</mi><mo>¯</mo></mover><mi>U</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mspace width="0.25em" /><mi>l</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>∼</mo></mover><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>B</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>η</mi><mo stretchy="false">) </mo><mi>B</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>η</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>∼</mo></mover></mrow></munderover><mover accent="true"><mi>F</mi><mo>¯</mo></mover></mstyle><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mi>F</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">式中: t表示目标序列的帧数;<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>F</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>表示傅里叶域第t帧、第k维特征的复共轭;U<sub>t</sub>=<font face="EU-HT">F</font>{<b><i>P</i></b><sub><i>t</i></sub><i>u</i><sub><i>t</i></sub>}, 其中<i>u</i><sub><i>t</i></sub>是一个目标模板, <i>u</i><sub><i>t</i></sub>= (1-<i>η</i>) <i>u</i><sub><i>t</i>-1</sub>+<i>ηf</i><sub><i>t</i></sub>, 用于减少快速傅里叶变换 (FFT) 需要的计算量, <b><i>P</i></b><sub><i>t</i></sub>是通过最小化重构目标模板<i>u</i><sub><i>t</i></sub>构造出的投影矩阵, 其大小为<mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>∼</mo></mover><mo>×</mo><mi>d</mi></mrow></math></mathml>, 作用是将提取到的特征投影到低维子空间, <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>∼</mo></mover></mrow></math></mathml>表示特征经过压缩后的特征维度;F<sub>t</sub>=<font face="EU-HT">F</font>{<b><i>P</i></b><sub><i>t</i></sub><i>f</i><sub><i>t</i></sub>};<i>η</i>表示学习率, 作为模型的更新参数。 (5) 式和 (6) 式分别表示 (4) 式中分子和分母部分的更新策略。</p>
                </div>
                <div class="p1">
                    <p id="102">在第<i>t</i>帧图像中, 参照目标在<i>t</i>-1帧的位置, 在指定的背景区域提取不同维度的特征样本, 对样本进行压缩后使用相关滤波器得到<i>z</i><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, 对z<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>进行<i>FFT</i>得到Z<sub>t</sub>=<font face="EU-HT">F</font>{<b><i>P</i></b><sub><i>t</i>-1</sub><i>z</i><sub><i>t</i></sub>}, 最终得到的响应分数为</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>∼</mo></mover></mrow></munderover><mover accent="true"><mi>A</mi><mo>¯</mo></mover></mstyle><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mi>Ζ</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mrow><mi>B</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>λ</mi></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">式中:<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>A</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>表示第t-1帧、第l维特征的复共轭。对得到的响应分数R<sub>t</sub>进行傅里叶逆变换 (<i>IFFT</i>) 得到相关滤波器的响应值r<sub>t</sub>, 响应值越大, 则越有可能是目标的位置。r<sub>t</sub>的计算式为</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">{</mo><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">}</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">式中:<font face="EU-HT">F</font><sup>-1</sup>表示傅里叶逆变换。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110">3.1.2 颜色概率模型</h4>
                <div class="p1">
                    <p id="111">颜色概率模型主要用于将目标的前景与背景分开, 通过统计前景目标和背景区域的颜色直方图并进行归一化, 判别每个像素属于前景和背景的概率, 得到像素级的前景概率响应图, 前景响应分数越高的位置, 越有可能是目标的位置。响应图的得分计算式为</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">s</mi><mo stretchy="false"> (</mo><mi>Φ</mi><mo>;</mo><mi mathvariant="bold-italic">α</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>Μ</mi><mo stretchy="false">|</mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>Μ</mi></mrow></munder><mi mathvariant="bold-italic">α</mi></mstyle><msup><mrow></mrow><mtext>Τ</mtext></msup><mi>Φ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">式中:<i>M</i>为待计算区域提取出的像素个数;<i>α</i>为模型参数;<i>Φ</i> (<i>x</i>) 为第<i>x</i>帧中像素网格内统计颜色特征的函数。通过提取并统计前景和背景网格中的颜色特征, 使用 (9) 式计算出像素的响应分数, 从而得到目标的颜色概率响应图。每帧确定跟踪目标位置后, 采用最小化损失函数的方式更新模型。该模型的损失函数为</p>
                </div>
                <div class="p1">
                    <p id="114" class="code-formula">
                        <mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">α</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>Ο</mi><mo stretchy="false">|</mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>Ο</mi></mrow></munder><mo stretchy="false">[</mo></mstyle><mi mathvariant="bold-italic">α</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi>Φ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>B</mi></mrow></munder><mo stretchy="false">[</mo></mstyle><mi mathvariant="bold-italic">α</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi>Φ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="115">式中:<i>O</i>表示模型所提取的目标区域的图像个数;<i>B</i>表示模型所提取的背景区域的图像个数。 (10) 式由两项组成, 前一项表示目标区域的损失, 后一项表示背景区域的损失。在模型中使用的颜色特征为RGB, 统计直方图得分可以使用平均投票的方式。在损失函数中, 使用每个图像的目标函数对每个特征像素进行线性回归。对于模型参数<i>α</i>, 采用以下策略解决相关岭回归问题:</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>α</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>arg</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">α</mi></munder><mrow><mo>{</mo><mrow><mi>L</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">α</mi><mo stretchy="false">) </mo><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">α</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>}</mo></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>α</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>ρ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi>Ο</mi><mo stretchy="false">) </mo></mrow><mrow><mi>ρ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi>Ο</mi><mo stretchy="false">) </mo><mo>+</mo><mi>ρ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi>B</mi><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>m</mi><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">式中:<i>ρ</i><sup> (<i>j</i>) </sup> (<i>O</i>) 为目标区域中特征<i>j</i>的非零区域中像素比例;<i>ρ</i><sup> (<i>j</i>) </sup> (<i>B</i>) 为背景区域中特征<i>j</i>的非零区域中像素比例;<i>j</i>为特征的维度;<i>λ</i>为模型的权重参数。</p>
                </div>
                <div class="p1">
                    <p id="118">跟踪第<i>t</i>帧时, 当学习率为<i>η</i>, 采用以下策略在线更新像素比例:</p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>ρ</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>Ο</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>η</mi><mo stretchy="false">) </mo><mi>ρ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>Ο</mi><mo stretchy="false">) </mo><mo>+</mo><mi>η</mi><msup><mi>ρ</mi><mo>′</mo></msup><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>Ο</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>ρ</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>B</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>η</mi><mo stretchy="false">) </mo><mi>ρ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>B</mi><mo stretchy="false">) </mo><mo>+</mo><mi>η</mi><msup><mi>ρ</mi><mo>′</mo></msup><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>B</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">式中:<i>ρ</i><sub><i>t</i></sub> (<i>O</i>) 为颜色概率模型中第<i>t</i>帧目标区域的更新策略;<i>ρ</i>′<sub><i>t</i></sub> (<i>O</i>) 为第<i>t</i>帧计算目标区域的直方图得分;<i>ρ</i><sub><i>t</i></sub> (<i>B</i>) 为颜色概率模型中第<i>t</i>帧背景区域的更新策略;<i>ρ</i>′<sub><i>t</i></sub> (<i>B</i>) 为第<i>t</i>帧计算背景区域的直方图得分。</p>
                </div>
                <h4 class="anchor-tag" id="121" name="121"><b>3.2 基于GMS的目标检测器</b></h4>
                <div class="p1">
                    <p id="122">在可信度策略的基础上, 针对最高可信度仍然较低的情况, 提出了一种基于GMS特征点匹配<citation id="196" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>的目标检测器。检测器首先通过特征点匹配判断是否进行重检测, 在重检测时将利用当前帧计算得出的目标位置与前一帧目标的预测位置进行比较, 选择可信度最高的位置作为当前帧目标的最终位置, 实现对目标的跟踪。以序列DragonBaby中的第5帧为例, 目标重检测过程示意图如图3所示, 图中的检测候选位置为<i>P</i><sub>0</sub> (107.8, 219.1) , 特征点对数为178, 5个预测区域的可信度集合为{3.27, 2.89, 3.14, 4.09, 3.18}, 可信度最高的位置为<i>P</i><sub>1</sub> (108.3, 205.2) , <i>P</i><sub>1</sub>为重检测确定的目标的最终跟踪位置。</p>
                </div>
                <div class="area_img" id="123">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906030_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 重检测过程示意图" src="Detail/GetImg?filename=images/GXXB201906030_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 重检测过程示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906030_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Schematic of redetection process</p>

                </div>
                <div class="p1">
                    <p id="124">GMS特征点匹配是文献<citation id="197" type="reference">[<a class="sup">16</a>]</citation>提出的一种基于网格运动统计的快速、超稳健的特征匹配算法, 其主要思想是:首先进行ORB (Oriented FAST and Rotated BRIEF) 特征匹配, 得到特征匹配点, 然后使用GMS方法对匹配结果进行过滤, 消除错误的特征匹配点对, 保留匹配成功的特征点。为了减少邻域之间像素的干扰, 增加特征匹配结果的差异, 构建了一种基本平滑运动核, 其大小为3×3, 每个网格的匹配统计值<i>S</i><sub><i>i</i></sub>满足二项分布, 每个核的匹配统计值计算式为</p>
                </div>
                <div class="p1">
                    <p id="125" class="code-formula">
                        <mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mrow><mi>a</mi><mo>, </mo><mi>b</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>u</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mo stretchy="false">|</mo></mstyle><mi>X</mi><msub><mrow></mrow><mrow><mi>a</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo></mrow></msup><mi>b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo></mrow></msup></mrow></msub><mo stretchy="false">|</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="126">式中:<i>K</i>=9为平滑运动核的网格个数;|<i>X</i><sub><i>a</i><sup> (<i>k</i>) </sup><i>b</i><sup> (<i>k</i>) </sup></sub>|为区域<i>a</i>与区域<i>b</i>中匹配成功的特征点对数。该算法设定判定阈值<mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>τ</mi><mo>=</mo><mi>m</mi><msub><mrow></mrow><mi>f</mi></msub><mo>+</mo><mi>δ</mi><mi>s</mi><msub><mrow></mrow><mi>f</mi></msub><mo>≈</mo><mi>δ</mi><mi>s</mi><msub><mrow></mrow><mi>f</mi></msub><mo>≈</mo><mi>δ</mi><msqrt><mi>n</mi></msqrt><mo stretchy="false"> (</mo><mi>δ</mi><mo>=</mo><mn>6</mn><mo stretchy="false">) </mo><mo>, </mo><mi>δ</mi></mrow></math></mathml>表示设定阈值的平衡常数。其中m<sub>f</sub>为错误匹配的对数均值, s<sub>f</sub>为方差, 每个核的匹配结果为</p>
                </div>
                <div class="p1">
                    <p id="128" class="code-formula">
                        <mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>e</mtext><mtext>l</mtext><mtext>l</mtext><mo>-</mo><mtext>p</mtext><mtext>a</mtext><mtext>i</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false">{</mo><mi>a</mi><mo>, </mo><mi>b</mi><mo stretchy="false">}</mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mtext>Τ</mtext></mtd><mtd columnalign="left"><mi>S</mi><msub><mrow></mrow><mrow><mi>a</mi><mo>, </mo><mi>b</mi></mrow></msub><mo>&gt;</mo><mi>τ</mi><msub><mrow></mrow><mi>a</mi></msub><mo>=</mo><mi>δ</mi><msqrt><mrow><mi>n</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></msqrt></mtd></mtr><mtr><mtd columnalign="left"><mtext>F</mtext></mtd><mtd columnalign="left"><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="129">式中:<i>T</i>表示成功匹配;<i>F</i>表示错误匹配。</p>
                </div>
                <div class="p1">
                    <p id="130">通过对当前帧可信度较高的目标区域与前一帧的目标区域进行特征点匹配, 得到匹配成功的特征点对数, 匹配成功点越多, 表示两帧之间的目标区域越相似, 匹配程度越高;反之, 则匹配程度越低。实验中设定特征点总数为S<sub><i>point</i></sub>, 匹配阈值为Y<sub><i>theshold</i></sub>, 匹配阈值为特征点总数的1/2。当匹配成功的特征点数大于阈值时, 表明两帧之间的相似程度高于50%, 虽然可信度较低, 但是模型定位的区域仍是目标区域, 对目标进行尺度估计以及整体模型的更新。当匹配成功的特征点数小于阈值时, 说明两帧之间的相似程度低于50%, 此时激活检测器, 重新检测目标区域。</p>
                </div>
                <div class="p1">
                    <p id="131">图4为以序列<i>Bird</i>2为例模拟得到的目标运动轨迹。结合图4中目标的运动轨迹可以看出, 目标运动是平滑渐进的过程, 所以两帧之间目标位置移动的距离不会呈阶跃性的变化。因此, 根据前一帧的目标中心点位置, 在其不同方向上设置预测区域作为重检测时的目标预测区域。同时考虑可信度较低以及匹配程度较低的原因可能在于:1) 漂移的发生, 导致目标丢失;2) 对目标的定位不够准确。因此将模型在当前帧定位的可信度最大但低于可信度阈值的目标位置也作为预测位置, 以实现对目标的准确定位。本文共设置5个预测区域, 预测区域集合为N={N<sub>1</sub>, N<sub>2</sub>, N<sub>3</sub>, N<sub>4</sub>, N<sub>5</sub>}, 其中N<sub>5</sub>表示当前帧候选目标位置确定的预测区域, N<sub>1</sub>, N<sub>2</sub>, N<sub>3</sub>, N<sub>4</sub>表示根据前一帧的目标位置设置4个预测区域, 分别对应4个方向, 预测区域的构建过程如图5所示。</p>
                </div>
                <div class="p1">
                    <p id="132">图5中, O<sub><i>center</i></sub>为前一帧的目标中心点, 预测区域的中心点分别为A<sub>0</sub>、B<sub>0</sub>、C<sub>0</sub>和D<sub>0</sub>, 前一帧的目标尺寸记作w<sub><i>prior</i></sub>×h<sub><i>prior</i></sub>。以A<sub>0</sub>为例, A<sub>0</sub>是通过O<sub><i>center</i></sub>先向右平移φ×w<sub><i>prior</i></sub>的距离, 然后再向上平移φ×h<sub><i>prior</i></sub>的距离得到。B<sub>0</sub>、C<sub>0</sub>与D<sub>0</sub>的获得过程与A<sub>0</sub>类似, 只是平移方向不同。根据实验效果, φ=1/4时效果最好。图5中所示的红色框是以O<sub><i>center</i></sub>为中心的搜索区域, 黄色框是以A<sub>0</sub>为中心的搜索区域, 虚线框表示A<sub>0</sub>、B<sub>0</sub>、C<sub>0</sub>、D<sub>0</sub> 4个点共同确定的搜索区域。</p>
                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906030_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 目标运动轨迹图" src="Detail/GetImg?filename=images/GXXB201906030_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 目标运动轨迹图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906030_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Trajectory of object motion</p>

                </div>
                <div class="area_img" id="134">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906030_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 预测区域构造示意图" src="Detail/GetImg?filename=images/GXXB201906030_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 预测区域构造示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906030_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Schematic of predicted area</p>

                </div>
                <div class="p1">
                    <p id="135">使用当前帧的融合特征组合方式导致可信度较低, 因此在对预测区域进行检测时, 采用的融合特征融合方式中不包括当前帧已经使用过的融合方式, 增强了检测器对偶然因素形成的可信度较高的特征的抗干扰能力。对于每个预测区域, 使用位置滤波器与颜色概率模型进行特征提取, 通过分配不同的权重得到融合特征, 选取最大的可信度作为该预测区域的可信度。预测区域可信度集合表示为<i>Q</i>={<i>Q</i><sub>1</sub>, <i>Q</i><sub>2</sub>, <i>Q</i><sub>3</sub>, <i>Q</i><sub>4</sub>, <i>Q</i><sub>5</sub>}, 取可信度集合中数值最大的预测区域为目标所在的区域, 同时使用该预测区域中选择的特征对目标的位置进行定位, 找到目标的中心位置, 实现跟踪。<i>Q</i>集合中最大可信度低于阈值<i>θ</i><sub>APCE</sub>时, 表明该区域的特征不足以表征目标, 因此对当前帧不进行模型更新, 以确保整体模型对目标描述的准确性。</p>
                </div>
                <div class="p1">
                    <p id="136">对序列DragonBaby的前10帧的位置数据进行采集, 结果如表2所示。通过对比表2中检测位置、重检测位置以及目标实际位置可以看出, 进行重新检测后, 得到的目标位置与实际位置接近, 说明了本文的重检测机制的必要性以及目标检测器的有效性。</p>
                </div>
                <div class="area_img" id="137">
                    <p class="img_tit">表2 目标中心位置比较 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Comparison of central positions of targets</p>
                    <p class="img_note"></p>
                    <table id="137" border="1"><tr><td><br />Frame</td><td>Detection position</td><td>Feature point</td><td>Redetection position</td><td>Actual position</td></tr><tr><td><br />1</td><td> (115.5, 188.0) </td><td>-</td><td>-</td><td> (115.5, 188.0) </td></tr><tr><td><br />2</td><td> (115.0, 195.2) </td><td>-</td><td>-</td><td> (116.5, 193.5) </td></tr><tr><td><br />3</td><td> (112.0, 202.5) </td><td>-</td><td>-</td><td> (110.5, 201.0) </td></tr><tr><td><br />4</td><td> (107.9, 203.7) </td><td>-</td><td>-</td><td> (107.5, 202.5) </td></tr><tr><td><br />5</td><td> (107.8, 219.1) </td><td>178</td><td> (108.3, 205.2) </td><td> (109.5, 207.0) </td></tr><tr><td><br />6</td><td> (102.5, 212.4) </td><td>198</td><td> (101.3, 215.7) </td><td> (99.5, 217.5) </td></tr><tr><td><br />7</td><td> (93.5, 212.7) </td><td>226</td><td> (91.1, 213.8) </td><td> (89.5, 215.5) </td></tr><tr><td><br />8</td><td> (79.1, 202.8) </td><td>120</td><td> (68.2, 194.0) </td><td> (66.0, 194.5) </td></tr><tr><td><br />9</td><td> (66.0, 188.8) </td><td>-</td><td>-</td><td> (67.5, 188.5) </td></tr><tr><td><br />10</td><td> (79.1, 182.3) </td><td>-</td><td>-</td><td> (76.5, 184.5) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="138" name="138"><b>3.3 多尺度滤波</b></h4>
                <div class="p1">
                    <p id="139">针对目标跟踪过程中的尺度变化问题, 在确定目标位置后, 使用尺度相关滤波器对目标的尺度变换进行估计, 构建目标尺度金字塔。尺度滤波器模型的期望响应函数是高斯函数, 使用相关滤波器对尺度池中不同的尺度进行评估, 响应分数最大的尺度即为目标当前帧的最佳尺度, 构建尺度池的方法为</p>
                </div>
                <div class="p1">
                    <p id="140" class="code-formula">
                        <mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>S</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext><mtext>e</mtext></mrow></msub><mo>=</mo><mi>σ</mi><msup><mrow></mrow><mi>n</mi></msup><mo>×</mo><mi>W</mi><mo>×</mo><mi>σ</mi><msup><mrow></mrow><mi>n</mi></msup><mo>×</mo><mi>Η</mi><mo>, </mo></mtd></mtr><mtr><mtd><mi>n</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mrow><mo>[</mo><mrow><mo>-</mo><mfrac><mrow><mi>S</mi><mo>-</mo><mn>1</mn></mrow><mn>2</mn></mfrac></mrow><mo>]</mo></mrow><mo>, </mo><mo>⋯</mo><mo>, </mo><mrow><mo>[</mo><mrow><mfrac><mrow><mi>S</mi><mo>-</mo><mn>1</mn></mrow><mn>2</mn></mfrac></mrow><mo>]</mo></mrow></mrow><mo>}</mo></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="141">式中:<i>W</i>×<i>H</i>为前一帧目标的大小;<i>σ</i>为尺度因子;<i>n</i>为比例滤波器的大小;<i>S</i>为压缩后的特征维数;<i>S</i><sub>scale</sub>为尺度响应值。</p>
                </div>
                <div class="p1">
                    <p id="142">尺度滤波器与位置滤波器的滤波过程相似, 采用一维高斯函数作为期望函数, 将尺度滤波器<i>h</i><sub>scale</sub>与尺度训练样本<i>f</i><sub>scale</sub>代入 (3) 式, 然后在傅里叶域进行计算, 得到17维的尺度响应向量, 通过对17维的尺度向量进行内插操作, 得到33维的响应向量, 响应值最大的尺度即为目标的最佳尺度。为提高计算和存储效率, 并未像位置滤波器中通过构建自相关矩阵的方式获得尺度投影矩阵<b><i>P</i></b><sub>scale</sub>, 而是通过对<i>f</i><sub>scale</sub>进行正交三角 (QR Decomposition) 分解得到, 这样处理不会影响跟踪结果的输出。表3中的数据表示序列CarScale的部分帧的尺度估计信息, 并将尺度滤波器选取的最佳尺度与实际尺度进行了比较, 从表中数据可以看出, 最佳估计尺度与目标的实际尺度差距很小, 表明本文的尺度估计方法能够适应目标的尺度变化。</p>
                </div>
                <div class="area_img" id="143">
                    <p class="img_tit">表3 估计尺度与实际尺度信息表 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Estimated scale and actual scale information</p>
                    <p class="img_note"></p>
                    <table id="143" border="1"><tr><td rowspan="2"><br />Estimated scale</td><td colspan="7"><br />Frame</td></tr><tr><td>22</td><td>32</td><td>42</td><td>52</td><td>62</td><td>72</td><td>82</td></tr><tr><td>1</td><td> (44, 27) </td><td> (45, 28) </td><td> (46, 29) </td><td> (47, 29) </td><td> (48, 30) </td><td> (51, 32) </td><td> (53, 33) </td></tr><tr><td><br />2</td><td> (45, 28) </td><td> (46, 29) </td><td> (47, 29) </td><td> (48, 30) </td><td> (49, 30) </td><td> (52, 32) </td><td> (54, 34) </td></tr><tr><td><br />3</td><td> (45, 28) </td><td> (47, 29) </td><td> (48, 30) </td><td> (49, 30) </td><td> (50, 31) </td><td> (53, 33) </td><td> (55, 34) </td></tr><tr><td><br />4</td><td> (46, 29) </td><td> (48, 30) </td><td> (49, 30) </td><td> (50, 31) </td><td> (51, 32) </td><td> (54, 34) </td><td> (57, 35) </td></tr><tr><td><br />5</td><td> (47, 29) </td><td> (49, 30) </td><td> (50, 31) </td><td> (51, 32) </td><td> (52, 32) </td><td> (55, 34) </td><td> (58, 36) </td></tr><tr><td><br />6</td><td> (48, 30) </td><td> (50, 31) </td><td> (51, 32) </td><td> (52, 32) </td><td> (53, 33) </td><td> (57, 35) </td><td> (59, 36) </td></tr><tr><td><br />7</td><td> (49, 30) </td><td> (51, 32) </td><td> (52, 32) </td><td> (53, 33) </td><td> (54, 34) </td><td> (58, 36) </td><td> (60, 37) </td></tr><tr><td><br />8</td><td> (50, 31) </td><td> (52, 32) </td><td> (53, 33) </td><td> (54, 34) </td><td> (55, 34) </td><td> (59, 36) </td><td> (61, 38) </td></tr><tr><td><br />9</td><td> (51, 32) </td><td> (53, 33) </td><td> (54, 34) </td><td> (55, 34) </td><td> (57, 35) </td><td> (60, 37) </td><td> (62, 39) </td></tr><tr><td><br />10</td><td> (52, 32) </td><td> (54, 34) </td><td> (55, 34) </td><td> (57, 35) </td><td> (58, 36) </td><td> (61, 38) </td><td> (64, 39) </td></tr><tr><td><br />11</td><td> (53, 33) </td><td> (55, 34) </td><td> (57, 35) </td><td> (58, 36) </td><td> (59, 36) </td><td> (62, 39) </td><td> (65, 40) </td></tr><tr><td><br />12</td><td> (54, 34) </td><td> (57, 35) </td><td> (58, 36) </td><td> (59, 36) </td><td> (60, 37) </td><td> (64, 39) </td><td> (66, 41) </td></tr><tr><td><br />13</td><td> (55, 34) </td><td> (58, 36) </td><td> (59, 36) </td><td> (60, 37) </td><td> (61, 38) </td><td> (65, 40) </td><td> (68, 42) </td></tr><tr><td><br />14</td><td> (57, 35) </td><td> (59, 36) </td><td> (60, 37) </td><td> (61, 38) </td><td> (62, 39) </td><td> (66, 41) </td><td> (69, 43) </td></tr><tr><td><br />15</td><td> (58, 36) </td><td> (60, 37) </td><td> (61, 38) </td><td> (62, 39) </td><td> (64, 39) </td><td> (68, 42) </td><td> (70, 44) </td></tr><tr><td><br />16</td><td> (59, 36) </td><td> (61, 38) </td><td> (62, 39) </td><td> (64, 39) </td><td> (65, 40) </td><td> (69, 43) </td><td> (72, 44) </td></tr><tr><td><br />17</td><td> (60, 37) </td><td> (62, 39) </td><td> (64, 39) </td><td> (65, 40) </td><td> (66, 41) </td><td> (70, 44) </td><td> (73, 45) </td></tr><tr><td><br />18</td><td> (32, 20) </td><td> (33, 21) </td><td> (34, 21) </td><td> (34, 21) </td><td> (35, 22) </td><td> (37, 23) </td><td> (39, 24) </td></tr><tr><td><br />19</td><td> (32, 20) </td><td> (34, 21) </td><td> (34, 21) </td><td> (35, 22) </td><td> (36, 22) </td><td> (38, 24) </td><td> (40, 25) </td></tr><tr><td><br />20</td><td> (33, 21) </td><td> (34, 21) </td><td> (35, 22) </td><td> (36, 22) </td><td> (37, 23) </td><td> (39, 24) </td><td> (40, 25) </td></tr><tr><td><br />21</td><td> (34, 21) </td><td> (35, 22) </td><td> (36, 22) </td><td> (37, 23) </td><td> (37, 23) </td><td> (40, 25) </td><td> (41, 25) </td></tr><tr><td><br />22</td><td> (34, 21) </td><td> (36, 22) </td><td> (37, 23) </td><td> (37, 23) </td><td> (38, 24) </td><td> (40, 25) </td><td> (42, 26) </td></tr><tr><td><br />23</td><td> (35, 22) </td><td> (37, 23) </td><td> (37, 23) </td><td> (38, 24) </td><td> (39, 24) </td><td> (41, 25) </td><td> (43, 27) </td></tr><tr><td><br />24</td><td> (36, 22) </td><td> (37, 23) </td><td> (38, 24) </td><td> (39, 24) </td><td> (40, 25) </td><td> (42, 26) </td><td> (44, 27) </td></tr><tr><td><br />25</td><td> (37, 23) </td><td> (38, 24) </td><td> (39, 24) </td><td> (40, 25) </td><td> (40, 25) </td><td> (43, 27) </td><td> (45, 28) </td></tr><tr><td><br />26</td><td> (37, 23) </td><td> (39, 24) </td><td> (40, 25) </td><td> (40, 25) </td><td> (41, 25) </td><td> (44, 27) </td><td> (45, 28) </td></tr><tr><td><br />27</td><td> (38, 24) </td><td> (40, 25) </td><td> (40, 25) </td><td> (41, 25) </td><td> (42, 26) </td><td> (45, 28) </td><td> (46, 29) </td></tr><tr><td><br />28</td><td> (39, 24) </td><td> (40, 25) </td><td> (41, 25) </td><td> (42, 26) </td><td> (43, 27) </td><td> (45, 28) </td><td> (47, 29) </td></tr><tr><td><br />29</td><td> (40, 25) </td><td> (41, 25) </td><td> (42, 26) </td><td> (43, 27) </td><td> (44, 27) </td><td> (46, 29) </td><td> (48, 30) </td></tr><tr><td><br />30</td><td> (40, 25) </td><td> (42, 26) </td><td> (43, 27) </td><td> (44, 27) </td><td> (45, 28) </td><td> (47, 29) </td><td> (49, 30) </td></tr><tr><td><br />31</td><td> (41, 25) </td><td> (43, 27) </td><td> (44, 27) </td><td> (45, 28) </td><td> (45, 28) </td><td> (48, 30) </td><td> (50, 31) </td></tr><tr><td><br />32</td><td> (42, 26) </td><td> (44, 27) </td><td> (45, 28) </td><td> (45, 28) </td><td> (46, 29) </td><td> (49, 30) </td><td> (51, 32) </td></tr><tr><td><br />33</td><td> (43, 27) </td><td> (45, 28) </td><td> (45, 28) </td><td> (46, 29) </td><td> (47, 29) </td><td> (50, 31) </td><td> (52, 32) </td></tr><tr><td><br />Optimum scale</td><td> (41, 25) </td><td> (45, 28) </td><td> (46, 29) </td><td> (47, 29) </td><td> (49, 30) </td><td> (52, 32) </td><td> (53, 33) </td></tr><tr><td><br />Actual scale</td><td> (43, 24) </td><td> (48, 25) </td><td> (46, 27) </td><td> (47, 27) </td><td> (53, 28) </td><td> (55, 26) </td><td> (54, 27) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="145" name="145" class="anchor-tag">4 算法步骤</h3>
                <div class="p1">
                    <p id="146">自适应特征选择的相关滤波跟踪算法步骤如下:</p>
                </div>
                <h4 class="anchor-tag" id="207" name="207">1) 建立模型。</h4>
                <div class="p1">
                    <p id="147">使用已知的首帧目标位置信息, 选取首帧中的目标区域和搜索区域, 分别训练2维位置滤波器、颜色概率模型以及1维尺度滤波器。</p>
                </div>
                <h4 class="anchor-tag" id="213" name="213">2) 融合特征。</h4>
                <div class="p1">
                    <p id="148">取<i>t</i>-1帧的目标位置作为中心位置, 利用该位置构建相应的背景区域和前景区域。使用位置滤波器在背景区域中采集候选样本<i>z</i><sub><i>t</i></sub>, 将<i>z</i><sub><i>t</i></sub>进行FFT变换后代入 (7) 式和 (8) 式, 得到滤波器的响应图<i>f</i><sub>cf</sub>;同时使用颜色概率模型统计前景和背景的颜色直方图并进行归一化, 将结果代入 (9) 式得到像素级的前景概率响应图<i>f</i><sub>color</sub>。对得到的<i>f</i><sub>cf</sub>与<i>f</i><sub>color</sub>分配不同的权重, 代入 (1) 式得到多个最终的特征响应图<i>f</i><sub>fuse</sub>。</p>
                </div>
                <h4 class="anchor-tag" id="212" name="212">3) 可信度判断。</h4>
                <div class="p1">
                    <p id="149">通过 (2) 式对步骤2) 中分配不同的权重<i>ω</i>的多个<i>f</i><sub>fuse</sub>进行可信度判断, 取最大可信度的融合特征作为当前帧的跟踪特征, 并估计出目标的候选位置。若最大可信度高于阈值<i>θ</i><sub>APCE</sub>, 则候选位置即为当前帧的目标位置, 转至步骤5) ;若最大可信度不超过阈值<i>θ</i><sub>APCE</sub>, 则可能需要对目标的位置进行重新检测, 转至步骤4) 。</p>
                </div>
                <h4 class="anchor-tag" id="211" name="211">4) 特征点匹配和重检测。</h4>
                <div class="p1">
                    <p id="150">对于步骤3) 中最大可信度仍较低的情况, 根据 (15) 式和 (16) 式, 使用特征点匹配的方法比较该可信度下<i>t</i>帧中确定的目标区域与<i>t</i>-1帧中确定的目标区域的相似程度, 若特征点匹配成功的点对数大于匹配阈值<i>Y</i><sub>threshold</sub>, 则转至步骤5) ;否则对预测区域进行重新检测, 选取可信度最大的预测区域作为目标区域。</p>
                </div>
                <h4 class="anchor-tag" id="210" name="210">5) 尺度估计。</h4>
                <div class="p1">
                    <p id="151">以当前帧确定的目标位置为中心位置, 使用 (17) 式提取不同的候选样本以构建尺度金字塔, 根据 (7) 式和 (8) 式计算尺度滤波器的响应值, 最大响应尺度即为第<i>t</i>帧目标的最佳尺度。</p>
                </div>
                <h4 class="anchor-tag" id="209" name="209">6) 模型更新。</h4>
                <div class="p1">
                    <p id="152">对于步骤 4) 中重新检测过程得到的预测区域中最大可信度仍不超过阈值<i>θ</i><sub>APCE</sub>的情况, 不进行模型更新, 使用 (5) 式和 (6) 式更新滤波器, 同时根据 (13) 式与 (14) 式更新颜色概率模型。</p>
                </div>
                <h4 class="anchor-tag" id="208" name="208">7) 输出跟踪结果。</h4>
                <div class="p1">
                    <p id="153">将跟踪窗口进行可视化输出, 继续执行步骤2) 。</p>
                </div>
                <h3 id="154" name="154" class="anchor-tag">5 实验结果及分析</h3>
                <h4 class="anchor-tag" id="155" name="155"><b>5.1 实验参数设置及数据</b></h4>
                <div class="p1">
                    <p id="156">所有实验均在CPU为Intel Core i7-6700, 3.4 GHz主频和16 GB内存的计算机上完成, 测试开发平台为:MATLAB R2016a。在位置滤波器模型中, 初始提取的特征维度<i>d</i>=32, 压缩后的特征维度<mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>∼</mo></mover><mo>=</mo><mn>1</mn><mn>7</mn></mrow></math></mathml>, 正则化权重参数<i>λ</i>=0.001, 学习率<i>η</i>=0.03。在颜色概率模型中, 正则化权重参数<i>λ</i>=0.001, 学习率<i>η</i>=0.04。在尺度滤波模型中, 尺度因子<i>σ</i>=1.02, 压缩后的特征维数<i>S</i>=17。学习率<i>η</i>=0.025, 在特征点匹配过程中, 特征点总数<i>S</i><sub>point</sub>=1000, 匹配阈值<i>Y</i><sub>threshold</sub>=500。</p>
                </div>
                <div class="p1">
                    <p id="158">为了便于与其他主流算法进行对比, 实验选取的标准测试数据集为OTB50<citation id="198" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>和OTB100<citation id="199" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。OTB50数据集包含50组视频序列, 涵盖11种不同视觉跟踪挑战的属性, 如光照变化 (IV) 、尺度变换 (SV) 、运动模糊 (MB) 等, 每段序列可能有多个属性。OTB100是在OTB50的基础上新增了50组测试视频序列, 背景属性更加复杂, 跟踪挑战性更大。</p>
                </div>
                <div class="p1">
                    <p id="159">实验采用一次性通过评估 (OPE) 评价准则来计算跟踪准确率和成功率, 将准确率、成功率以及跟踪速度作为跟踪性能的评价标准。跟踪准确率 (DP) 为跟踪中心点位置误差小于阈值的帧数占整个序列帧数的比例, 跟踪成功率 (OP) 则表示跟踪重叠率 (交并比) 大于阈值的帧数占总体序列帧数的比例, 跟踪速度 (FPS) 为跟踪器每秒读取视频序列中图片的个数, 其中准确率阈值设定为20 pixel, 成功率阈值设定为0.5。</p>
                </div>
                <h4 class="anchor-tag" id="160" name="160"><b>5.2 对比实验分析</b></h4>
                <div class="p1">
                    <p id="161">为了验证本文跟踪算法的有效性, 选取7种近几年出现的跟踪效果较好的跟踪算法进行对比, 分别为fDSST<citation id="200" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, SAMF<citation id="201" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, SRDCF<citation id="202" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, BACF<citation id="203" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, STRCF<citation id="204" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>, STAPLE<citation id="205" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, ECO<citation id="206" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。这些算法涵盖基于相关滤波的算法、相关滤波改进的算法以及深度学习的算法, 本节将从跟踪的准确性与时效性两个方面进行跟踪效果的对比。</p>
                </div>
                <h4 class="anchor-tag" id="162" name="162">5.2.1 准确性分析</h4>
                <div class="p1">
                    <p id="163">在OTB50与OTB100数据集的不同属性下测试了8种算法, 得到8种算法的距离精确度和跟踪重叠率见表4。8种算法分别在OTB50、OTB100上的跟踪成功率曲线和跟踪精确率曲线如图6和图7所示, 横坐标分别表示重叠率阈值和中心误差阈值, 纵坐标表示满足阈值条件的帧数占总帧数的比例。</p>
                </div>
                <div class="area_img" id="164">
                    <p class="img_tit">表4 8种跟踪算法平均跟踪性能比较 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Average tracking performance comparison among eight tracking algorithms</p>
                    <p class="img_note"></p>
                    <table id="164" border="1"><tr><td colspan="2">Algorithm</td><td>fDSST</td><td>SAMF</td><td>SRDCF</td><td>BACF</td><td>STAPLE</td><td>STRCF</td><td>ECO</td><td>Ours</td></tr><tr><td><br />Mean DP</td><td>OTB50</td><td>0.696</td><td>0.653</td><td>0.736</td><td>0.735</td><td>0.693</td><td>0.831</td><td>0.868</td><td>0.789</td></tr><tr><td><br /></td><td>OTB100</td><td>0.742</td><td>0.741</td><td>0.778</td><td>0.801</td><td>0.795</td><td>0.855</td><td>0.886</td><td>0.819</td></tr><tr><td><br />Mean OP</td><td>OTB50</td><td>0.623</td><td>0.558</td><td>0.652</td><td>0.673</td><td>0.605</td><td>0.684</td><td>0.698</td><td>0.675</td></tr><tr><td><br /></td><td>OTB100</td><td>0.583</td><td>0.554</td><td>0.612</td><td>0.643</td><td>0.604</td><td>0.680</td><td>0.697</td><td>0.654</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="165">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906030_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 8种跟踪算法在OTB50上的精确率和成功率" src="Detail/GetImg?filename=images/GXXB201906030_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 8种跟踪算法在OTB50上的精确率和成功率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906030_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Precisions and success rates for 8 tracking algorithms on OTB50</p>
                                <p class="img_note"> (a) 精确率对比曲线; (b) 成功率对比曲线</p>
                                <p class="img_note"> (a) Precision comparison; (b) success rate comparison</p>

                </div>
                <div class="area_img" id="166">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906030_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 8种跟踪算法在OTB100上的精确率和成功率" src="Detail/GetImg?filename=images/GXXB201906030_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 8种跟踪算法在OTB100上的精确率和成功率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906030_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Precisions and success rates of 8 tracking algorithms on OTB100</p>
                                <p class="img_note"> (a) 精确率对比曲线; (b) 成功率对比曲线</p>
                                <p class="img_note"> (a) Precision comparison; (b) success rate comparison</p>

                </div>
                <div class="p1">
                    <p id="168">结合表4中的数据和图6、图7中的曲线可以看出, 在OTB50数据集上, 本文提出的跟踪算法在跟踪成功率上接近STRCF算法, 低于基于深度学习的ECO算法, 高于其他的算法, 其在跟踪精度上的表现同样如此。在OTB100数据集上, 本文算法的跟踪成功率低于ECO算法与STRCF算法, 高于其他的算法, 跟踪精度也仅次于ECO算法和STRCF算法而高于其他算法。综上所述, 本文算法虽然比基于深度学习算法的跟踪准确度低, 但是在大多数情况下表现较好, 能准确、稳健地跟踪目标。不过与针对遮挡问题的STRCF算法相比, 本文算法对于遮挡情况的处理能力欠佳。因此在以后的工作中, 将重点对如何提高模型对遮挡的处理能力做进一步研究, 以提高跟踪方法的整体准确性和稳健性。同时, 虽然本文算法的滤波跟踪器是基于fDSST算法, 但是在跟踪的平均准确率和成功率上有明显提高。结合表4数据可知, 在OTB50数据集上, 相比fDSST算法, 本文算法的平均准确率与平均成功率均提升了约7%。为了更加直观地表示不同算法的跟踪效果, 将8种跟踪算法在部分序列上的跟踪效果进行对比, 结果如图8所示。</p>
                </div>
                <div class="area_img" id="169">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906030_169.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 8种跟踪方法在部分序列上的跟踪结果" src="Detail/GetImg?filename=images/GXXB201906030_169.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 8种跟踪方法在部分序列上的跟踪结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906030_169.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Tracking results of 8 tracking algorithms in partial sequences</p>

                </div>
                <div class="p1">
                    <p id="170">BlurOwl序列中目标具有运动模糊以及快速运动的属性, 在第10帧到第388帧的跟踪过程中, 目标在平面内快速运动, 部分跟踪算法逐渐漂移, 尤其在第156帧时, 只有ECO算法、STRCF算法以及本文算法能精确地跟踪到目标位置, 经历运动模糊过程后 (第470帧、第631帧) , 仅有STRCF算法与本文算法在尺度和位置上可精确地定位目标, 其中STRCF算法在第470帧时发生了轻度漂移, 表明本文算法对于快速运动和运动模糊的情形具有稳健性。</p>
                </div>
                <div class="p1">
                    <p id="171">在Couple序列对目标跟踪的过程中 (第10帧、第140帧) , 目标所处的背景比较杂乱且目标的姿态不断变化, 导致跟踪器跟丢目标或无法准确跟踪目标。在第47帧和第108帧中, STAPLE、SAMF以及fDSST算法因背景干扰未能正确跟踪目标, ECO算法因为目标尺度的不断变化, 不能精确地定位目标的尺度。在第140帧中, STRCF算法被相似目标吸引导致错误地跟踪了目标, 只有BACF算法与本文算法成功地跟踪了目标, 表明本文算法在背景杂乱与尺度变化时具有稳定性, 有效地实现了对目标的跟踪。</p>
                </div>
                <div class="p1">
                    <p id="172">Skiing序列中显示的是目标在光照变化和变形的情况下, 8种跟踪算法的跟踪效果。从第2帧到第81帧的跟踪过程中, 除了ECO算法与本文算法外, 其余算法均在第16帧时发生了漂移且无法再次跟踪到目标。在第33帧时, 由于目标发生了变形, ECO算法未能精确定位目标的位置, 仅本文算法准确地跟踪到目标, 表明本文选择特征的方法对光照变化以及变形具有较强的适应能力, 跟踪较为稳定。</p>
                </div>
                <div class="p1">
                    <p id="173">Bird2序列中的目标存在遮挡与平面外旋转的情况, 在第49帧时, 只有本文算法准确地跟踪到目标, 其他跟踪算法由于部分遮挡发生了不同程度的漂移, 除ECO算法外, 其他方法由于在漂移过程中学习了大量的背景信息, 导致跟丢目标 (第61帧、第83帧) 。在第98帧时, STRCF、SAMF以及fDSST跟踪算法丢失目标后无法重新检测跟踪到目标, 本文算法通过在跟踪过程中采用特征点匹配机制, 一定程度上克服了部分遮挡的影响, 准确地跟踪到了目标。</p>
                </div>
                <div class="p1">
                    <p id="174">Bolt2序列与Football1序列中目标背景比较复杂, 同时目标在平面内快速运动并存在大量相似目标的干扰。其中Bolt2序列中, 由于在目标的快速运动过程中, SAMF算法与fDSST算法中的跟踪器对于错误的跟踪结果没有可靠性判定, 导致目标逐渐跟踪错误 (第25帧、第88帧) 。STRCF算法由于相似目标的干扰, 在第182帧时错误地跟踪了目标。在对Football1序列的跟踪过程中, 目标还存在平面内旋转的属性。跟踪过程中目标经过快速运动和旋转后, 在第74帧时, 只有本文跟踪算法能长期、稳定地跟踪到目标。本文通过在跟踪过程中引入可信度策略, 对每帧的跟踪结果进行可靠性判定, 增强了跟踪器对于错误跟踪结果的判别能力, 从而提高了跟踪的准确性。</p>
                </div>
                <h4 class="anchor-tag" id="175" name="175">5.2.2 时效性分析</h4>
                <div class="p1">
                    <p id="176">8种算法在OTB50和OTB100数据集上的平均运行速度见表5。结合表5数据可以看出, 本文提出的算法在运行速度上仅次于fDSST算法, 优于其他算法。综合表4和表5的数据可知, 虽然基于深度学习的ECO算法与针对遮挡问题的STRCF算法在跟踪效果上略优于本文算法, 但是在跟踪速度上, 上述两种算法的运行速度较慢, 不能满足实时性的要求。相较于ECO算法与STRCF算法, 本文算法的优势在于能保证跟踪效果相对准确、稳定的同时, 还能基本满足实时性的要求;相较于固定等权重融合特征的STAPLE算法, 本文算法除了采用多权重分配方式融合特征之外, 在基础特征提取以及尺度估计过程中, 采用比STAPLE算法计算量更小的加速算法, 因此在跟踪准确率以及跟踪速度上均优于STAPLE算法;相较于运行速度比本文算法较高的fDSST算法, 本文算法的跟踪准确率更高, 跟踪性能更好。</p>
                </div>
                <div class="area_img" id="177">
                    <p class="img_tit">表5 8种跟踪算法平均跟踪速度比较 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Average tracking speed comparison among 8 tracking algorithms</p>
                    <p class="img_note"></p>
                    <table id="177" border="1"><tr><td colspan="2">Algorithm</td><td>fDSST</td><td>SAMF</td><td>SRDCF</td><td>BACF</td><td>STRCF</td><td>STAPLE</td><td>ECO</td><td>Proposed</td></tr><tr><td><br />Mean FPS / (frame·s<sup>-1</sup>) </td><td>OTB50</td><td>74.78</td><td>19.81</td><td>6.46</td><td>27.92</td><td>18.76</td><td>55.51</td><td>1.43</td><td>71.43</td></tr><tr><td><br /></td><td>OTB100</td><td>88.44</td><td>22.80</td><td>6.46</td><td>31.92</td><td>21.90</td><td>68.83</td><td>1.38</td><td>72.61</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="178" name="178" class="anchor-tag">6 结  论</h3>
                <div class="p1">
                    <p id="179">本文算法使用不同的权重分配方式融合两种基础特征, 从多样化的融合特征中选择可信度最大的特征, 将其作为目标的跟踪特征对目标进行跟踪, 同时提出了一个基于GMS特征点匹配的检测器, 用于重新检测。通过将多样的特征融合方式与高置信度选择策略相结合, 增强了模型对于目标以及背景变化的适应能力;当选择的特征可信度较低时, 根据特征点匹配的结果对目标进行重新检测, 实现对目标稳定、准确的跟踪。上述实验结果表明, 本文跟踪算法的跟踪速度基本满足实时性的要求, 有效地提高了目标跟踪的准确性和稳健性, 尤其对于快速运动、运动模糊、光照变化等场景具有良好的性能。本文算法没有更多地考虑大面积遮挡对模型的影响, 因此下一步的工作重点是将遮挡考虑在内, 以进一步提高目标跟踪效果。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="12">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-task Correlation Particle Filter for Robust Object Tracking">

                                <b>[1]</b> Zhang T Z, Xu C S, Yang M H.Multi-task correlation particle filter for robust object tracking[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4819-4827.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201708025&amp;v=MDQ0MTR6cXFCdEdGckNVUkxPZVplVnVGeXZuVTd2S0lqWFRiTEc0SDliTXA0OUhZWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Zhao G P, Shen Y P, Wang J Y.Adaptive feature fusion object tracking based on circulant structure with kernel[J].Acta Optica Sinica, 2017, 37 (8) :0815001.赵高鹏, 沈玉鹏, 王建宇.基于核循环结构的自适应特征融合目标跟踪[J].光学学报, 2017, 37 (8) :0815001.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902032&amp;v=MDkzMTQ5R1pvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5dm5VN3ZLSWpYVGJMRzRIOWpNclk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Zhang Z, Sun J, Yang L T.Tracking algorithm based on correlation filter fusing and keypoint matching[J].Acta Optica Sinica, 2019, 39 (2) :0215001.张哲, 孙瑾, 杨刘涛.融合相关滤波与关键点匹配的跟踪算法[J].光学学报, 2019, 39 (2) :0215001.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811025&amp;v=MjU4MjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnl2blU3dktJalhUYkxHNEg5bk5ybzlIWVlRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Ge B Y, Zuo X Z, Hu Y J.Long-term object tracking based on feature fusion[J].Acta Optica Sinica, 2018, 38 (11) :1115002.葛宝义, 左宪章, 胡永江.基于特征融合的长时目标跟踪算法[J].光学学报, 2018, 38 (11) :1115002.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201801008&amp;v=MjkzMzR2blU3dktLRDdZYkxHNEg5bk1ybzlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Lu H C, Li P X, Wang D.Visual object tracking:a survey[J].Pattern Recognition and Artificial Intelligence, 2018, 31 (1) :61-76.卢湖川, 李佩霞, 王栋.目标跟踪算法综述[J].模式识别与人工智能, 2018, 31 (1) :61-76.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">

                                <b>[6]</b> Bolme D S, Beveridge J R, Draper B A, <i>et al</i>.Visual object tracking using adaptive correlation filters[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:2544-2550.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">

                                <b>[7]</b> Henriques J F, Caseiro R, Martins P, <i>et al</i>.Exploiting the circulant structure of tracking by detection with kernels[M]//Fitzgibbon A, Lazebnik S, Perona P, <i>et al</i>.Computer Vision-ECCV 2012.Berlin, Heidelberg:Springer, 2012, 7575:702-715.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                 Henriques J F, Caseiro R, Martins P, <i>et al</i>.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 Danelljan M, Häger G, Khan F, <i>et al</i>.Accurate scale estimation for robust visual tracking[C]//British Machine Vision Conference 2014, September 1-5, 2014, Nottingham.Durham, England, UK:BMVA Press, 2014:65.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">

                                <b>[10]</b> Bertinetto L, Valmadre J, Golodetz S, <i>et al</i>.Staple:complementary learners for real-time tracking[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1401-1409.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-kernel Correlation Filter for Visual Tracking">

                                <b>[11]</b> Tang M, Feng J Y.Multi-kernel correlation filter for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:3038-3046.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast Tracking via Spatio-Temporal Context Learning">

                                <b>[12]</b> Zhang K H, Zhang L, Yang M H, <i>et al</i>.Fast tracking via spatio-temporal context learning[EB/OL]. (2013-11-08) [2018-12-15].https://arxiv.org/pdf/1311.1939.pdf.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 Danelljan M, Häger G, Khan F S, <i>et al</i>.Discriminative scale space tracking[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (8) :1561-1575.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=In defense of color-based model-free tracking">

                                <b>[14]</b> Possegger H, Mauthner T, Bischof H.In defense of color-based model-free tracking[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:2113-2120.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large Margin Object Tracking with Circulant Feature Maps">

                                <b>[15]</b> Wang M M, Liu Y, Huang Z Y.Large margin object tracking with circulant feature maps[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4800-4808.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GMS:Grid-based Motion Statistics for Fast,UltraRobust Feature Correspondence">

                                <b>[16]</b> Bian J W, Lin W Y, Matsushita Y, <i>et al</i>.GMS:grid-based motion statistics for fast, ultra-robust feature correspondence[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:2828-2837.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online Object Tracking:A Benchmark">

                                <b>[17]</b> Wu Y, Lim J, Yang M H.Online object tracking:a benchmark[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2013, Portland, OR, USA.New York:IEEE, 2013:2411-2418.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">

                                <b>[18]</b> Wu Y, Lim J, Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1834-1848.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;A scale adaptive kernel correlation filter tracker with feature integration &amp;quot;">

                                <b>[19]</b> Li Y, Zhu J K.A scale adaptive kernel correlation filter tracker with feature integration[M]//Agapito L, Bronstein M, Rother C.Computer Vision-ECCV 2014 Workshops.Cham:Springer, 2015, 8693:254-265.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_20" >
                                    <b>[20]</b>
                                 Danelljan M, Häger G, Khan F S, <i>et al</i>.Learning spatially regularized correlation filters for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:4310-4318.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Background-Aware Correlation Filters for Visual Tracking">

                                <b>[21]</b> Galoogahi H K, Fagg A, Lucey S.Learning background-aware correlation filters for visual tracking[C]//2017 IEEE International Conference on Computer Vision (ICCV) , October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:1144-1152.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatial-temporal regularized correlation filters for visual tracking">

                                <b>[22]</b> Li F, Tian C, Zuo W M, <i>et al</i>.Learning spatial-temporal regularized correlation filters for visual tracking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:4904-4913.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_23" >
                                    <b>[23]</b>
                                 Danelljan M, Bhat G, Khan F S, <i>et al</i>.ECO:efficient convolution operators for tracking[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:6931-6939.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201906030" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201906030&amp;v=MTc5NThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnl2blU3dkxJalhUYkxHNEg5ak1xWTlHWkk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="3" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

