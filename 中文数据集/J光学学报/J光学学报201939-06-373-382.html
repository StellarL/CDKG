

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133927339346250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201906044%26RESULT%3d1%26SIGN%3dFsUSC%252fKlxXO8fUktLKyrF4zFMDg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201906044&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201906044&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201906044&amp;v=MjgxNDVMRzRIOWpNcVk5QllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5dm5XcnZMSWpYVGI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#65" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#69" data-title="2 改进的SSD算法 ">2 改进的SSD算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="&lt;b&gt;2.1 SSD算法&lt;/b&gt;"><b>2.1 SSD算法</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;2.2 改进的SSD算法&lt;/b&gt;"><b>2.2 改进的SSD算法</b></a></li>
                                                <li><a href="#83" data-title="&lt;b&gt;2.3 损失函数及训练方法&lt;/b&gt;"><b>2.3 损失函数及训练方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#101" data-title="3 精度评估模型 ">3 精度评估模型</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#106" data-title="4 实验分析 ">4 实验分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#107" data-title="&lt;b&gt;4.1 实验平台与数据&lt;/b&gt;"><b>4.1 实验平台与数据</b></a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;4.2 改进SSD算法训练分析&lt;/b&gt;"><b>4.2 改进SSD算法训练分析</b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;4.3 改进SSD算法精度分析&lt;/b&gt;"><b>4.3 改进SSD算法精度分析</b></a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;4.4 检测效果对比&lt;/b&gt;"><b>4.4 检测效果对比</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#130" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="图1 SSD算法框架">图1 SSD算法框架</a></li>
                                                <li><a href="#76" data-title="图2 改进SSD算法框架">图2 改进SSD算法框架</a></li>
                                                <li><a href="#82" data-title="图3 融合前后的特征图对比">图3 融合前后的特征图对比</a></li>
                                                <li><a href="#105" data-title="表1 主要衡量指标">表1 主要衡量指标</a></li>
                                                <li><a href="#111" data-title="图4 训练样本在线采集系统界面">图4 训练样本在线采集系统界面</a></li>
                                                <li><a href="#115" data-title="表2 样本集统计">表2 样本集统计</a></li>
                                                <li><a href="#116" data-title="图5 样本集各类大小目标示意图">图5 样本集各类大小目标示意图</a></li>
                                                <li><a href="#117" data-title="图6 学习率衰减曲线">图6 学习率衰减曲线</a></li>
                                                <li><a href="#121" data-title="图7 迁移训练和随机初始化两种方式总损失和精度对比">图7 迁移训练和随机初始化两种方式总损失和精度对比</a></li>
                                                <li><a href="#123" data-title="表3 计算耗时及验证集上精度对比">表3 计算耗时及验证集上精度对比</a></li>
                                                <li><a href="#124" data-title="图8 改进SSD算法与其他算法随迭代次数精度变化对比">图8 改进SSD算法与其他算法随迭代次数精度变化对比</a></li>
                                                <li><a href="#125" data-title="表4 测试集上精度对比">表4 测试集上精度对比</a></li>
                                                <li><a href="#132" data-title="图9 改进SSD与其他方法检测效果对比">图9 改进SSD与其他方法检测效果对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Wang G X, Huang X T, Zhou Z M.UWB SAR change detection of target in foliage based on local statistic distribution change analysis[J].Journal of Electronics &amp;amp; Information Technology, 2011, 33 (1) :49-54.王广学, 黄晓涛, 周智敏.基于邻域统计分布变化分析的UWB SAR隐蔽目标变化检测[J].电子与信息学报, 2011, 33 (1) :49-54." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201101005&amp;v=MjMyODFUZlNkckc0SDlETXJvOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeXZuV3J2TEk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Wang G X, Huang X T, Zhou Z M.UWB SAR change detection of target in foliage based on local statistic distribution change analysis[J].Journal of Electronics &amp;amp; Information Technology, 2011, 33 (1) :49-54.王广学, 黄晓涛, 周智敏.基于邻域统计分布变化分析的UWB SAR隐蔽目标变化检测[J].电子与信息学报, 2011, 33 (1) :49-54.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Wu W.Research on knowledge-based target recognition and tracking techniques[D].Harbin:Harbin Institute of Technology, 2007:10-28.吴畏.基于知识的目标识别与跟踪技术研究[D].哈尔滨:哈尔滨工业大学, 2007:10-28." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=2008196104.nh&amp;v=MTY4MjdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnl2bldydkxWMTI3RnJLeEdORE1xNUViUElRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Wu W.Research on knowledge-based target recognition and tracking techniques[D].Harbin:Harbin Institute of Technology, 2007:10-28.吴畏.基于知识的目标识别与跟踪技术研究[D].哈尔滨:哈尔滨工业大学, 2007:10-28.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" Cao J Z, Song A G.Research on the texture image segmentation method based on Markov random field[J].Chinese Journal of Scientific Instrument, 2015, 36 (4) :776-786.曹家梓, 宋爱国.基于马尔科夫随机场的纹理图像分割方法研究[J].仪器仪表学报, 2015, 36 (4) :776-786." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201504008&amp;v=MDk5NjFEelRiTEc0SDlUTXE0OUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeXZuV3J2TFA=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Cao J Z, Song A G.Research on the texture image segmentation method based on Markov random field[J].Chinese Journal of Scientific Instrument, 2015, 36 (4) :776-786.曹家梓, 宋爱国.基于马尔科夫随机场的纹理图像分割方法研究[J].仪器仪表学报, 2015, 36 (4) :776-786.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Russakovsky O, Deng J, Su H, &lt;i&gt;et al&lt;/i&gt;.ImageNet large scale visual recognition challenge[J].International Journal of Computer Vision, 2015, 115 (3) :211-252." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet:Large scale visual recognition challenge">
                                        <b>[4]</b>
                                         Russakovsky O, Deng J, Su H, &lt;i&gt;et al&lt;/i&gt;.ImageNet large scale visual recognition challenge[J].International Journal of Computer Vision, 2015, 115 (3) :211-252.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" Girshick R.Fast R-CNN[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[5]</b>
                                         Girshick R.Fast R-CNN[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1440-1448.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Ren S Q, He K M, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">
                                        <b>[6]</b>
                                         Ren S Q, He K M, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" Dai J, Li Y, He K, &lt;i&gt;et al&lt;/i&gt;.R-FCN:object detection via region-based fully convolutional networks[C]∥NIPS′16 Proceedings of the 30th International Conference on Neural Information Processing Systems, December 5-10, 2016, Barcelona, Spain.USA:Curran Associates Inc., 2016:379-387." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=R-FCN:Object detection Via Region-based fully convolutional networks">
                                        <b>[7]</b>
                                         Dai J, Li Y, He K, &lt;i&gt;et al&lt;/i&gt;.R-FCN:object detection via region-based fully convolutional networks[C]∥NIPS′16 Proceedings of the 30th International Conference on Neural Information Processing Systems, December 5-10, 2016, Barcelona, Spain.USA:Curran Associates Inc., 2016:379-387.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                     Redmon J, Divvala S, Girshick R, &lt;i&gt;et al&lt;/i&gt;.You only look once:unified, real-time object detection[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:779-788.</a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" Liu W, Anguelov D, Erhan D, &lt;i&gt;et al&lt;/i&gt;.SSD:single shot multibox detector[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Computer Vision-ECCV 2016.Cham:Springer, 2016, 9905:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:Single shot multibox detector">
                                        <b>[9]</b>
                                         Liu W, Anguelov D, Erhan D, &lt;i&gt;et al&lt;/i&gt;.SSD:single shot multibox detector[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Computer Vision-ECCV 2016.Cham:Springer, 2016, 9905:21-37.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" Lin T Y, Goyal P, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Focal loss for dense object detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018:2858826." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Focal loss for dens object detection">
                                        <b>[10]</b>
                                         Lin T Y, Goyal P, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Focal loss for dense object detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018:2858826.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" Huang J, Rathod V, Sun C, &lt;i&gt;et al&lt;/i&gt;.Speed/accuracy trade-offs for modern convolutional object detectors[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:3296-3297." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors">
                                        <b>[11]</b>
                                         Huang J, Rathod V, Sun C, &lt;i&gt;et al&lt;/i&gt;.Speed/accuracy trade-offs for modern convolutional object detectors[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:3296-3297.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" Lin T Y, Maire M, Belongie S, &lt;i&gt;et al&lt;/i&gt;.Microsoft COCO:common objects in context[M]∥Fleet D, Pajdla T, Schiele B, &lt;i&gt;et al&lt;/i&gt;.Computer Vision-ECCV 2014.Cham:Springer, 2014, 8693:740-755." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Microsoft coco:Common objects in context">
                                        <b>[12]</b>
                                         Lin T Y, Maire M, Belongie S, &lt;i&gt;et al&lt;/i&gt;.Microsoft COCO:common objects in context[M]∥Fleet D, Pajdla T, Schiele B, &lt;i&gt;et al&lt;/i&gt;.Computer Vision-ECCV 2014.Cham:Springer, 2014, 8693:740-755.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" Xu Y Z, Yao X J, Li X, &lt;i&gt;et al&lt;/i&gt;.Object detection in high resolution remote sensing images based on fully convolution networks[J].Bulletin of Surveying and Mapping, 2018 (1) :77-82.徐逸之, 姚晓婧, 李祥, 等.基于全卷积网络的高分辨遥感影像目标检测[J].测绘通报, 2018 (1) :77-82." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHTB201801015&amp;v=MDA0OTFNcm85RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5dm5XcnZMSmlYZmJMRzRIOW4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Xu Y Z, Yao X J, Li X, &lt;i&gt;et al&lt;/i&gt;.Object detection in high resolution remote sensing images based on fully convolution networks[J].Bulletin of Surveying and Mapping, 2018 (1) :77-82.徐逸之, 姚晓婧, 李祥, 等.基于全卷积网络的高分辨遥感影像目标检测[J].测绘通报, 2018 (1) :77-82.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     Zhang Z Y.Plane detection in optical remote sensing images based on deep learning[D].Xiamen:Xiamen University, 2016:20-30.张志远.基于深度学习的光学遥感图像飞机检测[D].厦门:厦门大学, 2016:20-30.</a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" Wang J C, Tan X C, Wang Z H, &lt;i&gt;et al&lt;/i&gt;.Faster R-CNN deep learning network based object recognition of remote sensing image[J].Journal of Geo-Information Science, 2018, 20 (10) :1500-1508.王金传, 谭喜成, 王召海, 等.基于Faster R-CNN深度网络的遥感影像目标识别方法研究[J].地球信息科学学报, 2018, 20 (10) :1500-1508." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DQXX201810015&amp;v=MDI5OTVuV3J2TElUelRkckc0SDluTnI0OUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeXY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Wang J C, Tan X C, Wang Z H, &lt;i&gt;et al&lt;/i&gt;.Faster R-CNN deep learning network based object recognition of remote sensing image[J].Journal of Geo-Information Science, 2018, 20 (10) :1500-1508.王金传, 谭喜成, 王召海, 等.基于Faster R-CNN深度网络的遥感影像目标识别方法研究[J].地球信息科学学报, 2018, 20 (10) :1500-1508.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" Feng X Y, Mei W, Hu D S.Aerial target detection based on improved faster R-CNN[J].Acta Optica Sinica, 2018, 38 (6) :0615004.冯小雨, 梅卫, 胡大帅.基于改进Faster R-CNN的空中目标检测[J].光学学报, 2018, 38 (6) :0615004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806034&amp;v=MTE5NDJYVGJMRzRIOW5NcVk5R1lJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5dm5XcnZMSWo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         Feng X Y, Mei W, Hu D S.Aerial target detection based on improved faster R-CNN[J].Acta Optica Sinica, 2018, 38 (6) :0615004.冯小雨, 梅卫, 胡大帅.基于改进Faster R-CNN的空中目标检测[J].光学学报, 2018, 38 (6) :0615004.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_17" title=" Lin T Y, Doll&#225;r P, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Feature pyramid networks for object detection[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:936-944." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature pyramid networks for object detection">
                                        <b>[17]</b>
                                         Lin T Y, Doll&#225;r P, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Feature pyramid networks for object detection[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:936-944.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_18" title=" Huang G, Liu Z, Maaten L V D, &lt;i&gt;et al&lt;/i&gt;.Densely connected convolutional networks[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:2261-2269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Densely Connected Convolutional Networks">
                                        <b>[18]</b>
                                         Huang G, Liu Z, Maaten L V D, &lt;i&gt;et al&lt;/i&gt;.Densely connected convolutional networks[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:2261-2269.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_19" title=" Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2018-12-22].https://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">
                                        <b>[19]</b>
                                         Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2018-12-22].https://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_20" title=" Ioffe S, Szegedy C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]∥ICML′15 Proceedings of the 32nd International Conference on International Conference on Machine Learning, July 6-11, 2015, Lille, France.Massachusetts:JMLR.org, 2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift">
                                        <b>[20]</b>
                                         Ioffe S, Szegedy C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]∥ICML′15 Proceedings of the 32nd International Conference on International Conference on Machine Learning, July 6-11, 2015, Lille, France.Massachusetts:JMLR.org, 2015:448-456.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_21" title=" He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.Deep residual learning for image recognition[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[21]</b>
                                         He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.Deep residual learning for image recognition[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_22" title=" Parker J A, Kenyon R V, Troxel D E.Comparison of interpolating methods for image resampling[J].IEEE Transactions on Medical Imaging, 1983, 2 (1) :31-39." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Comparison of Interpolating methods for image resampling">
                                        <b>[22]</b>
                                         Parker J A, Kenyon R V, Troxel D E.Comparison of interpolating methods for image resampling[J].IEEE Transactions on Medical Imaging, 1983, 2 (1) :31-39.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_23" title=" de Boer P T, Kroese D P, Mannor S, &lt;i&gt;et al&lt;/i&gt;.A tutorial on the cross-entropy method[J].Annals of Operations Research, 2005, 134 (1) :19-67." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00000425224&amp;v=MDQ4MjdoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RlNqbFVMM0JKRjA9Tmo3QmFyTzRIdEhNcTQxQVp1a0xZM2s1ekJk&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         de Boer P T, Kroese D P, Mannor S, &lt;i&gt;et al&lt;/i&gt;.A tutorial on the cross-entropy method[J].Annals of Operations Research, 2005, 134 (1) :19-67.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_24" title=" Yosinski J, Clune J, Bengio Y, &lt;i&gt;et al&lt;/i&gt;.How transferable are features in deep neural networks?[EB/OL]. (2014-11-06) [2018-12-22].https://arxiv.org/abs/1411.1792." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=How transferable are features in deep neural networks?">
                                        <b>[24]</b>
                                         Yosinski J, Clune J, Bengio Y, &lt;i&gt;et al&lt;/i&gt;.How transferable are features in deep neural networks?[EB/OL]. (2014-11-06) [2018-12-22].https://arxiv.org/abs/1411.1792.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_25" title=" Everingham M, van Gool L, Williams C K I, &lt;i&gt;et al&lt;/i&gt;.The pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303-338." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MDU0MzZKRjA9Tmo3QmFyTzRIdEhQcVlkSFkrSUxZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTamxVTDNC&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         Everingham M, van Gool L, Williams C K I, &lt;i&gt;et al&lt;/i&gt;.The pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303-338.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_26" title=" Loshchilov I, Hutter F.SGDR:stochastic gradient descent with warm restarts[EB/OL]. (2017-03-03) [2018-12-25].https://arxiv.org/abs/1608.03983." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SGDR:stochastic gradient descent with warm restarts">
                                        <b>[26]</b>
                                         Loshchilov I, Hutter F.SGDR:stochastic gradient descent with warm restarts[EB/OL]. (2017-03-03) [2018-12-25].https://arxiv.org/abs/1608.03983.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_27" title=" Szegedy C, Vanhoucke V, Ioffe S, &lt;i&gt;et al&lt;/i&gt;.Rethinking the inception architecture for computer vision[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:2818-2826." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rethinking the Inception Architecture for Computer Vision">
                                        <b>[27]</b>
                                         Szegedy C, Vanhoucke V, Ioffe S, &lt;i&gt;et al&lt;/i&gt;.Rethinking the inception architecture for computer vision[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:2818-2826.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-03-19 09:09</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(06),373-382 DOI:10.3788/AOS201939.0628005            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>改进的SSD算法及其对遥感影像小目标检测性能的分析</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E4%BF%8A%E5%BC%BA&amp;code=40953981&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王俊强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%BB%BA%E8%83%9C&amp;code=20421498&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李建胜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E5%AD%A6%E6%96%87&amp;code=42150747&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周学文</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%97%AD&amp;code=39544270&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张旭</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E5%9C%B0%E7%90%86%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0199248&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息工程大学地理空间信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=78123%E9%83%A8%E9%98%9F&amp;code=1745004&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">78123部队</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对以Faster R-CNN为代表的基于候选框方式的遥感影像目标检测方法检测速度慢, 而现有SSD算法在小目标检测中性能低的问题, 提出一种改进的SSD算法, 综合利用现有基于候选框方式和一体化检测方式的优势, 提升检测性能。该算法利用密集连接网络替换原有的VGGNet作为骨干网络, 并且在密集连接模块之间构建特征金字塔, 代替原有多尺度特征图。为验证所提算法的精度及性能, 设计样本数据在线采集系统, 并采集飞机及运动场目标样本集作为实验样本, 通过对改进SSD算法的训练, 验证了其网络结构的稳定性, 在无迁移学习支持下依然能够达到良好效果, 且训练过程不易发散。通过对比以101层的残差网络 (ResNet101) 作为基础网络的Faster R-CNN算法和R-FCN算法可知, 改进SSD算法较Faster R-CNN算法和R-FCN算法的MAP在测试集上分别提升了9.13%和8.48%, 小目标检测的MAP分别提升了14.46%和13.92%, 检测单张影像耗时71.8 ms, 较Faster R-CNN和R-FCN算法分别减少45.7 ms和7.5 ms。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%A5%E6%84%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遥感;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">小目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度预测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E9%87%91%E5%AD%97%E5%A1%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征金字塔;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B9%B3%E5%9D%87%E5%87%86%E7%A1%AE%E7%8E%87%E5%9D%87%E5%80%BC&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">平均准确率均值;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李建胜 E-mail:xindawangjunqiang@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-16</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (41876105);</span>
                                <span>国家重点研发计划资助 (2017YFF0206000);</span>
                    </p>
            </div>
                    <h1><b>Improved SSD Algorithm and Its Performance Analysis of Small Target Detection in Remote Sensing Images</b></h1>
                    <h2>
                    <span>Wang Junqiang</span>
                    <span>Li Jiansheng</span>
                    <span>Zhou Xuewen</span>
                    <span>Zhang Xu</span>
            </h2>
                    <h2>
                    <span>Institute of Geospatial Information, Information Engineering University</span>
                    <span>78123 Troops</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>An improved single shot multibox detector (SSD) algorithm is proposed aiming at the problems of slow detection speed of the target proposal based remote sensing image target detection method represented by faster regions with convolutional neural network (R-CNN) and the low performance in small target detection by the SSD algorithm. The algorithm can combine the advantages of the existing detection methods based on target proposal and one-stage target detection to improve the target detection performance. Furthermore, the algorithm replaces the original visual geometry group net with a densely connected network as the backbone network and constructs a feature pyramid between the densely connected modules instead of the original multi-scale feature map. A sample data online acquisition system is designed to verify the accuracy and performance of the proposed algorithm. A sample set of aircraft and playground target is collected as the experimental sample. The network structure stability is verified by training the improved SSD algorithm. Consequently, good results can be achieved without the support of transfer learning. Moreover, the training process is not easy to diverge. By comparing the Faster R-CNN algorithm using ResNet101 as the backbone network and the R-FCN (region-based fully convolutional networks) algorithm, we find that the mean average precision (MAP) of the improved SSD algorithm is 9.13% and 8.48% higher than that of the faster R-CNN and R-FCN algorithms in the test set, respectively. The proposed SSD algorithm improves the MAP in the small target detection by 14.46% and 13.92% compared to the faster R-CNN and R-FCN algorithms, respectively. Detecting a single image takes 71.8 ms, which is 45.7 ms and 7.5 ms less than that of the faster R-CNN and R-FCN algorithms, respectively.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=remote%20sensing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">remote sensing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=small%20target%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">small target detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale%20prediction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale prediction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20pyramid&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature pyramid;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=mean%20average%20precision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">mean average precision;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-16</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="65" name="65" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="66">遥感影像目标检测是遥感影像解译的重要分支之一, 对于挖掘感兴趣信息至关重要。传统的方法有基于统计的目标检测<citation id="135" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、基于知识的目标检测<citation id="136" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>以及基于模型的目标检测<citation id="137" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等, 这些方法需要人工设定先验条件, 稳健性差, 智能程度低, 难以应用于大范围自动化作业的需求。</p>
                </div>
                <div class="p1">
                    <p id="67">近年来, 随着卷积神经网络在2012年的ImageNet<citation id="138" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>图像分类比赛中取得巨大成功后, 基于深度学习的目标检测方法开始应用于图像目标检测领域。2013年起, 基于深度学习的目标检测算法沿着两条主线发展:一条是以Fast R-CNN<citation id="139" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、Faster R-CNN<citation id="140" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、R-FCN<citation id="141" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>算法为典型的基于候选框方式的检测主线;另外一条是以YOLO (You Only Look Once) <citation id="142" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、SSD<citation id="143" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、Retina-Net<citation id="144" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>为代表的一体化卷积网络检测算法。一体化检测方式的速度要明显快于基于候选框的检测算法, 但检测精度要略逊于后者。文献<citation id="145" type="reference">[<a class="sup">11</a>]</citation>对当前主流的Faster R-CNN、R-FCN及SSD三种检测算法在Microsoft COCO (common objects in context) <citation id="146" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>公开数据集上的性能进行详细对比, 结果表明Faster R-CNN的精度最高, 而SSD的速度最快。随着深度学习技术的发展, 不少学者开始研究利用深度学习方法进行遥感影像目标检测, 文献<citation id="147" type="reference">[<a class="sup">13</a>]</citation>全面比较了Fast R-CNN、Faster R-CNN和R-FCN三种算法对飞机目标检测的识别性能, 其结果表明R-FCN算法的准确率和检测速度较优。文献<citation id="148" type="reference">[<a class="sup">14</a>]</citation>将深度学习引入飞机检测中, 构建了基于深度信念网络及基于卷积神经网络的两种飞机检测结构模型, 实现了飞机目标高精度检测。文献<citation id="149" type="reference">[<a class="sup">15</a>]</citation>基于Faster R-CNN方法对遥感影像中飞机、油罐等目标进行验证实验, 取得较好效果。文献<citation id="150" type="reference">[<a class="sup">16</a>]</citation>提出了适应空中目标检测任务的特点和需求的Faster R-CNN改进策略, 弥补了Faster R-CNN算法对弱小目标和被遮挡目标不敏感的缺陷并提升了检测精度。以上研究表明, 基于候选框的算法是当前应用于遥感影像目标检测的主流方法, 尤其是Faster R-CNN算法的应用最广泛, 但以上研究均未对遥感影像小目标检测性能进行全面分析, 而遥感影像相对于常规自然场景图片具有幅面大、分辨率低等特点, 但需要在较小分辨率下进行检测才能实现在大范围内自动化高效作业, 在此情况下, 目标大小在影像中常表现为中小型特征, 因此, 需要更多地关注中小型目标的检测。Faster R-CNN算法对于小目标的检测性能比SSD算法好, 但检测时间更长, 无法满足大范围自动化检测的需求。Feature Pyramid Networks (FPN) 方法<citation id="151" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>通过将网络中最顶层的特征图像逐层地反馈并与前层的特征图进行融合, 提升低层次特征的语义强度, 并且在不同尺度上进行目标检测, 可提升小目标的检测优势, 因此, 为平衡速度和精度, 可将FPN特征提取方法应用至SSD算法中, 但存在问题是仅在SSD算法特征提取网络部分引入FPN特征提取方法后, 检测速度将会进一步降低。</p>
                </div>
                <div class="p1">
                    <p id="68">为解决以上问题, 本文针对现有SSD算法进行改进, 参考密集连接的DenseNet (densely connected convolutional networks) 网络<citation id="152" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>设计特征提取网络, 并将其替代原有的16层VGGNet (VGG-16) <citation id="153" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>网络, 并引入FPN特征提取方法, 进行多尺度特征融合, 以替代原有的多尺度预测方法。</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag">2 改进的SSD算法</h3>
                <h4 class="anchor-tag" id="70" name="70"><b>2.1 SSD算法</b></h4>
                <div class="p1">
                    <p id="71">SSD是Liu等<citation id="154" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出的一种目标检测算法, 是目前较流行的检测框架之一, 相比于Faster R-CNN算法, SSD算法在速度上具有明显优势, 而相比于YOLO算法, SSD算法在精度上又有明显的优势。SSD算法框架如图1所示, 该算法以VGG-16为基础网络, 在此基础上添加辅助卷积、池化层等结构, 得到的多尺度特征图均用来进行目标检测, 较大特征图用于检测相对较小目标, 而尺寸较小特征图负责大目标检测。</p>
                </div>
                <div class="p1">
                    <p id="72">SSD算法参照YOLO算法的一体化检测方式, 但与YOLO算法最后采用全连接层预测不同的是, SSD算法采用卷积对不同尺度特征图进行检测, 同时借鉴了Faster R-CNN中anchor的思想, 在特征图中的每个单元设置长宽比不同的先验框, 预测时目标框以这些先验框为基准计算偏差, 可在一定程度上降低训练的难度。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906044_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 SSD算法框架" src="Detail/GetImg?filename=images/GXXB201906044_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 SSD算法框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906044_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Framework of SSD algorithm</p>

                </div>
                <h4 class="anchor-tag" id="74" name="74"><b>2.2 改进的SSD算法</b></h4>
                <div class="p1">
                    <p id="75">SSD算法虽采用多尺度预测, 但不同尺度特征相互独立, 其低层次特征位置信息较好, 但分类精度差。为降低低层次特征带来的误差, SSD算法从VGG-16网络中偏后的conv4_3卷积层开始构建多尺度特征图, 其检测小目标性能较差。此外, SSD算法的输入图像尺寸较小, 文献<citation id="155" type="reference">[<a class="sup">9</a>]</citation>选取的输入图像尺寸为300 pixel×300 pixel, 小尺寸下能够提升目标检测速度, 但对于遥感图像自动化检测任务, 将图像固定至小尺寸进行检测, 易造成目标特征信息的丢失。为实现良好检测性能, 本研究固定输入图像为800 pixel×800 pixel, 导致计算量大幅增加, 故采用原来网络来降低检测速度。为解决这些问题, 对现有SSD算法做以下改进:1) 参考密集连接的DenseNet网络结构, 重新设计SSD算法中的基础网络;2) 利用FPN方法对密集连接网络的dense模块构建特征金字塔, 再进行多尺度目标检测。改进后的算法框架如图2所示。</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906044_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 改进SSD算法框架" src="Detail/GetImg?filename=images/GXXB201906044_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 改进SSD算法框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906044_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Framework of improved SSD algorithm</p>

                </div>
                <div class="p1">
                    <p id="77">改进SSD算法基础网络设计由4个dense模块构成, 每个dense模块之间利用过渡层连接, 过渡层通过降维来减少计算量。dense模块内部由一系列的卷积层组成 (如dense模块1由6个卷积层构成) , 每个卷积层的输出设定为32通道特征图, 其输入为前面所有输出在通道维度上进行连接后的特征图, 输出表示为<citation id="156" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation><sup></sup></p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false"> (</mo><mo stretchy="false">[</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">]</mo><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">式中:<b><i>H</i></b><sub><i>k</i></sub> (·) 为非线性转化函数, 包括批归一化 (batch normalization) <citation id="157" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、非线性激活、池化、卷积等操作。本研究在<b><i>H</i></b><sub><i>k</i></sub> (·) 中引入批归一化操作以加快收敛速度, 在<b><i>H</i></b><sub><sub><i>k</i></sub></sub> (·) 中的卷积操作设计由1×1卷积和3×3卷积构成, <b><i>x</i></b><sub><i>k</i></sub>表示第<i>k</i>层输出, 相比ResNet网络<citation id="158" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, 这是一种密集连接, 直接来自不同层的特征图, 可以实现特征重用, 降低模型参数量。改进后的SSD算法将在5个尺度上通过卷积操作进行预测, 如图2所示, 网络中的dense模块4连接2个带有池化效果的卷积层构成其中2个尺度特征图, 另3个尺度特征图通过对dense模块2、dense模块3以及dense模块4构建特征金字塔获得, 首先, 利用dense模块4经过256个1×1卷积核降维操作后作为特征金字塔中的一个尺度特征图。其次, 该尺度特征图采用最近邻采样法<citation id="159" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>上采样2倍, 并与dense模块3经256个1×1卷积核降维后的特征图进行 element-wise addition<citation id="160" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>连接操作, 获得金字塔特征的第2个尺度特征图。element-wise addition操作可表示为</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">最后, 利用第2个尺度特征图上采样后与dense模块2经256个1×1卷积核降维后的特征图连接, 构建特征金字塔的第3个特征图。相比于SSD算法, 改进后的SSD算法融入了多个高层次特征的语义信息, 可弥补SSD算法低层次特征语义信息差的问题, 而大部分小目标的检测是利用低层次特征图实现, 因此可提升小目标检测精度。另一方面, 由于采用了密集连接的网络结构, 大幅度减少网络中的参数量, 可在一定程度上弥补输入图像尺寸增大带来的计算效率的降低, 而相比于以Faster R-CNN为代表的基于候选框的检测方式, 该方法无须进行区域建议网络的训练过程, 因此更易于训练。图3为改进后的SSD算法在dense模块融合前后特征图输出的对比, 选取了三个通道绘制, 图3 (e) 是图3 (f) 融合图3 (d) 后的特征图, 图3 (d) 是图3 (b) 融合图3 (e) 后的特征图, 显然图3 (d) 和图3 (e) 相比于图3 (b) 和图3 (c) 飞机目标高亮显示, 具备更强的语义信息, 理论上在图3 (d) 和图3 (e) 上预测结果会明显优于图3 (b) 和图3 (c) , 尤其是绝大部分小目标在低层次特征图上预测, 因此可提升小目标检测精度。</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906044_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 融合前后的特征图对比" src="Detail/GetImg?filename=images/GXXB201906044_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 融合前后的特征图对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906044_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Comparison of feature maps before and after integration</p>
                                <p class="img_note"> (a) 输入图像; (b) dense模块2输出; (c) dense模块3输出; (d) dense模块2融合特征后的输出; (e) dense模块3融合特征后的输出; (f) dense模块4输出</p>
                                <p class="img_note"> (a) Input image; (b) output of dense block2; (c) output of dense block3; (d) output of dense block2 with feature integration; (e) output of dense block3 with feature integration; (f) output of dense block4</p>

                </div>
                <h4 class="anchor-tag" id="83" name="83"><b>2.3 损失函数及训练方法</b></h4>
                <div class="p1">
                    <p id="84">将<i>SSD</i>算法的损失函数表示为位置损失与分类损失的加权和<citation id="161" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>c</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>g</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mo stretchy="false">[</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>a</mtext><mtext>s</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>c</mi><mo stretchy="false">) </mo><mo>+</mo><mi>α</mi><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>g</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">式中:x表示真实框;c表示预测框;l为预测的位置信息;g为真实框的位置信息;N为与真实框相匹配的先验框个数 (正样本数量) ;L<sub><i>class</i></sub> (x, c) 为分类损失;L<sub><i>loc</i></sub> (x, l, g) 为位置损失;α为权值系数, 本文设定为1。L<sub><i>loc</i></sub> (x, l, g) 借鉴<i>Faster R</i>-<i>CNN</i>的位置回归函数<i>smooth</i><sub><i>L</i>1</sub>, 表示为</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>g</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>s</mtext></mrow></msub></mrow><mi>Ν</mi></munderover><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>∈</mo><mo stretchy="false">{</mo><mi>C</mi><msub><mrow></mrow><mi>X</mi></msub><mo>, </mo><mi>C</mi><msub><mrow></mrow><mi>Y</mi></msub><mo>, </mo><mi>w</mi><mo>, </mo><mi>h</mi><mo stretchy="false">}</mo></mrow></munder><mi>x</mi></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>×</mo></mtd></mtr><mtr><mtd><mrow><mtext>s</mtext><mtext>m</mtext><mtext>o</mtext><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext></mrow><msub><mrow></mrow><mrow><mtext>L</mtext><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>l</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>-</mo><mover accent="true"><mi>g</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">式中:x<mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>i</mtext><mtext>j</mtext></mrow><mrow><mo stretchy="false"> (</mo><mtext>k</mtext><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>∈{0, 1}, 当x<mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>i</mtext><mtext>j</mtext></mrow><mrow><mo stretchy="false"> (</mo><mtext>k</mtext><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>=1时表示第i个先验框与第j个真实框相匹配, 并且类别为k, 否则为0;N<sub><i>pos</i></sub>表示正样例集合; (C<sub>X</sub>, C<sub>Y</sub>, w, h) 分别表示边界框中心像素坐标以及宽高;<mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mtext>g</mtext><mo>^</mo></mover><msubsup><mrow></mrow><mtext>j</mtext><mrow><mo stretchy="false"> (</mo><mtext>m</mtext><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>为编码后的真实框位置参数;<i>l</i><mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mtext>i</mtext><mrow><mo stretchy="false"> (</mo><mtext>m</mtext><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>表示先验框的预测值。smooth<sub>L1</sub>函数可表示为</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mtext>s</mtext><mtext>m</mtext><mtext>o</mtext><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext></mrow><msub><mrow></mrow><mrow><mtext>L</mtext><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>0</mn><mo>.</mo><mn>5</mn><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo></mtd><mtd columnalign="left"><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow><mo>&lt;</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left"><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow><mo>-</mo><mn>0</mn><mo>.</mo><mn>5</mn><mo>, </mo></mtd><mtd columnalign="left"><mtext>e</mtext><mtext>l</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94"><i>L</i><sub>class</sub> (<i>x</i>, <i>c</i>) 函数采用交叉熵损失函数<citation id="162" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>来表示:</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>a</mtext><mtext>s</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>c</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>s</mtext></mrow></msub></mrow><mi>Ν</mi></munderover><mi>x</mi></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow></msubsup><mi>log</mi><mo stretchy="false"> (</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub></mrow></munder><mtext>l</mtext></mstyle><mtext>o</mtext><mtext>g</mtext><mo stretchy="false"> (</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mn>0</mn><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">式中:<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mtext>c</mtext><mo>^</mo></mover><msubsup><mrow></mrow><mtext>i</mtext><mrow><mo stretchy="false"> (</mo><mn>0</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>表示正确且类别为背景预测框的概率;<i>N</i><sub>neg</sub>表示负样例集合;<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mtext>c</mtext><mo>^</mo></mover><msubsup><mrow></mrow><mtext>i</mtext><mrow><mo stretchy="false"> (</mo><mtext>p</mtext><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>为利用softmax函数计算的概率值, 可表示为</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>c</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi>c</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>p</mi></munder><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi>c</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo></mrow></mfrac><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">采用数据增强方式对图像进行增强, 丰富样本数量, 迁移学习带来的特征也优于直接从随机初始化学习的特征<citation id="163" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>, 因此, 基于ImageNet数据集上预训练的模型进行迁移学习, 利用随机梯度下降算法 (SGD) 对损失函数 (3) 式进行优化, 寻求最优解, 在训练过程中学习率逐渐递减, 学习动量为0.9。</p>
                </div>
                <h3 id="101" name="101" class="anchor-tag">3 精度评估模型</h3>
                <div class="p1">
                    <p id="102">以平均准确率均值 (MAP) <citation id="164" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>作为衡量模型训练结果精度的指标。每一个类别都可以根据召回率和准确率绘制一条曲线, 那么准确率均值 (AP) 就是该曲线下的面积, 而MAP是多个类别AP的平均值, AP计算可表示为</p>
                </div>
                <div class="p1">
                    <p id="103" class="code-formula">
                        <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><mrow><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mi>p</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi>r</mi><mo stretchy="false">) </mo><mtext>d</mtext><mi>r</mi><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="104">利用预测框与真实框的交并比 (<i>R</i><sub>IoU</sub>) 作为判定预测真假的前提, Pascal VOC2010数据集<citation id="165" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>以0.5为交并比阈值, 再根据预测框内类别置信度进行判定, 从而确定预测为真或假样例。本研究根据更加全面客观的COCO数据集衡量标准<citation id="166" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>进行衡量。COCO数据集依然采用MAP作为衡量标准, 但衡量更加多样化, 其主要衡量指标如表1所示。</p>
                </div>
                <div class="area_img" id="105">
                    <p class="img_tit">表1 主要衡量指标 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Main metrics</p>
                    <p class="img_note"></p>
                    <table id="105" border="1"><tr><td><br />Metric</td><td>Remarks</td></tr><tr><td><br />MAP</td><td>MAP at <i>R</i><sub>IoU</sub> in {0.5+0.05×<i>m</i>, <i>m</i>=0, 1, …, 9} (primary challenge metric) </td></tr><tr><td><br />MAP<sup><i>R</i><sub>IoU</sub>=0.50</sup></td><td>MAP at <i>R</i><sub>IoU</sub>=0.50 (pascal VOC metric) </td></tr><tr><td><br />MAP<sup><i>R</i><sub>IoU</sub>=0.75</sup></td><td>MAP at <i>R</i><sub>IoU</sub>=0.75 (strict metric) </td></tr><tr><td><br />MAP<sup>small</sup></td><td>MAP for small targets: <i>S</i><sub>area</sub>&lt; (32 pixel) <sup>2</sup></td></tr><tr><td><br />MAP<sup>medium</sup></td><td>MAP for medium targets: (32 pixel) <sup>2</sup>≤<i>S</i><sub>area</sub>≤ (96 pixel) <sup>2</sup></td></tr><tr><td><br />MAP<sup>large</sup></td><td>MAP for large targets: <i>S</i><sub>area</sub>&gt; (96 pixel) <sup>2</sup></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="106" name="106" class="anchor-tag">4 实验分析</h3>
                <h4 class="anchor-tag" id="107" name="107"><b>4.1 实验平台与数据</b></h4>
                <div class="p1">
                    <p id="108">实验硬件采用联想P920工作站, 配置32 G内存及NVIDIA TITAN Xp显卡, 操作系统为Ubuntu 16.04。在Python中基于Tensorflow深度学习框架构建算法模型。</p>
                </div>
                <div class="p1">
                    <p id="109">为验证小目标的检测性能, 参考COCO数据集的大、中、小目标所占比例, 采集制作飞机和运动场2类典型目标数据集, 样本采集工作利用基于ArcGIS API for JavaScript组件在B/S架构下设计开发的遥感影像目标检测训练样本数据在线采集系统开展, 系统界面如图4所示, 通过叠加DigitalGlobe公司的公开多源影像数据、公开民用机场点位数据 (网址:http://ourairports.com) , 快速定位机场位置及主要城市, 采集4个影像瓦片层级影像数据 (第15～18层) 并自动生成对应的XML标签数据, 相对于主流的LabelImg目标检测样本标记软件, 该方法更加高效、便捷。</p>
                </div>
                <div class="p1">
                    <p id="110">采集不同场景下的样本集图片共计2574张, 其中验证集412张, 测试集326张, 统计样本集目标分布情况如表2所示, 依据COCO数据集定义的大、中、小目标划分, 各类数据集均以中、小目标为主, 尤其是测试集的中、小目标占比最大, 可用于对小目标的检测验证, 图5所示为目标实际大小及其对应的目标等级。</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906044_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 训练样本在线采集系统界面" src="Detail/GetImg?filename=images/GXXB201906044_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 训练样本在线采集系统界面  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906044_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Interface of training sample online acquisition system</p>
                                <p class="img_note"> (a) 叠加主要机场点位数据; (b) 飞机样本采集</p>
                                <p class="img_note"> (a) Superimposed main airport point data; (b) aircraft sample collection</p>

                </div>
                <h4 class="anchor-tag" id="112" name="112"><b>4.2 改进SSD算法训练分析</b></h4>
                <div class="p1">
                    <p id="113">数据增强采用随机裁切、随机旋转及缩放操作, 初始学习率设定为0.01, 采用余弦函数衰减法<citation id="167" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>对学习率进行衰减, 学习率衰减曲线如图6所示。</p>
                </div>
                <div class="p1">
                    <p id="114">采用随机梯度下降法对总损失函数进行优化训练, 设定批处理尺寸为4, 训练曲线及MAP<sup><i>R</i><sub>IoU</sub>=0.50</sup>如图7所示, 同时也利用ImageNet数据集上预训练的DenseNet对改进SSD算法基础网络的部分权值参数进行迁移学习的训练曲线进行绘制, 作为对比实验。由图7 (a) 可知, 两种训练方式的总损失最终均能收敛至同一水准 (0.1以内) , 并且收敛效果均较好, 由图7 (b) 可知, 部分权值迁移训练方式在前期训练过程中精度增幅较快, 但后期基本保持一致, 最终MAP<sup><i>R</i><sub>IoU</sub>=0.50</sup>为90.55%, 较随机初始化仅增加0.53%, 增幅较小。综上所述, 改进后的SSD模型易于训练, 在反向传播过程中梯度不易发散, 并且在无迁移训练支撑下能够获得较理想的结果, 验证了改进SSD算法网络结构的有效性。</p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit">表2 样本集统计 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Sample set statistics</p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td rowspan="2">Data set</td><td rowspan="2">Class</td><td colspan="5"><br />Target amount</td><td colspan="5"><br />Percentage /%</td></tr><tr><td><br />Small</td><td>Medium</td><td>Large</td><td colspan="2">Total</td><td colspan="2"><br />Small</td><td colspan="2">Medium</td><td>Large</td></tr><tr><td rowspan="2">Training set</td><td><br />airplane</td><td>1204</td><td>1542</td><td>431</td><td rowspan="2">4401</td><td colspan="2"><br />27.36</td><td colspan="2">35.04</td><td colspan="2">9.79</td></tr><tr><td><br />playground</td><td>178</td><td>516</td><td>530</td><td colspan="2"><br />4.04</td><td colspan="2">11.72</td><td colspan="2">12.04</td></tr><tr><td rowspan="2"><br />Validation set</td><td><br />airplane</td><td>390</td><td>427</td><td>74</td><td rowspan="2">1169</td><td colspan="2"><br />33.36</td><td colspan="2">36.53</td><td colspan="2">6.33</td></tr><tr><td><br />playground</td><td>27</td><td>120</td><td>131</td><td colspan="2"><br />2.31</td><td colspan="2">10.27</td><td colspan="2">14.21</td></tr><tr><td rowspan="2"><br />Test set</td><td><br />airplane</td><td>940</td><td>366</td><td>111</td><td rowspan="2">1892</td><td colspan="2"><br />49.68</td><td colspan="2">19.34</td><td colspan="2">5.87</td></tr><tr><td><br />playground</td><td>133</td><td>294</td><td>48</td><td colspan="2"><br />7.03</td><td colspan="2">15.54</td><td colspan="2">2.54</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906044_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 样本集各类大小目标示意图" src="Detail/GetImg?filename=images/GXXB201906044_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 样本集各类大小目标示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906044_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Size of each target in sample set</p>

                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906044_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 学习率衰减曲线" src="Detail/GetImg?filename=images/GXXB201906044_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 学习率衰减曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906044_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Decay curve of learning rate</p>

                </div>
                <h4 class="anchor-tag" id="119" name="119"><b>4.3 改进SSD算法精度分析</b></h4>
                <div class="p1">
                    <p id="120">为验证改进SSD算法的性能, 同时利用训练集训练了基于候选框方式的Faster R-CNN和R-FCN两种算法作为对比, 并在验证集上进行精度验证, Faster R-CNN分别以ResNet50和ResNet101两种网络作为基础网络, R-FCN以ResNet101作为基础网络, 利用精度更高的Inceptionv2<citation id="168" type="reference"><link href="63" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>网络替换SSD算法的VGG-16, 也参与对比实验。以上算法图像的输入尺寸均为800 pixel×800 pixel, 除改进SSD算法外, 其余算法均利用COCO数据集上的预训练模型进行迁移学习, 改进SSD算法利用4.2节所提的迁移方式进行部分参数迁移, 以上算法均训练迭代1.5×10<sup>5</sup>次, 并分别在验证集和测试集上进行精度分析统计。单张影像预测耗时及验证集精度指标的对比结果如表3所示, 验证集精度随迭代次数变化情况如图8所示, 测试集上的精度对比如表4所示。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906044_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 迁移训练和随机初始化两种方式总损失和精度对比" src="Detail/GetImg?filename=images/GXXB201906044_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 迁移训练和随机初始化两种方式总损失和精度对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906044_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Comparison of total loss and precision between transfer training and random initialization</p>
                                <p class="img_note"> (a) 总损失随迭代次数变化; (b) MAP<sup>R</sup>IoU<sup>=0.50</sup>随迭代次数变化</p>
                                <p class="img_note"> (a) Total loss varies with number of iterations; (b) MAP<sup>R</sup>IoU<sup>=0.50 </sup>varies with number of iterations</p>

                </div>
                <div class="area_img" id="123">
                    <p class="img_tit">表3 计算耗时及验证集上精度对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Comparison of calculation time and precision on validation set</p>
                    <p class="img_note"></p>
                    <table id="123" border="1"><tr><td rowspan="2">Method</td><td rowspan="2">Parameter /MB</td><td rowspan="2">Time <br />overhead /ms</td><td colspan="6"><br />Metric /%</td></tr><tr><td>MAP</td><td>MAP<sup>large</sup></td><td>MAP<sup>medium</sup></td><td>MAP<sup>small</sup></td><td>MAP<sup><i>R</i><sub>IoU</sub>=0.50</sup></td><td>MAP<sup><i>R</i><sub>IoU</sub>=0.75</sup></td></tr><tr><td>SSD+Inceptionv2</td><td><b>53.4</b></td><td><b>24.8</b></td><td>47.15</td><td>69.48</td><td>50.16</td><td>11.56</td><td>85.08</td><td>48.95</td></tr><tr><td><br />Faster R-CNN+ResNet50</td><td>173.3</td><td>108.6</td><td>47.12</td><td>72.81</td><td>48.91</td><td>10.16</td><td>82.72</td><td>50.59</td></tr><tr><td><br />Faster R-CNN+ResNet101</td><td>249.5</td><td>117.5</td><td>50.50</td><td>73.06</td><td>53.51</td><td>13.76</td><td>85.84</td><td>55.03</td></tr><tr><td><br />R-FCN+ResNet101</td><td>258.2</td><td>79.3</td><td>51.17</td><td><b>74.69</b></td><td>51.43</td><td>16.01</td><td>87.20</td><td>55.86</td></tr><tr><td><br />Improved SSD algorithm</td><td>59.8</td><td>71.8</td><td><b>54.14</b></td><td>73.31</td><td><b>54.32</b></td><td><b>21.16</b></td><td><b>90.55</b></td><td><b>57.38</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906044_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 改进SSD算法与其他算法随迭代次数精度变化对比" src="Detail/GetImg?filename=images/GXXB201906044_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 改进SSD算法与其他算法随迭代次数精度变化对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906044_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Comparison of precisions of improved SSD algorithm and other algorithms varying with number of iterations</p>
                                <p class="img_note"> (a) MAP; (b) MAP<sup>large</sup>; (c) MAP<sup>medium</sup>; (d) MAP<sup>small</sup>; (e) MAP<sup>R</sup>IoU<sup>=0.50</sup>; (f) MAP<sup>R</sup>IoU<sup>=0.75</sup></p>
                                <p class="img_note"> (a) MAP; (b) MAP<sup>large</sup>; (c) MAP<sup>medium</sup>; (d) MAP<sup>small</sup>; (e) MAP<sup>R</sup>IoU<sup>=0.50</sup>; (f) MAP<sup>R</sup>IoU<sup>=0.75</sup></p>

                </div>
                <div class="area_img" id="125">
                    <p class="img_tit">表4 测试集上精度对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Comparison of precision on test set</p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td rowspan="2"><br />Methods</td><td colspan="6"><br />Metric /%</td></tr><tr><td><br />MAP</td><td>MAP<sup>large</sup></td><td>MAP<sup>medium</sup></td><td>MAP<sup>small</sup></td><td>MAP<sup><i>R</i><sub>IoU</sub>=0.50</sup></td><td>MAP<sup><i>R</i><sub>IoU</sub>=0.75</sup></td></tr><tr><td>SSD+Inceptionv2</td><td>35.85</td><td>62.73</td><td>43.55</td><td>19.52</td><td>77.72</td><td>24.07</td></tr><tr><td><br />Faster R-CNN+ResNet50</td><td>28.72</td><td>61.53</td><td>34.67</td><td>13.43</td><td>68.87</td><td>17.60</td></tr><tr><td><br />Faster R-CNN+ResNet101</td><td>36.05</td><td>61.55</td><td>42.83</td><td>21.74</td><td>76.83</td><td>27.69</td></tr><tr><td><br />R-FCN+ResNet101</td><td>36.70</td><td>59.18</td><td>43.96</td><td>22.91</td><td>77.30</td><td>28.23</td></tr><tr><td><br />Improved SSD algorithm</td><td><b>45.18</b></td><td><b>65.31</b></td><td><b>50.68</b></td><td><b>31.65</b></td><td><b>83.95</b></td><td><b>42.15</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="126">由表3可知, 改进SSD算法的模型参数量较SSD+Inceptionv2仅增加了6.4 MB, 但相对于Faster R-CNN和R-FCN算法降低较多, 尤其是仅为R-FCN+ResNet101的23%。从预测单张影像时间开销来看, 改进SSD算法较SSD+Inceptionv2增加47 ms, 但相对于Faster R-CNN和R-FCN算法有所降低, 尤其是相对于Faster R-CNN+ResNet101降低45.7 ms。从精度指标来看, 改进SSD算法除验证集上的大目标检测精度MAP<sup>large</sup>相对于R-FCN+ResNet101降低了1.38%外, 其余指标均优于其他方法, 尤其是在检测小目标上优势更明显, 在测试集上, MAP<sup>small</sup>相比于次高的R-FCN+ResNet101增加了13.92%, 说明改进SSD算法在检测中、小型目标上优势明显, 如表4所示。由图8可知, Faster R-CNN和R-FCN算法相对于改进SSD算法收敛较快, 说明改进后的SSD算法收敛时间相对较长, 这与其中部分参数随机初始化具有一定关系, 对比改进SSD算法与SSD+Inceptionv2算法可知, SSD+Inceptionv2精度随着迭代步数的变化起伏较大, 尤其是图8 (d) 中小目标的精度突变最为明显, 说明SSD算法对小目标数据集训练效果差, 进一步验证了SSD低层次特征对于小目标识别较差, 但改进后的SSD算法能够较好适应小目标数据, 改进SSD算法的各精度指标变化总体较为平稳。</p>
                </div>
                <div class="p1">
                    <p id="127">综上可知, 改进SSD算法精度较SSD、Faster R-CNN和R-FCN算法有较大提升, 尤其是在检测小目标上优势更明显;改进SSD算法对单张图像预测耗时较Faster R-CNN算法和R-FCN算法有所降低, 尤其是相对Faster R-CNN算法降低较大, 但由于特征金字塔的构建, 相对于SSD算法有所增加;改进的SSD算法能够解决SSD算法对小目标数据集的适应效果差的问题。</p>
                </div>
                <h4 class="anchor-tag" id="128" name="128"><b>4.4 检测效果对比</b></h4>
                <div class="p1">
                    <p id="129">通过4.3节的分析可知, 以ResNet101为基础网络的Faster R-CNN算法和R-FCN算法的检测精度较高, 本节以这两种算法作为对比, 分析改进后的SSD算法检测小目标的效果, 如图9所示, 预测图像为2张分别带有飞机目标和运动场目标的大尺寸影像, 缩小至800 pixel×800 pixel的输入尺寸后, 目标将表现小目标特征, 图9中列出了检测效果图的局部区域放大图。由图9 (a) 和图9 (b) 可知, Faster R-CNN算法和R-FCN算法检测边框的置信度较高, 但存在一些误检, 误检框主要出现在对小目标的识别。由图9 (c) 可知, 所有小目标均被检测出, 且没有误检框, 尽管边界框置信度相对Faster R-CNN算法和R-FCN算法较低, 但不影响最终结果的判断, 主要原因是改进SSD算法属于一体化检测方式, 没有进行候选框预训练这个过程, 相对于基于候选框的检测方式较低。对比图9的所有放大图的检测边框位置信息可知, 改进后的SSD算法预测边框位置回归准确度明显高于Faster R-CNN算法和R-FCN算法。</p>
                </div>
                <h3 id="130" name="130" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="131">对现有SSD算法进行改进, 大幅度提升了算法的计算精度, 解决了原有SSD算法小目标检测精度低而现有基于候选框的方法检测速度慢的问题。通过对以飞机和运动场中小型样本为主的检测实验进行分析, 可得到以下结论。1) 改进后的SSD算法的网络结构设计更合理, 且训练过程更容易收敛, 在无迁移学习支撑下也能够获得较理想的结果。2) 改进后的SSD算法相对于原SSD算法、Faster R-CNN算法以及R-FCN算法总体精度有大幅度提升, 尤其是在检测小目标上, 效果更加明显, 测试集上相比于精度次高的R-FCN算法可提升13.92%, 说明这种高层次特征与低层次特征融合具有较好的语义和位置信息。3) 从目标检测时间开销来看, 改进后的SSD算法预测单张影像相比于Faster R-CNN算法降低45.7 ms, 相比于速度较快的R-FCN算法降低7.5 ms, 但相比于原SSD算法有所增加, 主要原因是特征金字塔构建耗时过多。4) 从预测效果来看, 改进后的SSD算法预测位置边框要优于原SSD算法、Faster R-CNN算法以及R-FCN算法, 其置信度相比Faster R-CNN算法以及R-FCN算法更低, 但不影响最终的预测结果。</p>
                </div>
                <div class="area_img" id="132">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906044_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 改进SSD与其他方法检测效果对比" src="Detail/GetImg?filename=images/GXXB201906044_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 改进SSD与其他方法检测效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906044_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Comparison of improved SSD algorithm and other algorithms in detection effect</p>
                                <p class="img_note"> (a) Faster R-CNN+ResNet101方法; (b) R-FCN+ResNet101方法; (c) 改进SSD算法</p>
                                <p class="img_note"> (a) Faster R-CNN+ResNet101; (b) R-FCN+ResNet101; (c) improved SSD algorithm</p>

                </div>
                <div class="p1">
                    <p id="134">改进的SSD算法对于将深度学习方法高效地用于大区域遥感影像目标检索具有一定参考价值, 尤其是在遥感影像小目标的检测上具有较大优势。尽管改进的SSD算法较基于候选框的目标检测方法时间开销少, 但相比于原SSD算法耗时依然过多, 下一步将对网络结构进一步优化, 提升其计算速度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201101005&amp;v=MzIzOTF1Rnl2bldydkxJVGZTZHJHNEg5RE1ybzlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Wang G X, Huang X T, Zhou Z M.UWB SAR change detection of target in foliage based on local statistic distribution change analysis[J].Journal of Electronics &amp; Information Technology, 2011, 33 (1) :49-54.王广学, 黄晓涛, 周智敏.基于邻域统计分布变化分析的UWB SAR隐蔽目标变化检测[J].电子与信息学报, 2011, 33 (1) :49-54.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=2008196104.nh&amp;v=MTQ2MDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnl2bldydkxWMTI3RnJLeEdORE1xNUViUElRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Wu W.Research on knowledge-based target recognition and tracking techniques[D].Harbin:Harbin Institute of Technology, 2007:10-28.吴畏.基于知识的目标识别与跟踪技术研究[D].哈尔滨:哈尔滨工业大学, 2007:10-28.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201504008&amp;v=MDM5MDJ6VGJMRzRIOVRNcTQ5RmJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5dm5XcnZMUEQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Cao J Z, Song A G.Research on the texture image segmentation method based on Markov random field[J].Chinese Journal of Scientific Instrument, 2015, 36 (4) :776-786.曹家梓, 宋爱国.基于马尔科夫随机场的纹理图像分割方法研究[J].仪器仪表学报, 2015, 36 (4) :776-786.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet:Large scale visual recognition challenge">

                                <b>[4]</b> Russakovsky O, Deng J, Su H, <i>et al</i>.ImageNet large scale visual recognition challenge[J].International Journal of Computer Vision, 2015, 115 (3) :211-252.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[5]</b> Girshick R.Fast R-CNN[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1440-1448.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">

                                <b>[6]</b> Ren S Q, He K M, Girshick R, <i>et al</i>.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=R-FCN:Object detection Via Region-based fully convolutional networks">

                                <b>[7]</b> Dai J, Li Y, He K, <i>et al</i>.R-FCN:object detection via region-based fully convolutional networks[C]∥NIPS′16 Proceedings of the 30th International Conference on Neural Information Processing Systems, December 5-10, 2016, Barcelona, Spain.USA:Curran Associates Inc., 2016:379-387.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                 Redmon J, Divvala S, Girshick R, <i>et al</i>.You only look once:unified, real-time object detection[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:779-788.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:Single shot multibox detector">

                                <b>[9]</b> Liu W, Anguelov D, Erhan D, <i>et al</i>.SSD:single shot multibox detector[M]//Leibe B, Matas J, Sebe N, <i>et al</i>.Computer Vision-ECCV 2016.Cham:Springer, 2016, 9905:21-37.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Focal loss for dens object detection">

                                <b>[10]</b> Lin T Y, Goyal P, Girshick R, <i>et al</i>.Focal loss for dense object detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018:2858826.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors">

                                <b>[11]</b> Huang J, Rathod V, Sun C, <i>et al</i>.Speed/accuracy trade-offs for modern convolutional object detectors[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:3296-3297.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Microsoft coco:Common objects in context">

                                <b>[12]</b> Lin T Y, Maire M, Belongie S, <i>et al</i>.Microsoft COCO:common objects in context[M]∥Fleet D, Pajdla T, Schiele B, <i>et al</i>.Computer Vision-ECCV 2014.Cham:Springer, 2014, 8693:740-755.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHTB201801015&amp;v=MDAxNTMzenFxQnRHRnJDVVJMT2VaZVZ1Rnl2bldydkxKaVhmYkxHNEg5bk1ybzlFWVlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Xu Y Z, Yao X J, Li X, <i>et al</i>.Object detection in high resolution remote sensing images based on fully convolution networks[J].Bulletin of Surveying and Mapping, 2018 (1) :77-82.徐逸之, 姚晓婧, 李祥, 等.基于全卷积网络的高分辨遥感影像目标检测[J].测绘通报, 2018 (1) :77-82.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 Zhang Z Y.Plane detection in optical remote sensing images based on deep learning[D].Xiamen:Xiamen University, 2016:20-30.张志远.基于深度学习的光学遥感图像飞机检测[D].厦门:厦门大学, 2016:20-30.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DQXX201810015&amp;v=MDAyOTJGeXZuV3J2TElUelRkckc0SDluTnI0OUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Wang J C, Tan X C, Wang Z H, <i>et al</i>.Faster R-CNN deep learning network based object recognition of remote sensing image[J].Journal of Geo-Information Science, 2018, 20 (10) :1500-1508.王金传, 谭喜成, 王召海, 等.基于Faster R-CNN深度网络的遥感影像目标识别方法研究[J].地球信息科学学报, 2018, 20 (10) :1500-1508.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806034&amp;v=MTIzODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeXZuV3J2TElqWFRiTEc0SDluTXFZOUdZSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> Feng X Y, Mei W, Hu D S.Aerial target detection based on improved faster R-CNN[J].Acta Optica Sinica, 2018, 38 (6) :0615004.冯小雨, 梅卫, 胡大帅.基于改进Faster R-CNN的空中目标检测[J].光学学报, 2018, 38 (6) :0615004.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature pyramid networks for object detection">

                                <b>[17]</b> Lin T Y, Dollár P, Girshick R, <i>et al</i>.Feature pyramid networks for object detection[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:936-944.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Densely Connected Convolutional Networks">

                                <b>[18]</b> Huang G, Liu Z, Maaten L V D, <i>et al</i>.Densely connected convolutional networks[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:2261-2269.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">

                                <b>[19]</b> Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2018-12-22].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift">

                                <b>[20]</b> Ioffe S, Szegedy C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]∥ICML′15 Proceedings of the 32nd International Conference on International Conference on Machine Learning, July 6-11, 2015, Lille, France.Massachusetts:JMLR.org, 2015:448-456.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[21]</b> He K M, Zhang X Y, Ren S Q, <i>et al</i>.Deep residual learning for image recognition[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Comparison of Interpolating methods for image resampling">

                                <b>[22]</b> Parker J A, Kenyon R V, Troxel D E.Comparison of interpolating methods for image resampling[J].IEEE Transactions on Medical Imaging, 1983, 2 (1) :31-39.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00000425224&amp;v=MTc0MjE0MUFadWtMWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGU2psVUwzQkpGMD1OajdCYXJPNEh0SE1x&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> de Boer P T, Kroese D P, Mannor S, <i>et al</i>.A tutorial on the cross-entropy method[J].Annals of Operations Research, 2005, 134 (1) :19-67.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=How transferable are features in deep neural networks?">

                                <b>[24]</b> Yosinski J, Clune J, Bengio Y, <i>et al</i>.How transferable are features in deep neural networks?[EB/OL]. (2014-11-06) [2018-12-22].https://arxiv.org/abs/1411.1792.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MTg3MTY0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGU2psVUwzQkpGMD1OajdCYXJPNEh0SFBxWWRIWStJTFkzazV6QmRo&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> Everingham M, van Gool L, Williams C K I, <i>et al</i>.The pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303-338.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SGDR:stochastic gradient descent with warm restarts">

                                <b>[26]</b> Loshchilov I, Hutter F.SGDR:stochastic gradient descent with warm restarts[EB/OL]. (2017-03-03) [2018-12-25].https://arxiv.org/abs/1608.03983.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rethinking the Inception Architecture for Computer Vision">

                                <b>[27]</b> Szegedy C, Vanhoucke V, Ioffe S, <i>et al</i>.Rethinking the inception architecture for computer vision[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:2818-2826.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201906044" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201906044&amp;v=MjgxNDVMRzRIOWpNcVk5QllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5dm5XcnZMSWpYVGI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

