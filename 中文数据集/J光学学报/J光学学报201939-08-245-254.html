

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133844344815000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201908029%26RESULT%3d1%26SIGN%3dauV%252ff1T%252f5ZmzFBbrUFUXrKWwaT0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201908029&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201908029&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201908029&amp;v=MzAxMDZPZVplVnVGeW5tVXIzS0lqWFRiTEc0SDlqTXA0OUhiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#38" data-title="2 基本原理 ">2 基本原理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="&lt;b&gt;2.1 增强型点对特征&lt;/b&gt;"><b>2.1 增强型点对特征</b></a></li>
                                                <li><a href="#48" data-title="&lt;b&gt;2.2 视点可见性约束&lt;/b&gt;"><b>2.2 视点可见性约束</b></a></li>
                                                <li><a href="#54" data-title="&lt;b&gt;2.3 三维目标识别定位算法&lt;/b&gt;"><b>2.3 三维目标识别定位算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#66" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#68" data-title="&lt;b&gt;3.1 数据集介绍&lt;/b&gt;"><b>3.1 数据集介绍</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;3.2 模型哈希表建立&lt;/b&gt;"><b>3.2 模型哈希表建立</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;3.3&lt;/b&gt; UWA&lt;b&gt;数据集识别结果分析&lt;/b&gt;"><b>3.3</b> UWA<b>数据集识别结果分析</b></a></li>
                                                <li><a href="#82" data-title="&lt;b&gt;3.4 实际采集的数据集的识别结果分析&lt;/b&gt;"><b>3.4 实际采集的数据集的识别结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#94" data-title="4 结  论 ">4 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#47" data-title="图1 原始点对特征二义性示意图。">图1 原始点对特征二义性示意图。</a></li>
                                                <li><a href="#50" data-title="图2 视点可见性约束。">图2 视点可见性约束。</a></li>
                                                <li><a href="#53" data-title="图3 视点可见性约束。">图3 视点可见性约束。</a></li>
                                                <li><a href="#56" data-title="图4 基于增强型点对特征的三维目标识别算法流程图">图4 基于增强型点对特征的三维目标识别算法流程图</a></li>
                                                <li><a href="#72" data-title="图5 实际采集的数据集。">图5 实际采集的数据集。</a></li>
                                                <li><a href="#73" data-title="图6 UWA数据集中的5个模型以及两个场景">图6 UWA数据集中的5个模型以及两个场景</a></li>
                                                <li><a href="#78" data-title="表1 6个目标模型的基本情况介绍">表1 6个目标模型的基本情况介绍</a></li>
                                                <li><a href="#80" data-title="表2 本文方法在整个UWA数据集上的识别结果">表2 本文方法在整个UWA数据集上的识别结果</a></li>
                                                <li><a href="#81" data-title="表3 2种方法在UWA数据集上的识别结果对比 (目标模型的遮挡率低于84%) ">表3 2种方法在UWA数据集上的识别结果对比 (目标模型的遮挡率低于84%) </a></li>
                                                <li><a href="#85" data-title="图7 R1数据集上8个场景的识别结果">图7 R1数据集上8个场景的识别结果</a></li>
                                                <li><a href="#86" data-title="图8 增强型点对特征与原始点对特征方法在R1数据集上的计算效率对比">图8 增强型点对特征与原始点对特征方法在R1数据集上的计算效率对比</a></li>
                                                <li><a href="#89" data-title="图9 R2数据集上S1～S4场景的识别结果">图9 R2数据集上S1～S4场景的识别结果</a></li>
                                                <li><a href="#90" data-title="图10 在R2数据集上的计算时间对比。">图10 在R2数据集上的计算时间对比。</a></li>
                                                <li><a href="#91" data-title="图11 R3数据集上5个场景S1～S5的识别结果">图11 R3数据集上5个场景S1～S5的识别结果</a></li>
                                                <li><a href="#92" data-title="图12 R3数据集上的计算时间对比。">图12 R3数据集上的计算时间对比。</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="9">


                                    <a id="bibliography_1" title=" Ulrich M, Wiedemann C, Steger C.Combining scale-space and similarity-based aspect graphs for fast 3D object recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (10) :1902-1914." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Combining Scale-Space and Similarity-Based Aspect Graphs for Fast 3D Object Recognition">
                                        <b>[1]</b>
                                         Ulrich M, Wiedemann C, Steger C.Combining scale-space and similarity-based aspect graphs for fast 3D object recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (10) :1902-1914.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_2" title=" Hinterstoisser S, Cagniart C, Ilic S, &lt;i&gt;et al&lt;/i&gt;.Gradient response maps for real-time detection of textureless objects[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (5) :876-888." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gradient response maps for real-time detection of textureless objects">
                                        <b>[2]</b>
                                         Hinterstoisser S, Cagniart C, Ilic S, &lt;i&gt;et al&lt;/i&gt;.Gradient response maps for real-time detection of textureless objects[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (5) :876-888.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_3" title=" Pan W, Zhu F, Hao Y M, &lt;i&gt;et al&lt;/i&gt;.Pose measurement method of three-dimensional object based on multi-sensor[J].Acta Optica Sinica, 2019, 39 (2) :0212007.潘旺, 朱枫, 郝颖明, 等.基于多传感器的三维目标位姿测量方法[J].光学学报, 2019, 39 (2) :0212007." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902024&amp;v=MDkwNDBGckNVUkxPZVplVnVGeW5tVXIzS0lqWFRiTEc0SDlqTXJZOUhZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Pan W, Zhu F, Hao Y M, &lt;i&gt;et al&lt;/i&gt;.Pose measurement method of three-dimensional object based on multi-sensor[J].Acta Optica Sinica, 2019, 39 (2) :0212007.潘旺, 朱枫, 郝颖明, 等.基于多传感器的三维目标位姿测量方法[J].光学学报, 2019, 39 (2) :0212007.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_4" title=" Tombari F, Salti S, di Stefano L.Performance evaluation of 3D keypoint detectors[J].International Journal of Computer Vision, 2013, 102 (1/2/3) :198-220." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130410188993&amp;v=MDg5NDJpZlp1OXVGQ3JsVTc3TElWd1dOajdCYXJLN0h0WE5yNDVOYk9JR0R4TTh6eFVTbURkOVNIN24zeEU5ZmJ2bkty&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Tombari F, Salti S, di Stefano L.Performance evaluation of 3D keypoint detectors[J].International Journal of Computer Vision, 2013, 102 (1/2/3) :198-220.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_5" title=" Guo Y L, Bennamoun M, Sohel F, &lt;i&gt;et al&lt;/i&gt;.3D object recognition in cluttered scenes with local surface features:a survey[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014, 36 (11) :2270-2287." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D Object Recognition in Cluttered Scenes with Local Surface Features:A Survey">
                                        <b>[5]</b>
                                         Guo Y L, Bennamoun M, Sohel F, &lt;i&gt;et al&lt;/i&gt;.3D object recognition in cluttered scenes with local surface features:a survey[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014, 36 (11) :2270-2287.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_6" title=" Yang J Q, Xian K, Xiao Y, &lt;i&gt;et al&lt;/i&gt;.Performance evaluation of 3D correspondence grouping algorithms[C]∥2017 International Conference on 3D Vision (3DV) , October 10-12, 2017, Qingdao.New York:IEEE, 2017:467-176." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Performance evaluation of 3D correspondence grouping algorithms">
                                        <b>[6]</b>
                                         Yang J Q, Xian K, Xiao Y, &lt;i&gt;et al&lt;/i&gt;.Performance evaluation of 3D correspondence grouping algorithms[C]∥2017 International Conference on 3D Vision (3DV) , October 10-12, 2017, Qingdao.New York:IEEE, 2017:467-176.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_7" title=" Schnabel R, Wahl R, Klein R.Efficient RANSAC for point-cloud shape detection[J].Computer Graphics Forum, 2007, 26 (2) :214-226." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000017953&amp;v=MTkyNjROaWZjYXJPNEh0SE1yNDVDYmU0TVkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RlNqbFVyekpJbHc9&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Schnabel R, Wahl R, Klein R.Efficient RANSAC for point-cloud shape detection[J].Computer Graphics Forum, 2007, 26 (2) :214-226.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_8" title=" Drost B, Ulrich M, Navab N, &lt;i&gt;et al&lt;/i&gt;.Model globally, match locally:efficient and robust 3D object recognition[C]∥2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:998-1005." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Model globally,match locally:Efficient and robust3D object recognition">
                                        <b>[8]</b>
                                         Drost B, Ulrich M, Navab N, &lt;i&gt;et al&lt;/i&gt;.Model globally, match locally:efficient and robust 3D object recognition[C]∥2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:998-1005.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_9" title=" Drost B, Ilic S.3D object detection and localization using multimodal point pair features[C]∥2012 Second International Conference on 3D Imaging, Modeling, Processing, Visualization &amp;amp; Transmission, October 13-15, 2012, Zurich, Switzerland.New York:IEEE, 2012:9-16." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D Object Detection and Localization Using Multimodal Point Pair Features">
                                        <b>[9]</b>
                                         Drost B, Ilic S.3D object detection and localization using multimodal point pair features[C]∥2012 Second International Conference on 3D Imaging, Modeling, Processing, Visualization &amp;amp; Transmission, October 13-15, 2012, Zurich, Switzerland.New York:IEEE, 2012:9-16.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_10" title=" Choi C, Christensen H I.3D pose estimation of daily objects using an RGB-D camera[C]//2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, October 7-12, 2012, Vilamoura-Algarve, Portugal.New York:IEEE, 2012:3342-3349." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D pose estimation of daily objects using an RGB-D camera">
                                        <b>[10]</b>
                                         Choi C, Christensen H I.3D pose estimation of daily objects using an RGB-D camera[C]//2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, October 7-12, 2012, Vilamoura-Algarve, Portugal.New York:IEEE, 2012:3342-3349.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_11" title=" Guo Y L, Sohel F, Bennamoun M, &lt;i&gt;et al&lt;/i&gt;.Rotational projection statistics for 3D local surface description and object recognition[J].International Journal of Computer Vision, 2013, 105 (1) :63-86." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13081300000476&amp;v=MjI2NDhhcks3SHRuTnJJOUZaT3NQQ0hzL29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUlWMFZhQkE9Tmo3Qg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Guo Y L, Sohel F, Bennamoun M, &lt;i&gt;et al&lt;/i&gt;.Rotational projection statistics for 3D local surface description and object recognition[J].International Journal of Computer Vision, 2013, 105 (1) :63-86.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_12" title=" Guo Y L, Sohel F, Bennamoun M, &lt;i&gt;et al&lt;/i&gt;.A novel local surface feature for 3D object recognition under clutter and occlusion[J].Information Sciences, 2015, 293:196-213." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15120800999794&amp;v=Mjc0NDZxUVRNbndaZVp1SHlqbVViL0lJVjBWYUJBPU5pZk9mYks5SDlQTXA0OUZiZUlHQzNVOW9CTVQ2VDRQUUgvaXJSZEdlcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Guo Y L, Sohel F, Bennamoun M, &lt;i&gt;et al&lt;/i&gt;.A novel local surface feature for 3D object recognition under clutter and occlusion[J].Information Sciences, 2015, 293:196-213.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-05-08 13:46</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(08),245-254 DOI:10.3788/AOS201939.0815006            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于增强型点对特征的三维目标识别方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%B2%81%E8%8D%A3%E8%8D%A3&amp;code=41392362&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">鲁荣荣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B1%E6%9E%AB&amp;code=09588420&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朱枫</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E6%B8%85%E6%BD%87&amp;code=11224531&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴清潇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E4%BD%9B%E8%AE%A1&amp;code=41736395&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈佛计</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B4%94%E8%8A%B8%E9%98%81&amp;code=41736391&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">崔芸阁</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%94%E7%A0%94%E8%87%AA&amp;code=41736393&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孔研自</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E6%B2%88%E9%98%B3%E8%87%AA%E5%8A%A8%E5%8C%96%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0183762&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院沈阳自动化研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%8E%E6%99%BA%E8%83%BD%E5%88%B6%E9%80%A0%E5%88%9B%E6%96%B0%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院机器人与智能制造创新研究院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%85%89%E7%94%B5%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院光电信息处理重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%BE%BD%E5%AE%81%E7%9C%81%E5%9B%BE%E5%83%8F%E7%90%86%E8%A7%A3%E4%B8%8E%E8%A7%86%E8%A7%89%E8%AE%A1%E7%AE%97%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">辽宁省图像理解与视觉计算重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对基于原始点对特征的三维目标识别算法中存在的内存浪费、效率不高的问题, 提出了一种基于增强型点对特征的三维目标识别算法。通过在原始点对特征的第4个分量上乘以一个符号函数, 得到了一种区分性更强的点对特征, 消除了原始点对特征存在的二义性。考虑到待识别目标三维模型存在的自遮挡, 利用点对之间的视点可见性约束, 剔除了目标三维模型哈希表中存在的大量冗余点对, 节省了内存开销并提高了三维目标识别算法的识别准确率和效率。在开放数据集和实际采集的数据集上的实验结果表明, 与基于原始点对特征的算法相比, 所提三维目标识别算法在识别准确率和效率上都有一定程度的提升。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%82%B9%E5%AF%B9%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">点对特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E7%BB%B4%E7%9B%AE%E6%A0%87%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三维目标识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%AF%E8%A7%81%E6%80%A7%E7%BA%A6%E6%9D%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">可见性约束;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *朱枫 E-mail:fzhu@sia.cn;;
                                </span>
                                <span>
                                    *鲁荣荣 E-mail:lurongrong@sia.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-05</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (U1713216);</span>
                                <span>机器人学国家重点实验室自主课题 (2017-Z21);</span>
                    </p>
            </div>
                    <h1><b>Three-Dimensional Object Recognition Based on Enhanced Point Pair Features</b></h1>
                    <h2>
                    <span>Lu Rongrong</span>
                    <span>Zhu Feng</span>
                    <span>Wu Qingxiao</span>
                    <span>Chen Foji</span>
                    <span>Cui Yunge</span>
                    <span>Kong Yanzi</span>
            </h2>
                    <h2>
                    <span>Shenyang Institute of Automation, Chinese Academy of Sciences</span>
                    <span>Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
                    <span>Key Laboratory of Opto-Electronic Information Process, Chinese Academy of Sciences</span>
                    <span>Key Laboratory of Image Understanding and Computer Vision</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problems of memory waste and low efficiency in three-dimensional (3 D) object recognition algorithm based on original point pair feature (PPF) , a 3 D object recognition algorithm based on enhanced point pair feature (EPPF) is proposed. By multiplying the fourth component of the original PPF with a sign function, a more distinguishing PPF is obtained, which eliminates the ambiguity of the original PPF. Considering the self-occlusion of the 3 D model of the target to be identified, the large numbers of redundant point pairs existing in the target 3 D model hash table are eliminated by means of the viewpoint visibility constraint between the point pairs, which reduces the memory overhead and improves the accuracy and efficiency of the 3 D object recognition algorithm. The experimental results on the open dataset and the actual collected dataset show that the proposed 3 D object recognition algorithm can improve recognition accuracy and recognition efficiency.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=point%20pair%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">point pair feature;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=three-dimensional%20object%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">three-dimensional object recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visible%20constraint&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visible constraint;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-05</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="34">三维 (3D) 目标识别主要包括目标检测与位姿估计两个任务, 即在采集的三维场景数据中检测是否存在目标模型, 如果存在需要返回其在场景中的位置和姿态 (位姿) 。在工业背景下, 待处理的金属零件一般表现为弱纹理、结构单一且表面易产生高光, 这些因素都会给基于图像的二维 (2D) 目标识别算法带来挑战。此外, 遮挡、场景杂乱等因素的干扰, 使得大部分二维目标识别算法不易获取目标精确的三维位姿。与之相比, 三维目标识别算法的处理对象一般为三维点云数据, 其抗光照、视角变化的能力更强, 稳健性和适应性更好。随着近年来消费级三维传感器的兴起, 许多学者从2D视觉研究逐渐转向3D视觉研究, 大大加快了3D视觉领域的研究进展。针对三维目标识别这一视觉任务, 目前主要有三类研究方法:基于视角投影图匹配的方法、基于点特征对应的方法和基于投票的方法。</p>
                </div>
                <div class="p1">
                    <p id="35">基于视角投影图匹配的方法的基本思路是利用一个虚拟相机离线渲染3D模型在不同视角下的2D视图, 从而建立相机位姿与2D视图的对应关系。在线识别阶段, 对这些2D模板逐一在场景图像中进行滑动匹配, 计算目标分布响应图;根据设定的阈值剔除弱响应位置, 并采用极大值抑制的策略获取目标最终的位置;根据当前采用的模板得出目标的三维位姿。这类方法易实现且扩展性好, 对于新增的模型, 只需添加新的2D模板即可。但是, 为了提高位姿估计的精度, 通常需要为每种模型渲染上千幅2D视图模板, 这无疑会增加计算开销, 降低识别效率。Ulrich等<citation id="96" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出基于尺度空间的搜索策略, 加快了识别速度。Hinterstoisser等<citation id="97" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>采用梯度图建立2D模板, 并利用线性化内存的方式加快模板匹配的速度, 但是随着模型数量的增长, 算法的计算成本仍会呈线性递增。潘旺等<citation id="98" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>利用多传感器融合技术, 克服了模板视图退化所引起的误匹配问题, 提高了识别定位算法的稳健性和精度, 但是系统的硬件成本也随之升高。</p>
                </div>
                <div class="p1">
                    <p id="36">基于点特征对应的方法是通过建立模型与场景的点对应关系进行目标识别与定位, 主要包含离线模型特征库建立与在线特征匹配识别两个阶段。离线阶段, 提取每个三维模型表面的特征点并对其进行局部特征描述。在线阶段, 首先利用同样的策略提取场景的特征点并计算相应的局部特征;然后通过场景特征与模型特征的近邻匹配建立若干点对应关系;最后通过对应性分组和位姿假设验证等手段完成目标的识别与定位。该类方法依赖于建立的点对应关系, 适用于几何特征丰富的模型识别。由于该方法采用局部特征匹配的策略, 因此其对遮挡和场景杂乱具有较好的抵抗力。目前针对该类方法的研究热点主要集中在三维特征点的提取、三维特征描述和对应性剔除等环节上, 从相关研究的综述性文献<citation id="99" type="reference">[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</citation>中可获取更多细节信息。</p>
                </div>
                <div class="p1">
                    <p id="37">基于投票的方法主要包含基于随机采样一致性<citation id="100" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和点对特征 (PPF) 投票<citation id="101" type="reference"><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>的三维目标识别方法。基于随机采样一致性的方法用于从场景中提取具有显示几何模型的物体, 如平面结构、圆柱和球体等, 本质上是一种模型拟合的方法。通过预先设定的几何模型, 利用场景点对该模型进行投票, 从而提取相应的几何构型。而基于点对特征投票的方法利用哈希索引的方式建立一系列弱的点对对应关系, 并基于这些点对对应生成众多的候选位姿变换;然后根据位姿的得票数高低选取若干可靠的候选位姿;最后对候选位姿集进行聚类以获取目标的识别结果。点对特征计算简单、灵活性好, 不仅适用于几何特征丰富的物体, 而且可以很好地识别特征较少、结构单一的工业零部件。针对原始点对特征存在的二义性和构建模型点对特征哈希表中出现点对冗余的问题, 本文提出了一种在视点可见性约束的前提下, 基于增强型点对特征 (EPPF) 的三维目标识别方法, 提高了点对特征的区分性并节省了内存开销, 改善了原始算法的识别性能。</p>
                </div>
                <h3 id="38" name="38" class="anchor-tag">2 基本原理</h3>
                <h4 class="anchor-tag" id="39" name="39"><b>2.1 增强型点对特征</b></h4>
                <div class="p1">
                    <p id="40">点对特征是由一对有方向的点之间所包含的4种简单几何属性构成的四元数组。以图1中所示的点对 (<b><i>p</i></b>, <b><i>q</i></b>) 为例, 其点对特征为</p>
                </div>
                <div class="area_img" id="41">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201908029_04100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="42">式中:<b><i>n</i></b><sub>1</sub>、<b><i>n</i></b><sub>2</sub>分别为<b><i>p</i></b>、<b><i>q</i></b>对应的法向量;<b><i>d</i></b>=<b><i>q</i></b>-<b><i>p</i></b>;<image id="43" type="formula" href="images/GXXB201908029_04300.jpg" display="inline" placement="inline"><alt></alt></image>;F为点对特征;∠为角度符号;α、β、θ分别对应三个向量间的夹角。由此定义可知, 点对特征不是对称的, 即通常F (<b><i>p</i></b>, <b><i>q</i></b>) ≠<i>F</i> (<b><i>q</i></b>, <b><i>p</i></b>) 。如果给定 (<b><i>p</i></b>, <b><i>q</i></b>) 的点对特征<i>F</i> (<b><i>p</i></b>, <b><i>q</i></b>) 以及点<b><i>p</i></b>的三维坐标和法向量<b><i>n</i></b><sub>1</sub>, 根据点对特征第1分量和第2分量的约束, 可确定点<b><i>q</i></b>位于空间中的某个圆周上, 该圆周上的点与点<b><i>p</i></b>的距离为<image id="44" type="formula" href="images/GXXB201908029_04400.jpg" display="inline" placement="inline"><alt></alt></image>, 且点<b><i>p</i></b>与该圆周上任意一点的连线与法向量<b><i>n</i></b><sub>1</sub>的夹角为<i>α</i>。在该圆周上任取一点<b><i>q</i></b>, 根据点对特征第3分量和第4分量的约束, 可得到两条满足约束条件的法向量<b><i>n</i></b><sub>2</sub>、<b><i>n</i></b>′<sub>2</sub>, 即∠ (<b><i>n</i></b><sub>2</sub>, <b><i>n</i></b><sub>3</sub>) =∠ (<b><i>n</i></b>′<sub>2</sub>, <b><i>n</i></b><sub>3</sub>) =<i>β</i>, ∠ (<b><i>n</i></b><sub>1</sub>, <b><i>n</i></b><sub>2</sub>) =∠ (<b><i>n</i></b><sub>1</sub>, <b><i>n</i></b>′<sub>2</sub>) =<i>θ</i>, 它们关于由<b><i>n</i></b><sub>1</sub>、<b><i>n</i></b><sub>3</sub>确定的平面<i>Π</i>对称。从这一分析可以看出, 利用原始的点对特征无法区分点<b><i>q</i></b>的法向量究竟是<b><i>n</i></b><sub>2</sub>还是<b><i>n</i></b>′<sub>2</sub>, 将该现象称为原始点对特征的二义性问题。事实上, 三维点的法向量刻画了该点邻域曲面的某种几何属性。该属性对于区分不同位置的点具有一定的参考意义。鉴于此, 提出一种增强型点对特征, 试图保留这一区分性。同样, 以点对 (<b><i>p</i></b>, <b><i>q</i></b>) 为例, 其增强型点对特征为</p>
                </div>
                <div class="area_img" id="45">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201908029_04500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="46">式中:<i>F</i><sub>e</sub>为增强型点对特征;<i>δ</i>为符号函数;<b><i>n</i></b>=<b><i>n</i></b><sub>3</sub>×<b><i>n</i></b><sub>1</sub>, 表示平面<i>Π</i>的法向量;<i>δ</i> (<i>x</i>) =1, 当且仅当<i>x</i>≥0, 否则<i>δ</i> (<i>x</i>) =-1。基于该设定, 点<b><i>q</i></b>的法向量取<b><i>n</i></b><sub>2</sub>或<b><i>n</i></b>′<sub>2</sub>时, 将对应不同的点对特征, 由此便可消除上述二义性问题, 提高点对特征的区分性。</p>
                </div>
                <div class="area_img" id="47">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908029_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 原始点对特征二义性示意图。" src="Detail/GetImg?filename=images/GXXB201908029_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 原始点对特征二义性示意图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908029_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Schematic of ambiguity of original point pair feature. </p>
                                <p class="img_note"> (a) 正视图; (b) 侧视图</p>
                                <p class="img_note"> (a) Front view; (b) side view</p>

                </div>
                <h4 class="anchor-tag" id="48" name="48"><b>2.2 视点可见性约束</b></h4>
                <div class="p1">
                    <p id="49">所谓视点可见性约束是指一对点能否在某个视角下被同时观测到。如图2 (a) 所示, 假设长方体不透明, 上表面的红色点与绿色点总能同时被观测到, 而下表面的蓝色点与红色点则无法同时被观测到, 就称红色点与绿色点满足视点可见性约束, 红色点与蓝色点不满足视点可见性约束。本文所提及的视点可见性问题主要是由物体的自遮挡产生的。</p>
                </div>
                <div class="area_img" id="50">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908029_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 视点可见性约束。" src="Detail/GetImg?filename=images/GXXB201908029_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 视点可见性约束。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908029_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Visible constraint between viewpoints. </p>
                                <p class="img_note"> (a) 视点可见性示意图; (b) 三维模型; (c) 2.5维场景</p>
                                <p class="img_note"> (a) Schematic of visible constraint between two points; (b) 3D model; (c) 2.5D scene</p>

                </div>
                <div class="p1">
                    <p id="51">在基于点对特征的三维目标识别算法中, 需要离线建立三维模型的点对特征哈希表。原始的目标识别算法并没有考虑视点可见性约束, 而是直接计算所有可能的点对特征, 并根据其对应的哈希值将相应的点对映射到模型哈希表中。由于扫描得到的场景点云通常是单一视角下的2.5维 (2.5D) 数据, 如图2 (c) 所示, 即场景中的任意点对之间自动满足视点可见性约束。而待识别的目标通常具有完整的3D模型, 如图2 (b) 所示, 因此模型哈希表中会出现许多不满足视点可见性约束的点对, 这些点对对于三维目标识别算法并没有实质性的贡献, 并且会增加系统的内存开销, 降低算法的运行效率。</p>
                </div>
                <div class="p1">
                    <p id="52">针对这一问题, 提出视点可见性强弱值的概念。给定一对有向点 (<b><i>p</i></b>, <b><i>n</i></b><sub>1</sub>) 与 (<b><i>q</i></b>, <b><i>n</i></b><sub>2</sub>) , 二者之间的可见性强弱值<i>ρ</i> (<b><i>p</i></b>, <b><i>q</i></b>) =<b><i>n</i></b><sub>1</sub>·<b><i>n</i></b><sub>2</sub>, 即二者法向量一致性越好, 可见性概率越高 (同时被观测的概率越大) 。反之, 当二者法向量恰好反向时, 则完全不可见。通过预先设定的最小可见性强弱值<i>τ</i>, 可以从模型中找出与每个点互为可见的点, 即<i>ρ</i> (<b><i>p</i></b>, <b><i>q</i></b>) ≥<i>τ</i>, 则称点对 (<b><i>p</i></b>, <b><i>q</i></b>) 互为可见, 否则不可见。以图2 (b) 中的公鸡模型为例, 选取模型上的一点<b><i>p</i></b>[图3 (a) 中红色标记点], 统计<i>τ</i>取不同值时, 与<b><i>p</i></b>互为可见的点的数量。为了方便观察, 令<i>τ</i>=cos <i>θ</i>。图3 (b) 显示了与<b><i>p</i></b>互为可见的点的数量随着角度<i>θ</i>变化的曲线图, 并给出了<i>θ</i>取30的倍数时与<b><i>p</i></b>互为可见点的分布情况 (绿色代表可见, 灰色代表不可见) 。从图中可以看出, 当<i>θ</i>=90°, 即<i>τ</i>=0时, 与其互为可见的点约为总数的1/2。这说明通过设定合理的可见性阈值, 可以有效剔除模型哈希表中存在的大量冗余点对, 节省内存开销。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908029_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 视点可见性约束。" src="Detail/GetImg?filename=images/GXXB201908029_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 视点可见性约束。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908029_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Viewpoint visibility constraint. </p>
                                <p class="img_note"> (a) 点p的位置; (b) 与p互为可见点的数量及分布随着θ变化的示意图</p>
                                <p class="img_note"> (a) Location of point p; (b) variations in distribution and number of points satisfying visible constraint with point p with θ</p>

                </div>
                <h4 class="anchor-tag" id="54" name="54"><b>2.3 三维目标识别定位算法</b></h4>
                <div class="p1">
                    <p id="55">基于增强型点对特征的三维目标识别算法分为两个阶段:离线训练阶段和在线识别阶段。其中, 离线训练阶段通过将目标三维模型中所有满足视点可见性约束的点对根据其增强型点对特征所生成的哈希值存储到哈希表中, 从而完成对目标模型的训练。在线识别阶段主要分为4个步骤:1) 种子点的选取;2) 位姿投票;3) 假设生成;4) 位姿聚类与验证。整个识别定位算法流程如图4所示, 其中, <i>m</i>为模型点编号;<i>a</i>为离散的旋转角;<i>n</i>为旋转角离散的份数;<i>r</i>为参考点下标;<i>N</i>为模型点的总数;<b><i>R</i></b>和<b><i>t</i></b>分别为目标到场景的旋转变换矩阵和平移向量。</p>
                </div>
                <div class="area_img" id="56">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908029_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 基于增强型点对特征的三维目标识别算法流程图" src="Detail/GetImg?filename=images/GXXB201908029_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 基于增强型点对特征的三维目标识别算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908029_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Flow chart of 3D object recognition based on enhanced point pair feature</p>

                </div>
                <div class="p1">
                    <p id="57">离线训练阶段:由于增强型点对特征的计算利用了模型点的法向量信息, 因此需要预先估计目标模型中每个点的法向量。对于目标模型中的任意点对 (<b><i>p</i></b>, <b><i>q</i></b>) , 如果其视点可见性强弱值<i>ρ</i> (<b><i>p</i></b>, <b><i>q</i></b>) 小于<i>τ</i> (所提方法中将<i>τ</i>设置为0) , 即不满足视点可见性约束, 则不考虑该点对;否则, 根据 (2) 式计算其增强型点对特征<i>F</i><sub>e</sub> (<b><i>p</i></b>, <b><i>q</i></b>) 。所提方法采取的哈希策略是将<i>F</i><sub>e</sub> (<b><i>p</i></b>, <b><i>q</i></b>) 的4个分量分别以固定步长离散化为4个整数, 不妨记为<i>N</i><sub>1</sub>, <i>N</i><sub>2</sub>, <i>N</i><sub>3</sub>, <i>N</i><sub>4</sub>, 然后根据这4个整数值计算其相应的哈希值<i>K</i> (<b><i>p</i></b>, <b><i>q</i></b>) =<i>N</i><sub>1</sub><i>N</i><sup>3</sup><sub>ang</sub>+<i>N</i><sub>2</sub><i>N</i><mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>n</mtext><mtext>g</mtext></mrow><mn>2</mn></msubsup></mrow></math></mathml>+<i>N</i><sub>3</sub><i>N</i><sub>ang</sub>+<i>N</i><sub>4</sub>, 其中<i>N</i><sub>ang</sub>为三个角度余弦分量的最大离散数, 所提方法中设置<i>N</i><sub>ang</sub>为15。当视点可见性阈值<i>τ</i>设置为0时, 增强型点对特征的第4个分量的取值范围为[-π/2, π/2], 为了保证离散化得到的整数值非负, 通过增加一个偏置值π/2将其变换到[0, π], 故后三个分量的取值范围皆为[0, π], 则增强型点对特征的第<i>i</i>个分量的离散数<i>N</i><sub><i>i</i></sub>=⎣<i>N</i><sub>ang</sub><i>f</i><sub><i>i</i></sub>/π」|, <i>i</i>=2, 3, 4, 其中<i>f</i><sub><i>i</i></sub>表示增强型点对特征<i>F</i><sub>e</sub> (<b><i>p</i></b>, <b><i>q</i></b>) 经过规范化后的4个分量的数值。由于增强型点对特征的第1个分量是点对之间的欧氏距离, 为了使该分量的离散化能够适应不同模型之间的尺寸变化, 将该分量的离散化步长<i>δ</i><sub>d</sub>设置为0.05<i>D</i><sub>M</sub>, 其中<i>D</i><sub>M</sub>是目标三维模型的直径 (模型中距离最远的两个点之间的距离) , 则<i>N</i><sub>1</sub>=⎣<i>f</i><sub>1</sub>/<i>δ</i><sub>d</sub>」|。最后, 将所有满足视点可见性约束的点对 (<b><i>p</i></b>, <b><i>q</i></b>) 根据其哈希值<i>K</i> (<b><i>p</i></b>, <b><i>q</i></b>) 存储到该模型的哈希表中, 从而完成离线训练。对于所有的目标模型, 其哈希表的构建过程只进行一次, 在线识别之前根据需要预加载即可。</p>
                </div>
                <div class="p1">
                    <p id="59">在线识别阶段:对于一个可能包含待识别目标的2.5D场景, 首先加载目标模型的哈希表并计算场景点云中每个点的法向量, 以便后续增强型点对特征的计算;然后在场景三维点云中随机选取<i>N</i><sub>seed</sub>个点作为种子点。针对每个种子点<b><i>p</i></b><sub>s</sub>, 以该点为中心, 在场景中搜索与之距离不超过半径<i>r</i>的点集<i>Q</i> (<b><i>p</i></b><sub>s</sub>) ={<b><i>q</i></b><sub>1</sub>, <b><i>q</i></b><sub>2</sub>, …, <b><i>q</i></b><sub><i>k</i></sub>}。依次遍历<i>Q</i> (<b><i>p</i></b><sub>s</sub>) 中的每个点<b><i>q</i></b><sub><i>l</i></sub>, <i>l</i>=1, 2, …, <i>N</i><sub><i>k</i></sub>, <i>N</i><sub><i>k</i></sub>为<i>Q</i> (<b><i>p</i></b><sub>s</sub>) 中点的总数, <i>l</i>为下标, 计算点对 (<b><i>p</i></b><sub>s</sub>, <b><i>q</i></b><sub><i>l</i></sub>) 的增强型点对特征<i>F</i><sub>e</sub> (<b><i>p</i></b><sub>s</sub>, <b><i>q</i></b><sub><i>l</i></sub>) 及其关于待识别目标模型的哈希值<i>K</i> (<b><i>p</i></b><sub>s</sub>, <b><i>q</i></b><sub><i>l</i></sub>) , 并根据该哈希值到目标模型的哈希表中查找与之匹配的候选点对集合<i>M</i>[<i>K</i> (<b><i>p</i></b><sub>s</sub>, <b><i>q</i></b><sub><i>l</i></sub>) ]={ (<b><i>m</i></b><sub><i>l</i>1</sub>, <b><i>m</i></b><sub><i>k</i>1</sub>) , (<b><i>m</i></b><sub><i>l</i>2</sub>, <b><i>m</i></b><sub><i>k</i>2</sub>) , …, (<b><i>m</i></b><sub><i>ln</i></sub>, <b><i>m</i></b><sub><i>kn</i></sub>) }。通过场景点对 (<b><i>p</i></b><sub>s</sub>, <b><i>q</i></b><sub><i>l</i></sub>) 与<i>M</i>[<i>K</i> (<b><i>p</i></b><sub>s</sub>, <b><i>q</i></b><sub><i>l</i></sub>) ]中的每个模型点对 (<b><i>m</i></b><sub><i>lj</i></sub>, <b><i>m</i></b><sub><i>kj</i></sub>) (<i>j</i>=1, 2, …, <i>N</i><sub><i>m</i></sub>) 的匹配可以得到一个旋转角<i>a</i><sub><i>lj</i></sub>, 其中<i>N</i><sub><i>m</i></sub>为模型哈希表中相应键值对应的所有点对的总数, <i>j</i>为下标。之后在预先初始化为零的累加器<b><i>A</i></b><sub>s</sub>的坐标 (<b><i>m</i></b><sub><i>lj</i></sub>, <i>a</i><sub><i>lj</i></sub>) 处加1, 其中每个种子点<b><i>p</i></b><sub>s</sub>都有一个与之对应的累加器<b><i>A</i></b><sub>s</sub>, <b><i>A</i></b><sub>s</sub>的行对应目标模型中每个点, 列对应一个将[0, 2π]等间距离散为30份后的夹角值。当种子点<b><i>p</i></b><sub>s</sub>与其邻域点集中的每个点都完成上述计算后, <b><i>A</i></b><sub>s</sub>中值最大的位置对应的行即为目标模型中与之最匹配的点, 列对应的角度即为二者之间的旋转角, 由此可以得到一个候选位姿变换[<b><i>R</i></b><sub>s</sub>, <b><i>t</i></b><sub>s</sub>], <b><i>R</i></b><sub>s</sub>和<b><i>t</i></b><sub>s</sub>分别为旋转矩阵和平移向量。当遍历完所有的种子点后, 可以得到<i>N</i><sub>seed</sub>个候选位姿, 将这些候选位姿按照各自累加器中最高得票数的高低降序重排, 并将得票数低于全局最高得票数一半的候选位姿直接剔除。最终得到的候选位姿集合记为<i>C</i>={ (<b><i>R</i></b><sub>1</sub>, <b><i>t</i></b><sub>1</sub>) , (<b><i>R</i></b><sub>2</sub>, <b><i>t</i></b><sub>2</sub>) , …, (<b><i>R</i></b><sub><i>h</i></sub>, <b><i>t</i></b><sub><i>h</i></sub>) }, 其中<i>h</i>为候选位姿的总数, 集合中每个元素都是由一个旋转矩阵和一个平移向量组成的刚体变换。</p>
                </div>
                <div class="p1">
                    <p id="60">得到候选位姿集<i>C</i>后, 需要从中找出真正的位姿变换 (可能没有) 。由上述分析可知, 只有当种子点落在场景中的目标上, 才可能生成正确的位姿变换。由于场景中可能有其他物体的干扰, 随机选取的<i>N</i><sub>seed</sub>个种子点中通常只有一部分落在目标上。为了从候选位姿集中提取正确的位姿变换, 本文采用聚类策略进行提取。位姿聚类算法步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="61">1) 以<i>C</i>中的第一个位姿建立一个新的类别, 中心为<i>T</i><sub>1</sub>=[<b><i>R</i></b><sub>1</sub>, <b><i>t</i></b><sub>1</sub>];从剩余的位姿集中找到属于该类别的位姿。所谓属于该类别, 是指候选位姿集中的第<i>i</i>个位姿变换[<b><i>R</i></b><sub><i>i</i></sub>, <b><i>t</i></b><sub><i>i</i></sub>]∈<i>C</i>与类别中心[<b><i>R</i></b><sub>1</sub>, <b><i>t</i></b><sub>1</sub>]之间满足以下距离约束, 即</p>
                </div>
                <div class="area_img" id="62">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201908029_06200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="63">式中:<i>ε</i>与<i>σ</i>分别表示旋转矩阵角度阈值以及平移向量距离阈值, 所提方法中将<i>ε</i>与<i>σ</i>分别设置为π/6和0.2<i>D</i><sub>M</sub>;trace (·) 表示求矩阵的迹;∧表示与运算符。保存该类别, 然后从<i>C</i>中剔除所有属于该类别的位姿。</p>
                </div>
                <div class="p1">
                    <p id="64">2) 令<i>C</i>等于剩余的位姿集合, 重复步骤1) , 直至<i>C</i>为空。统计每个类别包含的位姿个数, 并按照包含位姿个数的多少对所有类别进行降序重排, 记第一个类别中包含的位姿个数为<i>N</i><sub>max</sub>。剔除元素个数小于<i>N</i><sub>max</sub>/2的类别, 将剩余的类别返回, 算法结束。</p>
                </div>
                <div class="p1">
                    <p id="65">假设位姿聚类算法最终返回<i>N</i><sub>valid</sub>个类别, 对于每个类别, 求出它的位姿中心。具体的做法是:将平移向量的平均值作为位姿中心的平移向量, 将所有旋转矩阵转化为四元数, 计算四元数的平均值再将其转化为旋转矩阵作为位姿中心的旋转矩阵, 最终得到<i>N</i><sub>valid</sub>个粗匹配位姿。利用每个粗匹配位姿, 将模型变换到场景中, 并通过迭代最近邻 (ICP) 算法进行位姿优化, 如果拟合后的残差大于设定的阈值<i>τ</i><sub>rmes</sub>, 则剔除该位姿, 否则, 计算模型与场景点云的重叠率 (即与模型点距离小于等于<i>d</i><sub>o</sub>的场景点的数量与模型点总数的比值) , 如果重叠率低于设定的阈值<i>τ</i><sub>o</sub>, 则剔除该位姿, 所提方法中分别将<i>τ</i><sub>rmes</sub>、<i>τ</i><sub>o</sub>与<i>d</i><sub>o</sub>设置为3.5 mm、0.15与2<b><i>p</i></b><sub>r</sub> (<b><i>p</i></b><sub>r</sub>表示目标模型的平均分辨率) 。最后返回经过ICP优化并且满足重叠率的位姿作为三维目标识别算法的结果。</p>
                </div>
                <h3 id="66" name="66" class="anchor-tag">3 实验结果与分析</h3>
                <div class="p1">
                    <p id="67">本文算法在联想计算机Y430P上基于Matlab2017b平台实现。计算机的配置为Intel i7-4710MQ 处理器, 8 GB RAM, 64位操作系统, 算法实现没有依赖GPU等并行加速优化工具。所提算法与基于原始点对特征的三维目标识别算法分别在网上公开数据集和实验室实际采集的数据集上进行了效率与识别准确率的对比。下面对实验的具体情况进行介绍分析。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68"><b>3.1 数据集介绍</b></h4>
                <div class="p1">
                    <p id="69">为了测试算法的有效性以及实用性, 分别在实际采集的数据集 (Real dataset) 和西澳大利亚大学 (University of Western Australia, UWA) 三维目标识别数据集<citation id="102" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>上对所提算法进行验证。</p>
                </div>
                <div class="p1">
                    <p id="70">针对工业自动化中常见的机械臂抓取分拣任务, 利用实验室的线结构光扫描平台采集了三种类型的数据集:1) 单目标混叠装箱场景 (R1) , 目标的平均遮挡率约为50%; 2) 多目标混叠装箱场景 (R2) ;3) 多目标随机放置场景 (R3) , 分别如图5 (b) ～ (d) 所示, 目标的平均遮挡率约为45%。这三种类型的数据集分别包含8, 4, 5个不同的场景。其中待识别的目标模型主要包括眼镜盒和鼠标, 它们的3D模型由采集的不同视角的2.5D数据借助Meshlab软件手动拼接而成, 如图5 (a) 所示。在该数据集每个场景中均匀采样50个点, 作为种子点。</p>
                </div>
                <div class="p1">
                    <p id="71">UWA数据集包含5个完整3D模型以及50个2.5D场景 (5个模型与部分场景如图6所示) , 其中, 犀牛模型主要用于干扰。每个2.5D场景中包含4～5个模型, 模型受遮挡的程度范围在65%～95%之间。模型受遮挡的程度是指其在场景中未出现部分的顶点数与整个模型顶点数的比值。遮挡程度为100%即表示场景中没有出现该物体。由于该数据集相对复杂, 故在每个场景均匀采样100个点作为识别算法的种子点。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908029_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 实际采集的数据集。" src="Detail/GetImg?filename=images/GXXB201908029_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 实际采集的数据集。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908029_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Dataset collected in practice. </p>
                                <p class="img_note"> (a) 眼睛盒与鼠标模型; (b) 数据集R1; (c) 数据集R2; (d) 数据集R3</p>
                                <p class="img_note"> (a) Glass box and mouse model; (b) dataset R1; (c) dataset R2; (d) dataset R3</p>

                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908029_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 UWA数据集中的5个模型以及两个场景" src="Detail/GetImg?filename=images/GXXB201908029_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 UWA数据集中的5个模型以及两个场景  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908029_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Five models and two sample scenes of UWA dataset</p>

                </div>
                <h4 class="anchor-tag" id="74" name="74"><b>3.2 模型哈希表建立</b></h4>
                <div class="p1">
                    <p id="75">由于模型的点对特征哈希表需要离线建立, 表1统计了上述数据集中总共6个目标模型的基本情况, 包括模型的顶点数、不考率视点可见性约束的原始点对的个数、视点可见性约束阈值<i>τ</i>=0时的增强型点对的个数以及构建模型哈希表花费的时间成本。可以看到, 本文提出的视点可见性约束可以缩减大约一半的冗余点对, 且大幅加快了构建模型哈希表的速度。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76"><b>3.3</b> UWA<b>数据集识别结果分析</b></h4>
                <div class="p1">
                    <p id="77">所提三维目标识别算法在UWA数据集上的识别结果如表2所示, 分别列出了每个模型的识别情况, 包括正确的识别数、失败的场景编号以及对应模型在该场景中被遮挡的程度。这里需要特别指出的是, 每个模型的识别结果都是以它作为目标, 分别与每个场景进行匹配识别得到的, 即场景中除了它以外的其他物体都是干扰。因为有些识别算法<citation id="104" type="reference"><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>在该数据集上给出的结果是联合识别得到的, 即以4个模型为目标, 分别与每个场景进行匹配。区别在于后者在识别的过程中会将识别到的物体从场景中不断分割出去, 从而使得后面的目标更容易被识别。由表2可知, Chef模型在43号场景中 (图6) 的识别失败, 主要原因是该模型在该场景中被遮挡的程度太高, 其次是由于其他物体对它的干扰。可以看到, 另外三个模型在该场景中都被正确识别。如果按照联合识别的策略, 将容易识别的目标成功识别后分离出场景, 那么当场景中只剩下Chef模型时, 再去识别就没有干扰了。事实上, 对于一个三维目标识别算法而言, 直接无分割式识别更能体现其性能。因此, 本文所有的识别实验结果都是通过在原始场景上直接进行识别得到的。由表2可知, 识别失败的案例大多在原始场景中被遮挡的程度过高, 导致其有效信息不足, 加之场景中其他物体的干扰, 使得点对投票阶段目标模型获得的票数不够, 导致算法最终识别失败。由于文献<citation id="103" type="reference">[<a class="sup">8</a>]</citation>中只统计了遮挡率低于84%的识别结果, 为了与之进行客观比对, 表3给出本文算法在遮挡率低于84%的情形下的识别结果。可以看到, 所提方法较原始点对特征方法识别率略有提升, 但是识别速度较后者有明显提升。这说明利用视点可见性约束缩减模型哈希表并不会降低识别准确率, 且基于增强型点对特征匹配能够加快识别速度。</p>
                </div>
                <div class="area_img" id="78">
                    <p class="img_tit">表1 6个目标模型的基本情况介绍 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Summary of basic information of six target models</p>
                    <p class="img_note"></p>
                    <table id="78" border="1"><tr><td rowspan="2">Model</td><td rowspan="2">Point number</td><td colspan="2"><br />Original PPF</td><td colspan="2"><br />Enhanced PPF</td></tr><tr><td><br />Number</td><td>Time cost /s</td><td><br />Number</td><td>Time cost /s</td></tr><tr><td>Chef</td><td>3351</td><td>11225850</td><td>85.10</td><td>5557422</td><td>34.70</td></tr><tr><td><br />Chicken</td><td>2643</td><td>6982806</td><td>44.00</td><td>3434958</td><td>18.90</td></tr><tr><td><br />Para</td><td>2507</td><td>6282542</td><td>43.20</td><td>3114468</td><td>17.50</td></tr><tr><td><br />T-rex</td><td>2337</td><td>5459232</td><td>35.90</td><td>2713202</td><td>14.80</td></tr><tr><td><br />Glass Box</td><td>1134</td><td>1284822</td><td>3.94</td><td>638622</td><td>1.87</td></tr><tr><td><br />Mouse</td><td>1358</td><td>1842806</td><td>6.16</td><td>916592</td><td>2.96</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="80">
                    <p class="img_tit">表2 本文方法在整个UWA数据集上的识别结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Recognition results of proposed algorithm on whole UWA dataset</p>
                    <p class="img_note"></p>
                    <table id="80" border="1"><tr><td>Model</td><td>Correct number/total number</td><td>Failed scene number</td><td>Occlusion of the targets /%</td></tr><tr><td><br />Chef</td><td>49/50</td><td>43</td><td>91.30</td></tr><tr><td><br />Chicken</td><td>45/48</td><td>6, 26, 32</td><td>89.70, 86.50, 89.50</td></tr><tr><td><br />Parasaurolophus</td><td>40/45</td><td>7, 10, 38, 41, 50</td><td>86.40, 91.40, 89.00, 87.00, 83.90</td></tr><tr><td><br />T-rex</td><td>41/45</td><td>4, 10, 34, 48</td><td>84.00, 80.20, 83.80, 77.30</td></tr><tr><td><br />Average</td><td>175/188 (93.1%) </td><td>-</td><td>-</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="81">
                    <p class="img_tit">表3 2种方法在UWA数据集上的识别结果对比 (目标模型的遮挡率低于84%)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Comparison of two algorithms on the UWA dataset (occlusion of targets is less than 84%) in terms of recognition rate</p>
                    <p class="img_note"></p>
                    <table id="81" border="1"><tr><td><br />Algorithm</td><td>Recognition <br />rate /%</td><td>Time cost for <br />one object /s</td></tr><tr><td><br />Proposed</td><td>97.6</td><td>10</td></tr><tr><td><br />PPF in Ref. [8]</td><td>97.0</td><td>85</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="82" name="82"><b>3.4 实际采集的数据集的识别结果分析</b></h4>
                <div class="p1">
                    <p id="83">分析了本文方法与原始点对特征方法在三种实际采集的数据集上的识别结果。其中原始点对特征方法指的是没有使用增强型点对特征且不考虑视点可见性约束的方法。除此之外, 两种算法的其他流程及参数完全一致, 每个场景选取的种子点也相同。</p>
                </div>
                <div class="p1">
                    <p id="84">图7给出了本文算法对R1数据集上8个场景 (S1-S8) 的识别结果, 图中每个格子从左到右依次是:原始场景、识别结果和识别结果中遮挡率最低的物体。由于原始点对特征与本文算法的识别结果一致, 故只显示了本文算法的结果。图8给出了两种算法在每个场景中的时间开销 (不包括建立模型哈希表花费的时间) 。由图8可知, 在识别率一致的情形下, 本文算法计算效率更高。主要原因是增强型点对特征区分性更强, 使得在候选位姿集生成阶段中与每个场景点对匹配的模型点对更可靠且数量更少, 从而加速了投票过程, 提升了算法的整体效率。从图7中还可以看到, 本文算法能够同时识别出多个目标实例, 这对于提高整个分拣系统的实时性具有重要意义。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908029_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 R1数据集上8个场景的识别结果" src="Detail/GetImg?filename=images/GXXB201908029_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 R1数据集上8个场景的识别结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908029_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Recognition results of eight scenes on R1 dataset</p>

                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908029_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 增强型点对特征与原始点对特征方法在R1数据集上的计算效率对比" src="Detail/GetImg?filename=images/GXXB201908029_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 增强型点对特征与原始点对特征方法在R1数据集上的计算效率对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908029_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Time cost comparison between EPPF and original PPF based methods on R1 dataset</p>

                </div>
                <div class="p1">
                    <p id="87">图9展示了本文算法对R2数据集上4个场景 (S1-S4) 的识别结果, 同样, 由于原始点对特征方法也取得了相同的识别结果, 故没有单独显示。图9中每一列从上到下依次是原始场景、眼镜盒识别结果和鼠标识别结果。图10展示了两种算法在R2数据集上的时间开销对比。可以看到, 本文算法的识别效率仍然优于原始点对特征方法。</p>
                </div>
                <div class="p1">
                    <p id="88">图11显示了本文算法与原始点对方法对R3数据集中5个场景 (S1～S5) 的识别结果, 图12给出了它们的时间开销。与前两种数据集相比, 由于该数据集干扰物相对较多, 所以两种算法整体的时间成本都有所提高, 不过所提方法的时间开销依然优于原始点对特征方法。此外, 从识别准确率上来看, 所提方法在这5个场景中都能准确识别到目标, 识别率为100%;而原始点对特征方法没有正确识别场景1和场景4中的鼠标模型, 识别率为60%。由此可见, 与原始点对特征方法相比, 所提方法的识别可靠性也有所提升。主要原因仍然是增强型点对特征的区分性强于原始点对特征, 从而排除了一些错误的点对匹配, 使得投票阶段能够保留正确的位姿, 才有可能在位姿聚类时从候选位姿集中将其筛选出来。总之, 所提基于增强型点对特征的方法可提升原始点对特征方法的性能。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908029_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 R2数据集上S1～S4场景的识别结果" src="Detail/GetImg?filename=images/GXXB201908029_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 R2数据集上S1～S4场景的识别结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908029_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Recognition results of four scenes S1-S4 on R2 dataset</p>

                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908029_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 在R2数据集上的计算时间对比。" src="Detail/GetImg?filename=images/GXXB201908029_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 在R2数据集上的计算时间对比。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908029_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Time cost comparison on R2 dataset. </p>
                                <p class="img_note"> (a) 增强型点对特征; (b) 原始点对特征</p>
                                <p class="img_note"> (a) EPPF; (b) original PPF</p>

                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908029_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 R3数据集上5个场景S1～S5的识别结果" src="Detail/GetImg?filename=images/GXXB201908029_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 R3数据集上5个场景S1～S5的识别结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908029_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Recognition results of five scenes S1-S5 on R3 dataset</p>

                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908029_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 R3数据集上的计算时间对比。" src="Detail/GetImg?filename=images/GXXB201908029_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 R3数据集上的计算时间对比。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908029_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Time cost comparison on R3 dataset. </p>
                                <p class="img_note"> (a) 增强型点对特征; (b) 原始点对特征</p>
                                <p class="img_note"> (a) EPPF; (b) original PPF</p>

                </div>
                <h3 id="94" name="94" class="anchor-tag">4 结  论</h3>
                <div class="p1">
                    <p id="95">提出了一种新型点对特征方法, 增强了原始点对特征的区分性。通过建立模型点对之间的可见性约束, 剔除了大量冗余点对, 加快了构建模型点对特征哈希表的速度并节省了存储开销。在UWA数据集和实际采集的数据集上得到的识别实验结果表明, 所提方法较原始点对特征方法在识别准确率和识别效率上皆有提升, 证明了所提方法的有效性与实用性。事实上, 对于单目标混叠场景, 可以通过设置少量的种子点进一步加快识别效率。由于单目标混叠场景中的大部分点是由目标实例组成的, 因此有效种子点被选中的概率很大。此外, 对于较复杂的场景, 如果目标在场景中占比较小, 可适当多选一些种子点, 以提高识别准确率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="9">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Combining Scale-Space and Similarity-Based Aspect Graphs for Fast 3D Object Recognition">

                                <b>[1]</b> Ulrich M, Wiedemann C, Steger C.Combining scale-space and similarity-based aspect graphs for fast 3D object recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (10) :1902-1914.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gradient response maps for real-time detection of textureless objects">

                                <b>[2]</b> Hinterstoisser S, Cagniart C, Ilic S, <i>et al</i>.Gradient response maps for real-time detection of textureless objects[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (5) :876-888.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902024&amp;v=MjM4MDVMT2VaZVZ1RnlubVVyM0tJalhUYkxHNEg5ak1yWTlIWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Pan W, Zhu F, Hao Y M, <i>et al</i>.Pose measurement method of three-dimensional object based on multi-sensor[J].Acta Optica Sinica, 2019, 39 (2) :0212007.潘旺, 朱枫, 郝颖明, 等.基于多传感器的三维目标位姿测量方法[J].光学学报, 2019, 39 (2) :0212007.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130410188993&amp;v=MTU4OTBJR0R4TTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkNybFU3N0xJVndXTmo3QmFySzdIdFhOcjQ1TmJP&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Tombari F, Salti S, di Stefano L.Performance evaluation of 3D keypoint detectors[J].International Journal of Computer Vision, 2013, 102 (1/2/3) :198-220.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D Object Recognition in Cluttered Scenes with Local Surface Features:A Survey">

                                <b>[5]</b> Guo Y L, Bennamoun M, Sohel F, <i>et al</i>.3D object recognition in cluttered scenes with local surface features:a survey[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014, 36 (11) :2270-2287.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Performance evaluation of 3D correspondence grouping algorithms">

                                <b>[6]</b> Yang J Q, Xian K, Xiao Y, <i>et al</i>.Performance evaluation of 3D correspondence grouping algorithms[C]∥2017 International Conference on 3D Vision (3DV) , October 10-12, 2017, Qingdao.New York:IEEE, 2017:467-176.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000017953&amp;v=MDY3Mjl0SE1yNDVDYmU0TVkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RlNqbFVyekpJbHc9TmlmY2FyTzRI&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Schnabel R, Wahl R, Klein R.Efficient RANSAC for point-cloud shape detection[J].Computer Graphics Forum, 2007, 26 (2) :214-226.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Model globally,match locally:Efficient and robust3D object recognition">

                                <b>[8]</b> Drost B, Ulrich M, Navab N, <i>et al</i>.Model globally, match locally:efficient and robust 3D object recognition[C]∥2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:998-1005.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D Object Detection and Localization Using Multimodal Point Pair Features">

                                <b>[9]</b> Drost B, Ilic S.3D object detection and localization using multimodal point pair features[C]∥2012 Second International Conference on 3D Imaging, Modeling, Processing, Visualization &amp; Transmission, October 13-15, 2012, Zurich, Switzerland.New York:IEEE, 2012:9-16.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D pose estimation of daily objects using an RGB-D camera">

                                <b>[10]</b> Choi C, Christensen H I.3D pose estimation of daily objects using an RGB-D camera[C]//2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, October 7-12, 2012, Vilamoura-Algarve, Portugal.New York:IEEE, 2012:3342-3349.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13081300000476&amp;v=MjQzNzBNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JSVYwVmFCQT1OajdCYXJLN0h0bk5ySTlGWk9zUENIcy9vQg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Guo Y L, Sohel F, Bennamoun M, <i>et al</i>.Rotational projection statistics for 3D local surface description and object recognition[J].International Journal of Computer Vision, 2013, 105 (1) :63-86.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15120800999794&amp;v=MTUzNTBadUh5am1VYi9JSVYwVmFCQT1OaWZPZmJLOUg5UE1wNDlGYmVJR0MzVTlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Guo Y L, Sohel F, Bennamoun M, <i>et al</i>.A novel local surface feature for 3D object recognition under clutter and occlusion[J].Information Sciences, 2015, 293:196-213.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201908029" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201908029&amp;v=MzAxMDZPZVplVnVGeW5tVXIzS0lqWFRiTEc0SDlqTXA0OUhiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

