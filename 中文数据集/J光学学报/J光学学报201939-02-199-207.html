

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135528099475000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201902024%26RESULT%3d1%26SIGN%3dtoyExoYaVCHO4Xi4mtLA%252boOxTFM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201902024&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201902024&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902024&amp;v=MTY5MjVMRzRIOWpNclk5SFlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRGtWTHZLSWpYVGI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#52" data-title="2 视觉系统构建 ">2 视觉系统构建</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="&lt;b&gt;2.1&lt;/b&gt;&lt;b&gt;多传感器标定&lt;/b&gt;"><b>2.1</b><b>多传感器标定</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#67" data-title="3 定位策略 ">3 定位策略</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#71" data-title="&lt;b&gt;3.1&lt;/b&gt;&lt;b&gt;基于点云的目标分割&lt;/b&gt;"><b>3.1</b><b>基于点云的目标分割</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;3.2&lt;/b&gt;&lt;b&gt;基于灰度图的精确测量&lt;/b&gt;"><b>3.2</b><b>基于灰度图的精确测量</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#102" data-title="4 实验与结果 ">4 实验与结果</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#103" data-title="&lt;b&gt;4.1&lt;/b&gt;&lt;b&gt;实验环境&lt;/b&gt;"><b>4.1</b><b>实验环境</b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;4.2&lt;/b&gt;&lt;b&gt;测量特征提取结果&lt;/b&gt;"><b>4.2</b><b>测量特征提取结果</b></a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;4.3&lt;/b&gt;&lt;b&gt;精度评估&lt;/b&gt;"><b>4.3</b><b>精度评估</b></a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;4.4&lt;/b&gt;&lt;b&gt;效率评估&lt;/b&gt;"><b>4.4</b><b>效率评估</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#132" data-title="5 结 论 ">5 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#54" data-title="图1 视觉系统的坐标系定义和标定">图1 视觉系统的坐标系定义和标定</a></li>
                                                <li><a href="#69" data-title="图2 所提方法的流程图">图2 所提方法的流程图</a></li>
                                                <li><a href="#70" data-title="图3 目标模型和测量结果。 (a) 物体的三维模型和坐标系定义; (b) 位姿测量结果">图3 目标模型和测量结果。 (a) 物体的三维模型和坐标系定义; (b) 位姿测量结果</a></li>
                                                <li><a href="#80" data-title="图4 网格大小对采样剩余点数和RANSAC算法效率的影响">图4 网格大小对采样剩余点数和RANSAC算法效率的影响</a></li>
                                                <li><a href="#92" data-title="图5 目标点云分割结果">图5 目标点云分割结果</a></li>
                                                <li><a href="#95" data-title="图6 分割出的目标点云对应的灰度图像中的点">图6 分割出的目标点云对应的灰度图像中的点</a></li>
                                                <li><a href="#101" data-title="图7 直线在极坐标系下的表示">图7 直线在极坐标系下的表示</a></li>
                                                <li><a href="#118" data-title="图8 实验环境搭建。 (&lt;i&gt;a&lt;/i&gt;) 实验环境; (&lt;i&gt;b&lt;/i&gt;) 待测目标">图8 实验环境搭建。 (<i>a</i>) 实验环境; (<i>b</i>) 待测目标</a></li>
                                                <li><a href="#121" data-title="图9 直线特征提取结果。 (a) 航天扶手; (b) 抽屉把手">图9 直线特征提取结果。 (a) 航天扶手; (b) 抽屉把手</a></li>
                                                <li><a href="#127" data-title="表1 测量精度">表1 测量精度</a></li>
                                                <li><a href="#130" data-title="表2 运行时间对比">表2 运行时间对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Bay H, Ess A, Tuytelaars T, &lt;i&gt;et al&lt;/i&gt;. Speeded-up robust features (SURF) [J]. Computer Vision and Image Understanding, 2008, 110 (3) : 346-359." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501083888&amp;v=MDMyMzVHZXJxUVRNbndaZVp0RmlubFVyekpLRjhUYmhBPU5pZk9mYks3SHRETnFvOUVaT01NQkhReG9CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Bay H, Ess A, Tuytelaars T, &lt;i&gt;et al&lt;/i&gt;. Speeded-up robust features (SURF) [J]. Computer Vision and Image Understanding, 2008, 110 (3) : 346-359.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Hinterstoisser S, Benhimane S, Navab N. N&lt;sub&gt;3&lt;/sub&gt;M: natural 3D markers for real-time object detection and pose estimation[C]. IEEE 11th International Conference on Computer Vision, 2007: 1-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=N3M:Natural 3D Markers for Real-Time Object Detection and Pose Estimation">
                                        <b>[2]</b>
                                         Hinterstoisser S, Benhimane S, Navab N. N&lt;sub&gt;3&lt;/sub&gt;M: natural 3D markers for real-time object detection and pose estimation[C]. IEEE 11th International Conference on Computer Vision, 2007: 1-7.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" Lepetit V, Fua P. Keypoint recognition using randomized trees[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006, 28 (9) : 1465-1479." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Keypoint recognition using randomized trees">
                                        <b>[3]</b>
                                         Lepetit V, Fua P. Keypoint recognition using randomized trees[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006, 28 (9) : 1465-1479.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Ulrich M, Wiedemann C, Steger C. CAD-based recognition of 3D objects in monocular images[C]. IEEE International Conference on Robotics and Automation, 2009: 1191-1198." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cad-Based Recognition Of 3D Objects In Monocular Images">
                                        <b>[4]</b>
                                         Ulrich M, Wiedemann C, Steger C. CAD-based recognition of 3D objects in monocular images[C]. IEEE International Conference on Robotics and Automation, 2009: 1191-1198.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" Ulrich M, Wiedemann C, Steger C. Combining scale-space and similarity-based aspect graphs for fast 3D object recognition[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (10) : 1902-1914." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Combining Scale-Space and Similarity-Based Aspect Graphs for Fast 3D Object Recognition">
                                        <b>[5]</b>
                                         Ulrich M, Wiedemann C, Steger C. Combining scale-space and similarity-based aspect graphs for fast 3D object recognition[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (10) : 1902-1914.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Hinterstoisser S, Holzer S, Cagniart C, &lt;i&gt;et al&lt;/i&gt;. Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes[C]. International Conference on Computer Vision, 2011: 858-865." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multimodal Templates for Real-Time Detection of Texture-less Objects in Heavily Cluttered Scenes">
                                        <b>[6]</b>
                                         Hinterstoisser S, Holzer S, Cagniart C, &lt;i&gt;et al&lt;/i&gt;. Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes[C]. International Conference on Computer Vision, 2011: 858-865.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" Hinterstoisser S, Cagniart C, Ilic S, &lt;i&gt;et al&lt;/i&gt;. Gradient response maps for real-time detection of textureless objects[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (5) : 876-888." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gradient Response Maps for Real-Time Detection of Textureless Objects">
                                        <b>[7]</b>
                                         Hinterstoisser S, Cagniart C, Ilic S, &lt;i&gt;et al&lt;/i&gt;. Gradient response maps for real-time detection of textureless objects[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (5) : 876-888.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" Lepetit V, Moreno-Noguer F, Fua P. EPnP: an accurate &lt;i&gt;O&lt;/i&gt; (&lt;i&gt;n&lt;/i&gt;) solution to the PnP problem[J]. International Journal of Computer Vision, 2009, 81 (2) : 155-166." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15110100320222&amp;v=MjQ2MDFpbmxVcnpKS0Y4VGJoQT1OajdCYXJLOUg5RE1ybzlGWitrUERuNDdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0Rg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Lepetit V, Moreno-Noguer F, Fua P. EPnP: an accurate &lt;i&gt;O&lt;/i&gt; (&lt;i&gt;n&lt;/i&gt;) solution to the PnP problem[J]. International Journal of Computer Vision, 2009, 81 (2) : 155-166.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" Zhang S J, Cao X B, Zhang F, &lt;i&gt;et al&lt;/i&gt;. Monocular vision-based iterative pose estimation algorithm from corresponding feature points[J]. Science China Information Sciences, 2010, 53 (8) : 1682-1696." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003755464&amp;v=MzA0ODBZTzBMWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXZrVzc3UEpGdz1OajdCYXJPNEh0SFBxSXBB&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Zhang S J, Cao X B, Zhang F, &lt;i&gt;et al&lt;/i&gt;. Monocular vision-based iterative pose estimation algorithm from corresponding feature points[J]. Science China Information Sciences, 2010, 53 (8) : 1682-1696.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" Zhu F, Yu F S, Wu Y M, &lt;i&gt;et al&lt;/i&gt;. Analysis of calibration precision of camera attitude angles[J]. Acta Optica Sinica, 2018, 38 (11) : 1115005. 朱帆, 于芳苏, 吴易明, 等. P4P法相机姿态标定精度分析[J]. 光学学报, 2018, 38 (11) : 1115005." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811028&amp;v=MTYwODJYVGJMRzRIOW5Ocm85SGJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRGtWTHZLSWo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Zhu F, Yu F S, Wu Y M, &lt;i&gt;et al&lt;/i&gt;. Analysis of calibration precision of camera attitude angles[J]. Acta Optica Sinica, 2018, 38 (11) : 1115005. 朱帆, 于芳苏, 吴易明, 等. P4P法相机姿态标定精度分析[J]. 光学学报, 2018, 38 (11) : 1115005.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" von Gioi R G, Jakubowicz J, Morel J M, &lt;i&gt;et al&lt;/i&gt;. LSD: a line segment detector[J]. Image Processing on Line, 2012, 2: 35-55." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LSD:a Line Segment Detector">
                                        <b>[11]</b>
                                         von Gioi R G, Jakubowicz J, Morel J M, &lt;i&gt;et al&lt;/i&gt;. LSD: a line segment detector[J]. Image Processing on Line, 2012, 2: 35-55.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" Zhang H J, Fang Z J, Yang G L. RGB-D visual odometry in dynamic environments using line features[J]. Robot, 2018, 40 (5) : 1-8. 张慧娟, 方灶军, 杨桂林. 动态环境下基于线特征的RGB-D视觉里程计[J]. 机器人, 2018, 40 (5) : 1-8." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201901009&amp;v=MTYyNjZHNEg5ak1ybzlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEa1ZMdktMenpaZkw=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Zhang H J, Fang Z J, Yang G L. RGB-D visual odometry in dynamic environments using line features[J]. Robot, 2018, 40 (5) : 1-8. 张慧娟, 方灶军, 杨桂林. 动态环境下基于线特征的RGB-D视觉里程计[J]. 机器人, 2018, 40 (5) : 1-8.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     Zhang Z. A flexible new technique for camera calibration[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22 (11) : 1330-1334.</a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" Song J H, Ren Y J, Yang S R, &lt;i&gt;et al&lt;/i&gt;. Extrinsic parameter calibration method based on substitutable target sphere for vision sensors[J]. Acta Optica Sinica, 2017, 37 (9) : 0915003. 宋佳慧, 任永杰, 杨守瑞, 等. 基于合作靶球的视觉传感器外参标定方法[J]. 光学学报, 2017, 37 (9) : 0915003." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201709025&amp;v=MzE2NDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEa1ZMdktJalhUYkxHNEg5Yk1wbzlIWVlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Song J H, Ren Y J, Yang S R, &lt;i&gt;et al&lt;/i&gt;. Extrinsic parameter calibration method based on substitutable target sphere for vision sensors[J]. Acta Optica Sinica, 2017, 37 (9) : 0915003. 宋佳慧, 任永杰, 杨守瑞, 等. 基于合作靶球的视觉传感器外参标定方法[J]. 光学学报, 2017, 37 (9) : 0915003.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" Li R Z, Liu Y Y, Yang M, &lt;i&gt;et al&lt;/i&gt;. Three-dimensional point cloud segmentation algorithm based on improved region growing[J]. Laser &amp;amp; Optoelectronics Progress, 2018, 55 (5) : 051502. 李仁忠, 刘阳阳, 杨曼, 等. 基于改进的区域生长三维点云分割[J]. 激光与光电子学进展, 2018, 55 (5) : 051502." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201805041&amp;v=MzEyNTdMdktMeXJQWkxHNEg5bk1xbzlCWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEa1Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Li R Z, Liu Y Y, Yang M, &lt;i&gt;et al&lt;/i&gt;. Three-dimensional point cloud segmentation algorithm based on improved region growing[J]. Laser &amp;amp; Optoelectronics Progress, 2018, 55 (5) : 051502. 李仁忠, 刘阳阳, 杨曼, 等. 基于改进的区域生长三维点云分割[J]. 激光与光电子学进展, 2018, 55 (5) : 051502.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" Rusu R B, Cousins S. 3D is here: point cloud library (PCL) [C]. IEEE International Conference on Robotics and Automation, 2011: 1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D is here:Point Cloud Library (PCL)">
                                        <b>[16]</b>
                                         Rusu R B, Cousins S. 3D is here: point cloud library (PCL) [C]. IEEE International Conference on Robotics and Automation, 2011: 1-4.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_17" title=" Choi S, Kim T, Yu W. Performance evaluation of RANSAC family[C]. Proceedings of the British Machine Vision Conference, 2009: 355." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Performance Evaluation of RANSAC Family">
                                        <b>[17]</b>
                                         Choi S, Kim T, Yu W. Performance evaluation of RANSAC family[C]. Proceedings of the British Machine Vision Conference, 2009: 355.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_18" title=" Jin J J, Lu W L, Guo X T, &lt;i&gt;et al&lt;/i&gt;. Position registration method of simultaneous phase-shifting interferograms based on SURF and RANSAC algorithms[J]. Acta Optica Sinica, 2017, 37 (10) : 1012002. 靳京京, 卢文龙, 郭小庭, 等. 基于SURF和RANSAC算法的同步相移干涉图位置配准方法[J]. 光学学报, 2017, 37 (10) : 1012002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201710023&amp;v=MjQxMDdmWnVadEZpRGtWTHZLSWpYVGJMRzRIOWJOcjQ5SFo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Jin J J, Lu W L, Guo X T, &lt;i&gt;et al&lt;/i&gt;. Position registration method of simultaneous phase-shifting interferograms based on SURF and RANSAC algorithms[J]. Acta Optica Sinica, 2017, 37 (10) : 1012002. 靳京京, 卢文龙, 郭小庭, 等. 基于SURF和RANSAC算法的同步相移干涉图位置配准方法[J]. 光学学报, 2017, 37 (10) : 1012002.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_19" title=" Zeineldin R A, El-Fishawy N A. Fast and accurate ground plane detection for the visually impaired from 3D organized point clouds[C]. 2016 SAI Computing Conference, 2016: 373-379." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast and accurate ground plane detection for the visually impaired from 3D organized point clouds">
                                        <b>[19]</b>
                                         Zeineldin R A, El-Fishawy N A. Fast and accurate ground plane detection for the visually impaired from 3D organized point clouds[C]. 2016 SAI Computing Conference, 2016: 373-379.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-10-20 11:56</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(02),199-207 DOI:10.3788/AOS201939.0212007            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于多传感器的三维目标位姿测量方法</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%BD%98%E6%97%BA&amp;code=35792932&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">潘旺</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B1%E6%9E%AB&amp;code=09588420&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朱枫</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%9D%E9%A2%96%E6%98%8E&amp;code=09631007&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郝颖明</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%B8%BD%E6%95%8F&amp;code=34031828&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张丽敏</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E6%B2%88%E9%98%B3%E8%87%AA%E5%8A%A8%E5%8C%96%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0183762&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院沈阳自动化研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%85%89%E7%94%B5%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院光电信息处理重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%BE%BD%E5%AE%81%E7%9C%81%E5%9B%BE%E5%83%8F%E7%90%86%E8%A7%A3%E4%B8%8E%E8%A7%86%E8%A7%89%E8%AE%A1%E7%AE%97%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">辽宁省图像理解与视觉计算重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出了一种基于多传感器的三维目标位姿测量方法, 该方法利用多传感器技术, 充分发挥深度相机和高分辨率电荷耦合器件 (CCD) 相机各自的优势, 提高了测量的稳健性和效率。利用物体与其固定平面之间的关系, 在点云中粗略定位出目标区域, 通过预先标定信息将目标区域转换至灰度图像空间。在灰度图像中, 利用线段检测器 (LSD) 算法外加特征约束, 筛选出4条目标直线, 利用透视4点 (P4P) 算法求解出目标的六维位姿。实验验证了该算法的有效性, 其测量效率远优于经典模板匹配方法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BD%8D%E5%A7%BF%E6%B5%8B%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">位姿测量;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多传感器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E7%BB%B4%E7%9B%AE%E6%A0%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三维目标;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B4%E7%BA%BF%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">直线提取;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *朱枫, E-mail:fzhu@sia.cn;;
                                </span>
                                <span>
                                    *潘旺, E-mail:panwang@sia.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-16</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (U1713216);</span>
                    </p>
            </div>
                    <h1>Pose Measurement Method of Three-Dimensional Object Based on Multi-Sensor</h1>
                    <h2>
                    <span>Pan Wang</span>
                    <span>Zhu Feng</span>
                    <span>Hao Yingming</span>
                    <span>Zhang Limin</span>
            </h2>
                    <h2>
                    <span>Shenyang Institute of Automation, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
                    <span>Key Laboratory of Opto-Electronic Information Processing, Chinese Academy of Sciences</span>
                    <span>Key Laboratory of Image Understanding and Computer Vision of Liaoning Province</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A method based on multi-sensor to measure the pose of a three-dimensional object is proposed. We employ multi-sensor technology to make the advantages of the depth camera and the high-resolution charge-couple device (CCD) camera, improving the robustness and efficiency of measurement. The target area is coarsely located in the point cloud based on the relationship between the object and its fixed board, and the region is converted to gray image space by the prior calibration information. In the gray image, four target straight lines are extracted by the Line Segment Detector (LSD) and feature constraints. The perspective 4 point (P4 P) algorithm is utilized to calculate the six-dimensional pose of the object. The experiment verifies the validity of the algorithm and the measurement efficiency of the algorithm is much better than that of the classical template matching method.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=pose%20measurement&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">pose measurement;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-sensor&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-sensor;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=three-dimensional%20object&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">three-dimensional object;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=line%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">line extraction;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-08-16</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="50">视觉感知系统通过视觉传感器确定目标物体在三维空间中的位置和姿态, 对机器人的抓取操作具有重要意义。目前广泛使用的基于描述子的方法<citation id="134" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>通过在物体的特征点周围构造具有区分性的特征描述子来训练分类器。在搜索阶段, 将搜索图像与模型进行匹配来确定目标物体。这种方法最大的优点是其执行时间与几何搜索空间的大小无关, 在一些应用场景下表现出色, 但其仅局限于纹理丰富、清晰的物体。这是因为只有当物体的纹理比较清晰时, 才能检测到有意义的特征描述子。在实际工程应用中, 需要测量三维模型已知的无纹理物体的六维位姿, 该类物体表面光滑有光泽, 几乎没有纹理, 在图像获取点未知的情况下, 对三维物体所在的广阔空间进行搜索非常耗时, 因此对其精确测量的难度较大。Ulrich等<citation id="135" type="reference"><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>提出的基于二维视图形状模板匹配的方法是一种较好的解决方案。该方法的视图模板采样间隔小, 匹配精度高, 采用了由粗到精的加速策略。但其在大场景中的稳健性较差, 特别是在复杂混乱背景下极易出现误匹配, 并且在建模的姿态范围中不能包含物体的退化视图, 否则易导致误匹配。Hinterstoisser等<citation id="136" type="reference"><link href="21" rel="bibliography" /><link href="23" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>采用一种改进的模板匹配方法来检测并定位无纹理物体, 该方法综合利用了彩色图像中的梯度信息和深度图像中的表面法向量信息。然而, 模板的稀疏采样限制了该方法初始估计位置的准确性, 不能输出物体精确的六维姿态, 且同样存在视图退化情况下误匹配率较高的问题。当待测量物体的几何特征比较明显时, 拟采用传统的透视<i>n</i>点 (P<i>n</i>P) 问题求解算法<citation id="137" type="reference"><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>计算物体的精确位姿。当已知物体上4个特征点的三维空间坐标及其对应的成像坐标时, 采用该方法可以解算出物体的位姿。在图像中搜索并对应目标特征点是问题的关键, 然而图像搜索空间较大, 且金属表面的反射特性使得物体的成像受光照的影响明显, 因此在图像中找到目标特征点的难度较大。</p>
                </div>
                <div class="p1">
                    <p id="51">针对这些问题, 本文提出了一种基于多传感器的三维目标位姿测量方法, 该方法采用由粗到精的定位策略, 综合利用了深度相机和电荷耦合器件 (CCD) 相机各自的优势, 使得两种传感技术形成优势互补。因为深度相机的分辨率较低, 采集到的点云精度有限, 且噪声较大, 所以无法实现三维目标的高精度位姿测量, 且视觉特征没有二维图像直观。CCD相机采集的二维图像数据的分辨率和精度均优于深度相机, 但用于三维目标位姿测量时存在搜索空间巨大、搜索效率低等问题。三维点云用于空间平面拟合具有传统二维相机无法比拟的优势, 且对光照变化具有较好的稳健性。因此:首先从点云中拟合出物体所在平面, 进而分割出目标区域, 完成目标物体的粗略定位;然后将该区域通过预先标定信息转换到灰度图像空间, 划定感兴趣区域, 从而减小二维图像中的搜索空间, 弥补CCD相机用于三维目标位姿测量时的劣势。利用线段检测器 (LSD) <citation id="138" type="reference"><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>直线提取算法和特征约束提取出4条目标直线, 求出它们之间的交点。亚像素级的图像特征点提取弥补了深度相机分辨率低、精度差的劣势。最终利用透视4点 (P4P) 求解算法得到了物体的高精度位姿。两种传感技术的综合利用避免了大面积复杂背景的干扰, 解决了三维目标成像退化时的误匹配问题, 同时缩小了搜索空间, 大幅提高了效率, 实现了三维目标位姿的快速高精度测量。</p>
                </div>
                <h3 id="52" name="52" class="anchor-tag">2 视觉系统构建</h3>
                <div class="p1">
                    <p id="53">为仿人机器人设计的视觉测量系统主要由基于飞行时间测距的深度相机SwissRanger SR4000 (Mesa Imaging公司, 瑞士) 和紧凑型CCD工业相机Prosilica GC2450C (Allied Vision Technologies公司, 德国) 组成。系统的硬件组成及深度相机坐标系<i>O</i><sub>d</sub><i>X</i><sub>d</sub><i>Y</i><sub>d</sub><i>Z</i><sub>d</sub>、CCD相机坐标系<i>O</i><sub>c</sub><i>X</i><sub>c</sub><i>Y</i><sub>c</sub><i>Z</i><sub>c</sub>和世界坐标系<i>O</i><sub>w</sub><i>X</i><sub>w</sub><i>Y</i><sub>w</sub>Z<sub>w</sub>的定义如图1所示, 其中红色、绿色和蓝色箭头分别代表各坐标系的<i>x</i>轴、<i>y</i>轴和<i>z</i>轴。定义的坐标系均满足右手法则, 深度相机坐标系<i>O</i><sub>d</sub>位于光轴与相机正面的交点处, 假设观察者站在相机后面, 观察方向与相机视角一致, 则从相机出发沿光轴方向为<i>z</i>轴正方向, <i>y</i>轴正方向垂直向下, <i>x</i>轴正方向水平向右。CCD相机坐标系<i>O</i><sub>c</sub>位于相机的光心, <i>x</i>轴和<i>y</i>轴与图像的<i>x</i>轴、<i>y</i>轴平行, <i>z</i>轴为相机光轴, 它与图像平面垂直。世界坐标系<i>O</i><sub>w</sub>定义在与视觉系统固连的金属板上, 板上加工有3个通孔用于放置激光跟踪仪的反射靶球, 以获取目标物体在世界坐标系下的高精度真实位姿, 作为视觉测量参考的标准值。为确保多传感器能同时采集数据, 深度相机被用作同步触发源。本研究仅使用两个CCD相机中的一个, 另一个作为备用, 当然也可以作为冗余测量来增加系统的可靠性。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902024_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 视觉系统的坐标系定义和标定" src="Detail/GetImg?filename=images/GXXB201902024_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 视觉系统的坐标系定义和标定  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902024_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Coordinate system definition and calibration of the visual system</p>

                </div>
                <h4 class="anchor-tag" id="56" name="56"><b>2.1</b><b>多传感器标定</b></h4>
                <div class="p1">
                    <p id="57">为了准确测量物体的六维位姿, 需要先对视觉系统进行标定。深度相机在出厂前已由制造商完成标定, 这里仅标定其外参。如图1所示, 标定板1用于标定CCD相机的内外参, 标定板2用于标定深度相机的外参。用张正友标定法<citation id="139" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>完成CCD相机内参矩阵的标定, 透镜模型为</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ζ</mi><msub><mrow></mrow><mtext>c</mtext></msub><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>u</mi></mtd></mtr><mtr><mtd><mi>v</mi></mtd></mtr><mtr><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>f</mi><msub><mrow></mrow><mi>x</mi></msub></mtd><mtd><mn>0</mn></mtd><mtd><mi>u</mi><msub><mrow></mrow><mn>0</mn></msub></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mi>f</mi><msub><mrow></mrow><mi>y</mi></msub></mtd><mtd><mi>v</mi><msub><mrow></mrow><mn>0</mn></msub></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">R</mi></mtd><mtd><mi mathvariant="bold-italic">t</mi></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>X</mi><msub><mrow></mrow><mtext>w</mtext></msub></mtd></mtr><mtr><mtd><mi>Y</mi><msub><mrow></mrow><mtext>w</mtext></msub></mtd></mtr><mtr><mtd><mi>Ζ</mi><msub><mrow></mrow><mtext>w</mtext></msub></mtd></mtr><mtr><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">式中:<i>Z</i><sub>c</sub>为尺度因子;<i>u</i>、<i>v</i>为图像坐标;<i>X</i><sub>w</sub>、<i>Y</i><sub>w</sub>、<i>Z</i><sub>w</sub>为世界坐标系下的坐标;<i>f</i><sub><i>x</i></sub>、<i>f</i><sub><i>y</i></sub>、<i>u</i><sub>0</sub>、<i>v</i><sub>0</sub>和畸变系数构成相机内参;<b><i>R</i></b>、<b><i>t</i></b>分别为3×3的旋转矩阵和3×1的平移向量, 构成相机外参。外参表示从世界坐标系到相机坐标系的刚体变换<citation id="140" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。激光跟踪仪可以精确获取反射靶球的三维坐标, 借助瑞士Leica Geosystems公司生产的AT901激光跟踪仪标定各相机的外参。多传感器视觉系统的外参标定如图1所示。首先将3个反射靶球放置在金属板的3个靶座位置上, 用激光跟踪仪得到这3个点的坐标<i>A</i> (<i>x</i><sub>0</sub>, <i>y</i><sub>0</sub>, <i>z</i><sub>0</sub>) 、<i>B</i> (<i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>, <i>z</i><sub>1</sub>) 、<i>C</i> (<i>x</i><sub>2</sub>, <i>y</i><sub>2</sub>, <i>z</i><sub>2</sub>) 。将<b><i>BC</i></b>= (<i>x</i><sub>2</sub>-<i>x</i><sub>1</sub>, <i>y</i><sub>2</sub>-<i>y</i><sub>1</sub>, <i>z</i><sub>2</sub>-<i>z</i><sub>1</sub>) 作为世界坐标系<i>O</i><sub>w</sub><i>X</i><sub>w</sub><i>Y</i><sub>w</sub><i>Z</i><sub>w</sub>的<i>y</i>轴, 由<i>A</i>点向直线<i>BC</i>作垂线得到垂足<i>D</i> (<i>x</i><sub>3</sub>, <i>y</i><sub>3</sub>, <i>z</i><sub>3</sub>) , <b><i>AD</i></b>= (<i>x</i><sub>3</sub>-<i>x</i><sub>0</sub>, <i>y</i><sub>3</sub>-<i>y</i><sub>0</sub>, <i>z</i><sub>3</sub>-<i>z</i><sub>0</sub>) 作为世界坐标系<i>O</i><sub>w</sub><i>X</i><sub>w</sub><i>Y</i><sub>w</sub><i>Z</i><sub>w</sub>的<i>x</i>轴, <i>x</i>轴叉乘<i>y</i>轴得到<i>z</i>轴, 至此世界坐标系建立完毕。然后将3个靶球放到标定板1的<i>P</i><sub>1</sub>、<i>P</i><sub>2</sub>、<i>P</i><sub>3</sub>三个靶座位置, 激光跟踪仪可以直接测得这3个点在世界坐标系<i>O</i><sub>w</sub>下的坐标。已知这3个点在标定板1坐标系 (以<i>O</i><sub>p</sub>为原点) 下的坐标, 可以计算该坐标系与<i>O</i><sub>w</sub><i>X</i><sub>w</sub><i>Y</i><sub>w</sub><i>Z</i><sub>w</sub>之间的变换关系[<b><i>R</i></b><sub>2</sub>|<b><i>t</i></b><sub>2</sub>]。同时, 标定板1坐标系与CCD相机坐标系<i>O</i><sub>c</sub><i>X</i><sub>c</sub><i>Y</i><sub>c</sub><i>Z</i><sub>c</sub>之间的变换关系[<b><i>R</i></b><sub>1</sub>|<b><i>t</i></b><sub>1</sub>]可以通过标定板上的三维点坐标和二维成像点坐标之间的对应关系求得。于是得到了从<i>O</i><sub>w</sub><i>X</i><sub>w</sub><i>Y</i><sub>w</sub><i>Z</i><sub>w</sub>到<i>O</i><sub>c</sub><i>X</i><sub>c</sub><i>Y</i><sub>c</sub><i>Z</i><sub>c</sub>的变换矩阵, 即CCD相机的外参矩阵。使用标定板2, 按照上述方法完成深度相机的外参标定。内外参的标定结果在实验部分给出。完成多传感器的标定后, 即可实现空间一点在不同坐标系下的坐标对应。设空间一点在坐标系<i>O</i><sub>c</sub><i>X</i><sub>c</sub><i>Y</i><sub>c</sub><i>Z</i><sub>c</sub>、<i>O</i><sub>d</sub><i>X</i><sub>d</sub><i>Y</i><sub>d</sub><i>Z</i><sub>d</sub>、<i>O</i><sub>w</sub><i>X</i><sub>w</sub><i>Y</i><sub>w</sub><i>Z</i><sub>w</sub>下的坐标向量分别为<b><i>P</i></b><sub>c</sub>、<b><i>P</i></b><sub>d</sub>、<b><i>P</i></b><sub>w</sub>, 则有</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo>=</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo>⋅</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mtext>w</mtext></msub><mo>+</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>=</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>⋅</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mtext>w</mtext></msub><mo>+</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61">式中:[<b><i>R</i></b><sub>c</sub>|<b><i>t</i></b><sub>c</sub>]为CCD相机的外参;[<b><i>R</i></b><sub>d</sub>|<b><i>t</i></b><sub>d</sub>]为深度相机的外参。将 (3) 式两边同时左乘<b><i>R</i></b><mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mtext>d</mtext><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>, 可得</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">R</mi><msubsup><mrow></mrow><mtext>d</mtext><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>⋅</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>=</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mtext>w</mtext></msub><mo>+</mo><mi mathvariant="bold-italic">R</mi><msubsup><mrow></mrow><mtext>d</mtext><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>⋅</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">将<b><i>P</i></b><sub>w</sub>代入 (2) 式中, 可得到</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo>=</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo>⋅</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">R</mi><msubsup><mrow></mrow><mtext>d</mtext><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>⋅</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">R</mi><msubsup><mrow></mrow><mtext>d</mtext><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>⋅</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">将空间一点在深度相机坐标系下的坐标<b><i>P</i></b><sub>d</sub>代入 (5) 式即可计算出其在CCD相机坐标系下的坐标<b><i>P</i></b><sub>c</sub>, 再利用CCD相机内参可得到对应的成像坐标<i>u</i>、<i>v</i>。所提方法利用这一对应关系在灰度图像中确定目标特征点的搜索范围。</p>
                </div>
                <h3 id="67" name="67" class="anchor-tag">3 定位策略</h3>
                <div class="p1">
                    <p id="68">当机器人与目标有一定距离时, 视觉传感器感知的场景较大, 会采集到较多复杂混乱的背景信息, 这些信息不仅会干扰对目标特征的搜索定位, 还会大幅降低搜索效率。如果仅利用灰度图像对无纹理物体进行位姿测量, 在整个图像空间中进行穷尽搜索, 则需耗费大量无用的计算成本。尤其是在杂乱背景下, 搜索效率极低。当背景中存在与物体相似的特征时, 它们易被误判为目标, 导致算法不稳定, 稳健性差。仅利用点云信息时, 点云匹配同样面临着待匹配点数量大、遍历耗时长的问题, 且基于飞行时间原理采集的点云数据精度通常较低。所使用的SR4000深度相机名义上在99%目标反射率和0.8～8.0 m的标定范围内, 对于相机中心的11 pixel×11 pixel, 其绝对精度为15 mm, 远不及基于灰度图像的测量精度。为了解决复杂背景和巨大搜索空间带来的问题, 并满足一定的测量精度和效率, 提出了一种基于多传感器的位姿测量方法。该方法主要分为两个阶段, 首先利用深度相机采集的点云实现感兴趣区域的分割, 再用CCD相机实现高精度的位姿测量, 整个算法流程如图2所示。借助点云的三维信息优势, 用随机采样一致性 (RANSAC) 算法估计物体所在平面, 分割出固定物体的面板区域, 从场景中移除混乱背景, 进而用欧式聚类算法粗略分割出目标区域。然后将分割结果变换到灰度图像中, 利用LSD直线检测算法外加特征约束, 提取出4条目标直线, 进而求出它们的交点。最后根据这4个共面特征点, 利用P4P算法计算目标位姿, 实现高精度的位姿测量。待测目标物体的三维计算机辅助设计 (CAD) 模型及其坐标系定义如图3 (a) 所示, 其中, <i>O</i><sub>o</sub>为物体坐标系<i>O</i><sub>o</sub><i>x</i><sub>o</sub><i>y</i><sub>o</sub><i>z</i><sub>o</sub>的原点, <i>a</i>、<i>b</i>、<i>c</i>、<i>d</i>为P4P算法用到的4个目标特征点, 它们在目标物体坐标系下的坐标分别为 (-19, -175, -38.5) , (-19, 175, -38.5) , (19, 175, -38.5) , (19, -175, -38.5) 。位姿测量结果如图3 (b) 所示, 其中绿色线条是根据位姿测量结果将物体模型反投影到图像中的效果。</p>
                </div>
                <div class="area_img" id="69">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902024_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 所提方法的流程图" src="Detail/GetImg?filename=images/GXXB201902024_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 所提方法的流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902024_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Flow chart of the proposed method</p>

                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902024_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 目标模型和测量结果。 (a) 物体的三维模型和坐标系定义; (b) 位姿测量结果" src="Detail/GetImg?filename=images/GXXB201902024_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 目标模型和测量结果。 (a) 物体的三维模型和坐标系定义; (b) 位姿测量结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902024_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Object model and measurement results. (a) Three-dimensional model and definition of coordinate system of the object; (b) pose measurement result</p>

                </div>
                <h4 class="anchor-tag" id="71" name="71"><b>3.1</b><b>基于点云的目标分割</b></h4>
                <div class="p1">
                    <p id="72">通过深度相机获取点云数据, 估计出物体所在平面, 进而实现物体的粗定位。将这些点坐标转换到灰度图像坐标系, 一方面可以避免杂乱背景的干扰, 减小搜索空间, 提高效率, 另一方面有利于在图像中筛选得到目标直线。此外, 根据分割出的点云的空间坐标值, 可与最终位姿求解结果形成冗余信息, 进行互验证, 提高测量结果的可靠性。分割是点云数据处理中的关键操作<citation id="141" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 点云库 (PCL) <citation id="142" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提供了点云处理的基础操作框架。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73">3.1.1 点云预处理</h4>
                <div class="p1">
                    <p id="74">从SR4000相机输出的<i>x</i>、<i>y</i>、<i>z</i>笛卡儿坐标分别采用3 pixel×3 pixel中值滤波预处理, 以消除噪声, 然后将它们构成原始点云, 大小为176×144, 共计25344个点。考虑到过多的数据点会增大后续算法的计算开销, 且感兴趣点是与机器人的距离在一定范围内的点, 因此, 首先沿<i>z</i>轴进行一次简单的直通滤波, 消除距离过近和过远的点:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>≤</mo><mi>z</mi><mo>≤</mo><mi>z</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">设定<i>z</i>轴的距离范围为0.3～1.5 m, 这取决于机器人操作范围和实际应用需求。用直通滤波器遍历一遍点云有两个作用:1) 删除虚假的无效点;2) 移除位于设定区间之外的点, 为后续操作减小运算量。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">3.1.2 估计平面</h4>
                <div class="p1">
                    <p id="78">首先从点云中得到物体所固定的平面, 平面方程定义为<i>Ax</i>+<i>By</i>+<i>Cz</i>+<i>D</i>=0, 其法向量<b><i>N</i></b>= (<i>A</i>, <i>B</i>, <i>C</i>) 。这一步的目标是找到最适合的参数 (<i>A</i>, <i>B</i>, <i>C</i>, <i>D</i>) 来确定目标物体所在的平面。很多点不在平面上, 属于离群干扰点, 经典的拟合技术 (如最小二乘法) 在这种情况下处理的效果不佳。RANSAC算法是目前应用广泛的算法之一<citation id="143" type="reference"><link href="43" rel="bibliography" /><link href="45" rel="bibliography" /><link href="47" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>, 其优势在于能够对模型参数进行稳健估计, 即使是在数据集中存在大量异常值的情况下, 也可以高准确度地估计参数。理论上三个点即可确定一个平面方程, RANSAC算法的思想是随机选择三个点来确定一个平面, 再利用距离阈值判断剩余点中有多少个点属于该平面, 迭代此过程, 得到包含最多局内点的平面, 再对这些局内点用最小二乘法优化求精得到平面方程。</p>
                </div>
                <div class="p1">
                    <p id="79">由于迭代过程非常耗时, 在此之前使用体素网格方法对点云进行降采样, 同时保持其形状特征和细节。位于每个体素内的点都由它们的质心表示。网格的大小对RANSAC算法效率的影响如图4所示。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902024_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 网格大小对采样剩余点数和RANSAC算法效率的影响" src="Detail/GetImg?filename=images/GXXB201902024_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 网格大小对采样剩余点数和RANSAC算法效率的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902024_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Influence of grid size on the number of sampling points and efficiency of RANSAC</p>

                </div>
                <div class="p1">
                    <p id="81">可以看出, 降采样可移除大量数据点, 得到具有较少点的等效点云, 显著提高RANSAC算法的效率。在估计平面时除了点到平面距离的限制, 还有法向量夹角的限制, 即局内点的法向量与估计平面的法向量应近似平行。每个点周围选取50个邻域点, 计算表面法向量。采用RANSAC算法估计平面, 在迭代遍历过程中判断点属于平面内点的依据是点到平面的欧式距离和法向量夹角的加权和小于特定阈值。假设点<i>i</i>的坐标为 (<i>x</i><sub>0</sub>, <i>y</i><sub>0</sub>, <i>z</i><sub>0</sub>) , 则点到平面的欧式距离为</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>u</mtext><mtext>c</mtext><mtext>l</mtext><mtext>i</mtext><mtext>d</mtext></mrow></msub><mo>=</mo><mrow><mo>|</mo><mrow><mi>A</mi><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>B</mi><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>C</mi><mi>z</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>D</mi></mrow><mo>|</mo></mrow><mo>/</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ν</mi><mo stretchy="false">∥</mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">点的法向量<b><i>n</i></b><sub><i>i</i></sub>与平面法向量<b><i>N</i></b>之间的夹角为</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>r</mtext><mtext>m</mtext><mtext>a</mtext><mtext>l</mtext></mrow></msub><mo>=</mo><mi>arccos</mi><mfrac><mrow><mi mathvariant="bold-italic">n</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">Ν</mi></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">n</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><mo>⋅</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ν</mi><mo stretchy="false">∥</mo></mrow></mfrac><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">为了综合考虑这两项约束, 实现欧式距离和法向量夹角的协同优化, 点与平面模型拟合程度的评价指标为</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo>=</mo><mi>w</mi><mo>⋅</mo><mi>D</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>r</mtext><mtext>m</mtext><mtext>a</mtext><mtext>l</mtext></mrow></msub><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>w</mi><mo stretchy="false">) </mo><mo>⋅</mo><mi>D</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>u</mtext><mtext>c</mtext><mtext>l</mtext><mtext>i</mtext><mtext>d</mtext></mrow></msub><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">式中:<i>w</i>为法向量夹角的加权系数, 取值范围为0～1。设<i>w</i>为0.1, 误差阈值<i>E</i>为6 cm, 这意味着当误差不大于6 cm时, 该点被认为是平面模型的局内点。阈值的大小与面板的表面反射率密切相关。所用的深度相机是基于飞行时间的, 因此测距噪声主要取决于信号幅度和背景照度。信号幅度取决于物距和物体反射率, 其中, 被测物体的反射率对测量噪声和重复性的影响很大。在实验中, 黑色金属表面吸收大部分入射光, 即反射率低, 测距噪声较大, 测量得到的距离在较大范围内振荡, 因此选择了较大的阈值 (6 cm) 。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88">3.1.3 分割物体</h4>
                <div class="p1">
                    <p id="89">完成平面估计后, 部分属于估计平面的局内点在目标面板外, 这些点不属于面板上的点, 会影响感兴趣区域的确定。因此, 采用统计离群值滤波算法统计点的邻域来消除这些不需要的点, 获得实际的面板区域。得到面板内点后, 下一步的目标是只保留固定在面板上的物体点来粗定位物体。先利用估计的平面方程将面板内的点投影到平面上并计算其最小外接矩形。该矩形划定了可固定物体的面板区域, 在此矩形基础上, 沿平面法向量方向创建一个三维包围盒, 通过立方体裁剪滤波器, 即三个空间维度上的直通滤波器, 仅保留三维包围盒内的点, 即</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>x</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>≤</mo><mi>x</mi><mo>≤</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>y</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>≤</mo><mi>y</mi><mo>≤</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>z</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>≤</mo><mi>z</mi><mo>≤</mo><mi>z</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">式 (10) 中边界坐标值<i>x</i><sub>min</sub>、<i>x</i><sub>max</sub>、<i>y</i><sub>min</sub>、<i>y</i><sub>max</sub>用于限制面板的区域。为了保证目标物体点坐标的精度, 裁剪滤波器的输入点云是原始点云, 而不是降采样后的点云。由于每个物体对应一片紧凑的点云, 因而采用欧式聚类的方法将点云聚类成多个簇, 每个簇代表一个物体, 同时, 将点数少于一定阈值的簇剔除。通过对点云的一系列操作, 最终获得了代表目标物体的点云, 结果如图5所示。其中, 红绿蓝三轴表示深度相机坐标系, 红色点云表示分割出的目标, 绿色点表示输入点云。图5还显示了估计出的平面以及裁剪立方体。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902024_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 目标点云分割结果" src="Detail/GetImg?filename=images/GXXB201902024_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 目标点云分割结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902024_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Result of target point cloud segmentation</p>

                </div>
                <h4 class="anchor-tag" id="93" name="93"><b>3.2</b><b>基于灰度图的精确测量</b></h4>
                <div class="p1">
                    <p id="94">CCD相机和深度相机的外参已标定至同一世界坐标系下, 故点云分割出的点可通过变换矩阵转换到灰度图像坐标, 在灰度图中限定物体的区域, 如图6所示。可以看出, 红色的点比较稀疏, 可以清晰地分辨出点与点之间的间隔, 在局部存在缺失和孔洞。因此, 仅使用深度相机进行位姿测量, 达到的精度有限。但这样的粗定位结果有助于后续在图像中筛选出目标直线, 得到交点, 进而精确求解出目标位姿。与基于模板匹配的方法相比, 所提的基于直线交点的测量方法速度更快, 测量精度更高, 且不需要离线训练过程, 无需模板的存储空间。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902024_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 分割出的目标点云对应的灰度图像中的点" src="Detail/GetImg?filename=images/GXXB201902024_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 分割出的目标点云对应的灰度图像中的点  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902024_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Points in the gray image corresponding to the segmented target point cloud</p>

                </div>
                <h4 class="anchor-tag" id="96" name="96">3.2.1 提取最大轮廓区域</h4>
                <div class="p1">
                    <p id="97">为减小后续计算量, 先计算最小外接矩形, 裁剪出这部分灰度图形成小图像。小图像左上角坐标 (0, 0) 在原图像中记为 (<i>c</i><sub><i>x</i></sub>, <i>c</i><sub><i>y</i></sub>) , 在新的小图像中检测4条目标直线并求其交点坐标, 加上偏移量 (<i>c</i><sub><i>x</i></sub>, <i>c</i><sub><i>y</i></sub>) 即为4个特征点在原图像中的坐标。</p>
                </div>
                <div class="p1">
                    <p id="98">为准确提取物体前表面的4条直线特征, 首先对小图像进行如下操作。采用Otsu法自适应地选取阈值对图像进行二值化。以二值图像形态学中的腐蚀和膨胀操作为基础来提取包含目标边缘的局部区域。先进行腐蚀操作, 侵蚀二值图像的前景像素区域, 提取选择最大轮廓区域, 再进行膨胀操作, 扩大边界上的前景像素区域。通过二值图像形态学处理可得到物体前表面的局部区域。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99">3.2.2 提取目标特征点</h4>
                <div class="p1">
                    <p id="100">在该局部区域内采用LSD直线检测算法检测直线段, 并通过构成直线的像素点的梯度方向来确定该直线的平均梯度方向。由于噪声和光照的影响, LSD算法最主要的缺陷是图像中同一直线上的直线段可能出现断裂, 即一条直线段被LSD算法提取成多条直线段。因此, 将检测出的直线用极坐标形式表示, 并进行直线分组。在极坐标系下将直线<i>L</i>表示为参数为 (<i>ρ</i>, <i>θ</i>) 的方程, 其中<i>ρ</i>为直线到原点的距离, <i>θ</i>为夹角, 如图7所示。具有相似极坐标的直线被划分为一组, 将所有直线划分为4组。目标直线的梯度方向由边缘内部指向边缘外侧, 分别在4组中选择满足梯度方向且最长的直线段作为目标直线。通过计算目标直线相应的交点得到4个目标特征点。已知4个特征点在图像中的坐标和它们在物体坐标系下的坐标, 采用P4P位姿求解算法计算出物体在CCD相机坐标系下的位姿, 最后计算出物体在世界坐标系下的位姿。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902024_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 直线在极坐标系下的表示" src="Detail/GetImg?filename=images/GXXB201902024_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 直线在极坐标系下的表示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902024_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Representation of a line in a polar coordinate system</p>

                </div>
                <h3 id="102" name="102" class="anchor-tag">4 实验与结果</h3>
                <h4 class="anchor-tag" id="103" name="103"><b>4.1</b><b>实验环境</b></h4>
                <div class="p1">
                    <p id="104">多传感器视觉测量系统实验环境如图8 (a) 所示, 主要包括固定在机器人操作面板上的待测物体、两台CCD相机、一台深度相机和计算机等。其中:CCD相机的分辨率为2448 pixel×2050 pixel, 镜头焦距为8 mm。标定得到的左相机内外参矩阵分别为</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>2</mn><mn>3</mn><mn>3</mn><mn>1</mn><mo>.</mo><mn>1</mn><mn>0</mn><mn>1</mn><mn>7</mn><mn>1</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn><mn>2</mn><mn>0</mn><mn>3</mn><mo>.</mo><mn>2</mn><mn>7</mn><mn>5</mn><mn>3</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>2</mn><mn>3</mn><mn>3</mn><mn>0</mn><mo>.</mo><mn>8</mn><mn>2</mn><mn>1</mn><mn>5</mn><mn>2</mn></mtd><mtd><mn>1</mn><mn>0</mn><mn>4</mn><mn>9</mn><mo>.</mo><mn>4</mn><mn>7</mn><mn>2</mn><mn>5</mn><mn>7</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">和</p>
                </div>
                <div class="p1">
                    <p id="107" class="code-formula">
                        <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>9</mn><mn>6</mn><mn>9</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>9</mn><mn>9</mn><mn>9</mn><mn>9</mn><mn>3</mn><mn>5</mn></mtd><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>5</mn><mn>9</mn><mn>8</mn></mtd><mtd><mn>4</mn><mn>4</mn><mo>.</mo><mn>3</mn><mn>8</mn><mn>9</mn><mn>3</mn><mn>7</mn></mtd></mtr><mtr><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>9</mn><mn>9</mn><mn>9</mn><mn>9</mn><mn>5</mn></mtd><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>9</mn><mn>7</mn><mn>1</mn></mtd><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>1</mn><mn>9</mn><mn>3</mn></mtd><mtd><mn>5</mn><mn>6</mn><mo>.</mo><mn>5</mn><mn>1</mn><mn>8</mn><mn>7</mn><mn>1</mn></mtd></mtr><mtr><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>1</mn><mn>9</mn><mn>9</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>5</mn><mn>9</mn><mn>6</mn><mn>3</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>9</mn><mn>9</mn><mn>9</mn><mn>9</mn><mn>8</mn></mtd><mtd><mn>1</mn><mn>7</mn><mo>.</mo><mn>9</mn><mn>0</mn><mn>7</mn><mn>4</mn><mn>9</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="108">, 畸变系数 (<i>k</i><sub>1</sub>, <i>k</i><sub>2</sub>, <i>p</i><sub>1</sub>, <i>p</i><sub>2</sub>) = (-0.0840758, 0.0807441, 0.0002128, -0.0000821) 。右相机内外参矩阵分别为</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>2</mn><mn>3</mn><mn>1</mn><mn>4</mn><mo>.</mo><mn>6</mn><mn>3</mn><mn>2</mn><mn>6</mn><mn>5</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn><mn>1</mn><mn>9</mn><mn>5</mn><mo>.</mo><mn>9</mn><mn>1</mn><mn>0</mn><mn>3</mn><mn>5</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>2</mn><mn>3</mn><mn>1</mn><mn>4</mn><mo>.</mo><mn>2</mn><mn>3</mn><mn>5</mn><mn>7</mn><mn>1</mn></mtd><mtd><mn>9</mn><mn>9</mn><mn>1</mn><mo>.</mo><mn>0</mn><mn>1</mn><mn>8</mn><mn>1</mn><mn>6</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">和</p>
                </div>
                <div class="p1">
                    <p id="111" class="code-formula">
                        <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>6</mn><mn>9</mn><mn>9</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>9</mn><mn>9</mn><mn>9</mn><mn>9</mn><mn>6</mn><mn>8</mn></mtd><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>3</mn><mn>8</mn><mn>3</mn></mtd><mtd><mo>-</mo><mn>4</mn><mn>5</mn><mo>.</mo><mn>2</mn><mn>1</mn><mn>7</mn><mn>8</mn></mtd></mtr><mtr><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>9</mn><mn>9</mn><mn>9</mn><mn>9</mn><mn>8</mn></mtd><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>6</mn><mn>9</mn><mn>9</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>0</mn><mn>9</mn><mn>3</mn><mn>6</mn></mtd><mtd><mn>5</mn><mn>6</mn><mo>.</mo><mn>7</mn><mn>0</mn><mn>1</mn><mn>1</mn><mn>6</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>0</mn><mn>9</mn><mn>0</mn><mn>9</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>3</mn><mn>8</mn><mn>3</mn><mn>3</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>9</mn><mn>9</mn><mn>9</mn><mn>9</mn><mn>9</mn><mn>2</mn></mtd><mtd><mn>1</mn><mn>6</mn><mo>.</mo><mn>6</mn><mn>1</mn><mn>7</mn><mn>8</mn><mn>2</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="112">, 畸变系数为 (<i>k</i><sub>1</sub>, <i>k</i><sub>2</sub>, <i>p</i><sub>1</sub>, <i>p</i><sub>2</sub>) = (-0.0874617, 0.0719448, 0.0003057, 0.0003541) 。深度相机的外参矩阵为</p>
                </div>
                <div class="area_img" id="113">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201902024_11300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="115">将深度相机和左<i>CCD</i>相机的外参代入 (5) 式, 可得到两坐标系下的坐标变换关系为</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>0</mn><mo>.</mo><mn>9</mn><mn>8</mn><mn>8</mn><mn>1</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>0</mn><mn>1</mn><mn>9</mn><mn>5</mn></mtd><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>2</mn><mn>5</mn></mtd></mtr><mtr><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>1</mn><mn>8</mn><mn>0</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>9</mn><mn>9</mn><mn>0</mn><mn>6</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>4</mn><mn>6</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>2</mn><mn>6</mn></mtd><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>4</mn><mn>4</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>9</mn><mn>7</mn><mn>9</mn><mn>2</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>⋅</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>+</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>4</mn><mn>4</mn><mo>.</mo><mn>5</mn><mn>4</mn><mn>6</mn><mn>4</mn></mtd></mtr><mtr><mtd><mn>1</mn><mn>0</mn><mn>4</mn><mo>.</mo><mn>0</mn><mn>5</mn><mn>6</mn><mn>6</mn></mtd></mtr><mtr><mtd><mn>4</mn><mn>8</mn><mo>.</mo><mn>8</mn><mn>6</mn><mn>6</mn><mn>8</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">为验证所提算法的有效性, 以航天扶手和抽屉把手作为待测物体, 用螺钉将其固定在面板上, 如图8 (<i>b</i>) 所示。机器人距离面板0.8～1.3 <i>m</i>, 这是一个较大的测量范围, 基于单相机的方法很难满足高精度、高速度的测量要求。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902024_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 实验环境搭建。 (a) 实验环境; (b) 待测目标" src="Detail/GetImg?filename=images/GXXB201902024_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 实验环境搭建。 (<i>a</i>) 实验环境; (<i>b</i>) 待测目标  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902024_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 8 <i>Establishment of experimental environment</i>. (<i>a</i>) <i>Experimental environment</i>; (<i>b</i>) <i>targets to be measured</i></p>

                </div>
                <h4 class="anchor-tag" id="119" name="119"><b>4.2</b><b>测量特征提取结果</b></h4>
                <div class="p1">
                    <p id="120">采用所提方法提取直线特征并求其交点, 结果如图9所示。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902024_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 直线特征提取结果。 (a) 航天扶手; (b) 抽屉把手" src="Detail/GetImg?filename=images/GXXB201902024_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 直线特征提取结果。 (a) 航天扶手; (b) 抽屉把手  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902024_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Results of linear feature extraction. (a) Space handrail; (b) drawer handle</p>

                </div>
                <h4 class="anchor-tag" id="122" name="122"><b>4.3</b><b>精度评估</b></h4>
                <div class="p1">
                    <p id="123">为了评估所提方法的测量精度, 将视觉系统在距离面板0.8～1.3 m的范围内移动, 变换不同的视点位置和观察角度。对每个物体在80个不同位姿下采集数据, 深度相机采集点云, CCD相机采集8位单通道灰度图像。在每个位姿下重复采集10次, 对于每个物体获取了800个点云及其对应的灰度图, 将它们作为精度评估数据集。利用激光跟踪仪获取物体坐标系相对于世界坐标系的真实位姿 (平移向量<b><i>T</i></b><sub>truth</sub>和旋转角度<b><i>A</i></b><sub>truth</sub>) , 然后采用所提方法对目标物体进行位姿测量。为了评估测量的位置误差和姿态误差, 以绝对误差作为评价指标, 即测量值与真实值之差的绝对值。位置误差的绝对值和姿态误差的绝对值分别为</p>
                </div>
                <div class="p1">
                    <p id="124" class="code-formula">
                        <mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">e</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>r</mtext></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>u</mtext><mtext>t</mtext><mtext>h</mtext></mrow></msub><mo stretchy="false">|</mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">e</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>n</mtext><mtext>g</mtext></mrow></msub><mo>=</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>r</mtext></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>u</mtext><mtext>t</mtext><mtext>h</mtext></mrow></msub><mo stretchy="false">|</mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="125">式中:<b><i>T</i></b><sub>our</sub>为平移向量的测量值;<b><i>A</i></b><sub>our</sub>为旋转角度的测量值。</p>
                </div>
                <div class="p1">
                    <p id="126">为了评估算法的可重复性和稳健性, 对同一位姿下重复采集的10次数据, 计算其测量位姿的平均值作为该位姿下的测量值, 然后计算80个不同位姿下测量结果的平均绝对误差和最大绝对误差, 统计结果如表1所示, 其中<i>E</i><sub><i>x</i></sub>、<i>E</i><sub><i>y</i></sub>、<i>E</i><sub><i>z</i></sub>分别为<i>x</i>、<i>y</i>、<i>z</i>轴上的位置误差, <i>E</i><sub><i>α</i></sub>、<i>E</i><sub><i>β</i></sub>、<i>E</i><sub><i>γ</i></sub>分别为绕<i>x</i>、<i>y</i>、<i>z</i>轴的旋转角误差。针对航天扶手的误差数据进行分析, 可以看出:<i>y</i>轴和<i>z</i>轴上的位置误差较大, 绕<i>x</i>轴的旋转角误差较大。结合图3 (a) 可知, 扶手上<i>ad</i>、<i>bc</i>两条边的长度较短, 在有些视角下成像不够清晰, 无法准确定位出直线<i>ad</i>, 直线提取存在一定偏差, 求取的交点出现偏差, 位姿测量误差较大。但总体上平移向量的平均绝对误差小于2 mm, 最大绝对误差小于3 mm, 旋转角平均绝对误差在1°以内, 最大绝对误差在2°以内。实验结果表明所提方法具有较高的测量精度, 同时也证明了所提方法的有效性和准确性。Ulrich的经典模板匹配方法<citation id="144" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>在成像视图退化情况下的稳健性较差, 无法正常完成测量, 故没有进行测量精度的对比。</p>
                </div>
                <div class="area_img" id="127">
                    <p class="img_tit">表1 测量精度 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Measurement accuracy</p>
                    <p class="img_note"></p>
                    <table id="127" border="1"><tr><td>Object</td><td>Deviation</td><td><i>E</i><sub><i>x</i></sub> /mm</td><td><i>E</i><sub><i>y</i></sub> /mm</td><td><i>E</i><sub><i>z</i></sub> /mm</td><td><i>E</i><sub><i>α</i></sub> / (°) </td><td><i>E</i><sub><i>β</i></sub> / (°) </td><td><i>E</i><sub><i>γ</i></sub> / (°) </td></tr><tr><td><br /><br />Space handrail</td><td>Mean absolute deviation</td><td>0.31</td><td>1.20</td><td>0.98</td><td>0.97</td><td>0.41</td><td>0.32</td></tr><tr><td><br /></td><td>Maximum absolute deviation</td><td>0.94</td><td>2.76</td><td>2.73</td><td>1.88</td><td>1.56</td><td>1.34</td></tr><tr><td><br /><br />Drawer handle</td><td>Mean absolute deviation</td><td>1.26</td><td>0.68</td><td>1.35</td><td>0.51</td><td>0.49</td><td>0.37</td></tr><tr><td><br /></td><td>Maximum absolute deviation</td><td>2.92</td><td>1.19</td><td>2.51</td><td>1.72</td><td>1.24</td><td>1.42</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="128" name="128"><b>4.4</b><b>效率评估</b></h4>
                <div class="p1">
                    <p id="129">除测量精度外, 衡量算法的另一重要性能指标是算法效率。将所提方法与Ulrich的经典模板匹配方法<citation id="145" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>进行对比, 所有实验均在便携式计算机 (Intel Core i7-7700HQ CPU 2.80 GHz, 8.00 GB内存) 上进行。所提方法在Visual Studio 2013环境release模式下运行并记录时间, Ulrich方法在机器视觉商业软件Halcon 11下记录运行时间。表2所示为运行时间的对比, 可以看出, 所提方法具有较高的效率。这主要是因为所提方法通过点云分割规避了复杂背景, 节省了在大面积背景上匹配的时间。另一方面, 所提方法没有大量模板的匹配过程, 节约了大量时间开销。</p>
                </div>
                <div class="area_img" id="130">
                    <p class="img_tit">表2 运行时间对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Running time comparison</p>
                    <p class="img_note"></p>
                    <table id="130" border="1"><tr><td><br />Methods</td><td>Average time /ms</td></tr><tr><td><br />Ulrich′s method</td><td>2120.43</td></tr><tr><td><br />Proposed method</td><td>148.14</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="132" name="132" class="anchor-tag">5 结 论</h3>
                <div class="p1">
                    <p id="133">提出了一种基于多传感器的三维目标位姿测量方法, 充分发挥深度相机和CCD相机各自的优势, 实现了对具有矩形特征三维目标的快速、高精度位姿测量。解决了经典模板匹配算法在成像视图退化情况下无法正确测量的问题, 大大提高了测量效率。实验结果证明了该方法的有效性。所提方法借助物体与面板之间位置关系的先验知识, 完成了目标物体的粗定位。不利用先验知识, 基于多传感器快速准确地完成三维目标位姿测量将是后续的研究方向。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501083888&amp;v=Mjc3NDZmT2ZiSzdIdEROcW85RVpPTU1CSFF4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpKS0Y4VGJoQT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Bay H, Ess A, Tuytelaars T, <i>et al</i>. Speeded-up robust features (SURF) [J]. Computer Vision and Image Understanding, 2008, 110 (3) : 346-359.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=N3M:Natural 3D Markers for Real-Time Object Detection and Pose Estimation">

                                <b>[2]</b> Hinterstoisser S, Benhimane S, Navab N. N<sub>3</sub>M: natural 3D markers for real-time object detection and pose estimation[C]. IEEE 11th International Conference on Computer Vision, 2007: 1-7.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Keypoint recognition using randomized trees">

                                <b>[3]</b> Lepetit V, Fua P. Keypoint recognition using randomized trees[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006, 28 (9) : 1465-1479.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cad-Based Recognition Of 3D Objects In Monocular Images">

                                <b>[4]</b> Ulrich M, Wiedemann C, Steger C. CAD-based recognition of 3D objects in monocular images[C]. IEEE International Conference on Robotics and Automation, 2009: 1191-1198.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Combining Scale-Space and Similarity-Based Aspect Graphs for Fast 3D Object Recognition">

                                <b>[5]</b> Ulrich M, Wiedemann C, Steger C. Combining scale-space and similarity-based aspect graphs for fast 3D object recognition[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (10) : 1902-1914.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multimodal Templates for Real-Time Detection of Texture-less Objects in Heavily Cluttered Scenes">

                                <b>[6]</b> Hinterstoisser S, Holzer S, Cagniart C, <i>et al</i>. Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes[C]. International Conference on Computer Vision, 2011: 858-865.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gradient Response Maps for Real-Time Detection of Textureless Objects">

                                <b>[7]</b> Hinterstoisser S, Cagniart C, Ilic S, <i>et al</i>. Gradient response maps for real-time detection of textureless objects[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (5) : 876-888.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15110100320222&amp;v=MTgzNDdsVXJ6SktGOFRiaEE9Tmo3QmFySzlIOURNcm85Rlora1BEbjQ3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Lepetit V, Moreno-Noguer F, Fua P. EPnP: an accurate <i>O</i> (<i>n</i>) solution to the PnP problem[J]. International Journal of Computer Vision, 2009, 81 (2) : 155-166.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003755464&amp;v=MTc1MjZ2a1c3N1BKRnc9Tmo3QmFyTzRIdEhQcUlwQVlPMExZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZp&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Zhang S J, Cao X B, Zhang F, <i>et al</i>. Monocular vision-based iterative pose estimation algorithm from corresponding feature points[J]. Science China Information Sciences, 2010, 53 (8) : 1682-1696.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811028&amp;v=MDAyMDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEa1ZMdktJalhUYkxHNEg5bk5ybzlIYklRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Zhu F, Yu F S, Wu Y M, <i>et al</i>. Analysis of calibration precision of camera attitude angles[J]. Acta Optica Sinica, 2018, 38 (11) : 1115005. 朱帆, 于芳苏, 吴易明, 等. P4P法相机姿态标定精度分析[J]. 光学学报, 2018, 38 (11) : 1115005.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LSD:a Line Segment Detector">

                                <b>[11]</b> von Gioi R G, Jakubowicz J, Morel J M, <i>et al</i>. LSD: a line segment detector[J]. Image Processing on Line, 2012, 2: 35-55.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201901009&amp;v=MjI0MTE0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVkx2S0x6elpmTEc0SDlqTXJvOUZiWVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Zhang H J, Fang Z J, Yang G L. RGB-D visual odometry in dynamic environments using line features[J]. Robot, 2018, 40 (5) : 1-8. 张慧娟, 方灶军, 杨桂林. 动态环境下基于线特征的RGB-D视觉里程计[J]. 机器人, 2018, 40 (5) : 1-8.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 Zhang Z. A flexible new technique for camera calibration[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22 (11) : 1330-1334.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201709025&amp;v=MDEwNzF0RmlEa1ZMdktJalhUYkxHNEg5Yk1wbzlIWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Song J H, Ren Y J, Yang S R, <i>et al</i>. Extrinsic parameter calibration method based on substitutable target sphere for vision sensors[J]. Acta Optica Sinica, 2017, 37 (9) : 0915003. 宋佳慧, 任永杰, 杨守瑞, 等. 基于合作靶球的视觉传感器外参标定方法[J]. 光学学报, 2017, 37 (9) : 0915003.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201805041&amp;v=MTIwNzBuTXFvOUJaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVkx2S0x5clBaTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Li R Z, Liu Y Y, Yang M, <i>et al</i>. Three-dimensional point cloud segmentation algorithm based on improved region growing[J]. Laser &amp; Optoelectronics Progress, 2018, 55 (5) : 051502. 李仁忠, 刘阳阳, 杨曼, 等. 基于改进的区域生长三维点云分割[J]. 激光与光电子学进展, 2018, 55 (5) : 051502.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D is here:Point Cloud Library (PCL)">

                                <b>[16]</b> Rusu R B, Cousins S. 3D is here: point cloud library (PCL) [C]. IEEE International Conference on Robotics and Automation, 2011: 1-4.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Performance Evaluation of RANSAC Family">

                                <b>[17]</b> Choi S, Kim T, Yu W. Performance evaluation of RANSAC family[C]. Proceedings of the British Machine Vision Conference, 2009: 355.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201710023&amp;v=MDQ0MjhadVp0RmlEa1ZMdktJalhUYkxHNEg5Yk5yNDlIWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Jin J J, Lu W L, Guo X T, <i>et al</i>. Position registration method of simultaneous phase-shifting interferograms based on SURF and RANSAC algorithms[J]. Acta Optica Sinica, 2017, 37 (10) : 1012002. 靳京京, 卢文龙, 郭小庭, 等. 基于SURF和RANSAC算法的同步相移干涉图位置配准方法[J]. 光学学报, 2017, 37 (10) : 1012002.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast and accurate ground plane detection for the visually impaired from 3D organized point clouds">

                                <b>[19]</b> Zeineldin R A, El-Fishawy N A. Fast and accurate ground plane detection for the visually impaired from 3D organized point clouds[C]. 2016 SAI Computing Conference, 2016: 373-379.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201902024" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902024&amp;v=MTY5MjVMRzRIOWpNclk5SFlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRGtWTHZLSWpYVGI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

