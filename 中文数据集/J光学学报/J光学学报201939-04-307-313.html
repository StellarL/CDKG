

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134123086533750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201904036%26RESULT%3d1%26SIGN%3dc5%252bYotwV0reXQ8LTiMdD9x7Pa2I%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201904036&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201904036&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201904036&amp;v=MDY0NTVxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtWNzdQSWpYVGJMRzRIOWpNcTQ5R1lvUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#46" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="2 MTYOLO目标检测 ">2 MTYOLO目标检测</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#52" data-title="&lt;b&gt;2.1 MTYOLO目标检测原理&lt;/b&gt;"><b>2.1 MTYOLO目标检测原理</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;2.2 MTYOLO网络设计&lt;/b&gt;"><b>2.2 MTYOLO网络设计</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#109" data-title="3 实验结果及分析 ">3 实验结果及分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#127" data-title="4 结 论 ">4 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#54" data-title="图1 MTYOLO目标检测算法流程图">图1 MTYOLO目标检测算法流程图</a></li>
                                                <li><a href="#92" data-title="图2 深度可分离卷积网络的基本结构。 (a) 标准卷积滤波 器; (b) 深度可分离卷积滤波器; (c) 点卷积滤波器">图2 深度可分离卷积网络的基本结构。 (a) 标准卷积滤波 器; (b) 深度可分离卷积滤波器; (......</a></li>
                                                <li><a href="#108" data-title="图3 MTYOLO网络架构">图3 MTYOLO网络架构</a></li>
                                                <li><a href="#113" data-title="表1 不同网络层融合后的检测精度对比">表1 不同网络层融合后的检测精度对比</a></li>
                                                <li><a href="#117" data-title="表2 MTYOLO使用BN层的实验结果对比">表2 MTYOLO使用BN层的实验结果对比</a></li>
                                                <li><a href="#120" data-title="表3 不同网络架构的实验结果对比">表3 不同网络架构的实验结果对比</a></li>
                                                <li><a href="#123" data-title="表4 所提算法与Tiny-Yolo在VOC数据集上的检测结果对比">表4 所提算法与Tiny-Yolo在VOC数据集上的检测结果对比</a></li>
                                                <li><a href="#125" data-title="图4 所提算法与Tiny-Yolo算法的对比效果。 (a) 输入图片; (b) Tiny-Yolo算法; (c) 所提算法">图4 所提算法与Tiny-Yolo算法的对比效果。 (a) 输入图片; (b) Tiny-Yolo算......</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" title=" Erhan D, Szegedy C, Toshev A, &lt;i&gt;et al&lt;/i&gt;.Scalable object detection using deep neural networks[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2014:2155-2162." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalable Object Detection Using Deep Neural Networks">
                                        <b>[1]</b>
                                         Erhan D, Szegedy C, Toshev A, &lt;i&gt;et al&lt;/i&gt;.Scalable object detection using deep neural networks[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2014:2155-2162.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" Wang T Y.The motion detection based on background difference method and active contour model[C]//IEEE Joint International Information Technology and Artificial Intelligence Conference, 2011:480-483." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The motion detection based on background difference method and active contourmodel">
                                        <b>[2]</b>
                                         Wang T Y.The motion detection based on background difference method and active contour model[C]//IEEE Joint International Information Technology and Artificial Intelligence Conference, 2011:480-483.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" title=" Lee D S.Effective Gaussian mixture learning for video background subtraction[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (5) :827-832." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Effective Gaussian Mixture Learning for Video Background Subtraction">
                                        <b>[3]</b>
                                         Lee D S.Effective Gaussian mixture learning for video background subtraction[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (5) :827-832.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" title=" Yuan G W, Chen Z Q, Gong J, &lt;i&gt;et al&lt;/i&gt;.A moving object detection algorithm based on a combination of optical flow and three-frame difference[J].Journal of Chinese Computer Systems, 2013, 34 (3) :668-671.袁国武, 陈志强, 龚健, 等.一种结合光流法与三帧差分法的运动目标检测算法[J].小型微型计算机系统, 2013, 34 (3) :668-671." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201303050&amp;v=MTIxOTBMTXJJOUFaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhrVjc3T1BUWGNkckc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Yuan G W, Chen Z Q, Gong J, &lt;i&gt;et al&lt;/i&gt;.A moving object detection algorithm based on a combination of optical flow and three-frame difference[J].Journal of Chinese Computer Systems, 2013, 34 (3) :668-671.袁国武, 陈志强, 龚健, 等.一种结合光流法与三帧差分法的运动目标检测算法[J].小型微型计算机系统, 2013, 34 (3) :668-671.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" title=" Xiao J, Zhu S P, Huang H, &lt;i&gt;et al&lt;/i&gt;.Object detecting and tracking algorithm based on optical flow[J].Journal of Northeastern University (Natural Science) , 2016, 37 (6) :770-774.肖军, 朱世鹏, 黄杭, 等.基于光流法的运动目标检测与跟踪算法[J].东北大学学报 (自然科学版) , 2016, 37 (6) :770-774." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DBDX201606003&amp;v=MTA0MjBGckNVUkxPZVplVnVGeUhrVjc3T0lTL1Bkckc0SDlmTXFZOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Xiao J, Zhu S P, Huang H, &lt;i&gt;et al&lt;/i&gt;.Object detecting and tracking algorithm based on optical flow[J].Journal of Northeastern University (Natural Science) , 2016, 37 (6) :770-774.肖军, 朱世鹏, 黄杭, 等.基于光流法的运动目标检测与跟踪算法[J].东北大学学报 (自然科学版) , 2016, 37 (6) :770-774.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" title=" Girshick R, Donahue J, Darrell T, &lt;i&gt;et al&lt;/i&gt;.Rich feature hierarchies for accurate object detection and semantic segmentation[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[6]</b>
                                         Girshick R, Donahue J, Darrell T, &lt;i&gt;et al&lt;/i&gt;.Rich feature hierarchies for accurate object detection and semantic segmentation[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2014:580-587.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.Spatial pyramid pooling in deep convolutional networks for visual recognition[M]//He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.eds.Computer Vision-ECCV 2014.Cham:Springer International Publishing, 2014:346-361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition">
                                        <b>[7]</b>
                                         He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.Spatial pyramid pooling in deep convolutional networks for visual recognition[M]//He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.eds.Computer Vision-ECCV 2014.Cham:Springer International Publishing, 2014:346-361.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" title=" Girshick R.Fast R-CNN[C]//IEEE International Conference on Computer Vision (ICCV) , 2015:1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[8]</b>
                                         Girshick R.Fast R-CNN[C]//IEEE International Conference on Computer Vision (ICCV) , 2015:1440-1448.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" Zhu W J, Wang G L, Tian J, &lt;i&gt;et al&lt;/i&gt;.Detection of moving objects in complex scenes based on multiple features[J].Acta Optica Sinica, 2018, 38 (6) :0612004.朱文杰, 王广龙, 田杰, 等.基于多特征的复杂场景运动目标检测[J].光学学报, 2018, 38 (6) :0612004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806026&amp;v=MDI0MzJPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtWNzdPSWpYVGJMRzRIOW5NcVk5SFlvUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Zhu W J, Wang G L, Tian J, &lt;i&gt;et al&lt;/i&gt;.Detection of moving objects in complex scenes based on multiple features[J].Acta Optica Sinica, 2018, 38 (6) :0612004.朱文杰, 王广龙, 田杰, 等.基于多特征的复杂场景运动目标检测[J].光学学报, 2018, 38 (6) :0612004.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" title=" Feng X Y, Mei W, Hu D S.Aerial target detection based on improved faster R-CNN[J].Acta Optica Sinica, 2018, 38 (6) :0615004.冯小雨, 梅卫, 胡大帅.基于改进Faster R-CNN的空中目标检测[J].光学学报, 2018, 38 (6) :0615004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806034&amp;v=MTY0MTh0R0ZyQ1VSTE9lWmVWdUZ5SGtWNzdPSWpYVGJMRzRIOW5NcVk5R1lJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Feng X Y, Mei W, Hu D S.Aerial target detection based on improved faster R-CNN[J].Acta Optica Sinica, 2018, 38 (6) :0615004.冯小雨, 梅卫, 胡大帅.基于改进Faster R-CNN的空中目标检测[J].光学学报, 2018, 38 (6) :0615004.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     Ren S Q, He K M, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.</a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" title=" Liu W, Anguelov D, Erhan D, &lt;i&gt;et al&lt;/i&gt;.SSD:single shot multiBox detector[M]//Liu W, Anguelov D, Erhan D, &lt;i&gt;et al&lt;/i&gt;.eds.Computer Vision-ECCV 2016.Cham:Springer International Publishing, 2016:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:Single shot multibox detector">
                                        <b>[12]</b>
                                         Liu W, Anguelov D, Erhan D, &lt;i&gt;et al&lt;/i&gt;.SSD:single shot multiBox detector[M]//Liu W, Anguelov D, Erhan D, &lt;i&gt;et al&lt;/i&gt;.eds.Computer Vision-ECCV 2016.Cham:Springer International Publishing, 2016:21-37.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" title=" Redmon J, Divvala S, Girshick R, &lt;i&gt;et al&lt;/i&gt;.You only look once:unified, real-time object detection[C]//IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016:779-788." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You Only Look Once:Unified,Real-Time Object Detection">
                                        <b>[13]</b>
                                         Redmon J, Divvala S, Girshick R, &lt;i&gt;et al&lt;/i&gt;.You only look once:unified, real-time object detection[C]//IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016:779-788.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" Shaifee M J, Chywl B, Li F, &lt;i&gt;et al&lt;/i&gt;.Fast YOLO:a fast you only look once system for real-time embedded object detection in video[EB/OL]. (2018-10-20) [2017-01-18].https://arxiv.org/abs/1709.05943." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast YOLO:a fast you only look once system for real-time embedded object detection in video">
                                        <b>[14]</b>
                                         Shaifee M J, Chywl B, Li F, &lt;i&gt;et al&lt;/i&gt;.Fast YOLO:a fast you only look once system for real-time embedded object detection in video[EB/OL]. (2018-10-20) [2017-01-18].https://arxiv.org/abs/1709.05943.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" Howard A G, Zhu M, Chen B, &lt;i&gt;et al&lt;/i&gt;.Mobilenets:efficient convolutional neural networks for mobile vision applications[EB/OL]. (2018-10-15) [2017-01-17].https://arxiv.org/abs/1704.04861." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mobilenets:efficient convolutional neural networks for mobile vision applications">
                                        <b>[15]</b>
                                         Howard A G, Zhu M, Chen B, &lt;i&gt;et al&lt;/i&gt;.Mobilenets:efficient convolutional neural networks for mobile vision applications[EB/OL]. (2018-10-15) [2017-01-17].https://arxiv.org/abs/1704.04861.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" title=" Huang G, Liu Z, Maaten L V D, &lt;i&gt;et al&lt;/i&gt;.Densely connected convolutional networks[C]//IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2017:2261-2269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Densely connected convolutional networks">
                                        <b>[16]</b>
                                         Huang G, Liu Z, Maaten L V D, &lt;i&gt;et al&lt;/i&gt;.Densely connected convolutional networks[C]//IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2017:2261-2269.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" title=" Redmon J, Farhadi A.YOLO&lt;sub&gt;9000&lt;/sub&gt;:better, faster, stronger[C]//IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2017:6517-6525." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better Faster,Stronger">
                                        <b>[17]</b>
                                         Redmon J, Farhadi A.YOLO&lt;sub&gt;9000&lt;/sub&gt;:better, faster, stronger[C]//IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2017:6517-6525.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" Everingham M, van Gool L, Williams C K I, &lt;i&gt;et al&lt;/i&gt;.The pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303-338." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15110200782102&amp;v=MDY1NTE3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JS1Y4UWF4UT1OajdCYXJLOUg5RE1yWTlGWStNTkRYdw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Everingham M, van Gool L, Williams C K I, &lt;i&gt;et al&lt;/i&gt;.The pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303-338.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-27 11:14</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(04),307-313 DOI:10.3788/AOS201939.0415006            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>面向嵌入式平台的轻量级目标检测网络</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B4%94%E5%AE%B6%E5%8D%8E&amp;code=37749723&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">崔家华</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%BA%91%E6%B4%B2&amp;code=10996198&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张云洲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E4%BA%89&amp;code=37749722&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王争</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%8F%8A%E6%83%9F&amp;code=38083563&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘及惟</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%9C%E5%8C%97%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0111402&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">东北大学信息科学与工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%9C%E5%8C%97%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">东北大学机器人科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于深度可分离卷积, 提出了一种适用于嵌入式平台的小型目标检测网络MTYOLO (MobileNet Tiny-Yolo) , 它将待检测的图片平均分割成多个单元格, 并采用深度可分离卷积代替传统卷积, 减少了参数量和计算量。采用点卷积和特征图融合的方法来提高检测精度。实验结果表明, 所提MTYOLO网络模型大小为41 MB, 约为Tiny-Yolo模型的67%, 其在PASCAL VOC 2007数据集上的检测准确率可达到57.25%, 检测效果优于Tiny-Yolo模型, 更适合应用于嵌入式系统。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%B3%BB%E7%BB%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">嵌入式系统;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%9E%E6%97%B6%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">实时性;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *张云洲, E-mail:zhangyunzhou@mail.neu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-22</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61471110);</span>
                                <span>中央高校基本科研业务专项资金 (N172608005);</span>
                                <span>辽宁省自然科学基金 (20180520040);</span>
                                <span>沈阳市高层次创新人才支持计划 (RC170490);</span>
                    </p>
            </div>
                    <h1><b>Light-Weight Object Detection Networks for Embedded Platform</b></h1>
                    <h2>
                    <span>Cui Jiahua</span>
                    <span>Zhang Yunzhou</span>
                    <span>Wang Zheng</span>
                    <span>Liu Jiwei</span>
            </h2>
                    <h2>
                    <span>College of Information Science & Engineering, Northeastern University</span>
                    <span>Faculty of Robot Science and Engineering, Northeastern University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Based on depth separable convolution, a small object detection network for embedded platform, MTYOLO (MobileNet Tiny-Yolo) , is proposed. It divides the image into many grids and replaces the traditional convolution by the depth separable convolution, which decreases the number of parameters and computational cost. The point convolution and the feature map merging are adopted to improve the detection accuracy. The experimental results show that the size of the proposed MTYOLO network model is 41 MB, approximately 67% of that of Tiny-Yolo model. Furthermore, its detection accuracy on the PASCAL VOC 2007 dataset is up to 57.25%, superior to the Tiny-Yolo model's. The proposed model is particularly suitable for application in embedded platforms.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=object%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">object detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=embedded%20system&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">embedded system;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=real%20time&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">real time;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-22</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="46" name="46" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="47">目标检测是计算机视觉领域重要的研究热点<citation id="129" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 包括场景中的目标分类和目标定位等。传统的目标检测算法主要是通过建立数学模型来完成, 如帧差法<citation id="130" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、背景减除法<citation id="131" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和光流法<citation id="132" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等。针对传统检测算法的改进方法也层出不穷, 如结合图像金字塔的改进版光流法<citation id="133" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等。近年来, 基于深度学习的目标检测取得了飞速发展, 研究者们提出了日趋复杂的网络结构以提高目标检测的准确率, 包括R-CNN<citation id="134" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation> (Region-Convolutional Neural Network) 、SPP-Net<citation id="135" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation> (Spatial Pyramid Pooling-Networks) 和Faster R-CNN<citation id="136" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等。同时, 针对深度学习的改进方法也有很多<citation id="138" type="reference"><link href="26" rel="bibliography" /><link href="28" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>。尽管这些目标检测网络可以达到很高的准确率, 但由于嵌入式平台的计算能力和内存资源非常有限, 即使采用实时效果好的Faster R-CNN<citation id="137" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 也只能达到1 frame/s以下的帧频。</p>
                </div>
                <div class="p1">
                    <p id="48">在面向移动端目标检测的深度神经网络方面, 研究者们提出了一些小型化的深度神经网络。例如, 旨在提高检测速度的SSD<citation id="139" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation> (Single Shot MultiBox Detector) 和YOLO<citation id="140" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation> (You Only Look Once) , 采用计算机 (PC) 的高性能图形处理器 (GPU) 可以达到实时目标检测的效果。然而, 这些模型应用于嵌入式平台时, 目标检测速度会明显下降<citation id="141" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。出现该现象的原因是:嵌入式平台的GPU性能远比不上PC的GPU性能, 前者的性能相比于后者至少下降1/10。为了解决这些问题, Tiny-Yolo通过减少网络模型的卷积层数来减小模型 (61 MB) , 同时大大减少了浮点运算次数, 但检测精度也大幅下降 (在PASCAL VOC 2007数据集上的准确率为48.4%) 。针对手机等嵌入式设备的轻量级深层卷积神经网络MobileNet<citation id="142" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 其核心思想是将常规卷积核进行巧妙分解, 替换成深度可分离卷积核, 从而有效解决了神经网络中计算效率低和模型参数量大等问题。深度可分离卷积由两部分组成, 即深度可分离卷积和点卷积 (1×1) 。深度可分离卷积在每一个卷积通道起作用, 而点卷积则将各通道的卷积输出进行整合。</p>
                </div>
                <div class="p1">
                    <p id="49">为了实现目标检测精度和实时性的良好平衡 (即在降低模型大小的同时保证嵌入式平台的计算能力, 以满足模型的计算量需求) , 本文设计了一种计算能力需求小、目标检测效果稳定的小型网络架构MTYOLO (MobileNet Tiny-Yolo) 。该网络由深度可分离卷积堆叠而成, 使用多层特征图融合算法进行特征融合, 充分利用多层浅层网络特征提高目标检测精度, 同时采用点卷积增加网络结构的深度, 让MTYOLO网络“深且窄”。所提网络在保持检测精度甚至提高检测精度的同时提高了检测速度, 做到在尽量最小化网络模型体积的同时, 保证了模型的检测精度。</p>
                </div>
                <div class="p1">
                    <p id="50">本文将从MTYOLO检测原理出发, 讲述目标检测原理, 随后讲解MTYOLO网络架构, 使用深度可分离卷积和点卷积减少网络参数, 以及使用多层特征图融合算法维持检测精度。最后, 通过进行多组实验, 验证了MTYOLO网络架构的合理性和可行性。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag">2 MTYOLO目标检测</h3>
                <h4 class="anchor-tag" id="52" name="52"><b>2.1 MTYOLO目标检测原理</b></h4>
                <div class="p1">
                    <p id="53">所提MTYOLO网络沿用了YOLO<citation id="143" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>目标检测的思想, 使用整个图片的特征来预测每个边框, 即一次预测所有类别的所有边框, 这样端到端的预测可以保证很高的检测速度。MTYOLO目标检测算法的流程如图1所示。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904036_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 MTYOLO目标检测算法流程图" src="Detail/GetImg?filename=images/GXXB201904036_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 MTYOLO目标检测算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904036_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Flow chart of MTYOLO object detection algorithm</p>

                </div>
                <div class="p1">
                    <p id="55">MTYOLO目标检测算法的具体做法如下:首先将输入图像按网格划分, 即输入<i>S</i>×<i>S</i>个网格。如果检测目标的中心落在其中一个网格内, 则检测目标的任务由该网格负责。网格的输出是<i>B</i>个目标边界框, 每个边界框输出5个参数, 即4个与目标相关的预测参数和边框置信度评分。4个与目标相关的预测参数分别为目标边框的中心点横坐标<i>x</i>、纵坐标<i>y</i>、宽度<i>w</i>和高度<i>h</i>。边框置信度评分可表示为</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>S</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>c</mtext><mtext>o</mtext><mtext>r</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false">) </mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>Ο</mi><mo stretchy="false">) </mo><mo>⋅</mo><mrow><mi>U</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>Ο</mtext><mtext>U</mtext></mrow></msub></mrow><msubsup><mrow></mrow><mi>p</mi><mi>t</mi></msubsup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">式中:<i>O</i>表示目标对象;<i>S</i><sub>score</sub>表示置信度;<i>P</i> (<i>O</i>) 表示MTYOLO预测目标边框内有目标的可能性;<i>U</i><sub>IOU</sub><mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>p</mi><mi>t</mi></msubsup></mrow></math></mathml> (<i>Intersection</i>-<i>Over</i>-<i>Union</i>, <i>IOU</i>) 表示<i>MTYOLO</i>预测目标边框的准确率。P (O) 为0的情况代表目标边框内不存在物体;P (O) 为1的情况代表目标边框内存在物体。若存在物体, 则根据输出的目标边框p和真实的目标边框t计算U<sub><i>IOU</i></sub><mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>p</mi><mi>t</mi></msubsup></mrow></math></mathml>, U<sub><i>IOU</i></sub><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>p</mi><mi>t</mi></msubsup></mrow></math></mathml>可表示为</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>U</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>Ο</mtext><mtext>U</mtext></mrow></msub></mrow><msubsup><mrow></mrow><mi>p</mi><mi>t</mi></msubsup><mo>=</mo><mfrac><mrow><mi>X</mi><msub><mrow></mrow><mrow><mtext>b</mtext><mtext>o</mtext><mtext>x</mtext><mi>p</mi></mrow></msub><mstyle displaystyle="true"><mo>∩</mo><mi>X</mi></mstyle><msub><mrow></mrow><mrow><mtext>b</mtext><mtext>o</mtext><mtext>x</mtext><mi>t</mi></mrow></msub></mrow><mrow><mi>X</mi><msub><mrow></mrow><mrow><mtext>b</mtext><mtext>o</mtext><mtext>x</mtext><mi>p</mi></mrow></msub><mstyle displaystyle="true"><mo>∪</mo><mi>X</mi></mstyle><msub><mrow></mrow><mrow><mtext>b</mtext><mtext>o</mtext><mtext>x</mtext><mi>t</mi></mrow></msub></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">式中:X<sub><i>box</i>t</sub>表示真实的目标边框 (<i>ground truth</i>) ;X<sub><i>box</i>p</sub>表示预测的目标边框 (<i>ground predict</i>) 。可以看到, U<sub><i>IOU</i></sub><mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>p</mi><mi>t</mi></msubsup></mrow></math></mathml>表示预测目标边框和真实的目标边框的交并比。同时, 各个边界框输出类别的置信度评分为</p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>S</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>c</mtext><mtext>o</mtext><mtext>r</mtext><mtext>e</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">) </mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>Ο</mi><mo stretchy="false">) </mo><mo>×</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>Ο</mi><mo stretchy="false">) </mo><mo>×</mo><mrow><mi>U</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>Ο</mtext><mtext>U</mtext></mrow></msub></mrow><msubsup><mrow></mrow><mi>p</mi><mi>t</mi></msubsup><mo>=</mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>×</mo><mrow><mi>U</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>Ο</mtext><mtext>U</mtext></mrow></msub></mrow><msubsup><mrow></mrow><mi>p</mi><mi>t</mi></msubsup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">式中:S<sub><i>scores</i></sub>表示多边界框置信度;P (<i>C</i><sub>i</sub>|O) 为可能存在目标时该目标属于某一类的后验概率。边界框的类别置信度评分有两层含义:1) 该边框预测的目标分属各类的概率;2) 边框和目标的匹配程度。为了过滤网络的预测框, 计算边界框置信度。</p>
                </div>
                <div class="p1">
                    <p id="66"><i>Faster R</i>-<i>CNN</i><citation id="144" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和<i>SSD</i><citation id="145" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>方法具有一定的主观性, 即人工确定先验框的维度。先验框的选择至关重要, 选取的先验框维度如果比较合适, 那么模型收敛得更快, 同时也会获得更好的检测结果。为了减小人为主观因素的影响, 采用<i>K</i>-<i>Means</i>聚类方法对训练集中的标定的边界框进行聚类分析。设置先验框的目的是使预测框X<sub><i>box</i></sub>与实际边界框T<sub><i>cent</i></sub>的交并集更大, 这样表明预测效果更好。因此聚类分析时选用两者之前的交并比的值作为距离指标:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mo stretchy="false"> (</mo><mi>X</mi><msub><mrow></mrow><mrow><mtext>b</mtext><mtext>o</mtext><mtext>x</mtext></mrow></msub><mo>, </mo><mi>Τ</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>e</mtext><mtext>n</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">) </mo><mo>=</mo><mn>1</mn><mo>-</mo><mi>U</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>Ο</mtext><mtext>U</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>X</mi><msub><mrow></mrow><mrow><mtext>b</mtext><mtext>o</mtext><mtext>x</mtext></mrow></msub><mo>, </mo><mi>Τ</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>e</mtext><mtext>n</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">) </mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68"><i>MTYOLO</i>目标检测算法用回归思想优化目标检测问题, 损失函数是常用的均方差损失。在进行预测时, 相关系数采用了不同的权重值。在定义的损失函数中有两种误差项, 即定位误差和分类误差。采用较大权重 (5) 的误差项是定位误差;采用较小权重 (1) 的误差项是分类误差。此外, 不包含目标的边界框和包含目标的边界框的置信度也有不同的权重, 分别为0.5和1。由于使用均方误差, 这样的权重操作会等同处理不同大小的边界框。为了提高预测小目标的能力, 需要将网络边界框的线性预测替换成平方根预测, 即预测值变为<mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo>, </mo><msqrt><mi>w</mi></msqrt><mo>, </mo><msqrt><mi>h</mi></msqrt><mo>, </mo><mi>c</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>c</mi></mrow></math></mathml>为置信度评分) , 从而达到减小边框坐标误差的目的。</p>
                </div>
                <div class="p1">
                    <p id="70">虽然一个单元格会输出多组边界框, 但其对应的目标只属于一个类别。在训练时, 如果已经确定该单元格存在目标, 则只选择与真实边界框的交并比的值最大的边界框作为目标边界框进行目标检测, 而舍弃其他交并比相对较小的边界框。这样设置将会使一个单元格的边界框的普适性更好, 即可以适用不同大小的待检测目标, 从而改善了小目标检测效果不好的情况, 提高了模型的整体检测性能。<i>MTYOLO</i>模型的损失函数计算式为</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo>=</mo><mi>λ</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>o</mtext><mtext>r</mtext><mtext>d</mtext></mrow></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>S</mi><mn>2</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>B</mi></munderover><mn>1</mn></mstyle></mrow></mstyle><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup></mrow><mo stretchy="false">[</mo><mrow><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>o</mtext><mtext>r</mtext><mtext>d</mtext></mrow></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>S</mi><mn>2</mn></mrow></munderover><mrow></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>B</mi></munderover><mn>1</mn></mstyle><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup></mrow><mrow><mo stretchy="false"> (</mo><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>C</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>S</mi><mn>2</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>B</mi></munderover><mn>1</mn></mstyle></mrow></mstyle><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup></mrow><mrow><mo stretchy="false"> (</mo><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>C</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>S</mi><mn>2</mn></mrow></munderover><mrow></mrow></mstyle></mrow><mn>1</mn><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup></mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mtext>c</mtext><mtext>l</mtext><mtext>a</mtext><mtext>s</mtext><mtext>s</mtext><mtext>e</mtext><mtext>s</mtext></mrow></munder><mrow></mrow></mstyle><mrow><mo stretchy="false">{</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>C</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">) </mo><mo>-</mo><mover accent="true"><mi>p</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>C</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">}</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">式中:<i>λ</i><sub>coord</sub>为定位误差权重;<i>λ</i><sub>noobj</sub>为分类误差权重;<i>1</i><sup>obj</sup><sub><i>ij</i></sub>表示第<i>i</i>个单元格存在的目标, 且该单元格中的第<i>j</i>个边界框负责预测该目标;<i>1</i><sup>noobj</sup><sub><i>ij</i></sub>表示不存在该目标;<i>C</i><sub><i>i</i></sub>为第<i>i</i>个单元格的置信度的预测值;<mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>C</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>为第i个单元格的置信度的真实值;p<sub>i</sub> (C<sub><i>cls</i></sub>) 为属于第i个类别C<sub><i>cls</i></sub>概率的预测值;<mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>C</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>为属于第<i>i</i>个类别<i>C</i><sub>cls</sub>概率的真实值;<i>S</i>为图片划分为单元格的数量;<i>B</i>为预测的边界框数量; (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) 为边界框坐标预测值;<mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>为边界框坐标真实值。<i>λ</i><sub>coord</sub>和<i>λ</i><sub>noobj</sub>为经验权重, 分别设为4和0.5。设置<i>λ</i><sub>coord</sub>为较大权重是为了减小边界框回归的难度, 使网络模型更易收敛;而设置<i>λ</i><sub>noobj</sub>为较小权重是考虑了训练时负样本较多情况, 使得正负样本更加均衡。赋予定位误差和分类误差不同的权重的目的是更好地训练模型。 (5) 式的第1项是边界框中心坐标的误差项, 第2项是边界框的高与宽的误差项, 第3项是包含目标的边界框的置信度的误差项, 第4项是不包含目标的边界框的置信度误差项, 第5项是包含目标的单元格的分类误差项。</p>
                </div>
                <div class="p1">
                    <p id="76">此外, 利用<i>U</i><sub>IOU</sub>、<i>P</i><sub>AP</sub> (Average Precision) 、<i>P</i><sub>mAP</sub> (Mean Average Precision) 和帧频FPS (Frames Per Second) 来评价目标检测模型的优劣。<i>P</i><sub>AP</sub>和<i>P</i><sub>mAP</sub>的计算公式为</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>Μ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>a</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow></mrow></mstyle><mi>U</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>Ο</mtext><mtext>U</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>b</mi><msub><mrow></mrow><mi>a</mi></msub><mo>, </mo><mi>b</mi><msubsup><mrow></mrow><mi>a</mi><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msubsup><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>A</mtext><mtext>Ρ</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>C</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mrow><mn>2</mn><mn>0</mn></mrow></munderover><mrow></mrow></mstyle><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext></mrow></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">式中:<i>b</i><sub><i>a</i></sub>为检测框;<i>b</i><sup>gt</sup><sub><i>a</i></sub>为标签框;<i>M</i>为测试集的标签框总数;<i>C</i>为类别数, <i>C</i>=20。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79"><b>2.2 MTYOLO网络设计</b></h4>
                <div class="p1">
                    <p id="80">所提MTYOLO是一种基于回归思想的端到端的目标检测框架, 采用深度可分离卷积提取特征可有效提高卷积网络的计算效率并减少其庞大的参数量。同时, 通过使用多层特征融合和使用点卷积增加网络深度的方法来提高卷积网络模型的检测精度。</p>
                </div>
                <div class="p1">
                    <p id="81">假定在特征图的通道维度映射中, 常规卷积可变换分解成线性组合的形式。用<b><i>K</i></b>表示一个常规卷积核, 则有</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Κ</mi><mo>=</mo><mi mathvariant="bold-italic">Μ</mi><mo>⋅</mo><mo>∧</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">b</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">式中:<b><i>b</i></b>为由<i>S</i>×<i>S</i>大小的2维卷积组成的<i>m</i>维矩阵, 可表示为</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">b</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>b</mi><msubsup><mrow></mrow><mrow><mn>0</mn><mn>0</mn></mrow><mi>i</mi></msubsup></mtd><mtd><mo>⋯</mo></mtd><mtd><mtext>b</mtext><msubsup><mrow></mrow><mrow><mn>0</mn><mi>s</mi></mrow><mi>i</mi></msubsup></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>b</mi><msubsup><mrow></mrow><mrow><mi>s</mi><mn>0</mn></mrow><mi>i</mi></msubsup></mtd><mtd><mo>⋯</mo></mtd><mtd><mtext>b</mtext><msubsup><mrow></mrow><mrow><mi>s</mi><mi>s</mi></mrow><mi>i</mi></msubsup></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">定义∧ (<b><i>b</i></b>) 为以<i>b</i><sub><i>i</i></sub> (<i>i</i>=0, 1, 2, ..., <i>m</i>) 为对角元素的对角矩阵, 即</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∧</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">b</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>b</mi><msub><mrow></mrow><mn>0</mn></msub></mtd><mtd><mn>0</mn></mtd><mtd><mo>⋯</mo></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>b</mi><msub><mrow></mrow><mi>m</mi></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87"><b><i>M</i></b>是一个<i>n</i>×<i>m</i>的数值矩阵, <i>n</i>和<i>m</i>分别表示输出特征图和输入特征图的维度, “·”表示一种特殊的矩阵乘法。常规卷积核<b><i>K</i></b>的运算公式为</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Κ</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>k</mi><msub><mrow></mrow><mrow><mn>0</mn><mn>0</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>k</mi><msub><mrow></mrow><mrow><mn>0</mn><mi>m</mi></mrow></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>k</mi><msub><mrow></mrow><mrow><mi>n</mi><mn>0</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>k</mi><msub><mrow></mrow><mrow><mi>n</mi><mi>m</mi></mrow></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>μ</mi><msub><mrow></mrow><mrow><mn>0</mn><mn>0</mn></mrow></msub><mi>b</mi><msub><mrow></mrow><mn>0</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>μ</mi><msub><mrow></mrow><mrow><mn>0</mn><mi>m</mi></mrow></msub><mi>b</mi><msub><mrow></mrow><mi>m</mi></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>μ</mi><msub><mrow></mrow><mrow><mi>n</mi><mn>0</mn></mrow></msub><mi>b</mi><msub><mrow></mrow><mn>0</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>μ</mi><msub><mrow></mrow><mrow><mi>n</mi><mi>m</mi></mrow></msub><mi>b</mi><msub><mrow></mrow><mi>m</mi></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>μ</mi><msub><mrow></mrow><mrow><mn>0</mn><mn>0</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>μ</mi><msub><mrow></mrow><mrow><mn>0</mn><mi>m</mi></mrow></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>μ</mi><msub><mrow></mrow><mrow><mi>n</mi><mn>0</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>μ</mi><msub><mrow></mrow><mrow><mi>n</mi><mi>m</mi></mrow></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>⋅</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>b</mi><msub><mrow></mrow><mn>0</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>b</mi><msub><mrow></mrow><mi>m</mi></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">式中:<i>μ</i><sub><i>nm</i></sub>为位置 (<i>i</i>, <i>j</i>) 上卷积核的权重系数;<i>b</i><sub><i>j</i></sub>为位置 (<i>n</i>, <i>m</i>) 上卷积核的偏置;<i>k</i><sub><i>nm</i></sub>为位置 (<i>n</i>, <i>m</i>) 上卷积核的数值, <i>k</i><sub><i>nm</i></sub>=<i>μ</i><sub><i>nm</i></sub><i>b</i><sub><i>m</i></sub>。由 (10) 式可知, 常规卷积和<b><i>M</i></b>的参数量为<i>s</i>×<i>s</i>×<i>n</i>×<i>m</i>, 而MTYOLO的深度可分离卷积参数量为<i>s</i>×<i>s</i>×<i>m</i>+<i>n</i>×<i>m</i>, 则压缩率为</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>s</mi><mo>×</mo><mi>s</mi><mo>×</mo><mi>m</mi><mo>+</mo><mi>n</mi><mo>×</mo><mi>m</mi></mrow><mrow><mi>s</mi><mo>×</mo><mi>s</mi><mo>×</mo><mi>n</mi><mo>×</mo><mi>m</mi></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>+</mo><mfrac><mn>1</mn><mrow><mi>s</mi><mo>×</mo><mi>s</mi></mrow></mfrac><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">此外, 使用 Batch Normalization的方法对相应的激活操作进行规范化处理, 令输出结果 (输出信号各个维度) 的均值为0、方差为1, 这样可以减小下一层网络输入数据分布情况的变化, 使输入数据的分布情况统一化, 同时可以有效地提高模型收敛速度, 并减少梯度爆炸等状况的产生。深度可分离卷积网络的基本结构如图2所示, 它展示的是将常规卷积核用2个卷积核替代, 即采用深度可分离卷积和点卷积。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904036_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 深度可分离卷积网络的基本结构。 (a) 标准卷积滤波 器; (b) 深度可分离卷积滤波器; (c) 点卷积滤波器" src="Detail/GetImg?filename=images/GXXB201904036_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 深度可分离卷积网络的基本结构。 (a) 标准卷积滤波 器; (b) 深度可分离卷积滤波器; (c) 点卷积滤波器  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904036_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Basic structure of depth separable convolution network. (a) Standard convolution filter; (b) depthwise convolution filter; (c) pointwise convolution filter</p>

                </div>
                <div class="p1">
                    <p id="93">MTYOLO模型就是使用上文提到的深度可分离卷积堆积而成。该网络为全卷积网络, 具体由常规卷积、深度可分离卷积和点卷积组成。同时, 对于每层卷积网络的输出, 均采用批规范化处理, 即添加BN (Batch Normalization) 层。</p>
                </div>
                <div class="p1">
                    <p id="94">BN层的提出解决了反向传播过程中的梯度消失和梯度爆炸。前向传播可表示为</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>e</mi></msub><mo>=</mo><mi mathvariant="bold-italic">ω</mi><msubsup><mrow></mrow><mi>e</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>e</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">式中:<b><i>h</i></b><sub><i>e</i></sub>为第<i>e</i>层神经网络节点向量;<i>ω</i><sub><i>e</i></sub>为第<i>e</i>层的权重系数矩阵。那么, 反向传播时便有</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>e</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>e</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>e</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>e</mi></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>e</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>e</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>e</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>e</mi></msub></mrow></mfrac><mi mathvariant="bold-italic">ω</mi><msub><mrow></mrow><mi>e</mi></msub><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">从第<i>e</i>层到第<i>k</i>层的传播可表示为</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>e</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>e</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>e</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mi>e</mi></munderover><mrow></mrow></mstyle><mi mathvariant="bold-italic">ω</mi><msub><mrow></mrow><mi>e</mi></msub><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">当网络层数很深时:若<i>ω</i><sub><i>e</i></sub>小于1, (15) 式<mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mi>e</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><i>ω</i><sub><i>i</i></sub>项会导致梯度消失问题;若<i>ω</i><sub><i>e</i></sub>大于1, 又会有梯度爆炸等问题。而BN层可以有效解决这个问题, 它作为神经网络中的一层可以将输入进行标准化, 减少权重<i>ω</i><sub><i>i</i></sub>对尺度的影响, 具体方程为</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>e</mi></msub><mo>=</mo><mtext>B</mtext><mtext>Ν</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">ω</mi><msub><mrow></mrow><mi>e</mi></msub><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>e</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>=</mo><mtext>B</mtext><mtext>Ν</mtext><mo stretchy="false"> (</mo><mi>a</mi><mi mathvariant="bold-italic">ω</mi><msub><mrow></mrow><mi>e</mi></msub><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>e</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">式中:BN (·) 表示对数据进行批量化处理。那么, 反向求导可得</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>e</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>e</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mtext>B</mtext><mtext>Ν</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">ω</mi><msub><mrow></mrow><mi>e</mi></msub><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>e</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>e</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mtext>B</mtext><mtext>Ν</mtext><mo stretchy="false"> (</mo><mi>a</mi><mi mathvariant="bold-italic">ω</mi><msub><mrow></mrow><mi>e</mi></msub><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>e</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>e</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mfrac><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105">因此, 通过使用BN层, 在反向传播过程中, 需要乘以的系数<i>a</i>便不再与<i>ω</i><sub><i>e</i></sub>的尺度相关, 这就意味即使<i>ω</i><sub><i>e</i></sub>在反向传播中改变了, 但是其梯度却可以正常传递。</p>
                </div>
                <div class="p1">
                    <p id="106">MTYOLO模型的整体网络架构如图3所示, 其中Conv表示常规卷积, Max表示最大池化层, Conv dw表示深度可分离卷积, Conv pw表示点卷积, 池化层在卷积层后使用。</p>
                </div>
                <div class="p1">
                    <p id="107">同时, 从Huang等<citation id="146" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>的工作中不难发现, 从浅层网络获得特征图并对其进行特征图融合, 最终得到网络的表现效果更好。与在YOLOv2<citation id="147" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>中进行单层特征图融合不同, 在此采用全新的特征图融合方法, 即多层特征图融合。该方法是通过点卷积方法改变特征图的通道, 然后使用reshape方法将特征图变换到指定大小, 对多层网络进行相同处理, 并与上层网络进行叠加, 得到融合后的特征图。此外, 从SSD<citation id="148" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>中汲取灵感, 设计了这个深而窄的网络架构, 更深的网络架构可获得更高的精度, 而更窄的网络架构限制网络的复杂度。因此, 所提网络与其他检测网络相比, 整体网络层数更少。因为过深的网络会导致计算复杂度过高, 为了限制网络的复杂度, 在此使用点卷积和深度可分离卷积构建网络。在网络结构末尾的主要卷积操作中, 通过增加点卷积数量来加大网络深度, 从而限制网络的复杂度。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904036_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 MTYOLO网络架构" src="Detail/GetImg?filename=images/GXXB201904036_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 MTYOLO网络架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904036_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Architecture of MTYOLO network</p>

                </div>
                <h3 id="109" name="109" class="anchor-tag">3 实验结果及分析</h3>
                <div class="p1">
                    <p id="110">实验硬件配置为Intel Xeon CPU E5-2640 v4处理器、TITAN Xp显卡、64 GB RAM的服务器, 主要用来训练MTYOLO网络模型。同时, 使用英伟达Jeston TX1作为移动端, 配置为Quad ARM® A57/2 MB L2处理器, NVIDIA Maxwell<sup>TM</sup>显卡、4 GB RAM。</p>
                </div>
                <div class="p1">
                    <p id="111">实验采用了数据集PASCAL VOC<citation id="149" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>, 这是一个视觉对象的分类识别和检测的标准数据集, 包含了20种不同的已经标注好的目标。实验使用其中的PASCAL VOC2007和PASCAL VOC2012两个数据集进行训练和测试。</p>
                </div>
                <div class="p1">
                    <p id="112">首先进行特征图融合后的检测精度对比实验, 对比基准为不进行特征图融合的检测精度, 其对比结果如表1所示。其中, Conv2-Conv7对应的“×”代表不进行特征图融合, “√”代表进行特征图融合。Conv10为最后添加的点卷积, 作用是增加网络深度, 对应的“×”代表不使用该点卷积, “√”代表使用点卷积。</p>
                </div>
                <div class="area_img" id="113">
                    <p class="img_tit">表1 不同网络层融合后的检测精度对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Comparison of detection accuracy after merging different layers</p>
                    <p class="img_note"></p>
                    <table id="113" border="1"><tr><td><br />Conv2</td><td>Conv3</td><td>Conv4</td><td>Conv5</td><td>Conv6</td><td>Conv7</td><td>Conv10</td><td><i>P</i><sub>mAP</sub></td></tr><tr><td><br />×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>54.2</td></tr><tr><td><br />×</td><td>×</td><td>×</td><td>√</td><td>√</td><td>×</td><td>×</td><td>56.7</td></tr><tr><td><br />×</td><td>×</td><td>√</td><td>√</td><td>√</td><td>×</td><td>×</td><td>57.6</td></tr><tr><td><br />×</td><td>√</td><td>√</td><td>√</td><td>√</td><td>×</td><td>×</td><td>58.4</td></tr><tr><td><br />√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>×</td><td>×</td><td>58.2</td></tr><tr><td><br />×</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>×</td><td>58.9</td></tr><tr><td><br />×</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>59.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="114">对同种物体进行检测, 可以看到随着特征图融合数量的增加, 检测器的精度也在提高。同时, 合并更深层的特征图对检测效果有优化, 而合并网络的初始卷积层对检测没有优化。从表1可以看出依次增加融合的卷积层数量对<i>P</i><sub>mAP</sub>的提升效果。其中, 增加Conv2的融合, 检测精度反而降低。此外, 实验结果也表明, 使用特征融合和点卷积的方法, 可使检测精度提高0.5 <i>P</i><sub>mAP</sub>。</p>
                </div>
                <div class="p1">
                    <p id="115">在VOC数据集上, 对是否使用BN层的情况进行检测结果对比实验, “×”代表不用BN层, “√”代表使用BN层。实验结果如表2所示。</p>
                </div>
                <div class="p1">
                    <p id="116">从表2实验结果可以看出, MTYOLO使用BN层后检测精度提升了3.5 <i>P</i><sub>mAP</sub>, 检测速度降低了2 frame·s<sup>-1</sup>。由于检测精度降低太多, 对MTYOLO网络使用BN层相应的激活操作进行规范化处理。此外, 在模型训练时, 没有使用BN层的MTYOLO网络的收敛速度非常慢, 耗时约为使用BN层的5倍。</p>
                </div>
                <div class="area_img" id="117">
                    <p class="img_tit">表2 MTYOLO使用BN层的实验结果对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Comparison of experimental results of MTYOLO with BN layer</p>
                    <p class="img_note"></p>
                    <table id="117" border="1"><tr><td><br />BN layer</td><td><i>P</i><sub>mAP</sub></td><td>Speed / (frame·s<sup>-1</sup>) </td></tr><tr><td><br />×</td><td>52.7</td><td>31</td></tr><tr><td><br />√</td><td>56.2</td><td>29</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="119">随后, 将模型部署于嵌入式平台Jeston TX1上, 进行不同网络架构的速度和模型大小对比实验。实验结果如表3所示。</p>
                </div>
                <div class="area_img" id="120">
                    <p class="img_tit">表3 不同网络架构的实验结果对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Comparison of experimental results of different network architectures</p>
                    <p class="img_note"></p>
                    <table id="120" border="1"><tr><td><br />Model</td><td>Speed / (frame·s<sup>-1</sup>) </td><td>Model size /MB</td></tr><tr><td><br />Yolo-v2</td><td>3</td><td>193</td></tr><tr><td><br />Tiny-Yolo</td><td>18</td><td>61</td></tr><tr><td><br />Using feature <br />map fusion</td><td>15</td><td>63</td></tr><tr><td><br />Using pointwise</td><td>23</td><td>50</td></tr><tr><td><br />Using depthwise</td><td>29</td><td>41</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="121">通过表3的实验结果, 可以发现MTYOLO的模型大小为41 MB, 分别为Yolo-v2模型和Tiny-Yolo模型大小的21%和67%, 而速度是Tiny-Yolo的1.6倍。从表3的对比实验结果可以看出:特征图融合会略微降低网络模型的速度以及增加模型的大小, 通过合理使用深度可分离卷积和点卷积可以有效提高模型的运行速度, 同时减小模型的大小。</p>
                </div>
                <div class="p1">
                    <p id="122">最后, 使用Pascal VOC 2007数据集测试检测器性能, 表4为不同模型在VOC数据集上的测试效果对比。可以看出, MTYOLO模型比Tiny-Yolo模型的检测速度快了近1.6倍, 同时MTYOLO算法在VOC数据集上的检测准确率可达57.25%, 比Tiny-Yolo的检测准确率高3.05%。尤其是对瓶子和鸟等小物体的检测精度的提升效果更为显著。</p>
                </div>
                <div class="area_img" id="123">
                    <p class="img_tit">表4 所提算法与Tiny-Yolo在VOC数据集上的检测结果对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Comparison of detection results on VOC dataset by proposed algorithm and Tiny-Yolo</p>
                    <p class="img_note"></p>
                    <table id="123" border="1"><tr><td><br />Method</td><td><i>P</i><sub>mAP</sub></td><td>FPS</td><td>aero</td><td>bike</td><td>bird</td><td>boat</td><td>bottle</td><td>bus</td><td>car</td><td>cat</td><td>chair</td></tr><tr><td><br />Tiny-Yolo</td><td>54.2</td><td>18</td><td>57.4</td><td>67.5</td><td>44.9</td><td>34.8</td><td>20.4</td><td>67.5</td><td>62.9</td><td>67.4</td><td>32.0</td></tr><tr><td><br />MTYOLO</td><td>57.25</td><td>29</td><td>65.3</td><td>74.2</td><td>49.6</td><td>39.1</td><td>30.2</td><td>69.8</td><td>65.8</td><td>69.5</td><td>34.2</td></tr><tr><td><br />Method</td><td>cow</td><td>table</td><td>dog</td><td>horse</td><td>mbike</td><td>person</td><td>plant</td><td>sheep</td><td>sofa</td><td>train</td><td>TV</td></tr><tr><td><br />Tiny-Yolo</td><td>53.7</td><td>58.1</td><td>61.6</td><td>70.5</td><td>69.1</td><td>58.0</td><td>27.8</td><td>52.8</td><td>51.1</td><td>68.5</td><td>57.4</td></tr><tr><td><br />MTYOLO</td><td>56.2</td><td>59.3</td><td>63.5</td><td>72.1</td><td>70.2</td><td>59.2</td><td>29.8</td><td>55.1</td><td>53.6</td><td>69.1</td><td>59.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="124">在VOC测试集上使用Tiny-Yolo和MTYOLO算法进行目标检测对比, 结果如图4所示。可以看到, MTYOLO算法的检测效果优于Tiny-Yolo。相对于Tiny-Yolo算法, MTYOLO算法的错误检测和漏检情况有所减少, 同时MTYOLO算法的预测边界框与真实边界框更接近、对物体的检测更为准确。</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904036_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 所提算法与Tiny-Yolo算法的对比效果。 (a) 输入图片; (b) Tiny-Yolo算法; (c) 所提算法" src="Detail/GetImg?filename=images/GXXB201904036_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 所提算法与Tiny-Yolo算法的对比效果。 (a) 输入图片; (b) Tiny-Yolo算法; (c) 所提算法  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904036_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Comparison of proposed algorithm and Tiny-Yolo algorithm. (a) Input image; (b) Tiny-Yolo algorithm; (c) proposed algorithm</p>

                </div>
                <h3 id="127" name="127" class="anchor-tag">4 结 论</h3>
                <div class="p1">
                    <p id="128">为解决模型嵌入式平台计算能力和内存资源有限、难以部署于大型网络的问题, 提出了一种基于深度可分离卷积的MTYOLO目标检测算法。该算法将常规卷积用深度可分离卷积替换, 大大减少了模型参数数目、提高了检测效率。同时, 利用点卷积和多层特征图融合的方法提高模型检测精度, 点卷积的作用在于增加网络深度、减小网络宽度、让网络架构“深且窄”, 多层特征图融合算法可以有效利用多层浅层网络特征, 从而提高网络检测精度。利用MTYOLO网络进行多次迭代训练得到轻量级的目标检测模型, 该检测模型对提到的移动端目标检测具有良好的检测效果。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalable Object Detection Using Deep Neural Networks">

                                <b>[1]</b> Erhan D, Szegedy C, Toshev A, <i>et al</i>.Scalable object detection using deep neural networks[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2014:2155-2162.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The motion detection based on background difference method and active contourmodel">

                                <b>[2]</b> Wang T Y.The motion detection based on background difference method and active contour model[C]//IEEE Joint International Information Technology and Artificial Intelligence Conference, 2011:480-483.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Effective Gaussian Mixture Learning for Video Background Subtraction">

                                <b>[3]</b> Lee D S.Effective Gaussian mixture learning for video background subtraction[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (5) :827-832.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201303050&amp;v=MTUzMjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIa1Y3N09QVFhjZHJHNEg5TE1ySTlBWklRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Yuan G W, Chen Z Q, Gong J, <i>et al</i>.A moving object detection algorithm based on a combination of optical flow and three-frame difference[J].Journal of Chinese Computer Systems, 2013, 34 (3) :668-671.袁国武, 陈志强, 龚健, 等.一种结合光流法与三帧差分法的运动目标检测算法[J].小型微型计算机系统, 2013, 34 (3) :668-671.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DBDX201606003&amp;v=MDUzOTg3T0lTL1Bkckc0SDlmTXFZOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhrVjc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Xiao J, Zhu S P, Huang H, <i>et al</i>.Object detecting and tracking algorithm based on optical flow[J].Journal of Northeastern University (Natural Science) , 2016, 37 (6) :770-774.肖军, 朱世鹏, 黄杭, 等.基于光流法的运动目标检测与跟踪算法[J].东北大学学报 (自然科学版) , 2016, 37 (6) :770-774.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[6]</b> Girshick R, Donahue J, Darrell T, <i>et al</i>.Rich feature hierarchies for accurate object detection and semantic segmentation[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2014:580-587.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition">

                                <b>[7]</b> He K M, Zhang X Y, Ren S Q, <i>et al</i>.Spatial pyramid pooling in deep convolutional networks for visual recognition[M]//He K M, Zhang X Y, Ren S Q, <i>et al</i>.eds.Computer Vision-ECCV 2014.Cham:Springer International Publishing, 2014:346-361.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[8]</b> Girshick R.Fast R-CNN[C]//IEEE International Conference on Computer Vision (ICCV) , 2015:1440-1448.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806026&amp;v=MDk1OTVrVjc3T0lqWFRiTEc0SDluTXFZOUhZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Zhu W J, Wang G L, Tian J, <i>et al</i>.Detection of moving objects in complex scenes based on multiple features[J].Acta Optica Sinica, 2018, 38 (6) :0612004.朱文杰, 王广龙, 田杰, 等.基于多特征的复杂场景运动目标检测[J].光学学报, 2018, 38 (6) :0612004.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806034&amp;v=MTkzMjhIOW5NcVk5R1lJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtWNzdPSWpYVGJMRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Feng X Y, Mei W, Hu D S.Aerial target detection based on improved faster R-CNN[J].Acta Optica Sinica, 2018, 38 (6) :0615004.冯小雨, 梅卫, 胡大帅.基于改进Faster R-CNN的空中目标检测[J].光学学报, 2018, 38 (6) :0615004.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 Ren S Q, He K M, Girshick R, <i>et al</i>.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:Single shot multibox detector">

                                <b>[12]</b> Liu W, Anguelov D, Erhan D, <i>et al</i>.SSD:single shot multiBox detector[M]//Liu W, Anguelov D, Erhan D, <i>et al</i>.eds.Computer Vision-ECCV 2016.Cham:Springer International Publishing, 2016:21-37.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You Only Look Once:Unified,Real-Time Object Detection">

                                <b>[13]</b> Redmon J, Divvala S, Girshick R, <i>et al</i>.You only look once:unified, real-time object detection[C]//IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016:779-788.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast YOLO:a fast you only look once system for real-time embedded object detection in video">

                                <b>[14]</b> Shaifee M J, Chywl B, Li F, <i>et al</i>.Fast YOLO:a fast you only look once system for real-time embedded object detection in video[EB/OL]. (2018-10-20) [2017-01-18].https://arxiv.org/abs/1709.05943.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mobilenets:efficient convolutional neural networks for mobile vision applications">

                                <b>[15]</b> Howard A G, Zhu M, Chen B, <i>et al</i>.Mobilenets:efficient convolutional neural networks for mobile vision applications[EB/OL]. (2018-10-15) [2017-01-17].https://arxiv.org/abs/1704.04861.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Densely connected convolutional networks">

                                <b>[16]</b> Huang G, Liu Z, Maaten L V D, <i>et al</i>.Densely connected convolutional networks[C]//IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2017:2261-2269.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better Faster,Stronger">

                                <b>[17]</b> Redmon J, Farhadi A.YOLO<sub>9000</sub>:better, faster, stronger[C]//IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2017:6517-6525.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15110200782102&amp;v=MDQ0ODc9Tmo3QmFySzlIOURNclk5RlkrTU5EWHc3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JS1Y4UWF4UQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Everingham M, van Gool L, Williams C K I, <i>et al</i>.The pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303-338.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201904036" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201904036&amp;v=MDY0NTVxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtWNzdQSWpYVGJMRzRIOWpNcTQ5R1lvUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

