

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134122758252500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201904035%26RESULT%3d1%26SIGN%3dLVSzHMuZelSxfrgC2Yml%252bktKyNg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201904035&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201904035&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201904035&amp;v=MjA0MDVMRzRIOWpNcTQ5R1lZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtWcjNOSWpYVGI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#57" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#62" data-title="2 光流运动场模型与解耦 ">2 光流运动场模型与解耦</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="&lt;b&gt;2.1 光流运动场模型&lt;/b&gt;"><b>2.1 光流运动场模型</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;2.2 解耦光流运动场模型&lt;/b&gt;"><b>2.2 解耦光流运动场模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="3 解耦光流运动场仿真算法 ">3 解耦光流运动场仿真算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#114" data-title="&lt;b&gt;3.1 算法设计与流程&lt;/b&gt;"><b>3.1 算法设计与流程</b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;3.2 算法仿真结果&lt;/b&gt;"><b>3.2 算法仿真结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#126" data-title="4 真实场景光流矢量及解耦对比分析 ">4 真实场景光流矢量及解耦对比分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#127" data-title="&lt;b&gt;4.1 真实场景光流矢量及解耦&lt;/b&gt;"><b>4.1 真实场景光流矢量及解耦</b></a></li>
                                                <li><a href="#143" data-title="&lt;b&gt;4.2 对比分析&lt;/b&gt;"><b>4.2 对比分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#149" data-title="5 结 论 ">5 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#74" data-title="图1 光流运动场投影模型">图1 光流运动场投影模型</a></li>
                                                <li><a href="#120" data-title="表1 解耦光流运动场的仿真算法">表1 解耦光流运动场的仿真算法</a></li>
                                                <li><a href="#121" data-title="图2 光流运动场模型解耦为六自由度的仿真结果。 (a) 仅&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;x&lt;/i&gt;&lt;/sub&gt;≠0时的右向平移场景; (b) 仅&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;≠0时的向下平移场景; (c) 仅&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;≠0时的前向平移场景; (d) 仅&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;x&lt;/i&gt;&lt;/sub&gt;≠0时的俯仰角旋转场景; (e) 仅&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;≠0时的航向角旋转场景; (f) 仅&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;≠0 时的横滚角旋转场景">图2 光流运动场模型解耦为六自由度的仿真结果。 (a) 仅<i>t</i><sub><i>x</i></sub>≠0时的右向平移场景; (b) 仅<i>t</i><sub><i>y</i></sub>≠0时的向下平移场景; (c) 仅<i>t</i><sub><i>z</i></sub>≠0时的前向平移场景; (d) 仅<i>ω</i><sub><i>x</i></sub>≠0时的俯仰角旋转场景; (e) 仅<i>ω</i><sub><i>y</i></sub>≠0时的航向角旋转场景; (f) 仅<i>ω</i><sub><i>z</i></sub>≠0 时的横滚角旋转场景</a></li>
                                                <li><a href="#124" data-title="图3 将&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;x&lt;/i&gt;&lt;/sub&gt;、&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;、&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;分量分别解耦为&lt;i&gt;u&lt;/i&gt;、&lt;i&gt;v&lt;/i&gt;分量。 (a) &lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;x&lt;/i&gt;&lt;/sub&gt;的&lt;i&gt;u&lt;/i&gt;分量; (b) &lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;x&lt;/i&gt;&lt;/sub&gt;的&lt;i&gt;v&lt;/i&gt;分量; (c) &lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;的&lt;i&gt;u&lt;/i&gt;分量; (d) &lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;的&lt;i&gt;v&lt;/i&gt;分量; (e) &lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;的&lt;i&gt;u&lt;/i&gt;分量; (f) &lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;的&lt;i&gt;v&lt;/i&gt;分量">图3 将<i>t</i><sub><i>x</i></sub>、<i>t</i><sub><i>y</i></sub>、<i>t</i><sub><i>z</i></sub>分量分别解耦为<i>u</i>、<i>v</i>分量。 (a) <i>t</i><sub><i>x</i></sub>的<i>u</i>分量; (b) <i>t</i><sub><i>x</i></sub>的<i>v</i>分量; (c) <i>t</i><sub><i>y</i></sub>的<i>u</i>分量; (d) <i>t</i><sub><i>y</i></sub>的<i>v</i>分量; (e) <i>t</i><sub><i>z</i></sub>的<i>u</i>分量; (f) <i>t</i><sub><i>z</i></sub>的<i>v</i>分量</a></li>
                                                <li><a href="#129" data-title="图4 将&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;x&lt;/i&gt;&lt;/sub&gt;、&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;、&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;分量分别解耦为&lt;i&gt;u&lt;/i&gt;、&lt;i&gt;v&lt;/i&gt;分量。 (a) &lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;x&lt;/i&gt;&lt;/sub&gt;的&lt;i&gt;u&lt;/i&gt;分量; (b) &lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;x&lt;/i&gt;&lt;/sub&gt;的&lt;i&gt;v&lt;/i&gt;分量; (c) &lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;的&lt;i&gt;u&lt;/i&gt;分量; (d) &lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;的&lt;i&gt;v&lt;/i&gt;分量; (e) &lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;的&lt;i&gt;u&lt;/i&gt;分量; (f) &lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;的&lt;i&gt;v&lt;/i&gt;分量">图4 将<i>ω</i><sub><i>x</i></sub>、<i>ω</i><sub><i>y</i></sub>、<i>ω</i><sub><i>z</i></sub>分量分别解耦为<i>u</i>、<i>v</i>分量。 (a) <i>ω</i><sub><i>x</i></sub>的<i>u</i>分量; (b) <i>ω</i><sub><i>x</i></sub>的<i>v</i>分量; (c) <i>ω</i><sub><i>y</i></sub>的<i>u</i>分量; (d) <i>ω</i><sub><i>y</i></sub>的<i>v</i>分量; (e) <i>ω</i><sub><i>z</i></sub>的<i>u</i>分量; (f) <i>ω</i><sub><i>z</i></sub>的<i>v</i>分量</a></li>
                                                <li><a href="#130" data-title="图5 典型运动场景的光流场。 (a) 前向运动场景; (b) 旋转运动场景">图5 典型运动场景的光流场。 (a) 前向运动场景; (b) 旋转运动场景</a></li>
                                                <li><a href="#137" data-title="图6 前向与旋转运动场景光流的平移分量&lt;b&gt;&lt;i&gt;t&lt;/i&gt;&lt;/b&gt;与旋转分量&lt;i&gt;ω&lt;/i&gt;。 (a) 前向运动场景的平移分量&lt;b&gt;&lt;i&gt;t&lt;/i&gt;&lt;/b&gt;; (b) 旋转运动场景的平移分量&lt;b&gt;&lt;i&gt;t&lt;/i&gt;&lt;/b&gt;; (c) 前向运动场景的旋转分量&lt;i&gt;ω&lt;/i&gt;; (d) 旋转运动场景的旋转分量&lt;i&gt;ω&lt;/i&gt;">图6 前向与旋转运动场景光流的平移分量<b><i>t</i></b>与旋转分量<i>ω</i>。 (a) 前向运动场景的平移分量<b><i>t</i></b>; (b) 旋转运动场景的平移分量<b><i>t</i></b>; (c) 前向运动场景的旋转分量<i>ω</i>; (d) 旋转运动场景的旋转分量<i>ω</i></a></li>
                                                <li><a href="#138" data-title="图7 前向与旋转运动场景的&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;x&lt;/i&gt;&lt;/sub&gt;、&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;、&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;分量。 (a) 前向运动场景的&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;x&lt;/i&gt;&lt;/sub&gt;分量; (b) 旋转运动场景的&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;x&lt;/i&gt;&lt;/sub&gt;分量; (c) 前向运动场景的&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;分量; (d) 旋转运动场景的&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;分量; (e) 前向运动场景的&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;分量; (f) 旋转运动场景的&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;分量">图7 前向与旋转运动场景的<i>t</i><sub><i>x</i></sub>、<i>t</i><sub><i>y</i></sub>、<i>t</i><sub><i>z</i></sub>分量。 (a) 前向运动场景的<i>t</i><sub><i>x</i></sub>分量; (b) 旋转运动场景的<i>t</i><sub><i>x</i></sub>分量; (c) 前向运动场景的<i>t</i><sub><i>y</i></sub>分量; (d) 旋转运动场景的<i>t</i><sub><i>y</i></sub>分量; (e) 前向运动场景的<i>t</i><sub><i>z</i></sub>分量; (f) 旋转运动场景的<i>t</i><sub><i>z</i></sub>分量</a></li>
                                                <li><a href="#139" data-title="图8 前向与旋转运动场景的&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;x&lt;/i&gt;&lt;/sub&gt;、&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;、&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;分量。 (a) 前向运动场景的&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;x&lt;/i&gt;&lt;/sub&gt;分量; (b) 旋转运动场景的&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;x&lt;/i&gt;&lt;/sub&gt;分量; (c) 前向运动场景的&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;分量; (d) 旋转运动场景的&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;分量; (e) 前向运动场景的&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;分量; (f) 旋转运动场景的&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;分量">图8 前向与旋转运动场景的<i>ω</i><sub><i>x</i></sub>、<i>ω</i><sub><i>y</i></sub>、<i>ω</i><sub><i>z</i></sub>分量。 (a) 前向运动场景的<i>ω</i><sub><i>x</i></sub>分量; (b) 旋转运动场景的<i>ω</i><sub><i>x</i></sub>分量; (c) 前向运动场景的<i>ω</i><sub><i>y</i></sub>分量; (d) 旋转运动场景的<i>ω</i><sub><i>y</i></sub>分量; (e) 前向运动场景的<i>ω</i><sub><i>z</i></sub>分量; (f) 旋转运动场景的<i>ω</i><sub><i>z</i></sub>分量</a></li>
                                                <li><a href="#140" data-title="图9 前向运动场景中&lt;i&gt;t&lt;/i&gt;&lt;sub&gt;&lt;i&gt;z&lt;/i&gt;&lt;/sub&gt;分量的解耦。 (a) &lt;i&gt;u&lt;/i&gt;分量; (b) &lt;i&gt;v&lt;/i&gt;分量">图9 前向运动场景中<i>t</i><sub><i>z</i></sub>分量的解耦。 (a) <i>u</i>分量; (b) <i>v</i>分量</a></li>
                                                <li><a href="#141" data-title="图10 旋转运动场景中&lt;i&gt;ω&lt;/i&gt;&lt;sub&gt;&lt;i&gt;y&lt;/i&gt;&lt;/sub&gt;分量的解耦。 (a) &lt;i&gt;u&lt;/i&gt;分量; (b) &lt;i&gt;v&lt;/i&gt;分量">图10 旋转运动场景中<i>ω</i><sub><i>y</i></sub>分量的解耦。 (a) <i>u</i>分量; (b) <i>v</i>分量</a></li>
                                                <li><a href="#145" data-title="图11 Buczko等选取的典型场景中的光流解耦结果。 (a) 前向运动场景中的平移分量; (b) 旋转运动场景中的平移分量; (c) 前向运动场景中的旋转分量; (d) 旋转运动场景中的旋转分量">图11 Buczko等选取的典型场景中的光流解耦结果。 (a) 前向运动场景中的平移分量; (b) 旋转运动场景中的平移分量; (c) 前向运动场景中的旋转分量; (d) 旋转运动场景中的旋转分量</a></li>
                                                <li><a href="#146" data-title="图12 Buczko等在普通场景中的对比结果。 (a) 普通场景中的光流矢量; (b) 未解耦的光流矢量; (c) 图12 (a) 中解耦后正确的平移光流分量及其汇聚规律; (d) 图12 (b) 中解耦后正确的平移光流分量及其汇聚规律">图12 Buczko等在普通场景中的对比结果。 (a) 普通场景中的光流矢量; (b) 未解耦的光流矢量; (c) 图12 (a) 中解耦后正确的平移光流分量及其汇聚规律; (d) 图12 (b) 中解耦后正确的平移光流分量及其汇聚规律</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Xiong C Z, Che M Q, Wang R L, &lt;i&gt;et al&lt;/i&gt;.Robust real-time visual tracking via dual model adaptive switching[J].Acta Optica Sinica, 2018, 38 (10) :1015002.熊昌镇, 车满强, 王润玲, 等.稳健的双模型自适应切换实时跟踪算法[J].光学学报, 2018, 38 (10) :1015002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201810031&amp;v=MDA3NjVMT2VaZVZ1RnlIa1ZyM05JalhUYkxHNEg5bk5yNDlHWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Xiong C Z, Che M Q, Wang R L, &lt;i&gt;et al&lt;/i&gt;.Robust real-time visual tracking via dual model adaptive switching[J].Acta Optica Sinica, 2018, 38 (10) :1015002.熊昌镇, 车满强, 王润玲, 等.稳健的双模型自适应切换实时跟踪算法[J].光学学报, 2018, 38 (10) :1015002.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Lin H C, L&#252; Q, Wei H, &lt;i&gt;et al&lt;/i&gt;.Quadrotor autonomous flight and three-dimensional dense reconstruction based on VI-SLAM[J].Acta Optica Sinica, 2018, 38 (7) :0715004.林辉灿, 吕强, 卫恒, 等.基于VI-SLAM的四旋翼自主飞行与三维稠密重构[J].光学学报, 2018, 38 (7) :0715004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201807028&amp;v=MDIwNzh0R0ZyQ1VSTE9lWmVWdUZ5SGtWcjNOSWpYVGJMRzRIOW5NcUk5SGJJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Lin H C, L&#252; Q, Wei H, &lt;i&gt;et al&lt;/i&gt;.Quadrotor autonomous flight and three-dimensional dense reconstruction based on VI-SLAM[J].Acta Optica Sinica, 2018, 38 (7) :0715004.林辉灿, 吕强, 卫恒, 等.基于VI-SLAM的四旋翼自主飞行与三维稠密重构[J].光学学报, 2018, 38 (7) :0715004.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" Xiao J S, Tian H, Zou W T, &lt;i&gt;et al&lt;/i&gt;.Stereo matching based on convolutional neural network[J].Acta Optica Sinica, 2018, 38 (8) :0815017.肖进胜, 田红, 邹文涛, 等.基于深度卷积神经网络的双目立体视觉匹配算法[J].光学学报, 2018, 38 (8) :0815017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201808018&amp;v=MjU3MjZPZVplVnVGeUhrVnIzTklqWFRiTEc0SDluTXA0OUViSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Xiao J S, Tian H, Zou W T, &lt;i&gt;et al&lt;/i&gt;.Stereo matching based on convolutional neural network[J].Acta Optica Sinica, 2018, 38 (8) :0815017.肖进胜, 田红, 邹文涛, 等.基于深度卷积神经网络的双目立体视觉匹配算法[J].光学学报, 2018, 38 (8) :0815017.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Gibson J J.The perception of the visual world[M].Cambridge:The Riverside Press, 1950." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The perception of the visual world">
                                        <b>[4]</b>
                                         Gibson J J.The perception of the visual world[M].Cambridge:The Riverside Press, 1950.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" Loianno G, Scaramuzza D, Kumar V.Special issue on high-speed vision-based autonomous navigation of UAVs[J].Journal of Field Robotics, 2018, 35 (1) :3-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Special Issue on High‐Speed Vision‐Based Autonomous Navigation of UAVs">
                                        <b>[5]</b>
                                         Loianno G, Scaramuzza D, Kumar V.Special issue on high-speed vision-based autonomous navigation of UAVs[J].Journal of Field Robotics, 2018, 35 (1) :3-4.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Meneses M C, Matos L N, Oprado B.Low-cost autonomous navigation system based on optical-flow classification[EB/OL]. (2018-03-11) [2018-10-15].https://arxiv.org/abs/1803.03966." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Low-cost autonomous navigation system based on optical-flow classification">
                                        <b>[6]</b>
                                         Meneses M C, Matos L N, Oprado B.Low-cost autonomous navigation system based on optical-flow classification[EB/OL]. (2018-03-11) [2018-10-15].https://arxiv.org/abs/1803.03966.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" Wan F H.Research on multi-sensor based UAV obstacle avoidance technique[D].Hangzhou:Zhejiang University of Technology, 2017.万富华.基于多传感器的无人机定位和避障技术研究[D].杭州:浙江工业大学, 2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017249632.nh&amp;v=MjkxOTVxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtWcjNOVkYyNkdiRzhGOWZQclpFYlBJUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Wan F H.Research on multi-sensor based UAV obstacle avoidance technique[D].Hangzhou:Zhejiang University of Technology, 2017.万富华.基于多传感器的无人机定位和避障技术研究[D].杭州:浙江工业大学, 2017.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" Dai B X.Research on obstacle avoidance method of MAV in indoor environment based on optical flow[D].Chengdu:University of Electronic Science and Technology of China, 2015.戴碧霞.基于光流的微小型飞行器室内避障方法研究[D].成都:电子科技大学, 2015." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1015708658.nh&amp;v=Mjg1NTVrVnIzTlZGMjZHN1M0RnRmSnA1RWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Dai B X.Research on obstacle avoidance method of MAV in indoor environment based on optical flow[D].Chengdu:University of Electronic Science and Technology of China, 2015.戴碧霞.基于光流的微小型飞行器室内避障方法研究[D].成都:电子科技大学, 2015.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" Forster C, Zhang Z C, Gassner M, &lt;i&gt;et al&lt;/i&gt;.SVO:Semidirect visual odometry for monocular and multicamera systems[J].IEEE Transactions on Robotics, 2017, 33 (2) :249-265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SVO:Semidirect Visual Odometry for Monocular and Multicamera Systems">
                                        <b>[9]</b>
                                         Forster C, Zhang Z C, Gassner M, &lt;i&gt;et al&lt;/i&gt;.SVO:Semidirect visual odometry for monocular and multicamera systems[J].IEEE Transactions on Robotics, 2017, 33 (2) :249-265.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" Engel J, Koltun V, Cremers D.Direct sparse odometry[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (3) :611-625." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Direct sparse odometry&amp;quot;">
                                        <b>[10]</b>
                                         Engel J, Koltun V, Cremers D.Direct sparse odometry[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (3) :611-625.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" Longuet-Higgins H C, Prazdny K.The interpretation of a moving retinal image[J].Proceedingsof the Royal Society of London B, 1980, 208 (1173) :385-397." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJRS&amp;filename=SJRS121022017257&amp;v=MDY2NjJLNkg5SE9yWTlFWStrS0N4TTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkNybFU3YkpKVndSTmlmWmZi&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Longuet-Higgins H C, Prazdny K.The interpretation of a moving retinal image[J].Proceedingsof the Royal Society of London B, 1980, 208 (1173) :385-397.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" Matthies L, Szeliski R, Kanade T.Kalman filter-based algorithms for estimating depth from image sequences[M].Heidelberg:Springer, 1993:87-130." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Kalman filter-based algorithms for estimating depth from image sequences">
                                        <b>[12]</b>
                                         Matthies L, Szeliski R, Kanade T.Kalman filter-based algorithms for estimating depth from image sequences[M].Heidelberg:Springer, 1993:87-130.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" de Luca A, Oriolo G, Robuffo Giordano P.Feature depth observation for image-based visual servoing:theory and experiments[J].The International Journal of Robotics Research, 2008, 27 (10) :1093-1116." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature depth observation for image-based visual servoing: Theory and experiments">
                                        <b>[13]</b>
                                         de Luca A, Oriolo G, Robuffo Giordano P.Feature depth observation for image-based visual servoing:theory and experiments[J].The International Journal of Robotics Research, 2008, 27 (10) :1093-1116.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" Sabatini S, Corno M, Fiorenti S, &lt;i&gt;et al&lt;/i&gt;.Vision-based pole-like obstacle detection and localization for urban mobile robots[C]∥Proceedings of the 29th IEEE Intelligent Vehicles Symposium, June 26-30, 2018, Changshu, China.New York:IEEE, 2018:1209-1214." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vision-based pole-like obstacle detection and localization for urban mobile robots">
                                        <b>[14]</b>
                                         Sabatini S, Corno M, Fiorenti S, &lt;i&gt;et al&lt;/i&gt;.Vision-based pole-like obstacle detection and localization for urban mobile robots[C]∥Proceedings of the 29th IEEE Intelligent Vehicles Symposium, June 26-30, 2018, Changshu, China.New York:IEEE, 2018:1209-1214.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" Buczko M, Willert V.Flow-decoupled normalized reprojection error for visual odometry[C]//Proceedings of IEEE 19th International Conference on Intelligent Transportation Systems, November 1-4, 2016, Rio de Janeiro, Brazil.New York:IEEE, 2016:1161-1167." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Flow-decoupled normalized reprojection error for visual odometry">
                                        <b>[15]</b>
                                         Buczko M, Willert V.Flow-decoupled normalized reprojection error for visual odometry[C]//Proceedings of IEEE 19th International Conference on Intelligent Transportation Systems, November 1-4, 2016, Rio de Janeiro, Brazil.New York:IEEE, 2016:1161-1167.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" Jaegle A, Phillips S, Daniilidis K.Fast, robust, continuous monocular egomotion computation[C]∥Proceedings of IEEE International Conference on Robotics and Automation, May 16-21, 2016, Stockholm, Sweden.New York:IEEE, 2016:773-780." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast,robust,continuous monocular egomotion computation">
                                        <b>[16]</b>
                                         Jaegle A, Phillips S, Daniilidis K.Fast, robust, continuous monocular egomotion computation[C]∥Proceedings of IEEE International Conference on Robotics and Automation, May 16-21, 2016, Stockholm, Sweden.New York:IEEE, 2016:773-780.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_17" title=" Wu Z L, Li J, Guan Z Y, &lt;i&gt;et al&lt;/i&gt;.Optical flow-based autonomous landing control for fixed-wing small UAV[J].Systems Engineering and Electronics, 2016, 38 (12) :2827-2834.吴政隆, 李杰, 关震宇, 等.基于光流的固定翼小型无人机自主着陆控制[J].系统工程与电子技术, 2016, 38 (12) :2827-2834." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYD201612022&amp;v=MTI2MjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIa1ZyM05QVG5TYXJHNEg5Zk5yWTlIWm9RS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         Wu Z L, Li J, Guan Z Y, &lt;i&gt;et al&lt;/i&gt;.Optical flow-based autonomous landing control for fixed-wing small UAV[J].Systems Engineering and Electronics, 2016, 38 (12) :2827-2834.吴政隆, 李杰, 关震宇, 等.基于光流的固定翼小型无人机自主着陆控制[J].系统工程与电子技术, 2016, 38 (12) :2827-2834.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_18" title=" Guo L, Liu M Y, Wang Y, &lt;i&gt;et al&lt;/i&gt;.A motion estimation method for optical flow detection device of aircraft with two cameras:CN104880187A[P/OL]. (2015-09-02) [2018-10-15].https://patentimages.storage.googleapis.com/79/9a/cf/6a6371ddd46e75/CN104880187A.pdf.郭雷, 刘梦瑶, 王岩, 等.一种双摄像机的飞行器光流检测装置的运动估计方法:CN104880187A[P/OL]. (2015-09-02) [2018-10-15].https://patentimages.storage.googleapis.com/79/9a/cf/6a6371ddd46e75/CN104880187A.pdf." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SCPD&amp;filename=CN104880187A&amp;v=MjY2NzFPUzNmbXBXRmFlN0tWVEx1ZFpPZHVIaW5nVWJvPUppTzZIcmV3RnRITnA0ZzBDKzRQRDMxTHh4WVQ2em8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Guo L, Liu M Y, Wang Y, &lt;i&gt;et al&lt;/i&gt;.A motion estimation method for optical flow detection device of aircraft with two cameras:CN104880187A[P/OL]. (2015-09-02) [2018-10-15].https://patentimages.storage.googleapis.com/79/9a/cf/6a6371ddd46e75/CN104880187A.pdf.郭雷, 刘梦瑶, 王岩, 等.一种双摄像机的飞行器光流检测装置的运动估计方法:CN104880187A[P/OL]. (2015-09-02) [2018-10-15].https://patentimages.storage.googleapis.com/79/9a/cf/6a6371ddd46e75/CN104880187A.pdf.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_19" title=" Geiger A, Lenz P, Urtasun R.The KITTI vision benchmark suite[EB/OL]. (2018-09-15) [2018-10-07].http://www.cvlibs.net/datasets/kitti/eval_odometry.php." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The KITTI Vision Benchmark Suite">
                                        <b>[19]</b>
                                         Geiger A, Lenz P, Urtasun R.The KITTI vision benchmark suite[EB/OL]. (2018-09-15) [2018-10-07].http://www.cvlibs.net/datasets/kitti/eval_odometry.php.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_20" title=" Buczko M, Willert V.Monocular outlier detection for visual odometry[C]//Proceedings of IEEE Intelligent Vehicles Symposium, June 11-14, 2017, Los Angeles, CA, USA.New York:IEEE, 2017:1124-1131." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Monocular outlier detection for visual odometry">
                                        <b>[20]</b>
                                         Buczko M, Willert V.Monocular outlier detection for visual odometry[C]//Proceedings of IEEE Intelligent Vehicles Symposium, June 11-14, 2017, Los Angeles, CA, USA.New York:IEEE, 2017:1124-1131.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_21" title=" Zhang Y J.A course of computer vision[M].Beijing:Posts &amp;amp; Telecom Press, 2011.章毓晋.计算机视觉教程[M].北京:人民邮电出版社, 2011." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787115242907001&amp;v=MTkwODE5UElyWVpGWStzUERSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkNybFU3YkpKVndSWEZxekdiSzVH&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         Zhang Y J.A course of computer vision[M].Beijing:Posts &amp;amp; Telecom Press, 2011.章毓晋.计算机视觉教程[M].北京:人民邮电出版社, 2011.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_22" title=" Jitendra M.Dynamic perspective[EB/OL]. (2015-05-02) [2018-10-07].http://www-inst.eecs.berkeley.edu/～cs280/sp15/lectures/4.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dynamic perspective">
                                        <b>[22]</b>
                                         Jitendra M.Dynamic perspective[EB/OL]. (2015-05-02) [2018-10-07].http://www-inst.eecs.berkeley.edu/～cs280/sp15/lectures/4.pdf.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_23" title=" Geiger A, Lenz P, Urtasun R.Are we ready for autonomous driving?The KITTI vision benchmark suite[C]∥Proceedings of, June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:3354-3361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Are we ready for autonomous driving?The KITTI vision benchmark suite">
                                        <b>[23]</b>
                                         Geiger A, Lenz P, Urtasun R.Are we ready for autonomous driving?The KITTI vision benchmark suite[C]∥Proceedings of, June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:3354-3361.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-25 07:03</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(04),295-306 DOI:10.3788/AOS201939.0415005            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>解耦光流运动场模型的车载平台仿真</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B9%8C%E8%90%8C&amp;code=36159191&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">乌萌</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%9D%E9%87%91%E6%98%8E&amp;code=20552045&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郝金明</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%98%E6%B5%A9&amp;code=23537926&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">付浩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E6%89%AC&amp;code=08513573&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高扬</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E5%9C%B0%E7%90%86%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0199248&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息工程大学地理空间信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0131836&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">地理信息工程国家重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%AE%89%E6%B5%8B%E7%BB%98%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0269230&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西安测绘研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E6%99%BA%E8%83%BD%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">国防科技大学智能科学学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对无人车平台利用光流进行载体位姿估计时面临的不同运动状态光流矢量解耦与分析问题, 推导了光流运动场模型, 分析了载体在六自由度位姿独立变化时的解耦光流运动场模型;根据解耦光流运动场模型, 设计了车载平台的仿真算法, 并给出完全解耦的仿真结果;利用解耦光流运动场模型量化分析了仿真结果的正确性;利用KITTI数据集的平移、旋转两个典型场景开展真实光流解耦实验, 进行了模型分析、仿真过程、真实数据、对比结果的一致性验证。结果表明:所给出的解耦模型分析、仿真算法、仿真与真实结果以及对比分析不仅可用于车载平台利用光流开展位姿解耦估计中的误差分析和算法验证, 还对深入理解光流运动成像、开展无人车平台光流应用的研究具一定的借鉴和指导。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%89%E6%B5%81%E8%BF%90%E5%8A%A8%E5%9C%BA%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">光流运动场模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%A3%E8%80%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">解耦;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%A0%E4%BA%BA%E8%BD%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">无人车;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%89%E6%B5%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">光流;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BB%BF%E7%9C%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">仿真;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *乌萌, E-mail:wumeng19nudt@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (65103400);</span>
                    </p>
            </div>
                    <h1><b>Simulation on Vehicle Platform Based on Decoupled Optical Flow Motion Field Model</b></h1>
                    <h2>
                    <span>Wu Meng</span>
                    <span>Hao Jinming</span>
                    <span>Fu Hao</span>
                    <span>Gao Yang</span>
            </h2>
                    <h2>
                    <span>Institute of Geospatial Information, Information Engineering University</span>
                    <span>State Key Laboratory of Geo-Information Engineering</span>
                    <span>Xi'an Research Institute of Surveying and Mapping</span>
                    <span>Institute of Intelligent Science, National University of Defense Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problems of decoupling and analysis of optical flow vectors with various motion states confronted in the pose estimation process of autonomous vehicles (AVs) using optical flow, the optical flow motion field model (OFMFM) is derived and the decoupled optical flow motion filed model (DOFMFM) is analysed as the vehicle poses change independently in six degrees of freedom. According to the DOFMFM, a simulation algorithm is designed for the vehicle platform, and the completely decoupled simulation results are presented. The DOFMFM is applied to quantify and verify the simulation results. Two real scenes of translation and rotation from the KITTI dataset are utilized for the flow-decoupled experiments. The consistency among model analysis, simulation process, real data and comparison results is verified. The results show that the proposed decoupled model analysis, simulation algorithm, simulation and real results together with comparison analysis can not only be applied in the error analysis and algorithm test of pose estimation, but also provide a reference or instruction for the improvement in the understanding of optical flow motion imaging and the research on the optical flow based applications on an AV platform.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=optical%20flow%20motion%20field%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">optical flow motion field model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=decoupling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">decoupling;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=autonomous%20vehicle&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">autonomous vehicle;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=optical%20flow&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">optical flow;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=simulation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">simulation;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-15</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="57" name="57" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="58">随着近年来计算机视觉和人工智能技术的快速发展, 利用视觉传感器进行无人平台导航的应用愈加广泛<citation id="152" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。图像特征提取 (特征法) 和图像光流跟踪 (光流法) 是视觉传感器获取空间信息的两种重要手段, 其中具有代表性的光流法源自Gibson等<citation id="151" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>于1950年提出的光流的概念。眼睛在持续观察周围环境时, 景象在视网膜上生成了一系列不断变化的图像, 像场景信息连续不断地“流过”视网膜, 恰似“光”的“流动”, 因而取名为“光流”。光流反映了场景中瞬时相对运动引起的灰度变化, 是三维环境速度场的二维投影, 场景包含了丰富的运动信息和结构信息, 因此常用于解决场景中的运动目标检测、障碍检测、导航等问题<citation id="153" type="reference"><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。利用光流进行自主导航应用的无人平台包括无人机 (UAV) 平台和无人车 (AV) 平台。无人机平台采用多种传感器组合导航, 部分平台还同时安装开源光流传感器, 用于获得无人机在左、右、前、后方向的移动速度, 积分后可获得移动距离<citation id="154" type="reference"><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>。对于无人车平台, 通常在视觉里程计 (VO) 算法或同时定位与建图 (VSLAM) 算法中应用光流算法<citation id="155" type="reference"><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>。无人机平台与无人车平台视觉位姿估计的一个显著区别在于, 无人机成像平面距离地面点的垂直高度为图像的深度信息, 无人车前方目标距离成像平面的垂直距离为图像的深度信息。</p>
                </div>
                <div class="p1">
                    <p id="59">1980年, Longuet-Higgins等<citation id="156" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>推导了光流运动场模型, 此后该模型多被用于序列场景的深度估计<citation id="162" type="reference"><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>, 近年来逐渐被应用于多传感器融合的载体位姿估计<citation id="163" type="reference"><link href="37" rel="bibliography" /><link href="39" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>。光流运动场的解耦是近年来为了进一步分离和精化载体位姿测量中旋转角度分量与平移位移分量的估计而提出的<citation id="164" type="reference"><link href="39" rel="bibliography" /><link href="41" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>。在无人机应用中, 吴政隆等<citation id="157" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>将无人机着陆运动解耦为横、纵向分量, 利用跑道线特征计算稀疏直线光流场, 将跑道线水平分量光流作为控制反馈, 在仿真环境下实现了飞行器的着陆控制。郭雷等<citation id="158" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>利用无人机的前视、下视双摄像机进行光流检测和运动估计, 根据前视图光流平移分量分布和光流解耦来计算无人机的角速度, 并更新载体姿态, 利用角速度和特征光流获得下视图的平移光流, 利用姿态角和深度信息建立平移运动的估计方程, 通过滤波求解飞行速度, 最终得到飞行器六自由度运动的估计结果。在无人车的应用中, Buczko等<citation id="159" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>利用重投影误差最小化函数分析位姿误差对最小化估计结果的影响, 将包含误差的平移分量和旋转分量进行解耦, 降低了平移分量误差对位姿估计的影响, 在KITTI数据集视觉基准平台测试评估精度的排名中靠前<citation id="160" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。Jaegle等<citation id="161" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>通过将光流运动场模型向平移分量矩阵的零空间进行投影, 将六自由度的位姿估计解耦为仅依赖于平移分量变化的最小二乘代价函数, 最后利用期望残差似然估计获取最优平移分量的估计结果, 再将估计结果代入到光流运动场模型中解算姿态的估计结果, 从而较好地解决了无人车单目视觉里程计的相对位姿估计。</p>
                </div>
                <div class="p1">
                    <p id="60">已有的研究中包括大量利用光流进行运动检测、解耦分析、姿态估计、运动控制的应用, 但对无人机平台和无人车平台的光流场进行不同自由度解耦和仿真分析的文献很少。Buczko等<citation id="165" type="reference"><link href="39" rel="bibliography" /><link href="49" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">20</a>]</sup></citation>对光流平移分量和旋转分量的解耦进行了定性分析, 并对平移、旋转解耦前后光流矢量延长后的分布规律进行了定性探讨, 但其模型分析主要是针对像素和亚像素级特征光流检测误差开展的, 最终对解耦光流分量进行阈值外点剔除, 从而提升了载体位置姿态的估计精度。</p>
                </div>
                <div class="p1">
                    <p id="61">事实上, 在对无人车平台光流进行位姿估计时, 除需计算真实场景的光流矢量场外, 光流运动场模型的仿真与分析也是位姿估计研究的必要过程。这是由于解耦为不同自由度的光流分量直接反映了载体在该自由度的运动状态, 可以作为载体位姿解耦估计的直接依据, 因此有必要开展深入探讨和分析。本文延续了解耦光流运动场模型这一思路, 但与已有的方法不同, 本文对光流矢量的解耦运动状态进行模型推导, 并对对应的矢量场进行分析, 以揭示光流矢量、光流分量与载体位姿间的变化规律;然后对车载平台的光流运动场模型进行运动状态仿真和解耦仿真, 给出了一个较为完善的光流运动场仿真算法;最后通过仿真结果、真实场景数据的光流矢量解耦结果, 以及其他的光流解耦仿真结果, 分别验证了模型推导和仿真算法的正确性, 为进一步研究车载平台位姿估计的误差消减算法奠定了理论和实验基础。</p>
                </div>
                <h3 id="62" name="62" class="anchor-tag">2 光流运动场模型与解耦</h3>
                <h4 class="anchor-tag" id="63" name="63"><b>2.1 光流运动场模型</b></h4>
                <div class="p1">
                    <p id="64">当载体与场景存在相对运动时, 移动的场景目标的表面光学特征投影到固联于载体的相机图像平面后形成光流。光流表达了图像的变化, 包含目标的运动信息, 可用来确定观察者相对目标的运动情况。光流的形成需要3个要素<citation id="166" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, 即运动场或速度场、带光学特征的部位、从场景到图像的投影。光流的经典模型是在亮度恒定和速度平滑这两个假设的基础上进行计算的<citation id="167" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, 这里不再赘述。以下重点探讨光流场与载体运动之间的关系模型。</p>
                </div>
                <div class="p1">
                    <p id="65">在推导光流矢量与载体位置姿态变化之间的关系时, 设相机坐标系下的物方点<i>P</i> (<i>X</i>, <i>Y</i>, <i>Z</i>) 经小孔成像后投影于相机平面点 (<i>x</i>, <i>y</i>) 。当相机在世界坐标系下以平移速度<b><i>t</i></b>= (<i>t</i><sub><i>x</i></sub>, <i>t</i><sub><i>y</i></sub>, <i>t</i><sub><i>z</i></sub>) <sup>T</sup>和角速度<i>ω</i>= (<i>ω</i><sub><i>x</i></sub>, <i>ω</i><sub><i>y</i></sub>, <i>ω</i><sub><i>z</i></sub>) <sup>T</sup>移动时, 点<i>P</i> (<i>X</i>, <i>Y</i>, <i>Z</i>) 的运动可以描述为</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˙</mo></mover><mo>=</mo><mo>-</mo><mi mathvariant="bold-italic">t</mi><mo>-</mo><mi mathvariant="bold-italic">ω</mi><mo>×</mo><mi mathvariant="bold-italic">X</mi><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">式中:<b><i>X</i></b>为点<i>P</i>的三维物方坐标;×为外积运算符号。将 (1) 式展开为</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mover accent="true"><mi>X</mi><mo>˙</mo></mover></mtd></mtr><mtr><mtd><mover accent="true"><mi>Y</mi><mo>˙</mo></mover></mtd></mtr><mtr><mtd><mover accent="true"><mi>Ζ</mi><mo>˙</mo></mover></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>=</mo><mo>-</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>t</mi><msub><mrow></mrow><mi>x</mi></msub></mtd></mtr><mtr><mtd><mi>t</mi><msub><mrow></mrow><mi>y</mi></msub></mtd></mtr><mtr><mtd><mi>t</mi><msub><mrow></mrow><mi>z</mi></msub></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>-</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mi>y</mi></msub><mi>Ζ</mi><mo>-</mo><mi>ω</mi><msub><mrow></mrow><mi>z</mi></msub><mi>Y</mi></mtd></mtr><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mi>z</mi></msub><mi>X</mi><mo>-</mo><mi>ω</mi><msub><mrow></mrow><mi>x</mi></msub><mi>Ζ</mi></mtd></mtr><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mi>x</mi></msub><mi>Y</mi><mo>-</mo><mi>ω</mi><msub><mrow></mrow><mi>y</mi></msub><mi>X</mi></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">依据小孔成像关系<i>x</i>=<i>fX</i>/<i>Z</i>与<i>y</i>=<i>fY</i>/<i>Z</i> (其中<i>f</i>为成像焦距) 对小孔成像关系式两边关于时间求导数, 则有</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mover accent="true"><mi>x</mi><mo>˙</mo></mover><mo>=</mo><mi>f</mi><mfrac><mrow><mover accent="true"><mi>X</mi><mo>˙</mo></mover><mi>Ζ</mi><mo>-</mo><mover accent="true"><mi>Ζ</mi><mo>˙</mo></mover><mi>X</mi></mrow><mrow><mi>Ζ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mtd></mtr><mtr><mtd><mover accent="true"><mi>y</mi><mo>˙</mo></mover><mo>=</mo><mi>f</mi><mfrac><mrow><mover accent="true"><mi>Y</mi><mo>˙</mo></mover><mi>Ζ</mi><mo>-</mo><mover accent="true"><mi>Ζ</mi><mo>˙</mo></mover><mi>Y</mi></mrow><mrow><mi>Ζ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">将 (2) 式代入 (3) 式, 整理后可得光流运动场模型<citation id="168" type="reference"><link href="37" rel="bibliography" /><link href="53" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">22</a>]</sup></citation>, 即</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>u</mi></mtd></mtr><mtr><mtd><mi>v</mi></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>=</mo><mi>f</mi><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mover accent="true"><mi>x</mi><mo>˙</mo></mover></mtd></mtr><mtr><mtd><mover accent="true"><mi>y</mi><mo>˙</mo></mover></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mo>-</mo><mi>f</mi><mo>/</mo><mi>Ζ</mi></mtd><mtd><mn>0</mn></mtd><mtd><mi>x</mi><mo>/</mo><mi>Ζ</mi></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mo>-</mo><mi>f</mi><mo>/</mo><mi>Ζ</mi></mtd><mtd><mi>y</mi><mo>/</mo><mi>Ζ</mi></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>t</mi><msub><mrow></mrow><mi>x</mi></msub></mtd></mtr><mtr><mtd><mi>t</mi><msub><mrow></mrow><mi>y</mi></msub></mtd></mtr><mtr><mtd><mi>t</mi><msub><mrow></mrow><mi>z</mi></msub></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>+</mo></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>x</mi><mi>y</mi><mo>/</mo><mi>f</mi></mtd><mtd><mo>-</mo><mo stretchy="false"> (</mo><mi>f</mi><mo>+</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mi>f</mi><mo stretchy="false">) </mo></mtd><mtd><mi>y</mi></mtd></mtr><mtr><mtd><mi>f</mi><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mi>f</mi></mtd><mtd><mo>-</mo><mi>x</mi><mi>y</mi><mo>/</mo><mi>f</mi></mtd><mtd><mo>-</mo><mi>x</mi></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mi>x</mi></msub></mtd></mtr><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mi>y</mi></msub></mtd></mtr><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mi>z</mi></msub></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">式中:<i>u</i>、<i>v</i>分别为光流矢量在图像平面上的水平、竖直方向分量。模型中变量的物理意义如图1所示, 其中<i>X</i><sub>w</sub>、<i>Y</i><sub>w</sub>、<i>Z</i><sub>w</sub>为世界坐标系坐标。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904035_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 光流运动场投影模型" src="Detail/GetImg?filename=images/GXXB201904035_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 光流运动场投影模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904035_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Projection model of optical flow motion field</p>

                </div>
                <div class="p1">
                    <p id="75">为了便于计算, 本研究中的坐标系与后续实际场景数据常用的公开数据集KITTI中的坐标系定义<citation id="169" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>一致, 角速度旋转正方向定义为右手系。当场景与相机间发生相对运动时, <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>x</mi><mo>˙</mo></mover><mo>, </mo><mover accent="true"><mi>y</mi><mo>˙</mo></mover><mo stretchy="false">) </mo></mrow></math></mathml>即为P点对应相机坐标系中点的速度, (u, v) 为点P投影到图像的帧间像素坐标变化量, 即光流矢量。因此, (4) 式中第1个等号左、右侧的表达式均为图像中的光流矢量, 第2个等号右侧的表达式为光流的平移分量与旋转分量之和, 它描述了相机坐标系下投影点的运动与场景深度Z、相机沿世界坐标系三轴平移变化量<b><i>t</i></b>、相机绕世界坐标系3个轴的旋转变化量<i>ω</i>之间的关系。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>2.2 解耦光流运动场模型</b></h4>
                <div class="p1">
                    <p id="78">为了进一步分析车辆载体六自由度位置姿态中单个位姿变量变化对光流矢量的影响, 分别将平移速度<b><i>t</i></b>= (<i>t</i><sub><i>x</i></sub>, <i>t</i><sub><i>y</i></sub>, <i>t</i><sub><i>z</i></sub>) <sup>T</sup>和角速度<i>ω</i>= (<i>ω</i><sub><i>x</i></sub>, <i>ω</i><sub><i>y</i></sub>, <i>ω</i><sub><i>z</i></sub>) <sup>T</sup>中单个分量的取值保留为非零, 其他取零值, 则 (4) 式可以写成以下6种情况:</p>
                </div>
                <div class="p1">
                    <p id="79">1) 仅水平平移速度<i>t</i><sub><i>x</i></sub>≠0时, (4) 式可写成</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>u</mi></mtd></mtr><mtr><mtd><mi>v</mi></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>=</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mo>-</mo><mi>f</mi><mo>/</mo><mi>Ζ</mi></mtd></mtr><mtr><mtd><mn>0</mn></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mi>t</mi><msub><mrow></mrow><mi>x</mi></msub><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">因此, 光流矢量的模为<image id="181" type="formula" href="images/GXXB201904035_18100.jpg" display="inline" placement="inline"><alt></alt></image>, 光流矢量的方向为<i>θ</i>=arctan (<i>v</i>/<i>u</i>) =0或π, 光流矢量方向与<i>t</i><sub><i>x</i></sub>在<i>u</i>轴上的投影方向相反 (即当<i>t</i><sub><i>x</i></sub>&gt;0时, <i>θ</i>=π;当<i>t</i><sub><i>x</i></sub>&lt;0时, <i>θ</i>=0) 。若进一步解耦到<i>u</i>、<i>v</i>分量, 则由 (5) 式可知, <i>u</i>分量与解耦前的光流矢量相同, <i>v</i>分量为0。</p>
                </div>
                <div class="p1">
                    <p id="84">2) 仅竖直平移速度<i>t</i><sub><i>y</i></sub>≠0时, (4) 式可写成</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>u</mi></mtd></mtr><mtr><mtd><mi>v</mi></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>=</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mo>-</mo><mi>f</mi><mo>/</mo><mi>Ζ</mi></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mi>t</mi><msub><mrow></mrow><mi>y</mi></msub><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">因此, 光流矢量的模为<mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mo>=</mo><mrow><mo>|</mo><mrow><mi>f</mi><mi>t</mi><msub><mrow></mrow><mi>y</mi></msub><mo>/</mo><mi>Ζ</mi></mrow><mo>|</mo></mrow></mrow></math></mathml>, 光流矢量方向为<i>θ</i>=±π/2, 光流矢量的方向与<i>t</i><sub><i>y</i></sub>在<i>v</i>轴上的投影方向相反 (即当<i>t</i><sub><i>y</i></sub>&gt;0时, <i>θ</i>=-π/2;当<i>t</i><sub><i>y</i></sub>&lt;0时, <i>θ</i>=π/2) 。若进一步解耦到<i>u</i>、<i>v</i>分量, 则由 (6) 式可知, <i>v</i>分量与解耦前光流矢量相同, <i>u</i>分量为0。</p>
                </div>
                <div class="p1">
                    <p id="88">3) 仅前向平移速度<i>t</i><sub><i>z</i></sub>≠0时, (4) 式可写成</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>u</mi></mtd></mtr><mtr><mtd><mi>v</mi></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>=</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>x</mi><mo>/</mo><mi>Ζ</mi></mtd></mtr><mtr><mtd><mi>y</mi><mo>/</mo><mi>Ζ</mi></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mi>t</mi><msub><mrow></mrow><mi>z</mi></msub><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">因此, 光流矢量的模为<mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mo>=</mo><msqrt><mrow><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mrow><mo>|</mo><mrow><mi>t</mi><msub><mrow></mrow><mi>z</mi></msub><mo>/</mo><mi>Ζ</mi></mrow><mo>|</mo></mrow></mrow></math></mathml>, 光流矢量的方向为<i>θ</i>=arctan (<i>y</i>/<i>x</i>) , 此时光流矢量方向随光流点所在相机坐标位置的不同而不同。为了便于分析解耦模型以及对比仿真、真实光流解耦实验结果, 将 (7) 式进一步解耦到<i>u</i>、<i>v</i>分量, 可得<i>u</i>分量光流的模为<mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><msub><mrow></mrow><mi>u</mi></msub><mo>=</mo><mrow><mo>|</mo><mrow><mi>x</mi><mi>t</mi><msub><mrow></mrow><mi>z</mi></msub><mo>/</mo><mi>Ζ</mi></mrow><mo>|</mo></mrow><mo>, </mo><mi>u</mi></mrow></math></mathml>分量光流的方向由xt<sub>z</sub>的符号决定, 指向u轴的正向或负向;v分量光流的模为<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><msub><mrow></mrow><mi>v</mi></msub><mo>=</mo><mrow><mo>|</mo><mrow><mi>y</mi><mi>t</mi><msub><mrow></mrow><mi>z</mi></msub><mo>/</mo><mi>Ζ</mi></mrow><mo>|</mo></mrow><mo>, </mo><mi>v</mi></mrow></math></mathml>分量光流的方向由yt<sub>z</sub>的符号决定, 指向v轴的正向或负向。</p>
                </div>
                <div class="p1">
                    <p id="94">4) 仅绕水平轴旋转的角速度ω<sub>x</sub>≠0时, (4) 式可写成</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>u</mi></mtd></mtr><mtr><mtd><mi>v</mi></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>=</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>x</mi><mi>y</mi></mtd></mtr><mtr><mtd><mi>f</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mi>ω</mi><msub><mrow></mrow><mi>x</mi></msub><mo>/</mo><mi>f</mi><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">此时光流的模为<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mo>=</mo><msqrt><mrow><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo stretchy="false"> (</mo><mi>f</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mrow><mo>|</mo><mrow><mi>ω</mi><msub><mrow></mrow><mi>x</mi></msub><mo>/</mo><mi>f</mi></mrow><mo>|</mo></mrow></mrow></math></mathml>, 光流矢量的方向为<i>θ</i>=arctan[ (<i>f</i><sup>2</sup>+<i>y</i><sup>2</sup>) / (<i>xy</i>) ], 这2个函数表达式不便于直接看出光流分量的变化规律, 可进一步解耦为<i>u</i>、<i>v</i>分量进行考察。因此, 由 (8) 式可得:<i>u</i>分量光流的模为<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><msub><mrow></mrow><mi>u</mi></msub><mo>=</mo><mrow><mo>|</mo><mrow><mi>x</mi><mi>y</mi><mi>ω</mi><msub><mrow></mrow><mi>x</mi></msub><mo>/</mo><mi>f</mi></mrow><mo>|</mo></mrow><mo>, </mo><mi>u</mi></mrow></math></mathml>分量光流的方向由xyω<sub>x</sub>的符号决定, 指向u轴的正向或负向;v分量光流的模为<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><msub><mrow></mrow><mi>v</mi></msub><mo>=</mo><mo stretchy="false"> (</mo><mi>f</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mrow><mo>|</mo><mrow><mi>ω</mi><msub><mrow></mrow><mi>x</mi></msub><mo>/</mo><mi>f</mi></mrow><mo>|</mo></mrow><mo>, </mo><mi>v</mi></mrow></math></mathml>分量光流的方向与ω<sub>x</sub>在v轴的投影方向相反 (即当ω<sub>x</sub>&gt;0时, θ=-<i>π</i>/2;当ω<sub>x</sub>&lt;0时, θ=<i>π</i>/2) 。</p>
                </div>
                <div class="p1">
                    <p id="100">5) 当仅绕竖直轴旋转的角速度ω<sub>y</sub>≠0时, (4) 式为</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>u</mi></mtd></mtr><mtr><mtd><mi>v</mi></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>=</mo><mo>-</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>f</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup></mtd></mtr><mtr><mtd><mi>x</mi><mi>y</mi></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mi>ω</mi><msub><mrow></mrow><mi>y</mi></msub><mo>/</mo><mi>f</mi><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">此时光流的模为<mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mo>=</mo><msqrt><mrow><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo stretchy="false"> (</mo><mi>f</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mrow><mo>|</mo><mrow><mi>ω</mi><msub><mrow></mrow><mi>y</mi></msub><mo>/</mo><mi>f</mi></mrow><mo>|</mo></mrow></mrow></math></mathml>, 光流矢量的方向为<i>θ</i>=arctan[<i>xy</i>/ (<i>f</i><sup>2</sup>+<i>x</i><sup>2</sup>) ], 这2个函数不便于直接看出光流分量的变化规律, 因此进一步解耦为<i>u</i>、<i>v</i>分量。由 (9) 式可得:<i>u</i>分量光流的模为<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><msub><mrow></mrow><mi>u</mi></msub><mo>=</mo><mo stretchy="false"> (</mo><mi>f</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mrow><mo>|</mo><mrow><mi>ω</mi><msub><mrow></mrow><mi>y</mi></msub><mo>/</mo><mi>f</mi></mrow><mo>|</mo></mrow><mo>, </mo><mi>u</mi></mrow></math></mathml>分量光流的方向为θ=0或<i>π</i>, 与ω<sub>y</sub>在u轴上的投影方向相反 (即当ω<sub>y</sub>&gt;0时, θ=<i>π</i>;当ω<sub>y</sub>&lt;0时, θ=0) ;v分量光流的模为<mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><msub><mrow></mrow><mi>v</mi></msub><mo>=</mo><mrow><mo>|</mo><mrow><mi>x</mi><mi>y</mi><mi>ω</mi><msub><mrow></mrow><mi>y</mi></msub><mo>/</mo><mi>f</mi></mrow><mo>|</mo></mrow><mo>, </mo><mi>v</mi></mrow></math></mathml>分量光流的方向由xyω<sub>y</sub>的符号决定, 指向v轴的正向或负向。</p>
                </div>
                <div class="p1">
                    <p id="106">6) 仅绕前向轴旋转角的速度ω<sub>z</sub>≠0时, (4) 式可写成</p>
                </div>
                <div class="p1">
                    <p id="107" class="code-formula">
                        <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>u</mi></mtd></mtr><mtr><mtd><mi>v</mi></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>=</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>y</mi></mtd></mtr><mtr><mtd><mo>-</mo><mi>x</mi></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mi>ω</mi><msub><mrow></mrow><mi>z</mi></msub><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="108">因此, 光流矢量的模为<mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mo>=</mo><msqrt><mrow><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mrow><mo>|</mo><mrow><mi>ω</mi><msub><mrow></mrow><mi>z</mi></msub></mrow><mo>|</mo></mrow></mrow></math></mathml>, 光流矢量的方向为<i>θ</i>=-arctan (<i>y</i>/<i>x</i>) 。将 (7) 式进一步解耦到<i>u</i>、<i>v</i>分量, 可得:<i>u</i>分量光流的模为<mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><msub><mrow></mrow><mi>u</mi></msub><mo>=</mo><mrow><mo>|</mo><mrow><mi>y</mi><mi>ω</mi><msub><mrow></mrow><mi>z</mi></msub></mrow><mo>|</mo></mrow><mo>, </mo><mi>u</mi></mrow></math></mathml>分量光流的方向由yω<sub>z</sub>的符号决定, 指向u轴的正向或负向;v分量光流的模为<mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><msub><mrow></mrow><mi>v</mi></msub><mo>=</mo><mrow><mo>|</mo><mrow><mi>x</mi><mi>ω</mi><msub><mrow></mrow><mi>z</mi></msub></mrow><mo>|</mo></mrow><mo>, </mo><mi>v</mi></mrow></math></mathml>分量光流的方向由xω<sub>z</sub>的符号决定, 指向v轴的正向或负向。</p>
                </div>
                <div class="p1">
                    <p id="112">以上六自由度独立运动时的解耦光流运动场模型可以用于3.2节中仿真算法结果及第4节中真实数据光流矢量的解耦结果分析, 从而实现解耦模型分析、仿真算法和结果、真实数据解耦结果三者的一致性检验。</p>
                </div>
                <h3 id="113" name="113" class="anchor-tag">3 解耦光流运动场仿真算法</h3>
                <h4 class="anchor-tag" id="114" name="114"><b>3.1 算法设计与流程</b></h4>
                <div class="p1">
                    <p id="115">根据 (4) 式所示的光流运动场模型, 可以在不同运动场景输入时, 仿真光流运动场模型解耦后光流矢量分量的大小和方向变化规律, 以便于后续开展更有针对性的载体运动与光流矢量间的误差分析, 有助于位姿估计过程开展误差消除等算法设计。</p>
                </div>
                <div class="p1">
                    <p id="116">不同载体运动场景下必要的输入参数设置如下:每幅图像内随机分布N个光流矢量, 载体以帧间平移速度<b><i>t</i></b>= (<i>t</i><sub><i>x</i></sub>, <i>t</i><sub><i>y</i></sub>, <i>t</i><sub><i>z</i></sub>) <sup>T</sup>、旋转角速度<i>ω</i>= (<i>ω</i><sub><i>x</i></sub>, <i>ω</i><sub><i>y</i></sub>, <i>ω</i><sub><i>z</i></sub>) <sup>T</sup>进行运动, 载体运动变化的噪声水平为<i>L</i><sub>noise</sub>, 场景深度范围从<i>d</i><sub>min</sub>随机生成至<i>d</i><sub>max</sub>, 场景中外点的比例为<i>R</i><sub>outlier</sub>, 是否添加深度误差为<i>I</i><sub>IsDepthError</sub>。在未设置输入项时, 采用默认值。输出结果显示每帧<i>N</i>个随载体运动状态不断变化的随机分布的解耦光流分量。在仿真计算中, 深度误差、外点误差为可选项。具体解耦光流运动场仿真算法流程如表1所示。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117"><b>3.2 算法仿真结果</b></h4>
                <div class="p1">
                    <p id="118">利用3.1节给出的算法进行仿真后, 可以清晰地看到解耦后载体单变量运动时的光流矢量场, 如图2所示, 其中蓝色点、绿色点分别为光流矢量的起点、终点, 箭头指向光流矢量方向。图2 (a) ～ (c) 所示分别为载体在单个方向平移变化场景下的光流矢量场, 图2 (d) ～ (f) 所示分别为载体在单个方向旋转变化场景下的光流矢量场。</p>
                </div>
                <div class="p1">
                    <p id="119">图2中采用的仿真参数如下:图像大小为1241 pixel×376 pixel, 每帧仿真光流点个数为<i>N</i>=300;随机噪声水平<i>L</i><sub>noise</sub>=0.1;场景深度范围<i>d</i><sub>min</sub>=5 m, <i>d</i><sub>max</sub>=15 m;外点比例<i>R</i><sub>outlier</sub>=0.01;是否添加深度误差<i>I</i><sub>IsDepthError</sub>=1, 仅在下标<i>I</i><sub>index</sub>为50、100、150、200、250的深度值中添加了5%的随机误差;假设载体分别以50 km/h的速率进行3个方向的运动, 图像采集帧率为10 Hz, 载体姿态以3 (°) /s的角速度进行3个方向的变化。</p>
                </div>
                <div class="area_img" id="120">
                    <p class="img_tit">表1 解耦光流运动场的仿真算法 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Simulation algorithm for decoupled optical flow motion field</p>
                    <p class="img_note"></p>
                    <table id="120" border="1"><tr><td><br /><b>Algorithm</b>: Simulation algorithm for decoupled optical flow motion field model</td></tr><tr><td><br /><b>Input</b>: Simulated flow number of per frame <i>N</i>; simulated camera motion <b><i>t</i></b>=[<i>t</i><sub><i>x</i></sub>, <i>t</i><sub><i>y</i></sub>, <i>t</i><sub><i>z</i></sub>]<sup>T</sup> and <i>ω</i>=[<i>ω</i><sub><i>x</i></sub>, <i>ω</i><sub><i>y</i></sub>, <i>ω</i><sub><i>z</i></sub>]<sup>T</sup>; random noise level <i>L</i><sub>noise</sub>; depth range in the scene from <i>d</i><sub>min</sub> to <i>d</i><sub>max</sub>;ratio of outlier flow <i>R</i><sub>Outlier</sub>; if adding depth error <i>I</i><sub>IsDepthError</sub>;<br /><b>Output</b>: Simulated <i>N</i> flows and plot;<br />1: Randomly produce <i>N</i> flows inside image size and store them as <b><i>I</i></b><sub><i>xy</i></sub>;<br />2: Transform <b><i>I</i></b><sub><i>xy</i></sub> from image coordinate into <b><i>C</i></b><sub><i>xy</i></sub> in camera coordinate;<br />3: Randomly produce <i>N</i> depths ranging from <i>d</i><sub>min</sub> to <i>d</i><sub>max</sub> of the scene and store them as <b><i>D</i></b>;<br />4: <b>if</b><i>I</i><sub>IsDepthError</sub>≠0<br /> Add random error into all/part of <b><i>D</i></b>;<br /><b>end</b><br />5: Put <b><i>D</i></b>、<b><i>C</i></b><sub><i>xy</i></sub>、<b><i>t</i></b>、<i>ω</i> into Equation (4) and calculate flow <b><i>F</i></b><sub><i>uv</i></sub>;<br />6: <b>if</b><i>R</i><sub>Outlier</sub>&gt;0<br /><i>N</i><sub>Outlier</sub>=<i>R</i><sub>Outlier</sub>×<i>N</i>;<br /> Randomly produce <i>N</i><sub>Outlier</sub> outlier flow inside <i>N</i> flows which is indexed as <b><i>I</i></b><sub>OutlierIndex</sub>;<br /> Randomly produce <i>N</i><sub>Outlier</sub> magnitudes <b><i>M</i></b>;<br /> Randomly produce <i>N</i><sub>Outlier</sub> angles <i>θ</i>;<br /><b><i>F</i></b><sub>Outlier</sub>=[<b><i>M</i></b>×cos (<i>θ</i>) , <b><i>M</i></b>×sin (<i>θ</i>) ], and substitute <b><i>F</i></b><sub><i>uv</i></sub> which is indexed as <b><i>I</i></b><sub>OutlierIndex</sub>;<br /><b>end</b><br />7: Transform <b><i>F</i></b><sub></sub><i>uv</i> in camera coordinate to <b><i>I</i></b><sub><i>uv</i></sub> in image coordinate.</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904035_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 光流运动场模型解耦为六自由度的仿真结果。 (a) 仅tx≠0时的右向平移场景; (b) 仅ty≠0时的向下平移场景; (c) 仅tz≠0时的前向平移场景; (d) 仅ωx≠0时的俯仰角旋转场景; (e) 仅ωy≠0时的航向角旋转场景; (f) 仅ωz≠0 时的横滚角旋转场景" src="Detail/GetImg?filename=images/GXXB201904035_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 光流运动场模型解耦为六自由度的仿真结果。 (a) 仅<i>t</i><sub><i>x</i></sub>≠0时的右向平移场景; (b) 仅<i>t</i><sub><i>y</i></sub>≠0时的向下平移场景; (c) 仅<i>t</i><sub><i>z</i></sub>≠0时的前向平移场景; (d) 仅<i>ω</i><sub><i>x</i></sub>≠0时的俯仰角旋转场景; (e) 仅<i>ω</i><sub><i>y</i></sub>≠0时的航向角旋转场景; (f) 仅<i>ω</i><sub><i>z</i></sub>≠0 时的横滚角旋转场景  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904035_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Simulation results of optical flow motion field model decoupled into six degrees of freedom. (a) Rightward translational scene only at <i>t</i><sub><i>x</i></sub>≠0; (b) downward translational scene only at <i>t</i><sub><i>y</i></sub>≠0; (c) forward translational scene only at <i>t</i><sub><i>z</i></sub>≠0; (d) pitch rotational scene only at <i>ω</i><sub><i>x</i></sub>≠0; (e) yaw rotational scene only at <i>ω</i><sub><i>y</i></sub>≠0; (f) roll rotational scene only at <i>ω</i><sub><i>z</i></sub>≠0</p>

                </div>
                <div class="p1">
                    <p id="122">由图2 (a) ～ (c) 可知:当相机仅进行右向平移运动时, 场景中的光流矢量场向左平移;当相机仅进行垂直向下平移运动时, 场景中的光流矢量场垂直向上平移;当载体仅进行前向平移运动时, 场景中的光流矢量场从图像主点向四周散射, 且矢量模逐渐增大。由图2 (d) ～ (f) 可知:当载体进行仅俯仰角变化的旋转运动时, 场景中的光流矢量场沿图像水平中线向俯仰角变化的反方向旋转, 且两侧向沿水平方向向图像中部“收缩”;当载体进行仅航向角变化的旋转运动时, 场景中的光流矢量场沿图像垂直中线向航向角变化的反方向旋转, 且两侧沿垂直方向向图像中部“收缩”;当载体进行仅滚动角变化的旋转运动时, 场景中的光流矢量场沿相机主轴向滚动角变化的反方向旋转, 且越靠近主轴, 旋转的光流弧长越小, 越远离主轴, 旋转的光流弧长越大。</p>
                </div>
                <div class="p1">
                    <p id="123">为了清晰地对应2.2节中模型与光流分量的关系, 将六自由度分量光流进一步解耦到<i>u</i>、<i>v</i>分量, 对图2中的六自由度光流分量进行解耦, 对应得到的分量结果分别如图3～4所示。</p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904035_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 将tx、ty、tz分量分别解耦为u、v分量。 (a) tx的u分量; (b) tx的v分量; (c) ty的u分量; (d) ty的v分量; (e) tz的u分量; (f) tz的v分量" src="Detail/GetImg?filename=images/GXXB201904035_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 将<i>t</i><sub><i>x</i></sub>、<i>t</i><sub><i>y</i></sub>、<i>t</i><sub><i>z</i></sub>分量分别解耦为<i>u</i>、<i>v</i>分量。 (a) <i>t</i><sub><i>x</i></sub>的<i>u</i>分量; (b) <i>t</i><sub><i>x</i></sub>的<i>v</i>分量; (c) <i>t</i><sub><i>y</i></sub>的<i>u</i>分量; (d) <i>t</i><sub><i>y</i></sub>的<i>v</i>分量; (e) <i>t</i><sub><i>z</i></sub>的<i>u</i>分量; (f) <i>t</i><sub><i>z</i></sub>的<i>v</i>分量  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904035_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Decoupling of components <i>t</i><sub><i>x</i></sub>, <i>t</i><sub><i>y</i></sub>, and <i>t</i><sub><i>z</i></sub> into components <i>u</i> and <i>v</i>. (a) Component <i>u</i> of <i>t</i><sub><i>x</i></sub>; (b) component <i>v</i> of <i>t</i><sub><i>x</i></sub>; (c) component <i>u</i> of <i>t</i><sub><i>y</i></sub>; (d) component <i>v</i> of <i>t</i><sub><i>y</i></sub>; (e) component <i>u</i> of <i>t</i><sub><i>z</i></sub>; (f) component <i>v</i> of <i>t</i><sub><i>z</i></sub></p>

                </div>
                <div class="p1">
                    <p id="125">根据图3～4中的<i>u</i>、<i>v</i>分别考察六自由度独立变化情况下光流矢量的模与方向, 能够清晰地看出其与2.2节中的模型分析完全一致, 其中, <i>t</i><sub><i>z</i></sub>、<i>ω</i><sub><i>z</i></sub>的<i>u</i>、<i>v</i>分量, <i>ω</i><sub><i>x</i></sub>的<i>u</i>分量, <i>ω</i><sub><i>y</i></sub>的<i>v</i>分量的光流矢量方向受到两个或多个变量的影响, 因此在图3～4中的对应解耦结果均以图像的水平、竖直中线为轴呈对称性变化, 其他光流分量在各自图像内则呈方向一致的分布。反之, 图3与图4中六自由度各<i>u</i>、<i>v</i>分量的矢量合成即对应图2中的六自由度光流场。此外, 根据仿真算法可知, 图2～4中所有场景中方向与整体方向显著不一致的部分矢量均为仿真算法中设置的外点矢量。</p>
                </div>
                <h3 id="126" name="126" class="anchor-tag">4 真实场景光流矢量及解耦对比分析</h3>
                <h4 class="anchor-tag" id="127" name="127"><b>4.1 真实场景光流矢量及解耦</b></h4>
                <div class="p1">
                    <p id="128">为了考察真实场景数据中光流及解耦分量在不同相对运动场景下的变化规律, 选取KITTI数据集的两个典型场景, 给出不同载体运动状态下的光流场。图5 (a) 所示为KITTI序列2中第3、4帧间运动的光流场, 该场景以载体快速前向运动为主, 旋转运动较小。与之相对比, 图5 (b) 所示为KITTI序列2中第45、46帧间运动的光流场, 该场景以载体低速转向为主, 平移运动较小。图5中两个光流场中的每个光流矢量均以蓝色点、绿色点为起点、终点, 黄色箭头为光流线段, 箭头指向光流变化方向。为了使每帧光流矢量大小、方向的显示结果更清晰, 本研究选取特征点光流进行分析。</p>
                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904035_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 将ωx、ωy、ωz分量分别解耦为u、v分量。 (a) ωx的u分量; (b) ωx的v分量; (c) ωy的u分量; (d) ωy的v分量; (e) ωz的u分量; (f) ωz的v分量" src="Detail/GetImg?filename=images/GXXB201904035_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 将<i>ω</i><sub><i>x</i></sub>、<i>ω</i><sub><i>y</i></sub>、<i>ω</i><sub><i>z</i></sub>分量分别解耦为<i>u</i>、<i>v</i>分量。 (a) <i>ω</i><sub><i>x</i></sub>的<i>u</i>分量; (b) <i>ω</i><sub><i>x</i></sub>的<i>v</i>分量; (c) <i>ω</i><sub><i>y</i></sub>的<i>u</i>分量; (d) <i>ω</i><sub><i>y</i></sub>的<i>v</i>分量; (e) <i>ω</i><sub><i>z</i></sub>的<i>u</i>分量; (f) <i>ω</i><sub><i>z</i></sub>的<i>v</i>分量  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904035_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Decoupling of components <i>ω</i><sub><i>x</i></sub>, <i>ω</i><sub><i>y</i></sub>, and <i>ω</i><sub><i>z</i></sub> into components <i>u</i> and <i>v</i>. (a) Component <i>u</i> of <i>ω</i><sub><i>x</i></sub>; (b) component <i>v</i> of <i>ω</i><sub><i>x</i></sub>; (c) component <i>u</i> of <i>ω</i><sub><i>y</i></sub>; (d) component <i>v</i> of <i>ω</i><sub><i>y</i></sub>; (e) component <i>u</i> of <i>ω</i><sub><i>z</i></sub>; (f) component <i>v</i> of <i>ω</i><sub><i>z</i></sub></p>

                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904035_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 典型运动场景的光流场。 (a) 前向运动场景; (b) 旋转运动场景" src="Detail/GetImg?filename=images/GXXB201904035_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 典型运动场景的光流场。 (a) 前向运动场景; (b) 旋转运动场景  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904035_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Optical flow fields in typical motion scenes. (a) Forward motion scene; (b) rotational motion scene</p>

                </div>
                <div class="p1">
                    <p id="132">将光流矢量解耦为平移速度分量 (即平移分量) <b><i>t</i></b>和旋转角速度分量 (即旋转分量) <i>ω</i>, 如图6所示。</p>
                </div>
                <div class="p1">
                    <p id="133">将两个典型场景下的平移分量进一步解耦为相机沿世界坐标系3个轴的3个平移分量, 即<b><i>t</i></b>= (<i>t</i><sub><i>x</i></sub>, <i>t</i><sub><i>y</i></sub>, <i>t</i><sub><i>z</i></sub>) <sup>T</sup>, 前向运动场景的3个平移分量分别如图7 (a) 、图7 (c) 、图7 (e) 所示, 旋转运动场景的3个平移分量分别如图7 (b) 、图7 (d) 、图7 (f) 所示。</p>
                </div>
                <div class="p1">
                    <p id="134">将两个典型场景下的旋转分量进一步解耦为相机绕世界坐标系3个轴的3个旋转角分量, 即<i>ω</i>= (<i>ω</i><sub><i>x</i></sub>, <i>ω</i><sub><i>y</i></sub>, <i>ω</i><sub><i>z</i></sub>) <sup>T</sup>, 前向运动场景的3个旋转角分量分别如图8 (a) 、图8 (c) 、图8 (e) 所示, 旋转运动场景的3个旋转角分量分别如图8 (b) 、图8 (d) 、图8 (f) 所示。</p>
                </div>
                <div class="p1">
                    <p id="135">对比图7、8中前向运动场景、旋转运动场景的6个分量可知:前向运动场景中除<i>t</i><sub><i>z</i></sub>分量外的其他5个分量均较小, <i>t</i><sub><i>z</i></sub>分量呈现出以相机主点为中心向四周散射的矢量分布, 且越靠近主点, 矢量越小, 越远离主点, 矢量越大;旋转运动场景除<i>ω</i><sub><i>y</i></sub>和<i>t</i><sub><i>z</i></sub>分量外的其他4个分量均较小, <i>t</i><sub><i>z</i></sub>分量相比左侧前向运动场景矢量的模略小, 但也呈现出以相机主点为中心向四周散射的矢量分布, 且越靠近主点, 矢量越小, 越远离主点, 矢量越大;<i>ω</i><sub><i>y</i></sub>分量呈现绕相机<i>y</i>轴旋转产生的几乎平行于相机<i>x</i>轴的光流分量分布状态。</p>
                </div>
                <div class="p1">
                    <p id="136">为了进一步验证真实场景与仿真解耦、解耦模型分析的一致性, 将图7 (e) 中前向运动场景<i>t</i><sub><i>z</i></sub>分量解耦为光流<i>u</i>分量和<i>v</i>分量, 如图9所示。将图8 (c) 中旋转运动场景的<i>ω</i><sub><i>y</i></sub>分量解耦为<i>u</i>分量和<i>v</i>分量, 如图10所示。</p>
                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904035_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 前向与旋转运动场景光流的平移分量t与旋转分量ω。 (a) 前向运动场景的平移分量t; (b) 旋转运动场景的平移分量t; (c) 前向运动场景的旋转分量ω; (d) 旋转运动场景的旋转分量ω" src="Detail/GetImg?filename=images/GXXB201904035_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 前向与旋转运动场景光流的平移分量<b><i>t</i></b>与旋转分量<i>ω</i>。 (a) 前向运动场景的平移分量<b><i>t</i></b>; (b) 旋转运动场景的平移分量<b><i>t</i></b>; (c) 前向运动场景的旋转分量<i>ω</i>; (d) 旋转运动场景的旋转分量<i>ω</i>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904035_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Translational component <b><i>t</i></b> and rotational component <i>ω</i> of optical flow in forward and rotational motion scenes. (a) Translational component <b><i>t</i></b> in forward motion scene; (b) translational component <b><i>t</i></b> in rotational motion scene; (c) rotational component <i>ω</i> in forward motion scene; (d) rotational component <i>ω</i> in rotational motion scene</p>

                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904035_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 前向与旋转运动场景的tx、ty、tz分量。 (a) 前向运动场景的tx分量; (b) 旋转运动场景的tx分量; (c) 前向运动场景的ty分量; (d) 旋转运动场景的ty分量; (e) 前向运动场景的tz分量; (f) 旋转运动场景的tz分量" src="Detail/GetImg?filename=images/GXXB201904035_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 前向与旋转运动场景的<i>t</i><sub><i>x</i></sub>、<i>t</i><sub><i>y</i></sub>、<i>t</i><sub><i>z</i></sub>分量。 (a) 前向运动场景的<i>t</i><sub><i>x</i></sub>分量; (b) 旋转运动场景的<i>t</i><sub><i>x</i></sub>分量; (c) 前向运动场景的<i>t</i><sub><i>y</i></sub>分量; (d) 旋转运动场景的<i>t</i><sub><i>y</i></sub>分量; (e) 前向运动场景的<i>t</i><sub><i>z</i></sub>分量; (f) 旋转运动场景的<i>t</i><sub><i>z</i></sub>分量  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904035_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Components <i>t</i><sub><i>x</i></sub>, <i>t</i><sub><i>y</i></sub>, and <i>t</i><sub><i>z</i></sub> in forward and rotational motion scenes. (a) Component <i>t</i><sub><i>x</i></sub> in forward motion scene; (b) component <i>t</i><sub><i>x</i></sub> in rotational motion scene; (c) component <i>t</i><sub><i>y</i></sub> in forward motion scene; (d) component <i>t</i><sub><i>y</i></sub> in rotational motion scene; (e) component <i>t</i><sub><i>z</i></sub> in forward motion scene; (f) component <i>t</i><sub><i>z</i></sub> in rotational motion scene</p>

                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904035_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 前向与旋转运动场景的ωx、ωy、ωz分量。 (a) 前向运动场景的ωx分量; (b) 旋转运动场景的ωx分量; (c) 前向运动场景的ωy分量; (d) 旋转运动场景的ωy分量; (e) 前向运动场景的ωz分量; (f) 旋转运动场景的ωz分量" src="Detail/GetImg?filename=images/GXXB201904035_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 前向与旋转运动场景的<i>ω</i><sub><i>x</i></sub>、<i>ω</i><sub><i>y</i></sub>、<i>ω</i><sub><i>z</i></sub>分量。 (a) 前向运动场景的<i>ω</i><sub><i>x</i></sub>分量; (b) 旋转运动场景的<i>ω</i><sub><i>x</i></sub>分量; (c) 前向运动场景的<i>ω</i><sub><i>y</i></sub>分量; (d) 旋转运动场景的<i>ω</i><sub><i>y</i></sub>分量; (e) 前向运动场景的<i>ω</i><sub><i>z</i></sub>分量; (f) 旋转运动场景的<i>ω</i><sub><i>z</i></sub>分量  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904035_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Components <i>ω</i><sub><i>x</i></sub>, <i>ω</i><sub><i>y</i></sub>, and <i>ω</i><sub><i>z</i></sub> in forward and rotational motion scenes. (a) Component <i>ω</i><sub><i>x</i></sub> in forward motion scene; (b) component <i>ω</i><sub><i>x</i></sub> in rotational motion scene; (c) component <i>ω</i><sub><i>y</i></sub> in forward motion scene; (d) component <i>ω</i><sub><i>y</i></sub> in rotational motion scene; (e) component <i>ω</i><sub><i>z</i></sub> in forward motion scene; (f) component <i>ω</i><sub><i>z</i></sub> in rotational motion scene</p>

                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904035_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 前向运动场景中tz分量的解耦。 (a) u分量; (b) v分量" src="Detail/GetImg?filename=images/GXXB201904035_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 前向运动场景中<i>t</i><sub><i>z</i></sub>分量的解耦。 (a) <i>u</i>分量; (b) <i>v</i>分量  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904035_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Decoupling of component <i>t</i><sub><i>z</i></sub> in forward motion scene. (a) Component <i>u</i>; (b) component <i>v</i></p>

                </div>
                <div class="area_img" id="141">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904035_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 旋转运动场景中ωy分量的解耦。 (a) u分量; (b) v分量" src="Detail/GetImg?filename=images/GXXB201904035_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 旋转运动场景中<i>ω</i><sub><i>y</i></sub>分量的解耦。 (a) <i>u</i>分量; (b) <i>v</i>分量  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904035_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Decoupling of component <i>ω</i><sub><i>y</i></sub> in rotational motion scene. (a) Component <i>u</i>; (b) component <i>v</i></p>

                </div>
                <div class="p1">
                    <p id="142">将图9～10与图3 (e) 、 (f) 中<i>t</i><sub><i>z</i></sub>和图4 (c) 、 (d) 中<i>ω</i><sub><i>y</i></sub>的仿真场景对应分量的解耦结果进行对比后可以发现, 图9～10的真实场景很好地验证了2.2节中的解耦分析模型—— (7) 式和 (9) 式, 以及3.2节中的仿真结果, 模型分析、仿真结果、真实场景解耦结果完全一致。</p>
                </div>
                <h4 class="anchor-tag" id="143" name="143"><b>4.2 对比分析</b></h4>
                <div class="p1">
                    <p id="144">目前, 开展图像光流场与载体运动位姿分量间关系解耦仿真分析的文献较少。Buczko等<citation id="170" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>的研究中涉及了光流平移分量和旋转分量的解耦定性分析, 如图11所示, 他们选取了平移与旋转两个典型的运动场景, 并分别将两个场景下的光流矢量解耦为平移分量 (黄色) 和旋转分量 (绿色) 。由图11可知, Buczko等<citation id="171" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>的分析结果与本文图6中两个典型场景的实验结果完全一致, 而且本文还进一步将平移、旋转两个分量进一步解耦到六自由度位姿分量。此外, 为了清晰地考察解耦结果与模型间的对应关系, 又将平移和旋转两个分量解耦到六自由度<i>u</i>分量和<i>v</i>分量, 揭示了载体位姿变化与光流间的紧密关系。</p>
                </div>
                <div class="area_img" id="145">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904035_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 Buczko等[15]选取的典型场景中的光流解耦结果。 (a) 前向运动场景中的平移分量; (b) 旋转运动场景中的平移分量; (c) 前向运动场景中的旋转分量; (d) 旋转运动场景中的旋转分量" src="Detail/GetImg?filename=images/GXXB201904035_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 Buczko等<citation id="172" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>选取的典型场景中的光流解耦结果。 (a) 前向运动场景中的平移分量; (b) 旋转运动场景中的平移分量; (c) 前向运动场景中的旋转分量; (d) 旋转运动场景中的旋转分量  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904035_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Decoupling results of optical flow in typical scenes selected by Buczko, <i>et al</i><citation id="173" type="reference"><link href="39" rel="bibliography" /><sup>[15]</sup></citation>. (a) Translational components in forward motion scene; (b) translational components in rotational motion scene; (c) rotational components in forward motion scene; (d) rotational components in rotational motion scene</p>

                </div>
                <div class="area_img" id="146">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904035_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 Buczko等[20]在普通场景中的对比结果。 (a) 普通场景中的光流矢量; (b) 未解耦的光流矢量; (c) 图12 (a) 中解耦后正确的平移光流分量及其汇聚规律; (d) 图12 (b) 中解耦后正确的平移光流分量及其汇聚规律" src="Detail/GetImg?filename=images/GXXB201904035_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 Buczko等<citation id="174" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>在普通场景中的对比结果。 (a) 普通场景中的光流矢量; (b) 未解耦的光流矢量; (c) 图12 (a) 中解耦后正确的平移光流分量及其汇聚规律; (d) 图12 (b) 中解耦后正确的平移光流分量及其汇聚规律  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904035_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Comparison results in ordinary scenes obtained by Buczko, <i>et al</i><citation id="175" type="reference"><link href="49" rel="bibliography" /><sup>[20]</sup></citation>. (a) Optical flow vectors in ordinary scene; (b) undecoupled optical flow vectors; (c) decoupled right translational optical flow components in Fig. 12 (a) and their convergence rules; (d) decoupled right translational optical flow components in Fig. 12 (b) and their convergence rules</p>

                </div>
                <div class="p1">
                    <p id="147">Buczko等<citation id="176" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>对平移、旋转解耦前后光流矢量的分布规律进行了定性探讨, 选取的场景和实验结果如图12所示。图12中左、右两个场景的载体运动包含了平移和旋转六自由度中多个分量的运动变化, 因此图12 (a) 中的光流场呈无规则变化的光流矢量分布, 在仅显示图12 (a) 中的前向平移分量后, 得到图12 (c) , 其中包含了正确的光流平移分量 (绿色) 和错误的光流平移分量 (红色) , 而正确的光流平移分量最终汇聚于同一个光心投影点, 即图12 (c) 中蓝色的“x”位置, 该结果与本文图2中对<i>t</i><sub><i>z</i></sub>分量的仿真实验结果和模型分析完全一致。</p>
                </div>
                <div class="p1">
                    <p id="148">图12 (b) 中绿色线段是包含平移和旋转分量的光流矢量, Buczko等<citation id="177" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>发现其延长线 (黄色线) 不能汇聚于一点, 但是在图12 (d) 中将光流矢量解耦, 仅给出正确的前向平移分量时, 该分量所有矢量的延长线完全交汇于一点, 该结果同样与对<i>t</i><sub><i>z</i></sub>分量的仿真实验结果和模型分析完全一致。Buczko等<citation id="178" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>的研究也进一步验证了本文图2中各分量仿真结果分析以及2.2节中分量的解耦模型分析。值得指出的是, Buczko等<citation id="179" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>仅对前向平移分量的分布结果进行了定性分析, 而本文给出了所有自由度分量光流的模型和分量光流结果的仿真结果。</p>
                </div>
                <h3 id="149" name="149" class="anchor-tag">5 结 论</h3>
                <div class="p1">
                    <p id="150">本课题组将无人机平台位姿估计常采用的光流场运动模型引入到车载平台应用中, 对车载平台六自由度运动分量进行解耦仿真和深入分析。首先, 根据车载平台相机的运动速度、角速度与光流场间的投影关系, 推导出了光流运动场模型, 并对光流矢量在不同载体运动状态下的解耦光流运动场模型进行模型推导和分析;然后给出了包含各类误差输入的较完备的算法流程, 通过六自由度解耦光流分量的仿真结果给出了解耦模型对应的光流场;最后, 对KITTI数据集中平移和旋转两类典型真实场景进行解耦光流实验和分析, 验证了前向平移和水平旋转场景光流解耦与模型分析、仿真结果的一致性, 并与Buczko等<citation id="180" type="reference"><link href="39" rel="bibliography" /><link href="49" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">20</a>]</sup></citation>的光流解耦分析结果进行了一致性对比。目前尚未有专门针对车载平台对光流运动场模型进行六自由度解耦的仿真和模型分析的文献, 本文给出的仿真算法可用于后续开展车载平台光流运动场解耦位姿估计中的误差解析与算法验证, 给出的模型分析和仿真实验、真实场景实验结果对开展无人平台光流运动应用的研究具有一定的借鉴和指导。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201810031&amp;v=MjIyNzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIa1ZyM05JalhUYkxHNEg5bk5yNDlHWlk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Xiong C Z, Che M Q, Wang R L, <i>et al</i>.Robust real-time visual tracking via dual model adaptive switching[J].Acta Optica Sinica, 2018, 38 (10) :1015002.熊昌镇, 车满强, 王润玲, 等.稳健的双模型自适应切换实时跟踪算法[J].光学学报, 2018, 38 (10) :1015002.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201807028&amp;v=MzIxNjFqWFRiTEc0SDluTXFJOUhiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhrVnIzTkk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Lin H C, Lü Q, Wei H, <i>et al</i>.Quadrotor autonomous flight and three-dimensional dense reconstruction based on VI-SLAM[J].Acta Optica Sinica, 2018, 38 (7) :0715004.林辉灿, 吕强, 卫恒, 等.基于VI-SLAM的四旋翼自主飞行与三维稠密重构[J].光学学报, 2018, 38 (7) :0715004.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201808018&amp;v=MTI2ODFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIa1ZyM05JalhUYkxHNEg5bk1wNDlFYklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Xiao J S, Tian H, Zou W T, <i>et al</i>.Stereo matching based on convolutional neural network[J].Acta Optica Sinica, 2018, 38 (8) :0815017.肖进胜, 田红, 邹文涛, 等.基于深度卷积神经网络的双目立体视觉匹配算法[J].光学学报, 2018, 38 (8) :0815017.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The perception of the visual world">

                                <b>[4]</b> Gibson J J.The perception of the visual world[M].Cambridge:The Riverside Press, 1950.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Special Issue on High‐Speed Vision‐Based Autonomous Navigation of UAVs">

                                <b>[5]</b> Loianno G, Scaramuzza D, Kumar V.Special issue on high-speed vision-based autonomous navigation of UAVs[J].Journal of Field Robotics, 2018, 35 (1) :3-4.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Low-cost autonomous navigation system based on optical-flow classification">

                                <b>[6]</b> Meneses M C, Matos L N, Oprado B.Low-cost autonomous navigation system based on optical-flow classification[EB/OL]. (2018-03-11) [2018-10-15].https://arxiv.org/abs/1803.03966.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017249632.nh&amp;v=MTUwNjIyNkdiRzhGOWZQclpFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtWcjNOVkY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Wan F H.Research on multi-sensor based UAV obstacle avoidance technique[D].Hangzhou:Zhejiang University of Technology, 2017.万富华.基于多传感器的无人机定位和避障技术研究[D].杭州:浙江工业大学, 2017.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1015708658.nh&amp;v=MDY5OTZxQnRHRnJDVVJMT2VaZVZ1RnlIa1ZyM05WRjI2RzdTNEZ0ZkpwNUViUElRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Dai B X.Research on obstacle avoidance method of MAV in indoor environment based on optical flow[D].Chengdu:University of Electronic Science and Technology of China, 2015.戴碧霞.基于光流的微小型飞行器室内避障方法研究[D].成都:电子科技大学, 2015.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SVO:Semidirect Visual Odometry for Monocular and Multicamera Systems">

                                <b>[9]</b> Forster C, Zhang Z C, Gassner M, <i>et al</i>.SVO:Semidirect visual odometry for monocular and multicamera systems[J].IEEE Transactions on Robotics, 2017, 33 (2) :249-265.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Direct sparse odometry&amp;quot;">

                                <b>[10]</b> Engel J, Koltun V, Cremers D.Direct sparse odometry[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (3) :611-625.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJRS&amp;filename=SJRS121022017257&amp;v=MDA2MDc5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3JsVTdiSkpWd1JOaWZaZmJLNkg5SE9yWTlFWStrS0N4TTh6eFVTbURk&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Longuet-Higgins H C, Prazdny K.The interpretation of a moving retinal image[J].Proceedingsof the Royal Society of London B, 1980, 208 (1173) :385-397.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Kalman filter-based algorithms for estimating depth from image sequences">

                                <b>[12]</b> Matthies L, Szeliski R, Kanade T.Kalman filter-based algorithms for estimating depth from image sequences[M].Heidelberg:Springer, 1993:87-130.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature depth observation for image-based visual servoing: Theory and experiments">

                                <b>[13]</b> de Luca A, Oriolo G, Robuffo Giordano P.Feature depth observation for image-based visual servoing:theory and experiments[J].The International Journal of Robotics Research, 2008, 27 (10) :1093-1116.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vision-based pole-like obstacle detection and localization for urban mobile robots">

                                <b>[14]</b> Sabatini S, Corno M, Fiorenti S, <i>et al</i>.Vision-based pole-like obstacle detection and localization for urban mobile robots[C]∥Proceedings of the 29th IEEE Intelligent Vehicles Symposium, June 26-30, 2018, Changshu, China.New York:IEEE, 2018:1209-1214.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Flow-decoupled normalized reprojection error for visual odometry">

                                <b>[15]</b> Buczko M, Willert V.Flow-decoupled normalized reprojection error for visual odometry[C]//Proceedings of IEEE 19th International Conference on Intelligent Transportation Systems, November 1-4, 2016, Rio de Janeiro, Brazil.New York:IEEE, 2016:1161-1167.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast,robust,continuous monocular egomotion computation">

                                <b>[16]</b> Jaegle A, Phillips S, Daniilidis K.Fast, robust, continuous monocular egomotion computation[C]∥Proceedings of IEEE International Conference on Robotics and Automation, May 16-21, 2016, Stockholm, Sweden.New York:IEEE, 2016:773-780.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYD201612022&amp;v=MzEzOTFOclk5SFpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtWcjNOUFRuU2FyRzRIOWY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> Wu Z L, Li J, Guan Z Y, <i>et al</i>.Optical flow-based autonomous landing control for fixed-wing small UAV[J].Systems Engineering and Electronics, 2016, 38 (12) :2827-2834.吴政隆, 李杰, 关震宇, 等.基于光流的固定翼小型无人机自主着陆控制[J].系统工程与电子技术, 2016, 38 (12) :2827-2834.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SCPD&amp;filename=CN104880187A&amp;v=Mjk5MzF1SGluZ1Vibz1KaU82SHJld0Z0SE5wNGcwQys0UEQzMUx4eFlUNnpvT1MzZm1wV0ZhZTdLVlRMdWRaT2Q=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Guo L, Liu M Y, Wang Y, <i>et al</i>.A motion estimation method for optical flow detection device of aircraft with two cameras:CN104880187A[P/OL]. (2015-09-02) [2018-10-15].https://patentimages.storage.googleapis.com/79/9a/cf/6a6371ddd46e75/CN104880187A.pdf.郭雷, 刘梦瑶, 王岩, 等.一种双摄像机的飞行器光流检测装置的运动估计方法:CN104880187A[P/OL]. (2015-09-02) [2018-10-15].https://patentimages.storage.googleapis.com/79/9a/cf/6a6371ddd46e75/CN104880187A.pdf.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The KITTI Vision Benchmark Suite">

                                <b>[19]</b> Geiger A, Lenz P, Urtasun R.The KITTI vision benchmark suite[EB/OL]. (2018-09-15) [2018-10-07].http://www.cvlibs.net/datasets/kitti/eval_odometry.php.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Monocular outlier detection for visual odometry">

                                <b>[20]</b> Buczko M, Willert V.Monocular outlier detection for visual odometry[C]//Proceedings of IEEE Intelligent Vehicles Symposium, June 11-14, 2017, Los Angeles, CA, USA.New York:IEEE, 2017:1124-1131.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787115242907001&amp;v=MjMzNzZZK3NQRFJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3JsVTdiSkpWd1JYRnF6R2JLNUc5UElyWVpG&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> Zhang Y J.A course of computer vision[M].Beijing:Posts &amp; Telecom Press, 2011.章毓晋.计算机视觉教程[M].北京:人民邮电出版社, 2011.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dynamic perspective">

                                <b>[22]</b> Jitendra M.Dynamic perspective[EB/OL]. (2015-05-02) [2018-10-07].http://www-inst.eecs.berkeley.edu/～cs280/sp15/lectures/4.pdf.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Are we ready for autonomous driving?The KITTI vision benchmark suite">

                                <b>[23]</b> Geiger A, Lenz P, Urtasun R.Are we ready for autonomous driving?The KITTI vision benchmark suite[C]∥Proceedings of, June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:3354-3361.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201904035" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201904035&amp;v=MjA0MDVMRzRIOWpNcTQ5R1lZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtWcjNOSWpYVGI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

