

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134136134502500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201903017%26RESULT%3d1%26SIGN%3dgvxhZ71wmhZm4cnRm8x3l6dBnyc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201903017&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201903017&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201903017&amp;v=MTcwNDNUYkxHNEg5ak1ySTlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIbVZMdklJalg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#50" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#54" data-title="2 GAN及其衍生模型 ">2 GAN及其衍生模型</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="3 算法原理 ">3 算法原理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#68" data-title="&lt;b&gt;3.1 C-DCGAN模型&lt;/b&gt;"><b>3.1 C-DCGAN模型</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;3.2 网络训练过程&lt;/b&gt;"><b>3.2 网络训练过程</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#78" data-title="4 实 验 ">4 实 验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#79" data-title="&lt;b&gt;4.1 数据集&lt;/b&gt;"><b>4.1 数据集</b></a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;4.2 迭代次数与合成结果关系&lt;/b&gt;"><b>4.2 迭代次数与合成结果关系</b></a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;4.3 质量评价&lt;/b&gt;"><b>4.3 质量评价</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#95" data-title="5 结 论 ">5 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="图1 CGAN模型构架图">图1 CGAN模型构架图</a></li>
                                                <li><a href="#61" data-title="图2 DCGAN的生成网络结构图">图2 DCGAN的生成网络结构图</a></li>
                                                <li><a href="#66" data-title="图3 基于C-DCGAN模型的仿真算法流程图">图3 基于C-DCGAN模型的仿真算法流程图</a></li>
                                                <li><a href="#71" data-title="图4 C-DCGAN生成网络结构图">图4 C-DCGAN生成网络结构图</a></li>
                                                <li><a href="#73" data-title="图5 C-DCGAN判别网络结构图">图5 C-DCGAN判别网络结构图</a></li>
                                                <li><a href="#83" data-title="图6 红外数据集上损失函数的变化趋势。">图6 红外数据集上损失函数的变化趋势。</a></li>
                                                <li><a href="#87" data-title="图7 C-DCGAN生成红外目标的效果图">图7 C-DCGAN生成红外目标的效果图</a></li>
                                                <li><a href="#89" data-title="图8 C-DCGAN生成MNIST样本的效果图">图8 C-DCGAN生成MNIST样本的效果图</a></li>
                                                <li><a href="#92" data-title="表1 各种分类方法的准确率对比">表1 各种分类方法的准确率对比</a></li>
                                                <li><a href="#94" data-title="表2 各种数据增强方法的效果对比">表2 各种数据增强方法的效果对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" title=" Huang X, Zhang J Q, Zhang S Z, &lt;i&gt;et al&lt;/i&gt;. GPU-based high-precision real-time radiometric rendering for IR scene generation[J]. Infrared Physics &amp;amp; Technology, 2014, 65: 134-143." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061700250124&amp;v=Mjk3MzdYNDlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lLVjBUYmhJPU5pZk9mYks4SHRmTnFJOUZadTRQRA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Huang X, Zhang J Q, Zhang S Z, &lt;i&gt;et al&lt;/i&gt;. GPU-based high-precision real-time radiometric rendering for IR scene generation[J]. Infrared Physics &amp;amp; Technology, 2014, 65: 134-143.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" Sun W, Wang B. Calculation and image simulation of aircraft infrared radiation[J]. Laser &amp;amp; Infrared, 2017, 47 (6) : 728-735. 孙卫, 王彪. 飞机红外辐射计算及图像仿真[J]. 激光与红外, 2017, 47 (6) : 728-735." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGHW201706016&amp;v=MDEwODBMeXJEZWJHNEg5Yk1xWTlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIbVZMdkk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Sun W, Wang B. Calculation and image simulation of aircraft infrared radiation[J]. Laser &amp;amp; Infrared, 2017, 47 (6) : 728-735. 孙卫, 王彪. 飞机红外辐射计算及图像仿真[J]. 激光与红外, 2017, 47 (6) : 728-735.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" title=" Shao L, Bai T Z, Zheng H J, &lt;i&gt;et al&lt;/i&gt;. Imaging analysis of infrared thermal imaging system based on sea-sky model[J]. Laser &amp;amp; Optoelectronics Progress, 2017, 54 (11) : 111104. 邵龙, 白廷柱, 郑海晶, 等. 基于海空模型的红外热成像系统成像影响分析[J]. 激光与光电子学进展, 2017, 54 (11) : 111104." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201711020&amp;v=MTQ5ODRSTE9lWmVWdUZ5SG1WTHZJTHlyUFpMRzRIOWJOcm85SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Shao L, Bai T Z, Zheng H J, &lt;i&gt;et al&lt;/i&gt;. Imaging analysis of infrared thermal imaging system based on sea-sky model[J]. Laser &amp;amp; Optoelectronics Progress, 2017, 54 (11) : 111104. 邵龙, 白廷柱, 郑海晶, 等. 基于海空模型的红外热成像系统成像影响分析[J]. 激光与光电子学进展, 2017, 54 (11) : 111104.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" title=" Zhu Q R. Design of dynamic infrared scene simulation system based on augmented reality[D]. Beijing: University of Chinese Academy of Sciences (Shanghai Institute of Technical Physics, Chinese Academy of Sciences) , 2017. 祝清瑞. 基于增强现实的动态红外场景仿真系统的设计研究[D]. 北京: 中国科学院大学 (中国科学院上海技术物理研究所) , 2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1017824395.nh&amp;v=MDk1NDl0TEZxcEViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIbVZMdklWRjI2R2J1Nkc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Zhu Q R. Design of dynamic infrared scene simulation system based on augmented reality[D]. Beijing: University of Chinese Academy of Sciences (Shanghai Institute of Technical Physics, Chinese Academy of Sciences) , 2017. 祝清瑞. 基于增强现实的动态红外场景仿真系统的设计研究[D]. 北京: 中国科学院大学 (中国科学院上海技术物理研究所) , 2017.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" title=" Yu S Q, Han Z, Tang Y D, &lt;i&gt;et al&lt;/i&gt;. Texture synthesis method based on generative adversarial networks[J]. Infrared and Laser Engineering, 2018, 47 (2) : 34-39. 余思泉, 韩志, 唐延东, 等. 基于对抗生成网络的纹理合成方法[J]. 红外与激光工程, 2018, 47 (2) : 34-39." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HWYJ201802006&amp;v=MTQ0MTdvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SG1WTHZJTFRyU1pMRzRIOW5Nclk5Rlk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Yu S Q, Han Z, Tang Y D, &lt;i&gt;et al&lt;/i&gt;. Texture synthesis method based on generative adversarial networks[J]. Infrared and Laser Engineering, 2018, 47 (2) : 34-39. 余思泉, 韩志, 唐延东, 等. 基于对抗生成网络的纹理合成方法[J]. 红外与激光工程, 2018, 47 (2) : 34-39.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" title=" Yousef A M, Omar Y M K, Fakharany E. Deep generative image model using a hybrid system of generative adversarial nets (GANs) [C]. International Conference on Advanced Control Circuits Systems, 2017: 278-285." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep generative image model using a hybrid system of generative adversarial nets (GANs)">
                                        <b>[6]</b>
                                         Yousef A M, Omar Y M K, Fakharany E. Deep generative image model using a hybrid system of generative adversarial nets (GANs) [C]. International Conference on Advanced Control Circuits Systems, 2017: 278-285.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" Liu X H, Xie C J, Kuang H L, &lt;i&gt;et al&lt;/i&gt;. Face aging simulation with deep convolutional generative adversarial networks[C]. International Conference on Measuring Technology and Mechatronics Automation, 2018: 220-224." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face aging simulation with deep convolutional generative adversarial networks">
                                        <b>[7]</b>
                                         Liu X H, Xie C J, Kuang H L, &lt;i&gt;et al&lt;/i&gt;. Face aging simulation with deep convolutional generative adversarial networks[C]. International Conference on Measuring Technology and Mechatronics Automation, 2018: 220-224.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" title=" Su&#225;rez P L, Sappa A D, Vintimilla B X. Infrared image colorization based on a triplet DCGAN architecture[C]. IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2017: 212-217." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Infrared Image Colorization Based on a Triplet DCGAN Architecture">
                                        <b>[8]</b>
                                         Su&#225;rez P L, Sappa A D, Vintimilla B X. Infrared image colorization based on a triplet DCGAN architecture[C]. IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2017: 212-217.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" Zhao D C, Weng J K, Liu Y H. Generating traffic scene with deep convolutional generative adversarial networks[C]. Chinese Automation Congress, 2017: 6612-6617." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generating traffic scene with deep convolutional generative adversarial networks">
                                        <b>[9]</b>
                                         Zhao D C, Weng J K, Liu Y H. Generating traffic scene with deep convolutional generative adversarial networks[C]. Chinese Automation Congress, 2017: 6612-6617.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" title=" Zhang H, Patel V M, Riggan B S, &lt;i&gt;et al&lt;/i&gt;. Generative adversarial network-based synthesis of visible faces from polarimetrie thermal faces[C]. IEEE International Joint Conference on Biometrics, 2017: 100-107." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial network-based synthesis of visible faces from polarimetrie thermal faces">
                                        <b>[10]</b>
                                         Zhang H, Patel V M, Riggan B S, &lt;i&gt;et al&lt;/i&gt;. Generative adversarial network-based synthesis of visible faces from polarimetrie thermal faces[C]. IEEE International Joint Conference on Biometrics, 2017: 100-107.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" title=" Enomoto K, Sakurada K, Wang W M, &lt;i&gt;et al&lt;/i&gt;. Filmy cloud removal on satellite imagery with multispectral conditional generative adversarial nets[C]. IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2017: 1533-1541." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Filmy cloud removal on satellite imagery with multispectral conditional generative adversarial nets">
                                        <b>[11]</b>
                                         Enomoto K, Sakurada K, Wang W M, &lt;i&gt;et al&lt;/i&gt;. Filmy cloud removal on satellite imagery with multispectral conditional generative adversarial nets[C]. IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2017: 1533-1541.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" title=" Goodfellow I J, Pouget-Abadie J, Mirza M, &lt;i&gt;et al&lt;/i&gt;. Generative adversarial networks[C]. Advances in Neural Information Processing Systems, 2014: 2672-2680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">
                                        <b>[12]</b>
                                         Goodfellow I J, Pouget-Abadie J, Mirza M, &lt;i&gt;et al&lt;/i&gt;. Generative adversarial networks[C]. Advances in Neural Information Processing Systems, 2014: 2672-2680.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" title=" Mirza M, Osindero S. Conditional generative adversarial nets[EB/OL]. (2014-11-06) [2018-07-28]. https://arxiv.org/abs/1411.1784." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Conditional generative adversarial nets">
                                        <b>[13]</b>
                                         Mirza M, Osindero S. Conditional generative adversarial nets[EB/OL]. (2014-11-06) [2018-07-28]. https://arxiv.org/abs/1411.1784.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL]. (2016-01-07) [2018-07-28]. https://arxiv.org/abs/1511.06434" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks[Online]">
                                        <b>[14]</b>
                                         Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL]. (2016-01-07) [2018-07-28]. https://arxiv.org/abs/1511.06434
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" Tang X L, Du Y M, Liu Y W, &lt;i&gt;et al&lt;/i&gt;. Image recognition with conditional deep convolutional generative adversarial networks[J]. Acta Automatica Sinica, 2018, 44 (5) : 855-864. 唐贤伦, 杜一铭, 刘雨微, 等. 基于条件深度卷积生成对抗网络的图像识别方法[J]. 自动化学报, 2018, 44 (5) : 855-864." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805009&amp;v=MjA0NzZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhtVkx2SUtDTGZZYkc0SDluTXFvOUY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Tang X L, Du Y M, Liu Y W, &lt;i&gt;et al&lt;/i&gt;. Image recognition with conditional deep convolutional generative adversarial networks[J]. Acta Automatica Sinica, 2018, 44 (5) : 855-864. 唐贤伦, 杜一铭, 刘雨微, 等. 基于条件深度卷积生成对抗网络的图像识别方法[J]. 自动化学报, 2018, 44 (5) : 855-864.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" title=" Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2018-10-16]. https://arxiv.org/abs/1409.1556" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">
                                        <b>[16]</b>
                                         Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2018-10-16]. https://arxiv.org/abs/1409.1556
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" >
                                        <b>[17]</b>
                                     Liu X, Liu B, Jin W Q, &lt;i&gt;et al&lt;/i&gt;. A new infrared image detail enhancement and dynamic range compression algorithm[J]. Acta Optica Sinica, 2011, 31 (s1) : s100504. 刘秀, 刘斌, 金伟其, 等. 一种红外图像细节增强和动态范围压缩处理算法[J]. 光学学报, 2011, 31 (s1) : s100504.</a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" Liu D W, Han L, Han X Y. High spatial resolution remote sensing image classification based on deep learning[J]. Acta Optica Sinica, 2016, 36 (4) : 0428001. 刘大伟, 韩玲, 韩晓勇. 基于深度学习的高分辨率遥感影像分类研究[J]. 光学学报, 2016, 36 (4) : 0428001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604039&amp;v=MzA0MDRSTE9lWmVWdUZ5SG1WTHZJSWpYVGJMRzRIOWZNcTQ5R2JZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Liu D W, Han L, Han X Y. High spatial resolution remote sensing image classification based on deep learning[J]. Acta Optica Sinica, 2016, 36 (4) : 0428001. 刘大伟, 韩玲, 韩晓勇. 基于深度学习的高分辨率遥感影像分类研究[J]. 光学学报, 2016, 36 (4) : 0428001.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_19" >
                                        <b>[19]</b>
                                     LeCun Y, Bottou L, Bengio Y, &lt;i&gt;et al&lt;/i&gt;. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86 (11) : 2278-2324.</a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_20" title=" Frid-Adar M, Diamant I, Klang E, &lt;i&gt;et al&lt;/i&gt;. GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification[EB/OL]. (2018-03-03) [2018-10-16]. https://arxiv.org/abs/1803.01229." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification">
                                        <b>[20]</b>
                                         Frid-Adar M, Diamant I, Klang E, &lt;i&gt;et al&lt;/i&gt;. GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification[EB/OL]. (2018-03-03) [2018-10-16]. https://arxiv.org/abs/1803.01229.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-11-13 10:08</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(03),150-156 DOI:10.3788/AOS201939.0311002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于生成对抗式神经网络的红外目标仿真方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%A2%E6%B1%9F%E8%8D%A3&amp;code=36524103&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谢江荣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E8%8C%83%E9%B8%A3&amp;code=09619181&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李范鸣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8D%AB%E7%BA%A2&amp;code=15704392&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卫红</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%86%B0&amp;code=28604097&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李冰</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E4%B8%8A%E6%B5%B7%E6%8A%80%E6%9C%AF%E7%89%A9%E7%90%86%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0028796&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院上海技术物理研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E7%BA%A2%E5%A4%96%E6%8E%A2%E6%B5%8B%E4%B8%8E%E6%88%90%E5%83%8F%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0165468&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院红外探测与成像技术重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出了一种应用于红外目标仿真的模型。利用训练后的条件深度卷积生成对抗网络, 只需输入随机噪声和类别标签, 便能够自动产生预期类别的红外目标仿真图像。在手写数字数据集 (MNIST) 和红外数据集上分别训练模型参数, 再进行自动生成实验, 均可以产生高真实度的样本图像;将判别网络提取的特征用于分类实验, 并将所提方法合成的图像用于数据增强, 以提升分类器性能。研究结果表明, 所提方法能够有效模仿红外辐射特征。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%88%90%E5%83%8F%E7%B3%BB%E7%BB%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">成像系统;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">红外图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E4%BB%BF%E7%9C%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标仿真;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9D%A1%E4%BB%B6%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">条件深度卷积;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成对抗网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *李范鸣 E-mail:lfmjws@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-26</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家十三五国防预研项目 (Jzx2016-0404/Y72-2);</span>
                                <span>上海市现场物证重点实验室基金 (2017xcwzk08);</span>
                    </p>
            </div>
                    <h1><b>Infrared Target Simulation Method Based on Generative Adversarial Neural Networks</b></h1>
                    <h2>
                    <span>Xie Jiangrong</span>
                    <span>Li Fanming</span>
                    <span>Wei Hong</span>
                    <span>Li Bing</span>
            </h2>
                    <h2>
                    <span>Shanghai Institute of Technical Physics, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
                    <span>Key Laboratory of Infrared System Detection and Imaging Technology, Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A model applied to the simulation of infrared targets is proposed. By the trained conditional deep convolutional generative adversarial networks, only the random noise and category label are necessary for the automatic generation of the simulation images of infrared targets belonging to the expected category. The parameters are trained on the handwritten digital dataset (MNIST) and the infrared dataset, respectively, and subsequently the automatic generation experiment is carried out, which can produce the high trueness sample images. The features extracted by the discrimination network are used in the classification experiments, and the images synthesized by the proposed method are used for data augmentation to improve the classifier performance. The research results show that the proposed method can effectively imitate the infrared radiation characteristics.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=imaging%20systems&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">imaging systems;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=infrared%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">infrared image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=target%20simulation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">target simulation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=conditional%20deep%20convolutional&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">conditional deep convolutional;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=generative%20adversarial%20networks&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">generative adversarial networks;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-09-26</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="50" name="50" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="51">红外成像系统能够接收物体反射和自身发射的红外辐射, 具备全天候、高灵敏、高隐蔽、可穿透云雾恶劣天气等优势, 在夜视监控、国防安全、医疗成像等方面发挥着重要作用。由于红外成像探测系统的应用越来越广泛, 因此需要对系统性能及算法进行大量的测试和评估。受限于高昂的时间、人力、财力成本, 外场实验次数有限且不全面, 使用红外场景仿真系统可以模拟外场实验数据, 大大地缩短研发周期。目前主流的红外目标仿真框架包含以下部分:1) 建立目标的三维几何模型, 通过计算温度场对红外辐射分布进行建模;2) 结合大气红外传输模型, 计算出入瞳处辐射量;3) 根据系统定标参数, 实现红外辐射量与灰度值的映射<citation id="97" type="reference"><link href="10" rel="bibliography" /><link href="12" rel="bibliography" /><link href="14" rel="bibliography" /><link href="16" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。整个框架步骤复杂, 建模过程繁琐, 参数数量巨大。</p>
                </div>
                <div class="p1">
                    <p id="52">近年来, 随着深度学习理论的不断发展和完善, 涌现出许多基于神经网络的新框架, 其中生成对抗网络 (GAN) 可实现无需标记训练样本的无监督学习, 已广泛应用于图片生成、补全、风格转换等领域。文献<citation id="98" type="reference">[<a class="sup">5</a>]</citation>提出一种基于GAN的纹理合成方法, 它不同于传统的方法, 无需提取特征样式或不受统计量的约束, 通过迭代生成的图像在视觉上与观测纹理图像保持一致, 同时具有一定的随机性。文献<citation id="99" type="reference">[<a class="sup">6</a>]</citation>通过耦合多种技术构建复合生成对抗模型 (Hybrid-GAN) , 用于数字图像集的自动生成, 将该模型与标准GANs模型进行对比, 结果表明该模型能够极大地增大对数似然函数。文献<citation id="100" type="reference">[<a class="sup">7</a>]</citation>在深度卷积生成对抗网络 (DCGAN) 模型的基础上, 加入个体潜在特征和年龄特征的子编码器, 并用视觉相似性损失函数替代传统的对抗损失函数作为优化目标, 实现了各个年龄段人脸图像的演化。文献<citation id="101" type="reference">[<a class="sup">8</a>]</citation>在DCGAN的基础上, 提出一种新颖的近红外 (NIR) 图像彩色化的方法, 基于三元模型对每个颜色通道独立进行学习, 在训练过程可以达到更快速的收敛以及更大的相似性。文献<citation id="102" type="reference">[<a class="sup">9</a>]</citation>为了克服特殊交通场景图像难以采集的问题, 通过DCGAN学习训练样本的概率分布, 最后生成多样性的交通场景图像和视频。文献<citation id="103" type="reference">[<a class="sup">10</a>]</citation>提出了利用GAN模型实现将红外偏振图像合成相应可见光图像的框架, 其中, 向导子网络的加入使重建后的图像包含更丰富的语义信息;将视觉损失函数结合一致性损失函数进行优化, 保证合成的人脸图像的可分辨特征, 同时增强了视觉真实性。文献<citation id="104" type="reference">[<a class="sup">11</a>]</citation>将条件生成对抗网络 (CGANs) 从可见光RBG (red, blue, green) 图像拓展到多光谱通道, 用于卫星遥感图像时可消除云彩造成的遮挡、模糊, 同时将其转换成相应的可见光图像, 便于后续的监视、预警等。目前国内外尚未有公开文献将GANs模型应用于红外目标的图像仿真领域。</p>
                </div>
                <div class="p1">
                    <p id="53">各类红外目标的检测、跟踪等算法, 都以可信的空中红外目标为研究对象, 为了获得高真实感的仿真红外目标, 除了需要考虑外部环境、内部因素进行温度场分布计算外, 还要进一步增强红外纹理细节分布的自然性。本文提出一种基于条件深度卷积生成对抗网络 (C-DCGAN) 的红外目标仿真框架, 采用反卷积神经网络构建生成网络, 输入为随机噪声, 输出为仿真样本;再将真实目标样本与生成的仿真样本用于训练判别网络, 输出样本属于相应类别的概率。通过反向传播算法更新生成网络参数后, 生成一组新的仿真样本, 用于判别网络的训练, 以上过程交替迭代训练生成网络和判别网络, 最终生成的仿真样本与真实样本的数据分布基本相同, 判别网络几乎无法区别, 便随机产生了红外目标的仿真图像。</p>
                </div>
                <h3 id="54" name="54" class="anchor-tag">2 GAN及其衍生模型</h3>
                <div class="p1">
                    <p id="55">GAN是一种概率生成网络, 最早在2014年由Goodfellow等<citation id="105" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>引入深度学习领域, 普通的GAN模型包括了一个生成网络<i>G</i>和一个判别网络<i>D</i>。利用生成网络<i>G</i>进行概率分布的逆变换采样, 捕捉样本数据<i>x</i>的分布<i>P</i><sub>g</sub>, 用服从某一分布 (均匀分布或高斯分布等) 的噪声<i>z</i>生成一个类似真实训练数据的样本<i>G</i> (<i>z</i>, <i>θ</i><sub>g</sub>) , <i>G</i> (<i>z</i>, <i>θ</i><sub>g</sub>) 越像真实样本效果越好;判别网络<i>D</i>是一个二分类器, <i>D</i> (<i>x</i>) 表示<i>x</i>来自于真实数据的概率, 如果样本来自于真实的训练数据<i>x</i>, 则<i>D</i> (<i>x</i>) 输出大概率, 否则输出小概率。GAN的训练过程本质上是通过训练<i>D</i>来最大化判别正确率, 同时通过训练<i>G</i>来最小化ln{1-<i>D</i>[<i>G</i> (<i>z</i>) ]}, 即<i>D</i>和<i>G</i>的训练是关于值函数<i>V</i> (<i>G</i>, <i>D</i>) 的极小极大的博弈问题:</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>~</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">[</mo><mtext>l</mtext><mtext>n</mtext><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>E</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>~</mo><mi>Ρ</mi><msub><mrow></mrow><mtext>z</mtext></msub><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">{</mo><mi>ln</mi><mo stretchy="false">{</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false">[</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo stretchy="false">}</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">式中:<i>x</i>～<i>P</i><sub>data</sub> (<i>x</i>) 表示<i>x</i>取自真实分布的数据;<i>z</i>～<i>P</i><sub>z</sub> (<i>z</i>) 表示随机向量<i>z</i>取自模拟分布的数据 (比如高斯噪声分布) ;<i>E</i> (·) 表示期望值。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903017_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 CGAN模型构架图" src="Detail/GetImg?filename=images/GXXB201903017_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 CGAN模型构架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903017_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Framework of CGAN model</p>

                </div>
                <div class="p1">
                    <p id="59">传统的GAN模型对于数据的属性不加限定, 无法有效地控制生成的结果, 在生成的图像中往往会存在较多的噪点;此外, 传统 GAN 模型一次只能学习一类数据, 对于包含多个类的数据样本集, 需逐类学习及生成相应类的样本集, 导致效率低下。Mirza等<citation id="106" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了CGAN 模型, CGAN的结构如图1所示, 通过对生成网络<i>G</i>和判别网络<i>D</i>添加额外的信息<i>y</i> (如数据标签、模态数据) , 可以起到连接或者区分<i>z</i>和<i>x</i>的作用, 引导学习过程, 使得网络的性能得到提升, 同时使GAN 模型具有生成多类数据的能力。</p>
                </div>
                <div class="p1">
                    <p id="60">DCGAN是一种改进模型<citation id="107" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 引入具有强大特征提取能力的卷积网络, 从而提高生成网络的无监督学习的效果。DCGAN 的生成网络结构如图2所示, 其中FC为全连接层, BN为批归一化, Deconv为反卷积层, ReLU和tanh都是非线性激活函数。相对于原始的GAN, 整个网络去除全连接层, 直接使用卷积层替代;判别网络几乎是和生成网络对称的, 判别网络采用卷积步长替代空间池化, 生成网络使用反卷积操作实现上采样数据维度的扩大, 获得了更好的训练稳定性。</p>
                </div>
                <div class="area_img" id="61">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903017_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 DCGAN的生成网络结构图" src="Detail/GetImg?filename=images/GXXB201903017_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 DCGAN的生成网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903017_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Structural diagram of generation network of DCGAN</p>

                </div>
                <div class="p1">
                    <p id="62">构建的网络模型能够充分模仿真实红外目标图像的数据分布, 为了提取更深层次的特征, 引入深度卷积神经网络结构;另外, 添加类别标签以引导训练过程, 使得模型对于多分布的数据具有更强大的模仿能力。</p>
                </div>
                <h3 id="63" name="63" class="anchor-tag">3 算法原理</h3>
                <div class="p1">
                    <p id="64">在红外目标图像仿真的问题中, 提出基于C-DCGAN 模型的方法, 生成网络<i>G</i>和判别网络<i>D</i>在对抗训练过程中能够学习到原始样本潜在数据的分布, 最终获得生成网络权重<i>G</i><sub><i>θ</i></sub>, <i>G</i><sub><i>θ</i></sub>可用于自动高效地生成符合要求的图像内容。</p>
                </div>
                <div class="p1">
                    <p id="65">算法总体流程如图3所示, 生成网络<i>G</i>的输入为符合特定分布的随机噪声<i>z</i>, 输入与红外目标类别条件<i>y</i>相连接, 进入多层反卷积网络结构, 输出结果为该类别的仿真样本<i>G</i> (<i>z</i>|<i>y</i>) 。在生成网络中, 对于不同的噪声变量<i>z</i>, 映射了目标图像中的不同内容, 即在限定标签<i>y</i>的情况下, 仅改变输入的噪声<i>z</i>, 可获得同一类飞行器不同角度、不同距离等特征的仿真图像, 并且类别标签<i>y</i>的引入使得网络能够学习多种数据的基本分布, 提升了生成效率和性能。将原始图像样本<i>χ</i><sub><i>y</i></sub>和仿真样本<i>G</i> (<i>z</i>|<i>y</i>) 输入判别网络<i>D</i>, 其多个卷积层的强大特征提取能力, 不仅可以区分出真实样本最符合的类别, 还可以输出仿真样本属于指定类别的概率。通过反向传播损失函数<i>V</i> (<i>G</i>, <i>D</i>) 的梯度, 更新生成网络权重<i>G</i><sub><i>θ</i></sub> 和判别网络权重<i>D</i><sub><i>θ</i></sub> , 在不断地交替迭代的训练过程中, 判别网络具备了区分力, 而生成网络也具备了产生“迷惑”样本的能力, 此时, 便获得了可用于图像仿真的生成网络。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903017_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于C-DCGAN模型的仿真算法流程图" src="Detail/GetImg?filename=images/GXXB201903017_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 基于C-DCGAN模型的仿真算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903017_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Flow chart of simulation algorithm based on C-DCGAN model</p>

                </div>
                <h4 class="anchor-tag" id="68" name="68"><b>3.1 C-DCGAN模型</b></h4>
                <div class="p1">
                    <p id="69">C-DCGAN模型主要由生成网络<i>G</i>和判别网络<i>D</i>组成, 对原始GAN 的生成网络<i>G</i>增加约束条件<i>y</i>, 并对生成网络<i>G</i>和判别网络<i>D</i>使用CNN进行结构调整, 具体步骤如下:1) 取消所有的池化层, <i>G</i>网络中采用反卷积实现上采样, <i>D</i>网络中的卷积加入步幅替代池化;2) <i>G</i>网络和<i>D</i>网络加入批归一化;3) 采用全卷积网络替代全连接层;4) <i>G</i>网络除最后一层使用tanh 函数作为激活函数外, 其他层使用ReLU函数;5) <i>D</i>网络使用LeakyReLU作为激活函数, 最后一层使用softmax 函数。</p>
                </div>
                <div class="p1">
                    <p id="70">借鉴原始DCGAN生成网络的结构, 不同之处在于:各层网络输入标量均与类别标签<i>y</i>相连接 (concat) , 以增强标签数据在训练过程中的引导效果。具体实现过程如图4所示, 首先将符合特定分布的随机噪声<i>z</i>与类别标签<i>y</i>进行concat后的数据输入到两个全连接网络 (FC1, FC2) , 再对其输出进行维度变换 (reshape) , 形成三维张量;将三维张量与扩维后的标签<i>y</i><sub>b</sub>进行concat, 输入多层转置卷积网络 (Deconv1-8) , 每次完成卷积操作后进行批归一化 (BN) , 除最后一层使用tanh 函数外, 其他层均采用ReLU作为激活函数<citation id="108" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。通常多个小感受野卷积核的组合, 比单个大感受野卷积核的表现更好<citation id="109" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 为此在生成网络<i>G</i>中采用三层3 pixel×3 pixel大小的卷积层, 实现对输入数据7 pixel×7 pixel的感受野, 具体的网络参数设置如图4所示, 其中<i>w</i>为卷积核大小, <i>s</i>为卷积步幅, FC1和FC2层的输入节点数分别为105和1029, 输出节点数分别为1024和8192, 整个过程设置批处理图片数<i>N</i><sub>batch</sub>=64, 随机噪声<i>z</i>和类别<i>y</i>的维度分别为100和5, 最终生成的图片尺寸为32 pixel×32 pixel。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903017_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 C-DCGAN生成网络结构图" src="Detail/GetImg?filename=images/GXXB201903017_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 C-DCGAN生成网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903017_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Structural diagram of generation network of C-DCGAN</p>

                </div>
                <div class="p1">
                    <p id="72">判别网络与生成网络的结构基本对称, 本质上实现了分类器的功能, 其具体实现结构如图5所示。输入样本的来源包括真实数据和生成的数据;首先给标签<i>y</i>扩充两维, 变为<i>y</i><sub>b</sub>, 利用concat与图像实现连接;经过4个相似的卷积层 (Conv1-4) , 卷积核大小为3 pixel×3 pixel、步幅为2 pixel;每次卷积之后进行BN 操作, 激活函数使用LeakyReLU函数;将卷积层输出标量与标签<i>y</i>进行concat之后, 通过两个全连接层 (FC1, FC2) 送入Sigmoid 逻辑函数, 并输出1维的判别概率。具体网络参数设置如图5所示, FC1和FC2层的输入节点数分别为1024和1029, 输出节点数分别为8197和 1, 整个过程设置批处理图片数<i>N</i><sub>batch</sub>=64, 输入图片尺寸为32 pixel×32 pixel, 类别数<i>y</i>为5。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903017_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 C-DCGAN判别网络结构图" src="Detail/GetImg?filename=images/GXXB201903017_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 C-DCGAN判别网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903017_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Structural diagram of discrimination network of C-DCGAN</p>

                </div>
                <h4 class="anchor-tag" id="74" name="74"><b>3.2 网络训练过程</b></h4>
                <div class="p1">
                    <p id="75">对抗生成网络的训练过程本质上是对损失函数的优化过程, 对判别网络<i>D</i>来说, 该过程的目的是获得足够的区分能力:1) 要让真实样本通过判别网络<i>D</i>后输出接近1的概率;2) 让生成网络<i>G</i>生成的样本通过判别网络<i>D</i>后输出接近0的概率。对生成网络<i>G</i>而言, 通过判别网络<i>D</i>来判断最小化<i>G</i> (<i>z</i>) 和真实样本的数据分布差异, 所以生成网络<i>G</i>的目的是要生成的样本<i>G</i> (<i>z</i>) 通过判别网络<i>D</i>后结果接近1。损失函数采用交叉熵的组合形式, 可以表示为</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>~</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">[</mo><mtext>l</mtext><mtext>n</mtext><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">|</mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>E</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>~</mo><mi>Ρ</mi><msub><mrow></mrow><mtext>z</mtext></msub><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">{</mo><mi>ln</mi><mo stretchy="false">{</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false">[</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">|</mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo stretchy="false">}</mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">在训练的过程中固定<i>G</i>和<i>D</i>中的一方, 更新另一方的网络权重, 交替迭代, 双方在博弈中极力优化自身网络, 直到达到一个动态平衡 (纳什均衡) 。在实际的训练过程中, 判别网络<i>D</i>在对抗初期很容易压制生成网络<i>G</i>, 导致其出现梯度消失, 不利于达到纳什均衡。一般情况下, 每更新一次判别网络<i>D</i>的参数, 需要更新<i>k</i>次生成网络<i>G</i>的参数, 根据数据集规模的不同选取<i>k</i>的值, 为避免出现生成网络<i>G</i>的梯度消失或梯度来回振荡的问题, 本文模型训练中取<i>k</i>=2。</p>
                </div>
                <h3 id="78" name="78" class="anchor-tag">4 实 验</h3>
                <h4 class="anchor-tag" id="79" name="79"><b>4.1 数据集</b></h4>
                <div class="p1">
                    <p id="80">为了充分验证生成网络的有效性, 分别在MNIS数据集和自建长波红外数据集上进行了实验。其中, MNIST数据集包含10类 (0～9) 手写数字灰度图像, 图像大小为28 pixel×28 pixel, 整个数据集有60000个训练样本, 10000个测试样本。在使用数据时, 需要对图像样本进行归一化处理, 对标签数据进行独热编码。另外, 目前可供选择的公开红外图像数据集较少, 自建红外数据集样本从以往外场实验数据中筛选, 主要对象为天空背景下的多距离、多角度、多种类的飞行器, 通过合理的动态范围压缩和细节增强方法, 将原始16 bit深度的数据映射为8 bit深度<citation id="110" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 样本图像统一裁剪为32 pixel×32 pixel的尺寸。自建的数据集标记包含战斗机、直升机、无人机、民用客机、飞鸟5类, 总样本数量为6000张。</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>4.2 迭代次数与合成结果关系</b></h4>
                <div class="p1">
                    <p id="82">实验中通过Adam算法优化损失函数, 设置全局学习率为0.0002, 动量为0.5, 每个批次为64个样本, 经过15000次迭代该模型达到收敛。训练迭代次数与损失函数的变化如图6所示, 随着迭代次数的增加, 真实数据通过判别网络的损失函数<i>L</i><sub>d_loss_real</sub>与生成数据通过判别网络的损失函数<i>L</i><sub>d_loss_fake</sub>都呈现逐步下降的趋势;判别网络损失函数<i>L</i><sub>d_loss</sub>和生成网络损失函数<i>L</i><sub>g_loss</sub>在训练前期的下降相对平滑, 后期振荡起伏较明显, 但总体而言, 判别网络损失函数逐渐下降, 生成网络损失函数先迅速下降后缓慢上升。以上现象说明, 设定合理的<i>k</i>值, 有利于生成网络<i>G</i>的快速优化参数, 随后两个成熟的网络对抗竞争, 判别网络会以微弱的优势胜过生成网络。</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903017_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 红外数据集上损失函数的变化趋势。" src="Detail/GetImg?filename=images/GXXB201903017_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 红外数据集上损失函数的变化趋势。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903017_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Trends of loss function on infrared dataset.</p>
                                <p class="img_note"> (a) L<sub>d</sub>＿<sub>loss</sub>＿<sub>real</sub>; (b) L<sub>d</sub>＿<sub>loss</sub>＿<sub>fake</sub>; (c) L<sub>d</sub>＿<sub>loss</sub>; (d) L<sub>g</sub>＿<sub>loss</sub></p>
                                <p class="img_note"> (a) L<sub>d</sub>＿<sub>loss</sub>＿<sub>real</sub>; (b) L<sub>d</sub>＿<sub>loss</sub>＿<sub>fake</sub>; (c) L<sub>d</sub>＿<sub>loss</sub>; (d) L<sub>g</sub>＿<sub>loss</sub></p>

                </div>
                <h4 class="anchor-tag" id="84" name="84"><b>4.3 质量评价</b></h4>
                <div class="p1">
                    <p id="85">通常评价生成图像的方法是在视觉上对生成样本进行评价, 实验时生成网络的输入为一个100维的、在0～1之间均匀分布的随机采样, 生成网络输出为一幅32 pixel×32 pixel分辨率的图像;判别网络输入为32 pixel×32 pixel的生成样本和真实目标样本, 输出对输入图像真伪的判别概率。</p>
                </div>
                <div class="p1">
                    <p id="86">图7为C-DCGAN 对红外目标图像的仿真结果, 左侧2列为输入的训练图, 右侧5列为生成图像。从实验结果来看, 各类飞行目标的红外辐射分布特性与真实样本相似, 如战斗机的尾焰辐射、客机两翼发动机的高温辐射、直升机螺旋桨部件的辐射等都得到了充分的学习;在生成过程中, 对数据分布的重新采样可获得纹理丰富、姿态多样的样本, 如鸟类飞行过程中翅膀形态、直升机旋转的螺旋桨等产生了训练样本不具备的全新采样。所提模型在多种飞行目标红外图像的仿真中取得了较好的效果。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903017_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 C-DCGAN生成红外目标的效果图" src="Detail/GetImg?filename=images/GXXB201903017_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 C-DCGAN生成红外目标的效果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903017_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Effect image of infrared targets generated by C-DCGAN</p>

                </div>
                <div class="p1">
                    <p id="88">类似地, 在MNIST 数据集上进行效果验证, 图8为C-DCGAN模型生成MNIST样本的效果图, 可见生成样本的轮廓清晰、结构合理。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903017_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 C-DCGAN生成MNIST样本的效果图" src="Detail/GetImg?filename=images/GXXB201903017_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 C-DCGAN生成MNIST样本的效果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903017_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Effect image of MNIST samples generated by C-DCGAN</p>

                </div>
                <div class="p1">
                    <p id="90">为了评价C-DCGAN 模型表示的仿真方法的性能, 将其网络结构用于提取测试数据的特征, 将变换维度后的特征向量送入线性模型, 最后评估线性模型的表现。本文实验中, 对于训练完成后的判别网络<i>D</i>中的每一层卷积, 去除其连接的类别标签部分, 经过最大池化获取特征, 并将各层特征展平、连接后输出7168维的特征向量<citation id="111" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>, 再将特征向量送入线性支持向量机 (LSVM) 完成分类。除此之外, 在相同的测试数据集上进行了简单的线性分类和K最临近算法<citation id="112" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, 以上两种方法没有设计相应的特征提取器, 都直接作用在样本的像素值上。</p>
                </div>
                <div class="p1">
                    <p id="91">各分类方法的性能对比如表1所示, 在MNIST手写数字数据集中, 三种方法的分类正确率都达到90%以上, 其中K最临近算法最优异;红外目标数据集上, C-DCGAN+SVM算法相较于其他算法具有较低的分类错误率。可见, 本文模型确实能够学习图像的本质特征, 并具备较好的多分布数据的模仿能力, 将其应用于红外目标图像仿真, 可以高效地、随机地生成特定类别的样本。</p>
                </div>
                <div class="area_img" id="92">
                    <p class="img_tit">表1 各种分类方法的准确率对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Accuracy comparison among different classification methods %</p>
                    <p class="img_note"></p>
                    <table id="92" border="1"><tr><td><br />Method</td><td>MNIST</td><td>IR dataset</td></tr><tr><td><br />Linear classifier (1-layer NN) </td><td>91.60</td><td>80.16</td></tr><tr><td><br />K-nearest-neighbors, Euclidean (L2) </td><td>95.00</td><td>81.44</td></tr><tr><td><br />C-DCGAN+L2 SVM</td><td>93.69</td><td>84.31</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="93">深度卷积网络 (CNN) 在图像分类实验中表现突出, 但是其参数众多, 当训练样本数量较少时, 容易产生过拟合, 而数据增强能够增大样本数量, 改善网络性能。将C-DCGAN合成图像作为数据增强的手段, 与常用的仿射变换进行对比实验, 以验证合成样本的有效性。构建3层卷积层+池化层的结构, 再加2个隐藏FC层, 并最终由softmax输出, 得到各个类别的预测概率<citation id="113" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。选取每个类别的原始图像200张, 加入数据增强方法合成的100张图像, 组成分类器网络的训练集, 并另外选取约150张原始样本作为测试集, 进行对比实验。实验结果如表2所示, 在MNIST 数据集上, 由于CNN分类器已经获得了较高的准确率, 各数据增强方法对其性能提升不明显;在红外数据集上, 基于C-DCGAN 增强的数据集的分类性能要优于复制样本和利用仿射变换增强的CNN分类器性能, 较好地避免了过拟合风险, 同时表明本文方法合成图像的有效性。</p>
                </div>
                <div class="area_img" id="94">
                    <p class="img_tit">表2 各种数据增强方法的效果对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Performance comparison among different data augmentation methods %</p>
                    <p class="img_note"></p>
                    <table id="94" border="1"><tr><td><br />Method</td><td>MNIST</td><td>IR dataset</td></tr><tr><td><br />Copy</td><td>95.40</td><td>82.22</td></tr><tr><td><br />Affine transformation</td><td>96.83</td><td>84.17</td></tr><tr><td><br />C-DCGAN generator</td><td>96.15</td><td>85.61</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="95" name="95" class="anchor-tag">5 结 论</h3>
                <div class="p1">
                    <p id="96">提出了将C-DCGAN模型用于红外目标仿真的方法, 在标签条件的限制下, 可以生成预期类别的样本, 并且卷积层高效提取特征的能力保证了高质量的样本输出。在MNIST 数据集和红外目标数据集上对C-DCGAN 模型的样本生成能力进行验证, 从视觉上来看, 生成结果具有与真实图像相似的结构, 纹理信息带有随机性, 同时背景噪声得到了较好的抑制;另外, 该模型提取的特征可用于图像分类实验, 与其他分类算法相比具备一定优势, 反之也验证了该模型对于图像本质特征的模仿能力。所提模型不是对原有图像的简单采样或变换, 而是在充分理解红外图像的辐射特征后, 生成全新的红外目标图像, 相比于传统的红外目标仿真方法, 提升了对多分布数据的生成效率。下一步将针对模型的训练不稳定的原因以及如何生成更高分辨率的图像等问题进行深入研究。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061700250124&amp;v=MjI4NDZJPU5pZk9mYks4SHRmTnFJOUZadTRQRFg0OW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUtWMFRiaA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Huang X, Zhang J Q, Zhang S Z, <i>et al</i>. GPU-based high-precision real-time radiometric rendering for IR scene generation[J]. Infrared Physics &amp; Technology, 2014, 65: 134-143.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGHW201706016&amp;v=MDMyNzh0R0ZyQ1VSTE9lWmVWdUZ5SG1WTHZJTHlyRGViRzRIOWJNcVk5RVlvUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Sun W, Wang B. Calculation and image simulation of aircraft infrared radiation[J]. Laser &amp; Infrared, 2017, 47 (6) : 728-735. 孙卫, 王彪. 飞机红外辐射计算及图像仿真[J]. 激光与红外, 2017, 47 (6) : 728-735.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201711020&amp;v=MDA3Mjc0SDliTnJvOUhaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhtVkx2SUx5clBaTEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Shao L, Bai T Z, Zheng H J, <i>et al</i>. Imaging analysis of infrared thermal imaging system based on sea-sky model[J]. Laser &amp; Optoelectronics Progress, 2017, 54 (11) : 111104. 邵龙, 白廷柱, 郑海晶, 等. 基于海空模型的红外热成像系统成像影响分析[J]. 激光与光电子学进展, 2017, 54 (11) : 111104.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1017824395.nh&amp;v=MTg4MTZxQnRHRnJDVVJMT2VaZVZ1RnlIbVZMdklWRjI2R2J1Nkd0TEZxcEViUElRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Zhu Q R. Design of dynamic infrared scene simulation system based on augmented reality[D]. Beijing: University of Chinese Academy of Sciences (Shanghai Institute of Technical Physics, Chinese Academy of Sciences) , 2017. 祝清瑞. 基于增强现实的动态红外场景仿真系统的设计研究[D]. 北京: 中国科学院大学 (中国科学院上海技术物理研究所) , 2017.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HWYJ201802006&amp;v=MTA4NjJyU1pMRzRIOW5Nclk5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SG1WTHZJTFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Yu S Q, Han Z, Tang Y D, <i>et al</i>. Texture synthesis method based on generative adversarial networks[J]. Infrared and Laser Engineering, 2018, 47 (2) : 34-39. 余思泉, 韩志, 唐延东, 等. 基于对抗生成网络的纹理合成方法[J]. 红外与激光工程, 2018, 47 (2) : 34-39.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep generative image model using a hybrid system of generative adversarial nets (GANs)">

                                <b>[6]</b> Yousef A M, Omar Y M K, Fakharany E. Deep generative image model using a hybrid system of generative adversarial nets (GANs) [C]. International Conference on Advanced Control Circuits Systems, 2017: 278-285.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face aging simulation with deep convolutional generative adversarial networks">

                                <b>[7]</b> Liu X H, Xie C J, Kuang H L, <i>et al</i>. Face aging simulation with deep convolutional generative adversarial networks[C]. International Conference on Measuring Technology and Mechatronics Automation, 2018: 220-224.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Infrared Image Colorization Based on a Triplet DCGAN Architecture">

                                <b>[8]</b> Suárez P L, Sappa A D, Vintimilla B X. Infrared image colorization based on a triplet DCGAN architecture[C]. IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2017: 212-217.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generating traffic scene with deep convolutional generative adversarial networks">

                                <b>[9]</b> Zhao D C, Weng J K, Liu Y H. Generating traffic scene with deep convolutional generative adversarial networks[C]. Chinese Automation Congress, 2017: 6612-6617.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial network-based synthesis of visible faces from polarimetrie thermal faces">

                                <b>[10]</b> Zhang H, Patel V M, Riggan B S, <i>et al</i>. Generative adversarial network-based synthesis of visible faces from polarimetrie thermal faces[C]. IEEE International Joint Conference on Biometrics, 2017: 100-107.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Filmy cloud removal on satellite imagery with multispectral conditional generative adversarial nets">

                                <b>[11]</b> Enomoto K, Sakurada K, Wang W M, <i>et al</i>. Filmy cloud removal on satellite imagery with multispectral conditional generative adversarial nets[C]. IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2017: 1533-1541.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">

                                <b>[12]</b> Goodfellow I J, Pouget-Abadie J, Mirza M, <i>et al</i>. Generative adversarial networks[C]. Advances in Neural Information Processing Systems, 2014: 2672-2680.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Conditional generative adversarial nets">

                                <b>[13]</b> Mirza M, Osindero S. Conditional generative adversarial nets[EB/OL]. (2014-11-06) [2018-07-28]. https://arxiv.org/abs/1411.1784.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks[Online]">

                                <b>[14]</b> Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL]. (2016-01-07) [2018-07-28]. https://arxiv.org/abs/1511.06434
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805009&amp;v=MDY5OTdCdEdGckNVUkxPZVplVnVGeUhtVkx2SUtDTGZZYkc0SDluTXFvOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Tang X L, Du Y M, Liu Y W, <i>et al</i>. Image recognition with conditional deep convolutional generative adversarial networks[J]. Acta Automatica Sinica, 2018, 44 (5) : 855-864. 唐贤伦, 杜一铭, 刘雨微, 等. 基于条件深度卷积生成对抗网络的图像识别方法[J]. 自动化学报, 2018, 44 (5) : 855-864.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">

                                <b>[16]</b> Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2018-10-16]. https://arxiv.org/abs/1409.1556
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" >
                                    <b>[17]</b>
                                 Liu X, Liu B, Jin W Q, <i>et al</i>. A new infrared image detail enhancement and dynamic range compression algorithm[J]. Acta Optica Sinica, 2011, 31 (s1) : s100504. 刘秀, 刘斌, 金伟其, 等. 一种红外图像细节增强和动态范围压缩处理算法[J]. 光学学报, 2011, 31 (s1) : s100504.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604039&amp;v=MTM4ODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhtVkx2SUlqWFRiTEc0SDlmTXE0OUdiWVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Liu D W, Han L, Han X Y. High spatial resolution remote sensing image classification based on deep learning[J]. Acta Optica Sinica, 2016, 36 (4) : 0428001. 刘大伟, 韩玲, 韩晓勇. 基于深度学习的高分辨率遥感影像分类研究[J]. 光学学报, 2016, 36 (4) : 0428001.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_19" >
                                    <b>[19]</b>
                                 LeCun Y, Bottou L, Bengio Y, <i>et al</i>. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86 (11) : 2278-2324.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification">

                                <b>[20]</b> Frid-Adar M, Diamant I, Klang E, <i>et al</i>. GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification[EB/OL]. (2018-03-03) [2018-10-16]. https://arxiv.org/abs/1803.01229.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201903017" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201903017&amp;v=MTcwNDNUYkxHNEg5ak1ySTlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIbVZMdklJalg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

