

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133863487940000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201907026%26RESULT%3d1%26SIGN%3dwWnouIWovJqJcUsNsOjKB9EDcoc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201907026&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201907026&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201907026&amp;v=MjM1ODVMRzRIOWpNcUk5SFlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmhVTHpPSWpYVGI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#68" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#73" data-title="2 算法概述 ">2 算法概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="3 基于多层深度卷积特征的目标跟踪算法 ">3 基于多层深度卷积特征的目标跟踪算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="&lt;b&gt;3.1 深度卷积特征&lt;/b&gt;"><b>3.1 深度卷积特征</b></a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;3.2 相关滤波框架&lt;/b&gt;"><b>3.2 相关滤波框架</b></a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;3.3 模型更新策略&lt;/b&gt;"><b>3.3 模型更新策略</b></a></li>
                                                <li><a href="#137" data-title="&lt;b&gt;3.4 算法总体流程&lt;/b&gt;"><b>3.4 算法总体流程</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#142" data-title="4 实  验 ">4 实  验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#143" data-title="&lt;b&gt;4.1 实验平台及参数配置&lt;/b&gt;"><b>4.1 实验平台及参数配置</b></a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;4.2 评价标准&lt;/b&gt;"><b>4.2 评价标准</b></a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;4.3 定性分析&lt;/b&gt;"><b>4.3 定性分析</b></a></li>
                                                <li><a href="#170" data-title="&lt;b&gt;4.4 定量分析&lt;/b&gt;"><b>4.4 定量分析</b></a></li>
                                                <li><a href="#182" data-title="&lt;b&gt;4.5 算法跟踪速率&lt;/b&gt;"><b>4.5 算法跟踪速率</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#190" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#75" data-title="图1 目标跟踪算法框架图">图1 目标跟踪算法框架图</a></li>
                                                <li><a href="#84" data-title="表1 VGG-Net-19的各层参数">表1 VGG-Net-19的各层参数</a></li>
                                                <li><a href="#85" data-title="图2 VGG-Net-19_OT网络结构图">图2 VGG-Net-19_OT网络结构图</a></li>
                                                <li><a href="#230" data-title="图3 算法流程">图3 算法流程</a></li>
                                                <li><a href="#147" data-title="表2 OTB-2015视频属性">表2 OTB-2015视频属性</a></li>
                                                <li><a href="#148" data-title="表3 UAV123视频属性">表3 UAV123视频属性</a></li>
                                                <li><a href="#155" data-title="图4 10个跟踪算法在不同视频序列上的定性结果显示。">图4 10个跟踪算法在不同视频序列上的定性结果显示。</a></li>
                                                <li><a href="#168" data-title="图5 10个跟踪算法在部分遮挡视频序列上的定性结果显示。">图5 10个跟踪算法在部分遮挡视频序列上的定性结果显示。</a></li>
                                                <li><a href="#175" data-title="图6 基于OTB-2015评估基准OPE的跟踪算法。">图6 基于OTB-2015评估基准OPE的跟踪算法。</a></li>
                                                <li><a href="#176" data-title="图7 基于UAV123评估基准OPE的跟踪算法。">图7 基于UAV123评估基准OPE的跟踪算法。</a></li>
                                                <li><a href="#184" data-title="图8 OTB-2015 11种不同属性视频序列跟踪精度曲线">图8 OTB-2015 11种不同属性视频序列跟踪精度曲线</a></li>
                                                <li><a href="#185" data-title="表4 UAV123 12种不同属性视频序列跟踪精度与跟踪成功率">表4 UAV123 12种不同属性视频序列跟踪精度与跟踪成功率</a></li>
                                                <li><a href="#186" data-title="图9 OTB-2015 11种不同属性视频序列跟踪成功率曲线">图9 OTB-2015 11种不同属性视频序列跟踪成功率曲线</a></li>
                                                <li><a href="#192" data-title="表5 跟踪速率">表5 跟踪速率</a></li>
                                                <li><a href="#193" data-title="表6 基于深度学习的跟踪算法的平均跟踪速率对比">表6 基于深度学习的跟踪算法的平均跟踪速率对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" title=" Bolme D S, Beveridge J R, Draper B A, &lt;i&gt;et al&lt;/i&gt;.Visual object tracking using adaptive correlation filters[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:2544-2550." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">
                                        <b>[1]</b>
                                         Bolme D S, Beveridge J R, Draper B A, &lt;i&gt;et al&lt;/i&gt;.Visual object tracking using adaptive correlation filters[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:2544-2550.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;.Exploiting the circulant structure of tracking-by-detection with kernels[M]//Fitzgibbon A, Lazebnik S, Perona P, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Berlin, Heidelberg:Springer, 2012, 7575:702-715." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with Kernels">
                                        <b>[2]</b>
                                         Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;.Exploiting the circulant structure of tracking-by-detection with kernels[M]//Fitzgibbon A, Lazebnik S, Perona P, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Berlin, Heidelberg:Springer, 2012, 7575:702-715.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" title=" Danelljan M, Khan F S, Felsberg M, &lt;i&gt;et al&lt;/i&gt;.Adaptive color attributes for real-time visual tracking[C]∥2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:1090-1097." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive color attributes for real-time visual tracking">
                                        <b>[3]</b>
                                         Danelljan M, Khan F S, Felsberg M, &lt;i&gt;et al&lt;/i&gt;.Adaptive color attributes for real-time visual tracking[C]∥2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:1090-1097.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" >
                                        <b>[4]</b>
                                     Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596.</a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" title=" Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Learning spatially regularized correlation filters for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:4310-4318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">
                                        <b>[5]</b>
                                         Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Learning spatially regularized correlation filters for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:4310-4318.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" title=" Wang X, Hou Z Q, Yu W S, &lt;i&gt;et al&lt;/i&gt;.Target scale adaptive robust tracking based on fusion of multilayer convolutional features[J].Acta Optica Sinica, 2017, 37 (11) :1115005.王鑫, 侯志强, 余旺盛, 等.基于多层卷积特征融合的目标尺度自适应稳健跟踪[J].光学学报, 2017, 37 (11) :1115005." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201711027&amp;v=MDc1MTJybzlIWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnluaFVMekJJalhUYkxHNEg5Yk4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Wang X, Hou Z Q, Yu W S, &lt;i&gt;et al&lt;/i&gt;.Target scale adaptive robust tracking based on fusion of multilayer convolutional features[J].Acta Optica Sinica, 2017, 37 (11) :1115005.王鑫, 侯志强, 余旺盛, 等.基于多层卷积特征融合的目标尺度自适应稳健跟踪[J].光学学报, 2017, 37 (11) :1115005.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" Cai Y Z, Yang D D, Mao N, &lt;i&gt;et al&lt;/i&gt;.Visual tracking algorithm based on adaptive convolutional features[J].Acta Optica Sinica, 2017, 37 (3) :0315002.蔡玉柱, 杨德东, 毛宁, 等.基于自适应卷积特征的目标跟踪算法[J].光学学报, 2017, 37 (3) :0315002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201703032&amp;v=MDE4NjZHNEg5Yk1ySTlHWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnluaFVMekJJalhUYkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Cai Y Z, Yang D D, Mao N, &lt;i&gt;et al&lt;/i&gt;.Visual tracking algorithm based on adaptive convolutional features[J].Acta Optica Sinica, 2017, 37 (3) :0315002.蔡玉柱, 杨德东, 毛宁, 等.基于自适应卷积特征的目标跟踪算法[J].光学学报, 2017, 37 (3) :0315002.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" title=" Li C, Lu C Y, Zhao X, &lt;i&gt;et al&lt;/i&gt;.Scale adaptive correlation filtering tracing algorithm based on feature fusion[J].Acta Optica Sinica, 2018, 38 (5) :0515001.李聪, 鹿存跃, 赵珣, 等.特征融合的尺度自适应相关滤波跟踪算法[J].光学学报, 2018, 38 (5) :0515001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201805026&amp;v=MjE3MjZHNEg5bk1xbzlIWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnluaFVMekJJalhUYkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Li C, Lu C Y, Zhao X, &lt;i&gt;et al&lt;/i&gt;.Scale adaptive correlation filtering tracing algorithm based on feature fusion[J].Acta Optica Sinica, 2018, 38 (5) :0515001.李聪, 鹿存跃, 赵珣, 等.特征融合的尺度自适应相关滤波跟踪算法[J].光学学报, 2018, 38 (5) :0515001.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" Wang H Y, Wang L, Yin W R, &lt;i&gt;et al&lt;/i&gt;.Multi-scale correlation filtering visual tracking algorithm combined with target detection[J].Acta Optica Sinica, 2019, 39 (1) :0115004.王红雨, 汪梁, 尹午荣, 等.结合目标检测的多尺度相关滤波视觉跟踪算法[J].光学学报, 2019, 39 (1) :0115004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201901035&amp;v=MTk3OTdMekJJalhUYkxHNEg5ak1ybzlHWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnluaFU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Wang H Y, Wang L, Yin W R, &lt;i&gt;et al&lt;/i&gt;.Multi-scale correlation filtering visual tracking algorithm combined with target detection[J].Acta Optica Sinica, 2019, 39 (1) :0115004.王红雨, 汪梁, 尹午荣, 等.结合目标检测的多尺度相关滤波视觉跟踪算法[J].光学学报, 2019, 39 (1) :0115004.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" title=" Wang N Y, Yeung D Y.Learning a deep compact image representation for visual tracking[C]//Proceedings of the 26th International Conference on Neural Information Processing Systems, December 5-10, 2013, Lake Tahoe, Nevada.USA:Curran Associates Inc., 2013, 1:809-817." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a deep compact image representation for visual tracking">
                                        <b>[10]</b>
                                         Wang N Y, Yeung D Y.Learning a deep compact image representation for visual tracking[C]//Proceedings of the 26th International Conference on Neural Information Processing Systems, December 5-10, 2013, Lake Tahoe, Nevada.USA:Curran Associates Inc., 2013, 1:809-817.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" title=" Wang N, Li S, Gupta A, &lt;i&gt;et al&lt;/i&gt;.Transferring rich feature hierarchies for robust visual tracking[EB/OL]. (2015-04-23) [2019-01-06].https://arxiv.org/abs/1501.04587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transferring rich feature hierarchies for robust visual tracking">
                                        <b>[11]</b>
                                         Wang N, Li S, Gupta A, &lt;i&gt;et al&lt;/i&gt;.Transferring rich feature hierarchies for robust visual tracking[EB/OL]. (2015-04-23) [2019-01-06].https://arxiv.org/abs/1501.04587.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" title=" Ma C, Huang J B, Yang X K, &lt;i&gt;et al&lt;/i&gt;.Hierarchical convolutional features for visual tracking[C]∥2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:3074-3082." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical convolutional features for visual tracking">
                                        <b>[12]</b>
                                         Ma C, Huang J B, Yang X K, &lt;i&gt;et al&lt;/i&gt;.Hierarchical convolutional features for visual tracking[C]∥2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:3074-3082.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" title=" Ma C, Huang J B, Yang X K, &lt;i&gt;et al&lt;/i&gt;.Robust visual tracking via hierarchical convolutional features[J].IEEE Transactions on Pattern Analysis and Machine Intelligence (Early Access) , (2018-08-13) [2019-01-06].DOI:10.1109/TPAMI.2018.2865311." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust visual tracking via hierarchical convolutional features">
                                        <b>[13]</b>
                                         Ma C, Huang J B, Yang X K, &lt;i&gt;et al&lt;/i&gt;.Robust visual tracking via hierarchical convolutional features[J].IEEE Transactions on Pattern Analysis and Machine Intelligence (Early Access) , (2018-08-13) [2019-01-06].DOI:10.1109/TPAMI.2018.2865311.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" Wang L J, Ouyang W L, Wang X G, &lt;i&gt;et al&lt;/i&gt;.Visual tracking with fully convolutional networks[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:3119-3127." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual Tracking with Fully Convolutional Networks">
                                        <b>[14]</b>
                                         Wang L J, Ouyang W L, Wang X G, &lt;i&gt;et al&lt;/i&gt;.Visual tracking with fully convolutional networks[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:3119-3127.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" Nam H, Han B.Learning multi-domain convolutional neural networks for visual tracking[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:4293-4302." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning multi-domain convolutional neural networks for visual tracking">
                                        <b>[15]</b>
                                         Nam H, Han B.Learning multi-domain convolutional neural networks for visual tracking[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:4293-4302.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" title=" Bertinetto L, Valmadre J, Henriques J F, &lt;i&gt;et al&lt;/i&gt;.Fully-convolutional Siamese networks for object tracking[M]//Hua G, J&#233;gou H.Lecture notes in computer science.Cham:Springer, 2016, 9914:850-865." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully-convolutional siamese networks for object tracking">
                                        <b>[16]</b>
                                         Bertinetto L, Valmadre J, Henriques J F, &lt;i&gt;et al&lt;/i&gt;.Fully-convolutional Siamese networks for object tracking[M]//Hua G, J&#233;gou H.Lecture notes in computer science.Cham:Springer, 2016, 9914:850-865.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" title=" Valmadre J, Bertinetto L, Henriques J, &lt;i&gt;et al&lt;/i&gt;.End-to-end representation learning for correlation filter based tracking[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:5000-5008." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-End Representation Learning for Correlation Filter Based Tracking">
                                        <b>[17]</b>
                                         Valmadre J, Bertinetto L, Henriques J, &lt;i&gt;et al&lt;/i&gt;.End-to-end representation learning for correlation filter based tracking[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:5000-5008.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" Tao R, Gavves E, Smeulders A W M.Siamese instance search for tracking[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1420-1429." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Siamese Instance Search for Tracking">
                                        <b>[18]</b>
                                         Tao R, Gavves E, Smeulders A W M.Siamese instance search for tracking[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1420-1429.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_19" >
                                        <b>[19]</b>
                                     LeCun Y, Bengio Y, Hinton G.Deep learning[J].Nature, 2015, 521 (7553) :436-444.</a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_20" title=" Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2019-01-06].https://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[20]</b>
                                         Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2019-01-06].https://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_21" title=" Deng J, Dong W, Socher R, &lt;i&gt;et al&lt;/i&gt;.ImageNet:a large-scale hierarchical image database[C]∥2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 20-25, 2009, Miami, FL, USA.New York:IEEE, 2009:248-255." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet:a large-scale hierarchical image database">
                                        <b>[21]</b>
                                         Deng J, Dong W, Socher R, &lt;i&gt;et al&lt;/i&gt;.ImageNet:a large-scale hierarchical image database[C]∥2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 20-25, 2009, Miami, FL, USA.New York:IEEE, 2009:248-255.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_22" title=" Liu W, Anguelov D, Erhan D, &lt;i&gt;et al&lt;/i&gt;.SSD:single shot multibox detector[C]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Cham:Springer, 2016, 9905:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:single shot multibox detector">
                                        <b>[22]</b>
                                         Liu W, Anguelov D, Erhan D, &lt;i&gt;et al&lt;/i&gt;.SSD:single shot multibox detector[C]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Cham:Springer, 2016, 9905:21-37.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_23" title=" Wu Y, Lim J, Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1834-1848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">
                                        <b>[23]</b>
                                         Wu Y, Lim J, Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1834-1848.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_24" title=" Mueller M, Smith N, Ghanem B.A benchmark and simulator for UAV tracking[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Cham:Springer, 2016, 9905:445-461." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Benchmark and Simulator for UAV Tracking">
                                        <b>[24]</b>
                                         Mueller M, Smith N, Ghanem B.A benchmark and simulator for UAV tracking[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Cham:Springer, 2016, 9905:445-461.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_25" title=" Wu Y, Lim J, Yang M H.Online object tracking:a benchmark[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2013, Portland, OR, USA.New York:IEEE, 2013:2411-2418." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online object tracking:A benchmark">
                                        <b>[25]</b>
                                         Wu Y, Lim J, Yang M H.Online object tracking:a benchmark[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2013, Portland, OR, USA.New York:IEEE, 2013:2411-2418.
                                    </a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_26" title=" Jia X, Lu H C, Yang M H.Visual tracking via adaptive structural local sparse appearance model[C]∥2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:1822-1829." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual tracking via adaptive structural local sparse appearance model">
                                        <b>[26]</b>
                                         Jia X, Lu H C, Yang M H.Visual tracking via adaptive structural local sparse appearance model[C]∥2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:1822-1829.
                                    </a>
                                </li>
                                <li id="62">


                                    <a id="bibliography_27" title=" Hong S, You T, Kwak S, &lt;i&gt;et al&lt;/i&gt;.Online tracking by learning discriminative saliency map with convolutional neural network[C]//Proceedings of the 32nd international Conference on Machine Learning, July 6-11, 2015, Lille, France.Massachusetts:JMLR.org, 2015, 37:597-606." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online tracking by learning discriminative saliency map with convolutional neural network">
                                        <b>[27]</b>
                                         Hong S, You T, Kwak S, &lt;i&gt;et al&lt;/i&gt;.Online tracking by learning discriminative saliency map with convolutional neural network[C]//Proceedings of the 32nd international Conference on Machine Learning, July 6-11, 2015, Lille, France.Massachusetts:JMLR.org, 2015, 37:597-606.
                                    </a>
                                </li>
                                <li id="64">


                                    <a id="bibliography_28" title=" Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Accurate scale estimation for robust visual tracking[C]∥Proceedings of the British Machine Vision Conference 2014, September 1-5, 2014, Nottingham.Durham, England, UK:BMVA Press, 2014:1-11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate Scale Estimation for Robust Visual Tracking">
                                        <b>[28]</b>
                                         Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Accurate scale estimation for robust visual tracking[C]∥Proceedings of the British Machine Vision Conference 2014, September 1-5, 2014, Nottingham.Durham, England, UK:BMVA Press, 2014:1-11.
                                    </a>
                                </li>
                                <li id="66">


                                    <a id="bibliography_29" title=" Ma C, Huang J B, Yang X K, &lt;i&gt;et al&lt;/i&gt;.Adaptive correlation filters with long-term and short-term memory for object tracking[J].International Journal of Computer Vision, 2018, 126 (8) :771-796." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive correlation filters with long-term and short-term memory for object tracking">
                                        <b>[29]</b>
                                         Ma C, Huang J B, Yang X K, &lt;i&gt;et al&lt;/i&gt;.Adaptive correlation filters with long-term and short-term memory for object tracking[J].International Journal of Computer Vision, 2018, 126 (8) :771-796.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-02 15:14</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(07),229-242 DOI:10.3788/AOS201939.0715002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于多层深度卷积特征的抗遮挡实时跟踪算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B4%94%E6%B4%B2%E6%B6%93&amp;code=40084404&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">崔洲涓</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AE%89%E5%86%9B%E7%A4%BE&amp;code=28927043&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安军社</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B4%94%E5%A4%A9%E8%88%92&amp;code=40084403&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">崔天舒</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%9B%BD%E5%AE%B6%E7%A9%BA%E9%97%B4%E7%A7%91%E5%AD%A6%E4%B8%AD%E5%BF%83%E5%A4%8D%E6%9D%82%E8%88%AA%E5%A4%A9%E7%B3%BB%E7%BB%9F%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=1697354&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院国家空间科学中心复杂航天系统电子信息技术重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为提高复杂场景中目标跟踪算法的准确性与实时性, 提出一种基于多层深度卷积特征的抗遮挡实时目标跟踪算法。针对目标跟踪任务, 先对深度卷积网络VGG-Net-19进行微调, 再提取目标区域的多层深度卷积特征。根据相关滤波框架构建位置相关滤波器, 确定目标中心位置。设计尺度相关滤波器对目标区域进行不同尺度采样, 确定目标尺度。目标遮挡时, 采用阶段性评估策略进行模型更新与恢复, 解决模型误差积累问题。选取目标跟踪评估数据集OTB-2015 (100组视频序列) 与UAV123 (123组视频序列) 进行测试。实验结果表明, 本文算法具有更高的准确性, 能够适应目标遮挡、外观变化及背景干扰等复杂情况, 平均速度为29.6 frame/s, 满足目标跟踪任务的实时性要求。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标跟踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度卷积特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%85%B3%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相关滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E5%9E%8B%E6%81%A2%E5%A4%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模型恢复;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *崔洲涓, E-mail:constance669@126.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-01</p>

                    <p>

                            <b>基金：</b>
                                                        <span>中国科学院复杂航天系统电子信息技术重点实验室自主部署基金 (Y42613A32S);</span>
                    </p>
            </div>
                    <h1><b>Real-Time and Anti-Occlusion Visual Tracking Algorithm Based on Multi-Layer Deep Convolutional Features</b></h1>
                    <h2>
                    <span>Cui Zhoujuan</span>
                    <span>An Junshe</span>
                    <span>Cui Tianshu</span>
            </h2>
                    <h2>
                    <span>Key Laboratory of Electronics and Information Technology for Space Systems, National Space Science Center, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to improve the accuracy and real-time performance of visual tracking in complex scenes, a real-time and anti-occlusion visual tracking algorithm based on multi-layer deep convolutional features is proposed. For the visual tracking task, the deep convolutional networks VGG-Net-19 are fine-tuned, and then the multi-layer deep convolutional features of the target region are extracted from the adjusted model. The location correlation filters are constructed to determine the target center position. In order to determine the target scale, a scale correlation filter is performed to sample multi-scale images surrounding the target region. When the target is occluded, the stage evaluation strategy is used to update and recover the model, which solves the problem of template error accumulation. The experimental results on the tracking benchmark OTB-2015 which concludes 100 video sequences and UAV123 which concludes 123 video sequences show that the proposed algorithm has higher accuracy and can adapt to complex situations such as target occlusion, appearance change and background clutters. The average speed is 29.6 frame/s, which meets the real-time requirements of the visual tracking task.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=object%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">object tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20convolutional%20features&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep convolutional features;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=correlation%20filters&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">correlation filters;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=model%20recovery&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">model recovery;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-01</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="68" name="68" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="69">视觉目标跟踪是一个综合视觉特征提取、视觉信息分析、目标运动信息检测和识别等的交叉课题, 是机器视觉领域一个重要的研究方向。随着目标跟踪理论研究的深入和计算机软硬件的发展, 目标跟踪算法在商业和军事领域中的应用日益广泛。然而在实际应用中, 设计一个可以处理好各种复杂多变场景的稳健算法依然具有很大的挑战。</p>
                </div>
                <div class="p1">
                    <p id="70">近年来, 源自信号处理理论的相关滤波视觉跟踪算法以优异的跟踪速度成为研究的热点方向。Bolme 等<citation id="195" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation> 将相关理论引入目标跟踪领域, 设计了一个最小输出平方误差和 (MOSSE) 滤波器, 在跟踪过程中通过提取图像灰度特征寻找目标最大响应值, 实现了速度的飞跃。Henriques 等<citation id="196" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出的核循环结构算法 (CSK) 将训练阶段的密集采样问题转化为特征矩阵的循环移位运算。Danelljan 等<citation id="197" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出的颜色特征算法 (CN) 通过提取灰度特征与降维的颜色特征提升跟踪效果。 Henriques等<citation id="198" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation> 提出的核相关滤波算法 (KCF) 在CSK基础上, 通过方向梯度直方图特征 (HOG) 将适用范围从灰度图扩大到多通道有色图, 其跟踪速度达到172 frame/s。Danelljan等<citation id="199" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation> 针对KCF中利用循环矩阵求解损失函数时出现的边界效应问题, 提出空间正则化算法 (SRDCF) , 通过在损失函数中引入惩罚项, 抑制离中心较远的特征对跟踪算法的影响, 进一步提高了跟踪精度。此外, 国内一些研究人员在相关滤波框架上对多尺度适应问题进行了探索<citation id="200" type="reference"><link href="20" rel="bibliography" /><link href="22" rel="bibliography" /><link href="24" rel="bibliography" /><link href="26" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="71">随着深度学习方法在图像分类、目标检测等领域取得突破性进展, 基于深度学习的跟踪算法也引起了广泛关注。Wang等<citation id="201" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation> 提出的深度学习算法 (DLT) , 在大规模数据集上通过栈式降噪自编码器进行离线预训练得到通用物体表征能力, 引用粒子滤波框架, 对输入跟踪数据集的第1帧带标注的样本进行在线微调。Wang等<citation id="202" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>又在DLT基础上改进, 提出了结构化输出深度学习算法 (SO-DLT) , 跟踪时利用当前目标的有限样本信息对预训练卷积网络模型进行微调。Ma 等<citation id="208" type="reference"><link href="32" rel="bibliography" /><link href="34" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation> 将预训练深度网络中不同卷积层提取的特征与相关滤波的框架结合, 提出分层卷积特征算法 (HCFT) , 经过优化更新得到稳健的分层卷积特征算法 (HCFTstar) 。Wang 等<citation id="203" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation> 提出全卷积网络算法 (FCNT) , 设计出特征筛选网络和互补的预测网络, 提升了跟踪精度。Nam 等<citation id="204" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation> 使用大规模具有标注框的视频序列训练卷积网络得到通用的目标表观模型, 提出多域网络 (MDNet) 结构, 包括共享层及多分支的全连接层, 解决了跟踪训练数据不足的问题。Bertinetto等<citation id="205" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation> 引入基于对比性损失函数的孪生 (Siamese) 体系结构, 训练一个完全端到端的跟踪模型, 同时输入示例样本和候选样本, 通过离线训练模型评估二者的相似程度, 决策层选择适合的匹配算法计算相似度, 匹配程度最高的候选样本作为目标当前最优区域。作者又在此基础上改进, 得到了端到端相关滤波算法 (CFNet) <citation id="206" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。Tao等<citation id="207" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出了一个基于Siamese的实例搜索算法 (SINT) , 通过学习匹配函数, 对第1帧的初始块与当前帧候选样本进行相似度计算, 返回最相似的候选样本作为目标当前状态, 不再进行遮挡检测, 无需更新模型, 取得不错的跟踪效果。</p>
                </div>
                <div class="p1">
                    <p id="72">综上, 基于相关滤波框架的跟踪算法速度较快, 但由于使用HOG、CN等单一特征, 对遮挡问题没有做特别处理, 对背景有强边缘、目标形变的场景表现不稳健。另外, 基于卷积神经网络的跟踪算法精度较高, 但难以预先获得大量样本进行训练, 而且由于网络结构庞大而复杂, 计算量大, 直接影响跟踪算法的实时性。因此针对复杂场景下的快速稳健跟踪问题, 本文提出一种利用深度卷积模型提取特征的抗遮挡实时目标跟踪算法。一方面, 在针对目标任务改进的深度卷积模型上提取多层卷积特征, 另一方面基于相关滤波框架, 通过位置相关滤波器和尺度相关滤波器确定当前目标位置和目标尺度。同时, 通过置信度指标判断目标当前遮挡状态, 选择适宜的模型更新策略。本文算法在复杂环境下不但能够取得良好的跟踪精度, 而且能够达到较快的跟踪速度, 同时解决了跟踪过程中目标遮挡等问题。</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag">2 算法概述</h3>
                <div class="p1">
                    <p id="74">算法的整体框图如图1所示, 主要分为5个部分。1) 将视频序列第1帧中给定的目标候选位置区域的图像输入到针对目标跟踪任务改进的深度卷积网络 (VGG-Net-19_OT) 中, 由相应的深度卷积层提取特征图, 分别记作Conv3_4_OT、Conv4_4_OT和Conv5_4_OT, 进行训练并初始化3个相关滤波器W<sup> (3) </sup>、W<sup> (4) </sup>、W<sup> (5) </sup>。2) 对于输入视频的第<i>t</i>帧图像, 以<i>t</i>-1帧图像中的目标预测结果为中心, 确定搜索框, 使用VGG-Net-19_OT网络模型获取搜索框区域内图像的深度卷积特征。3) 根据提取的深度卷积特征, 与滤波器进行相关运算操作, 根据快速傅里叶变换进行滤波器训练和响应图计算, 从中获得响应值最大点的位置, 即为跟踪目标在第<i>t</i>帧图像中的新位置, 再通过尺度滤波器估计当前目标的最佳跟踪尺度。4) 基于得到的跟踪目标的新位置, 利用 VGG-Net-19_OT网络模型在该中心位置区域内提取图像的深度特征, 在线训练相关滤波器模型。5) 计算3个置信度评估指标, 根据结果判断是否有遮挡, 如有遮挡, 将当前模型备份, 对目标位置进行自适应更新, 直到视频最后1帧。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907026_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 目标跟踪算法框架图" src="Detail/GetImg?filename=images/GXXB201907026_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 目标跟踪算法框架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907026_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Framework of the proposed visual tracking algorithm</p>

                </div>
                <h3 id="76" name="76" class="anchor-tag">3 基于多层深度卷积特征的目标跟踪算法</h3>
                <h4 class="anchor-tag" id="77" name="77"><b>3.1 深度卷积特征</b></h4>
                <div class="p1">
                    <p id="78">目标的特征表达是影响跟踪性能的重要因素, 适用于目标跟踪任务的特征表达需要具备较高的区分性, 能够对背景和非目标物体保持较好的判别性, 还需要具备较强的泛化能力, 能够适应各种遮挡、外观变化等不确定因素。同时, 为达到跟踪的实时要求, 特征表达的参数计算量需要尽可能地少。传统的目标特征提取如HOG 特征、CN特征等浅层特征, 带有一定的先验知识, 特征提取速度快, 对于某些特定场景具有很好的表达能力和区分性, 但在快速运动、遮挡、光照变化等复杂环境情况下稳健性较差。随着深度学习方法特别是卷积神经网络 (CNNs) <citation id="209" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation> 在图像分类领域取得巨大的成功, 出现了诸多性能优秀的网络模型, 越来越多的研究者开始将其应用到目标跟踪领域。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79">3.1.1 卷积神经网络VGG-Net</h4>
                <div class="p1">
                    <p id="80">VGG-Net<citation id="210" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>是牛津大学计算机视觉组和DeepMind公司共同研发的深度卷积网络, 探索了卷积神经网络的深度与性能之间的关系, 通过反复堆叠3×3的小型卷积核和2×2的最大池化层, 构筑了16～19层的卷积神经网络, 证明小型卷积核可以通过增加网络的深度模仿较大卷积核, 实现对图像的局部感知, 减少网络训练参数, 有效提升模型效果, 影响网络最终的性能。卷积模型主要由 5 段卷积、2个全连接特征层和 1个全连接分类层组成。卷积核专注于扩大通道数, 池化着重于缩小特征图的宽和高, 逐渐忽略局部信息。网络结构复杂, 参数占用空间与计算量大, 以VGG-Net-19<citation id="211" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>为例, 各层参数量如表1所示。</p>
                </div>
                <div class="p1">
                    <p id="81">VGG-Net以物体分类作为回归训练标准, 在层数递增时, 仅对训练图片的判别性特征进行提取, 对分类任务无贡献的背景等冗余信息将逐渐消失。目标跟踪与分类的目的不同, 需要在跟踪时将目标从背景中区分出来, 然而在实际场景中, 背景包含同类型物体的可能性也存在。因此, VGG-Net提取的深度特征并不完全适合直接用于目标跟踪任务。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82">3.1.2 目标跟踪的微调网络模型VGG-Net-19_OT</h4>
                <div class="p1">
                    <p id="83">深度卷积模型的优势来自于对大量标注训练数据的有效学习, 而目标跟踪仅仅提供第1帧的边框作为训练数据。为了更好地在深度卷积模型中提取特征应用于目标跟踪任务, 需要对VGG-Net的结构进行微调, 微调过程的本质是针对特定数据集对卷积核进行微调, 每个卷积核对应一个通道, 提取一种更契合数据集的判别性特征。具有冗余的卷积层中, 各通道的特征提取有很大重叠。减少特征提取能力差的卷积核, 保留学习良好的卷积核作为微调初始值, 以迭代删减的方式修剪冗余的网络, 在保持网络精度前提下, 有效减少存储空间、提升网络速度。调整后的网络VGG-Net-19_OT如图2所示。实线框中是基于大规模数据集ImageNet<citation id="212" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation> 预训练的VGG-Net-19深度卷积模型, 由于目标跟踪数据集的数据量相对较小, 因此只选取3个不同层级Conv3_4、Conv4_4、Conv5_4卷积层输出的特征图作为微调对象, 虚线框中为针对目标跟踪任务添加的跟踪网络微调分支。</p>
                </div>
                <div class="area_img" id="84">
                                            <p class="img_tit">
                                                表1 VGG-Net-19的各层参数
                                                    <br />
                                                Table 1 Parameters of VGG-Net-19
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907026_08400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/GXXB201907026_08400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907026_08400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 VGG-Net-19的各层参数" src="Detail/GetImg?filename=images/GXXB201907026_08400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907026_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 VGG-Net-19_OT网络结构图" src="Detail/GetImg?filename=images/GXXB201907026_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 VGG-Net-19_OT网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907026_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Network structure of VGG-Net-19_OT</p>

                </div>
                <h4 class="anchor-tag" id="87" name="87">3.1.3 网络训练策略</h4>
                <div class="p1">
                    <p id="88">首先对VGG-Net-19 网络模型初始化, 保持其结构固定不变。为适应目标在跟踪过程中表观模型的变化, 将目标跟踪数据集中的图像筛选部分输入网络中进行训练。</p>
                </div>
                <div class="p1">
                    <p id="89">采用SSD<citation id="213" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>的方式设定损失函数<i>L</i> (<i>x</i>, <i>c</i>, <i>l</i>, <i>g</i>) , 包括置信度损失<i>L</i><sub>conf</sub> (<i>x</i>, <i>c</i>) 和位置损失<i>L</i><sub>loc</sub> (<i>x</i>, <i>l</i>, <i>g</i>) 两部分:</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>c</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>g</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow></mfrac><mo stretchy="false">[</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>f</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>c</mi><mo stretchy="false">) </mo><mo>+</mo><mi>α</mi><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>g</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">式中, <i>c</i>是默认框类别的置信度, <i>α</i>表示权重系数, <i>l</i>表示预测到的目标框, <i>g</i>是真实的目标框, <i>N</i><sub>tr</sub>为匹配的默认盒个数。则匹配类别<i>p</i>的第<i>i</i><sub>tr</sub>个默认框与第<i>j</i><sub>tr</sub>个真实目标框的置信度损失函数为</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>f</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>c</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>o</mtext><mtext>s</mtext></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow></munderover><mi>x</mi></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub><mi>j</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow></msubsup><mrow><mi>log</mi></mrow><mo stretchy="false">[</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">]</mo><mo>-</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mrow><mtext>Ν</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub></mrow></munder><mrow><mi>log</mi></mrow></mstyle><mo stretchy="false">[</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mn>0</mn><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">]</mo><mo>, </mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false">[</mo><mi>c</mi><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">]</mo></mrow><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mi>p</mi></msub><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false">[</mo><mi>c</mi><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">]</mo></mrow></mfrac><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">式中:<i>S</i><sub>Pos</sub>, <i>S</i><sub>Neg</sub>分别代表正负样本集;<mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>c</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mn>0</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>表示预测值为背景的概率, 概率越高则损失越小;<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>c</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>表示预测值为目标的概率, p为类别;概率越高则损失越小。</p>
                </div>
                <div class="p1">
                    <p id="96">第i<sub><i>tr</i></sub>个默认框与第j<sub><i>tr</i></sub>个真实目标框的位置损失通过<i>smooth</i><sub>L1</sub>损失函数计算</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>g</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>o</mtext><mtext>s</mtext></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>u</mi><mo>∈</mo><mo stretchy="false">{</mo><mi>c</mi><msub><mrow></mrow><mi>x</mi></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mi>y</mi></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo>, </mo><mi>h</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false">}</mo></mrow></munder><mi>x</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub><mi>j</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>⋅</mo></mtd></mtr><mtr><mtd><mrow><mtext>s</mtext><mtext>m</mtext><mtext>o</mtext><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext></mrow><msub><mrow></mrow><mrow><mi>L</mi><mn>1</mn></mrow></msub><mo stretchy="false">[</mo><mi>l</mi><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>-</mo><mover accent="true"><mi>g</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>j</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">式中: {c<sub>x</sub>, c<sub>y</sub>, w<sub><i>tr</i></sub>, h<sub><i>tr</i></sub>}为默认框的位置尺寸;x<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub><mi>j</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>表示第i<sub><i>tr</i></sub>个默认框与第j<sub><i>tr</i></sub>个真实目标框关于类别k是否匹配;<mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mover accent="true"><mi>g</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>j</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>分别表示第u个预测到的目标框与真实框。由于g<sup> (u) </sup>是真实框正规化的几何参数, 在相关滤波算法中, 目标大小和采样框的大小是固定的, 为减少训练参数, 仅进行粗略定位, (3) 式可化简为</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>g</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>o</mtext><mtext>s</mtext></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>u</mi><mo>∈</mo><mo stretchy="false">{</mo><mi>c</mi><msub><mrow></mrow><mi>x</mi></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false">}</mo></mrow></munder><mi>x</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub><mi>j</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>⋅</mo></mtd></mtr><mtr><mtd><mrow><mtext>s</mtext><mtext>m</mtext><mtext>o</mtext><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext></mrow><msub><mrow></mrow><mrow><mi>L</mi><mn>1</mn></mrow></msub><mo stretchy="false">[</mo><mi>l</mi><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>-</mo><mover accent="true"><mi>g</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>j</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">]</mo><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">由于深度卷积网络中不同卷积核存在不同的稀疏度, 部分卷积核的权重参数过于稀疏, 对模型性能提升效果不高。为了更有针对性地提取目标特征, 减少卷积核冗余, 可以设定稀疏度阈值进行过滤。将小于阈值的卷积核权重设置为0, 逐层反复训练、微调, 直至无法检测到冗余的卷积核, 训练得到收敛的<i>VGG</i>-<i>Net</i>-19_<i>OT</i>网络。在应用场景中有同类背景干扰的情况下, 当目标与背景是同一类物体时, 调整后的网络训练出的特征也能进行区分, 达到很好的跟踪效果。</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103">3.1.4 深度卷积特征提取</h4>
                <div class="p1">
                    <p id="104">预训练的深度卷积网络在不同卷积层提取到不同的特征, 充分利用各个层次的特征可以提升目标跟踪的性能。底层特征具有较高的空间分辨率, 包含丰富的空间特征和纹理信息, 作为类内分类器时, 可以剔除表观相似的干扰背景, 同时易于捕捉位置的变化, 进行精确定位。高层特征包含更多的语义信息, 忽略物体的细节差异, 难以识别或定位较小目标, 作为类间分类器时, 进行粗略定位, 对于目标发生形变、遮挡等表观变化表现比较稳健<citation id="214" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="105">将视频图像目标候选区域输入改进后的预训练网络<i>VGG</i>-<i>Net</i>-19_<i>OT</i>, 并提取出适用于跟踪序列的多层深度卷积特征图, 随着层数加深, 特征图尺寸逐渐变小, 空间分辨率逐步降低。对不同层级的特征图利用双线性插值进行上采样, 获得一系列相同尺寸不同层级的深度特征图, 进而层次化地构造目标外观模型, 设<b><i>h</i></b>为原始特征图, <b><i>x</i></b>为上采样后的特征图, <i>β</i><sub><i>ik</i></sub>是插值系数, 取决于位置<i>i</i>及其<i>k</i>邻域的特征向量, 取决于第<i>i</i>个位置的特征向量:</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>β</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mi>h</mi><msub><mrow></mrow><mi>k</mi></msub><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="107" name="107"><b>3.2 相关滤波框架</b></h4>
                <div class="p1">
                    <p id="108">目标跟踪领域有很多主流的算法, 如KCF, 都是基于核相关滤波框架。此滤波框架利用目标周围区域的循环矩阵采集正负样本, 通过岭回归训练目标分类器, 以循环矩阵在傅里叶空间可对角化的性质, 将矩阵的运算转化为向量的Hadamard积, 降低了运算量, 提高了运算速度。本文算法在此相关滤波理论基础上进行构建。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109">3.2.1 训练阶段</h4>
                <div class="p1">
                    <p id="110">选定跟踪目标, 在给定的目标位置提取训练样本, 将目标跟踪序列第1帧输入改进后的深度卷积网络模型VGG-Net-19_OT, 提取第<i>q</i>层深度卷积特征记为<b><i>x</i></b>, 维度为<i>M</i>×<i>N</i>×<i>D</i>, 采用循环平移矩阵稠密采样的方法, 在训练分类器时, 根据样本<i>x</i><sub><i>ij</i></sub>, <i>i</i>, <i>j</i>∈{0, 1, …, <i>M</i>}×{0, 1, …, <i>N</i>}, 求得训练样本对应的二维高斯分布的回归标签<i>y</i><sub><i>ij</i></sub>, 构造相关滤波的目标函数为<citation id="215" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation><sup></sup></p>
                </div>
                <div class="area_img" id="111">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201907026_11100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="112">式中, <i>λ</i>为正则化参数。利用傅里叶变换得到 (6) 式的闭式解, 第<i>q</i>层<i>d</i>通道分类器权重的频域变换为</p>
                </div>
                <div class="p1">
                    <p id="113" class="code-formula">
                        <mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">Y</mi><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msup></mrow><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup><mi mathvariant="bold-italic">X</mi></mstyle><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msup><mo>+</mo><mi>λ</mi></mrow></mfrac><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="114">式中, <b><i>X</i></b>、<b><i>Y</i></b>分别为<b><i>x</i></b>、<b><i>y</i></b>的频域变换, <mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover></math></mathml>为<b><i>X</i></b>的共轭, ⊙表示Hadamard积。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116">3.2.2 检测阶段</h4>
                <div class="p1">
                    <p id="117">将待检测样本提取出的第<i>q</i>层深度卷积特征构成的循环矩阵<b><i>z</i></b>输入训练好的分类器, 得到回归函数<i>f</i><sub><i>q</i></sub> (<b><i>z</i></b>) :</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo><mo>=</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">W</mi></mstyle><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msup><mo>⊙</mo><mi mathvariant="bold-italic">Ζ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">式中, <font face="EU-HT">F</font><sup>-1</sup>为傅里叶逆变换。首先计算位置 (<i>m</i>, <i>n</i>) 在第<i>q</i>层特征图的响应值, 判断响应值最大的位置为跟踪目标在当前帧的精确位置<mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>m</mi><mo>^</mo></mover><mo>, </mo><mover accent="true"><mi>n</mi><mo>^</mo></mover><mo stretchy="false">) </mo></mrow></math></mathml>:</p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula">
                        <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>m</mi><mo>^</mo></mover><mo>, </mo><mover accent="true"><mi>n</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></munder><mi>f</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="122">再逐层向下搜索位置<mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>m</mi><mo>^</mo></mover><mo>, </mo><mover accent="true"><mi>n</mi><mo>^</mo></mover><mo stretchy="false">) </mo></mrow></math></mathml>的r×r邻域, 由于不同的卷积层的特征描述能力各不相同, 每一层的响应峰值也不一样。可计算第q-1层的响应图, 作更细粒度的位置预测, 逐层计算, 以最低层的预测结果作为最后输出, 计算式为</p>
                </div>
                <div class="p1">
                    <p id="124" class="code-formula">
                        <mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>m</mi><mo>^</mo></mover><mo>, </mo><mover accent="true"><mi>n</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></munder><mstyle displaystyle="true"><munder><mo>∑</mo><mi>q</mi></munder><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><msup><mrow></mrow><mrow><mn>5</mn><mo>-</mo><mi>q</mi></mrow></msup><mi>max</mi><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac></mrow></mstyle><mi>f</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="125" name="125">3.2.3 尺度估计</h4>
                <div class="p1">
                    <p id="126">基于相关滤波框架的目标跟踪算法大都局限于对目标位置的预测, 并未考虑针对运动目标的尺度变化进行估计。本文算法加入尺度相关滤波器, 主要预测流程包括以下几步:首先, 位置相关滤波器定位到目标后, 在其周围采集不同尺度的图像, 构成训练样本;接着, 由于深度卷积特征计算量大, 为保证算法的高效性, 只提取<i>HOG</i>特征;然后, 为保留目标的关键信息及平滑图像的边界效应, 对提取到的特征进行加窗处理;最后, 使用多尺度图像的特征训练核函数最小二乘分类器, 得到尺度相关滤波器, 寻找最大响应, 其对应的尺度就是目标的新尺度。采用可变窗口大小的高斯窗函数代替余弦窗, 通过<mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>σ</mi><msub><mrow></mrow><mi>m</mi></msub><mo>=</mo></mrow><mfrac><mi>m</mi><mi>w</mi></mfrac></mrow></math></mathml>与<mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mfrac><mi>n</mi><mi>h</mi></mfrac></mrow></math></mathml>控制开窗大小, (m, n, w, h) 为位置尺度信息。其中二维高斯窗函数为</p>
                </div>
                <div class="p1">
                    <p id="129" class="code-formula">
                        <mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>G</mi><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo>, </mo><mi>w</mi><mo>, </mo><mi>h</mi><mo stretchy="false">) </mo><mo>=</mo><mi>exp</mi><mrow><mo>{</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mrow><mo>[</mo><mrow><mfrac><mi>i</mi><mrow><mi>σ</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false"> (</mo><mi>m</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>]</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>}</mo></mrow><mo>×</mo></mtd></mtr><mtr><mtd><mi>exp</mi><mrow><mo>{</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mrow><mo>[</mo><mrow><mfrac><mi>j</mi><mrow><mi>σ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>]</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>}</mo></mrow><mo>, </mo></mtd></mtr><mtr><mtd><mn>0</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>m</mi><mo>, </mo><mn>0</mn><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>n</mi><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="130" name="130"><b>3.3 模型更新策略</b></h4>
                <div class="p1">
                    <p id="131">为防止在跟踪过程中, 由于目标表观模型的形变或外部条件的干扰, 而造成漂移现象, 需要对其进行模型更新。令第t帧d通道分类器权重为<mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mrow><mi mathvariant="bold-italic">B</mi><msubsup><mrow></mrow><mtext>t</mtext><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mi>λ</mi></mrow></mfrac></mrow></math></mathml>, 则</p>
                </div>
                <div class="p1">
                    <p id="133" class="code-formula">
                        <mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>η</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mi>η</mi><mi mathvariant="bold-italic">Y</mi><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">B</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>η</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">B</mi><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mi>η</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">X</mi></mstyle><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="134">式中, 学习速率<i>η</i>表征目标的外观模型对新视频图像帧的学习能力。<i>η</i>值越小, 学习速率越慢, 无法及时捕捉目标外观模型变化;<i>η</i>值越大, 学习速率越快, 易受外部噪声干扰。</p>
                </div>
                <div class="p1">
                    <p id="135">对于相关滤波类跟踪算法, 当目标被遮挡时, 如果持续进行模型更新, 会产生累积误差, 导致模型污染, 造成跟踪漂移, 当遮挡逐步消除后, 则很容易误判目标。因此模型更新策略需要重点解决局部遮挡判断问题。</p>
                </div>
                <div class="p1">
                    <p id="136">本文算法采用高置信度的遮挡判断恢复机制, 通过最大响应值<i>F</i><sub>max</sub>、平均峰值相关能量比<i>R</i><sub>APCE</sub>、<i>d</i>通道遮挡因子<i>E</i><sub>OCC<sub><i>d</i></sub></sub>三种指标进行评估。当这三种指标综合判定遮挡发生时, 一方面, 当前模型正常更新, 以适应跟踪目标的表观变化。另一方面, 由于当前模型包含较多目标信息, 当目标重现时仍可对其进行识别, 同时将其保存作为模型备份, 等待遮挡结束后恢复。相关滤波器在判定遮挡已经发生时, 计算当前模型和留存模型备份的响应, 选择最优结果。当模型备份与当前模型响应差距较大且模型备份响应值足够好时, 推断模型备份与当前模型识别不同的目标。若模型备份识别出目标与未遮挡时一致, 此时响应值足够大, 推断遮挡结束目标复现, 用模型备份替换当前模型, 完成整个模型恢复过程。</p>
                </div>
                <h4 class="anchor-tag" id="137" name="137"><b>3.4 算法总体流程</b></h4>
                <div class="p1">
                    <p id="138">结合上述对本文算法中关键部分的描述, 给出算法的主要步骤, 如图3所示。</p>
                </div>
                <div class="area_img" id="230">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907026_23000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 算法流程" src="Detail/GetImg?filename=images/GXXB201907026_23000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907026_23000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Flow chart of algorithm</p>

                </div>
                <h3 id="142" name="142" class="anchor-tag">4 实  验</h3>
                <h4 class="anchor-tag" id="143" name="143"><b>4.1 实验平台及参数配置</b></h4>
                <div class="p1">
                    <p id="144">实验平台硬件配置为CPU:Intel (R) Core (TM) i7-6700, 3.4 GHz, 16 GB内存;GPU:NVIDIA GeForce GTX-1080。软件配置为 MATLAB 2016b 和 C++在 Matconvnet 深度学习库混合编程。算法的正则化参数<i>λ</i>=10<sup>-4</sup>, 高斯核宽<i>σ</i>=0.1。</p>
                </div>
                <h4 class="anchor-tag" id="145" name="145"><b>4.2 评价标准</b></h4>
                <div class="p1">
                    <p id="146">为评估算法的性能, 在 OTB-2015<citation id="216" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>与UAV123<citation id="217" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation> 数据集上进行测试。OTB-2015有100组完全标注的视频, 涵盖11个属性。UAV123有123组完全标注的视频, 涵盖12个属性。各属性包括视频序列个数分别如表2、表3所示。</p>
                </div>
                <div class="area_img" id="147">
                    <p class="img_tit">表2 OTB-2015视频属性 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Video attributes of OTB-2015</p>
                    <p class="img_note"></p>
                    <table id="147" border="1"><tr><td>Video attribute</td><td>Value</td><td>Video attribute</td><td>Value</td></tr><tr><td><br />Background clutters (BC) </td><td>31</td><td>Motion blur (MB) </td><td>29</td></tr><tr><td><br />Deformation</td><td>44</td><td>Occlusion</td><td>49</td></tr><tr><td><br />Fast motion (FM) </td><td>39</td><td>Out-of-plane rotation (OPR) </td><td>63</td></tr><tr><td><br />Illumination variation (IV) </td><td>38</td><td>Out-of-view (OV) </td><td>14</td></tr><tr><td><br />In-plane rotation (IPR) </td><td>51</td><td>Scale variation (SV) </td><td>64</td></tr><tr><td><br />Low resolution (LR) </td><td>9</td><td></td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="148">
                    <p class="img_tit">表3 UAV123视频属性 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Video attributes of UAV123</p>
                    <p class="img_note"></p>
                    <table id="148" border="1"><tr><td>Video attribute</td><td>Value</td><td>Video attribute</td><td>Value</td></tr><tr><td><br />Scale variation (SV) </td><td>109</td><td>Out of view (OV) </td><td>30</td></tr><tr><td><br />Aspect ratio change (ARC) </td><td>68</td><td>Background clutter (BC) </td><td>21</td></tr><tr><td><br />Low resolution (LR) </td><td>48</td><td>Illumination variation (IV) </td><td>31</td></tr><tr><td><br />Fast motion (FM) </td><td>28</td><td>Viewpoint change (VC) </td><td>60</td></tr><tr><td><br />Full occlusion (FOC) </td><td>33</td><td>Camera motion (CM) </td><td>70</td></tr><tr><td><br />Partial occlusion (POC) </td><td>73</td><td>Similar object (SOB) </td><td>39</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="149">选择一次通过评估 (OPE) 方法, 绘制精确度图 (Precision plot) 和成功率图 (Success plot) 。采用4种常见的评估指标<citation id="218" type="reference"><link href="58" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation> :中心位置误差 (CLE) 、距离精度 (DP) 、重叠精度 (OP) 以及平均跟踪帧率 (FPS) 。</p>
                </div>
                <div class="p1">
                    <p id="150">将OTB-2015代码库中自带的ALSA<citation id="219" type="reference"><link href="60" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation> 等算法, 以及近几年主流跟踪算法CFNet<citation id="220" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation> 、CNN-SVM<citation id="221" type="reference"><link href="62" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation> 、DSST<citation id="222" type="reference"><link href="64" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation> 、HCFT<citation id="223" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation> 、HCFTstar<citation id="224" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation> 、KCF<citation id="225" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation> 、LCTDeep<citation id="226" type="reference"><link href="66" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation> 、SRDCF<citation id="227" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation> 等进行定性定量分析。针对OTB-2015数据集, 将排名前10的跟踪算法在精确度图和成功率图上显示。针对UAV123数据集, 仅选择HCFTstar<citation id="228" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation> 、KCF<citation id="229" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation> 与本文算法共3种算法在精确度图和成功率图上比较。</p>
                </div>
                <h4 class="anchor-tag" id="151" name="151"><b>4.3 定性分析</b></h4>
                <div class="p1">
                    <p id="152">基于OTB-2015数据集测试中排名前10的算法的部分跟踪结果如图4、图5所示, 从7个方面进行分析。</p>
                </div>
                <h4 class="anchor-tag" id="153" name="153">4.3.1 背景复杂</h4>
                <div class="p1">
                    <p id="154">以“Ironman 1”视频序列为例, 如图4 (a) 所示。目标运动过程中, 背景与目标极为相似, 多数算法都远离目标, 本文算法与HCFT、HCFTstar算法得益于卷积网络提取的目标稳健性特征描述, 能够跟上目标。</p>
                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907026_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 10个跟踪算法在不同视频序列上的定性结果显示。" src="Detail/GetImg?filename=images/GXXB201907026_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 10个跟踪算法在不同视频序列上的定性结果显示。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907026_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Qualitative results of the 10 tracking algorithms on different video sequences. </p>
                                <p class="img_note"> (a) Ironman 1; (b) Ironman 2; (c) Doll; (d) MotorRolling; (e) Bolt2; (f) Skiing</p>
                                <p class="img_note"> (a) Ironman 1; (b) Ironman 2; (c) Doll; (d) MotorRolling; (e) Bolt2; (f) Skiing</p>

                </div>
                <h4 class="anchor-tag" id="156" name="156">4.3.2 光照变化</h4>
                <div class="p1">
                    <p id="157">如图4 (b) 所示, 在“Ironman 2”中, 由于不断有光束出现, 光照剧烈变化, 对算法的稳健性提出了极大的挑战, 多数算法在跟踪开始就产生跟踪漂移直至失效, 只有本文算法和HCFT、HCFTstar算法能够始终跟踪目标。</p>
                </div>
                <h4 class="anchor-tag" id="158" name="158">4.3.3 尺度变化</h4>
                <div class="p1">
                    <p id="159">在“Doll”中, 如图4 (c) 所示, 由于距离镜头时远时近, 目标在跟踪过程中尺度不断变化, 虽然90%的所选算法都能跟踪目标, 但本文算法能够更好地根据尺度进行调整。</p>
                </div>
                <h4 class="anchor-tag" id="160" name="160">4.3.4 目标旋转</h4>
                <div class="p1">
                    <p id="161">参考图4 (d) 中“MotorRolling”视频序列, 目标在跟踪过程中经历了超过360°的旋转, 对算法提出了很大的挑战。除了本文算法和HCFT、HCFTstar、LCTDeep、CFNet算法, 其余都出现了跟踪失败。</p>
                </div>
                <h4 class="anchor-tag" id="162" name="162">4.3.5 快速运动</h4>
                <div class="p1">
                    <p id="163">图4 (e) 中, 视频序列“Bolt2”是短跑比赛场景, 跟踪目标为其中一名运动员。目标姿态不断变化, 且随着镜头的转动图像中运动员也从正面逐渐转向背面。本文算法由于提取了高层特征, 受目标外观变化的影响不大, 可以始终跟踪目标。</p>
                </div>
                <h4 class="anchor-tag" id="164" name="164">4.3.6 目标分辨率低</h4>
                <div class="p1">
                    <p id="165">以视频“Skiing”为例, 如图4 (f) 所示, 目标分辨率低且尺寸小, 对特征提取的性能提出了更高的要求。由于本文算法从微调后的预训练网络提取到多层深度卷积特征, 能够更好地跟踪到弱小目标。</p>
                </div>
                <h4 class="anchor-tag" id="166" name="166">4.3.7 目标遮挡</h4>
                <div class="p1">
                    <p id="167">选取4组典型的序列“Jogging-1”、“Walking2”、“Coke”和“Soccer”, 目标在跟踪过程中受到部分或全部遮挡, 如图5所示。当目标被遮挡后, 本文算法通过微调后的深度卷积网络提取特征, 使遮挡物和目标的区分性更强, 跟踪上目标, 未出现跟踪漂移。另外, 利用置信度指标判断模型更新机制, 避免在遮挡因素下产生错误模型更新, 在遮挡消失后, 通过恢复备份模型, 更新模型。</p>
                </div>
                <div class="area_img" id="168">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907026_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 10个跟踪算法在部分遮挡视频序列上的定性结果显示。" src="Detail/GetImg?filename=images/GXXB201907026_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 10个跟踪算法在部分遮挡视频序列上的定性结果显示。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907026_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Qualitative results of the 10 tracking algorithms on different occluded video sequences. </p>
                                <p class="img_note"> (a) Jogging-1; (b) Walking2; (c) Coke; (d) Soccer</p>
                                <p class="img_note"> (a) Jogging-1; (b) Walking2; (c) Coke; (d) Soccer</p>

                </div>
                <h4 class="anchor-tag" id="170" name="170"><b>4.4 定量分析</b></h4>
                <div class="p1">
                    <p id="171">为进一步全面评估所提算法的性能, 对OTB-2015与UAV123的测试视频序列的综合性能进行定量分析。</p>
                </div>
                <h4 class="anchor-tag" id="172" name="172">4.4.1 算法综合性能的定量分析</h4>
                <div class="p1">
                    <p id="173">图6是基于OTB-2015数据集排名前10的跟踪算法的精度曲线和成功率曲线, 图例中标注的是每种算法的性能评分。本文算法在所有100段视频上的跟踪精度评分0.845、成功率评分0.751, 在对比算法中表现最好。</p>
                </div>
                <div class="p1">
                    <p id="174">图7是基于UAV123数据集的精度曲线和成功率曲线, 本文算法跟踪精度评分0.688、成功率评分0.581, 与HCFTstar算法性能持平。</p>
                </div>
                <div class="area_img" id="175">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907026_175.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 基于OTB-2015评估基准OPE的跟踪算法。" src="Detail/GetImg?filename=images/GXXB201907026_175.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 基于OTB-2015评估基准OPE的跟踪算法。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907026_175.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Algorithm of OPE on OTB-2015. </p>
                                <p class="img_note"> (a) 精度曲线图; (b) 成功率曲线图</p>
                                <p class="img_note"> (a) Precision plot; (b) overlap success plot</p>

                </div>
                <div class="area_img" id="176">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907026_176.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 基于UAV123评估基准OPE的跟踪算法。" src="Detail/GetImg?filename=images/GXXB201907026_176.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 基于UAV123评估基准OPE的跟踪算法。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907026_176.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Algorithm of OPE on UAV123. </p>
                                <p class="img_note"> (a) 精度曲线图; (b) 成功率曲线图</p>
                                <p class="img_note"> (a) Precision plot; (b) overlap success plot</p>

                </div>
                <h4 class="anchor-tag" id="177" name="177">4.4.2 基于OTB-2015的不同视频属性的定量分析</h4>
                <div class="p1">
                    <p id="178">面对不同挑战, 平均精度和平均成功率结果分别如图8和图9所示。</p>
                </div>
                <div class="p1">
                    <p id="179">本文算法在 11 种不同属性的跟踪挑战中, 跟踪精度始终取得最优或次优的成绩。跟踪成功率在LR、OV属性排名第三, 在SV属性处于次优, 其余属性均排名第一。由此表明, 本文算法可以较好地适应于复杂场景下的目标跟踪任务。</p>
                </div>
                <h4 class="anchor-tag" id="180" name="180">4.4.3 基于UAV123的不同视频属性的定量分析</h4>
                <div class="p1">
                    <p id="181">如表4所示, 本文算法在 12 种不同属性的跟踪挑战中, 跟踪精度、跟踪成功率与HCFTstar算法持平, 特别是在遮挡和部分遮挡序列挑战中可以较好地适应。</p>
                </div>
                <h4 class="anchor-tag" id="182" name="182"><b>4.5 算法跟踪速率</b></h4>
                <div class="p1">
                    <p id="183">跟踪算法对实时性的要求比较高, 任何冗余计算都会影响算法的实用性。由于高维卷积特征需要进行复杂的计算, 这导致VGG-Net提取目标特征用时较多, 本文算法在其模型结构基础上增加目标跟踪分支, 优化筛选更适用于目标跟踪任务的卷积核, 将小于阈值的卷积核权重设置为0, 移除, 逐层训练, 得到VGG-Net-19_OT深度卷积网络模型, 各层有效卷积核的个数约降低为VGG-Net-19的1/9。目标特征提取层参数的减少直接减少了跟踪过程的计算量, 提升了跟踪的速度。本文算法在OTB-2015的100组视频的测试中, CPU模式下平均速度为16.3 frame/s, GPU加速条件下, 平均跟踪速率为29.6 frame/s, 达到实时跟踪要求。表5列举了部分视频序列的跟踪速率。</p>
                </div>
                <div class="area_img" id="184">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907026_184.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 OTB-2015 11种不同属性视频序列跟踪精度曲线" src="Detail/GetImg?filename=images/GXXB201907026_184.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 OTB-2015 11种不同属性视频序列跟踪精度曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907026_184.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Precision plots on 11 different attributes video sequences of OTB-2015</p>

                </div>
                <div class="area_img" id="185">
                    <p class="img_tit">表4 UAV123 12种不同属性视频序列跟踪精度与跟踪成功率 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Precision values and success rates on 12 different attributes video sequences of UAV123</p>
                    <p class="img_note"></p>
                    <table id="185" border="1"><tr><td rowspan="2"><br />Sequence</td><td colspan="2"><br />Proposed algorithm</td><td colspan="2"><br />KCF</td><td colspan="2"><br />HCFTstar</td></tr><tr><td><br />Precision value</td><td>Success rate</td><td><br />Precision value</td><td>Success rate</td><td><br />Precision value</td><td>Success rate</td></tr><tr><td><br />Aspect ratio change (ARC) </td><td>0.619</td><td>0.464</td><td>0.447</td><td>0.292</td><td>0.610</td><td>0.434</td></tr><tr><td><br />Background clutter (BC) </td><td>0.585</td><td>0.447</td><td>0.536</td><td>0.413</td><td>0.584</td><td>0.470</td></tr><tr><td><br />Camera motion (CM) </td><td>0.677</td><td>0.556</td><td>0.502</td><td>0.366</td><td>0.682</td><td>0.543</td></tr><tr><td><br />Fast motion (FM) </td><td>0.544</td><td>0.402</td><td>0.301</td><td>0.200</td><td>0.516</td><td>0.377</td></tr><tr><td><br />Full occlusion (FOC) </td><td>0.567</td><td>0.358</td><td>0.420</td><td>0.243</td><td>0.561</td><td>0.381</td></tr><tr><td><br />Illumination variation (IV) </td><td>0.627</td><td>0.506</td><td>0.464</td><td>0.334</td><td>0.614</td><td>0.451</td></tr><tr><td><br />Low resolution (LR) </td><td>0.555</td><td>0.333</td><td>0.435</td><td>0.251</td><td>0.579</td><td>0.346</td></tr><tr><td><br />Out of view (OV) </td><td>0.609</td><td>0.500</td><td>0.406</td><td>0.277</td><td>0.603</td><td>0.467</td></tr><tr><td><br />Partial occlusion (POC) </td><td>0.632</td><td>0.499</td><td>0.497</td><td>0.365</td><td>0.628</td><td>0.491</td></tr><tr><td><br />Scale variation (SV) </td><td>0.644</td><td>0.534</td><td>0.497</td><td>0.339</td><td>0.646</td><td>0.498</td></tr><tr><td><br />Similar object (SOB) </td><td>0.691</td><td>0.566</td><td>0.616</td><td>0.418</td><td>0.693</td><td>0.552</td></tr><tr><td><br />Viewpoint change (VC) </td><td>0.637</td><td>0.494</td><td>0.450</td><td>0.302</td><td>0.625</td><td>0.440</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="186">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907026_186.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 OTB-2015 11种不同属性视频序列跟踪成功率曲线" src="Detail/GetImg?filename=images/GXXB201907026_186.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 OTB-2015 11种不同属性视频序列跟踪成功率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907026_186.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Success plots on 11 different attributes video sequences of OTB-2015</p>

                </div>
                <div class="p1">
                    <p id="188">表6列出了FCNT、MDNet、HCFT等当前基于深度学习的主流跟踪算法的跟踪速率, 并与本文算法对比。从表中可以看出, 较传统的基于深度学习的跟踪算法, 本文跟踪算法在跟踪速率上有较大的提升, 基本可以满足实时要求。</p>
                </div>
                <div class="p1">
                    <p id="189">综上分析, 本文算法在跟踪过程中, 遇到背景干扰、光照变化、遮挡、尺度变化、快速运动等变化时, 均表现出良好的跟踪性能, 且速度优于其他深度学习类算法。</p>
                </div>
                <h3 id="190" name="190" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="191">本文提出一种结合深度卷积特征和相关滤波框架的抗遮挡实时算法。对卷积网络模型VGG-Net-19结构进行调整, 加入针对目标跟踪任务训练的卷积层, 优化冗余卷积核, 减轻高维卷积特征结构复杂度。通过微调后的深度卷积模型提取目标表观特征, 在维持跟踪精度的同时, 降低了特征提取的计算量, 从而加快了算法运行速度。同时本文算法引入阶段式模型更新恢复机制, 解决了相关滤波跟踪误差随时间积累导致模型污染的问题。实验结果表明, 与近年来的主流算法相比, 本文算法不仅得到较高的跟踪精度, 在目标遮挡、外观变化、背景干扰等复杂场景下也有稳健的表现, 而且平均跟踪速度可达到29.6 frame/s, 满足实际应用的要求。</p>
                </div>
                <div class="area_img" id="192">
                    <p class="img_tit">表5 跟踪速率 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Tracking speeds</p>
                    <p class="img_note"> frame /s</p>
                    <table id="192" border="1"><tr><td>Sequence</td><td>Basketball</td><td>FaceOcc1</td><td>Football1</td><td>Girl</td><td>Jogging1</td><td>Jumping</td><td>Soccer</td><td>Sylvester</td><td>Trellis</td></tr><tr><td><br />Speed</td><td>31.3</td><td>34.5</td><td>26.1</td><td>35.9</td><td>28.2</td><td>26.1</td><td>25.1</td><td>32.6</td><td>27.9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="193">
                    <p class="img_tit">表6 基于深度学习的跟踪算法的平均跟踪速率对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 6 Average tracking speed comparison for the deep learning-based tracking algorithm</p>
                    <p class="img_note"> frame /s</p>
                    <table id="193" border="1"><tr><td>Algorithm</td><td>Proposed</td><td>FCNT</td><td>MDNet</td><td>HCFT</td></tr><tr><td><br />Tracking speed</td><td>29.6</td><td>3</td><td>1</td><td>10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">

                                <b>[1]</b> Bolme D S, Beveridge J R, Draper B A, <i>et al</i>.Visual object tracking using adaptive correlation filters[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:2544-2550.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with Kernels">

                                <b>[2]</b> Henriques J F, Caseiro R, Martins P, <i>et al</i>.Exploiting the circulant structure of tracking-by-detection with kernels[M]//Fitzgibbon A, Lazebnik S, Perona P, <i>et al</i>.Lecture notes in computer science.Berlin, Heidelberg:Springer, 2012, 7575:702-715.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive color attributes for real-time visual tracking">

                                <b>[3]</b> Danelljan M, Khan F S, Felsberg M, <i>et al</i>.Adaptive color attributes for real-time visual tracking[C]∥2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:1090-1097.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" >
                                    <b>[4]</b>
                                 Henriques J F, Caseiro R, Martins P, <i>et al</i>.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">

                                <b>[5]</b> Danelljan M, Häger G, Khan F S, <i>et al</i>.Learning spatially regularized correlation filters for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:4310-4318.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201711027&amp;v=MzAyMTh0R0ZyQ1VSTE9lWmVWdUZ5bmhVTHpCSWpYVGJMRzRIOWJOcm85SFk0UUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Wang X, Hou Z Q, Yu W S, <i>et al</i>.Target scale adaptive robust tracking based on fusion of multilayer convolutional features[J].Acta Optica Sinica, 2017, 37 (11) :1115005.王鑫, 侯志强, 余旺盛, 等.基于多层卷积特征融合的目标尺度自适应稳健跟踪[J].光学学报, 2017, 37 (11) :1115005.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201703032&amp;v=MjU3MzlCSWpYVGJMRzRIOWJNckk5R1pvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmhVTHo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Cai Y Z, Yang D D, Mao N, <i>et al</i>.Visual tracking algorithm based on adaptive convolutional features[J].Acta Optica Sinica, 2017, 37 (3) :0315002.蔡玉柱, 杨德东, 毛宁, 等.基于自适应卷积特征的目标跟踪算法[J].光学学报, 2017, 37 (3) :0315002.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201805026&amp;v=MTY5ODc0SDluTXFvOUhZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5oVUx6QklqWFRiTEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Li C, Lu C Y, Zhao X, <i>et al</i>.Scale adaptive correlation filtering tracing algorithm based on feature fusion[J].Acta Optica Sinica, 2018, 38 (5) :0515001.李聪, 鹿存跃, 赵珣, 等.特征融合的尺度自适应相关滤波跟踪算法[J].光学学报, 2018, 38 (5) :0515001.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201901035&amp;v=MTcyNTJPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmhVTHpCSWpYVGJMRzRIOWpNcm85R1lZUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Wang H Y, Wang L, Yin W R, <i>et al</i>.Multi-scale correlation filtering visual tracking algorithm combined with target detection[J].Acta Optica Sinica, 2019, 39 (1) :0115004.王红雨, 汪梁, 尹午荣, 等.结合目标检测的多尺度相关滤波视觉跟踪算法[J].光学学报, 2019, 39 (1) :0115004.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a deep compact image representation for visual tracking">

                                <b>[10]</b> Wang N Y, Yeung D Y.Learning a deep compact image representation for visual tracking[C]//Proceedings of the 26th International Conference on Neural Information Processing Systems, December 5-10, 2013, Lake Tahoe, Nevada.USA:Curran Associates Inc., 2013, 1:809-817.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transferring rich feature hierarchies for robust visual tracking">

                                <b>[11]</b> Wang N, Li S, Gupta A, <i>et al</i>.Transferring rich feature hierarchies for robust visual tracking[EB/OL]. (2015-04-23) [2019-01-06].https://arxiv.org/abs/1501.04587.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical convolutional features for visual tracking">

                                <b>[12]</b> Ma C, Huang J B, Yang X K, <i>et al</i>.Hierarchical convolutional features for visual tracking[C]∥2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:3074-3082.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust visual tracking via hierarchical convolutional features">

                                <b>[13]</b> Ma C, Huang J B, Yang X K, <i>et al</i>.Robust visual tracking via hierarchical convolutional features[J].IEEE Transactions on Pattern Analysis and Machine Intelligence (Early Access) , (2018-08-13) [2019-01-06].DOI:10.1109/TPAMI.2018.2865311.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual Tracking with Fully Convolutional Networks">

                                <b>[14]</b> Wang L J, Ouyang W L, Wang X G, <i>et al</i>.Visual tracking with fully convolutional networks[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:3119-3127.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning multi-domain convolutional neural networks for visual tracking">

                                <b>[15]</b> Nam H, Han B.Learning multi-domain convolutional neural networks for visual tracking[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:4293-4302.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully-convolutional siamese networks for object tracking">

                                <b>[16]</b> Bertinetto L, Valmadre J, Henriques J F, <i>et al</i>.Fully-convolutional Siamese networks for object tracking[M]//Hua G, Jégou H.Lecture notes in computer science.Cham:Springer, 2016, 9914:850-865.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-End Representation Learning for Correlation Filter Based Tracking">

                                <b>[17]</b> Valmadre J, Bertinetto L, Henriques J, <i>et al</i>.End-to-end representation learning for correlation filter based tracking[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:5000-5008.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Siamese Instance Search for Tracking">

                                <b>[18]</b> Tao R, Gavves E, Smeulders A W M.Siamese instance search for tracking[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1420-1429.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_19" >
                                    <b>[19]</b>
                                 LeCun Y, Bengio Y, Hinton G.Deep learning[J].Nature, 2015, 521 (7553) :436-444.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[20]</b> Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2019-01-06].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet:a large-scale hierarchical image database">

                                <b>[21]</b> Deng J, Dong W, Socher R, <i>et al</i>.ImageNet:a large-scale hierarchical image database[C]∥2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 20-25, 2009, Miami, FL, USA.New York:IEEE, 2009:248-255.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:single shot multibox detector">

                                <b>[22]</b> Liu W, Anguelov D, Erhan D, <i>et al</i>.SSD:single shot multibox detector[C]//Leibe B, Matas J, Sebe N, <i>et al</i>.Lecture notes in computer science.Cham:Springer, 2016, 9905:21-37.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">

                                <b>[23]</b> Wu Y, Lim J, Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1834-1848.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Benchmark and Simulator for UAV Tracking">

                                <b>[24]</b> Mueller M, Smith N, Ghanem B.A benchmark and simulator for UAV tracking[M]//Leibe B, Matas J, Sebe N, <i>et al</i>.Lecture notes in computer science.Cham:Springer, 2016, 9905:445-461.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online object tracking:A benchmark">

                                <b>[25]</b> Wu Y, Lim J, Yang M H.Online object tracking:a benchmark[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2013, Portland, OR, USA.New York:IEEE, 2013:2411-2418.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual tracking via adaptive structural local sparse appearance model">

                                <b>[26]</b> Jia X, Lu H C, Yang M H.Visual tracking via adaptive structural local sparse appearance model[C]∥2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:1822-1829.
                            </a>
                        </p>
                        <p id="62">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online tracking by learning discriminative saliency map with convolutional neural network">

                                <b>[27]</b> Hong S, You T, Kwak S, <i>et al</i>.Online tracking by learning discriminative saliency map with convolutional neural network[C]//Proceedings of the 32nd international Conference on Machine Learning, July 6-11, 2015, Lille, France.Massachusetts:JMLR.org, 2015, 37:597-606.
                            </a>
                        </p>
                        <p id="64">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate Scale Estimation for Robust Visual Tracking">

                                <b>[28]</b> Danelljan M, Häger G, Khan F S, <i>et al</i>.Accurate scale estimation for robust visual tracking[C]∥Proceedings of the British Machine Vision Conference 2014, September 1-5, 2014, Nottingham.Durham, England, UK:BMVA Press, 2014:1-11.
                            </a>
                        </p>
                        <p id="66">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive correlation filters with long-term and short-term memory for object tracking">

                                <b>[29]</b> Ma C, Huang J B, Yang X K, <i>et al</i>.Adaptive correlation filters with long-term and short-term memory for object tracking[J].International Journal of Computer Vision, 2018, 126 (8) :771-796.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201907026" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201907026&amp;v=MjM1ODVMRzRIOWpNcUk5SFlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmhVTHpPSWpYVGI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

