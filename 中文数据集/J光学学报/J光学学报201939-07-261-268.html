

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133864139346250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201907029%26RESULT%3d1%26SIGN%3dOHRsqXLbC5X44RpKE6jfzFuiqPk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201907029&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201907029&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201907029&amp;v=MDQ4ODVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5oVWIzTElqWFRiTEc0SDlqTXFJOUhiWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#46" data-title="2 基于最大Gabor相似度的人脸姿态校正 ">2 基于最大Gabor相似度的人脸姿态校正</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#47" data-title="&lt;b&gt;2.1 用加权LK算法求每块人脸的仿射变换参数&lt;/b&gt;"><b>2.1 用加权LK算法求每块人脸的仿射变换参数</b></a></li>
                                                <li><a href="#69" data-title="&lt;b&gt;2.2 基于最大Gabor相似度寻找每块全局最优的仿射变换参数&lt;/b&gt;"><b>2.2 基于最大Gabor相似度寻找每块全局最优的仿射变换参数</b></a></li>
                                                <li><a href="#82" data-title="&lt;b&gt;2.3 基于加权的分块Gabor向量人脸识别&lt;/b&gt;"><b>2.3 基于加权的分块Gabor向量人脸识别</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#104" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#109" data-title="&lt;b&gt;3.1 评估得到的每块最优参数&lt;/b&gt;"><b>3.1 评估得到的每块最优参数</b></a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;3.2 通过每块最优参数校正人脸&lt;/b&gt;"><b>3.2 通过每块最优参数校正人脸</b></a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;3.3 侧脸的识别率&lt;/b&gt;"><b>3.3 侧脸的识别率</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#121" data-title="4 结  论 ">4 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#49" data-title="图1 (a) 侧面和 (b) 正面人脸的分块示意图">图1 (a) 侧面和 (b) 正面人脸的分块示意图</a></li>
                                                <li><a href="#111" data-title="图2 基于最优参数的Gabor相似度表现。">图2 基于最优参数的Gabor相似度表现。</a></li>
                                                <li><a href="#117" data-title="图3 基于Gabor最优参数和像素差最优参数校正后的人脸对比">图3 基于Gabor最优参数和像素差最优参数校正后的人脸对比</a></li>
                                                <li><a href="#118" data-title="表1 若干方法在FERET人脸数据库中的识别率对比">表1 若干方法在FERET人脸数据库中的识别率对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Zhang X Z, Gao Y S.Face recognition across pose:a review[J].Pattern Recognition, 2009, 42 (11) :2876-2896." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738739&amp;v=MzA0NzlGWStnSEMzOHdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lJVm9XYUJFPU5pZk9mYks3SHRETnFZOQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Zhang X Z, Gao Y S.Face recognition across pose:a review[J].Pattern Recognition, 2009, 42 (11) :2876-2896.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Tang H, Yin B, Sun Y, &lt;i&gt;et al&lt;/i&gt;.Pose-invariant face recognition based on a single view [J].Journal of Information and Computer Science, 2010, 7 (12) :2369-2379." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pose-invariant face recognition based on a single view">
                                        <b>[2]</b>
                                         Tang H, Yin B, Sun Y, &lt;i&gt;et al&lt;/i&gt;.Pose-invariant face recognition based on a single view [J].Journal of Information and Computer Science, 2010, 7 (12) :2369-2379.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" Tong Y, Wei Y M, Shen Y H.Supervised sparsity preserving projection based on global constraint[J].Acta Optica Sinica, 2018, 38 (9) :0910001.童莹, 魏以民, 沈越泓.基于全局约束的监督稀疏保持投影降维方法研究[J].光学学报, 2018, 38 (9) :0910001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201809016&amp;v=MjQ3MzQ5RVlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmhVYjNMSWpYVGJMRzRIOW5NcG8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Tong Y, Wei Y M, Shen Y H.Supervised sparsity preserving projection based on global constraint[J].Acta Optica Sinica, 2018, 38 (9) :0910001.童莹, 魏以民, 沈越泓.基于全局约束的监督稀疏保持投影降维方法研究[J].光学学报, 2018, 38 (9) :0910001.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Chai X J, Shan S G, Chen X L, &lt;i&gt;et al&lt;/i&gt;.Local linear regression (LLR) for pose invariant face recognition[C]//7th International Conference on Automatic Face and Gesture Recognition (FGR06) , April 10-12, 2006, Southampton, UK.New York:IEEE, 2006:631-636." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Local Linear Regression (LLR) for Pose Invariant Face Recognition">
                                        <b>[4]</b>
                                         Chai X J, Shan S G, Chen X L, &lt;i&gt;et al&lt;/i&gt;.Local linear regression (LLR) for pose invariant face recognition[C]//7th International Conference on Automatic Face and Gesture Recognition (FGR06) , April 10-12, 2006, Southampton, UK.New York:IEEE, 2006:631-636.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" Ashraf A B, Lucey S, Chen T.Learning patch correspondences for improved viewpoint invariant face recognition[C]//2008 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2008, Anchorage, AK, USA.New York:IEEE, 2008:4587754." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning patch correspondences for improved viewpoint invariant face recognition">
                                        <b>[5]</b>
                                         Ashraf A B, Lucey S, Chen T.Learning patch correspondences for improved viewpoint invariant face recognition[C]//2008 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2008, Anchorage, AK, USA.New York:IEEE, 2008:4587754.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Ho H T, Chellappa R.Pose-invariant face recognition using Markov random fields[J].IEEE Transactions on Image Processing, 2013, 22 (4) :1573-1584." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pose-Invariant Face Recognition Using Markov Random Fields">
                                        <b>[6]</b>
                                         Ho H T, Chellappa R.Pose-invariant face recognition using Markov random fields[J].IEEE Transactions on Image Processing, 2013, 22 (4) :1573-1584.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" Li J, Zhao J, Zhao F, &lt;i&gt;et al&lt;/i&gt;.Robust face recognition with deep multi-view representation learning[C]//Proceedings of the 24th ACM International Conference on Multimedia, October 15-19, 2016, Amsterdam, Netherlands.New York:ACM, 2016:1068-1072." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust face recognition with deep multi-view representation learning">
                                        <b>[7]</b>
                                         Li J, Zhao J, Zhao F, &lt;i&gt;et al&lt;/i&gt;.Robust face recognition with deep multi-view representation learning[C]//Proceedings of the 24th ACM International Conference on Multimedia, October 15-19, 2016, Amsterdam, Netherlands.New York:ACM, 2016:1068-1072.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" Yim J, Jung H, Yoo B, &lt;i&gt;et al&lt;/i&gt;.Rotating your face using multi-task deep neural network[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:676-684." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rotating Your Face Using Multi-task Deep Neural Network">
                                        <b>[8]</b>
                                         Yim J, Jung H, Yoo B, &lt;i&gt;et al&lt;/i&gt;.Rotating your face using multi-task deep neural network[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:676-684.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" Peng X, Yu X, Sohn K, &lt;i&gt;et al&lt;/i&gt;.Reconstruction-based disentanglement for pose-invariant face recognition[C]//2017 IEEE International Conference on Computer Vision (ICCV) , October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:1632-1641." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reconstruction-based disentanglement for pose-invariant face recognition">
                                        <b>[9]</b>
                                         Peng X, Yu X, Sohn K, &lt;i&gt;et al&lt;/i&gt;.Reconstruction-based disentanglement for pose-invariant face recognition[C]//2017 IEEE International Conference on Computer Vision (ICCV) , October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:1632-1641.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" Liu C J, Wechsler H.Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition[J].IEEE Transactions on Image Processing, 2002, 11 (4) :467-476." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition">
                                        <b>[10]</b>
                                         Liu C J, Wechsler H.Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition[J].IEEE Transactions on Image Processing, 2002, 11 (4) :467-476.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" Lucas B D, Kanade T.An iterative image registration technique with an application to stereo vision[C]//Proceedings of 7th International Joint Conference on Artificial Intelligence, August 24-28, 1981, Vancouver, British Columbia.San Francisco:Margan Kaufmann Publishers Inc., 1981:674-679." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An iterative image registration technique with an application to stereo vision">
                                        <b>[11]</b>
                                         Lucas B D, Kanade T.An iterative image registration technique with an application to stereo vision[C]//Proceedings of 7th International Joint Conference on Artificial Intelligence, August 24-28, 1981, Vancouver, British Columbia.San Francisco:Margan Kaufmann Publishers Inc., 1981:674-679.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" Ashraf A B, Lucey S, Chen T.Fast image alignment in the Fourier domain[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:2480-2487." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast image alignment in the Fourier domain">
                                        <b>[12]</b>
                                         Ashraf A B, Lucey S, Chen T.Fast image alignment in the Fourier domain[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:2480-2487.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" Zhang W C, Shan S G, Gao W, &lt;i&gt;et al&lt;/i&gt;.Local Gabor binary pattern histogram sequence (LGBPHS) :a novel non-statistical model for face representation and recognition[C]//Tenth IEEE International Conference on Computer Vision (ICCV′05) Volume 1, October 17-21, 2005, Beijing, China.New York:IEEE, 2005:786-791." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Local Gabor Binary Pattern Histogram Sequence (LGBPHS): a Novel Non-Statistical Model for Face Representation and Recognition">
                                        <b>[13]</b>
                                         Zhang W C, Shan S G, Gao W, &lt;i&gt;et al&lt;/i&gt;.Local Gabor binary pattern histogram sequence (LGBPHS) :a novel non-statistical model for face representation and recognition[C]//Tenth IEEE International Conference on Computer Vision (ICCV′05) Volume 1, October 17-21, 2005, Beijing, China.New York:IEEE, 2005:786-791.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" Phillips P J, Moon H, Rizvi S A, &lt;i&gt;et al&lt;/i&gt;.The FERET evaluation methodology for face-recognition algorithms[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22 (10) :1090-1104." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The FERET Evaluation Methodology for Face-Recognition Algorithms">
                                        <b>[14]</b>
                                         Phillips P J, Moon H, Rizvi S A, &lt;i&gt;et al&lt;/i&gt;.The FERET evaluation methodology for face-recognition algorithms[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22 (10) :1090-1104.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" Sim T, Baker S, Bsat M.The CMU pose, illumination, and expression database[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2003, 25 (12) :1615-1618." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The CMU pose, illumination, and expression database">
                                        <b>[15]</b>
                                         Sim T, Baker S, Bsat M.The CMU pose, illumination, and expression database[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2003, 25 (12) :1615-1618.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-16 17:32</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(07),261-268 DOI:10.3788/AOS201939.0715005            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于Lucas-Kanade算法的最大Gabor相似度大姿态人脸识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A8%8B%E8%B6%85&amp;code=28340634&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">程超</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%BE%BE%E9%A3%9E%E9%B9%8F&amp;code=06593251&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">达飞鹏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E8%BE%B0%E6%98%9F&amp;code=27625788&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王辰星</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9C%E6%98%8C%E9%87%91&amp;code=06615128&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姜昌金</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%9C%E5%8D%97%E5%A4%A7%E5%AD%A6%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=0220478&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">东南大学自动化学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%9C%E5%8D%97%E5%A4%A7%E5%AD%A6%E5%A4%8D%E6%9D%82%E5%B7%A5%E7%A8%8B%E7%B3%BB%E7%BB%9F%E6%B5%8B%E9%87%8F%E4%B8%8E%E6%8E%A7%E5%88%B6%E6%95%99%E8%82%B2%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">东南大学复杂工程系统测量与控制教育部重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在人脸识别科学研究和实际应用领域中, 大角度姿态是影响人脸识别结果的主要因素之一, 成为限制人脸识别技术进步的难点, 而姿态的校正归一化是解决该问题的常用手段。首先通过加权的LK (Lucas-Kanade) 算法得到侧脸块和对应正脸块的仿射变换参数, 基于最大Gabor相似度寻找校正人脸姿态的最优参数。然后, 以每一人脸块最优参数得到的平均Gabor相似度作为这一块人脸的识别权重, 可以增加大姿态人脸识别的精度和稳健性。在FERET人脸数据库中进行了实验, 当水平偏转角度为45°时, 准确率达到97.3%, 证明本文提出的以最大Gabor相似度作为加权LK算法参数提取的依据是有效的, 得到的最优参数具有较好的光照无关性, 而将平均Gabor相似度作为识别权重, 有助于使算法的应用更加稳健和有效。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人脸识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E5%A4%A7Gaobr%E7%9B%B8%E4%BC%BC%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最大Gaobr相似度;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A0%E6%9D%83LK%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">加权LK算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E8%84%B8%E5%88%86%E5%9D%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人脸分块;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Gabor%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Gabor特征;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *王辰星, E-mail:cxwang@seu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-12</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61828501, 61462072, 61628304);</span>
                    </p>
            </div>
                    <h1><b>Pose Invariant Face Recognition Using Maximum Gabor Similarity Based on Lucas-Kanade Algorithm</b></h1>
                    <h2>
                    <span>Cheng Chao</span>
                    <span>Da Feipeng</span>
                    <span>Wang Chenxing</span>
                    <span>Jiang Changjin</span>
            </h2>
                    <h2>
                    <span>School of Automation, Southeast University</span>
                    <span>Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In the field of face recognition, pose variation is one of the significant challenges that affects the recognition performance and has been one of the major obstacles hindering the improvement of the face recognition technology. In this study, affine transformation parameters of side face and full-frontal face patches are obtained by applying the weighted Lucas-Kanade (LK) algorithm. We further propose that an optimal parameter for correcting face pose can be obtained based on the maximum Gabor similarity. Furthermore, the average Gabor similarity acquired from the optimal parameter of each face patch can be considered to be the face recognition weight, improving the recognition rate and enhancing the robustness of the pose invariant face recognition. Finally, the experimental results obtained based on the FERET face database denote that the recognition rate for the image with a pose of 45° can reach up to 97.3%, indicating that the usage of the maximum Gabor similarity as a basis for parameter extraction of the weighted LK algorithm is valid. This method can also handle illumination variations. Considering the average Gabor similarity as the recognition weight will ensure the robust and effective application of this algorithm.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=face%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">face recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=maximum%20Gabor%20similarity&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">maximum Gabor similarity;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=weighted%20Lucas-Kanade%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">weighted Lucas-Kanade algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=face%20patches&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">face patches;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Gabor%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Gabor feature;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-12-12</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="42">人脸识别技术是当代人工智能、模式识别和计算机视觉领域中热门的研究课题之一。在实际生活和科学应用中, 通过人脸识别检测, 得出大角度姿态是影响人脸识别结果的主要因素的结论。当输入的人脸图像是一张水平旋转角度较大的侧脸图像时, 即使是同一个人的两幅图像, 不同姿态反映的图像也会有很大的差异, 这种差异可能会比相同姿态而不同身份的人的差异还要大<citation id="123" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。这使得很多常规人脸识别算法的性能大幅下降, 进而导致识别率也明显降低。</p>
                </div>
                <div class="p1">
                    <p id="43">针对大角度人脸姿态的主流研究方法主要集中在三维人脸模板和二维平面技术。在三维人脸模板方面, Tang等<citation id="124" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>利用二维人脸重构三维虚拟人脸, 然后将三维人脸进行二维投影, 构建二维多姿态人脸库, 将多姿态人脸识别问题转化为某一角度下两幅相同姿态照片的对比问题<citation id="125" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。这种采用二维人脸重构三维虚拟人脸的思路, 在一定程度上改善了姿态变化对人脸识别的影响, 但三维人脸重建运算较复杂, 计算量较大, 难以实现实时性的要求。</p>
                </div>
                <div class="p1">
                    <p id="44">在二维平面技术方面, Chai等<citation id="126" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出了局部线性回归算法 (LLR) , 该算法假设人脸侧脸与正脸呈局部线性关系, 用侧脸图像与其对应的正脸图像进行训练, 得到侧脸到正脸之间的线性映射关系, 进而由侧脸估计出其对应的正脸图像。为解决侧脸到正脸的非线性问题, LLR算法把人脸分成了很多个小区域, 并对每个小区域分别进行线性映射, 这必然会导致非线性信息的丢失。Ashraf等<citation id="127" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>基于LK (Lucas-Kanade) 算法提出了人脸区域的流堆栈 (StackFlow) 算法, 该算法首先将人脸划分为若干块, 用仿射变换进行侧脸到正脸的校正;训练过程中使用LK算法, 经仿射变化后的侧脸与正脸的误差最小;作者提出采用流堆栈算法来获得每块人脸的全局最优, 即某一块的全局最优应该是使所有训练图像的对应块和正面人脸的对应块差值和最小的仿射变换参数, 但是这种方法是根据像素寻找全局最优参数, 所以对人脸区域的划分准确度要求较高, 同时由于姿态、光照和表情等因素不完全相同, 合成的正面人脸比较模糊, 导致识别率较低。类似地, Ho等<citation id="128" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出了一种通过马尔可夫随机场 (Markov Random Fields, MRF) 寻找仿射变换参数的方法, 该方法的目标函数与流堆栈算法相同, 但是其每块人脸全局最优参数的确定是通过马尔可夫随机场实现的。通过构建代价函数, 实现代价函数的最小化来选择最优参数, 并运用BP (Belief Propagation) 算法求出最优参数。但是在确定每一块最优参数时, 运行BP算法非常耗时, 同时确定最优参数是在某一张人脸上运行的, 因此不可避免地会带上该人脸的特性信息。Li等<citation id="129" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出了基于神经网络的深度多视角特征学习 (MVRL) 方法, 通过不同的深度学习模型来得到不同特征, 进而融合为一个整体特征作为此人脸特征的表示。Yim等<citation id="130" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出多任务的深度神经网络 (CPF) 方法来实现人脸校正, 主任务用于从输入人脸图像构建任意姿态的人脸图像, 副任务用于从得到的人脸图像再反过来构造输入图像, 以实现整个神经网络的反馈。Peng等<citation id="131" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出了基于深度神经网络学习的姿态校正 (Multi-Source Multi-Task, MSMT) 方法, 首先通过正面的二维人脸生成一个三维模板, 然后投影生成不同姿态的二维人脸, 再在训练识别模型时将身份信息与非身份信息 (如姿态和特征点) 放在一起训练, 得到一个包含姿态、特征点和身份的多特征, 最后把侧脸的姿态特征和正面人脸的身份特征拼接在一起重新构建, 以保证不同姿态的人脸得到的特征是相似的。但是基于神经网络的算法需要的训练集大, 同时需要手工标定特征点, 在训练阶段比较耗时。</p>
                </div>
                <div class="p1">
                    <p id="45">本文主要研究人脸姿态的二维平面校正方法, 采用基于加权的LK算法得到每块人脸的最优仿射变换参数, 提出每一块的最优仿射变换参数应使所有训练集中这一块校正的人脸块与对应正脸块Gabor<sup></sup><citation id="132" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>相似度最大, 并以每一人脸块最优参数得到的平均Gabor相似度作为这一块人脸的权重进行识别。最后, 在FERET人脸数据库中进行了每块最优参数的评估和侧脸姿态的校正实验, 同时进行了识别率实验, 证明了所提方法的有效性。</p>
                </div>
                <h3 id="46" name="46" class="anchor-tag">2 基于最大Gabor相似度的人脸姿态校正</h3>
                <h4 class="anchor-tag" id="47" name="47"><b>2.1 用加权LK算法求每块人脸的仿射变换参数</b></h4>
                <div class="p1">
                    <p id="48">LK算法<citation id="133" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>是一种有效的、实现图片对齐的算法。但是在侧脸角度情况下, 人脸各个区域变化的形态不同, 如果对整个人脸都运用LK算法, 则各个人脸区域的对齐效果较差, 所以首先要对某一个特定角度的侧面和对应的正面人脸进行分块。通过Gabor滤波器的尺度和方向对图像进行滤波, 将其结果作为纹理特征来选择全局最优参数。该过程基于提取到的人脸局部空间信息而不是每个点的像素差值信息, 因此用于校正的侧脸和正脸分块并不需要精确的点对点对应, 而是大致的局部区域对应即可。侧面和正面人脸的分块示意图如图1所示, 首先将正脸划分成<i>N</i>块矩形区域, 然后根据正脸区域的划分, 在侧脸上标定得到大致的对应区域。</p>
                </div>
                <div class="area_img" id="49">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907029_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 (a) 侧面和 (b) 正面人脸的分块示意图" src="Detail/GetImg?filename=images/GXXB201907029_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 (a) 侧面和 (b) 正面人脸的分块示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907029_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Diagrams of side face and full-frontal face patches</p>

                </div>
                <div class="p1">
                    <p id="50">用<i>I</i>表示侧脸的图像, 用<i>T</i>表示对应的正脸图像, 下标<i>r</i>表示块的编号。采用仿射变换来实现侧脸到正脸各部分的对齐。设仿射变换矩阵为</p>
                </div>
                <div class="p1">
                    <p id="51" class="code-formula">
                        <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">A</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>1</mn><mo>+</mo><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub></mtd><mtd><mi>p</mi><msub><mrow></mrow><mn>3</mn></msub></mtd><mtd><mi>p</mi><msub><mrow></mrow><mn>5</mn></msub></mtd></mtr><mtr><mtd><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub></mtd><mtd><mn>1</mn><mo>+</mo><mi>p</mi><msub><mrow></mrow><mn>4</mn></msub></mtd><mtd><mi>p</mi><msub><mrow></mrow><mn>6</mn></msub></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="52">仿射变换参数<b><i>p</i></b>= (<i>p</i><sub>1</sub>, <i>p</i><sub>2</sub>, <i>p</i><sub>3</sub>, <i>p</i><sub>4</sub>, <i>p</i><sub>5</sub>, <i>p</i><sub>6</sub>) <sup>T</sup>, 用<i>W</i>表示仿射变换。则一个坐标<i>x</i>仿射变换后得到<i>x</i>′, 则<i>x</i>′=<i>W</i> (<i>x</i>, <b><i>p</i></b>) 。目标函数可表示为</p>
                </div>
                <div class="p1">
                    <p id="53" class="code-formula">
                        <mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo>=</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>x</mi></munder><mo stretchy="false">{</mo></mstyle><mi>Ι</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">[</mo><mi>W</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>-</mo><mi>Τ</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="54">使目标函数最小时对应的<b><i>p</i></b>, 即可认为是最优的姿态校正参数。 (2) 式的求解可采用加权的LK算法<citation id="134" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>来实现。加权的LK算法引入二维Gabor变换<citation id="135" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>来计算权值, 以适应光照变化。二维Gabor变换的定义为</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>g</mi><msub><mrow></mrow><mrow><mi>μ</mi><mo>, </mo><mi>ν</mi></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">k</mi><msub><mrow></mrow><mrow><mi>μ</mi><mo>, </mo><mi>ν</mi></mrow></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mi>exp</mi><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">k</mi><msub><mrow></mrow><mrow><mi>μ</mi><mo>, </mo><mi>ν</mi></mrow></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>) </mo></mrow><mo>⋅</mo></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mrow><mi>exp</mi><mrow><mo> (</mo><mrow><mtext>i</mtext><mi mathvariant="bold-italic">k</mi><msub><mrow></mrow><mrow><mi>μ</mi><mo>, </mo><mi>ν</mi></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">z</mi></mrow><mo>) </mo></mrow><mo>-</mo><mi>exp</mi><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow></mrow><mo>]</mo></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56">式中:<b><i>z</i></b>= (<i>x</i>, <i>y</i>) <sup>T</sup>为空间位置坐标;参数<i>σ</i>=2π;<b><i>k</i></b><sub><i>μ</i>, <i>ν</i></sub>用来确定Gabor内核的方向和尺度, <i>μ</i>, <i>ν</i>表示Gabor变换方向和尺度变化, 每一组<i>μ</i>, <i>ν</i>参数都会进行一次Gabor变换, 从而得到相应的Gabor系数, 经过<i>μ</i>×<i>ν</i>个Gabor变换后, 可在不同粗细粒度上得到图像的有效特征。</p>
                </div>
                <div class="p1">
                    <p id="57">假设有<i>M</i>个Gabor变换, 用<i>g</i><sub><i>i</i></sub>表示第<i>i</i>个Gabor变换, 则目标函数为</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>E</mi><mo>=</mo><mo stretchy="false">∥</mo><mo stretchy="false">{</mo><mi>g</mi><msub><mrow></mrow><mn>1</mn></msub><mo>*</mo><mi>Ι</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">[</mo><mi>W</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>g</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo>*</mo><mi>Ι</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">[</mo><mi>W</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo>-</mo></mtd></mtr><mtr><mtd><mo stretchy="false">[</mo><mi>g</mi><msub><mrow></mrow><mn>1</mn></msub><mo>*</mo><mi>Τ</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>g</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo>*</mo><mi>Τ</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">式中:*代表卷积。将 (4) 式变换到傅里叶频域, 即</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>E</mi><mo>=</mo><mo stretchy="false">{</mo><mi>Ι</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">[</mo><mi>W</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>-</mo><mi>Τ</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mi mathvariant="bold-italic">F</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">F</mi><mo>⋅</mo></mtd></mtr><mtr><mtd><mo stretchy="false">{</mo><mi>Ι</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">[</mo><mi>W</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>-</mo><mi>Τ</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61">式中:<mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mo stretchy="false">[</mo></mstyle><mtext>d</mtext><mtext>i</mtext><mtext>a</mtext><mtext>g</mtext><mo stretchy="false"> (</mo><mover accent="true"><mi>g</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mtext>d</mtext><mtext>i</mtext><mtext>a</mtext><mtext>g</mtext><mo stretchy="false"> (</mo><mover accent="true"><mi>g</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>, 其中<mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>g</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>为g<sub>i</sub>对应的傅里叶变换;<b><i>F</i></b>为包含傅里叶基向量的矩阵;<i>Q</i>=<b><i>F</i></b><sup>T</sup><b><i>SF</i></b>为加权值。则最后的误差函数写为</p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>E</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>x</mi></munder><mo stretchy="false">{</mo></mstyle><mi>Ι</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">[</mo><mi>W</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>-</mo><mi>Τ</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>⋅</mo></mtd></mtr><mtr><mtd><mi>Q</mi><mo stretchy="false">{</mo><mi>Ι</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">[</mo><mi>W</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>-</mo><mi>Τ</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65"> (6) 式中将<b><i>p</i></b>换为<b><i>p</i></b>+Δ<b><i>p</i></b>, Δ<b><i>p</i></b>表示迭代步长, 通过Δ<b><i>p</i></b>来求取最终的参数<b><i>p</i></b>, 而不是直接求取参数<b><i>p</i></b>。然后进行一级泰勒展开, 并令其关于Δ<b><i>p</i></b>的偏导数等于0, 得到</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>Δ</mtext><mi mathvariant="bold-italic">p</mi><mo>=</mo><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mstyle displaystyle="true"><munder><mo>∑</mo><mi>x</mi></munder><mrow><mrow><mo>{</mo><mrow><mfrac><mrow><mo>∂</mo><mi>Ι</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">[</mo><mi>W</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">p</mi></mrow></mfrac></mrow><mo>}</mo></mrow></mrow></mstyle><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>⋅</mo></mtd></mtr><mtr><mtd><mi>Q</mi><mo stretchy="false">{</mo><mi>Ι</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">[</mo><mi>W</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>-</mo><mi>Τ</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">式中:<mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>x</mi></munder><mrow><mrow><mo>{</mo><mrow><mfrac><mrow><mo>∂</mo><mi>Ι</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">[</mo><mi>W</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">p</mi></mrow></mfrac></mrow><mo>}</mo></mrow></mrow></mstyle><msup><mrow></mrow><mtext>Τ</mtext></msup><mi>Q</mi><mfrac><mrow><mo>∂</mo><mi>Ι</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">[</mo><mi>W</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">p</mi></mrow></mfrac></mrow></math></mathml>。每一次迭代Δ<b><i>p</i></b>, <b><i>p</i></b>参数从零向量开始迭代<b><i>p</i></b>=<b><i>p</i></b>+Δ<b><i>p</i></b>, 直到达到收敛, 最后将收敛值看作这一块人脸的仿射变换参数<b><i>p</i></b>。</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69"><b>2.2 基于最大Gabor相似度寻找每块全局最优的仿射变换参数</b></h4>
                <div class="p1">
                    <p id="70">如果基于像素值寻找全局最优参数, 由于人脸块划分不标准、光照条件和姿态误差等因素的影响, 很难保证校正参数的准确性, 最终造成人脸合成效果不理想。因此, 利用Gabor特征提高光照和姿态的稳健性, 通过使Gabor特征向量余弦相似度最大, 来寻找一个全局最优参数。</p>
                </div>
                <div class="p1">
                    <p id="71">假设共有<i>B</i>张侧脸, 第<i>k</i>个图片得到的第<i>i</i>块参数为<b><i>p</i></b><mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>{<i>k</i>=1, 2, …, <i>B</i>}, 则第<i>i</i>块仿射变换参数候选集<i>P</i><sub><i>i</i></sub>={<b><i>p</i></b><sup> (<i>k</i>) </sup><sub><i>i</i></sub>, <i>k</i>=1, 2, …, <i>B</i>}。选择第<i>i</i>块最优参数的原则如下:</p>
                </div>
                <div class="p1">
                    <p id="73">求库中第<i>k</i>个侧面人脸中的第<i>i</i>块区域在参数<b><i>p</i></b><sub><i>i</i></sub>下的Gabor特征, 得到一个特征向量<b><i>b</i></b><sub><i>k</i>, <i>i</i></sub> (<b><i>p</i></b><sub><i>i</i></sub>) :</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">{</mo><mi>g</mi><msub><mrow></mrow><mn>1</mn></msub><mo>*</mo><mi>Ι</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false">[</mo><mi>W</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>;</mo><mo>⋯</mo><mo>;</mo></mtd></mtr><mtr><mtd><mi>g</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo>*</mo><mi>Ι</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false">[</mo><mi>W</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo><mspace width="0.25em" /></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">求库中对应第<i>k</i>个正面人脸图片第<i>i</i>块的Gabor特征<b><i>a</i></b><sub><i>k</i>, <i>i</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">a</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>i</mi></mrow></msub><mo>=</mo><mo stretchy="false">[</mo><mi>g</mi><msub><mrow></mrow><mn>1</mn></msub><mo>*</mo><mi>Τ</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>;</mo><mo>⋯</mo><mo>;</mo><mi>g</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo>*</mo><mi>Τ</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">对于第<i>i</i>块区域的每一个参数<i>p</i><sub><i>i</i></sub>, 计算其Gabor余弦相似度:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>a</mtext><mtext>b</mtext><mtext>o</mtext><mtext>r</mtext><mo>_</mo><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mrow><mfrac><mrow><mi mathvariant="bold-italic">a</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>i</mi></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">a</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">∥</mo></mrow></mfrac></mrow></mstyle><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">则第<i>i</i>块区域全局最优参数<mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">p</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>a</mtext><mtext>b</mtext><mtext>o</mtext><mtext>r</mtext><mo>_</mo><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="81">基于上述提出的<i>Gabor</i>相似度求取的全局最优姿态校正参数对光照变化有良好的适应性。此外, 全局最优参数的选择是基于<i>Gabor</i>变换提取的局部空间特征, 而不是利用每个点的像素信息, 所以人脸校正过程中侧脸的划分不需要很精确, 只需进行粗略的侧脸区域划分即可通过全局最优参数得到清晰的正面人脸。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82"><b>2.3 基于加权的分块Gabor向量人脸识别</b></h4>
                <div class="p1">
                    <p id="83">考虑到姿态校正过程中采用的是基于人脸分区域的Gabor特征校正方法, 所以加权的局部Gabor二值模式直方图序列 (LGBPHS) 算法<citation id="136" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>更适用于上述姿态校正后进一步的人脸识别。由于本方法基于局部分块, 而每个局部块在识别整个人脸中所起的作用大小不一, 且根据最优校正参数拟合的局部块也有相对的优劣之分, 因此提出为每块人脸设定一个权重值, 以提高最终的识别率。</p>
                </div>
                <div class="p1">
                    <p id="84">按照上述人脸分块, 将不同尺度、不同方向的Gabor滤波器分别与侧脸校正的图片块和库中正脸对应块进行卷积, 取其幅值部分组成Gabor幅值图谱 (GMM) , 对每个人脸区域的GMM应用局部二值模式 (LBP) 算法得到局部Gabor二值模式图谱并计算直方图, 将所有区域直方图串接为一个直方图序列, 并将其作为这一块人脸图像的描述符。</p>
                </div>
                <div class="p1">
                    <p id="85">假设两个直方图是<b><i>H</i></b><sup>1</sup>和<b><i>H</i></b><sup>2</sup>, 则两个直方图的相似度可表示为</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ψ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mn>1</mn></msup><mo>, </mo><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mtext>m</mtext></mstyle><mtext>i</mtext><mtext>n</mtext><mo stretchy="false"> (</mo><mi>h</mi><msubsup><mrow></mrow><mi>i</mi><mn>1</mn></msubsup><mo>, </mo><mi>h</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">式中:<i>h</i><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>1</mn></msubsup></mrow></math></mathml>表示第一个直方图中的第i个条目;h<mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></math></mathml>表示第二个直方图中的第i个条目;L为直方图中的条目数, 衡量的标准为对应条目中共有的部分。</p>
                </div>
                <div class="p1">
                    <p id="90">采用加权的局部<i>Gabor</i>二值模式直方图序列算法进行人脸识别, 即对上述每块人脸添加权重系数。假设灰度区间为[0, L-1], <b><i>H</i></b><sub><i>μ</i>, <i>ν</i>, <i>i</i></sub>表示第<i>i</i>块人脸在Gabor变换<i>g</i><sub><i>μ</i>, <i>ν</i></sub>下得到的局部二值模式直方图, 即</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mi>μ</mi><mo>, </mo><mi>ν</mi><mo>, </mo><mi>i</mi></mrow></msub><mo>=</mo><mo stretchy="false"> (</mo><mi>h</mi><msub><mrow></mrow><mrow><mi>μ</mi><mo>, </mo><mi>ν</mi><mo>, </mo><mi>i</mi><mo>, </mo><mn>0</mn></mrow></msub><mo>, </mo><mi>h</mi><msub><mrow></mrow><mrow><mi>μ</mi><mo>, </mo><mi>ν</mi><mo>, </mo><mi>i</mi><mo>, </mo><mn>1</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>h</mi><msub><mrow></mrow><mrow><mi>μ</mi><mo>, </mo><mi>ν</mi><mo>, </mo><mi>i</mi><mo>, </mo><mi>L</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">式中:<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>h</mi><msub><mrow></mrow><mrow><mi>μ</mi><mo>, </mo><mi>ν</mi><mo>, </mo><mi>i</mi><mo>, </mo><msup><mi>r</mi><mo>′</mo></msup></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>z</mi><mo>∈</mo><mi>i</mi></mrow></munder><mi>Ι</mi></mstyle><mo stretchy="false">{</mo><mtext>L</mtext><mtext>B</mtext><mtext>Ρ</mtext><mo stretchy="false">[</mo><mi>g</mi><msub><mrow></mrow><mrow><mi>μ</mi><mo>, </mo><mi>ν</mi></mrow></msub><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>=</mo><msup><mi>r</mi><mo>′</mo></msup><mo stretchy="false">}</mo></mrow></math></mathml>;<i>LBP</i>表示图像进行局部二值模式算法;r′表示灰度等级;I可表示为</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false">{</mo><mi>A</mi><mo stretchy="false">}</mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>, </mo><mspace width="0.25em" /><mi>A</mi><mtext> </mtext><mtext>i</mtext><mtext>s</mtext><mtext> </mtext><mtext>t</mtext><mtext>r</mtext><mtext>u</mtext><mtext>e</mtext></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>, </mo><mspace width="0.25em" /><mi>A</mi><mtext> </mtext><mtext>i</mtext><mtext>s</mtext><mtext> </mtext><mtext>f</mtext><mtext>a</mtext><mtext>l</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">最终每个人脸的直方图可表示为</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><mo>=</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>, </mo><mn>0</mn><mo>, </mo><mn>1</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>, </mo><mn>0</mn><mo>, </mo><mi>Ν</mi></mrow></msub><mo>, </mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>, </mo><mn>1</mn><mo>, </mo><mn>1</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>, </mo><mn>1</mn><mo>, </mo><mi>Ν</mi></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mn>7</mn><mo>, </mo><mn>4</mn><mo>, </mo><mi>Ν</mi></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">式中:N为人脸区域划分的数量。则衡量两个人脸相似度的加权<i>LGBPHS</i>为</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mtext>Ι</mtext></msub><mo>, </mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mtext>Τ</mtext></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>μ</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>ν</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>ω</mi></mstyle></mrow></mstyle></mrow></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>Ψ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Η</mi><msubsup><mrow></mrow><mrow><mi>μ</mi><mo>, </mo><mi>ν</mi><mo>, </mo><mi>i</mi></mrow><mtext>Ι</mtext></msubsup><mo>, </mo><mi mathvariant="bold-italic">Η</mi><msubsup><mrow></mrow><mrow><mi>μ</mi><mo>, </mo><mi>ν</mi><mo>, </mo><mi>i</mi></mrow><mtext>Τ</mtext></msubsup><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">式中:<b><i>H</i></b><sub>I</sub>表示侧脸的直方图;<b><i>H</i></b><sub>T</sub>表示正脸的直方图;<i>ω</i><sub><i>i</i></sub>为第<i>i</i>块人脸的权重值。</p>
                </div>
                <div class="p1">
                    <p id="100">设定第<i>i</i>块的权重值为通过此人脸块的最优参数<mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">p</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>得到的平均<i>Gabor</i>相似度, 即</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ω</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mrow><mfrac><mrow><mi mathvariant="bold-italic">a</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>i</mi></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">p</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">a</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">p</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">∥</mo></mrow></mfrac></mrow></mstyle></mrow><mi>B</mi></mfrac><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">最后, 找到使S (<b><i>H</i></b><sub>I</sub>, <b><i>H</i></b><sub>T</sub>) 最大的<b><i>H</i></b><sub>T</sub>即为此侧脸图片的身份。</p>
                </div>
                <h3 id="104" name="104" class="anchor-tag">3 实验结果与分析</h3>
                <div class="p1">
                    <p id="105">本实验采用FERET人脸数据库<citation id="137" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>进行实验。该数据库包含约200人, 共14051张灰度人脸图像, 同一个人包含不同的表情、光照和姿态图片。在训练过程中, 每一种角度都采用前60个人 (身份ID从1到60) 的<i>B</i>=60张侧脸图片和对应的正脸图片来得到每一块人脸区域的全局最优参数, 并用于后续的实验。将人脸图像裁剪成80×80的灰度图像, 并按照人脸形状特征 (图1) 将正脸划分为<i>N</i>=27块, 每一块的大小为12×12 (尺寸过大会降低校正效果, 尺寸过小会增大计算量) , 并将正脸分块对应到侧脸的相应位置, 使每个侧脸也是27块。划分的人脸块不需要正脸与侧脸严格准确地一一对应, 仅需大致对应即可。所有正脸与侧脸的局部区域划分及位置都保持一致。</p>
                </div>
                <div class="p1">
                    <p id="106">提取Gabor特征时, Gabor内核定义为<i>k</i><sub><i>μ</i>, <i>ν</i></sub>=<i>k</i><sub><i>ν</i></sub>exp (iϕ<sub><i>μ</i></sub>) , 其中<i>k</i><sub><i>ν</i></sub>=<i>k</i><sub>max</sub>/<i>f</i><sup><i>ν</i></sup>为采样尺度, ϕ<sub><i>μ</i></sub>=π<i>μ</i>/8为采样方向, <i>k</i><sub>max</sub>为最大频率, <i>f</i>为频域内最大间隔因子。为了获得较好的小波特征和辨识效果, 实验中令参数<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>k</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>=</mo><mtext>π</mtext><mo>/</mo><mn>2</mn><mo>, </mo><mi>f</mi><mo>=</mo><msqrt><mn>2</mn></msqrt><msup><mrow></mrow><mrow><mspace width="0.25em" /><mo stretchy="false">[</mo><mn>1</mn><mn>0</mn><mo stretchy="false">]</mo></mrow></msup></mrow></math></mathml>, 取5个尺度和8个方向, 分别标为<i>ν</i>= (0, 1, …, 4) 和<i>μ</i>= (0, 1, …, 7) 。因此, 每个局部区域可获取<i>M</i>=40个Gabor变换系数来提取Gabor特征, 这样可以充分利用图像的方向信息和尺度, 在不同粗细粒度上得到图像的特征。训练最终得到每个对应局部区域块的最优Gabor相似度参数。</p>
                </div>
                <div class="p1">
                    <p id="108">根据每一块得到的最优参数进行了三部分实验, 分别是关于最优参数的评估、侧脸的姿态校正和侧脸识别实验。本实验使用MATLAB语言, 在Intel (R) i5-3230M处理器上运行。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109"><b>3.1 评估得到的每块最优参数</b></h4>
                <div class="p1">
                    <p id="110">本实验测试了同一角度下 (40°) 60个人 (身份ID从61到120) 的侧脸区域经过最优参数校正后的侧脸块与正脸块Gabor余弦相似度, 其中实验的第1, 7, 9, 15块 (根据图1的划分, 从上往下数, 从左往右数) 结果如图2所示。横坐标表示测试的60个侧脸图像, 纵坐标表示每个图像对应区域的Gabor相似度。每一个人脸区域中的水平虚线即为用这一块最优参数求得的平均Gabor余弦相似度, 这个值也将作为每一块人脸的识别权重。从图中可以看到, 有的人脸块拟合得好, 而有的人脸块拟合得一般, 验证了每块人脸需要加权值的必要性。</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907029_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于最优参数的Gabor相似度表现。" src="Detail/GetImg?filename=images/GXXB201907029_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基于最优参数的Gabor相似度表现。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907029_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Performance of Gabor similarity based on optimal parameter.</p>
                                <p class="img_note"> (a) 第1块; (b) 第7块; (c) 第9块; (d) 第15块</p>
                                <p class="img_note"> (a) 1<sup>st </sup>patch; (b) 7<sup>th </sup>patch; (c) 9<sup>th </sup>patch; (d) 15<sup>th </sup>patch</p>

                </div>
                <h4 class="anchor-tag" id="112" name="112"><b>3.2 通过每块最优参数校正人脸</b></h4>
                <div class="p1">
                    <p id="113">本部分实验不仅使用FERET人脸数据库, 还选取了CMU_PIE人脸数据库<citation id="138" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>的图片进行测试。CMU_PIE数据库规模不大, 仅有68个人, 共41368张人脸图片, 但含有更多丰富光照变化的图片。实验对人脸库中标定为±40°的侧脸进行校正, 此外还实际拍摄了侧脸图片进行实验。效果如图3所示, 第1～3行为FERET库中图片, 第4、5行图片来自CMU_PIE库, 最后一行为实拍图片。</p>
                </div>
                <div class="p1">
                    <p id="114">从上述结果可以看出, 在光照不均匀的条件下, 根据Gabor特征得到的最优变换参数依然适用, 所以得到的全局最优参数对光照有一定的稳健性。另外, 虽然数据库中标定的角度有误差 (前后相差10°左右) , 但是恢复出来的正面人脸基本是清晰的。从这一点可以看出, 全局最优参数对姿态也具有一定的稳健性。同样情况下 (人脸分块也是粗略划分的) , 如果是根据像素差最小得到的最优参数对侧脸进行校正, 得到的效果图非常模糊, 这是因为依据像素差值最小寻得的最优参数对人脸分块的准确性有很大的依赖。</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115"><b>3.3 侧脸的识别率</b></h4>
                <div class="p1">
                    <p id="116">侧脸识别实验在FERET人脸数据库中进行, 测试了身份ID从61到190共130个人的对应角度的侧脸图片, 即每个角度测试数据为130张图片, 并与其他方法进行了对比。实验对比方法主要选择与本文分块提取相似度思想相似的3种经典方法, 即文献<citation id="139" type="reference">[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</citation>中的方法, 同时对比了近年来较热门的基于深度学习的方法, 即文献<citation id="140" type="reference">[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</citation>中的方法。实验结果如表1所示。</p>
                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907029_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于Gabor最优参数和像素差最优参数校正后的人脸对比" src="Detail/GetImg?filename=images/GXXB201907029_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 基于Gabor最优参数和像素差最优参数校正后的人脸对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907029_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Synthesizing frontal face comparison based on Gabor optimal parameters and pixel difference optimal parameters</p>

                </div>
                <div class="area_img" id="118">
                    <p class="img_tit">表1 若干方法在FERET人脸数据库中的识别率对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Comparison of recognition rates of different approaches on FERET database</p>
                    <p class="img_note"></p>
                    <table id="118" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="4"><br />Recognition rate /%</td></tr><tr><td><br />-45°</td><td>-30°</td><td>30°</td><td>45°</td></tr><tr><td>LLR<sup>[4]</sup></td><td>55.0</td><td>89.5</td><td>77.0</td><td>53.0</td></tr><tr><td><br />StackFlow<sup>[5]</sup></td><td>70.0</td><td>89.0</td><td>82.0</td><td>62.0</td></tr><tr><td><br />MRF<sup>[6]</sup></td><td>91.0</td><td>97.3</td><td>96.5</td><td>91.5</td></tr><tr><td><br />MVRL<sup>[7]</sup></td><td>96.2</td><td>99.2</td><td>100.0</td><td>96.9</td></tr><tr><td><br />CPF<sup>[8]</sup></td><td>98.4</td><td>100.0</td><td>99.2</td><td>97.7</td></tr><tr><td><br />MSMT<sup>[9]</sup></td><td>99.6</td><td>100.0</td><td>100.0</td><td>99.2</td></tr><tr><td><br />Our approach</td><td>96.9</td><td>98.4</td><td>99.2</td><td>97.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="119">从表1可以看出, 传统的LLR方法识别率最低, 这是由于传统方法只提取了人脸线性信息而忽略了非线性信息, 所以得到的校正图像比较模糊, 从而严重影响了识别率。StackFlow和MRF算法, 尤其是后者, 在大姿态人脸识别率上已经有了较大提升, 但这两种方法均是根据像素点的灰度值寻找最优参数, 而这种方式非常依赖人脸区域划分和映射的准确性, 并且灰度值受光照的影响。基于最大Gabor相似度寻找最优参数则不受上述因素的影响和限制, 因此, 识别率也得到了进一步的提高, 所提方法的操作更加简便, 也更适用于实际应用。</p>
                </div>
                <div class="p1">
                    <p id="120">对于基于深度学习的几种方法, 整体识别率都较高, 但这些方法对硬件设备和训练数据规模的要求均较高。对于CPF方法 (训练集用了14000张图片) , 虽然整个神经网络有反馈, 可以得到任意姿态的人脸图像, 但是图片的校正过程需要对图像另外添加一些编码信息, 而且提取的各个角度特征的区分度并不太好。MVRL方法 (训练集用了30000张图片) 通过不同深度学习模型提取的特征有些冗余, 并且只利用了纹理特征而没有对形状特征进行区分, 因而需要的训练图像数量要求规模更大。而最新的基于深度神经网络学习的MSMT方法, 训练集用了106402张图片, 计算量极大。相比而言, 所提方法操作简单, 对硬件设备要求不高, 在数据规模小的情况下具有高效率和高精度的优势。</p>
                </div>
                <h3 id="121" name="121" class="anchor-tag">4 结  论</h3>
                <div class="p1">
                    <p id="122">大姿态已成为限制人脸识别率提高的主要因素。采用基于加权的LK人脸分块算法分析可知, 每一人脸块的全局最优参数是使得Gabor特征的余弦相似度最大的参数, 同时将根据最优参数得到的平均Gabor相似度作为姿态识别中的权重可以提高识别率。所提方法的优点在于数据规模小的情况下具有独特优势, 以少量训练样本和较短训练时间即可得到理想的结果, 并且无需进行任何标定点的检测, 对人脸的分块进行粗略划分即可。下一步的研究应将深度神经网络学习与本文方法所提取的Gabor参数相结合, 以便更加高效地得到清晰的正脸图片和更高的识别精度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738739&amp;v=MTIwMTJUTW53WmVadUh5am1VYi9JSVZvV2FCRT1OaWZPZmJLN0h0RE5xWTlGWStnSEMzOHdvQk1UNlQ0UFFIL2lyUmRHZXJxUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Zhang X Z, Gao Y S.Face recognition across pose:a review[J].Pattern Recognition, 2009, 42 (11) :2876-2896.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pose-invariant face recognition based on a single view">

                                <b>[2]</b> Tang H, Yin B, Sun Y, <i>et al</i>.Pose-invariant face recognition based on a single view [J].Journal of Information and Computer Science, 2010, 7 (12) :2369-2379.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201809016&amp;v=MDAxOTJwbzlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnluaFViM0xJalhUYkxHNEg5bk0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Tong Y, Wei Y M, Shen Y H.Supervised sparsity preserving projection based on global constraint[J].Acta Optica Sinica, 2018, 38 (9) :0910001.童莹, 魏以民, 沈越泓.基于全局约束的监督稀疏保持投影降维方法研究[J].光学学报, 2018, 38 (9) :0910001.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Local Linear Regression (LLR) for Pose Invariant Face Recognition">

                                <b>[4]</b> Chai X J, Shan S G, Chen X L, <i>et al</i>.Local linear regression (LLR) for pose invariant face recognition[C]//7th International Conference on Automatic Face and Gesture Recognition (FGR06) , April 10-12, 2006, Southampton, UK.New York:IEEE, 2006:631-636.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning patch correspondences for improved viewpoint invariant face recognition">

                                <b>[5]</b> Ashraf A B, Lucey S, Chen T.Learning patch correspondences for improved viewpoint invariant face recognition[C]//2008 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2008, Anchorage, AK, USA.New York:IEEE, 2008:4587754.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pose-Invariant Face Recognition Using Markov Random Fields">

                                <b>[6]</b> Ho H T, Chellappa R.Pose-invariant face recognition using Markov random fields[J].IEEE Transactions on Image Processing, 2013, 22 (4) :1573-1584.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust face recognition with deep multi-view representation learning">

                                <b>[7]</b> Li J, Zhao J, Zhao F, <i>et al</i>.Robust face recognition with deep multi-view representation learning[C]//Proceedings of the 24th ACM International Conference on Multimedia, October 15-19, 2016, Amsterdam, Netherlands.New York:ACM, 2016:1068-1072.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rotating Your Face Using Multi-task Deep Neural Network">

                                <b>[8]</b> Yim J, Jung H, Yoo B, <i>et al</i>.Rotating your face using multi-task deep neural network[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:676-684.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reconstruction-based disentanglement for pose-invariant face recognition">

                                <b>[9]</b> Peng X, Yu X, Sohn K, <i>et al</i>.Reconstruction-based disentanglement for pose-invariant face recognition[C]//2017 IEEE International Conference on Computer Vision (ICCV) , October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:1632-1641.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition">

                                <b>[10]</b> Liu C J, Wechsler H.Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition[J].IEEE Transactions on Image Processing, 2002, 11 (4) :467-476.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An iterative image registration technique with an application to stereo vision">

                                <b>[11]</b> Lucas B D, Kanade T.An iterative image registration technique with an application to stereo vision[C]//Proceedings of 7th International Joint Conference on Artificial Intelligence, August 24-28, 1981, Vancouver, British Columbia.San Francisco:Margan Kaufmann Publishers Inc., 1981:674-679.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast image alignment in the Fourier domain">

                                <b>[12]</b> Ashraf A B, Lucey S, Chen T.Fast image alignment in the Fourier domain[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:2480-2487.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Local Gabor Binary Pattern Histogram Sequence (LGBPHS): a Novel Non-Statistical Model for Face Representation and Recognition">

                                <b>[13]</b> Zhang W C, Shan S G, Gao W, <i>et al</i>.Local Gabor binary pattern histogram sequence (LGBPHS) :a novel non-statistical model for face representation and recognition[C]//Tenth IEEE International Conference on Computer Vision (ICCV′05) Volume 1, October 17-21, 2005, Beijing, China.New York:IEEE, 2005:786-791.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The FERET Evaluation Methodology for Face-Recognition Algorithms">

                                <b>[14]</b> Phillips P J, Moon H, Rizvi S A, <i>et al</i>.The FERET evaluation methodology for face-recognition algorithms[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22 (10) :1090-1104.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The CMU pose, illumination, and expression database">

                                <b>[15]</b> Sim T, Baker S, Bsat M.The CMU pose, illumination, and expression database[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2003, 25 (12) :1615-1618.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201907029" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201907029&amp;v=MDQ4ODVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5oVWIzTElqWFRiTEc0SDlqTXFJOUhiWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

