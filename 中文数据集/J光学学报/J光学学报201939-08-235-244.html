

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133844072783750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201908028%26RESULT%3d1%26SIGN%3d8%252fzXanAgl93i4pbwv4aZnbEGTsk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201908028&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201908028&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201908028&amp;v=MTU2MzR6cXFCdEdGckNVUkxPZVplVnVGeW5tVXI3QUlqWFRiTEc0SDlqTXA0OUhiSVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#64" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="2 改进的FPN算法 ">2 改进的FPN算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#86" data-title="3 MSSD网络 ">3 MSSD网络</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#88" data-title="&lt;b&gt;3.1 基础特征提取网络&lt;/b&gt;"><b>3.1 基础特征提取网络</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;3.2 Fusion block及记忆和滤波通道&lt;/b&gt;"><b>3.2 Fusion block及记忆和滤波通道</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;3.3 锚点 (Anchors&lt;/b&gt;) "><b>3.3 锚点 (Anchors</b>) </a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;3.4 模型的训练&lt;/b&gt;"><b>3.4 模型的训练</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#98" data-title="4 实  验 ">4 实  验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#100" data-title="&lt;b&gt;4.1 算法有效性研究&lt;/b&gt;"><b>4.1 算法有效性研究</b></a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;4.2 算法性能研究&lt;/b&gt;"><b>4.2 算法性能研究</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#124" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="图1 FPN结构">图1 FPN结构</a></li>
                                                <li><a href="#75" data-title="图2 改进的FPN结构">图2 改进的FPN结构</a></li>
                                                <li><a href="#85" data-title="图3 记忆和滤波通道的结构">图3 记忆和滤波通道的结构</a></li>
                                                <li><a href="#90" data-title="图4 MSSD网络结构">图4 MSSD网络结构</a></li>
                                                <li><a href="#104" data-title="表1 加入FPN结构的SSD网络与MSSD网络的性能对比">表1 加入FPN结构的SSD网络与MSSD网络的性能对比</a></li>
                                                <li><a href="#107" data-title="表2 融合结构的改进效果对比">表2 融合结构的改进效果对比</a></li>
                                                <li><a href="#111" data-title="表3 记忆和滤波通道的有效性分析">表3 记忆和滤波通道的有效性分析</a></li>
                                                <li><a href="#114" data-title="表4 改变基础网络性能对比">表4 改变基础网络性能对比</a></li>
                                                <li><a href="#119" data-title="表5 各类先进深度学习算法在VOC2007数据集上精度对比">表5 各类先进深度学习算法在VOC2007数据集上精度对比</a></li>
                                                <li><a href="#120" data-title="表6 各类基于SSD的改进算法的测试结果对比">表6 各类基于SSD的改进算法的测试结果对比</a></li>
                                                <li><a href="#121" data-title="表7 SSD算法和MSSD算法对小目标的检测精度">表7 SSD算法和MSSD算法对小目标的检测精度</a></li>
                                                <li><a href="#123" data-title="图5 SSD算法和MSSD算法的可视化对比。">图5 SSD算法和MSSD算法的可视化对比。</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="12">


                                    <a id="bibliography_1" title=" Girshick R, Donahue J, Darrell T, &lt;i&gt;et al&lt;/i&gt;.Rich feature hierarchies for accurate object detection and semantic segmentation[C]∥2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[1]</b>
                                         Girshick R, Donahue J, Darrell T, &lt;i&gt;et al&lt;/i&gt;.Rich feature hierarchies for accurate object detection and semantic segmentation[C]∥2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:580-587.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_2" title=" Everingham M, Eslami S M A, van Gool L, &lt;i&gt;et al&lt;/i&gt;.The PASCAL visual object classes challenge:a retrospective[J].International Journal of Computer Vision, 2015, 111 (1) :98-136." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15011900002694&amp;v=MDU2NTVOcG85RlpPc05DblU5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JSVYwVmF4bz1OajdCYXJLOUh0RA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Everingham M, Eslami S M A, van Gool L, &lt;i&gt;et al&lt;/i&gt;.The PASCAL visual object classes challenge:a retrospective[J].International Journal of Computer Vision, 2015, 111 (1) :98-136.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_3" >
                                        <b>[3]</b>
                                     He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1904-1916.</a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_4" title=" Girshick R.Fast R-CNN[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[4]</b>
                                         Girshick R.Fast R-CNN[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1440-1448.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_5" >
                                        <b>[5]</b>
                                     Ren S Q, He K M, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.</a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_6" title=" Liu W, Anguelov D, Erhan D, &lt;i&gt;et al&lt;/i&gt;.SSD:single shot MultiBox detector[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Cham:Springer, 2016, 9905:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:single shot multibox detector">
                                        <b>[6]</b>
                                         Liu W, Anguelov D, Erhan D, &lt;i&gt;et al&lt;/i&gt;.SSD:single shot MultiBox detector[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Cham:Springer, 2016, 9905:21-37.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_7" title=" Lin T Y, Dollar P, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Feature pyramid networks for object detection[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI.New York:IEEE, 2017:936-944." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature pyramid networks for object detection">
                                        <b>[7]</b>
                                         Lin T Y, Dollar P, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Feature pyramid networks for object detection[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI.New York:IEEE, 2017:936-944.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_8" title=" Redmon J, Farhadi A.YOLOv3:an incremental improvement[J/OL]. (2018-04-08) [2019-01-30].https://arxiv.org/abs/1804.02767." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLOv3 an incremental improvement">
                                        <b>[8]</b>
                                         Redmon J, Farhadi A.YOLOv3:an incremental improvement[J/OL]. (2018-04-08) [2019-01-30].https://arxiv.org/abs/1804.02767.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     Lin T Y, Goyal P, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Focal loss for dense object detection[C]∥2017 IEEE International Conference on Computer Vision (ICCV) , October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:2999-3007.</a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     Zhang S F, Wen L Y, Bian X, &lt;i&gt;et al&lt;/i&gt;.Single-shot refinement neural network for object detection[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 18-23, 2018, Salt Lake City.New York:IEEE, 2018:4203-4212.</a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_11" title=" Fu C Y, Liu W, Ranga A, &lt;i&gt;et al&lt;/i&gt;.DSSD:deconvolutional single shot detector[J/OL]. (2017-01-23) [2019-01-30].https://arxiv.org/abs/1701.06659." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dssd:Deconvolutional single shot detector">
                                        <b>[11]</b>
                                         Fu C Y, Liu W, Ranga A, &lt;i&gt;et al&lt;/i&gt;.DSSD:deconvolutional single shot detector[J/OL]. (2017-01-23) [2019-01-30].https://arxiv.org/abs/1701.06659.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_12" title=" Li Z X, Zhou F Q.FSSD:feature fusion single shot multibox detector[J/OL]. (2018-05-17) [2019-01-30].https://arxiv.org/abs/1712.00960." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FSSD:feature fusion single shot multibox detector">
                                        <b>[12]</b>
                                         Li Z X, Zhou F Q.FSSD:feature fusion single shot multibox detector[J/OL]. (2018-05-17) [2019-01-30].https://arxiv.org/abs/1712.00960.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_13" title=" Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J/OL]. (2015-04-10) [2019-02-02].https://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[13]</b>
                                         Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J/OL]. (2015-04-10) [2019-02-02].https://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_14" title=" Jeong J, Park H, Kwak N.Enhancement of SSD by concatenating feature maps for object detection[J/OL]. (2017-05-26) [2019-02-01].https://arxiv.org/abs/1705.09587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enhancement of SSD by concatenating feature maps for object detection">
                                        <b>[14]</b>
                                         Jeong J, Park H, Kwak N.Enhancement of SSD by concatenating feature maps for object detection[J/OL]. (2017-05-26) [2019-02-01].https://arxiv.org/abs/1705.09587.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_15" title=" Zhao Q J, Sheng T, Wang Y T, &lt;i&gt;et al&lt;/i&gt;.M2Det:a single-shot object detector based on multi-level feature pyramid network[J/OL]. (2019-01-06) [2019-02-01].https://arxiv.org/abs/1705.09587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=M2Det:a single-shot object detector based on multi-level feature pyramid network">
                                        <b>[15]</b>
                                         Zhao Q J, Sheng T, Wang Y T, &lt;i&gt;et al&lt;/i&gt;.M2Det:a single-shot object detector based on multi-level feature pyramid network[J/OL]. (2019-01-06) [2019-02-01].https://arxiv.org/abs/1705.09587.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_16" title=" Liu X, Chen J, Yang D F, &lt;i&gt;et al&lt;/i&gt;.Scene-coupled intelligent multi-task detection algorithm for air-to-ground remote sensing image[J].Acta Optica Sinica, 2018, 38 (12) :1215008.刘星, 陈坚, 杨东方, 等.场景耦合的空对地多任务遥感影像智能检测算法[J].光学学报, 2018, 38 (12) :1215008." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201812032&amp;v=MDUzNjg2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5tVXI3QUlqWFRiTEc0SDluTnJZOUdab1FLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         Liu X, Chen J, Yang D F, &lt;i&gt;et al&lt;/i&gt;.Scene-coupled intelligent multi-task detection algorithm for air-to-ground remote sensing image[J].Acta Optica Sinica, 2018, 38 (12) :1215008.刘星, 陈坚, 杨东方, 等.场景耦合的空对地多任务遥感影像智能检测算法[J].光学学报, 2018, 38 (12) :1215008.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_17" title=" Graves A.Supervised sequence labelling with recurrent neural networks:long short-term memory[M].Berlin, Heidelberg:Springer, 2012:37-45." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long Short-Term Memory">
                                        <b>[17]</b>
                                         Graves A.Supervised sequence labelling with recurrent neural networks:long short-term memory[M].Berlin, Heidelberg:Springer, 2012:37-45.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_18" title=" Cai Z W, Fan Q F, Feris R S, &lt;i&gt;et al&lt;/i&gt;.A unified multi-scale deep convolutional neural network for fast object detection[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Cham:Springer, 2016, 9908:354-370." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A unified multi-scale deep convolutional neural network for fast object detection">
                                        <b>[18]</b>
                                         Cai Z W, Fan Q F, Feris R S, &lt;i&gt;et al&lt;/i&gt;.A unified multi-scale deep convolutional neural network for fast object detection[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Cham:Springer, 2016, 9908:354-370.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_19" >
                                        <b>[19]</b>
                                     He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.Deep residual learning for image recognition[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778.</a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_20" title=" Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[20]</b>
                                         Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_21" title=" Zeng X Y, Ouyang W L, Yan J J, &lt;i&gt;et al&lt;/i&gt;.Crafting GBD-net for object detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (9) :2109-2123." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Crafting GBD-net for object detection">
                                        <b>[21]</b>
                                         Zeng X Y, Ouyang W L, Yan J J, &lt;i&gt;et al&lt;/i&gt;.Crafting GBD-net for object detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (9) :2109-2123.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_22" title=" Dai J, Li Y, He K, &lt;i&gt;et al&lt;/i&gt;.R-FCN:object detection via region-based fully convolutional networks[C]∥Proceedings of the 30th International Conference on Neural Information Processing Systems, December 5-10, 2016, Barcelona, Spain.USA:Curran Associates Inc., 2016:379-387." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=R-FCN:Object Detection via Region based Fully Convolutional Networks">
                                        <b>[22]</b>
                                         Dai J, Li Y, He K, &lt;i&gt;et al&lt;/i&gt;.R-FCN:object detection via region-based fully convolutional networks[C]∥Proceedings of the 30th International Conference on Neural Information Processing Systems, December 5-10, 2016, Barcelona, Spain.USA:Curran Associates Inc., 2016:379-387.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_23" title=" Bell S, Zitnick C L, Bala K, &lt;i&gt;et al&lt;/i&gt;.Inside-outside net:detecting objects in context with skip pooling and recurrent neural networks[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:2874-2883." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inside-outside net:detecting objects in context w ith skip pooling and recurrent neural netw orks">
                                        <b>[23]</b>
                                         Bell S, Zitnick C L, Bala K, &lt;i&gt;et al&lt;/i&gt;.Inside-outside net:detecting objects in context with skip pooling and recurrent neural networks[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:2874-2883.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_24" title=" Gidaris S, Komodakis N.Object detection via a multi-region and semantic segmentation-aware CNN model[C]∥2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1134-1142." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object detection via a multi-region and semantic segmentation-aware CNN model">
                                        <b>[24]</b>
                                         Gidaris S, Komodakis N.Object detection via a multi-region and semantic segmentation-aware CNN model[C]∥2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1134-1142.
                                    </a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_25" title=" Xu M L, Cui L S, L&#252; P, &lt;i&gt;et al&lt;/i&gt;.MDSSD:multi-scale deconvolutional single shot detector for small objects[J/OL]. (2018-08-19) [2019-02-02].https://arxiv.org/abs/1805.07009." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MDSSD:multi-scale deconvolutional single shot detector for small objects">
                                        <b>[25]</b>
                                         Xu M L, Cui L S, L&#252; P, &lt;i&gt;et al&lt;/i&gt;.MDSSD:multi-scale deconvolutional single shot detector for small objects[J/OL]. (2018-08-19) [2019-02-02].https://arxiv.org/abs/1805.07009.
                                    </a>
                                </li>
                                <li id="62">


                                    <a id="bibliography_26" title=" Lin T Y, Maire M, Belongie S, &lt;i&gt;et al&lt;/i&gt;.Microsoft COCO:common objects in context[M]∥Fleet D, Pajdla T, Schiele B, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Cham:Springer, 2014, 8693:740-755." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Microsoft COCO:Common Objects in Context">
                                        <b>[26]</b>
                                         Lin T Y, Maire M, Belongie S, &lt;i&gt;et al&lt;/i&gt;.Microsoft COCO:common objects in context[M]∥Fleet D, Pajdla T, Schiele B, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Cham:Springer, 2014, 8693:740-755.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-25 11:39</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(08),235-244 DOI:10.3788/AOS201939.0815005            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种改进的多门控特征金字塔网络</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E5%BD%A4&amp;code=40365191&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵彤</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%B4%81%E7%91%9C&amp;code=35836002&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘洁瑜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B2%88%E5%BC%BA&amp;code=35836667&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">沈强</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%81%AB%E7%AE%AD%E5%86%9B%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E5%AF%BC%E5%BC%B9%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=1699750&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">火箭军工程大学导弹工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>特征金字塔网络 (FPN) 在融合不同尺度特征图时采用上采样和相加的方法, 然而经过上采样的特征图的空间层级化信息丢失严重, 简单地进行相加必然引入一定的误差。同时, FPN结构的深层特征信息前向传递性较差, 其对更浅层的辅助效果基本消失。对此, 结合长短时记忆 (LSTM) 网络在处理上下文信息上的优势对FPN结构进行改进, 在不同深度的特征层之间建立一条自上而下的记忆链接, 建立多门控结构对记忆链上的信息进行过滤和融合以产生表征能力更强的高级语义特征图。最后, 将改进的FPN结构加入到SSD (Single Shot MultiBox Detector) 算法框架中, 提出新的特征融合网络——MSSD (Memory SSD) , 并在Pascal VOC 2007数据集上进行验证。实验表明, 该改进取得了较好的测试结果, 相比于目前较先进的检测算法也有一定的优势。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E9%87%91%E5%AD%97%E5%A1%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征金字塔;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长短时记忆网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AE%B0%E5%BF%86%E6%BB%A4%E6%B3%A2%E9%80%9A%E9%81%93&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">记忆滤波通道;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SSD%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SSD网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *刘洁瑜 E-mail:601080018@qq.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-13</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金青年基金 (61503392);</span>
                    </p>
            </div>
                    <h1><b>An Improved Multi-Gate Feature Pyramid Network</b></h1>
                    <h2>
                    <span>Zhao Tong</span>
                    <span>Liu Jieyu</span>
                    <span>Shen Qiang</span>
            </h2>
                    <h2>
                    <span>College of Missile Engineering, Rocket Force University of Engineering</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The feature pyramid network (FPN) adopts the method of upsampling and addition when fusing different scale feature maps. However, the spatial stratification information of the upsampled feature map is seriously lost, so that direct addition will inevitably make certain errors. At the same time, the deep feature information of the FPN structure is poorly forward-transferred, and its auxiliary effect to the shallower layer basically disappears. This paper uses the advantages of Long Short-Term Memory (LSTM) network in processing context information to improve the FPN structure. A top-down memory chain is established between feature layers of different depths, and a multi-gate structure is constructed to filter and fuse the information on the memory chain to generate a higher semantic feature map with stronger representation ability. Finally, the improved FPN structure is added to the SSD (Single Shot MultiBox Detector) algorithm framework, and a new feature fusion network, MSSD (Memory SSD) , is proposed and verified on the Pascal VOC 2007 data set. Experiments show that the improved algorithm has achieved better test results, and it has certain advantages compared with the current advanced detection algorithms.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=target%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">target detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20pyramid&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature pyramid;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=long%20short-term%20memory%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">long short-term memory network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=memory%20and%20filter%20channel&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">memory and filter channel;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=single%20shot%20multibox%20detector%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">single shot multibox detector network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-13</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="64" name="64" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="65">目标检测在当今社会上有着广泛应用, 如无人驾驶、导弹制导和野外搜救等, 其主要任务是对图像中特定目标进行定位和判别。传统的基于手工特征的检测方法虽然取得了较好的效果, 但是处理过程繁琐, 并且针对不同类型的图像需要选择合适的检测特征, 当面对类别较多、内容复杂的图像时, 检测效果很差。</p>
                </div>
                <div class="p1">
                    <p id="66">2014年, Girdhivk等<citation id="126" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出的基于区域卷积的神经网络 (R-CNN) 使用卷积神经网络 (CNN) 来提取图像特征, 相对于传统方法在精度和速度上取得巨大突破, 在PSCAL VOC 2012<citation id="127" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>数据集上平均检测精度达到53.3%。于是, 如何构建CNN以产生更有表征能力的特征成为深度学习目标检测算法的重要发展方向。SPPNet<citation id="128" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、Fast R-CNN<citation id="129" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、Faster R-CNN<citation id="130" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等基于R-CNN的算法均在单个输入尺度上计算最顶层特征图来预测候选边界, 但由于最顶层特征具有固定尺度的接受场, 图像中与接受场尺度相差较大的目标的检测误差较大。尤其是对一些小尺度目标来说, 最顶层的特征层甚至忽略了其特征信息。2016年Liu等<citation id="131" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出了SSD (Single Shot MultiBox Detector) 算法, 采用一种多尺度预测的思想, 在多个不同深度的特征层上同时来预测候选边界, 以适应图像中不同尺度的目标。此方法兼顾了感受野和接受场对目标尺度的适应性, 不过其在不同的特征层上独立进行预测, 忽视了深层特征对浅层特征的辅助作用。为此, 众多学者们都对网络结构进行改进来融合不同深度的特征层。Lin等<citation id="132" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出了特征金字塔网络 (FPN) , 其利用深度卷积网络固有的多尺度、多层次特征建立自上而下横向连接的结构, 从而构建出更具表征能力的高级语义特征图。目前较为先进的检测网络均采用了FPN结构, 如YOLOV3<citation id="133" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, RetinaNet<citation id="134" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, RefineDet<citation id="135" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。2017年Fu等<citation id="136" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出的DSSD (Deconvolutional Single Shot Detector) 采用了FPN的思想, 并把SSD的基准网络从VGG (Visual Geometry Group) 换成了Resnet-101, 增强了特征提取能力, 然后使用反卷积层减少了深层特征图上采样的结构损失, 最终提升了目标检测精度 (尤其是小物体) , 但是其速度下降了很多。Li等<citation id="137" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出的FSSD (Feature Fusion Single Shot Multibox Detector) 先将VGG16<citation id="138" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>顶层的三个特征提取层进行融合后, 再产生6个额外特征层, 最后进行回归, 其在保证SSD速度的条件下提升了精确度。Jeong等<citation id="139" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出的RSSD (Rainbow Single Shot Detector) 利用池化和反卷积结构建立了双向融合结构, 解决了一个目标匹配到多框的问题同时提升了检测精度。2019年, Zhao等<citation id="140" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出的M2Det在FPN的基础上建立更深的TUM (Thinned U-shape Module) 结构, 在VGG16基础特征图上并联了三路TUM结构分别来提取浅层特征、中层特征及深层特征, 其精度在COCO (Common Objects in COntext) 数据集上取得了极大提升。Liu等<citation id="141" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>在FSSD的基础上建立了多任务检测算法, 结合场景信息特征进行融合, 提升了空对地目标的检测。</p>
                </div>
                <div class="p1">
                    <p id="67">虽然这些借鉴FPN特征融合思想的算法取得了较好的效果, 但很少有学者对特征融合时深层信息上采样产生的结构误差进行研究。为此, 本文针对FPN融合结构进行改进, 在不同深度的特征层之间建立一条自上而下的记忆链, 将深层的特征信息更有效地保留下来, 同时结合长短时记忆 (LSTM) 网络<citation id="142" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>的门控思想, 利用不同深度的特征信息对记忆链上的结构误差进行过滤, 对有效特征进行融合以产生表征能力更强的高级语义特征图。在SSD算法框架对改进的结构进行验证, 提出一种具有选择性和记忆性的网络结构——MSSD (Memory SSD) , 并在VOC2007数据集上进行验证。实验结果显示, 本改进算法在FPS (Frames Per Second) 下降有限的情况下, 在精度上相比传统的FPN结构有了较大提升, 输入300 pixel×300 pixel图像时, 平均检测精度达到79.0%。</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag">2 改进的FPN算法</h3>
                <div class="p1">
                    <p id="69">传统的FPN算法如图1所示, 左侧自底向上的结构为深度卷积神经网络的卷积过程, BN为批量标准化。随着卷积层的增加, 特征层的尺度逐渐减小, 每个像素位置包含的语义信息更强。而后, 在右侧建立了横向连接, 自上而下地对左侧特征层进行上采样和融合。图1中‘♁’为融合单元, 其结构如标注框内所示。先对上层特征层进行双线性插值的上采样, 而后与左侧经过1×1的卷积操作的特征层直接相加。为消除相加时造成的混叠效应, 在之后增加一个3×3的卷积操作, 最后输出稳健性更强、精准度更高的特征图<citation id="143" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。因为浅层特征可以提供更加准确的位置信息, 而多次的降采样和上采样操作使得深层网络的定位信息存在误差。FPN巧妙地将处理过的低层特征和处理过的高层特征进行累加, 这样就构建了一个更深的、融合多层信息的特征金字塔, 并在不同的特征进行输出, 最终有效地提高了检测精度。FPN的输出特征图表示为</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>φ</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mi>g</mi><mo stretchy="false"> (</mo><mtext>ϕ</mtext><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>=</mo><mi>f</mi><msup><mrow></mrow><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow></msup><mo stretchy="false">{</mo><mtext>ϕ</mtext><msub><mrow></mrow><mi>n</mi></msub><mo>+</mo><mi>f</mi><msup><mrow></mrow><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow></msup><mo stretchy="false">[</mo><mi>v</mi><mo stretchy="false"> (</mo><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext>ϕ</mtext><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mi>f</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>=</mo><mi>f</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">{</mo><mi>f</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">[</mo><mo>⋯</mo><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>Ι</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">式中:<i>φ</i><sub><i>n</i></sub>为第<i>n</i>个经FPN结构融合后的特征图;<i>g</i>为FPN结构函数;<i>v</i>是双线性插值函数;<i>f</i><sup>1×1</sup>为卷积核为1×1大小的卷积层;<i>f</i><sup>3×3</sup>为卷积核为3×3大小的卷积层;ϕ<sub><i>n</i></sub>为特征提取网络的第<i>n</i>层特征图;<i>I</i>为原始图像;<i>f</i><sub><i>n</i></sub>为特征提取网络的第<i>n</i>个卷积函数。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908028_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 FPN结构" src="Detail/GetImg?filename=images/GXXB201908028_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 FPN结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908028_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 FPN structure</p>

                </div>
                <div class="p1">
                    <p id="73">FPN在融合不同深度的特征层时采用直接相加的做法, 然而经过上采样后的深层特征图空间结构信息受损, 必然会产生噪声。一方面由于深层特征层拥有更大的感受野, 其特征包含大量的高级语义信息, 因此当深层特征被噪声污染时, 其对浅层特征中的位置信息必然造成负面干扰, 影响目标检测。同时, 深层特征包含的部分全局信息对浅层的小区域信息来说并不总有正面影响, 当全局信息中包含一类以上目标时, 必然仅对其中的某一类目标的定位有辅助作用。随着其自上而下融合, 噪声又被进一步累加, 即使FPN在不同层进行有监督的训练, 仍然不可避免地对浅层特征图的细粒度产生影响。另一方面, FPN在自上而下的融合过程中不相邻的特征层之间信息传递能力较低。在经过多层融合后, 与当前层相邻较远的特征层的影响基本消失, 这些特征层的辅助作用便被忽略。因此, 本研究结合LSTM的记忆和筛选能力, 对FPN进行改进。</p>
                </div>
                <div class="p1">
                    <p id="74">改进的网络结构如图2所示。图2中‘♁’为反卷积融合单元, 其结构如标注框内所示。为减小噪声引入, 增强高层特征表征能力, 采用反卷积对上层特征层进行上采样, 而后与当前层的特征层相加, 通过一个3×3的卷积单元减弱混叠效应。延续FPN自上而下的横向连接结构, 并加入记忆和滤波通道, 此结构对融合后特征图的有效信息进行保留, 对无效信息进行滤除。同时, 在所有的特征层之间建立记忆链, 增强上下文信息的传递效率。最后输出对上下文信息提取能力更强的高级特征图, 在其上进行下一步的边框回归。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908028_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 改进的FPN结构" src="Detail/GetImg?filename=images/GXXB201908028_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 改进的FPN结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908028_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Improved FPN structure</p>

                </div>
                <div class="p1">
                    <p id="76">反卷积融合单元如</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>φ</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mi>g</mi><mo stretchy="false"> (</mo><mtext>ϕ</mtext><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>=</mo><mi>f</mi><msup><mrow></mrow><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow></msup><mo stretchy="false">[</mo><mtext>ϕ</mtext><msub><mrow></mrow><mi>n</mi></msub><mo>+</mo><mi>w</mi><mo stretchy="false"> (</mo><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">所示, 式中, <i>w</i>为反卷积函数, 其余参数同 (1) 式。</p>
                </div>
                <div class="p1">
                    <p id="79">记忆和滤波通道的结构如图3所示。该结构有两个输入通道和三个输出通道, A输入为当前层之前的所有特征层的有效记忆信息, B输入为当前层的融合特征图, C输出为通过记忆和滤波通道筛选后的特征信息, D输出为融合当前层的记忆信息, E输出为最终输出的增强特征图。本结构采用LSTM网络的门控思想, 从A端开始的箭头实线为记忆链, 其贯穿网络中每个参与到边框回归的特征层, 并与这些特征层进行线性的信息交互。在每次进入记忆和滤波通道时, 都对保留的特征信息进行反卷积操作。I区域为遗忘门, 当新的特征信息加入进来后, 利用这些信息对记忆链中前层的特征进行筛选, 去除无用信息和噪声。其中3×3的卷积操作跨通道整合了融合后的特征信息, 增加了模型的深度, 在一定程度上提升了模型的非线性。Sigmoid函数的输出为0到1, 表示记忆链中信息的传递程度, 0表示信息截断, 1表示完全传递。II区域为输入门, 其对新特征进行筛选, 将有效特征提取出来对记忆链中的信息进行更新。其中Tanh函数用来生成候选的特征信息, Sigmoid函数决定候选信息的传递量, 两个函数共同作用完成对记忆链的更新。III区域为输出门, 其利用新加入的特征信息对更新后的记忆链进行选择。更新后的记忆链中包含当前层和当前层之前的所有特征层的融合信息, 相比原始B的输入包含更多上下文信息。采用Tanh函数对记忆链的信息进行压缩处理, 提升参数的稳健性, 采用Sigmoid函数对其进行过滤。C输出链接下一个新加入的特征层, D输出链接下一个记忆和滤波通道。E输出则参考MS-CNN (Multi-Scale Deep Convolutional Neural Network) <citation id="144" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>改善任务分支网络的技巧, 在C输出后增加1×1的卷积层, 再进行边框的回归。由于各个门结构和卷积层承担不同的选择功能, 因此本结构中的所有单元不共享参数。输出的特征图可表示为</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>φ</mi><msub><mrow></mrow><mtext>C</mtext></msub><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>Ι</mtext><mtext>Ι</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>φ</mi><msub><mrow></mrow><mtext>D</mtext></msub><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>φ</mi><msub><mrow></mrow><mtext>D</mtext></msub><mo>=</mo><mi>w</mi><mo stretchy="false"> (</mo><mtext>ϕ</mtext><msub><mrow></mrow><mtext>A</mtext></msub><mo stretchy="false">) </mo><mo>×</mo><mi>F</mi><msub><mrow></mrow><mtext>Ι</mtext></msub><mo stretchy="false"> (</mo><mtext>ϕ</mtext><msub><mrow></mrow><mtext>B</mtext></msub><mo stretchy="false">) </mo><mo>+</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>Ι</mtext></mrow></msub><mo stretchy="false"> (</mo><mtext>ϕ</mtext><msub><mrow></mrow><mtext>B</mtext></msub><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>φ</mi><msub><mrow></mrow><mtext>E</mtext></msub><mo>=</mo><mtext>r</mtext><mtext>e</mtext><mtext>l</mtext><mtext>u</mtext><mo stretchy="false">[</mo><mi>f</mi><msubsup><mrow></mrow><mn>0</mn><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow></msubsup><mo stretchy="false"> (</mo><mi>φ</mi><msub><mrow></mrow><mtext>D</mtext></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">式中:ϕ<sub>A</sub>、ϕ<sub>B</sub>为记忆和滤波通道的输入特征图;<i>φ</i><sub>C</sub>、<i>φ</i><sub>D</sub>、<i>φ</i><sub>E</sub>为输出特征图;<i>F</i><sub>I</sub>、<i>F</i><sub>II</sub>和<i>F</i><sub>III</sub>分别为遗忘门、输入门和输出门;relu[]为激活函数;<i>f</i><mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>为第n个卷积核大小为1×1的卷积层。三个门结构的计算方法可表示为</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>F</mi><msub><mrow></mrow><mtext>Ι</mtext></msub><mo stretchy="false"> (</mo><mi>λ</mi><mo stretchy="false">) </mo><mo>=</mo><mtext>S</mtext><mtext>i</mtext><mtext>g</mtext><mtext>m</mtext><mtext>o</mtext><mtext>i</mtext><mtext>d</mtext><mo stretchy="false">[</mo><mi>f</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow></msubsup><mo stretchy="false"> (</mo><mi>λ</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>F</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>Ι</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>λ</mi><mo stretchy="false">) </mo><mspace width="0.25em" /><mo>=</mo><mtext>S</mtext><mtext>i</mtext><mtext>g</mtext><mtext>m</mtext><mtext>o</mtext><mtext>i</mtext><mtext>d</mtext><mo stretchy="false">[</mo><mi>f</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow></msubsup><mo stretchy="false"> (</mo><mi>λ</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>×</mo><mtext>Τ</mtext><mtext>a</mtext><mtext>n</mtext><mtext>h</mtext><mo stretchy="false">[</mo><mi>f</mi><msubsup><mrow></mrow><mn>3</mn><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow></msubsup><mo stretchy="false"> (</mo><mi>λ</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>F</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>Ι</mtext><mtext>Ι</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>λ</mi><mo stretchy="false">) </mo><mo>=</mo><mtext>S</mtext><mtext>i</mtext><mtext>g</mtext><mtext>m</mtext><mtext>o</mtext><mtext>i</mtext><mtext>d</mtext><mo stretchy="false">[</mo><mi>f</mi><msubsup><mrow></mrow><mn>4</mn><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow></msubsup><mo stretchy="false"> (</mo><mtext>ϕ</mtext><msub><mrow></mrow><mtext>B</mtext></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>×</mo><mtext>Τ</mtext><mtext>a</mtext><mtext>n</mtext><mtext>h</mtext><mo stretchy="false">[</mo><mi>f</mi><msubsup><mrow></mrow><mn>3</mn><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow></msubsup><mo stretchy="false"> (</mo><mi>λ</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">式中, <i>Sigmoid</i>和<i>Tanh</i>为激活函数。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908028_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 记忆和滤波通道的结构" src="Detail/GetImg?filename=images/GXXB201908028_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 记忆和滤波通道的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908028_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Memory and filter channel structure</p>

                </div>
                <h3 id="86" name="86" class="anchor-tag">3 MSSD网络</h3>
                <div class="p1">
                    <p id="87">为验证改进的FPN网络的有效性, 以SSD网络为基础, 提出MSSD网络。MSSD算法是在SSD算法的基础上加入改进的FPN结构, 基础网络依旧沿用VGG16。而后, 在其中4个附加层上建立记忆和滤波通道。MSSD的网络结构如图4所示, 下面, 对MSSD网络的具体模块进行介绍。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88"><b>3.1 基础特征提取网络</b></h4>
                <div class="p1">
                    <p id="89">MSSD的基础特征提取网络沿用VGG16, 并与传统SSD网络一样在Conv4_3层之后增加了6个额外的特征层, 在不同感受野和接受场对目标进行特征的提取。也在ResNet101<citation id="145" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>上的同尺度的特征层上进行测试, 对比结果显示其在精度上有少量提升, 但是FPS大幅下降。虽然ResNet对图像的提取能力更强, 但其网络结构更深, 复杂度更高, 综合考虑后, 依旧采用VGG16进行特征提取。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908028_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 MSSD网络结构" src="Detail/GetImg?filename=images/GXXB201908028_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 MSSD网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908028_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 MSSD network structure</p>

                </div>
                <h4 class="anchor-tag" id="92" name="92"><b>3.2 Fusion block及记忆和滤波通道</b></h4>
                <div class="p1">
                    <p id="93">该结构即改进的FPN, Fusion block如 (3) 式所示。传统的FPN在融合当前层特征时采用1×1的卷积层进行降维处理, 而后与上层网络经过双线性插值后相加。而Fusion block为提升信息的利用率, 去除了1×1的卷积层, 相应地为保持维度一致, 利用反卷积对上层网络进行上采样。相比于双线性插值来说, 反卷积是网络通过训练习得的, 文献<citation id="146" type="reference">[<a class="sup">20</a>]</citation>也指出通过反卷积后进行融合可以得到较好的细节, 获得尽可能强的语义信息。本网络3组反卷积的参数输入通道、输出通道、卷积核尺度和步长分别为[256, 512, 2, 2]、[512, 1024, 1, 2]和[1024, 512, 2, 3]。记忆和滤波通道结构是结合LSTM网络提出的。LSTM网络在长序列依赖问题上提供了一个有效的解决方案, 而在CNN中, 随着网络的加深, 特征图的感受野增加, 特征也从细节敏感变到语义敏感。整个过程可以看作为视角上升的过程, 所有特征层可以看作为一组更加复杂的序列信息。本网络中Conv10_2和Pool11两层由于尺寸较小, 上采样重建难度较大, 夹杂噪声较多, 仅在Conv4_3、Conv7、Conv8_2和Conv9_2这4个特征层上建立横向链接。为匹配SSD网络的接受域, 最终得到的Module1、Module2和Module3特征层的尺度和维度与Conv4_3、Conv7和Conv8_2保持一致。</p>
                </div>
                <h4 class="anchor-tag" id="94" name="94"><b>3.3 锚点 (Anchors</b>) </h4>
                <div class="p1">
                    <p id="95">传统的SSD网络在每个特征层的锚点上生成4～6个默认框 (Default Boxes) 。在本网络中, 增加了锚点的覆盖度同时兼顾网络的复杂度, 在每个特征层的锚点上生成同样数目的默认框, 其比例参照文献<citation id="147" type="reference">[<a class="sup">9</a>]</citation>, 在每个尺度的特征层上设置3个横纵比[1/2, 1, 2], 每个横纵比分为[2<sup>0</sup>, 2<sup>1/3</sup>]两个级别。经计算, 对于300 pixel×300 pixel的输入图像, 产生的默认框覆盖的区域范围为30 pixel×30 pixel到330 pixel×330 pixel, 满足覆盖所有图像的目标。采用文献<citation id="148" type="reference">[<a class="sup">6</a>]</citation>的匹配方案将真实框 (Ground Truth Boxes) 匹配到默认框, 交并比 (IOU) 设置为0.5。边框回归的计算方式为计算边界框 (Bounding Boxes) 与匹配到目标的默认框的长, 宽和中心位置4个参数的偏移度。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96"><b>3.4 模型的训练</b></h4>
                <div class="p1">
                    <p id="97">在模型训练过程中, 首先对数据进行增广处理。对训练集随机进行水平翻转, 尺度变化, 亮度变化和旋转变化, 增加网络的稳健性。训练的损失函数同文献<citation id="149" type="reference">[<a class="sup">6</a>]</citation>, 位置损失采用smooth L1, 分类损失采用Log loss, 并采用难样本挖掘 (Hard Negative Mining) 对正负样本进行平衡。优化器采用传统的随机梯度下降法 (SGDR) , 学习率设置为0.001, 动量值为0.9。主干网络VGG16采用在ImageNet上的预训练模型, 其余卷积层采用均值为0、方差为0.01的高斯权重填充, 偏执值初始化为0。训练在1个GPU上进行, 批次设为32, 共进行140000批次的训练。</p>
                </div>
                <h3 id="98" name="98" class="anchor-tag">4 实  验</h3>
                <div class="p1">
                    <p id="99">实验在Ubuntu16.04系统的Pytorch框架下运行, 并使用CUDA8.0和cuDNN5.0来加速训练。计算机搭载的CPU为Corei7-8700k, 显卡为NVIDIA GTX1080Ti, 内存为32 G。网络的性能通过平均精度和检测速度 (即FPS) 来测试。在PASCAL VOC2007和PASCAL VOC2012的训练集上对MSSD网络进行训练, 训练集包含16551幅图像, 20类目标。在PASCAL VOC2007的测试集上对网络进行测试, 其中测试集包含4952幅图像。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100"><b>4.1 算法有效性研究</b></h4>
                <div class="p1">
                    <p id="101">为验证改进结构的有效性, 设置了若干组对比实验。其中实验的图像输入尺寸统一设置为300 pixel×300 pixel, 输出的检测框与真实框的IOU阈值统一为0.5, FPS为计算机在空载状态时在NVIDIA GTX1080Ti上测得。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">4.1.1 与FPN结构对比</h4>
                <div class="p1">
                    <p id="103">在MSSD网络的基础上将Conv4_3、Conv7、Conv8_2和Conv9_2这4个特征层的增强方式改为FPN, 其中横向链接的1×1卷积层的通道数同文献<citation id="150" type="reference">[<a class="sup">8</a>]</citation>的FPN结构, 设置为256。将此FPN-SSD结构与MSSD网络进行对比。MSSD选择在Conv4_3、Conv7、Conv8_2和Conv9_2这4个特征层上进行特征增强, 这里设置同时在6个特征层上进行增强对比实验, 如表1所示。从表中可以看出, MSSD网络取得了更好的检测结果, 在牺牲一定速度的情况下相比原始SSD网络, 精度提升了1.5%。当SSD网络加入FPN时, 检测精度仅仅提升了0.2%, 相对于改进的FPN在mAP (Mean Average Precision) 上相差1.3%。但是在速度上, FPN结构有明显的优势, 这是由于FPN结构首先采用了1×1的卷积对SSD的特征层进行降维处理。MSSD在6个特征层进行预测时, 精度有所下降, 这是由于Conv10_2和Pool11这两个特征层的尺度较小 (分别为3×3和1×1) , 在进行上采样时加入的噪声信息的影响更大。</p>
                </div>
                <div class="area_img" id="104">
                    <p class="img_tit">表1 加入FPN结构的SSD网络与MSSD网络的性能对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Comparison between SSD network with FPN structure and MSSD network</p>
                    <p class="img_note"></p>
                    <table id="104" border="1"><tr><td>Method</td><td>Feature layers</td><td>mAP /%</td><td>FPS</td></tr><tr><td><br />SSD<sup>[6]</sup></td><td>Conv4-Pool11</td><td>77.5</td><td>46 (Titan X) </td></tr><tr><td><br />SSD+FPN</td><td>Conv4-Conv9</td><td>77.7</td><td>53.9</td></tr><tr><td><br />MSSD</td><td>Conv4-Pool11</td><td>78.8</td><td>27.1</td></tr><tr><td><br />MSSD</td><td>Conv4-Conv9</td><td>79.0</td><td>31.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="105" name="105">4.1.2 特征融合结构</h4>
                <div class="p1">
                    <p id="106">MSSD的特征融合结构相比FPN去除了1×1的卷积层, 并将双线性插值的上采样改为反卷积, 特征融合采用相加的方式 (Sum) 。为验证该改进的有效性, 保留其他网络结构, 分别测试是否去除1×1的卷积层和是否采用反卷积的不同情况, 并与原始网络进行对比, 如表2所示, 其中√表示使用。最后, 采用参考文献<citation id="151" type="reference">[<a class="sup">21</a>]</citation>在经过门控后进行特征融合时采用的Max融合方式对本特征融合结构进行对比实验。从表中可以看出采用反卷积的算法相比于双线性插值方法在精度上有较大提升, 但速度有所下降。这是由于反卷积需要在线对参数进行训练, 复杂度相对较高。是否采用1×1的卷积层, 对网络的精度和速度影响不大。当采用Max的融合方法时, 算法检测速度基本不变, 但是精度下降较多。</p>
                </div>
                <div class="area_img" id="107">
                    <p class="img_tit">表2 融合结构的改进效果对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Comparison of improved effects of fusion structure</p>
                    <p class="img_note"></p>
                    <table id="107" border="1"><tr><td>Conv 1×1</td><td>Feature fusion mode</td><td>Deconv or bilinear interpolation</td><td>mAP /%</td><td>FPS</td></tr><tr><td><br />√</td><td>Sum</td><td>Deconv</td><td>78.9</td><td>31.2</td></tr><tr><td><br /></td><td>Sum</td><td>Deconv</td><td>79.0</td><td>31.7</td></tr><tr><td><br />√</td><td>Sum</td><td>Bilinear interpolation</td><td>78.5</td><td>40.1</td></tr><tr><td><br /></td><td>Sum</td><td>Bilinear interpolation</td><td>78.5</td><td>40.2</td></tr><tr><td><br /></td><td>Max</td><td>Deconv</td><td>78.2</td><td>36.4</td></tr><tr><td><br /></td><td>Max</td><td>Bilinear interpolation</td><td>76.6</td><td>42.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="109" name="109">4.1.3 记忆和滤波通道</h4>
                <div class="p1">
                    <p id="110">本结构是MSSD网络的核心, 该结构主要包含三个门结构。本部分分别对三个门结构的有效性作对比实验, 如表3所示。当不包含输入门时, 网络训练时发散。当仅有输入门时, 精度最低, 这是由于其对记忆链上的信息整合能力最差, 基本没有利用新加入的特征信息对记忆链进行滤波。遗忘门和输入门相结合的精度较高, 这也与文献<citation id="152" type="reference">[<a class="sup">17</a>]</citation>的结论一致——遗忘门和输出门是LSTM结构最重要的两个部分。当三个门结构同时作用时, 网络的精度最高。</p>
                </div>
                <div class="area_img" id="111">
                    <p class="img_tit">表3 记忆和滤波通道的有效性分析 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Analysis of effectiveness of memory and filter channels</p>
                    <p class="img_note"></p>
                    <table id="111" border="1"><tr><td>Number</td><td>Forget <br />gate</td><td>Input <br />gate</td><td>Output <br />gate</td><td>mAP /%</td></tr><tr><td><br />1</td><td>√</td><td></td><td></td><td>/</td></tr><tr><td><br />2</td><td></td><td>√</td><td></td><td>75.2</td></tr><tr><td><br />3</td><td></td><td></td><td>√</td><td>/</td></tr><tr><td><br />4</td><td>√</td><td>√</td><td></td><td>78.6</td></tr><tr><td><br />5</td><td>√</td><td></td><td>√</td><td>/</td></tr><tr><td><br />6</td><td></td><td>√</td><td>√</td><td>77.9</td></tr><tr><td><br />7</td><td>√</td><td>√</td><td>√</td><td>79</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="112" name="112">4.1.4 基础特征提取网络</h4>
                <div class="p1">
                    <p id="113">MSSD提出的特征提取网络采用了VGG16。而相比于ResNet来说, VGG16的特征提取能力明显不足, 因此将特征提取网络改为ResNet101进行对比。采取文献<citation id="153" type="reference">[<a class="sup">8</a>]</citation>的方法, 在ResNet101的第3～5个block上建立增强结构, 三个特征层分别为Conv3_12、Conv4_69和Conv5_9。对于增强结构, 对比FPN和本研究的改进FPN结构, 结果如表4所示。相比之下, 以ResNet101为基础网络的MSSD取得了最高的检测精度 (79.3%) , 相比加入FPN结构的SSD提升了0.6%, 相比VGG16下的MSSD提升了0.3%。但是ResNet101的网络层数更多 (101层) 复杂度更高, 因此FPS下降较多。其中SSD+FPN结构在ResNet101的基础网络下mAP有较大提升, FPS也大于以VGG16为基础网络的MSSD。这是由于ResNet101的特征提取能力更强, 网络深层的特征信息稳健性好, 噪声更少, 对FPN的干扰更少。FPN结构大幅度减小了预测层的通道数 (总共仅有768个通道) 。</p>
                </div>
                <div class="area_img" id="114">
                    <p class="img_tit">表4 改变基础网络性能对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Performance comparison of different basic networks</p>
                    <p class="img_note"></p>
                    <table id="114" border="1"><tr><td>Method</td><td>Network</td><td>mAP /%</td><td>FPS</td></tr><tr><td><br />SSD<sup>[11]</sup></td><td>ResNet101</td><td>77.1</td><td>18.9 (Titan X) </td></tr><tr><td><br />SSD+FPN</td><td>ResNet101</td><td>78.7</td><td>39.1</td></tr><tr><td><br />MSSD</td><td>ResNet101</td><td>79.3</td><td>23.7</td></tr><tr><td><br />MSSD</td><td>VGG16</td><td>79.0</td><td>31.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="115" name="115"><b>4.2 算法性能研究</b></h4>
                <div class="p1">
                    <p id="116">将所提算法与其他先进的深度学习目标检测算法进行比较, 如表5所示。Faster R-CNN和R-FCN<citation id="154" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>的输入尺度为600 pixel×600 pixel～1000 pixel×1000 pixel。在300 pixel×300 pixel的输入图像下, MSSD算法的平均精度可以达到79.0%, 相比SSD300的77.5%有了1.5%的提升, 可以与YOLOV3在416 pixel×416 pixel尺度的输入图像的精度相媲美。相比于在FPN下改进的、采用ResNet101基础网络的DSSD300的78.6%也有0.4%的提升。当输入图像尺寸为512 pixel×512 pixel时, MSSD网络在VGG16上精度可以达到81.0%。</p>
                </div>
                <div class="p1">
                    <p id="117">表6为其他在SSD网络上进行改进的算法对比。表中训练集均为VOC2007和VOC2012, 测试集为VOC2007, 所有算法均经过预训练, IOU阈值均取0.5。相比来说, MSSD300在以VGG16为特征提取网络时兼顾了速度和精度, 在SSD的众多改进网络中取得了更好的性能。在输入为300 pixel×300 pixel图像时, MSSD的精度在各类算法中最高。</p>
                </div>
                <div class="p1">
                    <p id="118">表7为在300 pixel×300 pixel 尺度输入时的SSD算法和MSSD算法对小目标的检测精度对比。按照文献<citation id="155" type="reference">[<a class="sup">26</a>]</citation>中将32 pixel×32 pixel 以下的目标定义为小目标进行测试 (验证集的真实目标仅保留32 pixel×32 pixel 以下的) 。从表中可以看出改进的网络对小目标的检测也有部分提升 (0.9%) 。</p>
                </div>
                <div class="area_img" id="119">
                    <p class="img_tit">表5 各类先进深度学习算法在VOC2007数据集上精度对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Accuracy comparison of various advanced deep learning algorithms on VOC2007 dataset</p>
                    <p class="img_note"></p>
                    <table id="119" border="1"><tr><td rowspan="2">Method</td><td rowspan="2">Network</td><td colspan="21"><br />mAP /%</td></tr><tr><td>Aero</td><td>Bike</td><td>Bird</td><td>Boat</td><td>Bottle</td><td>Bus</td><td>Car</td><td>Cat</td><td>Chair</td><td>Cow</td><td>Table</td><td>Dog</td><td>Horse</td><td>Mbike</td><td>Person</td><td>Plant</td><td>Sheep</td><td>Sofa</td><td>Train</td><td>Tv</td><td>Average</td></tr><tr><td>Faster<sup>[5]</sup></td><td>VGG16</td><td>76.5</td><td>79.0</td><td>70.9</td><td>65.5</td><td>52.1</td><td>83.1</td><td>84.7</td><td>86.4</td><td>52.0</td><td>81.9</td><td>65.7</td><td>84.8</td><td>84.6</td><td>77.5</td><td>76.7</td><td>38.8</td><td>73.6</td><td>73.9</td><td>83.0</td><td>72.6</td><td>73.2</td></tr><tr><td><br />ION<sup>[23]</sup></td><td>VGG16</td><td>79.2</td><td>83.1</td><td>77.6</td><td>65.6</td><td>54.9</td><td>85.4</td><td>85.1</td><td>87.0</td><td>54.4</td><td>80.6</td><td>73.8</td><td>85.3</td><td>82.2</td><td>82.2</td><td>74.4</td><td>47.1</td><td>75.8</td><td>72.7</td><td>84.2</td><td>80.4</td><td>75.6</td></tr><tr><td><br />Faster<sup>[18]</sup></td><td>ResNet101</td><td>79.8</td><td>80.7</td><td>76.2</td><td>68.3</td><td>55.9</td><td>85.1</td><td>85.3</td><td>89.8</td><td>56.7</td><td>87.8</td><td>69.4</td><td>88.3</td><td>88.9</td><td>80.9</td><td>78.4</td><td>41.7</td><td>78.6</td><td>79.8</td><td>85.3</td><td>72.0</td><td>76.4</td></tr><tr><td><br />MR-CNN<sup>[24]</sup></td><td>VGG16</td><td>80.3</td><td>84.1</td><td>78.5</td><td>70.8</td><td>68.5</td><td>88.0</td><td>85.9</td><td>87.8</td><td>60.3</td><td>85.2</td><td>73.7</td><td>87.2</td><td>86.5</td><td>85.0</td><td>76.4</td><td>48.5</td><td>76.3</td><td>75.5</td><td>85.0</td><td>81.0</td><td>78.2</td></tr><tr><td><br />R-FCN<sup>[22]</sup></td><td>ResNet101</td><td>79.9</td><td>87.2</td><td>81.5</td><td>72.0</td><td>69.8</td><td>86.8</td><td>88.5</td><td>89.8</td><td>67.0</td><td>88.1</td><td>74.5</td><td>89.8</td><td>90.6</td><td>79.9</td><td>81.2</td><td>53.7</td><td>81.8</td><td>81.5</td><td>85.9</td><td>79.9</td><td>80.5</td></tr><tr><td><br />SSD300<sup>[6]</sup></td><td>VGG16</td><td>79.5</td><td>83.9</td><td>76.0</td><td>69.6</td><td>50.5</td><td>87.0</td><td>85.7</td><td>88.1</td><td>60.3</td><td>81.5</td><td>77.0</td><td>86.1</td><td>87.5</td><td>83.97</td><td>79.4</td><td>52.3</td><td>77.9</td><td>79.5</td><td>87.6</td><td>76.8</td><td>77.5</td></tr><tr><td><br />SSD512<sup>[6]</sup></td><td>VGG16</td><td>84.8</td><td>85.1</td><td>81.5</td><td>73.0</td><td>57.8</td><td>87.8</td><td>88.3</td><td>87.4</td><td>63.5</td><td>85.4</td><td>73.2</td><td>86.2</td><td>86.7</td><td>83.9</td><td>82.5</td><td>55.6</td><td>81.7</td><td>79.0</td><td>86.6</td><td>80.0</td><td>79.5</td></tr><tr><td><br />DSSD321<sup>[11]</sup></td><td>ResNet101</td><td>81.9</td><td>84.9</td><td>80.5</td><td>68.4</td><td>53.9</td><td>85.6</td><td>86.2</td><td>88.9</td><td>61.1</td><td>83.5</td><td>78.7</td><td>86.7</td><td>88.7</td><td>86.7</td><td>79.7</td><td>51.7</td><td>78.0</td><td>80.9</td><td>87.2</td><td>79.4</td><td>78.6</td></tr><tr><td><br />DSSD513<sup>[11]</sup></td><td>ResNet101</td><td>86.6</td><td>86.2</td><td>82.6</td><td>74.9</td><td>62.5</td><td>89.0</td><td>88.7</td><td>88.8</td><td>65.2</td><td>87.0</td><td>78.7</td><td>88.2</td><td>89.0</td><td>87.5</td><td>83.7</td><td>51.1</td><td>86.3</td><td>81.6</td><td>85.7</td><td>83.7</td><td>81.5</td></tr><tr><td><br />MDSSD300<sup>[25]</sup></td><td>VGG16</td><td>86.5</td><td>87.6</td><td>78.9</td><td>70.6</td><td>55.0</td><td>86.9</td><td>87.0</td><td>88.1</td><td>58.5</td><td>84.8</td><td>73.4</td><td>84.8</td><td>89.2</td><td>88.1</td><td>78.0</td><td>52.3</td><td>78.6</td><td>74.5</td><td>86.8</td><td>80.7</td><td>78.6</td></tr><tr><td><br />MDSSD512<sup>[25]</sup></td><td>VGG16</td><td>88.8</td><td>88.7</td><td>83.2</td><td>73.7</td><td>58.3</td><td>88.2</td><td>89.3</td><td>87.4</td><td>62.4</td><td>85.1</td><td>75.1</td><td>84.7</td><td>89.7</td><td>88.3</td><td>83.2</td><td>56.7</td><td>84.0</td><td>77.4</td><td>83.9</td><td>77.6</td><td>80.3</td></tr><tr><td><br />YOLOV3<sup>[8]</sup></td><td>Darknet53</td><td>85.5</td><td>85.5</td><td>75.6</td><td>70.0</td><td>66.5</td><td>87.6</td><td>87.7</td><td>89.4</td><td>64.3</td><td>83.5</td><td>73.6</td><td>85.9</td><td>86.9</td><td>86.2</td><td>83.3</td><td>56.2</td><td>75.3</td><td>78.0</td><td>86.4</td><td>77.8</td><td>79.2</td></tr><tr><td><br />MSSD300</td><td>ResNet101</td><td>81.2</td><td>87.2</td><td>78.7</td><td>72.7</td><td>53.4</td><td>86.4</td><td>85.6</td><td>89.1</td><td>63.1</td><td>84.5</td><td>80.0</td><td>87.5</td><td>88.9</td><td>84.8</td><td>78.8</td><td>54.5</td><td>80.9</td><td>83.2</td><td>87.1</td><td>77.4</td><td>79.3</td></tr><tr><td><br />MSSD300</td><td>VGG16</td><td>81.6</td><td>85.8</td><td>78.0</td><td>74.0</td><td>55.3</td><td>86.2</td><td>86.5</td><td>88.2</td><td>64.6</td><td>85.9</td><td>76.9</td><td>85.4</td><td>87.7</td><td>85.2</td><td>79.5</td><td>51.1</td><td>78.8</td><td>80.8</td><td>87.9</td><td>78.6</td><td>79.0</td></tr><tr><td><br />MSSD500</td><td>VGG16</td><td>87.6</td><td>87.2</td><td>83.7</td><td>75.5</td><td>57.8</td><td>86.7</td><td>88.4</td><td>89.5</td><td>66.3</td><td>84.6</td><td>78.9</td><td>86.8</td><td>88.1</td><td>86.4</td><td>83.3</td><td>58.0</td><td>81.4</td><td>81.0</td><td>88.4</td><td>78.1</td><td>81.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="120">
                    <p class="img_tit">表6 各类基于SSD的改进算法的测试结果对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 6 Comparison of test results of various improved algorithms based on SSD</p>
                    <p class="img_note"></p>
                    <table id="120" border="1"><tr><td>Method</td><td>Network</td><td>FPS</td><td>GPU</td><td>#proposals</td><td>Input size / (pixel×pixel) </td><td>mAP /%</td></tr><tr><td><br />SSD300<sup>[6]</sup></td><td>VGG16</td><td>46</td><td>Titan X</td><td>8732</td><td>300×300</td><td>77.2</td></tr><tr><td><br />SSD512<sup>[6]</sup></td><td>VGG16</td><td>19</td><td>Titan X</td><td>24564</td><td>512×512</td><td>78.5</td></tr><tr><td><br />MDSSD300<sup>[25]</sup></td><td>VGG16</td><td>38.5</td><td>1080Ti</td><td>44530</td><td>300×300</td><td>78.6</td></tr><tr><td><br />MDSSD512<sup>[25]</sup></td><td>VGG16</td><td>17.3</td><td>1080Ti</td><td>-</td><td>512×512</td><td>80.3</td></tr><tr><td><br />DSSD321<sup>[11]</sup></td><td>ResNet101</td><td>9.5</td><td>Titan X</td><td>17088</td><td>321×321</td><td>78.6</td></tr><tr><td><br />DSSD513<sup>[11]</sup></td><td>ResNet101</td><td>5.5</td><td>Titan X</td><td>43688</td><td>513×513</td><td>81.5</td></tr><tr><td><br />RSSD300<sup>[14]</sup></td><td>VGG16</td><td>35</td><td>Titan X</td><td>8732</td><td>300×300</td><td>78.5</td></tr><tr><td><br />RSSD512<sup>[14]</sup></td><td>VGG16</td><td>16.6</td><td>Titan X</td><td>24564</td><td>512×512</td><td>80.8</td></tr><tr><td><br />MSSD300</td><td>ResNet101</td><td>23.7</td><td>1080Ti</td><td>8728</td><td>300×300</td><td>79.3</td></tr><tr><td><br />MSSD300</td><td>VGG16</td><td>31.7</td><td>1080Ti</td><td>8732</td><td>300×300</td><td>79.0</td></tr><tr><td><br />MSSD512</td><td>VGG16</td><td>17.3</td><td>1080Ti</td><td>24564</td><td>512×512</td><td>81.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="121">
                    <p class="img_tit">表7 SSD算法和MSSD算法对小目标的检测精度 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 7 Detection accuracy of small targets by SSD algorithm and MSSD algorithm</p>
                    <p class="img_note"></p>
                    <table id="121" border="1"><tr><td><br />Method</td><td>mAP /%</td></tr><tr><td><br />SSD<sup>[6]</sup></td><td>55.4</td></tr><tr><td><br />MSSD</td><td>56.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="122">MSSD算法的部分可视化结果如图5所示, 其中左侧为SSD算法, 右侧为MSSD算法, 坐标轴代表图像像素值。图5 (a) 中SSD算法漏检了人, 而图5 (b) 中的MSSD算法在目标有大量重合的情况下依然可以很好地检测到整个目标。图5 (c) 与图5 (d) 中都存在漏检的现象, SSD算法漏检了人和一辆摩托, MSSD算法只漏检了一辆摩托但是选框的准确度不够 (将人也包含进了选框) 。图5 (e) 和图5 (f) 中, 存在小尺度目标, SSD算法漏检而MSSD算法更好地检测到了汽车。图5 (g) 和图5 (h) 中SSD算法漏检了与沙发位置完全重合的人, 而MSSD算法检测准确。所有的选框得分上, SSD算法总体较高。总的来说, 两种算法各有优点与不足, 但是MSSD算法相对来说更具有优越性。</p>
                </div>
                <div class="area_img" id="123">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908028_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 SSD算法和MSSD算法的可视化对比。" src="Detail/GetImg?filename=images/GXXB201908028_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 SSD算法和MSSD算法的可视化对比。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908028_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Visual comparison of SSD algorithm and MSSD algorithm. </p>
                                <p class="img_note"> (a) (c) (e) (g) SSD算法; (b) (d) (f) (h) MSSD算法</p>
                                <p class="img_note"> (a) (c) (e) (g) SSD algorithm; (b) (d) (f) (h) MSSD algorithm</p>

                </div>
                <h3 id="124" name="124" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="125">分析了传统的FPN在特征融合过程中存在的不足, 为增强不同深度特征的融合效果, 结合LSTM提出了一种多门控的记忆滤波结构。为验证改进的有效性, 在SSD网络的框架上结合改进FPN提出了MSSD网络。在Pascal VOC 2007数据集上进行测试, 证明了本改进的有效性。并与其他先进的深度学习目标检测算法对比, 结果显示, 所提算法具有更好的效果, 为深度学习目标检测方向的研究者们提供了一种更有效的特征融合算法。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="12">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[1]</b> Girshick R, Donahue J, Darrell T, <i>et al</i>.Rich feature hierarchies for accurate object detection and semantic segmentation[C]∥2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:580-587.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15011900002694&amp;v=MjU2NTZDblU5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JSVYwVmF4bz1OajdCYXJLOUh0RE5wbzlGWk9zTg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Everingham M, Eslami S M A, van Gool L, <i>et al</i>.The PASCAL visual object classes challenge:a retrospective[J].International Journal of Computer Vision, 2015, 111 (1) :98-136.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_3" >
                                    <b>[3]</b>
                                 He K M, Zhang X Y, Ren S Q, <i>et al</i>.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1904-1916.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[4]</b> Girshick R.Fast R-CNN[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1440-1448.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_5" >
                                    <b>[5]</b>
                                 Ren S Q, He K M, Girshick R, <i>et al</i>.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:single shot multibox detector">

                                <b>[6]</b> Liu W, Anguelov D, Erhan D, <i>et al</i>.SSD:single shot MultiBox detector[M]//Leibe B, Matas J, Sebe N, <i>et al</i>.Lecture notes in computer science.Cham:Springer, 2016, 9905:21-37.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature pyramid networks for object detection">

                                <b>[7]</b> Lin T Y, Dollar P, Girshick R, <i>et al</i>.Feature pyramid networks for object detection[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI.New York:IEEE, 2017:936-944.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLOv3 an incremental improvement">

                                <b>[8]</b> Redmon J, Farhadi A.YOLOv3:an incremental improvement[J/OL]. (2018-04-08) [2019-01-30].https://arxiv.org/abs/1804.02767.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 Lin T Y, Goyal P, Girshick R, <i>et al</i>.Focal loss for dense object detection[C]∥2017 IEEE International Conference on Computer Vision (ICCV) , October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:2999-3007.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 Zhang S F, Wen L Y, Bian X, <i>et al</i>.Single-shot refinement neural network for object detection[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 18-23, 2018, Salt Lake City.New York:IEEE, 2018:4203-4212.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dssd:Deconvolutional single shot detector">

                                <b>[11]</b> Fu C Y, Liu W, Ranga A, <i>et al</i>.DSSD:deconvolutional single shot detector[J/OL]. (2017-01-23) [2019-01-30].https://arxiv.org/abs/1701.06659.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FSSD:feature fusion single shot multibox detector">

                                <b>[12]</b> Li Z X, Zhou F Q.FSSD:feature fusion single shot multibox detector[J/OL]. (2018-05-17) [2019-01-30].https://arxiv.org/abs/1712.00960.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[13]</b> Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J/OL]. (2015-04-10) [2019-02-02].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enhancement of SSD by concatenating feature maps for object detection">

                                <b>[14]</b> Jeong J, Park H, Kwak N.Enhancement of SSD by concatenating feature maps for object detection[J/OL]. (2017-05-26) [2019-02-01].https://arxiv.org/abs/1705.09587.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=M2Det:a single-shot object detector based on multi-level feature pyramid network">

                                <b>[15]</b> Zhao Q J, Sheng T, Wang Y T, <i>et al</i>.M2Det:a single-shot object detector based on multi-level feature pyramid network[J/OL]. (2019-01-06) [2019-02-01].https://arxiv.org/abs/1705.09587.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201812032&amp;v=MDU1NjRSTE9lWmVWdUZ5bm1VcjdBSWpYVGJMRzRIOW5Oclk5R1pvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> Liu X, Chen J, Yang D F, <i>et al</i>.Scene-coupled intelligent multi-task detection algorithm for air-to-ground remote sensing image[J].Acta Optica Sinica, 2018, 38 (12) :1215008.刘星, 陈坚, 杨东方, 等.场景耦合的空对地多任务遥感影像智能检测算法[J].光学学报, 2018, 38 (12) :1215008.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long Short-Term Memory">

                                <b>[17]</b> Graves A.Supervised sequence labelling with recurrent neural networks:long short-term memory[M].Berlin, Heidelberg:Springer, 2012:37-45.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A unified multi-scale deep convolutional neural network for fast object detection">

                                <b>[18]</b> Cai Z W, Fan Q F, Feris R S, <i>et al</i>.A unified multi-scale deep convolutional neural network for fast object detection[M]//Leibe B, Matas J, Sebe N, <i>et al</i>.Lecture notes in computer science.Cham:Springer, 2016, 9908:354-370.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_19" >
                                    <b>[19]</b>
                                 He K M, Zhang X Y, Ren S Q, <i>et al</i>.Deep residual learning for image recognition[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[20]</b> Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Crafting GBD-net for object detection">

                                <b>[21]</b> Zeng X Y, Ouyang W L, Yan J J, <i>et al</i>.Crafting GBD-net for object detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (9) :2109-2123.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=R-FCN:Object Detection via Region based Fully Convolutional Networks">

                                <b>[22]</b> Dai J, Li Y, He K, <i>et al</i>.R-FCN:object detection via region-based fully convolutional networks[C]∥Proceedings of the 30th International Conference on Neural Information Processing Systems, December 5-10, 2016, Barcelona, Spain.USA:Curran Associates Inc., 2016:379-387.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inside-outside net:detecting objects in context w ith skip pooling and recurrent neural netw orks">

                                <b>[23]</b> Bell S, Zitnick C L, Bala K, <i>et al</i>.Inside-outside net:detecting objects in context with skip pooling and recurrent neural networks[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:2874-2883.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object detection via a multi-region and semantic segmentation-aware CNN model">

                                <b>[24]</b> Gidaris S, Komodakis N.Object detection via a multi-region and semantic segmentation-aware CNN model[C]∥2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1134-1142.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MDSSD:multi-scale deconvolutional single shot detector for small objects">

                                <b>[25]</b> Xu M L, Cui L S, Lü P, <i>et al</i>.MDSSD:multi-scale deconvolutional single shot detector for small objects[J/OL]. (2018-08-19) [2019-02-02].https://arxiv.org/abs/1805.07009.
                            </a>
                        </p>
                        <p id="62">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Microsoft COCO:Common Objects in Context">

                                <b>[26]</b> Lin T Y, Maire M, Belongie S, <i>et al</i>.Microsoft COCO:common objects in context[M]∥Fleet D, Pajdla T, Schiele B, <i>et al</i>.Lecture notes in computer science.Cham:Springer, 2014, 8693:740-755.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201908028" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201908028&amp;v=MTU2MzR6cXFCdEdGckNVUkxPZVplVnVGeW5tVXI3QUlqWFRiTEc0SDlqTXA0OUhiSVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

