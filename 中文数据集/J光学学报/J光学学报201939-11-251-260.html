

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133067246690000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dGXXB201911030%26RESULT%3d1%26SIGN%3dhbUBewSLF85VnV5INW4LbDUCr8U%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201911030&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201911030&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201911030&amp;v=MjEyODllVnZGeXZnVjd6TklqWFRiTEc0SDlqTnJvOUdaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#89" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="2 基本原理 ">2 基本原理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#94" data-title="&lt;b&gt;2.1 堆叠沙漏网络&lt;/b&gt;"><b>2.1 堆叠沙漏网络</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;2.2 检测方法&lt;/b&gt;"><b>2.2 检测方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#102" data-title="3 深度网络模型 ">3 深度网络模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#103" data-title="&lt;b&gt;3.1 总体网络&lt;/b&gt;"><b>3.1 总体网络</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;3.2 面部分组特征线条化&lt;/b&gt;"><b>3.2 面部分组特征线条化</b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;3.3 点热图回归&lt;/b&gt;"><b>3.3 点热图回归</b></a></li>
                                                <li><a href="#123" data-title="&lt;b&gt;3.4 损失函数&lt;/b&gt;"><b>3.4 损失函数</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;3.5 坐标点预测&lt;/b&gt;"><b>3.5 坐标点预测</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#136" data-title="4 实  验 ">4 实  验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#137" data-title="&lt;b&gt;4.1 实验内容&lt;/b&gt;"><b>4.1 实验内容</b></a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;4.2 实验比较分析&lt;/b&gt;"><b>4.2 实验比较分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#165" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#96" data-title="图1 堆叠沙漏网络结构">图1 堆叠沙漏网络结构</a></li>
                                                <li><a href="#100" data-title="图2 面部特征点检测方法的总体框架">图2 面部特征点检测方法的总体框架</a></li>
                                                <li><a href="#105" data-title="图3 深度网络模型结构图">图3 深度网络模型结构图</a></li>
                                                <li><a href="#149" data-title="表1 面部特征点检测方法在300W测试集上的误差">表1 面部特征点检测方法在300W测试集上的误差</a></li>
                                                <li><a href="#150" data-title="表2 选择两外眼角距离作为归一化因子,面部特征点检测方法在300W full 集上的&lt;i&gt;N&lt;/i&gt;&lt;sub&gt;0.08&lt;/sub&gt;和错误率">表2 选择两外眼角距离作为归一化因子,面部特征点检测方法在300W full 集上的<i>N</i><sub>0.08</sub>和错误率</a></li>
                                                <li><a href="#152" data-title="图4 在姿态变化较大和脸部遮挡的人脸图像上的特征点检测对比实验">图4 在姿态变化较大和脸部遮挡的人脸图像上的特征点检测对比实验</a></li>
                                                <li><a href="#153" data-title="表3 选择两外眼角距离作为归一化因子,面部特征点检测方法在姿态变化较大和脸部遮挡的人脸图像上的误差">表3 选择两外眼角距离作为归一化因子,面部特征点检测方法在姿态变化较大和脸部遮挡的人脸图像上的误差</a></li>
                                                <li><a href="#155" data-title="表4 选择两外眼角距离作为归一化因子,面部特征点检测方法在300W比赛数据集上的&lt;i&gt;N&lt;/i&gt;&lt;sub&gt;0.08&lt;/sub&gt;和错误率">表4 选择两外眼角距离作为归一化因子,面部特征点检测方法在300W比赛数据集上的<i>N</i><sub>0.08</sub>和错误率</a></li>
                                                <li><a href="#157" data-title="图5 不同方法在300W竞赛数据集上的CED曲线图
(以两外眼角点矩为归一化因子)">图5 不同方法在300W竞赛数据集上的CED曲线图
(以两外眼角点矩为归一化因子)</a></li>
                                                <li><a href="#159" data-title="图6 300W竞赛数据集的全面部特征线条热图">图6 300W竞赛数据集的全面部特征线条热图</a></li>
                                                <li><a href="#160" data-title="图7 在300W比赛数据集上的检测结果">图7 在300W比赛数据集上的检测结果</a></li>
                                                <li><a href="#162" data-title="图8 在Menpo比赛数据集上的CED曲线图
(以面部图片的对角线距离为归一化因子)">图8 在Menpo比赛数据集上的CED曲线图
(以面部图片的对角线距离为归一化因子)</a></li>
                                                <li><a href="#163" data-title="表5 选择面部对角线距离作为归一化因子,面部特征点检测方法在Menpo 比赛数据集上的误差分析">表5 选择面部对角线距离作为归一化因子,面部特征点检测方法在Menpo 比赛数据集上的误差分析</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Cootes T F,Edwards G J,Taylor C J.Active appearance models[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2001,23(6):681-685." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active Appearance Models">
                                        <b>[1]</b>
                                         Cootes T F,Edwards G J,Taylor C J.Active appearance models[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2001,23(6):681-685.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Cootes T F,Taylor C J,Cooper D H,&lt;i&gt;et al&lt;/i&gt;.Active shape models-their training and application[J].Computer Vision and Image Understanding,1995,61(1):38-59." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501084429&amp;v=MDg0NDBIdEROcW85RVpPTUxDSDR3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYjdJSTFzUWFSYz1OaWZPZmJLNw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Cootes T F,Taylor C J,Cooper D H,&lt;i&gt;et al&lt;/i&gt;.Active shape models-their training and application[J].Computer Vision and Image Understanding,1995,61(1):38-59.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" Doll&#225;r P,Welinder P,Perona P.Cascaded pose regression[C]∥2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,June 13-18,2010,San Francisco,CA,USA.New York:IEEE,2010:1078-1085." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cascaded pose regression">
                                        <b>[3]</b>
                                         Doll&#225;r P,Welinder P,Perona P.Cascaded pose regression[C]∥2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,June 13-18,2010,San Francisco,CA,USA.New York:IEEE,2010:1078-1085.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Xiong X H,de la Torre F.Supervised descent method and its applications to face alignment[C]∥2013 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:532-539." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised descent method and its applications to face alignment">
                                        <b>[4]</b>
                                         Xiong X H,de la Torre F.Supervised descent method and its applications to face alignment[C]∥2013 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:532-539.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" >
                                        <b>[5]</b>
                                     Sun Y,Wang X G,Tang X O.Deep convolutional network cascade for facial point detection[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3476-3483.</a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Trigeorgis G,Snape P,Nicolaou M A,&lt;i&gt;et al&lt;/i&gt;.Mnemonic descent method:a recurrent process applied for end-to-end face alignment[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:4177-4187." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mnemonic descent method:A recurrent process applied for end-to-end face alignment">
                                        <b>[6]</b>
                                         Trigeorgis G,Snape P,Nicolaou M A,&lt;i&gt;et al&lt;/i&gt;.Mnemonic descent method:a recurrent process applied for end-to-end face alignment[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:4177-4187.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" L&#252; J J,Shao X H,Xing J L,&lt;i&gt;et al&lt;/i&gt;.A deep regression architecture with two-stage re-initialization for high performance facial landmark detection[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:3691-3700." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Deep Regression Architecture with Two-Stage Re-initialization for High Performance Facial Landmark Detection">
                                        <b>[7]</b>
                                         L&#252; J J,Shao X H,Xing J L,&lt;i&gt;et al&lt;/i&gt;.A deep regression architecture with two-stage re-initialization for high performance facial landmark detection[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:3691-3700.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" Newell A,Yang K Y,Deng J.Stacked hourglass networks for human pose estimation[M]//Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9912:483-499." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked Hourglass Networks for Human Pose Estimation">
                                        <b>[8]</b>
                                         Newell A,Yang K Y,Deng J.Stacked hourglass networks for human pose estimation[M]//Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9912:483-499.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" Yang J,Liu Q S,Zhang K H.Stacked hourglass network for robust facial landmark localisation[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2025-2033." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked hourglass network for robust facial landmark localisation">
                                        <b>[9]</b>
                                         Yang J,Liu Q S,Zhang K H.Stacked hourglass network for robust facial landmark localisation[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2025-2033.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     Wu W Y,Qian C,Yang S,&lt;i&gt;et al&lt;/i&gt;.Look at boundary:a boundary-aware face alignment algorithm[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:2129-2138.</a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" Shen J,Zafeiriou S,Chrysos G G,&lt;i&gt;et al&lt;/i&gt;.The first facial landmark tracking in-the-wild challenge:benchmark and results[C]//Proceedings of the IEEE International Conference on Computer Vision Workshops,December 11-18,2015,Santiago,Chile.New York:IEEE,2015:50-58." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The First Facial Landmark Tracking in-the-Wild Challenge:Benchmark and Results">
                                        <b>[11]</b>
                                         Shen J,Zafeiriou S,Chrysos G G,&lt;i&gt;et al&lt;/i&gt;.The first facial landmark tracking in-the-wild challenge:benchmark and results[C]//Proceedings of the IEEE International Conference on Computer Vision Workshops,December 11-18,2015,Santiago,Chile.New York:IEEE,2015:50-58.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" Sagonas C,Tzimiropoulos G,Zafeiriou S,&lt;i&gt;et al&lt;/i&gt;.300 faces in-the-wild challenge:the first facial landmark localization challenge[C]∥2013 IEEE International Conference on Computer Vision Workshops,December 2-8,2013,Sydney,NSW,Australia.New York:IEEE,2013:397-403." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=300 faces in-the-wild challenge:The first facial landmark localization challenge">
                                        <b>[12]</b>
                                         Sagonas C,Tzimiropoulos G,Zafeiriou S,&lt;i&gt;et al&lt;/i&gt;.300 faces in-the-wild challenge:the first facial landmark localization challenge[C]∥2013 IEEE International Conference on Computer Vision Workshops,December 2-8,2013,Sydney,NSW,Australia.New York:IEEE,2013:397-403.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     Belhumeur P N,Jacobs D W,Kriegman D J,&lt;i&gt;et al&lt;/i&gt;.Localizing parts of faces using a consensus of exemplars[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(12):2930-2940.</a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" Le V,Brandt J,Lin Z,&lt;i&gt;et al&lt;/i&gt;.Interactive facial feature localization[M]//Fitzgibbon A,Lazebnik S,Perona P,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2012.Lecture notes in computer science.Berlin,Heidelberg:Springer,2012,7574:679-692." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interactive Facial Feature Localization">
                                        <b>[14]</b>
                                         Le V,Brandt J,Lin Z,&lt;i&gt;et al&lt;/i&gt;.Interactive facial feature localization[M]//Fitzgibbon A,Lazebnik S,Perona P,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2012.Lecture notes in computer science.Berlin,Heidelberg:Springer,2012,7574:679-692.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     Zhu X X,Ramanan D.Face detection,pose estimation,and landmark localization in the wild[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition,June 16-21,2012,Providence,RI,USA.New York:IEEE,2012:2879-2886.</a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" Jain V,Learned-Miller E G.Fddb:a benchmark for face detection in unconstrained settings[R].Amherst:UMass Amherst Technical Report,2010." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FDDB:a benchmark for face detection in unconstrained settings">
                                        <b>[16]</b>
                                         Jain V,Learned-Miller E G.Fddb:a benchmark for face detection in unconstrained settings[R].Amherst:UMass Amherst Technical Report,2010.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_17" title=" K&#246;stinger M,Wohlhart P,Roth P M,&lt;i&gt;et al&lt;/i&gt;.Annotated Facial Landmarks in the Wild:a large-scale,real-world database for facial landmark localization[C]//2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops),November 6-13,2011,Barcelona,Spain.New York:IEEE,2011:2144-2151." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Annotated Facial Landmarks in the Wild:A Large-scale,Real-world Database for Facial Landmark Localization">
                                        <b>[17]</b>
                                         K&#246;stinger M,Wohlhart P,Roth P M,&lt;i&gt;et al&lt;/i&gt;.Annotated Facial Landmarks in the Wild:a large-scale,real-world database for facial landmark localization[C]//2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops),November 6-13,2011,Barcelona,Spain.New York:IEEE,2011:2144-2151.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_18" title=" Asthana A,Zafeiriou S,Cheng S Y,&lt;i&gt;et al&lt;/i&gt;.Robust discriminative response map fitting with constrained local models[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3444-3451." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust discriminative response map fitting with constrained local models">
                                        <b>[18]</b>
                                         Asthana A,Zafeiriou S,Cheng S Y,&lt;i&gt;et al&lt;/i&gt;.Robust discriminative response map fitting with constrained local models[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3444-3451.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_19" title=" Zhang J,Shan S G,Kan M N,&lt;i&gt;et al&lt;/i&gt;.Coarse-to-fine auto-encoder networks (CFAN) for real-time face alignment[M]//Fleet D,Pajdla T,Schiele B,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8690:1-16." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coarse-to-fine auto-encoder networks (CFAN)for real-time face alignment">
                                        <b>[19]</b>
                                         Zhang J,Shan S G,Kan M N,&lt;i&gt;et al&lt;/i&gt;.Coarse-to-fine auto-encoder networks (CFAN) for real-time face alignment[M]//Fleet D,Pajdla T,Schiele B,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8690:1-16.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_20" title=" Cao X D,Wei Y C,Wen F,&lt;i&gt;et al&lt;/i&gt;.Face alignment by explicit shape regression[J].International Journal of Computer Vision,2014,107(2):177-190." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14032100000890&amp;v=MDAwNjhOajdCYXJLOEh0TE9ybzlGWk9zUEJIVTVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViN0lJMXNRYVJjPQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         Cao X D,Wei Y C,Wen F,&lt;i&gt;et al&lt;/i&gt;.Face alignment by explicit shape regression[J].International Journal of Computer Vision,2014,107(2):177-190.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_21" title=" Kazemi V,Sullivan J.One millisecond face alignment with an ensemble of regression trees[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:1867-1874." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=One millisecond face alignment with an ensemble of regression trees">
                                        <b>[21]</b>
                                         Kazemi V,Sullivan J.One millisecond face alignment with an ensemble of regression trees[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:1867-1874.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_22" title=" Ren S Q,Cao X D,Wei Y C,&lt;i&gt;et al&lt;/i&gt;.Face alignment at 3000 fps via regressing local binary features[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:1685-1692." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face Alignment at 3000 FPS via Regressing Local Binary Features">
                                        <b>[22]</b>
                                         Ren S Q,Cao X D,Wei Y C,&lt;i&gt;et al&lt;/i&gt;.Face alignment at 3000 fps via regressing local binary features[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:1685-1692.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_23" title=" Shi B G,Bai X,Liu W Y,&lt;i&gt;et al&lt;/i&gt;.Deep regression for face alignment[J/OL].(2014-09-18)[2019-05-06].https://arxiv.org/abs/1409.5230." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep regression for face alignment">
                                        <b>[23]</b>
                                         Shi B G,Bai X,Liu W Y,&lt;i&gt;et al&lt;/i&gt;.Deep regression for face alignment[J/OL].(2014-09-18)[2019-05-06].https://arxiv.org/abs/1409.5230.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_24" title=" Zhu S Z,Li C,Loy C C,&lt;i&gt;et al&lt;/i&gt;.Face alignment by coarse-to-fine shape searching[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:4998-5006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face alignment by coarse-to-fine shape searching">
                                        <b>[24]</b>
                                         Zhu S Z,Li C,Loy C C,&lt;i&gt;et al&lt;/i&gt;.Face alignment by coarse-to-fine shape searching[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:4998-5006.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_25" title=" Zhang Z P,Luo P,Loy C C,&lt;i&gt;et al&lt;/i&gt;.Learning deep representation for face alignment with auxiliary attributes[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(5):918-930." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep representation for face alignment with auxiliary attributes">
                                        <b>[25]</b>
                                         Zhang Z P,Luo P,Loy C C,&lt;i&gt;et al&lt;/i&gt;.Learning deep representation for face alignment with auxiliary attributes[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(5):918-930.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_26" title=" Xiao S T,Feng J S,Xing J L,&lt;i&gt;et al&lt;/i&gt;.Robust facial landmark detection via recurrent attentive-refinement networks[M]//Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9905:57-72." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust facial landmark detection via recurrent attentive-refinement networks">
                                        <b>[26]</b>
                                         Xiao S T,Feng J S,Xing J L,&lt;i&gt;et al&lt;/i&gt;.Robust facial landmark detection via recurrent attentive-refinement networks[M]//Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9905:57-72.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_27" title=" Kumar A,Chellappa R.Disentangling 3D pose in a dendritic CNN for unconstrained 2D face alignment[C]∥2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:430-439." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Disentangling 3D pose in a dendritic CNN for unconstrained 2D face alignment">
                                        <b>[27]</b>
                                         Kumar A,Chellappa R.Disentangling 3D pose in a dendritic CNN for unconstrained 2D face alignment[C]∥2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:430-439.
                                    </a>
                                </li>
                                <li id="65">


                                    <a id="bibliography_28" title=" Dong X Y,Yan Y,Ouyang W L,&lt;i&gt;et al&lt;/i&gt;.Style aggregated network for facial landmark detection[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:379-388." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Style aggregated network for facial landmark detection">
                                        <b>[28]</b>
                                         Dong X Y,Yan Y,Ouyang W L,&lt;i&gt;et al&lt;/i&gt;.Style aggregated network for facial landmark detection[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:379-388.
                                    </a>
                                </li>
                                <li id="67">


                                    <a id="bibliography_29" title=" Kowalski M,Naruniec J.Face alignment using K-cluster regression forests with weighted splitting[J].IEEE Signal Processing Letters,2016,23(11):1567-1571." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face alignment using K-cluster regression forests with weighted splitting">
                                        <b>[29]</b>
                                         Kowalski M,Naruniec J.Face alignment using K-cluster regression forests with weighted splitting[J].IEEE Signal Processing Letters,2016,23(11):1567-1571.
                                    </a>
                                </li>
                                <li id="69">


                                    <a id="bibliography_30" title=" Baltrusaitis T,Robinson P,Morency L P.Constrained local neural fields for robust facial landmark detection in the wild[C]∥2013 IEEE International Conference on Computer Vision Workshops,December 2-8,2013,Sydney,NSW,Australia.New York:IEEE,2013:354-361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Constrained local neural fields for robust facial landmark detection in the w ild">
                                        <b>[30]</b>
                                         Baltrusaitis T,Robinson P,Morency L P.Constrained local neural fields for robust facial landmark detection in the wild[C]∥2013 IEEE International Conference on Computer Vision Workshops,December 2-8,2013,Sydney,NSW,Australia.New York:IEEE,2013:354-361.
                                    </a>
                                </li>
                                <li id="71">


                                    <a id="bibliography_31" title=" Zhou E J,Fan H Q,Cao Z M,&lt;i&gt;et al&lt;/i&gt;.Extensive facial landmark localization with coarse-to-fine convolutional network cascade[C]∥2013 IEEE International Conference on Computer Vision Workshops,December 2-8,2013,Sydney,NSW,Australia.New York:IEEE,2013:386-391." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extensive facial landmark localization with coarse-to-fine convolutional network cascade">
                                        <b>[31]</b>
                                         Zhou E J,Fan H Q,Cao Z M,&lt;i&gt;et al&lt;/i&gt;.Extensive facial landmark localization with coarse-to-fine convolutional network cascade[C]∥2013 IEEE International Conference on Computer Vision Workshops,December 2-8,2013,Sydney,NSW,Australia.New York:IEEE,2013:386-391.
                                    </a>
                                </li>
                                <li id="73">


                                    <a id="bibliography_32" title=" Yan J J,Lei Z,Yi D,&lt;i&gt;et al&lt;/i&gt;.Learn to combine multiple hypotheses for accurate face alignment[C]//2013 IEEE International Conference on Computer Vision Workshops,December 2-8,2013,Sydney,NSW,Australia.New York:IEEE,2013:392-396." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learn to combine multiple hypotheses for accurate face alignment">
                                        <b>[32]</b>
                                         Yan J J,Lei Z,Yi D,&lt;i&gt;et al&lt;/i&gt;.Learn to combine multiple hypotheses for accurate face alignment[C]//2013 IEEE International Conference on Computer Vision Workshops,December 2-8,2013,Sydney,NSW,Australia.New York:IEEE,2013:392-396.
                                    </a>
                                </li>
                                <li id="75">


                                    <a id="bibliography_33" title=" Zadeh A,Baltrusaitis T,Morency L P.Convolutional experts constrained local model for facial landmark detection[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2051-2059." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional experts constrained local model for facial landmark detection">
                                        <b>[33]</b>
                                         Zadeh A,Baltrusaitis T,Morency L P.Convolutional experts constrained local model for facial landmark detection[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2051-2059.
                                    </a>
                                </li>
                                <li id="77">


                                    <a id="bibliography_34" title=" Feng Z H,Kittler J,Awais M,&lt;i&gt;et al&lt;/i&gt;.Face detection,bounding box aggregation and pose estimation for robust facial landmark localisation in the wild[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2106-2111." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face detection bounding box aggregation and pose estimation for robust facial landmark localisation in the wild">
                                        <b>[34]</b>
                                         Feng Z H,Kittler J,Awais M,&lt;i&gt;et al&lt;/i&gt;.Face detection,bounding box aggregation and pose estimation for robust facial landmark localisation in the wild[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2106-2111.
                                    </a>
                                </li>
                                <li id="79">


                                    <a id="bibliography_35" title=" Shao X H,Xing J L,L&#252; J,&lt;i&gt;et al&lt;/i&gt;.Unconstrained face alignment without face detection[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2069-2077." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unconstrained face alignment without face detection">
                                        <b>[35]</b>
                                         Shao X H,Xing J L,L&#252; J,&lt;i&gt;et al&lt;/i&gt;.Unconstrained face alignment without face detection[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2069-2077.
                                    </a>
                                </li>
                                <li id="81">


                                    <a id="bibliography_36" title=" Xiao S T,Li J S,Chen Y P,&lt;i&gt;et al&lt;/i&gt;.3D-assisted coarse-to-fine extreme-pose facial landmark detection[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2060-2068." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D-assisted coarse-to-fine extreme-pose facial landmark detection">
                                        <b>[36]</b>
                                         Xiao S T,Li J S,Chen Y P,&lt;i&gt;et al&lt;/i&gt;.3D-assisted coarse-to-fine extreme-pose facial landmark detection[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2060-2068.
                                    </a>
                                </li>
                                <li id="83">


                                    <a id="bibliography_37" title=" Chen X,Zhou E J,Mo Y C,&lt;i&gt;et al&lt;/i&gt;.Delving deep into coarse-to-fine framework for facial landmark localization[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2088-2095." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Delving Deep Into Coarse-to-Fine Framework for Facial Landmark Localization">
                                        <b>[37]</b>
                                         Chen X,Zhou E J,Mo Y C,&lt;i&gt;et al&lt;/i&gt;.Delving deep into coarse-to-fine framework for facial landmark localization[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2088-2095.
                                    </a>
                                </li>
                                <li id="85">


                                    <a id="bibliography_38" title=" Wu W Y,Yang S.Leveraging intra and inter-dataset variations for robust face alignment[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2096-2105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Leveraging intra and inter-dataset variations for robust face alignment">
                                        <b>[38]</b>
                                         Wu W Y,Yang S.Leveraging intra and inter-dataset variations for robust face alignment[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2096-2105.
                                    </a>
                                </li>
                                <li id="87">


                                    <a id="bibliography_39" title=" He Z L,Zhang J,Kan M N,&lt;i&gt;et al&lt;/i&gt;.Robust FEC-CNN:a high accuracy facial landmark detection system[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2044-2050." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust FECCNN:a high accuracy facial landmark detection system">
                                        <b>[39]</b>
                                         He Z L,Zhang J,Kan M N,&lt;i&gt;et al&lt;/i&gt;.Robust FEC-CNN:a high accuracy facial landmark detection system[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2044-2050.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-07-23 13:23</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(11),251-260 DOI:10.3788/AOS201939.1115003            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于沙漏网络的人脸面部特征点检测</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E5%A8%81%E9%A9%B0&amp;code=43341140&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵威驰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E5%85%B6%E6%9D%B0&amp;code=08483866&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵其杰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%9F%E4%BF%8A%E6%99%94&amp;code=43341141&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江俊晔</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8D%A2%E5%BB%BA%E9%9C%9E&amp;code=25695980&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卢建霞</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6%E6%9C%BA%E7%94%B5%E5%B7%A5%E7%A8%8B%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=0017580&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海大学机电工程与自动化学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E5%B8%82%E6%99%BA%E8%83%BD%E5%88%B6%E9%80%A0%E5%8F%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海市智能制造及机器人重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对头部姿态变化较大、脸部遮挡等情况下,由面部特征类型多样和尺度不同造成的面部特征点检测准确度较低的问题,提出了一种面部分组特征线条化和点热图回归相结合的人脸特征点检测方法,并设计了两段式堆叠沙漏网络深度学习模型来实现图像特征分析与特征点定位。利用提出的方法开发了检测算法,并利用该领域几个典型的公共图像数据集,将所提方法与其他方法进行实验对比。结果表明,提出的方法可以适应姿态变化和脸部部分遮挡的应用,相比其他方法,具有检测误差较小、人脸面部特征点检测准确度较高的优势。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E8%84%B8%E7%89%B9%E5%BE%81%E7%82%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人脸特征点;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A0%86%E5%8F%A0%E6%B2%99%E6%BC%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">堆叠沙漏网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E7%BA%BF%E6%9D%A1%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征线条化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%83%AD%E5%9B%BE%E5%9B%9E%E5%BD%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">热图回归;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    赵其杰,E-mail:zqj@shu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-07</p>

                    <p>

                            <b>基金：</b>
                                                        <span>上海汽车工业科技发展基金(1735);</span>
                    </p>
            </div>
                    <h1><b>New Method for Face Landmark Detection Based on Stacked-Hourglass Network</b></h1>
                    <h2>
                    <span>Zhao Weichi</span>
                    <span>Zhao Qijie</span>
                    <span>Jiang Junye</span>
                    <span>Lu Jianxia</span>
            </h2>
                    <h2>
                    <span>School of Mechatronic Engineering and Automation, Shanghai University</span>
                    <span>Shanghai Key Laboratory of Intelligent Manufacturing and Robotics</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A method that combines facial dividing feature line and point heatmap regression is proposed to address the problem of low accuracy of face landmark detection caused by different facial feature types and scales in the cases of large posture changes and occlusion. A deep learning model based on two-stage stacked hourglass network is designed to realize feature analysis and landmark location. Based on the proposed method, the detection algorithm is developed, and the proposed method is compared with other methods by experiments based on several common image datasets. The experimental results show that the proposed method can adapt to the applications of large posture changes and face partial occlusion. Compared with other methods, the proposed method has less detection error and higher accuracy in face landmark detection.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=face%20landmarks&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">face landmarks;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=stacked-hourglass%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">stacked-hourglass network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20line&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature line;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=heatmap%20regression&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">heatmap regression;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-05-07</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="89" name="89" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="90">人脸面部特征检测是人脸识别、人脸认证等信息处理的关键。面部特征检测主要检测人脸的整体结构和几何特征,即人脸的关键特征点。人脸的面部特征具有不同的尺度和类型,如眼睛和鼻子的图像尺度和特征类型相差较大,如果直接从整体上求取面部不同特征的关键点,则在一定的姿势变化和遮挡干扰下无法确保特征点的精度。</p>
                </div>
                <div class="p1">
                    <p id="91">面部特征点检测方法包括基于主动外观模型(AAM)和主动形状模型(ASM)的方法<citation id="175" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>、基于级联回归和深度网络的方法,以及近期提出的热图回归方法。ASM和AAM利用形状变化模型与纹理变化模型检测特征点,容易受到形状特点变化的影响。在基于级联回归和深度网络的方法中:基于级联的姿态回归(CPR)<citation id="167" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>利用级联回归框架解决姿态估计问题,使用形状索引的关键点特征和级联回归器来预测形状残差;利用稀疏分布存储器模型(SDM)<citation id="168" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提取尺度不变特征变换(SIFT)特征,并采用更简单的线性回归量。近年来,深度网络在级联回归框架下的面部特征点检测领域取得了较大的进步:Sun等<citation id="169" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出级联深度卷积神经网络(DCNN)来逐步预测面部特征点;从粗到细的端到端递归卷积系统(MDM)<citation id="170" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>与DCNN类似,但其每个阶段都将前一阶段的隐藏层特征作为输入;Lü等<citation id="171" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>将面部分成几个部分以减轻面部部分特征的变化,并分别回归不同部分的坐标,使坐标回归模型具有无需任何后处理即可推断特征点坐标的优点。这类方法虽然精度比AAM和ASM高,但无法适应较大的姿态变化,人脸特征点检测表现不如热图回归深度学习模型。热图回归模型分别为每个特征点生成概率热图,在人脸特征点检测中表现优异。Newell等<citation id="172" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>使用热图回归模型,并设计出堆叠沙漏网络(Stacked-Hourglass),从多尺度提取特征来估计人体姿态关键点。堆叠沙漏网络可以反复获取不同尺度下图像所包含的信息,比较适用于人脸特征点检测。Yang等<citation id="173" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>使用标准化面部的监督变换和堆叠沙漏网络来获得预测热图,取得了较好的效果,证明了堆叠沙漏网络在面部特征检测上的优越性。Wu等<citation id="174" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>使用人脸边界热图代替人脸特征点热图来表达人脸几何结构,证明了边界信息的重要性。但是,上述研究只是部分地解决了面部特征类型不同和面部特征尺度不同的问题,而且组合过多的堆叠沙漏网络会影响检测速度。</p>
                </div>
                <div class="p1">
                    <p id="92">本文提出并设计了一种有效的人脸特征检测算法,解决了面部特征尺度不同和特征类型不同情况下,具有姿势变化、光照变化和遮挡干扰的人脸面部信息的提取问题,并提升了检测的精度。该研究主要利用面部分组特征,并将其全简化为线条来解决特征尺度不同和特征类型不同的问题。面部特征分开检测,特别适用于初步解决特征尺度不同的问题;特征线条化则能进一步解决特征类型不同的问题,将复杂多变的多维面部特征简化为简单同一的低维面部特征线条。用分组的面部特征线条组成整张全面部特征线条图,在此基础上进行点热图回归。这样可以避免大量干扰特征,相比直接在面部图像上进行检测,所提方法更加高效。也就是说,通过面部分组特征线条化和点热图回归相结合的方法(FDL-PHR)可求解面部特征点。</p>
                </div>
                <h3 id="93" name="93" class="anchor-tag">2 基本原理</h3>
                <h4 class="anchor-tag" id="94" name="94"><b>2.1 堆叠沙漏网络</b></h4>
                <div class="p1">
                    <p id="95">堆叠沙漏网络可以反复获取不同尺度下图像所包含的信息,适用于检测人脸特征点。此模型基于残差模块。残差模块能够基于卷积运算提取高级特征,同时可以通过跳过路径保留原始信息。如图1所示,特征图经残差模块卷积输入(input),之后进行池化下采样(max pooling)。下采样之前的特征图经过卷积得以保留,下采样之后的特征图则继续卷积下采样操作。当达到一定分辨率后,利用残差模块卷积和上采样对特征图进行处理,并将其与之前保留下来的具有相同分辨率特征的图像相融合(addition),如此反复,一直到达到原始分辨率为止,形成输出(output)。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911030_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 堆叠沙漏网络结构" src="Detail/GetImg?filename=images/GXXB201911030_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 堆叠沙漏网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911030_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Structure of stacked-hourglass network</p>

                </div>
                <h4 class="anchor-tag" id="97" name="97"><b>2.2 检测方法</b></h4>
                <div class="p1">
                    <p id="98">检测过程框架如图2所示,主要包括5个阶段:输入原始图像,面部特征线条化,点热图回归,阈值化确定坐标和输出面部特征点。首先,将面部特征转化为特征线条。将面部特征线条分为脸部轮廓、鼻子、眉毛、眼睛和嘴唇等5个部位的特征线条图,其中,由于嘴唇轮廓十分接近,将其单独拆分成上下嘴唇外边、上嘴唇内边和下嘴唇内边。在得到6个面部特征线条图后,将其组合成整个面部的特征线条图,即全面部特征线条图。在全面部特征线条图中使用堆叠沙漏网络求取特征点热图。在点热图中进行阈值分割和处理后,就能得到特征点<i>j</i>的具体坐标(<i>x</i><sup>*</sup><sub><i>j</i></sub>,<i>y</i><sup>*</sup><sub><i>j</i></sub>)。</p>
                </div>
                <div class="p1">
                    <p id="99">使用两段式堆叠沙漏网络提取脸部不同部位的特征线条,来解决遮挡和姿态变化引起的特征线条提取问题。脸部不同部位的特征线条可以互相参考预测,即知道鼻子的特征线条,可以更好地预测嘴唇的特征线条,同时又可以预测眼睛的特征线条。线条组热图代表不同部位的特征线条,即包含了所有部位的相互关系,可以看作图模型。因此将前一段堆叠沙漏网络得到的热图作为下一段堆叠沙漏网络的输入,意味着下一段堆叠沙漏网络可以使用人脸不同部位关节点的相互关系,从而解决遮挡和姿态变化引起的特征线条提取问题,避免特征线条的缺失。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911030_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 面部特征点检测方法的总体框架" src="Detail/GetImg?filename=images/GXXB201911030_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 面部特征点检测方法的总体框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911030_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Overall framework of face landmark detection method</p>

                </div>
                <h3 id="102" name="102" class="anchor-tag">3 深度网络模型</h3>
                <h4 class="anchor-tag" id="103" name="103"><b>3.1 总体网络</b></h4>
                <div class="p1">
                    <p id="104">如图3所示,所使用的深度学习模型可以分为4部分:图像初始化模块(图3中1),面部分组特征线条化模块(图3中2),点热图回归模块(图3中3)和坐标点预测模块(图3中4)。初始化指图像经过卷积、残差模块和池化下采样操作后,初步获得图像特征并将其作为下一部分的输入。面部分组特征线条化指使用两段堆叠沙漏网络多尺度地提取面部分组特征线条,得到分组的面部特征线条热图并将其组成人脸线条热图。在面部特征线条热图上,点热图模型同样使用两段堆叠沙漏网络,得到人脸特征点热图。在进行预测时,将所求得的人脸特征点热图进行阈值分割,就可以得到人脸特征点坐标。</p>
                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911030_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 深度网络模型结构图" src="Detail/GetImg?filename=images/GXXB201911030_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 深度网络模型结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911030_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Structural diagram of depth network model</p>

                </div>
                <h4 class="anchor-tag" id="106" name="106"><b>3.2 面部分组特征线条化</b></h4>
                <div class="p1">
                    <p id="107">面部分组特征线条化回归模型网络使用两段堆叠沙漏网络(图3)。当组合两个堆叠沙漏网络时,堆叠沙漏网络输出分成两支:一支经过1×1卷积形成一个线条热图的集合,使用热图来代替原来的全连接层,热图表示的是各个特征在该像素出现的概率;另一支经过残差模块卷积提取特征。这两支的信息与堆叠沙漏网络的输入信息融合,形成下一个堆叠沙漏网络的输入。第二段堆叠沙漏网络的输出经过1×1卷积,形成一个线条热图的集合。将第二段堆叠沙漏网络所得的面部特征线条热图集合进行加权合并(即merge模块),构成全面部特征线条热图,将其作为特征点热图的输入。</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>c</mtext><mtext>e</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext></mrow></msub></mrow></munderover><msup><mi mathvariant="bold-italic">Μ</mi><mo>′</mo></msup></mstyle><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">式中:<i><b>f</b></i><sub>ace</sub>为全面部特征线条图,分为6个部位的特征线条组;<i><b>M</b></i><sub>line,</sub><sub><i>i</i></sub>表示一个部位的特征线条组,<i>i</i>表示特征线条组的索引。<i><b>M</b></i><sub>line,</sub><sub><i>i</i></sub>由所在面部部位的多个特征线条<i><b>H</b></i><sub>line,</sub><sub><i>k</i></sub>组成:</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>i</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><mi mathvariant="bold-italic">Η</mi></mstyle><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>k</mi></mrow></msub><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">以左眼为例,其特征线条组由上眼部轮廓特征线条和下眼部轮廓特征线条组成。设<i><b>M</b></i>′<sub>line,</sub><sub><i>i</i></sub>是预测的特征线条组热图,<i><b>M</b></i><sub>line,</sub><sub><i>i</i></sub>是真实的特征线条组热图,<i>n</i><sub>line</sub>是线条组数量(设为6),下标line表示线条。定义线条组热图<i><b>M</b></i>′<sub>line,</sub><sub><i>i</i></sub>时,需要定义单个线条热图,即</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>k</mi></mrow></msub><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="false">)</mo></mtd><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="false">)</mo></mtd><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mn>0</mn><mo stretchy="false">)</mo></mtd><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">式中:<i><b>H</b></i><sub>line,</sub><sub><i>k</i></sub>表示真实特征线条<i>k</i>的热图;<i>f</i><sub>line,</sub><sub><i>k</i></sub>(<i>x</i>,<i>y</i>)表示<i><b>H</b></i><sub>line,</sub><sub><i>k</i></sub>中坐标(<i>x</i>,<i>y</i>)的像素值;<i>m</i>表示图片<i><b>H</b></i><sub>line,</sub><sub><i>k</i></sub>的高;<i>n</i>表示图片<i><b>H</b></i><sub>line,</sub><sub><i>k</i></sub>的宽。线条热图是面部特征线条在图像上的数值化表现形式,表示面部线条特征的概率,即特征线条在图中出现的概率,使用像素值表示此概率值。线条热图中每个像素的响应由其到特征线条的距离决定,像素越接近线条出现位置,像素值越大,即图中像素的值越大,表示线条越有可能出现在该位置。</p>
                </div>
                <div class="p1">
                    <p id="114">根据所有特征点的位置进行三次样条插值后,以插值生成的插值点和原来的特征点作为真实特征线条<i>k</i>的坐标,(<i>x</i><sub><i>k</i></sub>,<i>y</i><sub><i>k</i></sub>)表示线条<i>k</i>的坐标集合,<i>f</i><sub>line,</sub><sub><i>k</i></sub>(<i>x</i>,<i>y</i>)的值由坐标(<i>x</i>,<i>y</i>)到线条<i>k</i>所有坐标(<i>x</i><sub><i>k</i></sub>,<i>y</i><sub><i>k</i></sub>)的距离求和决定,即</p>
                </div>
                <div class="p1">
                    <p id="115" class="code-formula">
                        <mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mi>n</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mrow><mo>(</mo><mrow><mo>-</mo><mfrac><mrow><mrow><mrow><mo>|</mo><mrow><mi>x</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>k</mi></msub><mo>,</mo><mi>k</mi></mrow></msub></mrow><mo>|</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mrow><mo>|</mo><mrow><mi>y</mi><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>k</mi></msub><mo>,</mo><mi>k</mi></mrow></msub></mrow><mo>|</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>)</mo></mrow><mo>,</mo></mtd></mtr><mtr><mtd><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>k</mi></msub><mo>,</mo><mi>k</mi></mrow></msub><mo>∈</mo><mi>x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>k</mi></msub><mo>,</mo><mi>k</mi></mrow></msub><mo>∈</mo><mi>y</mi><msub><mrow></mrow><mi>k</mi></msub><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="116">式中:<i>i</i><sub><i>k</i></sub>为线条<i>k</i>上坐标点的索引;<i>n</i><sub><i>k</i></sub>为线条<i>k</i>上坐标点的数量;<i>x</i><sub><i>ik</i></sub><sub>,</sub><sub><i>k</i></sub>为线条<i>k</i>上坐标点的横坐标;<i>y</i><sub><i>ik</i></sub><sub>,</sub><sub><i>k</i></sub>为线条<i>k</i>上坐标点的纵坐标;<i>σ</i>为标准差,一般设为2。将各个面部部位的<i><b>H</b></i><sub>line,</sub><sub><i>k</i></sub>组成真实特征线条组热图<i><b>M</b></i><sub>line,</sub><sub><i>i</i></sub>。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117"><b>3.3 点热图回归</b></h4>
                <div class="p1">
                    <p id="118">全面部特征线条热图的点热图模型网络使用两段堆叠沙漏网络(图3)。真实特征点<i>j</i>的热图可表示为</p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="false">)</mo></mtd><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="false">)</mo></mtd><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mn>0</mn><mo stretchy="false">)</mo></mtd><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">式中:<i>f</i><sub>point,</sub><sub><i>j</i></sub>(<i>x</i>,<i>y</i>)表示<i><b>H</b></i><sub>point,</sub><sub><i>j</i></sub>中坐标(<i>x</i>,<i>y</i>)的像素值;(<i>x</i><sub><i>j</i></sub>,<i>y</i><sub><i>j</i></sub>)表示特征点<i>j</i>的坐标。<i>f</i><sub>point,</sub><sub><i>j</i></sub>(<i>x</i>,<i>y</i>)由坐标(<i>x</i>,<i>y</i>)到特征点坐标(<i>x</i><sub><i>j</i></sub>,<i>y</i><sub><i>j</i></sub>)的距离决定,越靠近特征点,其值越大,其表达式为</p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula">
                        <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mi>exp</mi><mrow><mo>(</mo><mrow><mo>-</mo><mfrac><mrow><mrow><mo>|</mo><mrow><mi>x</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo>|</mo><mrow><mi>y</mi><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>)</mo></mrow><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="122">一般地,点热图模型需要使用很多段堆叠沙漏网络,但因为面部线条热图上只保留了面部特征线条,提取了关键特征,所以面部点热图模型只需使用较少的堆叠沙漏网络就可以进行点热图回归。</p>
                </div>
                <h4 class="anchor-tag" id="123" name="123"><b>3.4 损失函数</b></h4>
                <div class="p1">
                    <p id="124">图3中的深度学习网络采用中间监督的方式,即模型整体输出特征线条热图和特征点热图,并计算其与目标值的损失值。面部分组特征线条化图模型采用均方差计算线条热图集合与目标线条特征热图集合的距离,并将其作为线条热图的损失值。特征点热图模型也采用均方差计算点热图集合与目标点特征热图集合的距离,并将其作为特征点热图的损失值。将两个损失值加权相加,形成最终的总体损失函数, 即</p>
                </div>
                <div class="p1">
                    <p id="125" class="code-formula">
                        <mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>s</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext></mrow></msub></mrow></munderover><mrow><mrow><mo>|</mo><mrow><msup><mi mathvariant="bold-italic">Μ</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>i</mi></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mo>,</mo><mi>i</mi></mrow></msub></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext></mrow></msub></mrow></munderover><mrow><mrow><mo>|</mo><mrow><msup><mi mathvariant="bold-italic">Η</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="126">式中:<i><b>M</b></i>′<sub>line,</sub><sub><i>i</i></sub>和<i><b>M</b></i><sub>line,</sub><sub><i>i</i></sub>分别表示预测线条组热图和真实线条组热图;<i><b>H</b></i>′<sub>point,</sub><sub><i>j</i></sub>和<i><b>H</b></i><sub>point,</sub><sub><i>j</i></sub>分别表示预测点热图和真实点热图;<i>n</i><sub>line</sub>和<i>n</i><sub>point</sub>分别是线条组和点的数量,<i>n</i><sub>line</sub>=6, <i>n</i><sub>point</sub>=68。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127"><b>3.5 坐标点预测</b></h4>
                <div class="p1">
                    <p id="128">在所得点热图<i><b>H</b></i>′<sub>point,</sub><sub><i>j</i></sub>(图2)上进行处理,<i><b>H</b></i>′<sub>point,</sub><sub><i>j</i></sub>表示特征点<i>j</i>的热图,对其进行阈值分割,得到二值化图像<i>G</i><sub>point,j</sub>,其中,<i>g</i><sub>point,</sub><sub><i>j</i></sub>(<i>x</i>,<i>y</i>)表示<i>G</i><sub>point,</sub><sub><i>j</i></sub>中坐标为(<i>x</i>,<i>y</i>)的像素值,其表达式为</p>
                </div>
                <div class="p1">
                    <p id="129" class="code-formula">
                        <mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>g</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mn>1</mn><mo>,</mo></mtd><mtd><msup><mi>f</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>≥</mo><mi>c</mi></mtd></mtr><mtr><mtd><mn>0</mn><mo>,</mo></mtd><mtd><msup><mi>f</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>&lt;</mo><mi>c</mi></mtd></mtr></mtable></mrow></mrow><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="130">式中:<i>f</i>′<sub>point,</sub><sub><i>j</i></sub>(<i>x</i>,<i>y</i>)表示<i><b>H</b></i>′<sub>point,</sub><sub><i>j</i></sub>中坐标为(<i>x</i>,<i>y</i>)的像素值。</p>
                </div>
                <div class="p1">
                    <p id="131">在二值化图像中遍历像素值为1的像素,<i>W</i><sub><i>j</i></sub>表示这些像素的坐标集合:</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi>g</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo stretchy="false">}</mo><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133">对大量样本的测试情况进行统计分析,基于设定的不同阈值和分割确定的坐标情况确定图像阈值的选取范围。图像阈值<i>c</i>的范围为190～210,取图像阈值<i>c</i>为200,经测试该阈值满足分割需要。对坐标集合<i>W</i><sub><i>j</i></sub>求平均,得到特征点的坐标点(<i>x</i><sup>*</sup><sub><i>j</i></sub>,<i>y</i><sup>*</sup><sub><i>j</i></sub>)为</p>
                </div>
                <div class="p1">
                    <p id="134" class="code-formula">
                        <mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo stretchy="false">(</mo><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup><mo>,</mo><mi>y</mi><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mi>w</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><msub><mrow></mrow><mi>w</mi></msub></mrow><mrow><mi>n</mi><msub><mrow></mrow><mi>w</mi></msub></mrow></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>w</mi></msub><mo>,</mo><mi>j</mi></mrow></msub><mo>,</mo><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mi>w</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>j</mi><mrow><mi>n</mi><msub><mrow></mrow><mi>w</mi></msub></mrow></munderover><mi>y</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>w</mi></msub><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo>)</mo></mrow><mo>,</mo></mtd></mtr><mtr><mtd><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>w</mi></msub><mo>,</mo><mi>j</mi></mrow></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>w</mi></msub><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><mo>∈</mo><mi>W</mi><msub><mrow></mrow><mi>j</mi></msub><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="135">式中:<i>n</i><sub><i>w</i></sub>为聚集的特征点数目;<i>i</i><sub>w</sub>表示聚集点的索引;<i>x</i><sub><i>iw</i></sub><sub>,</sub><sub><i>j</i></sub>表示聚集点的横坐标;<i>y</i><sub><i>iw</i></sub><sub>,</sub><sub><i>j</i></sub>表示聚集点的纵坐标。</p>
                </div>
                <h3 id="136" name="136" class="anchor-tag">4 实  验</h3>
                <h4 class="anchor-tag" id="137" name="137"><b>4.1 实验内容</b></h4>
                <div class="p1">
                    <p id="138">为了评估所提出的方法,运用300VW数据集<citation id="176" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、300W数据集<citation id="177" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、300竞赛测试集<citation id="178" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、Menpo挑战数据集进行实验。300VW数据集包含114段视频,标注68个点,将其转成约210000张图像作为预训练数据。300W数据集是来自5个数据集(LFPW<citation id="179" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>,HELEN<citation id="180" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,AFW<citation id="181" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>,IBUG<citation id="182" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>和300W比赛测试集)的图像汇编,数据集中的每个图像都使用68个标记进行注释,并伴随由人脸检测器生成的重叠框。将300W数据集划分为训练和测试部分:训练部分包括AFW数据集,以及LFPW和HELEN的训练子集,总共产生3148个图像;测试数据包括剩余的数据集,如IBUG、300W比赛测试集、LFPW测试集、HELEN。为了便于与以前的方法进行比较,将此测试数据拆分为三个子集:由LFPW和HELEN的测试子集组成(554张图像)common集;由IBUG数据集组成challenge集(135幅图像);由common 集和challenge 集组成full集(689张图像)。300W竞赛测试集包含室内和室外共600张图像,其中包括各种光照、遮挡干扰和较大姿态变换的图像。Menpo挑战数据集由来自FDDB<citation id="183" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>和AFLW<citation id="184" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>数据集的图像。该图像使用与300W竞争数据相同的68个标记集注释,但没有面部检测器边界框。</p>
                </div>
                <div class="p1">
                    <p id="139">在测试集上使用累积误差分布曲线函数的面积除以对应误差阈值的值(<i>N</i><sub><i>α</i></sub>)和失败率评估所提方法。人脸特征点的统计平均误差,是指规范化后的预测坐标与真实坐标之间的平均距离,其表达式为</p>
                </div>
                <div class="p1">
                    <p id="140" class="code-formula">
                        <mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>s</mi><mi>S</mi></munderover><mi>e</mi></mstyle><msub><mrow></mrow><mi>s</mi></msub><mo>=</mo><mfrac><mrow><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext></mrow></msub></mrow></munderover><mrow><mrow><mo>|</mo><mrow><msup><mi mathvariant="bold-italic">X</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mi>s</mi><mo>,</mo><mi>j</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mrow><mi>s</mi><mo>,</mo><mi>j</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></msub></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>d</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></mfrac><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="141">式中:<i>e</i><sub><i>s</i></sub>是第<i>s</i>个测试样本的误差;<i>n</i><sub>point</sub>是特征点的数量;<i><b>X</b></i>′<sub><i>s</i></sub><sub>,</sub><sub><i>js</i></sub>是第<i>s</i>个测试样本点<i>j</i><sub><i>s</i></sub>的预测坐标矩阵;<i><b>X</b></i><sub><i>s</i></sub><sub>,</sub><sub><i>js</i></sub>是第<i>s</i>个测试样本点<i>j</i><sub><i>s</i></sub>的真实坐标矩阵;<i>d</i><sub><i>s</i></sub>是归一化因子,目的是使性能度量与实际面部大小无关,如<i>d</i><sub><i>s</i></sub>可为外眼角距离、瞳孔间距离或人脸检测框宽度;<i>S</i>是测试样本数量。</p>
                </div>
                <div class="p1">
                    <p id="142"><i>N</i><sub><i>α</i></sub>为在误差阈值<i>α</i>下累积误差分布曲线函数<i>f</i>(<i>e</i>)的面积除以误差阈值<i>α</i>的值,其表达式为</p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ν</mi><msub><mrow></mrow><mi>α</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>α</mi></mfrac><mrow><mstyle displaystyle="true"><mrow><mo>∫</mo><mrow></mrow></mrow></mstyle></mrow><msubsup><mrow></mrow><mn>0</mn><mi>α</mi></msubsup><mi>f</mi><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mtext>d</mtext><mi>e</mi><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144">式中:<i>e</i>为误差。根据不同测试样本, <i>α</i>取值一般为0.1,0.08,0.05,结合三个测试数据集的特点将<i>α</i>取值为0.08和0.05,并与相关方法进行对比分析。<i>N</i><sub><i>α</i></sub>取值范围为0～1,1表示最好的成绩。如果测试样本误差大于<i>α</i>,则认为失败。</p>
                </div>
                <div class="p1">
                    <p id="145">本文实验使用python编程语言在tensorflow1.9.0环境下构建深度学习模型并进行训练,训练完成后,此模型可在NVIDIA GTX1060的GPU显卡加速下以36 frame/s运行。</p>
                </div>
                <h4 class="anchor-tag" id="146" name="146"><b>4.2 实验比较分析</b></h4>
                <div class="p1">
                    <p id="147">表1表示不同方法在300W common集、challenge 集和full集的平均误差。测试所有(68个)点坐标,将所提方法(FDL-PHR)与MDM<citation id="185" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等其他方法进行比较,分别选择两眼中心距离和两外眼角间距离作为归一化因子,可以看出,所提方法产生的平均误差最小。</p>
                </div>
                <div class="p1">
                    <p id="148">表2显示了不同方法在300W full集上的<i>N</i><sub><i>α</i></sub>值和失败率。选择两外眼角距离作为归一化因子,所提方法的<i>N</i><sub><i>α</i></sub>为0.6893,失败率为2.35%,所提方法与以前方法相比结果更加精准。</p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit">表1 面部特征点检测方法在300W测试集上的误差 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Error of face landmark detection methods on the 300W test set </p>
                    <p class="img_note">%</p>
                    <table id="149" border="1"><tr><td>Condition</td><td>Method</td><td>Common <br />subset</td><td>Challenging <br />subset</td><td>Full <br />set</td></tr><tr><td rowspan="12"><br />Inter-<br />pupil <br />normalization</td><td>Method in Ref. [18]</td><td>6.65</td><td>19.79</td><td>9.22</td></tr><tr><td><br />Method in Ref. [19]</td><td>5.50</td><td>16.78</td><td>7.69</td></tr><tr><td><br />Method in Ref. [20]</td><td>5.28</td><td>17.00</td><td>7.58</td></tr><tr><td><br />Method in Ref. [4]</td><td>5.60</td><td>15.40</td><td>7.52</td></tr><tr><td><br />Method in Ref. [21]</td><td>5.25</td><td>13.62</td><td>6.40</td></tr><tr><td><br />Method in Ref. [22]</td><td>4.95</td><td>11.98</td><td>6.32</td></tr><tr><td><br />Method in Ref. [23]</td><td>4.51</td><td>13.80</td><td>6.31</td></tr><tr><td><br />Method in Ref. [24]</td><td>4.73</td><td>9.98</td><td>5.76</td></tr><tr><td><br />Method in Ref. [25]</td><td>4.80</td><td>8.60</td><td>5.54</td></tr><tr><td><br />Method in Ref. [26]</td><td>4.12</td><td>8.35</td><td>4.94</td></tr><tr><td><br />Method in Ref. [27]</td><td>3.67</td><td>7.62</td><td>4.44</td></tr><tr><td><br />FDL-PHR</td><td>3.22</td><td>7.92</td><td>4.14</td></tr><tr><td rowspan="5"><br />Inter-ocular <br />normalization</td><td>Method in Ref. [27]</td><td>3.67</td><td>7.62</td><td>4.44</td></tr><tr><td><br />Method in Ref. [6]</td><td>3.33</td><td>6.99</td><td>4.05</td></tr><tr><td><br />Method in Ref. [28]</td><td>3.34</td><td>6.60</td><td>3.98</td></tr><tr><td><br />Method in Ref. [29]</td><td>3.34</td><td>6.56</td><td>3.97</td></tr><tr><td><br />FDL-PHR</td><td>3.11</td><td>5.71</td><td>3.62</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="150">
                    <p class="img_tit">表2 选择两外眼角距离作为归一化因子,面部特征点检测方法在300W full 集上的<i>N</i><sub>0.08</sub>和错误率 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 <i>N</i><sub>0.08</sub> and failure rate of face landmark detection methods on the 300W full test set by inter-ocular normalization</p>
                    <p class="img_note"></p>
                    <table id="150" border="1"><tr><td>Condition</td><td>Method</td><td><i>N</i><sub>0.08</sub></td><td>Failure /%</td></tr><tr><td rowspan="5"><br />Inter-ocular <br />normalization</td><td><br />Method in Ref. [4]</td><td>0.4294</td><td>10.89</td></tr><tr><td><br />Method in Ref. [20]</td><td>0.4312</td><td>10.45</td></tr><tr><td><br />Method in Ref. [24]</td><td>0.4987</td><td>5.08</td></tr><tr><td><br />Method in Ref. [6]</td><td>0.5212</td><td>4.21</td></tr><tr><td><br />FDL-PHR</td><td>0.6893</td><td>2.35</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="151">从300W数据集中选取头部姿态变化较大和脸部遮挡的图像进行对比实验,结果如图4和表3所示,其中,文献<citation id="186" type="reference">[<a class="sup">21</a>]</citation>中方法(ERT)是基于级联回归的方法,文献<citation id="187" type="reference">[<a class="sup">6</a>]</citation>中方法(MDM)是基于深度网络的方法。图4是从选取的图像中挑出的干扰特点较为明显的6张图像,从左到右分析可知:文献<citation id="188" type="reference">[<a class="sup">21</a>]</citation>中方法在实验比较中所检测的特征点偏离较为严重,例如第2张和第5张;文献<citation id="189" type="reference">[<a class="sup">6</a>]</citation>中方法在实验中从整体上看与所提方法差别不大,但从细节上来看,例如第4张图像,其所检测的特征点在下巴处没有紧贴脸部轮廓,在下嘴唇处也发生些许偏移;而所提方法充分发挥了面部特征线条化的优势,即让特征点仅与面部特征线条有关,效果优于文献<citation id="190" type="reference">[<a class="sup">6</a>]</citation>中方法。表3体现的是全部选取图像的检测结果平均误差,可以看出,所提方法(FDL-PHR)检测的特征点更为精准,比其他两种方法表现更好,由此表明所提方法更适于处理具有较大姿态和遮挡干扰的人脸图像。</p>
                </div>
                <div class="area_img" id="152">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911030_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 在姿态变化较大和脸部遮挡的人脸图像上的特征点检测对比实验" src="Detail/GetImg?filename=images/GXXB201911030_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 在姿态变化较大和脸部遮挡的人脸图像上的特征点检测对比实验  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911030_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Comparative experiment of face landmark detection on images with large posture changes and face partial occlusion</p>

                </div>
                <div class="area_img" id="153">
                    <p class="img_tit">表3 选择两外眼角距离作为归一化因子,面部特征点检测方法在姿态变化较大和脸部遮挡的人脸图像上的误差 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Error of face landmark detection methods on face images with large posture changes and face partial occlusion by inter-ocular normalization</p>
                    <p class="img_note"></p>
                    <table id="153" border="1"><tr><td><br />Condition</td><td>Method</td><td>Error /%</td></tr><tr><td><br />Inter-ocular <br />normalization</td><td>Method in Ref. [21]<br />Method in Ref. [6]<br />FDL-PHR</td><td>13.89<br />10.02<br />8.32</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="154">表4显示了所提方法和ESR、CFSS、MDM等方法在300W比赛数据集上的评估指标。其中,所提方法的平均损失、<i>N</i><sub>0.08</sub>和失败率分别为4.35%,0.5805和2.86%。</p>
                </div>
                <div class="area_img" id="155">
                    <p class="img_tit">表4 选择两外眼角距离作为归一化因子,面部特征点检测方法在300W比赛数据集上的<i>N</i><sub>0.08</sub>和错误率 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 <i>N</i><sub>0.08</sub>and failure rate of facial landmark detection methods on the 300W competition dataset by inter- ocular normalization</p>
                    <p class="img_note"></p>
                    <table id="155" border="1"><tr><td>Condition</td><td>Method</td><td><i>N</i><sub>0.08</sub></td><td>Failure /%</td></tr><tr><td rowspan="7"><br />Inter-ocular <br />normalization</td><td><br />Method in Ref. [30]</td><td>0.1955</td><td>38.83</td></tr><tr><td><br />Method in Ref. [20]</td><td>0.3235</td><td>17.00</td></tr><tr><td><br />Method in Ref. [31]</td><td>0.3281</td><td>13.00</td></tr><tr><td><br />Method in Ref. [32]</td><td>0.3497</td><td>12.67</td></tr><tr><td><br />Method in Ref. [24]</td><td>0.3981</td><td>12.30</td></tr><tr><td><br />Method in Ref. [6]</td><td>0.4532</td><td>6.80</td></tr><tr><td><br />FDL-PHR</td><td>0.5805</td><td>2.86<br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="156">图5显示了300W 比赛数据集上各方法的累积误差分布(CED)曲线,其中,NRMSE表示标准均方根误差,Image proportion表示图片比例。平均损失选择两外眼角距离作为归一化因子。可以看出,所提方法在300W比赛数据集上比其他方法在各方面都表现得更好。</p>
                </div>
                <div class="area_img" id="157">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911030_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同方法在300W竞赛数据集上的CED曲线图
(以两外眼角点矩为归一化因子)" src="Detail/GetImg?filename=images/GXXB201911030_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同方法在300W竞赛数据集上的CED曲线图
(以两外眼角点矩为归一化因子)  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911030_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 CED of different methods for 300W competition 
dataset with inter-ocular normalization</p>

                </div>
                <div class="p1">
                    <p id="158">图6为所提方法在300W竞赛数据集上求得的全面部特征线条热图,可以发现,面部分组特征线条化有助于简单和准确地提取面部特征,为点热图回归排除大量的干扰,如特征类型和尺度不同、光照引起的像素不均匀、遮挡引起的特征不全,以及较大姿态引起的特征仿射变化等问题。图7为模型所得的点坐标结果图,进一步说明了所提方法可以优异地处理此类问题。</p>
                </div>
                <div class="area_img" id="159">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911030_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 300W竞赛数据集的全面部特征线条热图" src="Detail/GetImg?filename=images/GXXB201911030_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 300W竞赛数据集的全面部特征线条热图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911030_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Face feature line heatmaps of 300W competition test set</p>

                </div>
                <div class="area_img" id="160">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911030_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 在300W比赛数据集上的检测结果" src="Detail/GetImg?filename=images/GXXB201911030_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 在300W比赛数据集上的检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911030_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Detection results on 300W competition dataset</p>

                </div>
                <div class="p1">
                    <p id="161">在Menpo数据集上对不同方法进行比较(图8,表5)。Menpo数据集包含侧脸图,即人物只露出半边脸,在这种情况下,眼间距离会变得非常小,对结果的影响非常大,不能作为合格的归一化因子。为此,在Menpo数据集上使用面部对角线距离作为归一化因子,使其对面部姿势的变化更稳健。图8是所得的CED曲线,所提方法的CED曲线明显好于其他方法,其<i>N</i><sub>0.05</sub>为0.8679。表5表示所有方法的误差平均值、标准差和最大误差,可以看出,所提方法的各项误差指标均较小,表明该模型可以更加准确地检测人脸坐标点。</p>
                </div>
                <div class="area_img" id="162">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911030_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 在Menpo比赛数据集上的CED曲线图
(以面部图片的对角线距离为归一化因子)" src="Detail/GetImg?filename=images/GXXB201911030_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 在Menpo比赛数据集上的CED曲线图
(以面部图片的对角线距离为归一化因子)  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911030_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 CED for the Menpo competition dataset with 
face diagonal normalization</p>

                </div>
                <div class="area_img" id="163">
                    <p class="img_tit">表5 选择面部对角线距离作为归一化因子,面部特征点检测方法在Menpo 比赛数据集上的误差分析 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Error analysis of face landmark detection methods on the Menpo competition dataset by face diagonal normalization</p>
                    <p class="img_note"></p>
                    <table id="163" border="1"><tr><td>Condition</td><td>Method</td><td>Mean<br />error</td><td>Standard <br />deviation</td><td>Max <br />error</td></tr><tr><td rowspan="10"><br />Face <br />diagonal <br />normalization</td><td>Method in Ref. [33]</td><td>0.0205</td><td>0.0340</td><td>0.9467</td></tr><tr><td><br />Method in Ref. [34]</td><td>0.0182</td><td>0.0179</td><td>0.4661</td></tr><tr><td><br />Method in Ref. [35]</td><td>0.0165</td><td>0.0235</td><td>0.9612</td></tr><tr><td><br />Method in Ref. [36]</td><td>0.0159</td><td>0.0201</td><td>0.6717</td></tr><tr><td><br />Method in Ref. [37]</td><td>0.0200</td><td>0.0756</td><td>0.7290</td></tr><tr><td><br />Method in Ref. [38]</td><td>0.0135</td><td>0.0095</td><td>0.5098</td></tr><tr><td><br />Method in Ref. [29]</td><td>0.0138</td><td>0.0157</td><td>0.6312</td></tr><tr><td><br />Method in Ref. [39]</td><td>0.0139</td><td>0.0260</td><td>0.9624</td></tr><tr><td><br />Method in Ref. [9]</td><td>0.0120</td><td>0.0060</td><td>0.1453</td></tr><tr><td><br />FDL-PHR</td><td>0.0199</td><td>0.0071</td><td>0.07184</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="164">综合以上对比分析情况,图4和图6的数据集中包括了脸部不同姿态、脸部遮挡等情况,从图4和图7的特征点检测定位情况来看,所提方法都取得了较好的结果,可以适应不同的图像。利用平均误差、<i>N</i><sub><i>α</i></sub>和失败率等评估指标,在300W测试集、300W竞赛数据集,以及Menpo比赛数据集上将所提方法与其他方法进行比较,从表1～5、图5和图8的结果分析可以看出,利用面部分组特征线条化与点热图相结合的方法进行检测的误差比较小,在检测的精确性方面有较好的效果。</p>
                </div>
                <h3 id="165" name="165" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="166">将面部分组特征、特征线条化和点热图回归组合在一起来检测人脸特征点。面部分组特征可以解决面部特征类型不同的问题,特征线条化可以解决面部特征尺度不同的问题,利用点热图可以进一步地提高检测精度。将所提方法与领域内的其他方法进行实验对比,结果表明,所提方法在检测精确度方面有较为明显的提升,并且可以适应一定的面部姿态变化和遮挡干扰。研究结果为人脸面部特征点检测及应用提供了一种新的方法。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active Appearance Models">

                                <b>[1]</b> Cootes T F,Edwards G J,Taylor C J.Active appearance models[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2001,23(6):681-685.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501084429&amp;v=MDkzMThmT2ZiSzdIdEROcW85RVpPTUxDSDR3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYjdJSTFzUWFSYz1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Cootes T F,Taylor C J,Cooper D H,<i>et al</i>.Active shape models-their training and application[J].Computer Vision and Image Understanding,1995,61(1):38-59.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cascaded pose regression">

                                <b>[3]</b> Dollár P,Welinder P,Perona P.Cascaded pose regression[C]∥2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,June 13-18,2010,San Francisco,CA,USA.New York:IEEE,2010:1078-1085.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised descent method and its applications to face alignment">

                                <b>[4]</b> Xiong X H,de la Torre F.Supervised descent method and its applications to face alignment[C]∥2013 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:532-539.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" >
                                    <b>[5]</b>
                                 Sun Y,Wang X G,Tang X O.Deep convolutional network cascade for facial point detection[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3476-3483.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mnemonic descent method:A recurrent process applied for end-to-end face alignment">

                                <b>[6]</b> Trigeorgis G,Snape P,Nicolaou M A,<i>et al</i>.Mnemonic descent method:a recurrent process applied for end-to-end face alignment[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:4177-4187.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Deep Regression Architecture with Two-Stage Re-initialization for High Performance Facial Landmark Detection">

                                <b>[7]</b> Lü J J,Shao X H,Xing J L,<i>et al</i>.A deep regression architecture with two-stage re-initialization for high performance facial landmark detection[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:3691-3700.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked Hourglass Networks for Human Pose Estimation">

                                <b>[8]</b> Newell A,Yang K Y,Deng J.Stacked hourglass networks for human pose estimation[M]//Leibe B,Matas J,Sebe N,<i>et al</i>.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9912:483-499.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked hourglass network for robust facial landmark localisation">

                                <b>[9]</b> Yang J,Liu Q S,Zhang K H.Stacked hourglass network for robust facial landmark localisation[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2025-2033.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 Wu W Y,Qian C,Yang S,<i>et al</i>.Look at boundary:a boundary-aware face alignment algorithm[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:2129-2138.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The First Facial Landmark Tracking in-the-Wild Challenge:Benchmark and Results">

                                <b>[11]</b> Shen J,Zafeiriou S,Chrysos G G,<i>et al</i>.The first facial landmark tracking in-the-wild challenge:benchmark and results[C]//Proceedings of the IEEE International Conference on Computer Vision Workshops,December 11-18,2015,Santiago,Chile.New York:IEEE,2015:50-58.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=300 faces in-the-wild challenge:The first facial landmark localization challenge">

                                <b>[12]</b> Sagonas C,Tzimiropoulos G,Zafeiriou S,<i>et al</i>.300 faces in-the-wild challenge:the first facial landmark localization challenge[C]∥2013 IEEE International Conference on Computer Vision Workshops,December 2-8,2013,Sydney,NSW,Australia.New York:IEEE,2013:397-403.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 Belhumeur P N,Jacobs D W,Kriegman D J,<i>et al</i>.Localizing parts of faces using a consensus of exemplars[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(12):2930-2940.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interactive Facial Feature Localization">

                                <b>[14]</b> Le V,Brandt J,Lin Z,<i>et al</i>.Interactive facial feature localization[M]//Fitzgibbon A,Lazebnik S,Perona P,<i>et al</i>.Computer vision-ECCV 2012.Lecture notes in computer science.Berlin,Heidelberg:Springer,2012,7574:679-692.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 Zhu X X,Ramanan D.Face detection,pose estimation,and landmark localization in the wild[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition,June 16-21,2012,Providence,RI,USA.New York:IEEE,2012:2879-2886.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FDDB:a benchmark for face detection in unconstrained settings">

                                <b>[16]</b> Jain V,Learned-Miller E G.Fddb:a benchmark for face detection in unconstrained settings[R].Amherst:UMass Amherst Technical Report,2010.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Annotated Facial Landmarks in the Wild:A Large-scale,Real-world Database for Facial Landmark Localization">

                                <b>[17]</b> Köstinger M,Wohlhart P,Roth P M,<i>et al</i>.Annotated Facial Landmarks in the Wild:a large-scale,real-world database for facial landmark localization[C]//2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops),November 6-13,2011,Barcelona,Spain.New York:IEEE,2011:2144-2151.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust discriminative response map fitting with constrained local models">

                                <b>[18]</b> Asthana A,Zafeiriou S,Cheng S Y,<i>et al</i>.Robust discriminative response map fitting with constrained local models[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3444-3451.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coarse-to-fine auto-encoder networks (CFAN)for real-time face alignment">

                                <b>[19]</b> Zhang J,Shan S G,Kan M N,<i>et al</i>.Coarse-to-fine auto-encoder networks (CFAN) for real-time face alignment[M]//Fleet D,Pajdla T,Schiele B,<i>et al</i>.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8690:1-16.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14032100000890&amp;v=MjM5MDZkR2VycVFUTW53WmVadUh5am1VYjdJSTFzUWFSYz1OajdCYXJLOEh0TE9ybzlGWk9zUEJIVTVvQk1UNlQ0UFFIL2lyUg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> Cao X D,Wei Y C,Wen F,<i>et al</i>.Face alignment by explicit shape regression[J].International Journal of Computer Vision,2014,107(2):177-190.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=One millisecond face alignment with an ensemble of regression trees">

                                <b>[21]</b> Kazemi V,Sullivan J.One millisecond face alignment with an ensemble of regression trees[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:1867-1874.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face Alignment at 3000 FPS via Regressing Local Binary Features">

                                <b>[22]</b> Ren S Q,Cao X D,Wei Y C,<i>et al</i>.Face alignment at 3000 fps via regressing local binary features[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:1685-1692.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep regression for face alignment">

                                <b>[23]</b> Shi B G,Bai X,Liu W Y,<i>et al</i>.Deep regression for face alignment[J/OL].(2014-09-18)[2019-05-06].https://arxiv.org/abs/1409.5230.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face alignment by coarse-to-fine shape searching">

                                <b>[24]</b> Zhu S Z,Li C,Loy C C,<i>et al</i>.Face alignment by coarse-to-fine shape searching[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:4998-5006.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep representation for face alignment with auxiliary attributes">

                                <b>[25]</b> Zhang Z P,Luo P,Loy C C,<i>et al</i>.Learning deep representation for face alignment with auxiliary attributes[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(5):918-930.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust facial landmark detection via recurrent attentive-refinement networks">

                                <b>[26]</b> Xiao S T,Feng J S,Xing J L,<i>et al</i>.Robust facial landmark detection via recurrent attentive-refinement networks[M]//Leibe B,Matas J,Sebe N,<i>et al</i>.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9905:57-72.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Disentangling 3D pose in a dendritic CNN for unconstrained 2D face alignment">

                                <b>[27]</b> Kumar A,Chellappa R.Disentangling 3D pose in a dendritic CNN for unconstrained 2D face alignment[C]∥2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:430-439.
                            </a>
                        </p>
                        <p id="65">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Style aggregated network for facial landmark detection">

                                <b>[28]</b> Dong X Y,Yan Y,Ouyang W L,<i>et al</i>.Style aggregated network for facial landmark detection[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:379-388.
                            </a>
                        </p>
                        <p id="67">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face alignment using K-cluster regression forests with weighted splitting">

                                <b>[29]</b> Kowalski M,Naruniec J.Face alignment using K-cluster regression forests with weighted splitting[J].IEEE Signal Processing Letters,2016,23(11):1567-1571.
                            </a>
                        </p>
                        <p id="69">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Constrained local neural fields for robust facial landmark detection in the w ild">

                                <b>[30]</b> Baltrusaitis T,Robinson P,Morency L P.Constrained local neural fields for robust facial landmark detection in the wild[C]∥2013 IEEE International Conference on Computer Vision Workshops,December 2-8,2013,Sydney,NSW,Australia.New York:IEEE,2013:354-361.
                            </a>
                        </p>
                        <p id="71">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extensive facial landmark localization with coarse-to-fine convolutional network cascade">

                                <b>[31]</b> Zhou E J,Fan H Q,Cao Z M,<i>et al</i>.Extensive facial landmark localization with coarse-to-fine convolutional network cascade[C]∥2013 IEEE International Conference on Computer Vision Workshops,December 2-8,2013,Sydney,NSW,Australia.New York:IEEE,2013:386-391.
                            </a>
                        </p>
                        <p id="73">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learn to combine multiple hypotheses for accurate face alignment">

                                <b>[32]</b> Yan J J,Lei Z,Yi D,<i>et al</i>.Learn to combine multiple hypotheses for accurate face alignment[C]//2013 IEEE International Conference on Computer Vision Workshops,December 2-8,2013,Sydney,NSW,Australia.New York:IEEE,2013:392-396.
                            </a>
                        </p>
                        <p id="75">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional experts constrained local model for facial landmark detection">

                                <b>[33]</b> Zadeh A,Baltrusaitis T,Morency L P.Convolutional experts constrained local model for facial landmark detection[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2051-2059.
                            </a>
                        </p>
                        <p id="77">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face detection bounding box aggregation and pose estimation for robust facial landmark localisation in the wild">

                                <b>[34]</b> Feng Z H,Kittler J,Awais M,<i>et al</i>.Face detection,bounding box aggregation and pose estimation for robust facial landmark localisation in the wild[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2106-2111.
                            </a>
                        </p>
                        <p id="79">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unconstrained face alignment without face detection">

                                <b>[35]</b> Shao X H,Xing J L,Lü J,<i>et al</i>.Unconstrained face alignment without face detection[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2069-2077.
                            </a>
                        </p>
                        <p id="81">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D-assisted coarse-to-fine extreme-pose facial landmark detection">

                                <b>[36]</b> Xiao S T,Li J S,Chen Y P,<i>et al</i>.3D-assisted coarse-to-fine extreme-pose facial landmark detection[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2060-2068.
                            </a>
                        </p>
                        <p id="83">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Delving Deep Into Coarse-to-Fine Framework for Facial Landmark Localization">

                                <b>[37]</b> Chen X,Zhou E J,Mo Y C,<i>et al</i>.Delving deep into coarse-to-fine framework for facial landmark localization[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2088-2095.
                            </a>
                        </p>
                        <p id="85">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Leveraging intra and inter-dataset variations for robust face alignment">

                                <b>[38]</b> Wu W Y,Yang S.Leveraging intra and inter-dataset variations for robust face alignment[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2096-2105.
                            </a>
                        </p>
                        <p id="87">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust FECCNN:a high accuracy facial landmark detection system">

                                <b>[39]</b> He Z L,Zhang J,Kan M N,<i>et al</i>.Robust FEC-CNN:a high accuracy facial landmark detection system[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:2044-2050.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201911030" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201911030&amp;v=MjEyODllVnZGeXZnVjd6TklqWFRiTEc0SDlqTnJvOUdaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

