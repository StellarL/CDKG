

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135527347131250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201902021%26RESULT%3d1%26SIGN%3df85aRQU%252fJl%252bE2m2%252fnomz7gg1hLI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201902021&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201902021&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902021&amp;v=MTQ1NjBGckNVUjdxZlp1WnRGaURrVjczT0lqWFRiTEc0SDlqTXJZOUhaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="2 相关研究工作 ">2 相关研究工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="3 研究方法 ">3 研究方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#78" data-title="&lt;b&gt;3.1&lt;/b&gt;&lt;b&gt;基于深度学习的激光条纹图像分割&lt;/b&gt;"><b>3.1</b><b>基于深度学习的激光条纹图像分割</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;3.2&lt;/b&gt;&lt;b&gt;基于梯度直方图统计确定法线主方向&lt;/b&gt;"><b>3.2</b><b>基于梯度直方图统计确定法线主方向</b></a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;3.3&lt;/b&gt;&lt;b&gt;基于方向模板的灰度重心法&lt;/b&gt;"><b>3.3</b><b>基于方向模板的灰度重心法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#111" data-title="4 实验与讨论 ">4 实验与讨论</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#112" data-title="&lt;b&gt;4.1&lt;/b&gt;&lt;b&gt;实验数据与预处理&lt;/b&gt;"><b>4.1</b><b>实验数据与预处理</b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;4.2&lt;/b&gt;&lt;b&gt;光条分割结果&lt;/b&gt;"><b>4.2</b><b>光条分割结果</b></a></li>
                                                <li><a href="#123" data-title="&lt;b&gt;4.3&lt;/b&gt;&lt;b&gt;光条中心提取结果&lt;/b&gt;"><b>4.3</b><b>光条中心提取结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#139" data-title="5 结 论 ">5 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="图1 镜面反射与图像显影">图1 镜面反射与图像显影</a></li>
                                                <li><a href="#62" data-title="图2 镜面反射对光条提取造成的影响。 (a) 轨头光带处发生镜面反射; (b) 图2 (a) 的二值化结果; (c) 打磨后的钢轨发生镜面反射; (d) 图2 (c) 的二值化结果">图2 镜面反射对光条提取造成的影响。 (a) 轨头光带处发生镜面反射; (b) 图2 (a) 的二值......</a></li>
                                                <li><a href="#68" data-title="图3 激光条纹的灰度分布特征。 (a) 激光条纹图像; (b) 激光条纹灰度分布; (c) 图3 (a) 的局部放大; (d) 图3 (b) 的局部放大">图3 激光条纹的灰度分布特征。 (a) 激光条纹图像; (b) 激光条纹灰度分布; (c) 图3 (......</a></li>
                                                <li><a href="#77" data-title="图4 基于分区域多模板匹配的光条中心线提取过程">图4 基于分区域多模板匹配的光条中心线提取过程</a></li>
                                                <li><a href="#81" data-title="图5 不同深度网络结构的图像分割性能统计图">图5 不同深度网络结构的图像分割性能统计图</a></li>
                                                <li><a href="#85" data-title="图6 钢轨轮廓数据标注">图6 钢轨轮廓数据标注</a></li>
                                                <li><a href="#93" data-title="图7 轨廓激光光条区段划分">图7 轨廓激光光条区段划分</a></li>
                                                <li><a href="#99" data-title="图8 像素梯度方向。 (a) 整幅图像求梯度方向; (b) 仅在分割得到的光条上求梯度方向">图8 像素梯度方向。 (a) 整幅图像求梯度方向; (b) 仅在分割得到的光条上求梯度方向</a></li>
                                                <li><a href="#101" data-title="图9 各分割子区间对应的方向模板">图9 各分割子区间对应的方向模板</a></li>
                                                <li><a href="#105" data-title="图10 基于方向模板提取像素级光条中心">图10 基于方向模板提取像素级光条中心</a></li>
                                                <li><a href="#108" data-title="图11 基于灰度重心法提取亚像素级光条中心">图11 基于灰度重心法提取亚像素级光条中心</a></li>
                                                <li><a href="#115" data-title="图12 图像旋转与缩放">图12 图像旋转与缩放</a></li>
                                                <li><a href="#119" data-title="表1 不同大小图像的ENet分割时间对比">表1 不同大小图像的ENet分割时间对比</a></li>
                                                <li><a href="#121" data-title="图13 光条分割结果对比。 (a) 原始轨廓图像; (b) 固定阈值二值化; (c) 动态阈值二值化; (d) 深度学习 分割结果">图13 光条分割结果对比。 (a) 原始轨廓图像; (b) 固定阈值二值化; (c) 动态阈值二值化......</a></li>
                                                <li><a href="#122" data-title="图14 光条图像的卷积特征。 (a) 第一层卷积特征, 大小为16&#215;256&#215;256; (b) 第二层卷积特征, 大小为64&#215;128&#215;128; (c) 第三层卷积特征, 大小为128&#215; 164&#215;164">图14 光条图像的卷积特征。 (a) 第一层卷积特征, 大小为16×256×256; (b) 第二层......</a></li>
                                                <li><a href="#132" data-title="图15 不同光条中心提取方法的实验结果及其局部放大。 (a) 灰度重心法; (b) 方向模板法; (c) Steger方法; (d) 所提方法">图15 不同光条中心提取方法的实验结果及其局部放大。 (a) 灰度重心法; (b) 方向模板法; (c) Steger方法; (d) 所提方法</a></li>
                                                <li><a href="#134" data-title="图16 不同光条中心提取方法的精度对比">图16 不同光条中心提取方法的精度对比</a></li>
                                                <li><a href="#137" data-title="图17 不同光条中心提取方法的执行时间对比">图17 不同光条中心提取方法的执行时间对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="13">


                                    <a id="bibliography_1" title=" Li T T, Yang F, Li C, &lt;i&gt;et al&lt;/i&gt;. Exposure time optimization for line structured light sensor based on light stripe reliability evaluation[J]. Acta Optica Sinica, 2018, 38 (1) : 0112005. 李涛涛, 杨峰, 李策, 等. 基于光条信度评价的线结构光传感器曝光时间优化[J]. 光学学报, 2018, 38 (1) : 0112005." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201801023&amp;v=MDU0NTBadEZpRGtWNzNCSWpYVGJMRzRIOW5Ncm85SFo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Li T T, Yang F, Li C, &lt;i&gt;et al&lt;/i&gt;. Exposure time optimization for line structured light sensor based on light stripe reliability evaluation[J]. Acta Optica Sinica, 2018, 38 (1) : 0112005. 李涛涛, 杨峰, 李策, 等. 基于光条信度评价的线结构光传感器曝光时间优化[J]. 光学学报, 2018, 38 (1) : 0112005.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_2" title=" Wang S, Xu J Z, Zhang Y X, &lt;i&gt;et al&lt;/i&gt;. Reliability evaluation method and application for light-stripe-center extraction[J]. Acta Optica Sinica, 2011, 31 (11) : 1115001. 王顺, 徐静珠, 张益昕, 等. 结构光光条中心点信度评价方法与应用[J]. 光学学报, 2011, 31 (11) : 1115001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201111034&amp;v=MDc0OTZWNzNCSWpYVGJMRzRIOUROcm85R1lJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRGs=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Wang S, Xu J Z, Zhang Y X, &lt;i&gt;et al&lt;/i&gt;. Reliability evaluation method and application for light-stripe-center extraction[J]. Acta Optica Sinica, 2011, 31 (11) : 1115001. 王顺, 徐静珠, 张益昕, 等. 结构光光条中心点信度评价方法与应用[J]. 光学学报, 2011, 31 (11) : 1115001.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_3" title=" Li F J, Li X J, Liu Z. Amulti-scale analysis based method for extracting coordinates of laser light stripe centers[J]. Acta Optica Sinica, 2014, 34 (11) : 1110002. 李凤娇, 李小菁, 刘震. 基于多尺度分析的激光光条中心点坐标提取方法[J]. 光学学报, 2014, 34 (11) : 1110002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201411017&amp;v=MDgyNjU3cWZadVp0RmlEa1Y3M0JJalhUYkxHNEg5WE5ybzlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Li F J, Li X J, Liu Z. Amulti-scale analysis based method for extracting coordinates of laser light stripe centers[J]. Acta Optica Sinica, 2014, 34 (11) : 1110002. 李凤娇, 李小菁, 刘震. 基于多尺度分析的激光光条中心点坐标提取方法[J]. 光学学报, 2014, 34 (11) : 1110002.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_4" title=" Zhang R Y, Zhou P, Feng X, &lt;i&gt;et al&lt;/i&gt;. Rapid extraction of line-structured light stripe in large field of view[J]. Journal of Applied Optics, 2010, 31 (3) : 432-436. 张瑞瑛, 周萍, 冯煦, 等. 大视场下线结构光光条中心的快速提取[J]. 应用光学, 2010, 31 (3) : 432-436." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YYGX201003025&amp;v=MTE4NTE0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVjczQlBEVE1kckc0SDlITXJJOUhZWVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Zhang R Y, Zhou P, Feng X, &lt;i&gt;et al&lt;/i&gt;. Rapid extraction of line-structured light stripe in large field of view[J]. Journal of Applied Optics, 2010, 31 (3) : 432-436. 张瑞瑛, 周萍, 冯煦, 等. 大视场下线结构光光条中心的快速提取[J]. 应用光学, 2010, 31 (3) : 432-436.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_5" title=" Yang S M, Cho M H, Lee H Y, &lt;i&gt;et al&lt;/i&gt;. Weld line detection and process control for welding automation[J]. Measurement Science and Technology, 2007, 18 (3) : 819-826." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Weld line detection and process control for welding automation">
                                        <b>[5]</b>
                                         Yang S M, Cho M H, Lee H Y, &lt;i&gt;et al&lt;/i&gt;. Weld line detection and process control for welding automation[J]. Measurement Science and Technology, 2007, 18 (3) : 819-826.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_6" title=" Ridler T W, Calvard S. Picture thresholding using an iterative selection method[J]. IEEE Transactions on Systems Man &amp;amp; Cybernetics, 1978, 8 (8) : 630-632." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Picture thresholding using an iterative selection method">
                                        <b>[6]</b>
                                         Ridler T W, Calvard S. Picture thresholding using an iterative selection method[J]. IEEE Transactions on Systems Man &amp;amp; Cybernetics, 1978, 8 (8) : 630-632.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_7" title=" Usamentiaga R, Molleda J, Garc&#237;a D F. Fast and robust laser stripe extraction for 3D reconstruction in industrial environments[J]. Machine Vision and Applications, 2012, 23 (1) : 179-196." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003808699&amp;v=MTgyNzhveGNNSDdSN3FkWitadUZpdmtXNzdNSWxjPU5qN0Jhck80SHRIUHA0OU5ZdUlHWTNrNXpCZGg0ajk5U1hxUnJ4&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Usamentiaga R, Molleda J, Garc&#237;a D F. Fast and robust laser stripe extraction for 3D reconstruction in industrial environments[J]. Machine Vision and Applications, 2012, 23 (1) : 179-196.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_8" title=" Li Y Y, Zhang Z Y, Yuan L. Survey on linear structured light stripe center extraction[J]. Laser &amp;amp; Optoelectronics Progress, 2013, 50 (10) : 100002. 李莹莹, 张志毅, 袁林. 线结构光光条中心提取综述[J]. 激光与光电子学进展, 2013, 50 (10) : 100002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201310002&amp;v=MjMwNjlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRGtWNzNCTHlyUFpMRzRIOUxOcjQ5RlpvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Li Y Y, Zhang Z Y, Yuan L. Survey on linear structured light stripe center extraction[J]. Laser &amp;amp; Optoelectronics Progress, 2013, 50 (10) : 100002. 李莹莹, 张志毅, 袁林. 线结构光光条中心提取综述[J]. 激光与光电子学进展, 2013, 50 (10) : 100002.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_9" title=" Wang W H, Sun J H, Liu Z, &lt;i&gt;et al&lt;/i&gt;. Stripe center extrication algorithm for structured-light in rail wear dynamic measurement[J]. Laser &amp;amp; Infrared, 2010, 40 (1) : 87-90. 王伟华, 孙军华, 刘震, 等. 钢轨磨耗动态测量结构光条纹中心提取算法[J]. 激光与红外, 2010, 40 (1) : 87-90." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGHW201001022&amp;v=MjA4ODRlYkc0SDlITXJvOUhab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVjczQkx5ckQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Wang W H, Sun J H, Liu Z, &lt;i&gt;et al&lt;/i&gt;. Stripe center extrication algorithm for structured-light in rail wear dynamic measurement[J]. Laser &amp;amp; Infrared, 2010, 40 (1) : 87-90. 王伟华, 孙军华, 刘震, 等. 钢轨磨耗动态测量结构光条纹中心提取算法[J]. 激光与红外, 2010, 40 (1) : 87-90.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_10" title=" Sun J H, Wang H, Liu Z, &lt;i&gt;et al&lt;/i&gt;. Rapid extraction algorithm of laser stripe center in rail wear dynamic measurement[J]. Optics and Precision Engineering, 2011, 19 (3) : 690-696. 孙军华, 王恒, 刘震, 等. 钢轨磨耗动态测量中激光光条中心的快速提取[J]. 光学精密工程, 2011, 19 (3) : 690-696." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201103039&amp;v=MTYyMDJYQlk3RzRIOURNckk5R2JZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRGtWNzNCSWo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Sun J H, Wang H, Liu Z, &lt;i&gt;et al&lt;/i&gt;. Rapid extraction algorithm of laser stripe center in rail wear dynamic measurement[J]. Optics and Precision Engineering, 2011, 19 (3) : 690-696. 孙军华, 王恒, 刘震, 等. 钢轨磨耗动态测量中激光光条中心的快速提取[J]. 光学精密工程, 2011, 19 (3) : 690-696.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_11" title=" Liu Z, Sun J H, Wang H, &lt;i&gt;et al&lt;/i&gt;. Simple and fast rail wear measurement method based on structured light[J]. Optics and Lasers in Engineering, 2011, 49 (11) : 1343-1351." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011400063131&amp;v=MDQzNTZmYks3SHRETnE0OUZaTzBNRFg4NG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SktGOFFhQnM9TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Liu Z, Sun J H, Wang H, &lt;i&gt;et al&lt;/i&gt;. Simple and fast rail wear measurement method based on structured light[J]. Optics and Lasers in Engineering, 2011, 49 (11) : 1343-1351.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_12" title=" Xu J Z. Research on methods and evaluation of stripe center extraction in structured light 3D measurement[D]. Nanjing: Nanjing University, 2012. 徐静珠. 结构光三维测量中光条中心提取方法及其评价的研究[D]. 南京: 南京大学, 2012." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1012375706.nh&amp;v=MTc4NDI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVjczQlZGMjZITEMvRzliTXFaRWJQSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Xu J Z. Research on methods and evaluation of stripe center extraction in structured light 3D measurement[D]. Nanjing: Nanjing University, 2012. 徐静珠. 结构光三维测量中光条中心提取方法及其评价的研究[D]. 南京: 南京大学, 2012.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_13" title=" Fisher R B, Naidu D K. Acomparison of algorithms for subpixel peak detection[M]. Berlin: Springer Press, 1996: 385-404." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Acomparison of algorithms for subpixel peak detection">
                                        <b>[13]</b>
                                         Fisher R B, Naidu D K. Acomparison of algorithms for subpixel peak detection[M]. Berlin: Springer Press, 1996: 385-404.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_14" title=" Canziani A, Paszke A, Culurciello E. An analysis of deep neural network models for practical applications[EB/OL]. (2017-04-14) [2018-05-17]http://arxiv.org/abs/1605.07678." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Culurciello E. An analysis of deep neural network models for practical applications">
                                        <b>[14]</b>
                                         Canziani A, Paszke A, Culurciello E. An analysis of deep neural network models for practical applications[EB/OL]. (2017-04-14) [2018-05-17]http://arxiv.org/abs/1605.07678.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_15" title=" Abadi M, Agarwal A, Barham P. &lt;i&gt;et al&lt;/i&gt;. Tensorfolw: Large-scale machine learning on heterogeneous distributed systems[EB/OL]. (2016-03-16) [2018-05-22]http://arxiv.org/abs/1603.04467v1." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TensorFlow:Large-Scale Machine Learning on Heterogeneous Distributed Systems">
                                        <b>[15]</b>
                                         Abadi M, Agarwal A, Barham P. &lt;i&gt;et al&lt;/i&gt;. Tensorfolw: Large-scale machine learning on heterogeneous distributed systems[EB/OL]. (2016-03-16) [2018-05-22]http://arxiv.org/abs/1603.04467v1.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_16" title=" Kingma D P, Ba J. Adam: A method for stochastic optimization[EB/OL]. (2017-01-30) [2018-06-03]http://cn.arxiv.org/abs/1412.6980." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">
                                        <b>[16]</b>
                                         Kingma D P, Ba J. Adam: A method for stochastic optimization[EB/OL]. (2017-01-30) [2018-06-03]http://cn.arxiv.org/abs/1412.6980.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_17" title=" Cui X J, Yang C, Liu B H, &lt;i&gt;et al&lt;/i&gt;. Self-adaptive iterative method of extracting center of linear-structured light stripe[J]. Journal of Xi′an Jiaotong University, 2007, 41 (1) : 73-76. 崔希君, 杨川, 刘保华, 等. 线性结构光心的自适应迭代提取法[J]. 西安交通大学学报, 2007, 41 (1) : 73-76." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XAJT200701017&amp;v=MzE0MjdmWnVadEZpRGtWNzNCUFN6QmVyRzRIdGJNcm85RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         Cui X J, Yang C, Liu B H, &lt;i&gt;et al&lt;/i&gt;. Self-adaptive iterative method of extracting center of linear-structured light stripe[J]. Journal of Xi′an Jiaotong University, 2007, 41 (1) : 73-76. 崔希君, 杨川, 刘保华, 等. 线性结构光心的自适应迭代提取法[J]. 西安交通大学学报, 2007, 41 (1) : 73-76.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_18" title=" Wu Q Y, Su X Y, Li J Z, &lt;i&gt;et al&lt;/i&gt;. A new method for extracting the centre-line of line structure light-stripe[J]. Journal of Sichuan University (Engineering Science Edition) , 2007, 39 (4) : 151-155. 吴庆阳, 苏显渝, 李景镇, 等. 一种新的线结构光光带中心提取算法[J]. 四川大学学报 (工程科学版) , 2007, 39 (4) : 151-155." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SCLH200704029&amp;v=MTIzMzh0R0ZyQ1VSN3FmWnVadEZpRGtWNzNCTmk3SFpyRzRIdGJNcTQ5SGJZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Wu Q Y, Su X Y, Li J Z, &lt;i&gt;et al&lt;/i&gt;. A new method for extracting the centre-line of line structure light-stripe[J]. Journal of Sichuan University (Engineering Science Edition) , 2007, 39 (4) : 151-155. 吴庆阳, 苏显渝, 李景镇, 等. 一种新的线结构光光带中心提取算法[J]. 四川大学学报 (工程科学版) , 2007, 39 (4) : 151-155.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_19" title=" Steger C. An unbiased detector of curvilinear structures[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1998, 20 (2) : 113-125." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unbiased detector of curvilinear structures">
                                        <b>[19]</b>
                                         Steger C. An unbiased detector of curvilinear structures[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1998, 20 (2) : 113-125.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-10-07 13:47</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(02),175-184 DOI:10.3788/AOS201939.0212004            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">行车环境下钢轨轮廓激光条纹中心的提取方法</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E8%83%9C%E6%98%A5&amp;code=37165087&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王胜春</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%9F%A9%E5%BC%BA&amp;code=25654345&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">韩强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%98%8A&amp;code=26611035&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王昊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E9%91%AB%E6%AC%A3&amp;code=36496803&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵鑫欣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%88%B4%E9%B9%8F&amp;code=30759888&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">戴鹏</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E9%93%81%E9%81%93%E7%A7%91%E5%AD%A6%E7%A0%94%E7%A9%B6%E9%99%A2%E9%9B%86%E5%9B%A2%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%A3%80%E6%B5%8B%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0211574&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国铁道科学研究院集团有限公司基础设施检测研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E7%90%86%E5%AD%A6%E9%99%A2&amp;code=0111847&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京交通大学理学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>研究了行车环境下激光条纹图像中心线快速、准确且可靠的提取方法。基于ENet深度学习模型实现了激光条纹的多区段快速分割;通过统计各区段内光条梯度方向的直方图来确定各分段光条的法线主方向, 并构造了相应的方向模板;利用分区域多模板匹配的灰度重心法实现了光条中心的亚像素坐标提取。研究结果表明, 该方法可以有效克服室外行车环境中各类干扰信息对光条中心提取的影响, 单幅钢轨轮廓图像的光条提取时间仅为2.1 ms, 误差均值约为0.082 pixel, 标准差为0.047 pixel, 兼顾了光条中心提取的时效性和准确率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%93%E6%9E%84%E5%85%89%E6%B5%8B%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">结构光测量;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%89%E6%9D%A1%E4%B8%AD%E5%BF%83%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">光条中心提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A8%A1%E6%9D%BF%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多模板匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%9A%E5%83%8F%E7%B4%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">亚像素;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *王胜春, E-mail:wangshengchun@rails.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-04</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61702551);</span>
                                <span>中国铁路总公司科技研究开发计划 (2017G003-I);</span>
                                <span>中国铁道科学研究院科技研究开发计划 (2017YJ129);</span>
                                <span>北京市科技研究开发计划 (D17110600060000);</span>
                    </p>
            </div>
                    <h1>Laser Stripe Center Extraction Method of Rail Profile in Train-Running Environment</h1>
                    <h2>
                    <span>Wang Shengchun</span>
                    <span>Han Qiang</span>
                    <span>Wang Hao</span>
                    <span>Zhao Xinxin</span>
                    <span>Dai Peng</span>
            </h2>
                    <h2>
                    <span>Infrastructure Inspection Research Institute, China Academy of Railway Sciences Corporation Limited</span>
                    <span>School of Science, Beijing Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The method for the fast, accurate and reliable extraction of laser stripe centers in the train-running environments is studied. The multi-section fast segmentation of laser stripe is achieved based on the ENet deep learning model. The histograms along the gradient direction of light stripes in each section are counted to determine the normal dominant direction and construct the corresponding direction template. The gray-level centroid method with sub-region multi-template matching is proposed for the extraction of sub-pixel coordinates of light stripe center. The research results show that the proposed method can effectively overcome the influences of various types of interference information on laser stripe center extraction in the outdoor train-running environment. The light stripe extraction time is only 2.1 ms for a single rail profile, the mean error is about 0.082 pixel and the standard deviation is 0.047 pixel. The method has good time-effectiveness and high accuracy for laser stripe center extraction.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=structured-light%20measurement&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">structured-light measurement;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=light%20stripe%20center%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">light stripe center extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-template%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-template matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sub-pixel&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sub-pixel;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-07-04</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="52">在线结构光测量系统中, 激光束垂直地投射到被测物体表面, 摄像机从另一个角度拍摄激光条纹图像, 并按照三角测量法获得激光断面数据。三角测量法要求投影到物体表面的轮廓线无限薄, 即仅有一个像素宽度。但由于光线散射、成像系统的点扩展效应等, 实际轮廓线在像平面的投影图像具有一定的厚度且呈光条状。因此, 快速准确地提取激光条纹中心的位置以获得实际轮廓线上每一点准确的空间坐标值, 是线结构光测量系统需要解决的关键问题之一<citation id="141" type="reference"><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="53">行车环境中, 基于主动激光三角测量技术的钢轨轮廓测量面临着一系列难题。其中一个主要问题是目前的激光条纹提取方法对噪声和光照的变化较为敏感, 而列车实际行驶环境中, 因照明变化、不均匀反射以及表面不平顺而引入的干扰信息以噪声形式反映在激光图像上, 这些干扰因素会改变激光的轮廓形状。传统的光条中心提取方法主要以牺牲稳健性为代价来提高激光检测的精度, 这类方法往往适用于环境光和激光功率可控的室内检测场景, 而室外行车环境中存在各种噪声干扰, 会造成激光轮廓变形, 图像的亮度不均匀, 甚至部分激光完全缺失, 因此传统方法并不适合应用于铁路场景中。</p>
                </div>
                <div class="p1">
                    <p id="54">本文提出了一种可应用于铁路动态行车环境下的快速、准确和稳健的激光光条中心提取方法。基于深度学习的语义分割技术提取完整的光条区域, 并根据光条的形状曲率和明暗程度将光条分割为多个子区域;针对不同的子区域采用分区域多模板匹配的方法对光条进行分段提取。该方法有效克服了噪声的影响, 并兼顾了光条提取的时效性和准确率。</p>
                </div>
                <h3 id="55" name="55" class="anchor-tag">2 相关研究工作</h3>
                <div class="p1">
                    <p id="56">在线结构光测量中, 条纹变形的程度反映了被测物体的深度, 故结构光变形条纹中心位置的准确检测是影响结构光系统精度的关键问题之一。在结构光系统中确定结构光条纹的中心位置与通常图像处理中的细化不同。细化仅仅要求的是保持拓扑结构, 而确定结构光条纹的中心位置对准确性的要求较高。为解决结构光条纹定位的问题, 研究者们提出了多种方法。对于表面反射情况较好、图像精度较高的场合, 可以先检测光带的边界, 然后取其中间线作为结构光条纹中心的位置;对于更复杂的情况, 需要寻找恰当的光条提取算法。一般的方法大致可分为两个步骤:1) 对光条图像进行分割;2) 计算光条的中心坐标。</p>
                </div>
                <div class="p1">
                    <p id="57">当使用结构光传感器时, 稳健的条纹分割是影响钢轨轮廓线定位精度的决定性因素。目前该领域大部分的研究都集中在激光条纹的中心线检测方面, 而对激光条纹的分割研究鲜有报道。已有的研究成果中, 大多数分割方法通常以牺牲稳健性和时效性为代价<citation id="142" type="reference"><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="58">传统的激光条纹中心线提取方法有极值法、灰度重心法、模板匹配法、Steger算法以及在此基础上提出的改进方法等<citation id="143" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。对于线结构光测量中所用的单色激光器, 其出射光线的强度在其截面上一般可认为是服从高斯分布的, 光条中心提取应尽量逼近该高斯分布的中心位置。在传统方法的基础上, 一些学者结合钢轨轮廓结构光测量的应用背景, 针对性地提出了钢轨轮廓光条中心线的提取方法<citation id="144" type="reference"><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>。但研究成果多关注于光条中心提取方法的改进, 对于受环境光严重干扰的光条分割问题却鲜有报道。此外, 现有的方法大多无法兼顾时效性、准确性和稳健性, 需根据不同的应用场景选择合适的算法。面向室外高速动态的成像环境开展的研究工作, 对光条中心提取算法的性能提出了更高的要求。</p>
                </div>
                <div class="p1">
                    <p id="59">成像设备接收到的光强为光源的漫反射光、镜面反射光及环境光经被测物体表面反射的光强的总和。其中, 光线反射与物体表面的光滑程度有关, 被测物体的表面越光滑, 镜面反射的光强越集中, 越接近于理想镜面反射。如图1所示, 当测量物体的某位置发生镜面反射时, 若反射光线方向与成像设备的光轴一致, 大部分反射光将集中射入镜头, 该位置的图像会显影过亮;若反射光线方向偏离成像设备的光轴, 则很少有反射光线入射到镜头, 该位置的图像会显影过暗。因此, 钢轨表面的镜面反射会使激光条纹图像的局部区域过亮或过暗, 严重影响测量的精度。</p>
                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 镜面反射与图像显影" src="Detail/GetImg?filename=images/GXXB201902021_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 镜面反射与图像显影  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Mirror reflection and image development</p>

                </div>
                <div class="p1">
                    <p id="61">在测量过程中, 均匀的漫反射光线经图像传感器捕捉生成平滑的光条图像, 能较好地保持钢轨轮廓的几何形状。而不均匀的反射会造成光条灰度值分布明暗不均, 轨头光带区域的镜面反射甚至会造成该处的光条过暗或过亮, 因此传统的基于图像二值化的方法<citation id="146" type="reference"><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><link href="23" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>或提取光条骨架的方法<citation id="145" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>难以得到完好的光条分割图像。如图2 (a) 所示, 在拍摄的钢轨轮廓图像中, 轨头光带处由于长期与车轮接触摩擦, 表面非常光滑, 投射到该处的激光由于镜面反射而集中偏离镜头光轴, 采集的图像局部很暗。图2 (b) 为图2 (a) 二值化的结果, 难以找到合适的阈值将整个光条区域完整地分割出来。图2 (c) 为打磨后的钢轨图像, 由于钢轨表面镜面反射后的光线正好集中射入相机镜头, 导致图像中轨头区域产生较强亮度的光斑, 它们会干扰光条中心的提取。图2 (d) 为图2 (c) 二值化的结果, 可以看出, 分割效果同样很不理想。因此, 若在实际测量中同时存在不均匀的反射或镜面反射, 极易提取到非真实的光条中心而造成测量结果与真实值不符, 需要特定的光条分割算法以避免这种干扰的影响。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 镜面反射对光条提取造成的影响。 (a) 轨头光带处发生镜面反射; (b) 图2 (a) 的二值化结果; (c) 打磨后的钢轨发生镜面反射; (d) 图2 (c) 的二值化结果" src="Detail/GetImg?filename=images/GXXB201902021_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 镜面反射对光条提取造成的影响。 (a) 轨头光带处发生镜面反射; (b) 图2 (a) 的二值化结果; (c) 打磨后的钢轨发生镜面反射; (d) 图2 (c) 的二值化结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Effect of mirror reflection on stripe extraction. (a) Mirror reflection at headband; (b) binarization result of Fig. 2 (a) ; (c) mirror reflection on polished rail; (d) binarization result of Fig. 2 (c) </p>

                </div>
                <h3 id="64" name="64" class="anchor-tag">3 研究方法</h3>
                <div class="p1">
                    <p id="65">图3所示为钢轨轮廓激光条纹图像的灰度分布特征。激光条纹图像如图3 (a) 所示, 可以看出, 大部分区域为黑色背景, 光条的灰度值呈不均匀的变化。由于大部分图像背景为无效信息且受噪声干扰, 因此首先要从图像中分割出轨廓光条区域。</p>
                </div>
                <div class="p1">
                    <p id="66">在照明良好的室内环境下, 将足够强度的激光投影到被测物体表面, 通过简单的阈值法即可准确检测到激光光条的位置。然而, 在室外列车运行环境下, 即使对于单幅图像, 周围环境光也是变化的。当激光光条强度分布不均匀时, 投影表面也是变化的, 如图3 (b) 所示。因此, 难以选择合适的单一阈值将激光条纹从背景中完整地提取出来<citation id="147" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。自适应动态阈值分割方法<citation id="148" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>虽然可以在整体强光或弱光环境下得到最优的分割阈值, 但在环境光可变及物体表面非均匀漫反射的情况下, 光条灰度值较低的区域易被误判为背景。</p>
                </div>
                <div class="p1">
                    <p id="67">图3 (c) 所示为轨廓光条的局部放大图, 可以看出, 在沿着像素的梯度方向 (蓝色箭头) 即光条法线方向, 像素灰度值的分布近似于高斯分布, 如图3 (d) 所示。根据结构光成像的光学机理, 光条沿边缘法线方向的横截面像素分布可以用高斯模型<citation id="149" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>来表示, 即</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 激光条纹的灰度分布特征。 (a) 激光条纹图像; (b) 激光条纹灰度分布; (c) 图3 (a) 的局部放大; (d) 图3 (b) 的局部放大" src="Detail/GetImg?filename=images/GXXB201902021_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 激光条纹的灰度分布特征。 (a) 激光条纹图像; (b) 激光条纹灰度分布; (c) 图3 (a) 的局部放大; (d) 图3 (b) 的局部放大  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Gray-level distribution characteristics of laser stripe. (a) Laser stripe image; (b) gray-level distribution of laser stripe; (c) local zoom of Fig. 3 (a) ; (d) local zoom of Fig. 3 (b) </p>

                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mi>A</mi><mtext>e</mtext><mtext>x</mtext><mtext>p</mtext><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mi>x</mi><mo>-</mo><mi>x</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup><mi>cos</mi><mrow></mrow><mi>θ</mi></mrow></mfrac></mrow><mo>) </mo></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">式中:<i>A</i>为高斯分布的峰值;<i>σ</i>为高斯分布的标准差;<i>x</i>为沿着法线方向的像素横坐标;<i>x</i><sup>*</sup>为光条中心位置的横坐标;<i>θ</i>为法线方向与横轴的夹角, 则法线方向可记作<b><i>n</i></b>= (cos<i>θ</i>, sin<i>θ</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="72">Fisher等<citation id="150" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>通过对几种经典的光条提取方法进行比较, 发现灰度重心法以亚像素精度提取激光中心位置, 可得到最佳结果, 但该方法未考虑图像每列之间的激光光条的连续性, 易受到噪声的干扰。此外, 传统的灰度重心法基于行或列两个方向进行中心提取, 不适用于光条弯曲变化的轨廓激光图像。基于骨架的灰度重心法和基于Hessian矩阵的Steger算法较好地解决了这个问题, 能处理情况比较复杂的光带图像。但这两种方法需要求取每一个数据点处的法线方向, 增加了计算的复杂度, 降低了数据处理的速度, 不利于实时处理任务。</p>
                </div>
                <div class="p1">
                    <p id="73">针对钢轨轮廓激光条纹提取中面临的问题, 提出了基于分区域多模板匹配的光条中心提取方法, 如图4所示, 其中<i>K</i><sub><i>i</i></sub> (<i>i</i>=1, 2, …, 6) 为各分割子区域, <b><i>n</i></b><sub><i>i</i></sub> (<i>i</i>=1, 2, …, 6) 为各区域的法线主方向。整个过程分为以下三个步骤。</p>
                </div>
                <div class="p1">
                    <p id="74">1) 基于深度学习网络实现钢轨轮廓光条图像的精细分割, 按照光条的灰度和梯度方向特征将光条划分为多个区段, 每个区段单独提取光条中心。这一操作去除了背景噪声以及不均匀的漫反射和镜面反射对光条提取造成的干扰, 提高了算法的稳健性。</p>
                </div>
                <div class="p1">
                    <p id="75">2) 基于梯度直方图统计确定各分割子区域的法线主方向, 各个子区域对应一个法线主方向, 避免了复杂的模板匹配和耗时的梯度计算, 提高了算法的时效性。</p>
                </div>
                <div class="p1">
                    <p id="76">3) 对各个分割的子区域构造相应的方向模板以求得法线方向的最大光强点, 以该点为中心, 沿法线方向按照灰度重心法提取光条中心的亚像素坐标, 保证了算法的提取精度。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 基于分区域多模板匹配的光条中心线提取过程" src="Detail/GetImg?filename=images/GXXB201902021_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 基于分区域多模板匹配的光条中心线提取过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Extraction process of light stripe center line based on sub-region multi-template matching</p>

                </div>
                <h4 class="anchor-tag" id="78" name="78"><b>3.1</b><b>基于深度学习的激光条纹图像分割</b></h4>
                <div class="p1">
                    <p id="79">钢轨廓形追踪算法已经实现了轨头和轨腰区域的精确定位, 因此将轨廓光条分割限定在轨头和轨腰区域, 可以有效提高分割的效率和稳定性。</p>
                </div>
                <div class="p1">
                    <p id="80">图5所示为深度学习网络结构用于图像分割的性能统计图<citation id="151" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 分割图像大小为1024 pixel×768 pixel。其中, 横轴表示中央处理器 (CPU) 加法器操作次数, 次数越小代表速度越快;纵轴表示分割精度;圆圈的大小表示训练得到的模型大小, 对于近似的网络结构, 一般模型越大, 分割越耗时。</p>
                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同深度网络结构的图像分割性能统计图[14]" src="Detail/GetImg?filename=images/GXXB201902021_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同深度网络结构的图像分割性能统计图<citation id="152" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation><sup></sup>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Image segmentation performance statistics for different depth network structures <citation id="153" type="reference"><link href="39" rel="bibliography" /><sup>[14]</sup></citation><sup></sup></p>

                </div>
                <div class="p1">
                    <p id="83">现有的深度神经网络难以实现需要实时分割的应用, 问题在于深度神经网络需要大量的浮点运算, 运行时间长, 时效性低。ENet是针对这一问题提出的一种新型有效的深度神经网络, 相比于目前常用的SegNet分割模型, 速度增大了18倍, 浮点计算量减至1/76, 参数减至1/80, 且具有相似的分割精度。</p>
                </div>
                <div class="p1">
                    <p id="84">由图5可知, ENet是目前实时性最好的深度网络之一, 且具有较高的准确率。因此, 选择ENet作为图像分割的深度学习网络结构模型, 具体流程如下。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 钢轨轮廓数据标注" src="Detail/GetImg?filename=images/GXXB201902021_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 钢轨轮廓数据标注  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Rail profile data labeling</p>

                </div>
                <div class="p1">
                    <p id="86">1) 训练数据。使用8846幅钢轨廓形激光图像作为深度学习模型训练和验证的数据集。随机抽取数据的70%用于模型训练, 15%用于验证, 15%用于模型测试。根据轮廓图像中光条的灰度和梯度方向对数据集进行标注, 如图6所示, 利用Pascal VOC格式标注工具 (http://host.robots.ox.ac.uk/pascal/VOC/) , 按照轨头和轨腰区域分别标注为上、中、下三个区段, 各区段具有近似的灰度值和梯度方向。</p>
                </div>
                <div class="p1">
                    <p id="87">2) 数据预处理。对所有的轨廓图像进行归一化处理, 使得绝大部分的像素强度落在-0.5～0.5之间, 即[-0.5, 0.5]为像素的有效区间。通过裁剪并调整图像大小, 提高钢轨轮廓在图像中的占比。裁剪图像会增大前景 (轨廓) 在图像中的比例, 从而更容易解决细节问题并帮助模型收敛。</p>
                </div>
                <div class="p1">
                    <p id="88">3) 训练。使用TensorFlow深度学习开发框架<citation id="154" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>来训练ENet深度网络。网络权重根据Adam规则更新<citation id="155" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 使用4块NVIDIA Titan XP图形处理器 (GPU) 来训练和测试模型。在训练过程中, 通过翻转、剪切、移动、缩放和旋转图像来增强数据。</p>
                </div>
                <div class="p1">
                    <p id="89">通过图像分割技术, 可以从原大小为<i>M</i>×<i>N</i>的轨廓图像中提取空间占比很小的轨廓激光条纹图像, 后续的光条中心提取被限定在分割后的前景区域内, 避免了背景噪声的干扰且大幅提升了提取算法的执行效率。激光条纹的前景图像分割可记作</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∪</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∪</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>Ι</mi></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>→</mo><mstyle displaystyle="true"><munderover><mo>∪</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>F</mi></mstyle><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">式中:<i>I</i> (<i>i</i>, <i>j</i>) 为轨廓图像像素;符号∪表示集合区域的合并操作;<i>M</i>为图像的行数;<i>N</i>为图像的列数;<i>F</i> (<i>l</i><sub><i>i</i></sub>, <i>r</i><sub><i>i</i></sub>, <i>d</i><sub><i>i</i></sub>) 为沿着第<i>i</i>行的激光条纹截面;<i>l</i><sub><i>i</i></sub>为第<i>i</i>行激光条纹的左边界;<i>r</i><sub><i>i</i></sub>为第<i>i</i>行激光条纹的右边界;<i>d</i><sub><i>i</i></sub>为该行条纹的宽度。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92"><b>3.2</b><b>基于梯度直方图统计确定法线主方向</b></h4>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 轨廓激光光条区段划分" src="Detail/GetImg?filename=images/GXXB201902021_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 轨廓激光光条区段划分  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Section partition of rail profile laser stripe</p>

                </div>
                <div class="p1">
                    <p id="94">沿着光条法线方向提取光条中心的方法有利于提升精度, 如基于方向模板匹配的方法、基于骨架追踪的方法以及基于Hessian矩阵的Steger算法。但这些算法需要执行复杂的模板匹配和逐点的梯度计算, 较为耗时且易受局部噪声干扰, 不适用于实时检测任务。如图7所示, 轨廓激光光条具有相对固定的几何形状, 轨头和轨腰区域可以按照梯度方向的变化划分为6个子区间, 分别记作<i>S</i><sub>1</sub>, <i>S</i><sub>2</sub>, <i>S</i><sub>3</sub>, <i>S</i><sub>4</sub>, <i>S</i><sub>5</sub>, <i>S</i><sub>6</sub>。每个区间设置一个法线主方向, 记作<b><i>n</i></b><sub>1</sub>, <b><i>n</i></b><sub>2</sub>, <b><i>n</i></b><sub>3</sub>, <b><i>n</i></b><sub>4</sub>, <b><i>n</i></b><sub>5</sub>, <b><i>n</i></b><sub>6</sub>。</p>
                </div>
                <div class="p1">
                    <p id="95">轨廓图像任一像素<i>I</i> (<i>i</i>, <i>j</i>) 的梯度方向<i>θ</i>可由该点横向<i>x</i>和纵向<i>y</i>的差分来表示, 记作</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><mo>=</mo><mi>arctan</mi><mfrac><mrow><mtext>d</mtext><mi>y</mi></mrow><mrow><mtext>d</mtext><mi>x</mi></mrow></mfrac><mo>=</mo><mi>arctan</mi><mfrac><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>-</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo></mrow><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo>-</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">式中<i>y</i>为沿着法线方向的像素纵坐标。</p>
                </div>
                <div class="p1">
                    <p id="98">图8所示为图像中各像素梯度方向的分布, 其中图8 (a) 对整幅图像求梯度方向, 而图8 (b) 仅对分割后的前景轮廓光条求梯度方向。可以观察到原始图像背景中存在大量由环境光引入的无效噪声信息, 不利于各分割区间法线主方向的选取。因此, 通过分别统计各个分割子区间内光条上各点的梯度方向直方图分布, 可以将直方图统计个数最大的梯度方向作为区间的法线主方向。</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 像素梯度方向。 (a) 整幅图像求梯度方向; (b) 仅在分割得到的光条上求梯度方向" src="Detail/GetImg?filename=images/GXXB201902021_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 像素梯度方向。 (a) 整幅图像求梯度方向; (b) 仅在分割得到的光条上求梯度方向  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Gradient direction of pixels. (a) Gradient direction solved for whole image; (b) gradient direction solved only on segmented light stripe</p>

                </div>
                <div class="p1">
                    <p id="100">根据各分割子区间的法线主方向构造的方向模板如图9所示, 每个子区域直接与对应的方向模板进行互相关运算, 避免了耗时的模板遍历操作。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 各分割子区间对应的方向模板" src="Detail/GetImg?filename=images/GXXB201902021_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 各分割子区间对应的方向模板  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Direction template corresponding to eachsub-range of segmentation</p>

                </div>
                <h4 class="anchor-tag" id="102" name="102"><b>3.3</b><b>基于方向模板的灰度重心法</b></h4>
                <div class="p1">
                    <p id="103">在确定各分割子区间的法线主方向并构建相应的方向模板后, 对每个子区域应用方向模板法求取初始的像素级光条中心。以<i>S</i><sub>2</sub>子区间为例, 光条中心的提取过程如图10所示, 可记作</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>B</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>0</mn></mrow><mi>Η</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>w</mi><mo>=</mo><mn>0</mn></mrow><mi>W</mi></munderover><mi>Τ</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mo stretchy="false"> (</mo><mi>h</mi><mo>, </mo><mi>w</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>0</mn></mrow><mi>Η</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>w</mi><mo>=</mo><mn>0</mn></mrow><mi>W</mi></munderover><mi>k</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi>h</mi><mo>, </mo><mi>w</mi><mo stretchy="false">) </mo><mo>⋅</mo></mtd></mtr><mtr><mtd><mi>S</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mrow><mo> (</mo><mrow><mi>i</mi><mo>-</mo><mfrac><mi>Η</mi><mn>2</mn></mfrac><mo>+</mo><mi>h</mi><mo>, </mo><mi>j</mi><mo>-</mo><mfrac><mi>W</mi><mn>2</mn></mfrac><mo>+</mo><mi>w</mi></mrow><mo>) </mo></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 基于方向模板提取像素级光条中心" src="Detail/GetImg?filename=images/GXXB201902021_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 基于方向模板提取像素级光条中心  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Extraction of pixel-level light stripe center based on direction template</p>

                </div>
                <div class="p1">
                    <p id="106">式中:<i>B</i><sub><i>i</i>, <i>j</i></sub>为 (<i>i</i>, <i>j</i>) 处沿模板方向的所有像素灰度值之和;<i>T</i><sub><i>i</i>, <i>j</i></sub>为 (<i>i</i>, <i>j</i>) 处沿模板方向的像素灰度值;<i>k</i><sub><i>h</i>, <i>w</i></sub>为大小为<i>H</i>×<i>W</i>的模板中的元素;<i>h</i>=0, 1, 2, …, <i>H</i>-1;<i>w</i>=0, 1, 2, …, <i>W</i>-1;<i>S</i><sub><i>i</i>, <i>j</i></sub>为子区间<i>S</i><sub>2</sub>中以 (<i>i</i>, <i>j</i>) 为中心、尺寸为<i>H</i>×<i>W</i>的局部放大子图;<i>i</i>=0, 1, 2, …, <i>M</i>-1;<i>j</i>=0, 1, 2, …, <i>N</i>-1。设提取过程为按行进行光条中心扫描, 则第<i>i</i>行的中心位置 (<i>L</i><sub><i>i</i></sub>, <i>L</i><sub><i>j</i></sub>) 为</p>
                </div>
                <div class="p1">
                    <p id="107" class="code-formula">
                        <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>i</mi></mtd></mtr><mtr><mtd columnalign="left"><mi>L</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mi>j</mi><mo>=</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mn>1</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mo stretchy="false"> (</mo><mi>B</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>, </mo><mi>B</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>l</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>B</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 基于灰度重心法提取亚像素级光条中心" src="Detail/GetImg?filename=images/GXXB201902021_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 基于灰度重心法提取亚像素级光条中心  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Extraction of sub-pixel level light stripe center based on gray-level centroid method</p>

                </div>
                <div class="p1">
                    <p id="109">如图11所示, 基于方向模板得到初步的光条中心 (<i>L</i><sub><i>i</i></sub>, <i>L</i><sub><i>j</i></sub>) 后, 沿着法线主方向<b><i>n</i></b><sub>2</sub>利用灰度重心法进一步提取光条中心的亚像素坐标, 即将中心位置分别沿着行和列的方向朝两边扩展<i>D</i><sub><i>i</i></sub>和<i>D</i><sub><i>j</i></sub>个像素, 形成两个区间[<i>L</i><sub><i>i</i></sub>-<i>D</i><sub><i>i</i></sub>, <i>L</i><sub><i>i</i></sub>+<i>D</i><sub><i>i</i></sub>]和[<i>L</i><sub><i>j</i></sub>-<i>D</i><sub><i>j</i></sub>, <i>L</i><sub><i>j</i></sub>+<i>D</i><sub><i>j</i></sub>], 这里<i>D</i><sub><i>i</i></sub>/<i>D</i><sub><i>j</i></sub>≈tan<i>θ</i>。在这两个区间范围内按照灰度重心法可求得光条中心的亚像素坐标为</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munderover><mi>Ι</mi></mstyle><mrow><mo> (</mo><mrow><mi>i</mi><mo>, </mo><mi>L</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>) </mo></mrow><mo>⋅</mo><mo stretchy="false"> (</mo><mi>i</mi><mo>-</mo><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munderover><mi>Ι</mi></mstyle><mrow><mo> (</mo><mrow><mi>i</mi><mo>, </mo><mi>L</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>) </mo></mrow></mrow></mfrac></mtd></mtr><mtr><mtd><mi>C</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mi>L</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>L</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mi>L</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>D</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></munderover><mi>Ι</mi></mstyle><mrow><mo> (</mo><mrow><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>j</mi></mrow><mo>) </mo></mrow><mo>⋅</mo><mo stretchy="false"> (</mo><mi>j</mi><mo>-</mo><mi>L</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>D</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>L</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mi>L</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>D</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></munderover><mi>Ι</mi></mstyle><mrow><mo> (</mo><mrow><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>j</mi></mrow><mo>) </mo></mrow></mrow></mfrac></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h3 id="111" name="111" class="anchor-tag">4 实验与讨论</h3>
                <h4 class="anchor-tag" id="112" name="112"><b>4.1</b><b>实验数据与预处理</b></h4>
                <div class="p1">
                    <p id="113">研究的实验数据来自检测车在某重载铁路采集的轨道轮廓激光图像, 实验计算平台配置为Intel Xeon@ 2.40 GHz×28 CPU、NVIDIA Geforce Titan X GPU以及256 GB内存。</p>
                </div>
                <div class="p1">
                    <p id="114">虽然采用了快速的ENet分割模型, 但相对于轨廓实时测量的应用需求, 光条分割的速度仍然有待提高。通过对图像进行旋转、裁剪和缩放等预处理操作, 可以大幅减少图像分析的数据量, 提高分割速度。如图12所示, 通过图像旋转和缩放预处理, 待分割的图像区域大小由800 pixel×600 pixel减小为480 pixel×100 pixel, 参与图像分割的数据量降低了90%。为了兼顾算法的时效性和精确度, 仅在旋转缩放后的图像上执行快速分割, 之后将光条分割的结果反算回原始图像中, 并在原始高分辨率图像中执行光条中心提取算法。</p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 图像旋转与缩放" src="Detail/GetImg?filename=images/GXXB201902021_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 图像旋转与缩放  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Image rotation and scaling</p>

                </div>
                <div class="p1">
                    <p id="116">使用ENet训练的分割模型对图12所示的图像进行分割, 并统计分割耗费的时间, 结果如表1所示。可以看出, 对于经过预处理后的轨廓图像, 单幅图像的平均分割时间约为1.8 ms。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117"><b>4.2</b><b>光条分割结果</b></h4>
                <div class="p1">
                    <p id="118">图13所示为不同方法的图像分割结果对比。</p>
                </div>
                <div class="area_img" id="119">
                    <p class="img_tit">表1 不同大小图像的ENet分割时间对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Segmentation time comparison of images with different sizes by ENet</p>
                    <p class="img_note"></p>
                    <table id="119" border="1"><tr><td><br />Image size /<br /> (pixel×pixel) </td><td>800×600</td><td>960×200</td><td>480×100</td></tr><tr><td><br />Time /ms</td><td>12.3</td><td>6.4</td><td>1.8</td></tr><tr><td><br />Frame rate /<br /> (frame·s<sup>-1</sup>) </td><td>81</td><td>156</td><td>554</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="120">在光条明亮不均、局部过亮或过暗以及受背景噪声干扰的情况下, 传统的固定阈值二值化方法和动态阈值二值化方法都是基于一个灰度阈值对图像进行分割, 仅仅利用灰度作为分割的特征, 无法保证能够得到完整的光条图像。而使用深度学习网络构建的分割模型从大量的轨廓图像中学习光条区域的机器特征, 学习到的特征符合人类的视觉感知机理, 是一个由浅到深、由具体到抽象的多重组合特征, 如图14所示, 可以有效克服不良因素的影响, 得到完整平滑的光条图像。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 光条分割结果对比。 (a) 原始轨廓图像; (b) 固定阈值二值化; (c) 动态阈值二值化; (d) 深度学习 分割结果" src="Detail/GetImg?filename=images/GXXB201902021_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 光条分割结果对比。 (a) 原始轨廓图像; (b) 固定阈值二值化; (c) 动态阈值二值化; (d) 深度学习 分割结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 13 Comparison of light stripe segmentation results. (a) Original rail profile image; (b) fixed threshold binarization; (c) dynamic threshold binarization; (d) deep learning segmentation results</p>

                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图14 光条图像的卷积特征。 (a) 第一层卷积特征, 大小为16&#215;256&#215;256; (b) 第二层卷积特征, 大小为64&#215;128&#215;128; (c) 第三层卷积特征, 大小为128&#215; 164&#215;164" src="Detail/GetImg?filename=images/GXXB201902021_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图14 光条图像的卷积特征。 (a) 第一层卷积特征, 大小为16×256×256; (b) 第二层卷积特征, 大小为64×128×128; (c) 第三层卷积特征, 大小为128× 164×164  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 14 Convolution features of light stripe image. (a) Convolution feature of first layer with size of 16×256×256; (b) convolution feature of second layer with size of 64×128×128; (c) convolution feature of third layer with size of 128×164×164</p>

                </div>
                <h4 class="anchor-tag" id="123" name="123"><b>4.3</b><b>光条中心提取结果</b></h4>
                <div class="p1">
                    <p id="124">图15所示为不同光条中心提取方法的实验结果对比。可以看出, 现有方法对亮度分布极不均匀的轨廓光条图像的分割效果有限, 受环境光和噪声的干扰严重, 不能获取完整的光条图像, 光条中心提取结果不连续。而所提方法的抗干扰能力强, 光条中心提取结果稳定。通过观察局部放大图细节可以看出, 所提方法提取的光条中心更为平滑, 且光条中心的位置更为精确。此外, 方向模板法预先设定了水平、垂直、左偏45°和右偏45°共4个方向模板检测条纹中心, 与钢轨轮廓激光光条的法线方向具有一定的偏差, 且光条上的每一个位置都需要与这4个方向模板进行图像互相关操作, 计算复杂度高。Steger方法局部提取效果较完整平滑, 但其需要对光条上的每一点求取Hessian矩阵以及该矩阵的特征值与特征向量, 运算量大, 难以实现光条中心的快速提取。</p>
                </div>
                <div class="p1">
                    <p id="125">为了验证所提光条中心提取方法的精度和稳定性, 选取100幅钢轨轮廓激光图像对不同光条中心提取方法进行了精度统计对比。首先对光条图像进行人工标注, 为每一行指定一个基准比对位置, 对提取的像素坐标与人工标注的基准坐标求距离均值, 将所得结果作为光条提取的误差, 该误差可用来评价激光光条中心检测的精度。光条中心点提取的距离均值定义为</p>
                </div>
                <div class="p1">
                    <p id="126" class="code-formula">
                        <mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mrow><mo>[</mo><mrow><msqrt><mrow><mrow><mrow><mo> (</mo><mrow><mi>C</mi><msubsup><mrow></mrow><mi>i</mi><mi>n</mi></msubsup><mo>-</mo><mi>G</mi><msubsup><mrow></mrow><mi>i</mi><mi>n</mi></msubsup></mrow><mo>) </mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mrow><mo> (</mo><mrow><mi>C</mi><msubsup><mrow></mrow><mi>j</mi><mi>n</mi></msubsup><mo>-</mo><mi>G</mi><msubsup><mrow></mrow><mi>j</mi><mi>n</mi></msubsup></mrow><mo>) </mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow><mo>]</mo></mrow></mrow></mstyle></mrow><mi>Ν</mi></mfrac><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="127">式中:<i>N</i>为激光条纹的像素行数; (<i>C</i><mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>n</mi></msubsup></mrow></math></mathml>, <i>C</i><mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>n</mi></msubsup></mrow></math></mathml>) 为求得的第<i>n</i>行条纹的中心点坐标; (<i>G</i><mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>n</mi></msubsup></mrow></math></mathml>, <i>G</i><mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>n</mi></msubsup></mrow></math></mathml>) 为人工标注的第<i>n</i>行条纹的中心点坐标。该值越小, 说明光条提取的精度越高。</p>
                </div>
                <div class="area_img" id="132">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图15 不同光条中心提取方法的实验结果及其局部放大。 (a) 灰度重心法[17]; (b) 方向模板法[18]; (c) Steger方法[19]; (d) 所提方法" src="Detail/GetImg?filename=images/GXXB201902021_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图15 不同光条中心提取方法的实验结果及其局部放大。 (a) 灰度重心法<citation id="156" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>; (b) 方向模板法<citation id="157" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>; (c) Steger方法<citation id="158" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>; (d) 所提方法  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 15 Experimental results and corresponding local zoom for different laser stripe center extraction methods. (a) Gray-level centroid method<citation id="159" type="reference"><link href="45" rel="bibliography" /><sup>[17]</sup></citation>; (b) direction-template method <citation id="160" type="reference"><link href="47" rel="bibliography" /><sup>[18]</sup></citation>; (c) Steger method <citation id="161" type="reference"><link href="49" rel="bibliography" /><sup>[19]</sup></citation>; (d) proposed method</p>

                </div>
                <div class="p1">
                    <p id="133">不同光条中心提取方法的精度对比结果如图16所示。</p>
                </div>
                <div class="area_img" id="134">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图16 不同光条中心提取方法的精度对比" src="Detail/GetImg?filename=images/GXXB201902021_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图16 不同光条中心提取方法的精度对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 16 Precision comparison of different laser stripe center extraction methods</p>

                </div>
                <div class="p1">
                    <p id="135">由图16可知, 所提方法在多幅图像上的光条提取结果误差都维持在较小水平 (蓝线) , 误差的平均值为0.082 pixel, 提取结果较为稳定, 误差的标准差为0.047 pixel。</p>
                </div>
                <div class="p1">
                    <p id="136">各算法的执行时间如图17所示, 轨廓图像大小为960 pixel×900 pixel。由于光条图像的多区域预分割缩小了搜索范围, 图像旋转缩放预处理减少了数据处理量, 分区域预设模板避免了遍历匹配等, 所提算法针对单幅轨廓图像的执行时间仅为2.1 ms (其中单幅图像分割耗时约为1.8 ms) , 显著优于现有的方法。</p>
                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902021_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图17 不同光条中心提取方法的执行时间对比" src="Detail/GetImg?filename=images/GXXB201902021_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图17 不同光条中心提取方法的执行时间对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902021_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 17 Execution time comparison of different laser strip center extraction methods</p>

                </div>
                <h3 id="139" name="139" class="anchor-tag">5 结 论</h3>
                <div class="p1">
                    <p id="140">提出了一种适用于室外动态行车环境的稳健、快速和准确的钢轨轮廓激光条纹中心线提取算法。通过构建基于深度学习的钢轨轮廓激光条纹前景分割算法, 有效去除了环境噪声和不均匀的光线反射对光条提取造成的干扰, 增强了算法的稳健性。对分割得到的光条图像应用基于梯度直方图统计的区域划分方法, 每个子区域局部对应一个法线主方向, 避免了全局复杂的模板匹配和耗时的梯度计算, 可提高算法的时效性。对各个分割的子区域构造相应的方向模板以求得法线方向的最大光强点, 以该点为中心, 沿法线方向按照灰度重心法提取光条中心的亚像素坐标, 确保了算法的提取精度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="13">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201801023&amp;v=MjY2NzdCdEdGckNVUjdxZlp1WnRGaURrVjczQklqWFRiTEc0SDluTXJvOUhaNFFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Li T T, Yang F, Li C, <i>et al</i>. Exposure time optimization for line structured light sensor based on light stripe reliability evaluation[J]. Acta Optica Sinica, 2018, 38 (1) : 0112005. 李涛涛, 杨峰, 李策, 等. 基于光条信度评价的线结构光传感器曝光时间优化[J]. 光学学报, 2018, 38 (1) : 0112005.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201111034&amp;v=Mjk4NjZxZlp1WnRGaURrVjczQklqWFRiTEc0SDlETnJvOUdZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Wang S, Xu J Z, Zhang Y X, <i>et al</i>. Reliability evaluation method and application for light-stripe-center extraction[J]. Acta Optica Sinica, 2011, 31 (11) : 1115001. 王顺, 徐静珠, 张益昕, 等. 结构光光条中心点信度评价方法与应用[J]. 光学学报, 2011, 31 (11) : 1115001.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201411017&amp;v=MTQ3MTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVjczQklqWFRiTEc0SDlYTnJvOUVZNFE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Li F J, Li X J, Liu Z. Amulti-scale analysis based method for extracting coordinates of laser light stripe centers[J]. Acta Optica Sinica, 2014, 34 (11) : 1110002. 李凤娇, 李小菁, 刘震. 基于多尺度分析的激光光条中心点坐标提取方法[J]. 光学学报, 2014, 34 (11) : 1110002.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YYGX201003025&amp;v=MjE4NDFEVE1kckc0SDlITXJJOUhZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVjczQlA=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Zhang R Y, Zhou P, Feng X, <i>et al</i>. Rapid extraction of line-structured light stripe in large field of view[J]. Journal of Applied Optics, 2010, 31 (3) : 432-436. 张瑞瑛, 周萍, 冯煦, 等. 大视场下线结构光光条中心的快速提取[J]. 应用光学, 2010, 31 (3) : 432-436.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Weld line detection and process control for welding automation">

                                <b>[5]</b> Yang S M, Cho M H, Lee H Y, <i>et al</i>. Weld line detection and process control for welding automation[J]. Measurement Science and Technology, 2007, 18 (3) : 819-826.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Picture thresholding using an iterative selection method">

                                <b>[6]</b> Ridler T W, Calvard S. Picture thresholding using an iterative selection method[J]. IEEE Transactions on Systems Man &amp; Cybernetics, 1978, 8 (8) : 630-632.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003808699&amp;v=MDIwNzVoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rml2a1c3N01JbGM9Tmo3QmFyTzRIdEhQcDQ5Tll1SUdZM2s1ekJk&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Usamentiaga R, Molleda J, García D F. Fast and robust laser stripe extraction for 3D reconstruction in industrial environments[J]. Machine Vision and Applications, 2012, 23 (1) : 179-196.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201310002&amp;v=MTk4MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEa1Y3M0JMeXJQWkxHNEg5TE5yNDlGWm8=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Li Y Y, Zhang Z Y, Yuan L. Survey on linear structured light stripe center extraction[J]. Laser &amp; Optoelectronics Progress, 2013, 50 (10) : 100002. 李莹莹, 张志毅, 袁林. 线结构光光条中心提取综述[J]. 激光与光电子学进展, 2013, 50 (10) : 100002.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGHW201001022&amp;v=MjEyOTBadEZpRGtWNzNCTHlyRGViRzRIOUhNcm85SFpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Wang W H, Sun J H, Liu Z, <i>et al</i>. Stripe center extrication algorithm for structured-light in rail wear dynamic measurement[J]. Laser &amp; Infrared, 2010, 40 (1) : 87-90. 王伟华, 孙军华, 刘震, 等. 钢轨磨耗动态测量结构光条纹中心提取算法[J]. 激光与红外, 2010, 40 (1) : 87-90.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201103039&amp;v=MTM1MjR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEa1Y3M0JJalhCWTdHNEg5RE1ySTlHYllRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Sun J H, Wang H, Liu Z, <i>et al</i>. Rapid extraction algorithm of laser stripe center in rail wear dynamic measurement[J]. Optics and Precision Engineering, 2011, 19 (3) : 690-696. 孙军华, 王恒, 刘震, 等. 钢轨磨耗动态测量中激光光条中心的快速提取[J]. 光学精密工程, 2011, 19 (3) : 690-696.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011400063131&amp;v=MDY5NzlRVE1ud1plWnRGaW5sVXJ6SktGOFFhQnM9TmlmT2ZiSzdIdEROcTQ5RlpPME1EWDg0b0JNVDZUNFBRSC9pclJkR2VycQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Liu Z, Sun J H, Wang H, <i>et al</i>. Simple and fast rail wear measurement method based on structured light[J]. Optics and Lasers in Engineering, 2011, 49 (11) : 1343-1351.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1012375706.nh&amp;v=MjI4MTJxWkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEa1Y3M0JWRjI2SExDL0c5Yk0=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Xu J Z. Research on methods and evaluation of stripe center extraction in structured light 3D measurement[D]. Nanjing: Nanjing University, 2012. 徐静珠. 结构光三维测量中光条中心提取方法及其评价的研究[D]. 南京: 南京大学, 2012.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Acomparison of algorithms for subpixel peak detection">

                                <b>[13]</b> Fisher R B, Naidu D K. Acomparison of algorithms for subpixel peak detection[M]. Berlin: Springer Press, 1996: 385-404.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Culurciello E. An analysis of deep neural network models for practical applications">

                                <b>[14]</b> Canziani A, Paszke A, Culurciello E. An analysis of deep neural network models for practical applications[EB/OL]. (2017-04-14) [2018-05-17]http://arxiv.org/abs/1605.07678.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TensorFlow:Large-Scale Machine Learning on Heterogeneous Distributed Systems">

                                <b>[15]</b> Abadi M, Agarwal A, Barham P. <i>et al</i>. Tensorfolw: Large-scale machine learning on heterogeneous distributed systems[EB/OL]. (2016-03-16) [2018-05-22]http://arxiv.org/abs/1603.04467v1.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">

                                <b>[16]</b> Kingma D P, Ba J. Adam: A method for stochastic optimization[EB/OL]. (2017-01-30) [2018-06-03]http://cn.arxiv.org/abs/1412.6980.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XAJT200701017&amp;v=MjA5NDRlckc0SHRiTXJvOUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVjczQlBTekI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> Cui X J, Yang C, Liu B H, <i>et al</i>. Self-adaptive iterative method of extracting center of linear-structured light stripe[J]. Journal of Xi′an Jiaotong University, 2007, 41 (1) : 73-76. 崔希君, 杨川, 刘保华, 等. 线性结构光心的自适应迭代提取法[J]. 西安交通大学学报, 2007, 41 (1) : 73-76.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SCLH200704029&amp;v=MzI3NDRSN3FmWnVadEZpRGtWNzNCTmk3SFpyRzRIdGJNcTQ5SGJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Wu Q Y, Su X Y, Li J Z, <i>et al</i>. A new method for extracting the centre-line of line structure light-stripe[J]. Journal of Sichuan University (Engineering Science Edition) , 2007, 39 (4) : 151-155. 吴庆阳, 苏显渝, 李景镇, 等. 一种新的线结构光光带中心提取算法[J]. 四川大学学报 (工程科学版) , 2007, 39 (4) : 151-155.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unbiased detector of curvilinear structures">

                                <b>[19]</b> Steger C. An unbiased detector of curvilinear structures[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1998, 20 (2) : 113-125.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201902021" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902021&amp;v=MTQ1NjBGckNVUjdxZlp1WnRGaURrVjczT0lqWFRiTEc0SDlqTXJZOUhaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

