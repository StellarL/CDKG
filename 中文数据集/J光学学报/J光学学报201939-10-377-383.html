

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133122534815000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dGXXB201910046%26RESULT%3d1%26SIGN%3d5P3V9%252bIe%252f9OrxomMcOr4myujsPQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201910046&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201910046&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201910046&amp;v=MDc4NjBJalhUYkxHNEg5ak5yNDlCWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2RnkzbFZiM0s=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#43" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#47" data-title="2 基本原理 ">2 基本原理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="&lt;b&gt;2.1 双目视差&lt;/b&gt;"><b>2.1 双目视差</b></a></li>
                                                <li><a href="#55" data-title="&lt;b&gt;2.2 运动像差&lt;/b&gt;"><b>2.2 运动像差</b></a></li>
                                                <li><a href="#59" data-title="&lt;b&gt;2.3 深度感知模型&lt;/b&gt;"><b>2.3 深度感知模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="3 实验设计 ">3 实验设计</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#69" data-title="&lt;b&gt;3.1 实验装置&lt;/b&gt;"><b>3.1 实验装置</b></a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;3.2 实验刺激和被试者&lt;/b&gt;"><b>3.2 实验刺激和被试者</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;3.3 实验1:远距离观察实验&lt;/b&gt;"><b>3.3 实验1:远距离观察实验</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;3.4 实验2:近距离观察实验&lt;/b&gt;"><b>3.4 实验2:近距离观察实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="4 分析与讨论 ">4 分析与讨论</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="&lt;b&gt;4.1 数据分析&lt;/b&gt;"><b>4.1 数据分析</b></a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;4.2 讨 论&lt;/b&gt;"><b>4.2 讨 论</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#102" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#52" data-title="图1 双目视差示意图">图1 双目视差示意图</a></li>
                                                <li><a href="#54" data-title="图2 双目视差产生的原理示意图">图2 双目视差产生的原理示意图</a></li>
                                                <li><a href="#58" data-title="图3 运动像差数学表示示意图。">图3 运动像差数学表示示意图。</a></li>
                                                <li><a href="#75" data-title="图4 实验刺激物。">图4 实验刺激物。</a></li>
                                                <li><a href="#81" data-title="图5 远距离观察实验过程示意图。">图5 远距离观察实验过程示意图。</a></li>
                                                <li><a href="#91" data-title="图6 近距离观察实验过程示意图。">图6 近距离观察实验过程示意图。</a></li>
                                                <li><a href="#95" data-title="图7 被试者移动后深度感知的PSE分布。">图7 被试者移动后深度感知的PSE分布。</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Howard I P,Rogers B J.Perceiving in depth,volume 2:stereoscopic vision[M].New York:Oxford University Press,2012:385-432." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceiving in depth volume 2:stereoscopic vision">
                                        <b>[1]</b>
                                         Howard I P,Rogers B J.Perceiving in depth,volume 2:stereoscopic vision[M].New York:Oxford University Press,2012:385-432.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Howard I P,Rogers B J.Perceiving in depth,volume 3:other mechanisms of depth perception[M].New York:Oxford University Press,2012:63-83." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceiving in depth volume 3:other mechanisms of depth perception">
                                        <b>[2]</b>
                                         Howard I P,Rogers B J.Perceiving in depth,volume 3:other mechanisms of depth perception[M].New York:Oxford University Press,2012:63-83.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" Snowden R,Thompson P,Troscianko T.Basic vision:an introduction to visual perception[M].New York:Oxford University Press,2012:220-222." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Basic vision:an introduction to visual perception">
                                        <b>[3]</b>
                                         Snowden R,Thompson P,Troscianko T.Basic vision:an introduction to visual perception[M].New York:Oxford University Press,2012:220-222.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Howard I P,Rogers B J.Perceiving in depth,volume 1:basic mechanisms[M].New York:Oxford University Press,2012:435-474." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceiving in depth volume 1:basic mechanisms">
                                        <b>[4]</b>
                                         Howard I P,Rogers B J.Perceiving in depth,volume 1:basic mechanisms[M].New York:Oxford University Press,2012:435-474.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" Howard I P,Rogers B J.Binocular vision and stereopsis[M].New York:Oxford University Press,1995:108-121." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Binocular vision and stereopsis">
                                        <b>[5]</b>
                                         Howard I P,Rogers B J.Binocular vision and stereopsis[M].New York:Oxford University Press,1995:108-121.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Shigemasu H,Okubo K,Yan P.Shape constancy from binocular disparity with self-motion in depth[J].Perception ECVP,2013,42:117-117." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape constancy from binocular disparity with self-motion in depth">
                                        <b>[6]</b>
                                         Shigemasu H,Okubo K,Yan P.Shape constancy from binocular disparity with self-motion in depth[J].Perception ECVP,2013,42:117-117.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" Scarfe P,Hibbard P B.Disparity-defined objects moving in depth do not elicit three-dimensional shape constancy[J].Vision Research,2006,46(10):1599-1610." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600404224&amp;v=MDM2NjdTYUJBPU5pZk9mYks3SHRETnFZOUZZT3NMRG40OW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWI3SUpWNA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Scarfe P,Hibbard P B.Disparity-defined objects moving in depth do not elicit three-dimensional shape constancy[J].Vision Research,2006,46(10):1599-1610.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" Braunstein M L.Sensitivity of the observer to transformations of the visual field[J].Journal of Experimental Psychology,1966,72(5):683-689." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sensitivity of the observer to transformations of the visual field">
                                        <b>[8]</b>
                                         Braunstein M L.Sensitivity of the observer to transformations of the visual field[J].Journal of Experimental Psychology,1966,72(5):683-689.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" Rogers B,Graham M.Motion parallax as an independent cue for depth perception[J].Perception,1979,8(2):125-134." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJIX&amp;filename=SJIXEA57DD539E9E8D867877DA53737822FC&amp;v=MzI0MzROdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxod2JpNXdxbz1OaWZDZHNiSkc5YTQyNHBHYlo0R2VYUk54eEFVNGpoNlBBN25yeFUyZnJxV1I4enNDTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Rogers B,Graham M.Motion parallax as an independent cue for depth perception[J].Perception,1979,8(2):125-134.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" Landy M S,Maloney L T,Johnston E B,&lt;i&gt;et al&lt;/i&gt;.Measurement and modeling of depth cue combination:in defense of weak fusion[J].Vision Research,1995,35(3):389-412." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600408361&amp;v=MjkyNjBEM280b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYjdJSlY0U2FCQT1OaWZPZmJLN0h0RE5xWTlGWU9zSA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Landy M S,Maloney L T,Johnston E B,&lt;i&gt;et al&lt;/i&gt;.Measurement and modeling of depth cue combination:in defense of weak fusion[J].Vision Research,1995,35(3):389-412.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" Rogers B,Graham M.Similarities between motion parallax and stereopsis in human depth perception[J].Vision Research,1982,22(2):261-270." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Similarities between motion parallax and stereopsis in human depth perception">
                                        <b>[11]</b>
                                         Rogers B,Graham M.Similarities between motion parallax and stereopsis in human depth perception[J].Vision Research,1982,22(2):261-270.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" Tittle J S,Norman J F,Perotti V J,&lt;i&gt;et al&lt;/i&gt;.The perception of scale-dependent and scale-independent surface structure from binocular disparity,texture,and shading[J].Perception,1998,27(2):147-166." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJIX&amp;filename=SJIX80D9A7AA1C7C6C45C9E6A3D6EDB0859C&amp;v=MDA1MjRhQnVIWWZPR1FsZkNwYlEzNWRsaHdiaTV3cW89TmlmQ2RydTRhdGk5cVA0MFpaZ0lmM3BLeXhOZzQwcDdPWHlXcW1kQkM3S2NRTFBzQ09OdkZTaVdXcjdKSUZwbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Tittle J S,Norman J F,Perotti V J,&lt;i&gt;et al&lt;/i&gt;.The perception of scale-dependent and scale-independent surface structure from binocular disparity,texture,and shading[J].Perception,1998,27(2):147-166.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" Nawrot M,Blake R.The interplay between stereopsis and structure from motion[J].Perception &amp;amp; Psychophysics,1991,49(3):230-244." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The interplay between stereopsis and structure from motion">
                                        <b>[13]</b>
                                         Nawrot M,Blake R.The interplay between stereopsis and structure from motion[J].Perception &amp;amp; Psychophysics,1991,49(3):230-244.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" Young M J,Landy M S,Maloney L T.A perturbation analysis of depth perception from combinations of texture and motion cues[J].Vision Research,1993,33(18):2685-2696." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A perturbation analysis of depth perception from combinations of texture and motion cues">
                                        <b>[14]</b>
                                         Young M J,Landy M S,Maloney L T.A perturbation analysis of depth perception from combinations of texture and motion cues[J].Vision Research,1993,33(18):2685-2696.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" Julesz B.Foundations of cyclopean perception[M].Chicago:University of Chicago Press,1971:142-148." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Foundations of cyclopean perception">
                                        <b>[15]</b>
                                         Julesz B.Foundations of cyclopean perception[M].Chicago:University of Chicago Press,1971:142-148.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" Li Z P.Understanding vision:theory,models,and data[M].New York:Oxford University Press,2014:16-66." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding vision:theory models and data">
                                        <b>[16]</b>
                                         Li Z P.Understanding vision:theory,models,and data[M].New York:Oxford University Press,2014:16-66.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-07-01 09:45</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(10),377-383 DOI:10.3788/AOS201939.1033002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>三维空间中不同线索对物体深度感知的影响</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B4%BA%E4%B9%A6%E8%8A%B3&amp;code=43191128&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贺书芳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%B9%81%E6%A1%9D%E5%8D%9A%E6%98%AD&amp;code=43191130&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">繁桝博昭</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%9F%B3%E5%B7%9D%E7%A5%90%E8%A8%98%E5%AD%90&amp;code=43191131&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">石川祐記子</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%A3%E5%BD%A9%E7%BA%A2&amp;code=21875084&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">代彩红</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E8%AE%A1%E9%87%8F%E7%A7%91%E5%AD%A6%E7%A0%94%E7%A9%B6%E9%99%A2%E5%85%89%E5%AD%A6%E4%B8%8E%E6%BF%80%E5%85%89%E8%AE%A1%E9%87%8F%E7%A7%91%E5%AD%A6%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=1039014&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国计量科学研究院光学与激光计量科学研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%AB%98%E7%9F%A5%E5%B7%A5%E7%A7%91%E5%A4%A7%E5%AD%A6%E7%A0%94%E7%A9%B6%E7%94%9F%E9%99%A2&amp;code=0276027&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高知工科大学研究生院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%AB%98%E7%9F%A5%E5%B7%A5%E7%A7%91%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高知工科大学信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>通过在虚拟三维空间中构建基于双目视差和运动像差的不同深度的柱形面,使被试者在三维空间中行走运动,采用心理物理学的固定值刺激法,获取被试者在不同观察距离下基于双目视差线索、运动像差线索,以及两种线索同时作用时的深度感知数据,研究双目视差线索和运动像差线索对人眼深度感知的影响。结果表明:在不同的观察距离下,双目视差线索和运动像差线索对人眼的深度感知有不同的影响;在远距离观察条件下,运动像差线索对深度感知的权重比在近距离观察条件下大;而在近距离观察条件下,两种线索同时作用时对深度感知没有促进作用,并且运动像差线索对深度感知的作用较小。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E5%85%89%E5%AD%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉光学;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E7%9B%AE%E8%A7%86%E5%B7%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双目视差;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%90%E5%8A%A8%E5%83%8F%E5%B7%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">运动像差;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E6%84%9F%E7%9F%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度感知;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E7%BB%B4%E7%A9%BA%E9%97%B4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三维空间;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BF%83%E7%90%86%E7%89%A9%E7%90%86%E5%AD%A6%E5%AE%9E%E9%AA%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">心理物理学实验;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *贺书芳,E-mail:hesf@nim.ac.cn;shufang.he@outlook.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-22</p>

            </div>
                    <h1><b>Depth Perception with Different Cues in Three-Dimensional Space</b></h1>
                    <h2>
                    <span>He Shufang</span>
                    <span>Shigemasu Hiroaki</span>
                    <span>Ishikawa Yukiko</span>
                    <span>Dai Caihong</span>
            </h2>
                    <h2>
                    <span>Division of Optics, National Institute of Metrology</span>
                    <span>Graduate School, Kochi University of Technology</span>
                    <span>School of Information, Kochi University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>By using the method of constant stimuli, binocular-disparity based and motion-parallax based cylinders with various depths were produced in three-dimensional space as experimental stimuli. Observers were asked to walk in the three-dimensional space, and depth perception data at different viewing distances were obtained by using binocular-disparity cue, motion-parallax cue, and both binocular-disparity and motion-parallax cues separately. The experimental results show: at different viewing distances, binocular-disparity and motion-parallax cues have different influences on depth perception. In the far-distance viewing condition, the weight of motion parallax cue in depth perception is relatively larger than that in the near-distance viewing condition; in the near-distance viewing condition, when binocular-disparity and motion-parallax cues are combined, there is no promotive effect in depth perception and the effect of motion parallax cue is very small.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visual%20optics&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visual optics;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=binocular%20disparity&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">binocular disparity;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=motion%20parallax&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">motion parallax;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=depth%20perception&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">depth perception;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=three-dimensional%20space&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">three-dimensional space;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=psychophysical%20experiment&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">psychophysical experiment;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-04-22</p>
                            </div>


        <!--brief start-->
                        <h3 id="43" name="43" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="44">随着三维(3D)显示技术的发展,头戴式虚拟现实(VR)和增强现实(AR)产品、3D影院、立体显示器已经被广泛应用于游戏、医疗、灾害监控、电影娱乐等各个领域。为了增强体验感,用户在虚拟3D空间中行走和互动的需求也与日俱增。如何能让用户在虚拟的三维空间中获得真实准确的深度感知,是3D显示技术发展的关键。</p>
                </div>
                <div class="p1">
                    <p id="45">在三维空间中,人眼一般通过双目视差或者运动像差线索来感知物体的深度信息<citation id="105" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。双目视差是指人的左右两眼在水平方向上分开一定的距离(6 cm左右)后,会导致立体空间中不同深度的点在左右眼视网膜上的成像位置不同,视觉信息处理的神经中枢——大脑根据左、右眼视网膜上提取到的物体成像位置差异来判断物体的三维空间位置,从而产生深度感知<citation id="104" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。运动像差是指当人体或者被观察的物体在3D空间中运动时,人眼视网膜上被观察物体的成像位置发生改变,从而产生深度感知。</p>
                </div>
                <div class="p1">
                    <p id="46">物体在3D空间移动时,人眼的双目视差和运动像差通常会发生改变<citation id="106" type="reference"><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。前人已经研究过观察者或者被观察物体在深度方向的移动对人眼深度感知的影响,发现不管是在远距离还是近距离观察条件下,人眼对深度的感知都会比实际深度值大<citation id="107" type="reference"><link href="21" rel="bibliography" /><link href="23" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>。但是,在双目视差和运动像差相结合的情况下,在不同的观察距离下,人眼如何感知3D深度在笔者检索范围内还未见报道。为此,本文基于被试者在三维空间中的运动,通过实验分析单个深度感知线索以及两个深度线索共同作用时的深度感知情况。同时,构建了较为复杂、也更能真实地反映三维空间的实际应用场景的视觉研究实验,以期为三维空间游戏场景构建、虚拟现实和增强现实眼镜设计、3D电影制作等提供理论支持和数据支撑。</p>
                </div>
                <h3 id="47" name="47" class="anchor-tag">2 基本原理</h3>
                <h4 class="anchor-tag" id="48" name="48"><b>2.1 双目视差</b></h4>
                <div class="p1">
                    <p id="49">当观看立体空间不同深度的点<i>F</i>和<i>Q</i>时(图1),<i>f</i><sub>l</sub>和<i>q</i><sub>l</sub>是在左眼视网膜上的成像,<i>f</i><sub>r</sub>和<i>q</i><sub>r</sub>是在右眼视网膜上的成像,则双目视差可表示为<citation id="108" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation></p>
                </div>
                <div class="p1">
                    <p id="50" class="code-formula">
                        <mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Δ</mtext><mi>θ</mi><mo>=</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi>θ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="51">式中:<i>θ</i><sub>1</sub>表示左眼对<i>F</i>点和<i>Q</i>点在视网膜上成像的夹角;<i>θ</i><sub>2</sub>表示右眼对<i>F</i>点和<i>Q</i>点在视网膜上成像的夹角。</p>
                </div>
                <div class="area_img" id="52">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910046_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 双目视差示意图" src="Detail/GetImg?filename=images/GXXB201910046_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 双目视差示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910046_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Schematic of binocular disparity</p>

                </div>
                <div class="p1">
                    <p id="53">在机器视觉或者立体显示技术中构建基于双目视差的三维信息时,可以将左、右眼的图像错开一定的位置,经过双目视觉融合产生3D效果。如图2所示,当双眼同时注视<i>P</i>点时,左右眼的图像位置满足<i>L</i><sub>0</sub>=<i>R</i><sub>0</sub>,此时物体的深度为<i>d</i>;当要产生更远的深度感知时,可以将左右眼图像位置分别移动至<i>L</i><sub>1</sub>和<i>R</i><sub>1</sub>,大脑会将左右眼图像融合并产生<i>d</i><sub>1</sub>的深度感知;当要产生较近的深度感知时,可以将左右眼图像分别移动至<i>L</i><sub>2</sub>和<i>R</i><sub>2</sub>,大脑会将左右眼图像融合并产生<i>d</i><sub>2</sub>的深度感知。通过编程计算可以精确地控制<i>L</i><sub>1</sub>和<i>R</i><sub>1</sub>的位置,以生成不同深度、不同形状的3D物体。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910046_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 双目视差产生的原理示意图" src="Detail/GetImg?filename=images/GXXB201910046_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 双目视差产生的原理示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910046_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Schematic of binocular disparity generation</p>

                </div>
                <h4 class="anchor-tag" id="55" name="55"><b>2.2 运动像差</b></h4>
                <div class="p1">
                    <p id="56">运动像差通过物体或者被试者运动来产生深度感知,其原理是:不管是物体运动还是被试者运动,都会造成被观察物体在视网膜上成像位置的变化。人脑根据视网膜上成像位置变化的快慢来感知物体的深度。通常认为三维空间深度(距离)较近的物体在视网膜上的成像位置变化较快;相反,三维空间深度(距离)较远的物体在视网膜上的成像位置变化较慢。比如,当人们坐在行驶的火车中观察窗外的景色时,会感觉到近处的站台运动得较快,而远处的山脉运动得较慢,这就是运动像差的一个典型例子。</p>
                </div>
                <div class="p1">
                    <p id="57">运动像差通常有两类情况:1)被观察物体不动,观察者从<i>H</i><sub>1</sub>移动到<i>H</i><sub>2</sub>的位置[图3(a)];2)观察者从<i>H</i><sub>1</sub>移动到<i>H</i><sub>2</sub>的位置,同时被观察物体从<i>A</i>点移动到<i>B</i>点[图3(b)]。与双目视差不同的是,运动像差是一种单眼线索<citation id="109" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。之前的研究者们分别通过使被观察物体运动、被试者不动,或者被观察物体不动、被试者运动来产生运动像差,并且通过控制视网膜成像位置变化的速度来改变运动像差<citation id="110" type="reference"><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910046_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 运动像差数学表示示意图。" src="Detail/GetImg?filename=images/GXXB201910046_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 运动像差数学表示示意图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910046_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Mathematic schematics of motion parallax. </p>
                                <p class="img_note">(a)被观察物体保持不动，观察者从H<sub>1</sub>移动到H<sub>2</sub>;(b)观察者从H<sub>1</sub>移动到H<sub>2</sub>，被观察物体从A点移动到B点</p>
                                <p class="img_note">(a)Object keeps static,and observer moves from H<sub>1</sub>to H<sub>2</sub>;(b)observer moves from H<sub>1</sub>to H<sub>2</sub>,and object moves from Ato B</p>

                </div>
                <h4 class="anchor-tag" id="59" name="59"><b>2.3 深度感知模型</b></h4>
                <div class="p1">
                    <p id="60">在实际的3D环境中,人眼深度感知可能源于多种不同的线索,比如双目视差、运动像差、纹理、阴影等,人眼在不同线索的共同作用下感知深度信息<citation id="111" type="reference"><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。关于不同的线索共同作用时如何影响人眼的深度感知,之前的研究者提出了弱融合和强融合两种可能的模式<citation id="112" type="reference"><link href="29" rel="bibliography" /><link href="37" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">14</a>]</sup></citation>。弱融合是指各线索之间对深度感知的影响基本是相互独立的;而强融合是指各线索之间互相有联系,可对深度感知产生交互作用。其中,比较有影响力的是改进的弱融合模型,即不同线索对深度感知的作用是接近于相互独立的,因此各线索对深度感知的影响符合线性模型<citation id="113" type="reference"><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mn>1</mn><mi>n</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mn>1</mn><mi>n</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">式中:<i>D</i>表示总体深度;<i>i</i>表示不同的线索;<i>w</i><sub><i>i</i></sub>表示每个线索的权重;<i>D</i><sub><i>i</i></sub>表示不同线索感知到的深度值。</p>
                </div>
                <div class="p1">
                    <p id="63">在改进的弱融合模型的基础上,Young等<citation id="114" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>通过扰动分析法分析了纹理线索和运动像差线索对深度感知的影响。实验结果表明:纹理线索和运动像差线索对深度的感知存在着线性加权的关系;而且,当其中一个线索受到噪声影响时,另一个未受到影响的线索的权重会变大<citation id="115" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="64">从生理物理学的角度来讲,不同的深度感知线索会在大脑的不同视觉区域产生响应。比如,双目视差信号主要在大脑的初级视皮层V1区域产生响应,运动线索主要在VT/MT区域产生响应,而物体形状认知主要发生在V4区域<citation id="116" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。不同的大脑区域之间可能会有一定程度的交互作用,但是相比于每个线索的特定响应区域来讲,该交互作用较为微弱。</p>
                </div>
                <div class="p1">
                    <p id="65">为了研究双目视差和运动像差的交互作用对人眼深度感知的影响,参照前人提出的线性加权模型,拟建立基于双目视差和运动像差两因素的线性模型,即</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mo>=</mo><mi>w</mi><msub><mrow></mrow><mtext>B</mtext></msub><mi>S</mi><msub><mrow></mrow><mtext>B</mtext></msub><mo>+</mo><mi>w</mi><msub><mrow></mrow><mtext>Μ</mtext></msub><mi>S</mi><msub><mrow></mrow><mtext>Μ</mtext></msub><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">式中:<i>S</i><sub>B</sub>和<i>S</i><sub>M</sub>分别表示双眼视差和运动像差;<i>w</i><sub>B</sub>和<i>w</i><sub>M</sub>分别表示双眼视差和运动像差的权重,<i>w</i><sub>B</sub>+<i>w</i><sub>M</sub>=1。</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag">3 实验设计</h3>
                <h4 class="anchor-tag" id="69" name="69"><b>3.1 实验装置</b></h4>
                <div class="p1">
                    <p id="70">实验在暗室中进行。采用MATLAB和Psychotoolbox编程生成刺激物。在远距离观察条件下,使用3D DLP投影仪(DepthQ-WXGA,InFocus公司,美国,分辨率1024 pixel×768 pixel,刷新频率120 Hz)和投影屏幕(ES-120W,KIC公司,日本)呈现刺激物;在近距离观察条件下,使用CRT显示器(FlexScan T961,EIZO公司,日本,分辨率1024 pixel×768 pixel,刷新频率120 Hz)呈现刺激物。被试者通过液晶开关眼镜(60GX,NuVision公司,美国)观看3D图像,对观察到的深度信息进行判断,并通过无线键盘(SRTK01,BUFFALO公司,日本)特定的按键作出选择。实验过程中,通过被试者随身佩戴的磁石式3D位置测量装置(FASTRAK,POLHEMUS公司,美国)实时测量被试者的实际位置。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71"><b>3.2 实验刺激和被试者</b></h4>
                <div class="p1">
                    <p id="72">实验的刺激物为分布在黑色背景上的白色随机点。双目视差的3D刺激物呈现方式为:通过视差计算,将左右眼的图像错开一定的位置,以生成特定深度的3D柱形面。运动像差的3D刺激物呈现方式为:通过像差计算,要求被试者在规定的时间内完成指定距离的往复移动。采用随机点作为刺激物可以有效排除其他线索对深度感知的影响<citation id="117" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="73">在远距离观察条件下,半圆柱面的尺寸为18 cm(纵向)×36 cm(横向),深度为9 cm[图4(a)]。实验过程中,被试者从270 cm的观察距离运动至180 cm距离处。在双目视差情况下,人眼感知4.5,6.0,7.5,9.0,10.5,12.0,13.5 cm深度的柱形曲面;在运动像差情况下,人体在水平方向移动的距离为45 cm。</p>
                </div>
                <div class="p1">
                    <p id="74">在近距离实验条件下,半圆柱面的尺寸为6 cm(纵向)×12 cm(横向),深度为3 cm[图4(b)]。实验过程中,被试者从90 cm的观察距离运动至60 cm距离处。在双目视差情况下,人眼感知1.5,2.0,2.5,3.0,3.5,4.0,4.5 cm深度的柱形曲面;在运动像差情况下,人体在水平方向移动的距离为15 cm。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910046_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 实验刺激物。" src="Detail/GetImg?filename=images/GXXB201910046_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 实验刺激物。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910046_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Stimuli in experiments. </p>
                                <p class="img_note">(a)远距离观察;
(b)近距离观察</p>
                                <p class="img_note">(a) Far-distance viewing;
 (b) near-distance viewing</p>

                </div>
                <div class="p1">
                    <p id="76">实验由8名(4名男性,4名女性,平均年龄22.5岁)被试者完成。所有被试者均具有正常视力或者矫正至正常视力。在实验前,所有被试者都需通过定制的立体视觉测试软件的测试,以证明其具有正常的立体视觉,并且立体视觉的敏锐度可达到1 arcmin,不符合要求者将不能参加实验。所有被试者均充分理解实验说明,并签署同意实验的说明书。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>3.3 实验1:远距离观察实验</b></h4>
                <div class="p1">
                    <p id="78">在远距离观察实验中,被试者从270 cm的观察距离移动到180 cm的观察距离,判断在哪种观察距离下看到的柱形面的深度较大,并根据实验指定按键进行选择。实验分为双目视差实验、运动像差实验、双目视差与运动像差同时提示三种情况。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79">3.3.1 双目视差</h4>
                <div class="p1">
                    <p id="80">实验过程如图5(a)所示,在实验开始前,被试者手持无线键盘站在270 cm位置。当被试者准备好后,按键盘指定的开始键(如数字键“5”)触发刺激,具有一定深度的3D柱形面将会显示在屏幕上。被试者观察柱形面的深度后,开始向屏幕方向移动至180 cm的距离位置(到达指定位置后,被试者会收到声音提示信息)。站定之后,被试者再次按下开始键触发刺激,具有一定深度的3D柱形面将会显示在屏幕上。被试者的任务是,判断哪个观察距离下柱形面的深度较大,并通过键盘上不同的按键进行选择(比如:如果被试者觉得第一次出现的柱形面深度大,就选择键盘按键“4”;如果觉得第二次出现的柱形面的深度大,则选择键盘按键“6”)。</p>
                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910046_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 远距离观察实验过程示意图。" src="Detail/GetImg?filename=images/GXXB201910046_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 远距离观察实验过程示意图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910046_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Schematics of experimental processes under far-distance viewing condition. </p>
                                <p class="img_note">(a)双目视差;(b)运动像差;(c)双目视差和运动像差</p>
                                <p class="img_note">(a) Binocular parallax; 
(b) motion parallax; (c) binocular parallax and motion parallax</p>

                </div>
                <div class="p1">
                    <p id="82">实验采用固定刺激值法,前后两次出现的柱形面深度,其中之一为固定值9.0 cm,而另一次出现的深度值为4.5,6.0,7.5,9.0,10.5,12.0,13.5 cm中的一个。固定值出现的次序(运动前还是运动后)以及7种深度值都是随机出现的,被试者无法预测下一次将会出现哪种深度值的柱形面。每名被试者在每种深度值情况下要进行20次实验,总计20×7=140次。实验分两组在不同日期完成。</p>
                </div>
                <div class="p1">
                    <p id="83">在被试者移动过程中,磁石式3D位置测量装置会实时获取被试者的位置并将坐标位置显示在显示屏上,被试者可以根据当前位置及时调整运动情况。同时,为了准确控制被试者的移动位置,在实验场地悬挂导引带,以确保在暗室情况下被试者仍可以按照规定的路线和距离进行移动。每组实验前,被试者都需要按照实验要求进行练习,直到可以熟练地掌握行进路线和实验节奏,方可正式开始实验。</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84">3.3.2 运动像差</h4>
                <div class="p1">
                    <p id="85">实验中,为了观察运动像差并排除双目视差的影响,通过编程控制立体眼镜中刺激物的呈现,使得被试者正常睁开双眼并通过立体眼镜观看时,只有一只眼睛能观察到刺激物。实验过程与双目视差的不同之处在于,被试者需要通过自身位置移动来感知深度信息。具体过程如图5(b)所示:当被试者站在270 cm位置并准备好后,按键盘指定的开始键(如数字键“5”)触发刺激,被试者需要在规定的时间内在45 cm范围内左右移动一个来回,由此运动视差产生3D深度感知。被试者观察了柱形面的深度后,开始向屏幕方向移动至180 cm的观察位置(到达指定位置后,被试者会收到提示信息)。站定之后,被试者再次按下开始键触发刺激,并在同样的时间内在45 cm范围内左右移动一个来回以感知3D深度信息。</p>
                </div>
                <div class="p1">
                    <p id="86">被试者的任务以及实验方法与3.3.1节双目视差实验相同。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87">3.3.3 双目视差与运动像差同时提示</h4>
                <div class="p1">
                    <p id="88">在实际的3D环境中,通常是既有双目视差又有运动像差。本实验旨在验证在两种深度线索同时存在的情况下,人眼如何感知3D深度信息。实验过程如图5(c)所示,与3.3.2节运动像差实验过程的不同之处在于,被试者的双眼能同时看到刺激物(而不是一只眼睛);与此同时,被试者还需要完成与3.3.2节运动像差实验中相同的身体运动,以获取最终的3D深度感知信息。被试者的任务以及实验方法与前述实验相同。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89"><b>3.4 实验2:近距离观察实验</b></h4>
                <div class="p1">
                    <p id="90">近距离观察实验中,被试者从90 cm的观察距离移动到60 cm,判断在哪种观察距离下看到的柱形面的深度较大,并根据实验指定的按键作出选择。实验分为双目视差、运动像差、双目视差与运动像差同时提示三种情况[图6(a)～(c)]。实验过程分别与3.3.1～3.3.3节相同,在此省略。与远距离观察实验不同的是,在双目视差实验中,人眼感知的柱形面的深度为1.5,2.0,2.5,3.0,3.5,4.0,4.5 cm;在运动像差实验中,被试者左右移动的距离是15 cm。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910046_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 近距离观察实验过程示意图。" src="Detail/GetImg?filename=images/GXXB201910046_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 近距离观察实验过程示意图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910046_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Schematics of experimental processes under near-distance viewing condition. </p>
                                <p class="img_note">(a)双目视差;(b)运动像差;(c)双目视差和运动像差</p>
                                <p class="img_note">(a) Binocular parallax; 
(b) motion parallax; (c) binocular parallax and motion parallax</p>

                </div>
                <h3 id="92" name="92" class="anchor-tag">4 分析与讨论</h3>
                <h4 class="anchor-tag" id="93" name="93"><b>4.1 数据分析</b></h4>
                <div class="p1">
                    <p id="94">通过分析主观等价点(PSE)来判断被试者在运动前后深度感知的变化。PSE是指与指定值(如实验1的中间值9.0 cm或实验2的中间值3.0 cm)有相同主观感知的数值点。如果PSE比指定值大,说明主观感知偏小,实际需要更大的值才能获得与指定值一致的主观感知;反之,如果PSE比指定值小,说明主观感知偏大。根据实验结果分别获取远距离和近距离观察条件下的实际深度感知数据,并以9.0 cm或者3.0 cm作为基准对深度感知数据进行归一化,提取每种情况对应的PSE。实验结果如图7所示,其中,图7(a)、(b)所示分别为远距离和近距离观察条件下8名被试者移动后深度感知的PSE分布。之后,对实验数据进行方差分析(ANOVA)。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910046_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 被试者移动后深度感知的PSE分布。" src="Detail/GetImg?filename=images/GXXB201910046_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 被试者移动后深度感知的PSE分布。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910046_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 PSEs of depth perception before and after subjective motions.</p>
                                <p class="img_note">(a)远距离观察;(b)近距离观察(p&lt;0.05)</p>
                                <p class="img_note">(a) Far-distance viewing; 
(b) near-distance viewing (p&lt;0.05)</p>

                </div>
                <div class="p1">
                    <p id="96">实验结果表明,在远距离观察条件下,运动像差条件下的PSE值为0.94,双目视差条件下的PSE值为0.97,运动像差和双目视差同时提示条件下的PSE值为0.96。在三种条件下均存在PSE&lt;1的情况,表明在远距离观察条件下存在深度感知过大判断的特点。ANOVA分析结果显示,不同线索(运动像差、双目视差、双目视差和运动相差同时提示三种情况)的PSE之间不存在显著性差异(<i>p</i>=0.87)。对运动像差和双目视差的深度感知进行线性加权计算,可以得到在远距离观察条件下,权重<i>w</i><sub>B</sub>=0.33,<i>w</i><sub>B</sub>=0.67,即<i>S</i><sub>far</sub>=0.67<i>S</i><sub>B</sub>+0.33<i>S</i><sub>M</sub>。</p>
                </div>
                <div class="p1">
                    <p id="97">在近距离观察条件下,运动像差条件下的PSE值为0.92,双目视差条件下的PSE值为1.04,运动像差和双目视差同时提示条件下的PSE值为1.06。在运动视差条件下PSE&lt;1,表明在近距离运动时,存在深度感知过大判断的特点;而在双目视差条件,以及运动像差和双目视差同时提示的条件下,PSE&gt;1,说明存在深度感知过小判断的特点,而且运动像差和双目视差同时提示时会进一步降低深度感知值。此外,运动像差和双目视差之间不存在线性加权关系。ANOVA分析结果显示,不同线索(运动像差、双目视差、运动像差+双目视差三种情况)的PSE之间存在显著性差异(<i>p</i>=0.03)。</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98"><b>4.2 讨 论</b></h4>
                <div class="p1">
                    <p id="99">在远距离观察条件下,运动像差线索比双目视差线索更易产生过大的深度感知;在运动像差和双目视差两个线索同时作用的情况下,双目视差线索对深度感知占有更大的权重;说明在远距离观察条件下,双目视差线索起主导作用。</p>
                </div>
                <div class="p1">
                    <p id="100">在近距离观察条件下,运动像差线索易产生过大的深度感知,双目视差线索易产生过小的深度感知;在运动像差和双目视差两个线索同时作用的情况下,深度感知并不存在线性加权的关系,而是运动像差和双目视差之间产生了相互抑制,造成更小的主观深度感知。由此可见,运动像差和双目视差之间并不是线性叠加关系,两者也不是相互独立的,而是可以相互影响的。</p>
                </div>
                <div class="p1">
                    <p id="101">此外,除了运动像差线索和双目视差线索,本实验中3D感知的轮廓线索也可能存在,可能会对深度感知产生一定的影响。接下来,将会隐藏3D形状轮廓,募集更多的被试者,进一步开展3D深度感知研究。</p>
                </div>
                <h3 id="102" name="102" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="103">结合人体常在三维空间中运动的实际使用场景,分别从远距离和近距离观察条件下,对被试者在运动像差线索、双目视差线索,以及运动像差和双目视差两种线索同时作用时的深度感知及其关系进行研究分析。实验结果显示,在远距离观察条件下,运动像差线索比双目视差线索更易产生过大的深度感知;而且在远距离观察条件下,运动像差线索对深度感知的权重比在近距离观察条件下大。在近距离观察条件下,运动像差线索易产生过大的深度感知,双目视差线索易产生过小的深度感知;两种线索同时作用对深度感知并不存在线性加权的关系,也没有相互促进作用,并且此时运动像差的线索对深度感知的作用较小。上述结果对于3D空间游戏场景构建、虚拟现实和增强现实眼镜设计、3D电影制作等实际应用具有指导作用。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceiving in depth volume 2:stereoscopic vision">

                                <b>[1]</b> Howard I P,Rogers B J.Perceiving in depth,volume 2:stereoscopic vision[M].New York:Oxford University Press,2012:385-432.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceiving in depth volume 3:other mechanisms of depth perception">

                                <b>[2]</b> Howard I P,Rogers B J.Perceiving in depth,volume 3:other mechanisms of depth perception[M].New York:Oxford University Press,2012:63-83.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Basic vision:an introduction to visual perception">

                                <b>[3]</b> Snowden R,Thompson P,Troscianko T.Basic vision:an introduction to visual perception[M].New York:Oxford University Press,2012:220-222.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceiving in depth volume 1:basic mechanisms">

                                <b>[4]</b> Howard I P,Rogers B J.Perceiving in depth,volume 1:basic mechanisms[M].New York:Oxford University Press,2012:435-474.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Binocular vision and stereopsis">

                                <b>[5]</b> Howard I P,Rogers B J.Binocular vision and stereopsis[M].New York:Oxford University Press,1995:108-121.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape constancy from binocular disparity with self-motion in depth">

                                <b>[6]</b> Shigemasu H,Okubo K,Yan P.Shape constancy from binocular disparity with self-motion in depth[J].Perception ECVP,2013,42:117-117.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600404224&amp;v=MTA1MTZUTW53WmVadUh5am1VYjdJSlY0U2FCQT1OaWZPZmJLN0h0RE5xWTlGWU9zTERuNDlvQk1UNlQ0UFFIL2lyUmRHZXJxUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Scarfe P,Hibbard P B.Disparity-defined objects moving in depth do not elicit three-dimensional shape constancy[J].Vision Research,2006,46(10):1599-1610.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sensitivity of the observer to transformations of the visual field">

                                <b>[8]</b> Braunstein M L.Sensitivity of the observer to transformations of the visual field[J].Journal of Experimental Psychology,1966,72(5):683-689.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJIX&amp;filename=SJIXEA57DD539E9E8D867877DA53737822FC&amp;v=MTIxMTFKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxod2JpNXdxbz1OaWZDZHNiSkc5YTQyNHBHYlo0R2VYUk54eEFVNGpoNlBBN25yeFUyZnJxV1I4enNDT052RlNpV1dyNw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Rogers B,Graham M.Motion parallax as an independent cue for depth perception[J].Perception,1979,8(2):125-134.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600408361&amp;v=Mjg2OTZOaWZPZmJLN0h0RE5xWTlGWU9zSEQzbzRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViN0lKVjRTYUJBPQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Landy M S,Maloney L T,Johnston E B,<i>et al</i>.Measurement and modeling of depth cue combination:in defense of weak fusion[J].Vision Research,1995,35(3):389-412.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Similarities between motion parallax and stereopsis in human depth perception">

                                <b>[11]</b> Rogers B,Graham M.Similarities between motion parallax and stereopsis in human depth perception[J].Vision Research,1982,22(2):261-270.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJIX&amp;filename=SJIX80D9A7AA1C7C6C45C9E6A3D6EDB0859C&amp;v=MDg5NjNCQzdLY1FMUHNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkbGh3Ymk1d3FvPU5pZkNkcnU0YXRpOXFQNDBaWmdJZjNwS3l4Tmc0MHA3T1h5V3FtZA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Tittle J S,Norman J F,Perotti V J,<i>et al</i>.The perception of scale-dependent and scale-independent surface structure from binocular disparity,texture,and shading[J].Perception,1998,27(2):147-166.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The interplay between stereopsis and structure from motion">

                                <b>[13]</b> Nawrot M,Blake R.The interplay between stereopsis and structure from motion[J].Perception &amp; Psychophysics,1991,49(3):230-244.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A perturbation analysis of depth perception from combinations of texture and motion cues">

                                <b>[14]</b> Young M J,Landy M S,Maloney L T.A perturbation analysis of depth perception from combinations of texture and motion cues[J].Vision Research,1993,33(18):2685-2696.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Foundations of cyclopean perception">

                                <b>[15]</b> Julesz B.Foundations of cyclopean perception[M].Chicago:University of Chicago Press,1971:142-148.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding vision:theory models and data">

                                <b>[16]</b> Li Z P.Understanding vision:theory,models,and data[M].New York:Oxford University Press,2014:16-66.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201910046" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201910046&amp;v=MDc4NjBJalhUYkxHNEg5ak5yNDlCWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2RnkzbFZiM0s=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

