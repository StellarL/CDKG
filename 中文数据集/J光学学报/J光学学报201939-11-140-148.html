

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133060674658750%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dGXXB201911017%26RESULT%3d1%26SIGN%3dXzwUmmj7uqtqhXVdf7WvYJSJK1o%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201911017&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201911017&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201911017&amp;v=MDc2MTF2Rnl2blZyek9JalhUYkxHNEg5ak5ybzlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#65" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#71" data-title="2 图像离焦仿真原理 ">2 图像离焦仿真原理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="&lt;b&gt;2.1 聚焦形貌恢复技术原理&lt;/b&gt;"><b>2.1 聚焦形貌恢复技术原理</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;2.2 虚拟测量模型的构建与点采样&lt;/b&gt;"><b>2.2 虚拟测量模型的构建与点采样</b></a></li>
                                                <li><a href="#83" data-title="&lt;b&gt;2.3 虚拟测量模型的构建与点采样&lt;/b&gt;"><b>2.3 虚拟测量模型的构建与点采样</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#114" data-title="3 仿真流程与实验 ">3 仿真流程与实验</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#129" data-title="4 应  用 ">4 应  用</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#146" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#74" data-title="图1 聚焦形貌恢复原理">图1 聚焦形貌恢复原理</a></li>
                                                <li><a href="#78" data-title="图2 点采样过程">图2 点采样过程</a></li>
                                                <li><a href="#81" data-title="图3 单个三角面片转换为点的算法流程">图3 单个三角面片转换为点的算法流程</a></li>
                                                <li><a href="#85" data-title="图4 透镜成像过程">图4 透镜成像过程</a></li>
                                                <li><a href="#89" data-title="图5 像平面与图像传感器平面重合">图5 像平面与图像传感器平面重合</a></li>
                                                <li><a href="#100" data-title="图6 像平面位于图像传感器平面前方">图6 像平面位于图像传感器平面前方</a></li>
                                                <li><a href="#102" data-title="图7 像平面位于图像传感器平面后方">图7 像平面位于图像传感器平面后方</a></li>
                                                <li><a href="#124" data-title="图8 仿真图像计算">图8 仿真图像计算</a></li>
                                                <li><a href="#127" data-title="图9 纹理图像及仿真结果。">图9 纹理图像及仿真结果。</a></li>
                                                <li><a href="#128" data-title="图10 两种复杂模型及仿真结果。">图10 两种复杂模型及仿真结果。</a></li>
                                                <li><a href="#137" data-title="图11 聚焦评价灰度图像">图11 聚焦评价灰度图像</a></li>
                                                <li><a href="#142" data-title="图12 使用不同算子得到的深度图。">图12 使用不同算子得到的深度图。</a></li>
                                                <li><a href="#144" data-title="图13 均方根误差对比">图13 均方根误差对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Danzl R,Helmli F,Scherer S.Focus variation-a new technology for high resolution optical 3D surface metrology[C]//The 10th international conference of the slovenian society for non-destructive testing,September 1-3,2009,Ljubljana,Slovenia.Ljubljana:Slovenian Society for Non-Destructive Testing,2009:484-491." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Focus variation-a new technology for high resolution optical 3D surface metrology">
                                        <b>[1]</b>
                                         Danzl R,Helmli F,Scherer S.Focus variation-a new technology for high resolution optical 3D surface metrology[C]//The 10th international conference of the slovenian society for non-destructive testing,September 1-3,2009,Ljubljana,Slovenia.Ljubljana:Slovenian Society for Non-Destructive Testing,2009:484-491.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Nayar S K,Nakagawa Y.Shape from focus[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1994,16(8):824-831." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape from Focus">
                                        <b>[2]</b>
                                         Nayar S K,Nakagawa Y.Shape from focus[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1994,16(8):824-831.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" Ni J,Yuan J H,Wu Q Z.Identification for optical image definition based on edge feature[J].Chinese Journal of Lasers,2009,36(1):172-176.倪军,袁家虎,吴钦章.基于边缘特征的光学图像清晰度判定[J].中国激光,2009,36(1):172-176." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JJZZ200901042&amp;v=MDcyMTZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXZuVnJ6T0x5ZlJkTEc0SHRqTXJvOUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Ni J,Yuan J H,Wu Q Z.Identification for optical image definition based on edge feature[J].Chinese Journal of Lasers,2009,36(1):172-176.倪军,袁家虎,吴钦章.基于边缘特征的光学图像清晰度判定[J].中国激光,2009,36(1):172-176.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Wang J Q,Zhang L G,Fu T J,&lt;i&gt;et al&lt;/i&gt;.Sharpness assessment for remote sensing image based on abstracting the edge image of skeleton[J].Laser &amp;amp; Optoelectronics Progress,2015,52(9):091002.王俊琦,张立国,付天骄,等.基于骨架边缘提取的遥感图像清晰度评价方法[J].激光与光电子学进展,2015,52(9):091002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201509016&amp;v=MjMxMTNvOUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXZuVnJ6T0x5clBaTEc0SDlUTXA=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Wang J Q,Zhang L G,Fu T J,&lt;i&gt;et al&lt;/i&gt;.Sharpness assessment for remote sensing image based on abstracting the edge image of skeleton[J].Laser &amp;amp; Optoelectronics Progress,2015,52(9):091002.王俊琦,张立国,付天骄,等.基于骨架边缘提取的遥感图像清晰度评价方法[J].激光与光电子学进展,2015,52(9):091002.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" Firestone L,Cook K,Culp K,&lt;i&gt;et al&lt;/i&gt;.Comparison of autofocus methods for automated microscopy[J].Cytometry,1991,12(3):195-206." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Comparison of Auto-focus Methods for Automated Microscopy">
                                        <b>[5]</b>
                                         Firestone L,Cook K,Culp K,&lt;i&gt;et al&lt;/i&gt;.Comparison of autofocus methods for automated microscopy[J].Cytometry,1991,12(3):195-206.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Helmli F S,Scherer S.Adaptive shape from focus with an error estimation in light microscopy[C]//ISPA 2001.Proceedings of the 2nd International Symposium on Image and Signal Processing and Analysis.In conjunction with 23rd International Conference on Information Technology Interfaces (IEEE Cat.No.01EX480),June 19-21,2001,Pula,Croatia.New York:IEEE,2001:188-193." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive Shape from Focus with an Error Estimation in Light Microscopy">
                                        <b>[6]</b>
                                         Helmli F S,Scherer S.Adaptive shape from focus with an error estimation in light microscopy[C]//ISPA 2001.Proceedings of the 2nd International Symposium on Image and Signal Processing and Analysis.In conjunction with 23rd International Conference on Information Technology Interfaces (IEEE Cat.No.01EX480),June 19-21,2001,Pula,Croatia.New York:IEEE,2001:188-193.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" Fan T T,Yu H B.A novel shape from focus method based on 3D steerable filters for improved performance on treating textureless region[J].Optics Communications,2018,410:254-261." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF69C17936B5EE877E51DDF0897EA11A3&amp;v=MjQwNzA3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRsaHg3cTZ3NjQ9TmlmT2ZjVytGNkxOcUlaR1lwa0tlUWt4eUJGbTd6NEpQQW5pcEJzeURNT1ZSTXVjQ09OdkZTaVdXcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Fan T T,Yu H B.A novel shape from focus method based on 3D steerable filters for improved performance on treating textureless region[J].Optics Communications,2018,410:254-261.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" Hariharan R,Rajagopalan A N.Shape-from-focus by tensor voting[J].IEEE Transactions on Image Processing,2012,21(7):3323-3328." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape-From-Focus by Tensor Voting">
                                        <b>[8]</b>
                                         Hariharan R,Rajagopalan A N.Shape-from-focus by tensor voting[J].IEEE Transactions on Image Processing,2012,21(7):3323-3328.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" Kaleem M,Mahmood M T.Combining focus measures through genetic algorithm for shape from focus[C]//2014 International Conference on Information Science &amp;amp; Applications (ICISA),May 6-9,2014,Seoul,Korea.New York:IEEE,2014:14431736." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Combining Focus Measures Through Genetic Algorithm for Shape From Focus">
                                        <b>[9]</b>
                                         Kaleem M,Mahmood M T.Combining focus measures through genetic algorithm for shape from focus[C]//2014 International Conference on Information Science &amp;amp; Applications (ICISA),May 6-9,2014,Seoul,Korea.New York:IEEE,2014:14431736.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" Minhas R,Mohammed A A,Wu Q M J,&lt;i&gt;et al&lt;/i&gt;.3D shape from focus and depth map computation using steerable filters[M]//Kamel M,Campilho A.International conference image analysis and recognition.Lecture notes in computer science.Berlin,Heidelberg:Springer,2009,5627:573-583." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D shape from focus and depth map computation using steerable filters">
                                        <b>[10]</b>
                                         Minhas R,Mohammed A A,Wu Q M J,&lt;i&gt;et al&lt;/i&gt;.3D shape from focus and depth map computation using steerable filters[M]//Kamel M,Campilho A.International conference image analysis and recognition.Lecture notes in computer science.Berlin,Heidelberg:Springer,2009,5627:573-583.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" Minhas R,Mohammed A A,Jonathan Wu Q M.Shape from focus using fast discrete curvelet transform[J].Pattern Recognition,2011,44(4):839-853." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738256&amp;v=MDY1MDJxUVRNbndaZVp1SHlqbVViN0lJMXdSYVJRPU5pZk9mYks3SHRETnFZOUZZK2dIRG5rL29CTVQ2VDRQUUgvaXJSZEdlcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Minhas R,Mohammed A A,Jonathan Wu Q M.Shape from focus using fast discrete curvelet transform[J].Pattern Recognition,2011,44(4):839-853.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" Hu T,Liu G D,Pu Z B.Depth from focus based on zero-phase filter[J].Opto-Electronic Engineering,2011,38(12):145-150.胡涛,刘国栋,浦昭邦.基于零相位滤波的聚焦形貌恢复技术[J].光电工程,2011,38(12):145-150." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDGC201112031&amp;v=MjA4NDlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5dm5WcnpPSWluTWJiRzRIOUROclk5R1pZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Hu T,Liu G D,Pu Z B.Depth from focus based on zero-phase filter[J].Opto-Electronic Engineering,2011,38(12):145-150.胡涛,刘国栋,浦昭邦.基于零相位滤波的聚焦形貌恢复技术[J].光电工程,2011,38(12):145-150.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" Jiang Z G,Shi W H,Han D B,&lt;i&gt;et al&lt;/i&gt;.Three-dimensional microscopy image system based on depth from focus[J].Computerized Tomography Theory and Applications,2004,13(4):9-15.姜志国,史文华,韩冬兵,等.基于聚焦合成的显微三维成像系统[J].CT理论与应用研究,2004,13(4):9-15." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CTLL200404003&amp;v=Mjg5MjRZckc0SHRYTXE0OUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXZuVnJ6T0pqbkg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Jiang Z G,Shi W H,Han D B,&lt;i&gt;et al&lt;/i&gt;.Three-dimensional microscopy image system based on depth from focus[J].Computerized Tomography Theory and Applications,2004,13(4):9-15.姜志国,史文华,韩冬兵,等.基于聚焦合成的显微三维成像系统[J].CT理论与应用研究,2004,13(4):9-15.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" Malik A S,Choi T S.Consideration of illumination effects and optimization of window size for accurate calculation of depth map for 3D shape recovery[J].Pattern Recognition,2007,40(1):154-170." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739672&amp;v=MjY4MjNQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWI3SUkxd1JhUlE9TmlmT2ZiSzdIdEROcVk5RlkrZ0dDbnM3b0JNVDZUNA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Malik A S,Choi T S.Consideration of illumination effects and optimization of window size for accurate calculation of depth map for 3D shape recovery[J].Pattern Recognition,2007,40(1):154-170.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" Lee I H,Shim S O,Choi T S.Improving focus measurement via variable window shape on surface radiance distribution for 3D shape reconstruction[J].Optics and Lasers in Engineering,2013,51(5):520-526." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011400065337&amp;v=MTM3ODN3WmVadUh5am1VYjdJSTF3UmFSUT1OaWZPZmJLN0h0RE5xNDlGWk8wS0QzOCtvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Lee I H,Shim S O,Choi T S.Improving focus measurement via variable window shape on surface radiance distribution for 3D shape reconstruction[J].Optics and Lasers in Engineering,2013,51(5):520-526.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" Muhammad M S,Mutahira H,Choi K W,&lt;i&gt;et al&lt;/i&gt;.Calculating accurate window size for shape-from-focus[C]//2014 International Conference on Information Science &amp;amp; Applications (ICISA),May 6-9,2014,Seoul,Korea.New York:IEEE,2014:14431716." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Calculating accurate window size for shape-from-focus">
                                        <b>[16]</b>
                                         Muhammad M S,Mutahira H,Choi K W,&lt;i&gt;et al&lt;/i&gt;.Calculating accurate window size for shape-from-focus[C]//2014 International Conference on Information Science &amp;amp; Applications (ICISA),May 6-9,2014,Seoul,Korea.New York:IEEE,2014:14431716.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_17" title=" Muhammad M,Choi T S.Sampling for shape from focus in optical microscopy[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(3):564-573." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sampling for Shape from Focus in Optical Microscopy">
                                        <b>[17]</b>
                                         Muhammad M,Choi T S.Sampling for shape from focus in optical microscopy[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(3):564-573.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_18" title=" Asif M,Choi T S.Shape from focus using multilayer feedforward neural networks[J].IEEE Transactions on Image Processing,2001,10(11):1670-1675." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape from focus using multilayer feedforward neural networks">
                                        <b>[18]</b>
                                         Asif M,Choi T S.Shape from focus using multilayer feedforward neural networks[J].IEEE Transactions on Image Processing,2001,10(11):1670-1675.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_19" title=" Kim H J,Mahmood M,Choi T S.An efficient neural network for shape from focus with weight passing method[J].Applied Sciences,2018,8(9):1648." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An efficient neural network for shape from focus with weight passing method">
                                        <b>[19]</b>
                                         Kim H J,Mahmood M,Choi T S.An efficient neural network for shape from focus with weight passing method[J].Applied Sciences,2018,8(9):1648.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_20" title=" Malik A S,Nisar H,Choi T S.A Fuzzy-Neural approach for estimation of depth map using focus[J].Applied Soft Computing,2011,11(2):1837-1850." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300297983&amp;v=MTkwMTBkR2VycVFUTW53WmVadUh5am1VYjdJSTF3UmFSUT1OaWZPZmJLN0h0RE5ySTlGWnVJSUJYUTZvQk1UNlQ0UFFIL2lyUg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         Malik A S,Nisar H,Choi T S.A Fuzzy-Neural approach for estimation of depth map using focus[J].Applied Soft Computing,2011,11(2):1837-1850.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_21" title=" Subbarao M,Lu M C.Image sensing model and computer simulation for CCD camera systems[J].Machine Vision and Applications,1994,7(4):277-289." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002055991&amp;v=MDM5MzU5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGU25sVUwzTkkxZz1OajdCYXJPNEh0SE9yNHBBYmVJT1kzazV6QmRoNGo5&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         Subbarao M,Lu M C.Image sensing model and computer simulation for CCD camera systems[J].Machine Vision and Applications,1994,7(4):277-289.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_22" title=" Subbarao M,Choi T S,Nikzad A,&lt;i&gt;et al&lt;/i&gt;.Focusing techniques[J].Optical Engineering,1993,32(11):2824-2837." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Focusing techniques">
                                        <b>[22]</b>
                                         Subbarao M,Choi T S,Nikzad A,&lt;i&gt;et al&lt;/i&gt;.Focusing techniques[J].Optical Engineering,1993,32(11):2824-2837.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_23" title=" Pertuz S,Puig D,Garcia M A.Analysis of focus measure operators for shape-from-focus[J].Pattern Recognition,2013,46(5):1415-1432." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739901&amp;v=MDY0OTZyUmRHZXJxUVRNbndaZVp1SHlqbVViN0lJMXdSYVJRPU5pZk9mYks3SHRETnFZOUZZK2dHQlh3NG9CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         Pertuz S,Puig D,Garcia M A.Analysis of focus measure operators for shape-from-focus[J].Pattern Recognition,2013,46(5):1415-1432.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_24" title=" Mutahira H,Muhammad M S,Jaffar A,&lt;i&gt;et al&lt;/i&gt;.Unorthodox approach toward microscopic shape from image focus using optical microscopy[J].Microscopy Research and Technique,2013,76(1):1-6." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD121229003889&amp;v=MDM1ODdjYXJLNkg5UE9wbzlGWitNSEJSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkNya1U3ektKVjBTTmlm&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                         Mutahira H,Muhammad M S,Jaffar A,&lt;i&gt;et al&lt;/i&gt;.Unorthodox approach toward microscopic shape from image focus using optical microscopy[J].Microscopy Research and Technique,2013,76(1):1-6.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_25" title=" Subbarao M,Choi T.Accurate recovery of three-dimensional shape from image focus[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1995,17(3):266-274." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate Recovery of Three-Dimensional Shape from Image Focus">
                                        <b>[25]</b>
                                         Subbarao M,Choi T.Accurate recovery of three-dimensional shape from image focus[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1995,17(3):266-274.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_26" title=" Pentland A P.A new sense for depth of field[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1987,PAMI-9(4):523-531." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A new sense for depth of field">
                                        <b>[26]</b>
                                         Pentland A P.A new sense for depth of field[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1987,PAMI-9(4):523-531.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_27" title=" Shim S O,Choi T S.A novel iterative shape from focus algorithm based on combinatorial optimization[J].Pattern Recognition,2010,43(10):3338-3347." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738375&amp;v=MjM1NjlycVFUTW53WmVadUh5am1VYjdJSTF3UmFSUT1OaWZPZmJLN0h0RE5xWTlGWStnSEQzczhvQk1UNlQ0UFFIL2lyUmRHZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         Shim S O,Choi T S.A novel iterative shape from focus algorithm based on combinatorial optimization[J].Pattern Recognition,2010,43(10):3338-3347.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-07-29 09:22</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(11),140-148 DOI:10.3788/AOS201939.1111001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种用于评价聚焦形貌恢复算法的图像离焦仿真技术</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%9F%A6%E5%8F%B7&amp;code=38300773&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">韦号</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B4%94%E6%B5%B7%E5%8D%8E&amp;code=17676087&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">崔海华</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A8%8B%E7%AD%B1%E8%83%9C&amp;code=08081411&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">程筱胜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%B0%8F%E8%BF%AA&amp;code=40663540&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张小迪</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%97%E4%BA%AC%E8%88%AA%E7%A9%BA%E8%88%AA%E5%A4%A9%E5%A4%A7%E5%AD%A6%E6%9C%BA%E7%94%B5%E5%AD%A6%E9%99%A2&amp;code=0014291&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南京航空航天大学机电学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E8%8B%8F%E7%9C%81%E6%95%B0%E5%AD%97%E5%8C%96%E8%AE%BE%E8%AE%A1%E5%88%B6%E9%80%A0%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=0248466&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江苏省数字化设计制造工程技术研究中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了提高聚焦形貌恢复技术的重建精度,寻找最优的聚焦形貌恢复算法,提出一种图像离焦仿真技术,用于对聚焦形貌恢复算法的精度进行评估。绘制一个三维模型,并选择一幅纹理图,根据图像分辨率对模型表面进行等间距点采样,生成有序点云数据。对通过点采样获取的数据进行处理并映射纹理,生成一组理想的或者包含噪声的序列仿真图像。利用得到的序列图像对聚焦形貌恢复算法进行实验验证,将生成的深度数据与点采样得到的真实深度数据进行对比以准确地评价算法的质量。研究结果表明,图像离焦仿真技术能够有效评价聚焦形貌恢复算法的质量,并有助于寻找更稳定、精度更高的算法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%88%90%E5%83%8F%E7%B3%BB%E7%BB%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">成像系统;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%9A%E7%84%A6%E5%BD%A2%E8%B2%8C%E6%81%A2%E5%A4%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">聚焦形貌恢复;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A6%BB%E7%84%A6%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">离焦模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B8%85%E6%99%B0%E5%BA%A6%E8%AF%84%E4%BB%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">清晰度评价;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BB%BF%E7%9C%9F%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">仿真算法;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    崔海华,E-mail:cuihh@nuaa.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-08</p>

                    <p>

                            <b>基金：</b>
                                                        <span>装备预研基金重点项目(6140923020102);</span>
                                <span>国家高档数控机床与基础制造装备——中小型飞机机身大部件复合加工机床项目(2014ZX04001071);</span>
                                <span>面向精密光学三维测量的跨尺度组合机理研究(SBK2019022827);</span>
                    </p>
            </div>
                    <h1><b>Image Defocus Simulation Technology Applied to Evaluation of Focused Morphology Recovery Algorithm</b></h1>
                    <h2>
                    <span>Wei Hao</span>
                    <span>Cui Haihua</span>
                    <span>Cheng Xiaosheng</span>
                    <span>Zhang Xiaodi</span>
            </h2>
                    <h2>
                    <span>College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics</span>
                    <span>Research Center of Digital Design and Manufacturing Engineering Technology of Jiangsu Province</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To improve the reconstruction accuracy of the focused morphology recovery technique and find the optimal focused morphology recovery algorithm, a simulation technology of image defocus is proposed in this paper, which is used to assess the reconstruction accuracy of the focused morphology recovery algorithm. A three-dimensional model is conducted and a texture image is selected. Then, the uniformly-spaced point sampling on the model surface according to the image resolution is performed and organized point clouds are generated. The acquired data obtained through point sampling is processed and the texture is mapped. At last, a set of sequence simulation images which contain the ideal image or images with noises are generated. The sequence images are used to verify the focused morphology recovery algorithm in experiments. The reconstructed depth data and the actual depth data are compared to accurately evaluate the performance of the proposed algorithm. The research results demonstrate that the image defocus simulation can assess the performance of the focused morphology recovery algorithm effectively and provide further support to find more algorithms with higher stability and accuracy.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=imaging%20system&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">imaging system;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=focused%20morphology%20recovery&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">focused morphology recovery;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=defocus%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">defocus model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=clarity-evaluation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">clarity-evaluation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=simulation%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">simulation algorithm;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-04-08</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="65" name="65" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="66">聚焦形貌恢复技术属于微观非接触光学测量技术的一种,具有不损伤测量表面、表面可测量倾斜角大(可达到80°)<citation id="148" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、能够提供表面颜色信息等优点。聚焦形貌恢复技术最初源于Nayar等<citation id="149" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出的Shape from Focus方法,也被称为Depth from Focus或者Focus Varation技术。聚焦形貌恢复算法主要包括像素点清晰度评价(聚焦评价算法)和三维重建算法两部分,该算法利用了显微镜有限景深的特点,在物体相对于显微镜光学系统纵向移动的过程中拍摄一组序列图像并同时记录拍摄图像的纵向高度信息。在移动过程中,图像中的每一个像素点都会经历从模糊到清晰再到模糊的过程。对图像中的每一个像素点进行清晰度评价,并对清晰度评价值进行拟合处理,最终得到每一个点的深度信息,从而得到微观场景的三维重建数据(注:本文所涉及的重建是指获取物体表面的三维点云数据,重建数据为有序点云数据)。聚焦形貌恢复技术中聚焦评价的结果直接影响后续获取的深度数据的精度,为此,学者门针对聚焦评价从不同角度提出了各种聚焦评价算子。</p>
                </div>
                <div class="p1">
                    <p id="67">Nayar等<citation id="150" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>在提出Shape from Focus方法的同时,提出了一种修正的拉普拉斯求和算子用于聚焦评价。基于拉普拉斯的聚焦评价方法在噪声较小的情况下对像素点清晰度的评价质量非常高,其缺点是对噪声过于敏感。许多图像清晰度评价算法也可应用于聚焦评价<citation id="153" type="reference"><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>,如广泛应用于自动对焦技术的灰度方差算子<citation id="151" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>同时也是一种常用的聚焦评价算子<citation id="152" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。灰度方差算子对图像噪声的敏感度低,但在低噪声下的图像清晰度评价质量并不如基于拉普拉斯的聚焦评价算子。为了提高像素点清晰度评价的质量和稳定性,学者们也提出了其他类型的聚焦评价算子<citation id="154" type="reference"><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="68">聚焦评价的窗口大小也会影响到聚焦评价的质量<citation id="155" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,因此寻找合适大小的聚焦评价窗口同样十分重要。Lee等<citation id="156" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>建议进行聚焦评价时使用动态的窗口,并给出了一种动态窗口计算方法。Muhammad等<citation id="157" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>认为窗口大小与成像设备参数有关,并给出了根据设备参数计算窗口大小的数学方法。此外,在聚焦评价值拟合方面,最常用的是高斯曲线拟合的方法,也有学者提出新的拟合方法<citation id="158" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。神经网络也被应用于聚焦形貌恢复技术中<citation id="159" type="reference"><link href="45" rel="bibliography" /><link href="47" rel="bibliography" /><link href="49" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>,并取得了不错的效果。</p>
                </div>
                <div class="p1">
                    <p id="69">关于聚焦形貌恢复算法的质量评价,需要通过分析重建数据与真实数据之间的差异才能得出可靠的结论。然而,获取测量物体表面的真实深度信息十分困难,这就使得新算法的效果验证变得十分困难。学者们在提出新的聚焦评价算子时,通常会设置一组仿真数据和一组真实数据来验证算法效果。仿真数据用于定量地评价算法质量,真实数据用于评估算法的实际表现。学者们在进行相关算法的验证时,最常用的一种仿真数据获取方式是通过Subbarao等<citation id="160" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提出的图像离焦模拟算法来产生仿真数据。在产生仿真图像时,该算法假定点扩展函数<citation id="161" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>是空间不变的。Pertuz等<citation id="162" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>在对各种聚焦评价算子进行分析对比时,认为点扩展函数空间不变性的假设并不合理,并计算了空间中每一个点的点扩展函数与聚焦图像的累积,得到最终的仿真图像。两种算法在推导空间点的点扩展半径时得到的结果并不准确,并且改变仿真的虚拟测量模型相对较难,最常用的仿真测量模型是一个圆锥体<citation id="163" type="reference"><link href="55" rel="bibliography" /><link href="57" rel="bibliography" /><link href="59" rel="bibliography" /><sup>[<a class="sup">23</a>,<a class="sup">24</a>,<a class="sup">25</a>]</sup></citation>,很少有文献使用其他类型的模型,然而实际测量对象的表面形貌是复杂多变的,仅使用圆锥形表面形貌来模拟实际测量对象并不合理。</p>
                </div>
                <div class="p1">
                    <p id="70">本文针对现有仿真算法的缺陷提出一种改进的获取仿真数据的算法。该算法可以建立任意的表面连续的仿真测量模型,用于模拟复杂多变的表面形貌,并推导出了更合理的空间点的点扩展半径计算公式。考虑到实际采集图像时存在图像噪声,仿真算法同样可以对图像添加高斯噪声,以更贴近实际采集的图像。使用仿真数据对最常用的两种聚焦评价算子进行分析,结果表明,该算法可以有效地评价聚焦评价算子,并可以用于评价其他聚焦形貌恢复算法。</p>
                </div>
                <h3 id="71" name="71" class="anchor-tag">2 图像离焦仿真原理</h3>
                <h4 class="anchor-tag" id="72" name="72"><b>2.1 聚焦形貌恢复技术原理</b></h4>
                <div class="p1">
                    <p id="73">聚焦形貌恢复技术利用了显微镜有限景深的特点来进行微观三维重建。聚焦形貌恢复的原理如图1所示,其中,<i>z</i><sub><i>n</i></sub>(<i>n</i>=1,2,…,<i>N</i>,其中<i>N</i>为采集图像的总数)表示第<i>n</i>幅图的高度,<i>V</i>(<i>x</i>,<i>y</i>,<i>z</i><sub><i>n</i></sub>)与<i>V</i>(<i>x</i>,<i>y</i>,<i>z</i><sub><i>n</i></sub><sub>-1</sub>)表示<i>x</i>和<i>y</i>相同而高度不同的点。在测量物体相对于显微镜光学系统纵向移动的过程中拍摄一组图片,同时记录采集每一幅图片时对应的高度信息<i>z</i>。序列图片中同一位置处的像素点会经历从模即其聚焦评价值会先糊到清晰再到模糊的过程,递增达到一个最大值然后递减。以高度信息<i>z</i>为自变量,聚焦评价值为因变量,对其进行拟合,得到聚焦评价值最大时对应的高度信息<i>z</i><sub>t</sub>,即得到了该点的深度信息。计算出图像中所有像素点的深度信息,即得到了微观场景的深度数据。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911017_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 聚焦形貌恢复原理" src="Detail/GetImg?filename=images/GXXB201911017_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 聚焦形貌恢复原理  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911017_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Principle of focused morphology recovery</p>

                </div>
                <h4 class="anchor-tag" id="76" name="76"><b>2.2 虚拟测量模型的构建与点采样</b></h4>
                <div class="p1">
                    <p id="77">虚拟模型,即假想的测量对象,可以模拟实际测量对象复杂的表面形貌。虚拟模型为曲面模型,可以通过函数生成,但是实际测量对象的表面形貌复杂多变,这就增加了函数设计的复杂性,使得曲面模型的生成变得困难。为此,通过三维建模软件来设计曲面模型。与通过函数生成曲面的方法相比,通过建模软件设计曲面可以便捷地调整曲面轮廓,使模型的生成变得更加简单和灵活。通过构建虚拟模型和点采样的方法可以构建任意表面连续的虚拟模型,并可以通过点采样算法获取模型表面的点云信息。这里所获取的点云的深度信息即为模型表面真实的深度信息,可用于与后面重建的点云进行逐点对比。为了模拟真实测量对象的表面纹理,需要选择一幅纹理图像,这里纹理是任意的。根据图像的大小设计虚拟模型,然后进行点采样,点采样流程如图2所示。</p>
                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911017_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 点采样过程" src="Detail/GetImg?filename=images/GXXB201911017_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 点采样过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911017_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Point sampling process</p>

                </div>
                <div class="p1">
                    <p id="79">首先,通过3D建模软件构建模型。所使用的建模软件为UG软件。需要注意的是,构建的模型并非一个实体,而是片体,片体的<i>x</i>、<i>y</i>方向的尺寸与所选取的纹理图片的尺寸一致,尺寸单位为mm。根据图像分辨率对虚拟模型表面进行点采样,如图像分辨率为520 pixel×520 pixel,则对模型横向、纵向均等间隔采集520个点,最终得到270400个点。将构建的片体模型文件转换成为STL格式的三角面片文件,然后针对这个STL文件中定义的每一个三角面片,求出其内部所有<i>x</i>、<i>y</i>坐标为整数的点,算法流程如图3所示。</p>
                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911017_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 单个三角面片转换为点的算法流程" src="Detail/GetImg?filename=images/GXXB201911017_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 单个三角面片转换为点的算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911017_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Algorithm process of transforming single triangle patch to points</p>

                </div>
                <div class="p1">
                    <p id="82">根据图3算法流程将STL文件中所有的三角面片都转换成点,即完成了对模型的表面点采样过程。注意,<i>x</i><sub>min</sub>与<i>x</i><sub>max</sub>分别表示一个三角面片三个顶点坐标中<i>x</i>的最大值与最小值,<i>y</i><sub>min</sub>和<i>y</i><sub>max</sub>的含义与此类似。最终得到的点与所选取纹理图像的像素点存在一一对应关系,点采样得到的点云为有序点云。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83"><b>2.3 虚拟测量模型的构建与点采样</b></h4>
                <div class="p1">
                    <p id="84">仿真图像的获取模拟了聚焦形貌恢复技术中被测量对象相对于光学系统纵向移动过程中图像的采集过程。由于显微镜的景深是有限的,因此通过工业相机拍摄到的图像清晰度是不均匀的,其几何光学成像原理如图4所示,其中<i>u</i>和<i>v</i>分别为物距和像距,<i>f</i>为焦距,<i>R</i>为透镜半径。对于空间中的一点,当图像传感器与焦平面重合时,<i>P</i>点在图像传感器上是一个清晰的点<i>Q</i>;当图像传感器偏离焦平面时,<i>P</i>点在图像传感器上是一个半径为<i>r</i>的模糊圆。一幅图像可以看作由物体上所有的点在图像传感器上对应的模糊圆叠加而成。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911017_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 透镜成像过程" src="Detail/GetImg?filename=images/GXXB201911017_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 透镜成像过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911017_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Process of lens imaging</p>

                </div>
                <div class="p1">
                    <p id="86">如图4所示,对于物体上的一个点<i>P</i>而言,其成像点<i>Q</i>偏离图像传感器平面的距离<i>δ</i>决定了其模糊圆的半径。模糊圆可以通过点扩展函数描述,Pentland<citation id="164" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>认为,点扩展函数<i>h</i><sub>PSF</sub>(<i>x</i>,<i>y</i>)可以通过高斯函数来进行近似,并且空间参数<i>σ</i><sub>h</sub>与模糊圆半径成正比。点扩展函数的公式为</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>h</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>S</mtext><mtext>F</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mtext>π</mtext><mi>σ</mi><msubsup><mrow></mrow><mtext>h</mtext><mn>2</mn></msubsup></mrow></mfrac><mi>exp</mi><mrow><mo>(</mo><mrow><mo>-</mo><mfrac><mrow><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msubsup><mrow></mrow><mtext>h</mtext><mn>2</mn></msubsup></mrow></mfrac></mrow><mo>)</mo></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">如图5所示,当<i>δ</i>为0时,图像传感器平面与焦平面重合,物体上的一点<i>P</i>在图像传感器上为一个清晰的点<i>Q</i>,<i>O</i>为物体表面与光轴交点。在聚焦形貌恢复技术中,像距<i>v</i><sub>0</sub>不变,而物距<i>u</i><sub>0</sub>改变。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911017_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 像平面与图像传感器平面重合" src="Detail/GetImg?filename=images/GXXB201911017_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 像平面与图像传感器平面重合  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911017_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Imaging plane coincides with image detector plane</p>

                </div>
                <div class="p1">
                    <p id="90">物体沿光轴反方向偏移一段距离Δ<i>u</i><sub>L</sub>,即以图5中物体位置为参考位置向左平移Δ<i>u</i><sub>L</sub>时,如图6所示,<i>O</i><sub>L</sub>为左移后物体表面与光轴的交点,成像点<i>Q</i>在图像传感器的前面,其偏移距离为Δ<i>v</i><sub>L</sub>,模糊圆半径为<i>r</i><sub>L</sub>,根据高斯透镜公式:</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mi>f</mi></mfrac><mo>=</mo><mfrac><mn>1</mn><mi>u</mi></mfrac><mo>+</mo><mfrac><mn>1</mn><mi>v</mi></mfrac><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo><mspace width="0.25em" /></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">得到</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mn>1</mn><mi>f</mi></mfrac><mo>=</mo><mfrac><mn>1</mn><mrow><mi>u</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mtext>Δ</mtext><mi>u</mi><msub><mrow></mrow><mtext>L</mtext></msub></mrow></mfrac><mo>+</mo><mfrac><mn>1</mn><mrow><mi>v</mi><msub><mrow></mrow><mn>0</mn></msub><mo>-</mo><mtext>Δ</mtext><mi>v</mi><msub><mrow></mrow><mtext>L</mtext></msub></mrow></mfrac><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mtext>Δ</mtext><mi>u</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>v</mi><msub><mrow></mrow><mn>0</mn></msub><mo>-</mo><mtext>Δ</mtext><mi>v</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false">)</mo><mi>f</mi></mrow><mrow><mi>v</mi><msub><mrow></mrow><mn>0</mn></msub><mo>-</mo><mtext>Δ</mtext><mi>v</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>-</mo><mi>f</mi></mrow></mfrac><mo>-</mo><mi>u</mi><msub><mrow></mrow><mn>0</mn></msub><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">利用比例关系</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mtext>Δ</mtext><mi>v</mi><msub><mrow></mrow><mtext>L</mtext></msub></mrow><mrow><mi>v</mi><msub><mrow></mrow><mn>0</mn></msub><mo>-</mo><mtext>Δ</mtext><mi>v</mi><msub><mrow></mrow><mtext>L</mtext></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>r</mi><msub><mrow></mrow><mtext>L</mtext></msub></mrow><mi>R</mi></mfrac><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">推导得到</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Δ</mtext><mi>v</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>=</mo><mfrac><mrow><mi>v</mi><msub><mrow></mrow><mn>0</mn></msub><mi>r</mi><msub><mrow></mrow><mtext>L</mtext></msub></mrow><mrow><mi>R</mi><mo>+</mo><mi>r</mi><msub><mrow></mrow><mtext>L</mtext></msub></mrow></mfrac><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">将(6)式代入(4)式,得到</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>=</mo><mi>R</mi><mrow><mo>(</mo><mrow><mfrac><mrow><mi>v</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mrow><mi>u</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></mfrac><mo>-</mo><mfrac><mrow><mi>v</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mrow><mi>u</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mtext>Δ</mtext><mi>u</mi><msub><mrow></mrow><mtext>L</mtext></msub></mrow></mfrac></mrow><mo>)</mo></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911017_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 像平面位于图像传感器平面前方" src="Detail/GetImg?filename=images/GXXB201911017_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 像平面位于图像传感器平面前方  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911017_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Imaging plane locates in front of image detector plane</p>

                </div>
                <div class="p1">
                    <p id="101">若物体沿光轴方向偏移一段距离Δ<i>u</i><sub>R</sub>,即以图5中物体位置为参考位置向右平移Δ<i>u</i><sub>R</sub>时,对于物体上的一点<i>P</i>,其成像点<i>Q</i>在图像传感器的后面,如图7所示,<i>O</i><sub>R</sub>为物体右移后物体表面与光轴交点,<i>r</i><sub>R</sub>为右移后的模糊圆半径。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911017_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 像平面位于图像传感器平面后方" src="Detail/GetImg?filename=images/GXXB201911017_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 像平面位于图像传感器平面后方  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911017_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Imaging plane locates in back of image detector plane</p>

                </div>
                <div class="p1">
                    <p id="103">同理,可推导出</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo>=</mo><mi>R</mi><mrow><mo>(</mo><mrow><mfrac><mrow><mi>v</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mrow><mi>u</mi><msub><mrow></mrow><mn>0</mn></msub><mo>-</mo><mtext>Δ</mtext><mi>u</mi><msub><mrow></mrow><mtext>R</mtext></msub></mrow></mfrac><mo>-</mo><mfrac><mrow><mi>v</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mrow><mi>u</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></mfrac></mrow><mo>)</mo></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105">物体上的一个点在图像传感器上形成的模糊圆图像称为点扩展图像。点(<i>x</i>,<i>y</i>)处的点扩展图像<i>S</i><sub>(</sub><sub><i>x</i></sub><sub>,</sub><sub><i>y</i></sub><sub>)</sub>可以看作点扩展函数与纹理图像的卷积:</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mi>h</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>S</mtext><mtext>F</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>*</mo><mi>Ι</mi><msub><mrow></mrow><mtext>Τ</mtext></msub><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">式中:<i>h</i><sub>PSF</sub>(<i>x</i>,<i>y</i>)为点(<i>x</i>,<i>y</i>)处的点扩展函数;<i>I</i><sub>T</sub>为纹理图像;*表示卷积。注意,这里所使用的卷积操作并不是真正的卷积,而是相关运算,即并未对卷积模板进行旋转操作。对于点扩展函数<i>h</i><sub>PSF</sub>(<i>x</i><sub>0</sub>,<i>y</i><sub>0</sub>),当像素点(<i>x</i>,<i>y</i>)偏离中心(<i>x</i><sub>0</sub>,<i>y</i><sub>0</sub>)的距离为2.5<i>σ</i><sub>h</sub>时,其对应的<i>h</i><sub>PSF</sub>(<i>x</i>,<i>y</i>)值可以忽略。因此,点扩展图像的计算可以限制为</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mi>y</mi><mo>-</mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo><mo>≤</mo><mn>6</mn><mo>.</mo><mn>2</mn><mn>5</mn><mi>σ</mi><msubsup><mrow></mrow><mtext>h</mtext><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">正如前面所提到的空间参数<i>σ</i><sub>h</sub>与模糊圆半径成正比,这里可以简单地取<i>σ</i><sub>h</sub>为2.5/<i>r</i>,这样就确定了点扩展函数的空间参数。计算出物体上所有点的点扩展图像,然后累加,即得到最终的仿真图像:</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>W</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Η</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>S</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>W</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Η</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>B</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></mfrac><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">式中:<i>W</i>为纹理图像的宽度;<i>H</i>为纹理图像的高度;<i>I</i>(<i>x</i><sub>0</sub>,<i>y</i><sub>0</sub>)为仿真图像在(<i>x</i><sub>0</sub>,<i>y</i><sub>0</sub>)处的值;<i>S</i><sub>(</sub><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub><sub>)</sub>(<i>x</i><sub>0</sub>,<i>y</i><sub>0</sub>)为点(<i>i</i>,<i>j</i>)处的点扩展图像在(<i>x</i><sub>0</sub>,<i>y</i><sub>0</sub>)处的值;<i>B</i><sub>(</sub><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub><sub>)</sub>(<i>x</i><sub>0</sub>,<i>y</i><sub>0</sub>)为点(<i>i</i>,<i>j</i>)处的点扩展函数<i>h</i><sub>PSF</sub>(<i>i</i>,<i>j</i>)在(<i>x</i><sub>0</sub>,<i>y</i><sub>0</sub>)处的值。对点扩展图像在(<i>x</i><sub>0</sub>,<i>y</i><sub>0</sub>)处的值进行加权平均,以抑制最终得到的仿真图像灰度值的突变。实际获取的图像往往存在噪声,为了模拟噪声对聚焦形貌恢复技术的影响,对于生成的仿真图像,仿真算法可选择性添加高斯噪声。对于图像中位于(<i>x</i><sub>0</sub>,<i>y</i><sub>0</sub>)的像素值<i>I</i>(<i>x</i><sub>0</sub>,<i>y</i><sub>0</sub>),其噪声添加可表示为</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><msub><mrow></mrow><msup><mtext>n</mtext><mo>′</mo></msup></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo><mo>+</mo><mi>G</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msub><mi>a</mi><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">式中:<i>I</i><sub>n′</sub>(<i>x</i><sub>0</sub>,<i>y</i><sub>0</sub>)为添加了噪声的图像;<i>G</i><sub>(0,1)</sub>为一个服从标准正态分布的随机量;<i>a</i>为一个噪声控制系数,用于控制噪声等级。</p>
                </div>
                <h3 id="114" name="114" class="anchor-tag">3 仿真流程与实验</h3>
                <div class="p1">
                    <p id="115">仿真流程可以分为模型构建与点采样,以及生成仿真图像两部分,这里以最常用的圆锥模型为例进行阐述,具体流程如下。</p>
                </div>
                <div class="p1">
                    <p id="116">1) 使用三维建模软件绘制一个三维圆锥面片作为虚拟测量模型,选取根据前面所述的点采样方法,对模型进行点采样,得到该模型的点云数据。</p>
                </div>
                <div class="p1">
                    <p id="117">2) 如图8所示,设模型高度为<i>h</i>,设置参考平面的数目为<i>q</i>,以<i>p</i><sub><i>q</i></sub>表示第<i>q</i>个基准面(<i>q</i>=1,2,…,<i>N</i>′,其中<i>N</i>′为基准面总数)<i>J</i>为一个采样点,参考平面间距<i>d</i><sub>b</sub>可表示为</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mtext>b</mtext></msub><mo>=</mo><mfrac><mrow><mi>D</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mrow><mi>n</mi><mo>-</mo><mn>2</mn><mi>m</mi></mrow></mfrac><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">式中:<i>D</i><sub>max</sub>与<i>D</i><sub>min</sub>分别为点采样点云的最大和最小深度值;<i>m</i>为圆锥模型顶部以上离焦图像或底部以下离焦图像数量。注意,这里<i>m</i>的设置是必须的,以便计算圆锥顶部或者底部点的高度。</p>
                </div>
                <div class="p1">
                    <p id="120">3) 选取一幅图像作为纹理图像,大小为520 pixel×520 pixel。初始化一个图像矩阵<i>E</i>、权值矩阵<i>W</i>,以<i>p</i><sub>4</sub>基准面为例,根据前面点采样得到的数据,计算点<i>p</i><sub>4</sub>到基准面的距离。以<i>J</i>点为例,其到基准面的距离为<i>δ</i><sub>4</sub>,判断其位于基准面的上方还是下方,计算其对应的点扩展函数。</p>
                </div>
                <div class="p1">
                    <p id="121">4) 计算点扩展函数与纹理图像的卷积,并将卷积结果赋值于<i>E</i>,对应区域的权值赋值于<i>W</i>。</p>
                </div>
                <div class="p1">
                    <p id="122">5) 对于每一个点,重复步骤3)和4),根据(10)式最终计算出<i>p</i><sub>4</sub>基准平面上的仿真图像。</p>
                </div>
                <div class="p1">
                    <p id="123">6) 重复步骤3)～5),直至计算出所有基准面上的仿真图像。判断是否需要添加噪声,输出无噪声或者包含噪声的仿真图像。</p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911017_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 仿真图像计算" src="Detail/GetImg?filename=images/GXXB201911017_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 仿真图像计算  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911017_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Calculation of simulation images</p>

                </div>
                <div class="p1">
                    <p id="125">上述仿真流程实际上是模拟了图像采集的过程。根据上述流程,选取<i>R</i>为6 mm,<i>v</i><sub>0</sub>为12.6 mm,<i>f</i>为6 mm,进而推导出<i>u</i><sub>0</sub>为6.3 mm。选取的纹理图像为使用带有工业相机的显微镜采集的真实的金属表面,所使用的工业相机为映美精DFK23G445,图像大小为520 pixel×520 pixel,构建的模型为前面提到过的圆锥。仿真结果如图9所示。</p>
                </div>
                <div class="p1">
                    <p id="126">如图9(c)所示,仿真算法生成了仿真序列图像,每一幅图像的清晰度都是不均匀的。本仿真算法可以模拟任意表面连续的形貌,选取任意的表面纹理。在所选取的参数下设计一个更加复杂的表面模型(为了更好地展示,已进行伪彩色处理)(图10),其中,图10(a)与图10(b)为两个表面形貌更加复杂的模型A与B,图10(c)与图10(d)分别为A与B对应的仿真序列图像的一部分(分别为第1,10,20,30帧仿真图像)。</p>
                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911017_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 纹理图像及仿真结果。" src="Detail/GetImg?filename=images/GXXB201911017_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 纹理图像及仿真结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911017_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Texture image and simulated results. </p>
                                <p class="img_note">(a)纹理图像;(b)仿真图像;(c)仿真序列图像</p>
                                <p class="img_note">(a) Texture image; (b) simulated image; (c) simulated sequence images</p>

                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911017_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 两种复杂模型及仿真结果。" src="Detail/GetImg?filename=images/GXXB201911017_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 两种复杂模型及仿真结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911017_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Two complex models and corresponding simulated images. </p>
                                <p class="img_note">(a)模型A;(b)模型B;(c)模型A的仿真图像;(d)模型B的仿真图像</p>
                                <p class="img_note">(a) Model A; (b) model B; 
(c) simulated images of model A; (d) simulated images of model B</p>

                </div>
                <h3 id="129" name="129" class="anchor-tag">4 应  用</h3>
                <div class="p1">
                    <p id="130">设计仿真算法的目的在于提供一种更合理的适用于聚焦形貌恢复算法质量评价的离焦仿真模型。这里以修正的拉普拉斯求和算子(SML)和灰度方差算子(GLV)为例,分析在聚焦评价窗口相同的条件下噪声对两者的影响,以说明所提出的离焦仿真模型的具体应用。</p>
                </div>
                <div class="p1">
                    <p id="131">修正的拉普拉斯求和算子<i>M</i><sub>L</sub>的表达式为</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Μ</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>=</mo><mo stretchy="false">|</mo><mn>2</mn><mo stretchy="false">(</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>-</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>-</mo><mi>s</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>e</mtext><mtext>p</mtext></mrow></msub><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>-</mo></mtd></mtr><mtr><mtd><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>s</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>e</mtext><mtext>p</mtext></mrow></msub><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mo>+</mo><mo stretchy="false">|</mo><mn>2</mn><mo stretchy="false">(</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>-</mo></mtd></mtr><mtr><mtd><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>-</mo><mi>s</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>e</mtext><mtext>p</mtext></mrow></msub><mo stretchy="false">)</mo><mo>-</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>+</mo><mi>s</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>e</mtext><mtext>p</mtext></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>F</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>Μ</mtext><mtext>L</mtext></mrow></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mi>i</mi><mo>-</mo><mi>Ν</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mrow><mi>i</mi><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></munderover><mspace width="0.25em" /></mstyle><mspace width="0.25em" /><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>y</mi><mo>=</mo><mi>j</mi><mo>-</mo><mi>Ν</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mrow><mi>j</mi><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></munderover><mi>Μ</mi></mstyle><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="left"><mi>Μ</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>≥</mo><mi>Τ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo></mtd></mtr></mtable></mrow></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133">式中:<i>F</i><sub>SML</sub>(<i>i</i>,<i>j</i>)为图像在(<i>i</i>,<i>j</i>)处的聚焦评价值;<i>s</i><sub>tep</sub>为步长;<i>N</i><sub>0</sub>为计算窗口大小;<i>T</i><sub><i>i</i></sub>为阈值。灰度方差算子的表达式为</p>
                </div>
                <div class="p1">
                    <p id="134" class="code-formula">
                        <mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mrow><mtext>G</mtext><mtext>L</mtext><mtext>V</mtext></mrow></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>Ω</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></munder><mo stretchy="false">[</mo></mstyle><mi>Ι</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>-</mo><mi>μ</mi><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="135">式中:<i>F</i><sub>GLV</sub>(<i>i</i>,<i>j</i>)与<i>I</i>(<i>i</i>,<i>j</i>)分别为图像在(<i>i</i>,<i>j</i>)处的聚焦评价值和灰度值;<i>μ</i>为区域<i>Ω</i>(<i>x</i>,<i>y</i>)内像素灰度值的平均值。</p>
                </div>
                <div class="p1">
                    <p id="136">选定计算窗口大小为3 pixel×3 pixel。为了模拟表面形貌的复杂性,以图10中的模型A为例,选择纹理图像,利用仿真算法生成点采样点云和序列图像。通过修改(12)式的系数<i>a</i>将序列图像分为7级,等级越高,噪声污染越严重。分别使用两种聚焦评价算法对序列图像进行聚焦评价,聚焦评价后的序列图像如图11所示。</p>
                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911017_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 聚焦评价灰度图像" src="Detail/GetImg?filename=images/GXXB201911017_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 聚焦评价灰度图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911017_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Grayscale images of focusing evaluation</p>

                </div>
                <div class="p1">
                    <p id="138">图11为使用GLV算子对无噪声图像进行聚焦评价的结果(已转换为灰度图),其中明亮的地方即为清晰度高的地方。可以看到,序列图像的清晰度是不均匀的。在其他条件相同的情况下分别使用SML和GLV算子对不同噪声等级的图像进行聚焦评价,最终计算得到的深度图如图12所示。</p>
                </div>
                <div class="p1">
                    <p id="139">图12中噪声等级从左到右依次为1～5级。由图12可知,随着噪声等级的增加,噪声对GLV算子影响较小,对SML算子的影响较大。噪声对灰度值方差的影响相对较小,而基于拉普拉斯的算子对噪声非常敏感,尽管采用了3 pixel×3 pixel的窗口来抑制噪声的影响,但是其抗噪能力仍然不如灰度方差算子。通过所提出的仿真方法,可以获取模型表面采样点的真实深度信息,这使得重建点云与采样点云的逐点对比变得可行;因此,可以准确地量化两种算子的实际表现。这里采用最为常用的均方根误差<i>R</i><sub>MSE</sub>来进行定量评估,<i>R</i><sub>MSE</sub>可表示为</p>
                </div>
                <div class="p1">
                    <p id="140" class="code-formula">
                        <mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>S</mtext><mtext>E</mtext></mrow></msub><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>l</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></munder><mo stretchy="false">[</mo></mstyle><mi>D</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>u</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>-</mo><mi>D</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mo>,</mo><mspace width="0.25em" /><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="142">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911017_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 使用不同算子得到的深度图。" src="Detail/GetImg?filename=images/GXXB201911017_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 使用不同算子得到的深度图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911017_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Depth maps obtained by different operators.</p>
                                <p class="img_note">(a) GLV算子;(b) SML算子</p>
                                <p class="img_note"> (a) GLV operator; (b) SML operator</p>

                </div>
                <div class="p1">
                    <p id="143">式中:<i>D</i><sub>true</sub>和<i>D</i><sub>cal</sub>分别为采样点云和重建点云在点(<i>i</i>,<i>j</i>)的深度值;<i>l</i>为点的总数。分别计算两组的<i>R</i><sub>MSE</sub>值,结果如图13所示。</p>
                </div>
                <div class="area_img" id="144">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911017_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 均方根误差对比" src="Detail/GetImg?filename=images/GXXB201911017_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 均方根误差对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911017_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 13 Comparison of root mean square error</p>

                </div>
                <div class="p1">
                    <p id="145"><i>R</i><sub>MSE</sub>值越小,表明重建点云与采样点云的差异越小。由图13可以明显地看出噪声对两种算子的影响。在低噪声下,SML算子评价的准确性要好于GLV算子,SML算子对图像灰度的变化非常敏感,使得其对图像清晰度的变化很敏感,也造成了其对噪声高敏感。这里得到的SML算子与GLV算子的对比结果与Shim等<citation id="165" type="reference"><link href="63" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>的实验结果一致。由于实际的测量环境十分复杂,测量对象的表面纹理、光照条件、表面形貌等都会影响最终的测量结果,因此每一种聚焦评价算子都有其局限性。上面的分析过程可以推广到其他因素分析中,包括聚焦评价算子窗口大小、聚焦评价值拟合算法、表面纹理等。所提出的方法能够用于模拟更复杂的表面形貌和选择任意的表面纹理,通过点采样算法可以获取复杂模型表面真实的深度数据,将重建点云数据与之对比,即可定量地评价聚焦形貌恢复算法的质量,这为寻找更好的聚焦形貌恢复算法提供了有力的支持。</p>
                </div>
                <h3 id="146" name="146" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="147">提出一种适用于对聚焦形貌恢复算法进行评估的图像离焦的仿真方法,并推导出了更合理的空间点的点扩展半径计算公式。仿真算法可以构建任意的表面连续的虚拟测量模型,选择任意的表面纹理以模拟复杂的表面形貌。仿真算法对构建的虚拟测量对象表面进行了点采样,获取了其真实的深度信息,模拟了显微镜的图像采集过程,并生成了一组序列仿真图像。对仿真图像应用聚焦形貌恢复算法进行虚拟测量模型的重建,将重建的深度信息与其真实的深度信息进行对比,就能够定量地评价聚焦形貌恢复算法的质量。实验表明,该仿真算法可以有效地评价相关聚焦形貌恢复算法的质量,并有助于寻找和开发新的算法来改善聚焦形貌恢复技术的重建质量。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Focus variation-a new technology for high resolution optical 3D surface metrology">

                                <b>[1]</b> Danzl R,Helmli F,Scherer S.Focus variation-a new technology for high resolution optical 3D surface metrology[C]//The 10th international conference of the slovenian society for non-destructive testing,September 1-3,2009,Ljubljana,Slovenia.Ljubljana:Slovenian Society for Non-Destructive Testing,2009:484-491.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape from Focus">

                                <b>[2]</b> Nayar S K,Nakagawa Y.Shape from focus[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1994,16(8):824-831.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JJZZ200901042&amp;v=MDA1NjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXZuVnJ6T0x5ZlJkTEc0SHRqTXJvOUJab1FLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Ni J,Yuan J H,Wu Q Z.Identification for optical image definition based on edge feature[J].Chinese Journal of Lasers,2009,36(1):172-176.倪军,袁家虎,吴钦章.基于边缘特征的光学图像清晰度判定[J].中国激光,2009,36(1):172-176.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201509016&amp;v=MjUxODZPZVplVnZGeXZuVnJ6T0x5clBaTEc0SDlUTXBvOUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Wang J Q,Zhang L G,Fu T J,<i>et al</i>.Sharpness assessment for remote sensing image based on abstracting the edge image of skeleton[J].Laser &amp; Optoelectronics Progress,2015,52(9):091002.王俊琦,张立国,付天骄,等.基于骨架边缘提取的遥感图像清晰度评价方法[J].激光与光电子学进展,2015,52(9):091002.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Comparison of Auto-focus Methods for Automated Microscopy">

                                <b>[5]</b> Firestone L,Cook K,Culp K,<i>et al</i>.Comparison of autofocus methods for automated microscopy[J].Cytometry,1991,12(3):195-206.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive Shape from Focus with an Error Estimation in Light Microscopy">

                                <b>[6]</b> Helmli F S,Scherer S.Adaptive shape from focus with an error estimation in light microscopy[C]//ISPA 2001.Proceedings of the 2nd International Symposium on Image and Signal Processing and Analysis.In conjunction with 23rd International Conference on Information Technology Interfaces (IEEE Cat.No.01EX480),June 19-21,2001,Pula,Croatia.New York:IEEE,2001:188-193.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF69C17936B5EE877E51DDF0897EA11A3&amp;v=MzI3MTVXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRsaHg3cTZ3NjQ9TmlmT2ZjVytGNkxOcUlaR1lwa0tlUWt4eUJGbTd6NEpQQW5pcEJzeURNT1ZSTXVjQ09OdkZTaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Fan T T,Yu H B.A novel shape from focus method based on 3D steerable filters for improved performance on treating textureless region[J].Optics Communications,2018,410:254-261.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape-From-Focus by Tensor Voting">

                                <b>[8]</b> Hariharan R,Rajagopalan A N.Shape-from-focus by tensor voting[J].IEEE Transactions on Image Processing,2012,21(7):3323-3328.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Combining Focus Measures Through Genetic Algorithm for Shape From Focus">

                                <b>[9]</b> Kaleem M,Mahmood M T.Combining focus measures through genetic algorithm for shape from focus[C]//2014 International Conference on Information Science &amp; Applications (ICISA),May 6-9,2014,Seoul,Korea.New York:IEEE,2014:14431736.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D shape from focus and depth map computation using steerable filters">

                                <b>[10]</b> Minhas R,Mohammed A A,Wu Q M J,<i>et al</i>.3D shape from focus and depth map computation using steerable filters[M]//Kamel M,Campilho A.International conference image analysis and recognition.Lecture notes in computer science.Berlin,Heidelberg:Springer,2009,5627:573-583.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738256&amp;v=MDMyMTduay9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViN0lJMXdSYVJRPU5pZk9mYks3SHRETnFZOUZZK2dIRA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Minhas R,Mohammed A A,Jonathan Wu Q M.Shape from focus using fast discrete curvelet transform[J].Pattern Recognition,2011,44(4):839-853.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDGC201112031&amp;v=MDY3NjllVnZGeXZuVnJ6T0lpbk1iYkc0SDlETnJZOUdaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Hu T,Liu G D,Pu Z B.Depth from focus based on zero-phase filter[J].Opto-Electronic Engineering,2011,38(12):145-150.胡涛,刘国栋,浦昭邦.基于零相位滤波的聚焦形貌恢复技术[J].光电工程,2011,38(12):145-150.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CTLL200404003&amp;v=MjczMDVyRzRIdFhNcTQ5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5dm5WcnpPSmpuSFk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Jiang Z G,Shi W H,Han D B,<i>et al</i>.Three-dimensional microscopy image system based on depth from focus[J].Computerized Tomography Theory and Applications,2004,13(4):9-15.姜志国,史文华,韩冬兵,等.基于聚焦合成的显微三维成像系统[J].CT理论与应用研究,2004,13(4):9-15.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739672&amp;v=MDIzNDdpclJkR2VycVFUTW53WmVadUh5am1VYjdJSTF3UmFSUT1OaWZPZmJLN0h0RE5xWTlGWStnR0NuczdvQk1UNlQ0UFFILw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Malik A S,Choi T S.Consideration of illumination effects and optimization of window size for accurate calculation of depth map for 3D shape recovery[J].Pattern Recognition,2007,40(1):154-170.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011400065337&amp;v=MTg5NjhhUlE9TmlmT2ZiSzdIdEROcTQ5RlpPMEtEMzgrb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYjdJSTF3Ug==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Lee I H,Shim S O,Choi T S.Improving focus measurement via variable window shape on surface radiance distribution for 3D shape reconstruction[J].Optics and Lasers in Engineering,2013,51(5):520-526.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Calculating accurate window size for shape-from-focus">

                                <b>[16]</b> Muhammad M S,Mutahira H,Choi K W,<i>et al</i>.Calculating accurate window size for shape-from-focus[C]//2014 International Conference on Information Science &amp; Applications (ICISA),May 6-9,2014,Seoul,Korea.New York:IEEE,2014:14431716.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sampling for Shape from Focus in Optical Microscopy">

                                <b>[17]</b> Muhammad M,Choi T S.Sampling for shape from focus in optical microscopy[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(3):564-573.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape from focus using multilayer feedforward neural networks">

                                <b>[18]</b> Asif M,Choi T S.Shape from focus using multilayer feedforward neural networks[J].IEEE Transactions on Image Processing,2001,10(11):1670-1675.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An efficient neural network for shape from focus with weight passing method">

                                <b>[19]</b> Kim H J,Mahmood M,Choi T S.An efficient neural network for shape from focus with weight passing method[J].Applied Sciences,2018,8(9):1648.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300297983&amp;v=MDUxNjExd1JhUlE9TmlmT2ZiSzdIdEROckk5Rlp1SUlCWFE2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYjdJSQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> Malik A S,Nisar H,Choi T S.A Fuzzy-Neural approach for estimation of depth map using focus[J].Applied Soft Computing,2011,11(2):1837-1850.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002055991&amp;v=MTA0NjI5OVNYcVJyeG94Y01IN1I3cWVidWR0RlNubFVMM05JMWc9Tmo3QmFyTzRIdEhPcjRwQWJlSU9ZM2s1ekJkaDRq&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> Subbarao M,Lu M C.Image sensing model and computer simulation for CCD camera systems[J].Machine Vision and Applications,1994,7(4):277-289.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Focusing techniques">

                                <b>[22]</b> Subbarao M,Choi T S,Nikzad A,<i>et al</i>.Focusing techniques[J].Optical Engineering,1993,32(11):2824-2837.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739901&amp;v=MDE0NjBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYjdJSTF3UmFSUT1OaWZPZmJLN0h0RE5xWTlGWStnR0JYdzRvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> Pertuz S,Puig D,Garcia M A.Analysis of focus measure operators for shape-from-focus[J].Pattern Recognition,2013,46(5):1415-1432.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD121229003889&amp;v=MDA4MzdySzZIOVBPcG85RlorTUhCUk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWnU5dUZDcmtVN3pLSlYwU05pZmNh&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b> Mutahira H,Muhammad M S,Jaffar A,<i>et al</i>.Unorthodox approach toward microscopic shape from image focus using optical microscopy[J].Microscopy Research and Technique,2013,76(1):1-6.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate Recovery of Three-Dimensional Shape from Image Focus">

                                <b>[25]</b> Subbarao M,Choi T.Accurate recovery of three-dimensional shape from image focus[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1995,17(3):266-274.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A new sense for depth of field">

                                <b>[26]</b> Pentland A P.A new sense for depth of field[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1987,PAMI-9(4):523-531.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738375&amp;v=MjQ0NTl3WmVadUh5am1VYjdJSTF3UmFSUT1OaWZPZmJLN0h0RE5xWTlGWStnSEQzczhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> Shim S O,Choi T S.A novel iterative shape from focus algorithm based on combinatorial optimization[J].Pattern Recognition,2010,43(10):3338-3347.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201911017" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201911017&amp;v=MDc2MTF2Rnl2blZyek9JalhUYkxHNEg5ak5ybzlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

