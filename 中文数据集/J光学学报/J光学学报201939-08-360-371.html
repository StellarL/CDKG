

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133848338565000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201908044%26RESULT%3d1%26SIGN%3dfBOeIVVxxT2SLVXNNZ8UqWO6XyU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201908044&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201908044&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201908044&amp;v=MDMzNjFqWFRiTEc0SDlqTXA0OUJZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5tVmIvTEk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#54" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#58" data-title="2 解析字典学习 ">2 解析字典学习</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#62" data-title="3 方  法 ">3 方  法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="&lt;b&gt;3.1 基于解析字典学习的词向量融合方法&lt;/b&gt;"><b>3.1 基于解析字典学习的词向量融合方法</b></a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;3.2 本文方法步骤&lt;/b&gt;"><b>3.2 本文方法步骤</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#119" data-title="4 实验及结果分析 ">4 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#120" data-title="&lt;b&gt;4.1 数据集及实验设置&lt;/b&gt;"><b>4.1 数据集及实验设置</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;4.2 定量实验结果及分析&lt;/b&gt;"><b>4.2 定量实验结果及分析</b></a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;4.3 定性实验结果及分析&lt;/b&gt;"><b>4.3 定性实验结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#156" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#64" data-title="图1 本文方法的整体框架图">图1 本文方法的整体框架图</a></li>
                                                <li><a href="#111" data-title="图2 本文方法运算流程图">图2 本文方法运算流程图</a></li>
                                                <li><a href="#122" data-title="图3 UCM数据集若干类的样本。">图3 UCM数据集若干类的样本。</a></li>
                                                <li><a href="#123" data-title="图4 AID数据集若干类的样本。">图4 AID数据集若干类的样本。</a></li>
                                                <li><a href="#124" data-title="图5 RSSCN7数据集类的样本。">图5 RSSCN7数据集类的样本。</a></li>
                                                <li><a href="#132" data-title="图6 不同模型词向量融合的结构对齐效果">图6 不同模型词向量融合的结构对齐效果</a></li>
                                                <li><a href="#133" data-title="图7 不同语料词向量融合的结构对齐效果图">图7 不同语料词向量融合的结构对齐效果图</a></li>
                                                <li><a href="#138" data-title="图8 UCM和AID数据集上本文方法在不同&lt;i&gt;α&lt;/i&gt;值上的OA值。">图8 UCM和AID数据集上本文方法在不同<i>α</i>值上的OA值。</a></li>
                                                <li><a href="#141" data-title="表1 不同训练模型词向量和不同训练语料词向量融合前后的OA">表1 不同训练模型词向量和不同训练语料词向量融合前后的OA</a></li>
                                                <li><a href="#142" data-title="图9 不同训练模型词向量和不同训练语料词向量的各unseen类融合效果。">图9 不同训练模型词向量和不同训练语料词向量的各unseen类融合效果。</a></li>
                                                <li><a href="#145" data-title="表2 本文方法及对比方法OA值">表2 本文方法及对比方法OA值</a></li>
                                                <li><a href="#147" data-title="图10 本文方法及对比方法的各unseen类S1+S2+S3词向量融合效果。 ">图10 本文方法及对比方法的各unseen类S1+S2+S3词向量融合效果。 </a></li>
                                                <li><a href="#150" data-title="表3 各ZSC算法在AID数据集上对S1词向量上的运算耗时">表3 各ZSC算法在AID数据集上对S1词向量上的运算耗时</a></li>
                                                <li><a href="#153" data-title="图11 测试遥感图像I的场景ZSC效果图">图11 测试遥感图像I的场景ZSC效果图</a></li>
                                                <li><a href="#155" data-title="图12 测试遥感图像II的场景ZSC效果图">图12 测试遥感图像II的场景ZSC效果图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="12">


                                    <a id="bibliography_1" title=" Chen S Z, Tian Y L.Pyramid of spatial relatons for scene-level land use classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (4) :1947-1957." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pyramid of Spatial Relatons for Scene-Level Land Use Classification">
                                        <b>[1]</b>
                                         Chen S Z, Tian Y L.Pyramid of spatial relatons for scene-level land use classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (4) :1947-1957.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_2" title=" Liu D W, Han L, Han X Y.High spatial resolution remote sensing image classification based on deep learning[J].Acta Optica Sinica, 2016, 36 (4) :0428001.刘大伟, 韩玲, 韩晓勇.基于深度学习的高分辨率遥感影像分类研究[J].光学学报, 2016, 36 (4) :0428001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604039&amp;v=MDk5OTZxQnRHRnJDVVJMT2VaZVZ1RnlubVZiL0xJalhUYkxHNEg5Zk1xNDlHYllRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Liu D W, Han L, Han X Y.High spatial resolution remote sensing image classification based on deep learning[J].Acta Optica Sinica, 2016, 36 (4) :0428001.刘大伟, 韩玲, 韩晓勇.基于深度学习的高分辨率遥感影像分类研究[J].光学学报, 2016, 36 (4) :0428001.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_3" title=" Li A X, Lu Z W, Wang L W, &lt;i&gt;et al&lt;/i&gt;.Zero-shot scene classification for high spatial resolution remote sensing images[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :4157-4167." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot scene classification for high spatial resolution remote sensing images">
                                        <b>[3]</b>
                                         Li A X, Lu Z W, Wang L W, &lt;i&gt;et al&lt;/i&gt;.Zero-shot scene classification for high spatial resolution remote sensing images[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :4157-4167.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_4" title=" Xian Y Q, Akata Z, Sharma G, &lt;i&gt;et al&lt;/i&gt;.Latent embeddings for zero-shot classification[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:69-77." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Latent embeddings for zero-shot classification">
                                        <b>[4]</b>
                                         Xian Y Q, Akata Z, Sharma G, &lt;i&gt;et al&lt;/i&gt;.Latent embeddings for zero-shot classification[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:69-77.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_5" title=" Wang D, Li Y N, Lin Y T, &lt;i&gt;et al&lt;/i&gt;.Relational knowledge transfer for zero-shot learning[C]∥Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA.California:AAAI, 2016:2145-2151." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Relational knowledge transfer for zero-shot learning">
                                        <b>[5]</b>
                                         Wang D, Li Y N, Lin Y T, &lt;i&gt;et al&lt;/i&gt;.Relational knowledge transfer for zero-shot learning[C]∥Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA.California:AAAI, 2016:2145-2151.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_6" title=" Zhang Z M, Saligrama V.Zero-shot learning via joint latent similarity embedding[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:6034-6042." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning via joint latent similarity embedding">
                                        <b>[6]</b>
                                         Zhang Z M, Saligrama V.Zero-shot learning via joint latent similarity embedding[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:6034-6042.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_7" title=" Zhang Z M, Saligrama V.Zero-shot learning via semantic similarity embedding[C]∥2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile, USA.New York:IEEE, 2015:4166-4174." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning via semantic similarity embedding">
                                        <b>[7]</b>
                                         Zhang Z M, Saligrama V.Zero-shot learning via semantic similarity embedding[C]∥2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile, USA.New York:IEEE, 2015:4166-4174.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_8" title=" Wang Q, Chen K.Zero-shot visual recognition via bidirectional latent embedding[J].International Journal of Computer Vision, 2017, 124 (3) :356-383." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDD31F20253A10A4B4D5AE16DA35AA17A9&amp;v=MTc1ODk1b09EQTA5dlJKbjcwNElTWG1XM1JFd0NNT1ZRc3VXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGhoeGJ1NXdLcz1OajdCYXNlN0g2Zk9yNDFBWg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Wang Q, Chen K.Zero-shot visual recognition via bidirectional latent embedding[J].International Journal of Computer Vision, 2017, 124 (3) :356-383.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_9" title=" Li Y N, Wang D H, Hu H H, &lt;i&gt;et al&lt;/i&gt;.Zero-shot recognition using dual visual-semantic mapping paths[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:3279-3287." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot recognition using dual visual-semantic mapping paths">
                                        <b>[9]</b>
                                         Li Y N, Wang D H, Hu H H, &lt;i&gt;et al&lt;/i&gt;.Zero-shot recognition using dual visual-semantic mapping paths[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:3279-3287.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_10" title=" Zhao B, Wu B T, Wu T F, &lt;i&gt;et al&lt;/i&gt;.Zero-shot learning posed as a missing data problem[C]∥2017 IEEE International Conference on Computer Vision Workshops (ICCVW) , October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:2616-2622." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning posed as a missing data problem">
                                        <b>[10]</b>
                                         Zhao B, Wu B T, Wu T F, &lt;i&gt;et al&lt;/i&gt;.Zero-shot learning posed as a missing data problem[C]∥2017 IEEE International Conference on Computer Vision Workshops (ICCVW) , October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:2616-2622.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_11" title=" Lampert C H, Nickisch H, Harmeling S.Attribute-based classification for zero-shot visual object categorization[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014, 36 (3) :453-465." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attribute-Based Classification for Zero-Shot Visual Object Categorization">
                                        <b>[11]</b>
                                         Lampert C H, Nickisch H, Harmeling S.Attribute-based classification for zero-shot visual object categorization[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014, 36 (3) :453-465.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_12" title=" Socher R, Ganjoo M, Manning C D, &lt;i&gt;et al&lt;/i&gt;.Zero-shot learning through cross-modal transfer[C]∥26th International Conference on Neural Information Processing Systems, December 5-10, 2013, Lake Tahoe, Nevada.[S.l.:s.n.], 2013:935-943." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-Shot Learning through Cross-Modal Transfer">
                                        <b>[12]</b>
                                         Socher R, Ganjoo M, Manning C D, &lt;i&gt;et al&lt;/i&gt;.Zero-shot learning through cross-modal transfer[C]∥26th International Conference on Neural Information Processing Systems, December 5-10, 2013, Lake Tahoe, Nevada.[S.l.:s.n.], 2013:935-943.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_13" title=" Mikolov T, Chen K, Corrado G, &lt;i&gt;et al&lt;/i&gt;.Efficient estimation of word representations in vector space[J/OL]. (2013-09-07) [2019-03-01].https://arxiv.org/abs/1301.3781." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient Estimation of Word Representations in Vector Space">
                                        <b>[13]</b>
                                         Mikolov T, Chen K, Corrado G, &lt;i&gt;et al&lt;/i&gt;.Efficient estimation of word representations in vector space[J/OL]. (2013-09-07) [2019-03-01].https://arxiv.org/abs/1301.3781.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_14" title=" Pennington J, Socher R, Manning C.Glove:global vectors for word representation[C]∥Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , 2014, Doha, Qatar.[S.l.:s.n.], 2014:1532-1543." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Glove:Global Vectors for Word Representation">
                                        <b>[14]</b>
                                         Pennington J, Socher R, Manning C.Glove:global vectors for word representation[C]∥Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , 2014, Doha, Qatar.[S.l.:s.n.], 2014:1532-1543.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_15" title=" Yang M, Chang H Y, Luo W X.Discriminative analysis-synthesis dictionary learning for image classification[J].Neurocomputing, 2017, 219:404-411." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative analysis-synthesis dictionary learning for image classification">
                                        <b>[15]</b>
                                         Yang M, Chang H Y, Luo W X.Discriminative analysis-synthesis dictionary learning for image classification[J].Neurocomputing, 2017, 219:404-411.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_16" title=" Wang J J, Guo Y Q, Guo J, &lt;i&gt;et al&lt;/i&gt;.Synthesis linear classifier based analysis dictionary learning for pattern classification[J].Neurocomputing, 2017, 238:103-113." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES579AE40DE8831392A28EB8D29F9C9B3D&amp;v=MzE1MzZRbGZDcGJRMzVkaGh4YnU1d0tzPU5pZk9mYmEvRjZDNXE0OHhFZU1IRDMwNnhoUmk2RGNJT25lV3JodERjTUdkTjduckNPTnZGU2lXV3I3SklGcG1hQnVIWWZPRw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         Wang J J, Guo Y Q, Guo J, &lt;i&gt;et al&lt;/i&gt;.Synthesis linear classifier based analysis dictionary learning for pattern classification[J].Neurocomputing, 2017, 238:103-113.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_17" title=" Ravishankar S, Bresler Y.Sparsifying transform learning with efficient optimal updates and convergence guarantees[J].IEEE Transactions on Signal Processing, 2015, 63 (9) :2389-2404." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=L0 sparsifying transform learning with efficient optimal updates and convergence guarantees">
                                        <b>[17]</b>
                                         Ravishankar S, Bresler Y.Sparsifying transform learning with efficient optimal updates and convergence guarantees[J].IEEE Transactions on Signal Processing, 2015, 63 (9) :2389-2404.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_18" title=" Yang Y, Newsam S.Bag-of-visual-words and spatial extensions for land-use classification[C]∥Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems-GIS′10, November 2-5, 2010, San Jose, California, USA.New York:IEEE, 2010:270-279." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bag of visual words and spatial extensions for land-use classification">
                                        <b>[18]</b>
                                         Yang Y, Newsam S.Bag-of-visual-words and spatial extensions for land-use classification[C]∥Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems-GIS′10, November 2-5, 2010, San Jose, California, USA.New York:IEEE, 2010:270-279.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_19" title=" Xia G S, Hu J W, Hu F, &lt;i&gt;et al&lt;/i&gt;.AID:a benchmark data set for performance evaluation of aerial scene classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :3965-3981." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=AID:ABenchmark Data Set for Performance Evaluation of Aerial Scene Classification">
                                        <b>[19]</b>
                                         Xia G S, Hu J W, Hu F, &lt;i&gt;et al&lt;/i&gt;.AID:a benchmark data set for performance evaluation of aerial scene classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :3965-3981.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_20" title=" Zou Q, Ni L H, Zhang T, &lt;i&gt;et al&lt;/i&gt;.Deep learning based feature selection for remote sensing scene classification[J].IEEE Geoscience and Remote Sensing Letters, 2015, 12 (11) :2321-2325." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Learning Based Feature Selection for Remote Sensing Scene Classification">
                                        <b>[20]</b>
                                         Zou Q, Ni L H, Zhang T, &lt;i&gt;et al&lt;/i&gt;.Deep learning based feature selection for remote sensing scene classification[J].IEEE Geoscience and Remote Sensing Letters, 2015, 12 (11) :2321-2325.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_21" title=" Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J/OL]. (2015-04-10) [2019-03-01].https://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[21]</b>
                                         Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J/OL]. (2015-04-10) [2019-03-01].https://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-16 17:45</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(08),360-371 DOI:10.3788/AOS201939.0828002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于词向量一致性融合的遥感场景零样本分类方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E6%99%A8&amp;code=41275593&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴晨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E5%85%89&amp;code=22174856&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">于光</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%87%A4%E6%99%B6&amp;code=22473546&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张凤晶</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%AE%87&amp;code=20481306&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A2%81%E6%98%B1%E7%BA%AC&amp;code=42150725&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">袁昱纬</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%85%A8%E5%90%89%E6%88%90&amp;code=20534592&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全吉成</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%B7%E5%86%9B%E8%88%AA%E7%A9%BA%E5%A4%A7%E5%AD%A6&amp;code=1701745&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">海军航空大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A9%BA%E5%86%9B%E8%88%AA%E7%A9%BA%E5%A4%A7%E5%AD%A6&amp;code=1020414&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空军航空大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=91977%E9%83%A8%E9%98%9F&amp;code=1743710&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">91977部队</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>遥感场景类别的语义词向量与图像特征原型的距离结构不一致问题, 严重影响遥感场景零样本分类效果。针对该问题, 利用不同词向量间一致性, 提出一种基于解析字典学习的语义词向量融合方法, 以提升遥感场景零样本分类效果。首先, 采用解析字典学习方法, 提取场景类别的不同词向量的公共稀疏系数, 并作为融合后的语义词向量;然后, 同样采用解析字典学习方法, 将场景类别的图像特征原型嵌入到融合后的词向量空间, 与融合后的词向量进行结构对齐, 降低距离结构的不一致性;最后, 通过联合优化获得未知类的图像特征空间类别原型表示, 并采用最近邻分类器完成未知类别遥感场景的分类。在3种遥感场景数据集和多种语义词向量上进行定量和定性实验。实验结果表明, 通过词向量融合可以获得与图像特征原型结构更一致的语义词向量, 从而显著提升遥感场景零样本分类的准确度。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%A5%E6%84%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遥感;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%BA%E6%99%AF%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">场景分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">零样本分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%93%E6%9E%84%E5%AF%B9%E9%BD%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">结构对齐;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%8D%E5%90%91%E9%87%8F%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">词向量融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%A3%E6%9E%90%E5%AD%97%E5%85%B8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">解析字典学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *于光 E-mail:1471612866@qq.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61301233);</span>
                    </p>
            </div>
                    <h1><b>Zero-Shot Classification Method for Remote-Sensing Scenes Based on Word Vector Consistent Fusion</b></h1>
                    <h2>
                    <span>Wu Chen</span>
                    <span>Yu Guang</span>
                    <span>Zhang Fengjing</span>
                    <span>Liu Yu</span>
                    <span>Yuan Yuwei</span>
                    <span>Quan Jicheng</span>
            </h2>
                    <h2>
                    <span>Naval Aviation University</span>
                    <span>Aviation University of Air Force</span>
                    <span>The 91977 of PLA</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The problem of distance structure difference between the word vectors and visual prototypes of remote-sensing scene classification seriously influences the performance of the zero-shot scene classification. Herein, a fusion method based on analytical dictionary learning is proposed to exploit the consistency among the different kinds of word vectors for the performance improvement of the zero-shot scene classification. Firstly, the common sparse coefficients of different kinds of word vectors of scene classification are extracted by analytical dictionary learning method and acted as the fused word vector. Secondly, the visual prototypes are embedded into and structure-aligned with the fused word vector by analytical dictionary learning method similarly, to reduce the distance structure inconsistency. Finally, the prototypes of the unseen classes in the image feature space are obtained via joint optimization, and the nearest neighbor classifier is used to complete the classification of remote-sensing scenes from the unseen classes. Quantitative and qualitative experiments are also conducted on three remote-sensing scene datasets with the fusion of various word vectors. The experimental results show that the fused word vector is more structure-consistent with the prototypes in the image feature space, and the zero-shot classification accuracies of the remote-sensing scenes can be significantly improved.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=remote%20sensing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">remote sensing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=scenes%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">scenes classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=zero-shot%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">zero-shot classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=structure%20alignment&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">structure alignment;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=word%20vector%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">word vector fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=analytical%20dictionary%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">analytical dictionary learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-15</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="54" name="54" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="55">传统的遥感图像分类方法主要在“像素”和“对象”层面进行, 针对的是空间分辨率不高的遥感图像分类任务, 然而近年来随着遥感图像空间分辨率不断提升, 这些方法越来越难以满足实际需要。场景分类作为高分辨率遥感图像快速分析与信息提取的重要手段, 近10年来受到广泛关注<citation id="159" type="reference"><link href="12" rel="bibliography" /><link href="14" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。这里的“场景”是指具有清晰类别语义的遥感图像块, 以其作为遥感图像分类的基本单元, 使场景分类能够适应大规模遥感图像快速分析的需要。然而, 目前的场景分类方法属于监督分类, 无法将识别能力灵活扩展到新类别场景, 因此阻碍了遥感场景分类研究的进一步发展。为解决现有场景分类方法的迁移识别能力不足问题, Li等<citation id="158" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出了遥感图像零样本场景分类方法, 即将场景分类与零样本学习方法结合, 提高对新类别场景的迁移识别能力。</p>
                </div>
                <div class="p1">
                    <p id="56">零样本分类 (ZSC) 是一种特殊的无监督分类方法, 其基本原理是:以类别名称的语义词向量为桥梁, 通过迁移由已知 (seen) 类别标注样本学习得到识别模型, 获得对新的 (unseen) 类别的识别能力。由于ZSC方法能够在不标注unseen类样本情况下, 获得对其的识别能力, 因此近年来受到广泛关注<citation id="169" type="reference"><link href="18" rel="bibliography" /><link href="20" rel="bibliography" /><link href="22" rel="bibliography" /><link href="24" rel="bibliography" /><link href="26" rel="bibliography" /><link href="28" rel="bibliography" /><link href="30" rel="bibliography" /><link href="32" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>。为进行细粒度的ZSC, Xian等<citation id="160" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>在兼容函数学习过程中引入隐式变量模型, 从而提出隐式嵌入方法 (LatEm) 。针对映射函数的泛化能力不足问题, Wang等<citation id="161" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出的关系知识迁移 (RKT) 方法通过语义映射方法还原unseen类别的流形结构。Zhang等<citation id="162" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出的联合隐式相似性嵌入 (JLSE) 方法将样本特征和对应的语义嵌入表示作为输入, 通过建立两者之间的相似性度, 实现对unseen类别样本的ZSC。Zhang等<citation id="163" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出的语义相似性嵌入 (SSE) 方法将源域或目标域数据视为训练类组合, 并将其映射到同一语义空间中。Wang等<citation id="164" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出的双向隐式嵌入 (BiDiLEL) 方法利用流形保持原理, 将图像特征和语义特征分别映射到第三方的公共空间。Li等<citation id="165" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出的双视觉语义映射 (DMaP) 方法利用语义空间流形和视觉语义映射迁移能力之间的关系, 修正了语义词向量。为估计unseen样本特征分布特点以提升ZSC效果, Zhao等<citation id="166" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出利用直推式框架 (MDP) 。除语义词向量外, 人工标注的类别属性向量也可用于ZSC研究中, 如Lampert等<citation id="167" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出的基于类别属性向量表示的零样本分类方法, 但是由于类别属性向量的标注成本较大且扩展性较弱, 近年来用到ZSC的研究越来越少。语义词向量<citation id="168" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>是采用自然语言训练模型, 在大规模文本语料集上, 通过无监督学习得到的实体单词高维向量表示。在ZSC中, 采用类别名称的语义词向量, 提供类别间距离结构关系, 来辅助推断图像特征空间unseen类别的原型表示。因此, 语义词向量能否反映图像特征空间的类间距离结构关系, 是ZSC方法的关键。现有ZSC方法针对均是某一领域内的细粒度类别的分类任务, 然而, 由于遥感场景类别涉及不同领域, 词向量需要反映场景类别间的距离关系。单种语义词向量受训练语料、训练模型限制, 难以满足多领域的遥感场景类别的情形。</p>
                </div>
                <div class="p1">
                    <p id="57">近几年, 随着自然语言处理技术的进步, 已能便捷获取不同训练模型 (如Word2Vector<citation id="170" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、Glove<citation id="171" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation> 等) 和不同训练语料 (如Wikipedia、Common Crawl等) 的语义词向量。这些语义词向量具有一定的一致性, 通过融合可获得与图像特征空间场景类别距离结构更一致的语义词向量, 从而提升遥感场景ZSC准确度。为利用不同语义词向量间的一致性, 本文提出一种基于词向量一致性融合的遥感场景ZSC方法。首先, 采用解析字典学习方法, 获取各语义词向量的稀疏系数;其次, 将各词向量的公共稀疏系数作为融合后的语义词向量表示;然后, 再采用解析字典方法, 将seen类图像特征原型表示嵌入到融合后的语义词向量空间, 与其中的seen类融合语义词向量进行结构对齐, 提升模型到unseen类的迁移效果;最后, 在图像特征空间以学习得到的unseen类原型表示为中心, 采用最近邻分类器对unseen类场景样本进行分类。</p>
                </div>
                <h3 id="58" name="58" class="anchor-tag">2 解析字典学习</h3>
                <div class="p1">
                    <p id="59">字典学习方法分为两类, 即合成性字典学习 (SDL) 和解析性字典学习 (ADL) 。SDL认为输入特征可以由字典和相应稀疏系数重建得到, 而ADL则将字典应用到输入特征上, 获得特征的稀疏系数。虽然SDL方法应用广泛, 但其计算效率不高。而ADL通常具有闭式解, 编码能力良好, 计算效率较高<citation id="172" type="reference"><link href="40" rel="bibliography" /><link href="42" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>。ADL的基本公式为</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>arg</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ω</mi><mo>, </mo><mi mathvariant="bold-italic">Ζ</mi></mrow></munder><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><mi mathvariant="bold-italic">X</mi></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>, </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mspace width="0.25em" /><mi mathvariant="bold-italic">Ω</mi><mo>∈</mo><mi mathvariant="bold-italic">Γ</mi><mo>, </mo><mspace width="0.25em" /><mrow><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow><msub><mrow></mrow><mn>0</mn></msub><mo>≤</mo><mi>Τ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61">式中:<b><i>X</i></b>=[<b><i>x</i></b><sub>1</sub>, …, <b><i>x</i></b><sub><i>n</i></sub>]∈ℝ<sup><i>m</i>×<i>n</i></sup>为<i>n</i>个输入样本组成的特征矩阵, <b><i>x</i></b><sub><i>i</i></sub>∈ℝ<sup><i>m</i></sup>为第<i>i</i>个样本;<b><i>Z</i></b>为<b><i>X</i></b>的稀疏系数, 其样本稀疏性采用<i>l</i><sub>0</sub>范数及参数<i>T</i><sub>0</sub>实现;<i>Ω</i>为解析字典;<i>Γ</i>是为避免出现平凡解而对<i>Ω</i>的log-det限制条件<citation id="173" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。</p>
                </div>
                <h3 id="62" name="62" class="anchor-tag">3 方  法</h3>
                <div class="p1">
                    <p id="63">采用ADL方法获得各词向量的稀疏系数, 并将公共的稀疏系数作为融合词向量表示, 与图像特征空间类别原型结构对齐。首先, 由于词向量中存在冗余信息, 影响类间距离结构信息表达, 需要对其进行稀疏编码处理, 以减少冗余信息, 突出类间距离结构信息。而解析字典学习方法具有优越的稀疏编码能力, 因此本文采用解析字典学习方法, 建立稀疏编码项, 获取各语义词向量的稀疏系数。其次, 为获取不同词向量的一致性, 将各词向量的公共稀疏系数作为融合后的语义词向量表示。然后, 融合后的词向量空间与场景图像特征空间来源不同, 再加上遥感场景类别涉及不同领域 (人类生产生活以及自然地貌) , 导致了两种空间中的场景类间距离存在较大差异, 降低了对unseen类的迁移效果。因此需要对这种空间差异性进行建模, 而ADL方法具有较强的稀疏编码能力, 能够将场景图像特征嵌入到稀疏的融合后语义词向量空间, 从而与其中的seen类场景图像特征对齐。最后, 通过对seen和unseen类上的目标函数进行联合迭代计算, 获得unseen类图像特征原型表示, 进而采用最近邻分类器完成对unseen样本的分类。本文方法的整体框架如图1所示。</p>
                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908044_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文方法的整体框架图" src="Detail/GetImg?filename=images/GXXB201908044_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文方法的整体框架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908044_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Whole framework of proposed method</p>

                </div>
                <h4 class="anchor-tag" id="65" name="65"><b>3.1 基于解析字典学习的词向量融合方法</b></h4>
                <div class="p1">
                    <p id="66">词向量融合的目标函数可表示为</p>
                </div>
                <div class="area_img" id="67">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201908044_06700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="68">式中:<i>ζ</i><sub>s</sub>、<i>ζ</i><sub>u</sub>分别为seen和unseen类词向量融合的目标函数;<i>ζ</i><sub>s</sub>和<i>ζ</i><sub>u</sub>的第一项为稀疏编码项, 旨在提取各词向量的一致性稀疏系数作为新的融合词向量表示, 第二项为结构对齐项, 将融合语义词向量表示与图像特征空间场景类别原型进行结构对齐, <i>ζ</i><sub>s</sub>的第三项为seen类图像特征原型学习项, 旨在学习seen类场景的类别原型表示;<b><i>C</i></b><sup>s</sup><sub><i>i</i></sub>∈ℝ<sup><i>d</i><sub><i>i</i></sub>×<i>c</i><sub>s</sub></sup>为seen类的第<i>i</i>种词向量 (共<i>M</i>种不同词向量) 矩阵, <b><i>C</i></b><sup>u</sup><sub><i>i</i></sub>∈ℝ<sup><i>d</i><sub><i>i</i></sub>×<i>c</i><sub>u</sub></sup>为unseen类的第<i>i</i>种词向量矩阵, <i>d</i><sub><i>i</i></sub>为第<i>i</i>种词向量维度, <i>c</i><sub>s</sub>为seen类数, <i>c</i><sub>u</sub>为unseen类数;<i>Ω</i><sub><i>i</i></sub>∈ℝ<sup><i>d</i><sub><i>i</i></sub>×<i>d</i><sub><i>i</i></sub></sup>为第<i>i</i>种词向量空间对应的解析字典。通过融合不同词向量, 获得不同词向量的一致性表示, 并与图像特征空间场景类别原型进行对齐, 从而计算得到unseen类的图像特征原型表示, 最后进行最近邻分类。seen类不同词向量的一致性稀疏系数<b><i>Z</i></b><sup>s</sup>∈ℝ<sup><i>d</i>×<i>c</i><sub>s</sub></sup>为融合后seen类别词向量表示, unseen类不同词向量的一致性稀疏系数<b><i>Z</i></b><sup>u</sup>∈ℝ<sup><i>d</i>×<i>c</i><sub>u</sub></sup>为融合后unseen类别词向量表示, 其中<i>d</i>为融合词向量的维数。<b><i>P</i></b><sup>s</sup>∈ℝ<sup><i>q</i>×<i>c</i><sub>s</sub></sup>为seen类场景在图像特征空间中的原型, <b><i>P</i></b><sup>u</sup>∈ℝ<sup><i>q</i>×<i>c</i><sub>u</sub></sup>为unseen类场景在图像特征空间中的原型, <i>q</i>为图像特征维数;<b><i>H</i></b><sub>s</sub>∈ℝ<sup><i>N</i><sub>s</sub>×<i>c</i><sub>s</sub></sup>为seen样本的类别标签矩阵, 其中的行向量表示seen样本的类别标签one-hot向量。<b><i>X</i></b><sup>s</sup>∈ℝ<sup><i>q</i>×<i>N</i><sub>s</sub></sup>为图像特征空间中seen样本的特征矩阵。由于图像特征空间中的类内样本分布结构复杂, 简单地以样本均值中心作为类别的原型, 没有充分利用seen类别样本的信息。因此<mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>s</mtext></msup><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msup><mrow></mrow><mtext>s</mtext></msup></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mrow></math></mathml>主要作用是通过建立<b><i>P</i></b><sup>s</sup>与<b><i>X</i></b><sup>s</sup>间的对应关系, 以更灵活地学习<b><i>P</i></b><sup>s</sup>, 而不是仅仅以样本均值作为seen类别原型表示。<b><i>D</i></b>∈ℝ<sup><i>d</i>×<i>q</i></sup>为图像特征空间的解析字典, 其主要作用是将<b><i>P</i></b><sup>s</sup>和<b><i>P</i></b><sup>u</sup>嵌入到融合后的语义词向量空间, 与融合后的语义词向量<b><i>Z</i></b><sup>s</sup>和<b><i>Z</i></b><sup>u</sup>对齐。</p>
                </div>
                <div class="p1">
                    <p id="70"> (2) 式对目标变量<i>Ω</i><sub><i>i</i></sub>、<b><i>D</i></b>、<b><i>Z</i></b><sup>s</sup>、<b><i>Z</i></b><sup>u</sup>、<b><i>P</i></b><sup>s</sup>和<b><i>P</i></b><sup>u</sup>同时非凸, 难以直接求解, 但可采用逐个循环方式进行求解。由于<b><i>D</i></b>的求解依赖于<b><i>Z</i></b><sup>s</sup>和<b><i>Z</i></b><sup>u</sup>, 而<b><i>Z</i></b><sup>s</sup>和<b><i>Z</i></b><sup>u</sup>一般可初始化为one-hot向量矩阵, 因此循环求解过程中, 最先求解<b><i>D</i></b>, 其次<i>Ω</i><sub><i>i</i></sub>, 然后<b><i>Z</i></b><sup>s</sup>和<b><i>Z</i></b><sup>u</sup>, 最后优化<b><i>P</i></b><sup>s</sup>和<b><i>P</i></b><sup>u</sup>。而<b><i>P</i></b><sup>s</sup>初始化为各seen类别样本的均值, 其中涉及到的unseen类原型<b><i>P</i></b><sup>u</sup>在第一次迭代时未知, 因此需要对其赋予初始值, 本文采用高斯分布对<b><i>P</i></b><sup>u</sup>进行随机初始化。具体步骤如下。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71">1) 固定<i>Ω</i><sub><i>i</i></sub>、<b><i>Z</i></b><sup>s</sup>、<b><i>Z</i></b><sup>u</sup>、<b><i>P</i></b><sup>s</sup>、<b><i>P</i></b><sup>u</sup>, 更新<b><i>D</i></b></h4>
                <div class="p1">
                    <p id="72">此时的总体目标函数为</p>
                </div>
                <div class="area_img" id="73">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201908044_07300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="74">由于<b><i>D</i></b>∈ℝ<sup><i>d</i>×<i>q</i></sup>的行列数不相等, 因此需采用正则项<i>R</i> (<b><i>D</i></b>) :</p>
                </div>
                <div class="area_img" id="75">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201908044_07500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="76">这里记[<b><i>Z</i></b><sup>s</sup>, <b><i>Z</i></b><sup>u</sup>]为<b><i>Z</i></b>, 记[<b><i>P</i></b><sup>s</sup>, <b><i>P</i></b><sup>u</sup>]为<b><i>P</i></b>。因此, 更新<b><i>D</i></b>的目标函数为</p>
                </div>
                <div class="area_img" id="77">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201908044_07700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="78">式中:<i>α</i>&gt;0为正则项<i>R</i> (<b><i>D</i></b>) 的重要性系数。然而, (5) 式仍然对字典<b><i>D</i></b>难以直接求解, 本文采用梯度下降方法进行求解<citation id="174" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。其中<image id="79" type="formula" href="images/GXXB201908044_07900.jpg" display="inline" placement="inline"><alt></alt></image>和<i>R</i> (<b><i>D</i></b>) 对字典<b><i>D</i></b>的梯度分别为<image id="80" type="formula" href="images/GXXB201908044_08000.jpg" display="inline" placement="inline"><alt></alt></image><image id="80" type="formula" href="images/GXXB201908044_08001.jpg" display="inline" placement="inline"><alt></alt></image>、∇<sub><b><i>D</i></b></sub>[<i>R</i> (<b><i>D</i></b>) ]=-2<b><i>D</i></b><sup>†</sup>。<b><i>D</i></b><sup>†</sup>为字典<b><i>D</i></b>的伪逆矩阵。因此, 具体的梯度下降公式为</p>
                </div>
                <div class="area_img" id="81">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201908044_08100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="82">式中:超参数<i>η</i>为梯度下降速率。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83">2) 固定<b><i>D</i></b>、<b><i>Z</i></b><sup>s</sup>、<b><i>Z</i></b><sup>u</sup>、<b><i>P</i></b><sup>s</sup>、<b><i>P</i></b><sup>u</sup>, 更新<i>Ω</i><sub><i>i</i></sub></h4>
                <div class="p1">
                    <p id="84">此时的目标函数为</p>
                </div>
                <div class="area_img" id="85">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201908044_08500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="86"> (7) 式的求解步骤与 (3) 式相同。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87">3) 固定<i>Ω</i><sub><i>i</i></sub>、<b><i>P</i></b><sup>u</sup>、<b><i>D</i></b>、<b><i>P</i></b><sup>s</sup>, 更新<b><i>Z</i></b><sup>s</sup>、<b><i>Z</i></b><sup>u</sup></h4>
                <div class="p1">
                    <p id="88">此时关于<b><i>Z</i></b><sup>s</sup>的目标函数为</p>
                </div>
                <div class="area_img" id="89">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201908044_08900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="90">对<b><i>Z</i></b><sup>s</sup>求导并置0, 可得<b><i>Z</i></b><sup>s</sup>=<mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi mathvariant="bold-italic">Ω</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mi>i</mi><mtext>s</mtext></msubsup><mo>+</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">Ρ</mi><msup><mrow></mrow><mtext>s</mtext></msup></mrow><mo>) </mo></mrow><mo>/</mo><mrow><mo stretchy="false"> (</mo><mi>Μ</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="92">此时关于<b><i>Z</i></b><sup>u</sup>的目标函数为</p>
                </div>
                <div class="area_img" id="93">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201908044_09300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="94">同理, 对<b><i>Z</i></b><sup>u</sup>求导并置<b>0</b>, 可得<b><i>Z</i></b><sup>u</sup>=<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi mathvariant="bold-italic">Ω</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mi>i</mi><mtext>u</mtext></msubsup><mo>+</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">Ρ</mi><msup><mrow></mrow><mtext>u</mtext></msup></mrow><mo>) </mo></mrow><mo>/</mo><mo stretchy="false"> (</mo><mi>Μ</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>。按照比例<i>T</i><sub>0</sub>保留幅值较大的前若干个元素且其余元素置0的方式稀疏化<b><i>Z</i></b><sup>s</sup>和<b><i>Z</i></b><sup>u</sup>中的列向量。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">4) 固定<i>Ω</i><sub><i>i</i></sub>、<b><i>Z</i></b><sup>s</sup>、<b><i>Z</i></b><sup>u</sup>、<b><i>D</i></b>、<b><i>P</i></b><sup>u</sup>, 更新<b><i>P</i></b><sup>s</sup></h4>
                <div class="p1">
                    <p id="97">此时的总体目标函数为</p>
                </div>
                <div class="area_img" id="98">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201908044_09800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="99">对<b><i>P</i></b><sup>s</sup>求导并置<b>0</b>, 可得<b><i>P</i></b><sup>s</sup>= (<b><i>D</i></b><sup>T</sup><b><i>D</i></b>+<i>I</i>) <sup>-1</sup> (<b><i>D</i></b><sup>T</sup><b><i>Z</i></b><sup>s</sup>+<b><i>X</i></b><sup>s</sup><b><i>H</i></b><sub>s</sub>) 。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100">5) 固定<i>Ω</i><sub><i>i</i></sub>、<b><i>Z</i></b><sup>s</sup>、<b><i>Z</i></b><sup>u</sup>、<b><i>D</i></b>、<b><i>P</i></b><sup>s</sup>, 更新<b><i>P</i></b><sup>u</sup></h4>
                <div class="p1">
                    <p id="101">此时的总体目标函数为</p>
                </div>
                <div class="area_img" id="102">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201908044_10200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="103">对<b><i>P</i></b><sup>u</sup>求导并置0, 可得<b><i>P</i></b><sup>u</sup>= (<b><i>D</i></b><sup>T</sup><b><i>D</i></b>) <sup>-1</sup><b><i>D</i></b><sup>T</sup><b><i>Z</i></b><sup>u</sup>。</p>
                </div>
                <div class="p1">
                    <p id="104">迭代循环结束后, 在图像特征空间中, 以学到的unseen类原型表示<b><i>P</i></b><sup>u</sup>, 作为最近邻分类器的中心, 对unseen类样本进行分类。</p>
                </div>
                <h4 class="anchor-tag" id="105" name="105"><b>3.2 本文方法步骤</b></h4>
                <div class="p1">
                    <p id="106">本文基于词向量一致性融合的遥感场景ZSC方法的计算流程如图2所示, 具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="107">输入:seen类场景图像特征<b><i>X</i></b><sup>s</sup>, <i>M</i>种不同的词向量 (其中seen类词向量<b><i>C</i></b><sup>s</sup><sub><i>i</i></sub>, unseen类词向量<b><i>C</i></b><sup>u</sup><sub><i>i</i></sub>, <i>i</i>=1, 2, …, <i>M</i>) , unseen类场景图像特征矩阵<b><i>X</i></b><sup>u</sup>=[<b><i>x</i></b><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mtext>u</mtext></msubsup></mrow></math></mathml>, …, <b><i>x</i></b><sup>u</sup><sub><i>N</i><sub>u</sub></sub>]∈ℝ<sup><i>q</i>×<i>N</i><sub>u</sub></sup>, <i>N</i><sub><i>u</i></sub>为unseen类场景图像个数, 最大迭代次数Iter_<i>N</i>。</p>
                </div>
                <div class="p1">
                    <p id="109">输出:对<b><i>X</i></b><sup>u</sup>中样本推断类别标签。</p>
                </div>
                <div class="p1">
                    <p id="110">步骤1:初始化<b><i>Z</i></b><sup>s</sup>和<b><i>Z</i></b><sup>u</sup>为one-hot向量矩阵, 初始化<b><i>P</i></b><sup>s</sup>为各seen类别样本的均值, 并采用高斯分布对<b><i>P</i></b><sup>u</sup>进行随机初始化;</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908044_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 本文方法运算流程图" src="Detail/GetImg?filename=images/GXXB201908044_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 本文方法运算流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908044_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Operational flow chart of proposed method</p>

                </div>
                <div class="p1">
                    <p id="112">步骤2:根据 (3) 式更新<b><i>D</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="113">步骤3:根据 (7) 式更新<i>Ω</i><sub><i>i</i></sub>;</p>
                </div>
                <div class="p1">
                    <p id="114">步骤4:根据 (8) 和 (9) 式分别更新<b><i>Z</i></b><sup>s</sup>、<b><i>Z</i></b><sup>u</sup>;</p>
                </div>
                <div class="p1">
                    <p id="115">步骤5:根据 (10) 式更新<b><i>P</i></b><sup>s</sup>= (<b><i>D</i></b><sup>T</sup><b><i>D</i></b>+<i>I</i>) <sup>-1</sup>× (<b><i>D</i></b><sup>T</sup><b><i>Z</i></b><sup>s</sup>+<b><i>X</i></b><sup>s</sup><b><i>H</i></b><sub>s</sub>) ;</p>
                </div>
                <div class="p1">
                    <p id="116">步骤6:根据 (11) 式更新<b><i>P</i></b><sup>u</sup>= (<b><i>D</i></b><sup>T</sup><b><i>D</i></b>) <sup>-1</sup><b><i>D</i></b><sup>T</sup><b><i>Z</i></b><sup>u</sup>;</p>
                </div>
                <div class="p1">
                    <p id="117">步骤7:判断是否达到最大循环次数Iter_<i>N</i>。若是, 则执行步骤8;若否, 则循环执行步骤2～7;</p>
                </div>
                <div class="p1">
                    <p id="118">步骤8:在图像特征空间中, 以<b><i>P</i></b><sup>u</sup>作为最近邻分类器的中心, 推断unseen类场景<b><i>X</i></b><sup>u</sup>的类别标签。</p>
                </div>
                <h3 id="119" name="119" class="anchor-tag">4 实验及结果分析</h3>
                <h4 class="anchor-tag" id="120" name="120"><b>4.1 数据集及实验设置</b></h4>
                <div class="p1">
                    <p id="121">实验采用3种遥感场景数据集:UC-Merced (UCM) 数据集<citation id="175" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、航空图像数据集 (AID) <citation id="176" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>以及RSSCN7数据集<citation id="177" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。其中, UCM和AID用于定量实验, RSSCN7用于定性实验, 即作为seen样本, 以测试遥感图像上unseen类场景的ZSC效果。UCM有21类场景, 共2100张图像, 图像大小为256 pixel×256 pixel, 若干样本如图3所示;AID共有30类, 共10000张场景图像, 图像大小为600 pixel×600 pixel, 若干样本如图4所示。RSSCN7共2800张遥感场景图像, 分为7个类别, 图像大小为400 pixel×400 pixel, 其样本如图5所示。</p>
                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908044_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 UCM数据集若干类的样本。" src="Detail/GetImg?filename=images/GXXB201908044_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 UCM数据集若干类的样本。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908044_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Images of several classes from UCM dataset.</p>
                                <p class="img_note"> (a) 农田; (b) 飞机; (c) 棒球场; (d) 密集住宅; (e) 高速公路; (f) 海港; (g) 储罐; (h) 网球场; (i) 立交桥; (j) 高尔夫球场</p>
                                <p class="img_note"> (a) Agricultural; (b) airplane; (c) baseball diamond; (d) dense residential; (e) freeway; (f) harbor; (g) storage tanks; (h) tennis court; (i) overpass; (j) golf course</p>

                </div>
                <div class="area_img" id="123">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908044_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 AID数据集若干类的样本。" src="Detail/GetImg?filename=images/GXXB201908044_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 AID数据集若干类的样本。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908044_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Images of several classes from AID dataset.</p>
                                <p class="img_note"> (a) 机场; (b) 贫瘠地; (c) 海滩; (d) 桥梁; (e) 商业区; (f) 运动场; (g) 池塘; (h) 火车站; (i) 体育场; (j) 立交桥</p>
                                <p class="img_note"> (a) Airport; (b) bare land; (c) beach; (d) bridge; (e) commercial; (f) playground; (g) pond; (h) railway station; (i) stadium; (j) viaduct</p>

                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908044_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 RSSCN7数据集类的样本。" src="Detail/GetImg?filename=images/GXXB201908044_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 RSSCN7数据集类的样本。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908044_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Images of several classes from RSSCN7 dataset. </p>
                                <p class="img_note"> (a) 草地; (b) 河湖; (c) 工厂; (d) 场地; (e) 森林; (f) 居民区; (g) 停车场</p>
                                <p class="img_note"> (a) Grass; (b) river laker; (c) industrial; (d) field; (e) forest; (f) residential; (g) parking</p>

                </div>
                <div class="p1">
                    <p id="125">实验采用卷积网络模型GoogLeNet<citation id="178" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>的全连接层输出作为场景图像特征。词向量融合分为不同训练模型、不同语料词向量融合。其中, 不同训练模型的词向量融合实验, 涉及2种训练模型:Glove (gl) 和Word2Vec (wv) 。这两种词向量均在Wikipedia语料上训练得到。不同语料词向量融合实验, 采用2种训练语料:Wikipedia (Wiki) 和Common Crawl (Crawl) , 均采用Glove模型训练。Iter_<i>N</i>为40。定量实验采用总体分类准确度 (OA, <i>x</i><sub>OA</sub>) 作为评价指标, <i>x</i><sub>OA</sub>=<i>T</i><sub>u</sub>/<i>N</i><sub>u</sub>, <i>N</i><sub>u</sub>为全体unseen样本个数, <i>T</i><sub>u</sub>为正确分类的unseen样本个数。UCM和AID分别采用16/5和25/5的seen/unseen类划分。根据实验运行效果, 将稀疏比例<i>T</i><sub>0</sub>设置为10% (即稀疏化时保留前10%最大的元素值, 其余元素值置0) , 超参数<i>α</i>取值范围设置为{10<sup>-3</sup>, 10<sup>-2</sup>, 10<sup>-1</sup>, 1, 10}, 超参数<i>η</i>设置为0.01。</p>
                </div>
                <h4 class="anchor-tag" id="126" name="126"><b>4.2 定量实验结果及分析</b></h4>
                <div class="p1">
                    <p id="127">在UCM和AID数据集上进行定量实验, 并从结构对齐、超参数取值、融合效果以及与典型ZSC方法对比等4个方面, 分别进行分析。</p>
                </div>
                <h4 class="anchor-tag" id="128" name="128">4.2.1 结构对齐效果分析</h4>
                <div class="p1">
                    <p id="129">ZSC方法的本质是借助语义词向量提供的类间距离关系, 将图像特征空间中类别原型, 迁移至unseen类, 获得unseen类的图像特征空间原型表示, 最后利用该原型对unseen样本进行分类。而本文结构对齐项的实质作用就是降低两种空间类别间距离的不一致性。因此, 这里定义语义词向量空间与图像特征空间的类别距离结构差异度为</p>
                </div>
                <div class="p1">
                    <p id="130" class="code-formula">
                        <mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mtext>Μ</mtext></msub><mo>=</mo><mrow><mrow><mo>{</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></munder><mo stretchy="false">[</mo></mstyle><mi>d</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>p</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>}</mo></mrow></mrow><msup><mrow></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="131">式中:<i>d</i> (<i>c</i><sub><i>i</i></sub>, <i>c</i><sub><i>j</i></sub>) 表示第<i>i</i>、<i>j</i>类别词向量<i>c</i><sub><i>i</i></sub>和<i>c</i><sub><i>j</i></sub>的余弦距离;<i>d</i> (<i>p</i><sub><i>i</i></sub>, <i>p</i><sub><i>j</i></sub>) 表示第<i>i</i>、<i>j</i>类别图像特征空间类原型<i>p</i><sub><i>i</i></sub>和<i>p</i><sub><i>j</i></sub>的余弦距离。<i>D</i><sub>M</sub>越大表示两个空间的类间距离结构越不一致, 越小则表示越一致。图6和7分别为不同训练模型、不同训练语料词向量融合前后的<i>D</i><sub>M</sub>变化情况。符号♁表示经过本文方法 (<i>M</i>=2) 融合, 符号⨂表示直接串接的词向量。</p>
                </div>
                <div class="area_img" id="132">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908044_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同模型词向量融合的结构对齐效果" src="Detail/GetImg?filename=images/GXXB201908044_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同模型词向量融合的结构对齐效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908044_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Structure alignment performance of word vector fusion with different models</p>

                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908044_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同语料词向量融合的结构对齐效果图" src="Detail/GetImg?filename=images/GXXB201908044_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同语料词向量融合的结构对齐效果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908044_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Structure alignment performance of word vector fusion with different corpora</p>

                </div>
                <div class="p1">
                    <p id="135">可以看出, 相比未融合的单词向量及直接串接词向量, 本文融合方法得到的词向量具有最小的<i>D</i><sub>M</sub>值, 表明结构对齐效果显著优于直接串接以及未融合的词向量。这主要因为基于ADL的结构对齐项能够对融合词向量空间与图像特征空间之间的嵌入关系建模, 从而得到与图像特征空间中的类间距离结构更一致的融合词向量。</p>
                </div>
                <h4 class="anchor-tag" id="136" name="136">4.2.2 超参数取值分析</h4>
                <div class="p1">
                    <p id="137">本文方法中的超参数<i>α</i>取不同值会影响方法的分类效果, 为选取最佳的<i>α</i>值, 分别在不同的<i>α</i>取值上进行实验, 比较获得的OA值, 确定最佳超参数, 在UCM和AID数据集上的运行情况如图8所示。可以看出, 在全体取值范围内, 融合词向量下的OA值均高于未融合词向量的OA值。</p>
                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908044_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 UCM和AID数据集上本文方法在不同α值上的OA值。" src="Detail/GetImg?filename=images/GXXB201908044_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 UCM和AID数据集上本文方法在不同<i>α</i>值上的OA值。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908044_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 OA values of proposed method for different <i>α</i> on UCM and AID datasets.</p>
                                <p class="img_note"> (a) UCM上不同训练模型词向量融合; (b) UCM上不同训练语料词向量融合; (c) AID上不同训练模型词向量融合; (d) AID上不同训练语料词向量融合</p>
                                <p class="img_note"> (a) Fusion of word vectors from different training models on UCM dataset; (b) fusion of word vectors from different training corpora on UCM dataset; (c) fusion of word vectors from different training models on AID dataset; (d) fusion of word vectors from different training corpora on AID dataset</p>

                </div>
                <h4 class="anchor-tag" id="139" name="139">4.2.3 词向量融合效果分析</h4>
                <div class="p1">
                    <p id="140">表1为在UCM和AID上, 不同训练模型词向量和不同训练语料词向量的融合前后的OA值。其中, 训练模型gl与wv的融合词向量, 在UCM的OA为61.23%, 比未融合的gl、wv词向量分别提升了9.84%和13.58%;在AID的OA为69.47%, 比未融合的gl、wv词向量分别提升了11.55%和7.94%。训练语料Wiki与Crawl的融合词向量, 在UCM的OA为59.77%, 比未融合的Wiki、Crawl词向量分别提升了8.38%和15.16%;在AID的OA为68.49%, 比未融合的Wiki、Crawl词向量分别提升了10.57%和10.20%。可以看出, 融合后的词向量在2种数据集上的OA值均得到显著提升。图9为在UCM和AID上, 不同训练模型词向量和不同训练语料词向量的融合前后的各unseen类的分类准确度。可以看到, 融合后的各unseen类的ZSC分类准确度比融合前均有明显提升。结果表明, 本文方法能够适应不同unseen类的情形, 通过融合不同语义词向量, 利用它们间的一致性, 显著提升OA值及各场景类别的分类准确度。</p>
                </div>
                <div class="area_img" id="141">
                    <p class="img_tit">表1 不同训练模型词向量和不同训练语料词向量融合前后的OA <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 OA values of different training models and different training corpora before and after fusion of word vectors %</p>
                    <p class="img_note"></p>
                    <table id="141" border="1"><tr><td rowspan="2"><br />Dataset</td><td colspan="3"><br />Fusion of word vectors from different models</td><td colspan="3"><br />Fusion of word vectors from different corpora</td></tr><tr><td><br />gl</td><td>wv</td><td>gl♁wv</td><td><br />Wiki</td><td>Crawl</td><td>Wiki♁Crawl</td></tr><tr><td>UCM</td><td>51.39</td><td>47.65</td><td><b>61.23</b></td><td>51.39</td><td>42.86/44.61</td><td><b>59.77</b></td></tr><tr><td><br />AID</td><td>57.92</td><td>61.53</td><td><b>69.47</b></td><td>57.92</td><td>56.16/58.29</td><td><b>68.49</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="142">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908044_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 不同训练模型词向量和不同训练语料词向量的各unseen类融合效果。" src="Detail/GetImg?filename=images/GXXB201908044_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 不同训练模型词向量和不同训练语料词向量的各unseen类融合效果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908044_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Fusion performance of different training models and different training corpora on unseen classes.</p>
                                <p class="img_note"> (a) UCM上不同训练模型词向量融合; (b) UCM上不同训练语料词向量融合; (c) AID上不同训练模型词向量融合; (d) AID上不同训练语料词向量融合</p>
                                <p class="img_note"> (a) Fusion of word vectors from different training models on UCM dataset; (b) fusion of word vectors from different training corpora on UCM dataset; (c) fusion of word vectors from different training models on AID dataset; (d) fusion of word vectors from different training corpora on AID dataset</p>

                </div>
                <h4 class="anchor-tag" id="143" name="143">4.2.4 与典型ZSC方法比较</h4>
                <div class="p1">
                    <p id="144">通过与6种典型ZSC方法进行对比, 验证本文方法是否具有更优的ZSC效果。表2中涉及3种语义词向量融合, 其中S1为Glove模型在Common Crawl语料上训练的词向量, S2为Word2Vector模型在Wikipedia语料上训练的词向量, S3为Glove模型在Wikipedia语料上训练的词向量。“+”符号在对比典型ZSC方法中代表词向量串接操作。相比典型ZSC方法, 本文方法在数据集UCM和AID上均获得了最高OA值。其中S1+S2+S3融合词向量在UCM和AID上获得了最高分类OA值68.56%和76.85%, 显著优于对比的典型ZSC方法。在UCM上, S1+S2+S3的OA值分别超过S1+S2、S2+S3的OA值7.40%、7.33%, 而超过未参与融合的S3的OA值17.17%;在AID上, S1+S2+S3的OA值分别超过S1+S2、S2+S3的OA值6.41%、7.38%, 而超过未参与融合的S3的OA值18.93%。典型ZSC方法中RKT表现较好, 但在不同词向量下的OA值仍低于本文方法, 主要原因是: 1) RKT方法没有考虑语义词向量空间与场景图像特征空间的类间距离结构差异, 而本文方法通过结构对齐项有效减轻了这种距离结构差异性, 提升了到unseen类的迁移效果;2) 与其他典型ZSC方法相似, RKT方法仅针对单一语义词向量情形, 没有考虑多词向量的融合问题, 而本文方法基于ADL融合不同词向量, 通过利用不同词向量之间的一致性, 有效提升了ZSC效果。由于目前可获取的词向量种类有限, 未来随着词向量种类越来越多, 可以采用本文方法进行更多种词向量的融合, 比如定义S4为World2Vector模型在Common Crawl语料上训练的词向量, 由于本文方法对词向量种类没有限制, 可对S1+S2+S3+S4进行融合, 从而获得更高的ZSC准确度OA值。</p>
                </div>
                <div class="area_img" id="145">
                    <p class="img_tit">表2 本文方法及对比方法OA值 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 OA values of proposed method and relative methods </p>
                    <p class="img_note">%</p>
                    <table id="145" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="6"><br />UCM</td><td colspan="6"><br />AID</td></tr><tr><td>S1</td><td>S2</td><td>S3</td><td>S1+S2</td><td>S2+S3</td><td>S1+S2+S3</td><td>S1</td><td>S2</td><td>S3</td><td>S1+S2</td><td>S2+S3</td><td>S1+S2+S3</td></tr><tr><td>LatEm<sup>[4]</sup></td><td>18.80</td><td>20.40</td><td>19.80</td><td>33.00</td><td>23.00</td><td>20.80</td><td>15.90</td><td>22.65</td><td>23.81</td><td>18.71</td><td>28.17</td><td>21.62</td></tr><tr><td><br />RKT<sup>[5]</sup></td><td>40.00</td><td>39.80</td><td>44.60</td><td>40.20</td><td>43.60</td><td>43.60</td><td>48.92</td><td>48.03</td><td>48.15</td><td>48.92</td><td>50.13</td><td>53.25</td></tr><tr><td><br />DMaP<sup>[9]</sup></td><td>38.20</td><td>39.60</td><td>41.60</td><td>40.80</td><td>42.00</td><td>40.20</td><td>39.24</td><td>43.44</td><td>38.54</td><td>46.67</td><td>45.22</td><td>44.97</td></tr><tr><td><br />BiDiLEL<sup>[8]</sup></td><td>28.51</td><td>33.48</td><td>39.20</td><td>40.40</td><td>40.00</td><td>41.00</td><td>32.91</td><td>42.55</td><td>32.40</td><td>47.85</td><td>50.44</td><td>49.63</td></tr><tr><td><br />JLSE<sup>[6]</sup></td><td>37.25</td><td>34.21</td><td>45.68</td><td>37.66</td><td>34.88</td><td>38.03</td><td>36.11</td><td>34.97</td><td>42.30</td><td>35.99</td><td>43.50</td><td>45.54</td></tr><tr><td><br />SSE<sup>[7]</sup></td><td>38.36</td><td>39.48</td><td>37.91</td><td>38.72</td><td>39.19</td><td>38.23</td><td>38.24</td><td>37.16</td><td>39.56</td><td>34.53</td><td>40.92</td><td>43.56</td></tr><tr><td><br /><b>Proposed</b></td><td>44.61</td><td><b>47.65</b></td><td><b>51.39</b></td><td><b>61.16</b></td><td><b>61.23</b></td><td><b>68.56</b></td><td><b>58.29</b></td><td><b>61.53</b></td><td><b>57.92</b></td><td><b>70.44</b></td><td><b>69.47</b></td><td><b>76.85</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="146">图10为本文方法及对比典型ZSC方法在UCM和AID数据集上S1+S2+S3的各个unseen类别的分类准确度。可以看出, 本文方法在各个unseen类别上的分类准确度均优于对比的ZSC方法, 尤其是优于LatEm方法。由此可知, 本文方法不仅在OA值上优于对比ZSC方法, 而且在每个unseen场景类别上的准确度上同样优于对比方法, 进一步证明本文方法的实际效果。</p>
                </div>
                <div class="area_img" id="147">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908044_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 本文方法及对比方法的各unseen类S1+S2+S3词向量融合效果。" src="Detail/GetImg?filename=images/GXXB201908044_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 本文方法及对比方法的各unseen类S1+S2+S3词向量融合效果。   <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908044_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Fusion performance of S1+S2+S3 word vectors on unseen classes by proposed method and relative methods. </p>
                                <p class="img_note"> (a) UCM数据集; (b) AID数据集</p>
                                <p class="img_note"> (a) UCM dataset; (b) AID dataset</p>

                </div>
                <h4 class="anchor-tag" id="148" name="148">4.2.5 计算效率分析</h4>
                <div class="p1">
                    <p id="149">为比较本文方法与其他ZSC方法的计算效率, 测试各ZSC方法在AID数据集上对S1词向量上的计算耗时, 结果如表3所示。可以看出:DMaP方法耗时最长, 为409.26 s, 其次是JLSE方法, 耗时为70.20 s, 而本文方法耗时最短, 为17.90 s。这主要因为ADL算法的时间复杂度低, 使本文方法的运算效率优于对比的典型ZSC方法。</p>
                </div>
                <div class="area_img" id="150">
                    <p class="img_tit">表3 各ZSC算法在AID数据集上对S1词向量上的运算耗时 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Computing time of different ZSC algorithms on AID dataset with S1 word vector</p>
                    <p class="img_note"></p>
                    <table id="150" border="1"><tr><td><br />Method</td><td>Time/s</td></tr><tr><td><br />LatEm<sup>[4]</sup></td><td>21.66</td></tr><tr><td><br />RKT<sup>[5]</sup></td><td>24.24</td></tr><tr><td><br />DMaP<sup>[9]</sup></td><td>409.26</td></tr><tr><td><br />BiDiLEL<sup>[8]</sup></td><td>28.81</td></tr><tr><td><br />JLSE<sup>[6]</sup></td><td>70.20</td></tr><tr><td><br />SSE<sup>[7]</sup></td><td>19.74</td></tr><tr><td><br /><b>Proposed</b></td><td><b>17.90</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="151" name="151"><b>4.3 定性实验结果及分析</b></h4>
                <div class="p1">
                    <p id="152">为定性分析本文方法的实际遥感场景ZSC效果, 以RSSCN7数据集作为seen类样本, 对2幅高分辨率遥感图像I和II (空间分辨率均为0.3 m) 进行ZSC分类。unseen类选择为ocean、airport和runway。S1+S2+S3得到的词向量, 用于定性实验。步骤为:首先, 用单类别支持向量机 (SVM) 判断遥感场景样本是否属于seen类;然后, 对不属于seen类的样本, 视为unseen类样本, 采用本文方法进行ZSC。遥感图像I的尺寸为17920 pixel×10752 pixel, 场景尺寸设定为256 pixel×256 pixel。本文及对比方法在遥感图像I上的ZSC效果, 如图11所示。可以看出, 本文方法对unseen类场景的分类效果优于对比方法, 其中airport类的场景分类效果更明显。本文及对比的典型ZSC方法对于ocean类场景均具有良好的识别效果, 但对于airport类场景的识别效果差异较大。其中对airport类场景识别效果最差的方法是LatEm, 可以看出, LatEm将airport类场景误分为ocean类场景, 其余方法的识别效果优于LatEm, 但是均不如本文方法。</p>
                </div>
                <div class="area_img" id="153">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908044_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 测试遥感图像I的场景ZSC效果图" src="Detail/GetImg?filename=images/GXXB201908044_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 测试遥感图像I的场景ZSC效果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908044_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Scene ZSC results of test remote-sensing image I</p>

                </div>
                <div class="p1">
                    <p id="154">遥感图像II的尺寸为25344 pixel×29952 pixel, 场景尺寸设定为256 pixel×256 pixel。本文及对比方法在遥感图像II上的ZSC效果, 如图12所示。可以看出, 本文词向量融合方法的ZSC效果, 总体优于对比典型ZSC方法, 其中ocean类场景的分类效果尤其明显。遥感图像II的场景组成较遥感图像I更复杂, 尤其是陆地场景的地物组成种类繁多。与遥感图像I的ZSC效果不同, 不同方法对ocean类场景的识别效果差异较大。其中LatEm方法将许多陆地场景误分为ocean类场景, RKT等方法对ocean类场景识别出现了部分误分现象, 只有本文及SSE方法对ocean类场景的识别效果最佳, 但是SSE方法对airport类的识别效果不如本文方法, 因此整体来说本文方法的ZSC效果最佳。综合上述定性实验结果可知, 本文通过词向量融合的方法能够获得优于对比典型ZSC方法的ZSC效果。总体而言, 本文方法对ocean和airport类别场景的识别效果优于runway的识别效果, 主要原因是ocean和airport类别场景构成比runway场景更简单 (仅单一的海水和机场水泥地面) , 而runway场景类别组成复杂 (包括草地、水泥地面和标识符等) 。因此, 本文方法对构成简单的场景效果优于构成复杂的场景。</p>
                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908044_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 测试遥感图像II的场景ZSC效果图" src="Detail/GetImg?filename=images/GXXB201908044_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 测试遥感图像II的场景ZSC效果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908044_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Scene ZSC results of test remote-sensing image II</p>

                </div>
                <h3 id="156" name="156" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="157">针对遥感场景类别的语义词向量与图像特征原型的距离结构不一致问题, 提出了面向遥感场景ZSC的词向量融合方法, 通过定量和定性实验, 验证了该方法在不同训练模型、不同训练语料词向量融合的有效性。该方法有以下特点:1) 为利用不同词向量一致性, 利用解析字典学习方法提取各词向量的公共稀疏编码系数, 并作为融合后的词向量;2) 为降低结构差异性, 将遥感场景图像特征类原型嵌入到融合词向量空间中与其进行对齐。实验结果表明:与典型ZSC方法相比, 本文方法在缩小距离结构差异、提升总体分类准确度方面都有更优表现, 本文方法能够有效利用不同词向量的一致性, 显著提升遥感场景ZSC效果。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="12">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pyramid of Spatial Relatons for Scene-Level Land Use Classification">

                                <b>[1]</b> Chen S Z, Tian Y L.Pyramid of spatial relatons for scene-level land use classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (4) :1947-1957.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604039&amp;v=Mjk3NzM0OUdiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5tVmIvTElqWFRiTEc0SDlmTXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Liu D W, Han L, Han X Y.High spatial resolution remote sensing image classification based on deep learning[J].Acta Optica Sinica, 2016, 36 (4) :0428001.刘大伟, 韩玲, 韩晓勇.基于深度学习的高分辨率遥感影像分类研究[J].光学学报, 2016, 36 (4) :0428001.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot scene classification for high spatial resolution remote sensing images">

                                <b>[3]</b> Li A X, Lu Z W, Wang L W, <i>et al</i>.Zero-shot scene classification for high spatial resolution remote sensing images[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :4157-4167.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Latent embeddings for zero-shot classification">

                                <b>[4]</b> Xian Y Q, Akata Z, Sharma G, <i>et al</i>.Latent embeddings for zero-shot classification[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:69-77.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Relational knowledge transfer for zero-shot learning">

                                <b>[5]</b> Wang D, Li Y N, Lin Y T, <i>et al</i>.Relational knowledge transfer for zero-shot learning[C]∥Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA.California:AAAI, 2016:2145-2151.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning via joint latent similarity embedding">

                                <b>[6]</b> Zhang Z M, Saligrama V.Zero-shot learning via joint latent similarity embedding[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:6034-6042.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning via semantic similarity embedding">

                                <b>[7]</b> Zhang Z M, Saligrama V.Zero-shot learning via semantic similarity embedding[C]∥2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile, USA.New York:IEEE, 2015:4166-4174.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDD31F20253A10A4B4D5AE16DA35AA17A9&amp;v=MDQ3MzBFd0NNT1ZRc3VXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGhoeGJ1NXdLcz1OajdCYXNlN0g2Zk9yNDFBWjVvT0RBMDl2UkpuNzA0SVNYbVczUg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Wang Q, Chen K.Zero-shot visual recognition via bidirectional latent embedding[J].International Journal of Computer Vision, 2017, 124 (3) :356-383.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot recognition using dual visual-semantic mapping paths">

                                <b>[9]</b> Li Y N, Wang D H, Hu H H, <i>et al</i>.Zero-shot recognition using dual visual-semantic mapping paths[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:3279-3287.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning posed as a missing data problem">

                                <b>[10]</b> Zhao B, Wu B T, Wu T F, <i>et al</i>.Zero-shot learning posed as a missing data problem[C]∥2017 IEEE International Conference on Computer Vision Workshops (ICCVW) , October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:2616-2622.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attribute-Based Classification for Zero-Shot Visual Object Categorization">

                                <b>[11]</b> Lampert C H, Nickisch H, Harmeling S.Attribute-based classification for zero-shot visual object categorization[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014, 36 (3) :453-465.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-Shot Learning through Cross-Modal Transfer">

                                <b>[12]</b> Socher R, Ganjoo M, Manning C D, <i>et al</i>.Zero-shot learning through cross-modal transfer[C]∥26th International Conference on Neural Information Processing Systems, December 5-10, 2013, Lake Tahoe, Nevada.[S.l.:s.n.], 2013:935-943.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient Estimation of Word Representations in Vector Space">

                                <b>[13]</b> Mikolov T, Chen K, Corrado G, <i>et al</i>.Efficient estimation of word representations in vector space[J/OL]. (2013-09-07) [2019-03-01].https://arxiv.org/abs/1301.3781.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Glove:Global Vectors for Word Representation">

                                <b>[14]</b> Pennington J, Socher R, Manning C.Glove:global vectors for word representation[C]∥Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , 2014, Doha, Qatar.[S.l.:s.n.], 2014:1532-1543.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative analysis-synthesis dictionary learning for image classification">

                                <b>[15]</b> Yang M, Chang H Y, Luo W X.Discriminative analysis-synthesis dictionary learning for image classification[J].Neurocomputing, 2017, 219:404-411.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES579AE40DE8831392A28EB8D29F9C9B3D&amp;v=MTU4MjVpZk9mYmEvRjZDNXE0OHhFZU1IRDMwNnhoUmk2RGNJT25lV3JodERjTUdkTjduckNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRoaHhidTV3S3M9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> Wang J J, Guo Y Q, Guo J, <i>et al</i>.Synthesis linear classifier based analysis dictionary learning for pattern classification[J].Neurocomputing, 2017, 238:103-113.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=L0 sparsifying transform learning with efficient optimal updates and convergence guarantees">

                                <b>[17]</b> Ravishankar S, Bresler Y.Sparsifying transform learning with efficient optimal updates and convergence guarantees[J].IEEE Transactions on Signal Processing, 2015, 63 (9) :2389-2404.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bag of visual words and spatial extensions for land-use classification">

                                <b>[18]</b> Yang Y, Newsam S.Bag-of-visual-words and spatial extensions for land-use classification[C]∥Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems-GIS′10, November 2-5, 2010, San Jose, California, USA.New York:IEEE, 2010:270-279.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=AID:ABenchmark Data Set for Performance Evaluation of Aerial Scene Classification">

                                <b>[19]</b> Xia G S, Hu J W, Hu F, <i>et al</i>.AID:a benchmark data set for performance evaluation of aerial scene classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :3965-3981.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Learning Based Feature Selection for Remote Sensing Scene Classification">

                                <b>[20]</b> Zou Q, Ni L H, Zhang T, <i>et al</i>.Deep learning based feature selection for remote sensing scene classification[J].IEEE Geoscience and Remote Sensing Letters, 2015, 12 (11) :2321-2325.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[21]</b> Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J/OL]. (2015-04-10) [2019-03-01].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201908044" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201908044&amp;v=MDMzNjFqWFRiTEc0SDlqTXA0OUJZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5tVmIvTEk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

