

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135535344787500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201902032%26RESULT%3d1%26SIGN%3dMEiYAhmJrvWmq4sFR38RPrV5vfw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201902032&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201902032&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902032&amp;v=MjY3MjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURsV3J2T0lqWFRiTEc0SDlqTXJZOUdab1FLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#58" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="2 支持相关滤波跟踪器 ">2 支持相关滤波跟踪器</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#90" data-title="3 CFMK算法 ">3 CFMK算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="&lt;b&gt;3.1&lt;/b&gt;&lt;b&gt;基于多滤波器的目标跟踪与验证&lt;/b&gt;"><b>3.1</b><b>基于多滤波器的目标跟踪与验证</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;3.2&lt;/b&gt;&lt;b&gt;基于关键点匹配的目标检测器&lt;/b&gt;"><b>3.2</b><b>基于关键点匹配的目标检测器</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#136" data-title="4 实验及结果分析 ">4 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#137" data-title="&lt;b&gt;4.1&lt;/b&gt;&lt;b&gt;实验参数与数据&lt;/b&gt;"><b>4.1</b><b>实验参数与数据</b></a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;4.2&lt;/b&gt;&lt;b&gt;连续&lt;i&gt;K&lt;/i&gt;帧关键点影响分析&lt;/b&gt;"><b>4.2</b><b>连续<i>K</i>帧关键点影响分析</b></a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;4.3&lt;/b&gt;&lt;b&gt;跟踪算法性能比较&lt;/b&gt;"><b>4.3</b><b>跟踪算法性能比较</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#156" data-title="5 结 论 ">5 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#95" data-title="图1 CFMK的长期跟踪框架">图1 CFMK的长期跟踪框架</a></li>
                                                <li><a href="#110" data-title="图2 多特征滤波响应融合">图2 多特征滤波响应融合</a></li>
                                                <li><a href="#144" data-title="表1 不同&lt;i&gt;K&lt;/i&gt;值下所提算法在不同数据库上的平均DP和平均OP">表1 不同<i>K</i>值下所提算法在不同数据库上的平均DP和平均OP</a></li>
                                                <li><a href="#148" data-title="表2 9种跟踪算法在不同数据库上的平均跟踪性能参数">表2 9种跟踪算法在不同数据库上的平均跟踪性能参数</a></li>
                                                <li><a href="#149" data-title="图3 9种算法在不同数据库上的跟踪结果。 OTB-13数据库上的 (a) 跟踪成功率和 (b) 跟踪准确率;OTB-15数据库上的 (c) 跟踪成功率和 (d) 跟踪准确率">图3 9种算法在不同数据库上的跟踪结果。 OTB-13数据库上的 (a) 跟踪成功率和 (b) 跟踪......</a></li>
                                                <li><a href="#152" data-title="图4 11种场景下各算法的跟踪成功率比较。 (a) 平面内旋转; (b) 低分辨率; (c) 目标遮挡; (d) 出视野; (e) 平面外旋转; (f) 尺度变换; (g) 快速运动; (h) 背景杂乱; (i) 运动模糊; (j) 物体变形; (k) 光照变化">图4 11种场景下各算法的跟踪成功率比较。 (a) 平面内旋转; (b) 低分辨率; (c) 目标遮......</a></li>
                                                <li><a href="#155" data-title="图5 6种跟踪算法在部分序列上的跟踪结果图。 (a) BlurOwl; (b) Liquor; (c) Box; (d) Jogging 1; (e) Jogging 2; (f) Tigger">图5 6种跟踪算法在部分序列上的跟踪结果图。 (a) BlurOwl; (b) Liquor; (c......</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" title=" Vojir T, Noskova J, Matas J. Robust scale-adaptive mean-shift for tracking[J]. Pattern Recognition Letters, 2014, 49: 250-258." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061600106283&amp;v=MDkzMjBmYks4SHRmTnFZOUZaZXNKRG5RNm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SktGNGRiaHM9TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Vojir T, Noskova J, Matas J. Robust scale-adaptive mean-shift for tracking[J]. Pattern Recognition Letters, 2014, 49: 250-258.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" Bolme D S, Beveridge J R, Draper B A, &lt;i&gt;et al&lt;/i&gt;. Visual object tracking using adaptive correlation filters[C]. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010: 2544-2550." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">
                                        <b>[2]</b>
                                         Bolme D S, Beveridge J R, Draper B A, &lt;i&gt;et al&lt;/i&gt;. Visual object tracking using adaptive correlation filters[C]. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010: 2544-2550.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" >
                                        <b>[3]</b>
                                     Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;. High-speed tracking with kernelized correlation filters[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) : 583-596.</a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" title=" Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;. Discriminative scale space tracking[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 39 (8) : 1561-1575." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative scale space tracking">
                                        <b>[4]</b>
                                         Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;. Discriminative scale space tracking[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 39 (8) : 1561-1575.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" title=" Li Y, Zhu J K. A scale adaptive kernel correlation filter tracker with feature integration[C]. European Conference on Computer Vision, 2014: 254-265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A scale adaptive kernel correlation filter tracker with feature integration">
                                        <b>[5]</b>
                                         Li Y, Zhu J K. A scale adaptive kernel correlation filter tracker with feature integration[C]. European Conference on Computer Vision, 2014: 254-265.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" title=" Liu T, Wang G, Yang Q X. Real-time part-based visual tracking via adaptive correlation filters[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 4902-4912." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time part-based visual tracking via adaptive correlation filters">
                                        <b>[6]</b>
                                         Liu T, Wang G, Yang Q X. Real-time part-based visual tracking via adaptive correlation filters[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 4902-4912.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" Liu S, Zhang T Z, Cao X C, &lt;i&gt;et al&lt;/i&gt;. Structural correlation filter for robust visual tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016: 4312-4320." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structural Correlation Filter for Robust Visual Tracking">
                                        <b>[7]</b>
                                         Liu S, Zhang T Z, Cao X C, &lt;i&gt;et al&lt;/i&gt;. Structural correlation filter for robust visual tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016: 4312-4320.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" title=" Zuo W M, Wu X H, Lin L, &lt;i&gt;et al&lt;/i&gt;. Learning support correlation filters for visual tracking[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018: 2829180." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Support Correlation Filters for Visual Tracking">
                                        <b>[8]</b>
                                         Zuo W M, Wu X H, Lin L, &lt;i&gt;et al&lt;/i&gt;. Learning support correlation filters for visual tracking[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018: 2829180.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" Kalal Z, Mikolajczyk K, Matas J. Tracking-learning-detection[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (7) : 1409-1422." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tracking-Learning-Detection">
                                        <b>[9]</b>
                                         Kalal Z, Mikolajczyk K, Matas J. Tracking-learning-detection[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (7) : 1409-1422.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" title=" Supan■i■ III J S, Ramanan D. Self-paced learning for Long-term tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2013: 2379-2386." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Self-paced learning for long-term tracking">
                                        <b>[10]</b>
                                         Supan■i■ III J S, Ramanan D. Self-paced learning for Long-term tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2013: 2379-2386.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" title=" Zhu G, Porikli F, Li H D. Beyond local search: tracking objects everywhere with instance-specific proposals[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016: 943-951." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond Local Search:Tracking Objects Everywhere with Instance-Specific Proposals">
                                        <b>[11]</b>
                                         Zhu G, Porikli F, Li H D. Beyond local search: tracking objects everywhere with instance-specific proposals[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016: 943-951.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" title=" Zhu G B, Wang J Q, Wu Y, &lt;i&gt;et al&lt;/i&gt;. Collaborative correlation tracking[C]. Proceedings of the British Machine Vision Conference, 2015: 184." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Collaborative Correlation Tracking[C/OL]">
                                        <b>[12]</b>
                                         Zhu G B, Wang J Q, Wu Y, &lt;i&gt;et al&lt;/i&gt;. Collaborative correlation tracking[C]. Proceedings of the British Machine Vision Conference, 2015: 184.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" title=" Ma C, Yang X K, Zhang C Y, &lt;i&gt;et al&lt;/i&gt;. Long-term correlation tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 5388-5396." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long-term correlation tracking">
                                        <b>[13]</b>
                                         Ma C, Yang X K, Zhang C Y, &lt;i&gt;et al&lt;/i&gt;. Long-term correlation tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 5388-5396.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" Wang M M, Liu Y, Huang Z Y. Large margin object tracking with circulant feature maps[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2017: 4800-4808." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large Margin Object Tracking with Circulant Feature Maps">
                                        <b>[14]</b>
                                         Wang M M, Liu Y, Huang Z Y. Large margin object tracking with circulant feature maps[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2017: 4800-4808.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" Danelljan M, Khan F S, Felsberg M, &lt;i&gt;et al&lt;/i&gt;. Adaptive color attributes for real-time visual tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2014: 1090-1097." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive color attributes for Real-Time visual tracking">
                                        <b>[15]</b>
                                         Danelljan M, Khan F S, Felsberg M, &lt;i&gt;et al&lt;/i&gt;. Adaptive color attributes for real-time visual tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2014: 1090-1097.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" title=" Ma C, Huang J B, Yang X K, &lt;i&gt;et al&lt;/i&gt;. Adaptive correlation filters with long-term and short-term memory for object tracking[J]. International Journal of Computer Vision, 2018, 126 (8) : 771-796." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive correlation filters with long-term and short-term memory for object tracking">
                                        <b>[16]</b>
                                         Ma C, Huang J B, Yang X K, &lt;i&gt;et al&lt;/i&gt;. Adaptive correlation filters with long-term and short-term memory for object tracking[J]. International Journal of Computer Vision, 2018, 126 (8) : 771-796.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" title=" Leutenegger S, Chli M, Siegwart R Y. BRISK: binary robust invariant scalable keypoints[C]. International Conference on Computer Vision, 2011: 2548-2555." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Brisk:Binary robust invariant scalable keypoints">
                                        <b>[17]</b>
                                         Leutenegger S, Chli M, Siegwart R Y. BRISK: binary robust invariant scalable keypoints[C]. International Conference on Computer Vision, 2011: 2548-2555.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" Nebehay G, Pflugfelder R. Clustering of static-adaptive correspondences for deformable object tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 2784-2791." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering of Static-Adaptive Correspondences for Deformable Object Tracking">
                                        <b>[18]</b>
                                         Nebehay G, Pflugfelder R. Clustering of static-adaptive correspondences for deformable object tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 2784-2791.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_19" title=" Wu Y, Lim J, Yang M H. Online object tracking: a benchmark[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2013: 2411-2418." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online object tracking:A benchmark">
                                        <b>[19]</b>
                                         Wu Y, Lim J, Yang M H. Online object tracking: a benchmark[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2013: 2411-2418.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_20" title=" Wu Y, Lim J, Yang M H. Object tracking benchmark[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) : 1834-1848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">
                                        <b>[20]</b>
                                         Wu Y, Lim J, Yang M H. Object tracking benchmark[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) : 1834-1848.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_21" title=" Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;. Exploiting the circulant structure of tracking-by-detection with kernels[M]//Fitzgibbon A, Lazebnik S, Perona P, &lt;i&gt;et al&lt;/i&gt;. Computer Vision-ECCV 2012. Heidelberg: Springer, 2012, 7575: 702-715." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">
                                        <b>[21]</b>
                                         Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;. Exploiting the circulant structure of tracking-by-detection with kernels[M]//Fitzgibbon A, Lazebnik S, Perona P, &lt;i&gt;et al&lt;/i&gt;. Computer Vision-ECCV 2012. Heidelberg: Springer, 2012, 7575: 702-715.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_22" title=" Galoogahi H K, Fagg A, Lucey S. Learning background-aware correlation filters for visual tracking[C]. IEEE International Conference on Computer Vision (ICCV) , 2017: 1144-1152." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Background-Aware Correlation Filters for Visual Tracking">
                                        <b>[22]</b>
                                         Galoogahi H K, Fagg A, Lucey S. Learning background-aware correlation filters for visual tracking[C]. IEEE International Conference on Computer Vision (ICCV) , 2017: 1144-1152.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_23" title=" Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;. Learning spatially regularized correlation filters for visual tracking[C]. IEEE International Conference on Computer Vision (ICCV) , 2015: 4310-4318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">
                                        <b>[23]</b>
                                         Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;. Learning spatially regularized correlation filters for visual tracking[C]. IEEE International Conference on Computer Vision (ICCV) , 2015: 4310-4318.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_24" title=" Bertinetto L, Valmadre J, Golodetz S, &lt;i&gt;et al&lt;/i&gt;. Staple: complementary learners for real-time tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016: 1401-1409." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">
                                        <b>[24]</b>
                                         Bertinetto L, Valmadre J, Golodetz S, &lt;i&gt;et al&lt;/i&gt;. Staple: complementary learners for real-time tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016: 1401-1409.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-09-25 07:25</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(02),259-267 DOI:10.3788/AOS201939.0215001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">融合相关滤波与关键点匹配的跟踪算法</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%93%B2&amp;code=35044410&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张哲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E7%91%BE&amp;code=08055904&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙瑾</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E5%88%98%E6%B6%9B&amp;code=39540424&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨刘涛</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%97%E4%BA%AC%E8%88%AA%E7%A9%BA%E8%88%AA%E5%A4%A9%E5%A4%A7%E5%AD%A6%E6%B0%91%E8%88%AA%E5%AD%A6%E9%99%A2&amp;code=0014291&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南京航空航天大学民航学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出一种融合相关滤波与关键点匹配的跟踪算法。利用多个基于支持向量机的相关滤波器, 分别对目标进行跟踪和验证, 同时建立并实时更新一个目标和背景关键点数据库。在验证跟踪失败后, 利用关键点匹配的方法对全局关键点进行分类, 根据分类结果对目标关键点进行分析, 从而得到重检测结果。实验结果表明, 在运动模糊、变形、目标遮挡、消失等复杂跟踪场景下, 所提算法比现有算法具有更好的准确性和稳健性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标跟踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%85%B3%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相关滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%B3%E9%94%AE%E7%82%B9%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">关键点匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%95%BF%E6%9C%9F%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长期跟踪;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *张哲, E-mail:zhangzhe@nuaa.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-24</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然基金青年科学基金 (61702260);</span>
                                <span>南京航空航天大学研究生创新基地 (实验室) 开放基金 (kfjj20170716);</span>
                    </p>
            </div>
                    <h1>Tracking Algorithm Based on Correlation Filter Fusing with Keypoint Matching</h1>
                    <h2>
                    <span>Zhang Zhe</span>
                    <span>Sun Jin</span>
                    <span>Yang Liutao</span>
            </h2>
                    <h2>
                    <span>College of Civil Aviation, Nanjing University of Aeronautics and Astronautics</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A tracking algorithm is proposed based on correlation filter fusing with keypoint matching. The object is tracked and verified by multiple support-vector-based correlation filters, respectively. Meanwhile, a database containing the keypoints of the target and the background is established and updated in real-time. After the validation of a tracking failure, the global keypoints are classified by utilizing the keypoint matching method and the target keypoints are analyzed according to these classified results. Thus the redetection results are obtained. The experimental results show that the proposed method has a better accuracy and robustness than those by the existing methods in the complex tracking scenes of motion blur, deformation, object occlusion, disappearance and so on.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=object%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">object tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=correlation%20filter&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">correlation filter;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=keypoint%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">keypoint matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=long-term%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">long-term tracking;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-07-24</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="58" name="58" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="59">目标跟踪是计算机视觉领域的重要研究方向之一, 被广泛应用于智能监控、人机交互、自动驾驶导航、行为分析等领域。近年来, 基于视觉的目标跟踪技术有了长足发展, 但是受限于光照变换、目标形变、物体遮挡、快速运动等多种因素, 长期稳定的目标跟踪仍是一个极具挑战性的问题。</p>
                </div>
                <div class="p1">
                    <p id="60">目标跟踪技术主要可分为生成式和判别式两类。生成式跟踪方法首先对跟踪目标进行特征建模, 再搜索与模型最相似的目标区域, 并将其作为跟踪预测结果。经典的算法有粒子滤波、卡尔曼滤波、均值漂移<citation id="158" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>等。与生成式跟踪方法不同, 判别式跟踪方法将目标跟踪转化为分类问题, 利用学习好的分类器将跟踪目标从背景中区分出来, 从而得到跟踪结果。判别式跟踪方法由于学习了背景信息, 具有更好的分辨性能, 在识别准确率上普遍优于生成式跟踪方法。判别式相关滤波跟踪算法一经提出, 便以其快速、稳定的跟踪表现, 迅速成为目标跟踪领域的研究热点之一。2010年, Bolme 等<citation id="159" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出了一种最小均方误差和 (MOSSE) 跟踪器滤波算法, 将相关滤波引入目标跟踪领域。Heriques 等<citation id="160" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出核相关滤波器 (KCF) , 将梯度方向直方图 (HOG) 特征与核方法引入相关滤波, 通过将HOG特征映射到非线性空间, 增强特征和滤波器的稳健性。针对跟踪过程中的目标尺度变化等影响, 文献<citation id="162" type="reference">[<a class="sup">4</a>,<a class="sup">5</a>]</citation>在平移滤波的同时增加了尺度滤波, 有效减小了由目标尺度变化引起的模型漂移等的影响。文献<citation id="163" type="reference">[<a class="sup">6</a>,<a class="sup">7</a>]</citation>提出基于分块的相关滤波跟踪模型, 有效提高了目标部分遮挡时的跟踪效果。Zuo等<citation id="161" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出的支持相关滤波能有效增大正负样本之间的特征间隔, 从而改善滤波器的分类效果。虽然基于相关滤波的跟踪算法在速度和效果上优于生成式跟踪算法, 但是在严重遮挡甚至目标消失的情况下, 目标跟踪失败后无法进行后续的长期跟踪。</p>
                </div>
                <div class="p1">
                    <p id="61">在长期跟踪过程中, 目标的跟踪更容易受到多种复杂条件共同作用的干扰, 如光照变化、目标遮挡、消失和形变等。这种情况下容易发生目标跟踪失败, 从而在跟踪模型中累积误差, 最终导致模型漂移直至无法正确跟踪。因此, 对于长期跟踪任务, 跟踪失败后的目标恢复机制 (重检测) 不可或缺。Kalal等<citation id="164" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出了一种基于经典“训练-学习-检测”机制的长期跟踪算法, 其检测器集成多个随机蕨得到的识别结果, 与跟踪器得到的目标进行对比验证, 得到最终的跟踪结果并更新检测器。但随机蕨应用随机局部二值特征对目标进行编码时, 容易受光照等因素的影响。Supan■i■ III等<citation id="165" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>利用自步学习方法结合支持向量机 (SVM) 筛选出“准确”跟踪的视频帧, 并以此帧中的目标对重检测器进行更新。跟踪过程中需要连续更新SVM, 计算量大, 导致算法不能实时进行。EBT (EdgeBox Tracking) 算法<citation id="166" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>根据先验条件对每一帧提取的类物体区域进行排名, 对排名靠前的区域再进行检测打分, 得到跟踪结果。EBT算法的全局搜索机制能降低误检率, 但是在相似度高的背景下, 跟踪效果差且不能实时进行。相关滤波方法出现后, 基于相关滤波的跟踪或重检测的方法也被引入长期跟踪领域。Zhu等<citation id="167" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>在相关滤波的基础上引入一种低秩矩阵近似方法, 通过随机采样构建低秩目标表征滤波器, 与全局图像进行滤波, 用得到的结果对滤波跟踪进行验证与重检测。低秩滤波器只包含对目标表征的重建, 缺少相应的背景信息, 影响样本的分类。Ma等<citation id="168" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出长期相关滤波跟踪 (LCT) 算法, 在相关滤波器外通过更新一个SVM分类器对目标进行重检测。但是, 较少的更新迭代次数影响了分类器的精度, 同时分类器使用的直方图特征缺少目标的局部特征, 容易产生误识别。为了更有针对性地进行目标重检测, Wang等<citation id="169" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了高置信度检测更新条件和多峰目标检测方法, 从而有效地提高检测效率。跟踪与检测使用同一个模型, 虽然能提升跟踪速度, 但是无法应对模型漂移问题。</p>
                </div>
                <div class="p1">
                    <p id="62">针对以上分析, 在此提出一种融合相关滤波和关键点匹配 (CFMK) 的长期跟踪框架。首先利用基于SVM改进的相关滤波算法<citation id="170" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>增强跟踪滤波器的可分辨性, 同时训练多个滤波器, 融合多滤波响应结果, 从而减小噪声误差并对跟踪结果进行验证;其次, 针对重检测问题, 提出一种基于关键点匹配的目标检测器, 通过连续更新目标关键点的时空信息, 增强重检测器的稳健性, 从而提高整体跟踪框架的准确性和稳定性。</p>
                </div>
                <h3 id="63" name="63" class="anchor-tag">2 支持相关滤波跟踪器</h3>
                <div class="p1">
                    <p id="64">相关滤波源于信号处理领域, 后被用于图像分类等方面。Bolme等<citation id="171" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出的MOSSE滤波器对每一帧目标表观进行编码, 从而实现快速跟踪, 第一次将相关滤波引入目标跟踪领域。在此基础上, Heriques等<citation id="172" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>利用循环矩阵的性质, 提出KCF, 将核相关滤波方法和多维HOG特征引入目标跟踪, 将线性空间滤波扩展至非线性空间, 提高了跟踪稳健性。</p>
                </div>
                <div class="p1">
                    <p id="65">在KCF中, 给定以跟踪目标为中心的包含目标及其相应背景的图像块<b><i>x</i></b>, 其大小为<i>M</i>×<i>N</i>个像素。通过<b><i>x</i></b>分别沿图像行与列方向循环位移, 得到密集采样样本<b><i>x</i></b><sub><i>m</i>, <i>n</i></sub>, 位移坐标 (<i>m</i>, <i>n</i>) ∈{0, 1, 2, …, <i>M</i>-1}×{0, 1, 2, …, <i>N</i>-1}, 且满足高斯分布的样本标签<b><i>y</i></b><sub><i>m</i>, <i>n</i></sub>。相关滤波器<b><i>w</i></b>满足条件:</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">w</mi><mo>=</mo><mi>arg</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">w</mi></munder><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></munder><mo stretchy="false">|</mo></mstyle><mo>〈</mo><mtext>ϕ</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>〉</mo><mo>-</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">式中:ϕ (·) 为非线性核空间映射;<i>λ</i>为正则项系数;〈·, ·〉为向量内积。令<mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">w</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></munder><mi>α</mi></mstyle><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mo>⋅</mo><mtext>ϕ</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>, 根据Heriques等<citation id="173" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出的非线性空间方法, 将<b><i>w</i></b>的求解转化为求对偶空间的系数<i>α</i> (<i>α</i><sub><i>m</i>, <i>n</i></sub>为<i>α</i>的元素) 。利用最小二乘法求得闭合解为</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">α</mi><mo>=</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><mo>[</mo><mrow><mfrac><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover><mrow><mover accent="true"><mtext>ϕ</mtext><mo>^</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>˚</mo><mover accent="true"><mtext>ϕ</mtext><mo>^</mo></mover><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi></mrow></mfrac></mrow><mo>]</mo></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">式中:<mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mo>⋅</mo><mo>^</mo></mover></math></mathml>和<font face="EU-HT">F</font><sup>-1</sup>分别表示傅里叶变换和傅里叶逆变换;* 表示取复共轭;。代表Hadamard乘积。在下一帧中, 用得到的滤波模板的<i>α</i>和表观模型的<b><i>x</i></b>对相同大小的搜索窗口样本<b><i>z</i></b>进行相关滤波操作, 得到目标滤波响应<mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover><mo>‚</mo></mrow></math></mathml>取其最大值作为目标预测结果。<mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover></math></mathml>可表示为</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover><mo>=</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">[</mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><mo>˚</mo><mover accent="true"><mtext>ϕ</mtext><mo>^</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>˚</mo><mover accent="true"><mtext>ϕ</mtext><mo>^</mo></mover><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">基于SVM的支持相关滤波方法<citation id="174" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>引入函数间隔, 增强正负样本的可分辨性, 该方法训练一个支持相关滤波器<b><i>w</i></b>以及一个偏差<i>b</i>, 每个目标的滤波响应为</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover><mo>=</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">x</mi><mo>+</mo><mi>b</mi><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">式中:上标T表示共轭转置。根据响应函数和SVM函数间隔模型, 训练的<b><i>w</i></b>、<i>b</i>、循环采样样本<b><i>x</i></b>=[<i>x</i><sub>1</sub>;<i>x</i><sub>2</sub>;…;<i>x</i><sub><i>mn</i></sub>]以及对应的样本标签<b><i>y</i></b>=[<i>y</i><sub>1</sub>;<i>y</i><sub>2</sub>;…;<i>y</i><sub><i>mn</i></sub>]应满足目标函数:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi>b</mi></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>C</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>ξ</mi></mstyle><msubsup><mrow></mrow><mi>j</mi><mn>2</mn></msubsup><mo>, </mo></mtd></mtr><mtr><mtd columnalign="left"><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mtext> </mtext><mi mathvariant="bold-italic">y</mi><mo>˚</mo><mo stretchy="false">[</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo>˚</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><mn>1</mn><mo stretchy="false">]</mo><mo>≥</mo><mn>1</mn><mo>-</mo><mi mathvariant="bold-italic">ξ</mi></mtd></mtr></mtable></mrow></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">式中:<i>ξ</i>=[<i>ξ</i><sub>1</sub>; <i>ξ</i><sub>2</sub>;…; <i>ξ</i><sub><i>mn</i></sub>]为松弛变量;<i>C</i>为正则系数;<b>1</b>表示元素全为1的向量;<i>j</i>代表采样样本序号。根据Zuo等<citation id="175" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的实验设计以及样本中心点与目标中心点的距离设置二分类标签。</p>
                </div>
                <div class="p1">
                    <p id="80">设变量<mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">u</mi><mo>=</mo><mi mathvariant="bold-italic">y</mi><mo>˚</mo><mo stretchy="false">[</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo>˚</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><mn>1</mn><mo stretchy="false">]</mo><mo>-</mo><mn>1</mn><mo>+</mo><mi mathvariant="bold-italic">ξ</mi></mrow></math></mathml>, 将约束条件代入目标函数得</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi>b</mi><mo>, </mo><mi mathvariant="bold-italic">u</mi></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>C</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">y</mi><mo>˚</mo><mo stretchy="false">[</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo>˚</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><mn>1</mn><mo stretchy="false">]</mo><mo>-</mo><mn>1</mn><mo>-</mo><mi mathvariant="bold-italic">u</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mtd></mtr><mtr><mtd columnalign="left"><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mtext> </mtext><mi mathvariant="bold-italic">u</mi><mo>≥</mo><mn>0</mn></mtd></mtr></mtable><mo>。</mo></mrow></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">针对上述多变量的优化问题, 采取迭代的交替优化算法求<b><i>w</i></b>、<i>b</i>的近似解, 每次迭代分步骤更新所求参数:</p>
                </div>
                <div class="p1">
                    <p id="84">1) 已知<b><i>w</i></b>, <i>b</i>, 更新<b><i>u</i></b>。由目标函数及约束条件可求得</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi mathvariant="bold-italic">u</mi><mo>=</mo><mi mathvariant="bold-italic">y</mi><mo>˚</mo><mo stretchy="false">[</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo>˚</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><mn>1</mn><mo stretchy="false">]</mo><mo>-</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left"><mi mathvariant="bold-italic">u</mi><mo>=</mo><mi>max</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">u</mi><mo>, </mo><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">2) 已知<b><i>u</i></b>, 更新<b><i>w</i></b>, <i>b</i>。设<b><i>p</i></b>=<b><i>y</i></b>。 (<b>1</b>+<b><i>u</i></b>) , 根据相关滤波的求解公式, 可得</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover><mo>=</mo><mfrac><mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><mo>˚</mo><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">p</mi><mo>^</mo></mover><mo>-</mo><mi>b</mi><mn>1</mn><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow></mrow></mstyle><mo>^</mo></mover></mrow><mspace width="0.25em" /><mo stretchy="false">) </mo></mrow><mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo>˚</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><mo>+</mo><mn>1</mn><mo>/</mo><mi>C</mi></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="left"><mi>b</mi><mo>=</mo><mn>1</mn><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">[</mo><mi mathvariant="bold-italic">p</mi><mo>-</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo>˚</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>=</mo><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">同理, 根据 (2) 式将样本和滤波器映射到非线性空间中, 迭代更新对偶系数:</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><mo>=</mo><mfrac><mrow><mover accent="true"><mi mathvariant="bold-italic">p</mi><mo>^</mo></mover><mo>-</mo><mi>b</mi><mn>1</mn><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow></mrow></mstyle><mo>^</mo></mover></mrow><mspace width="0.25em" /></mrow><mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo>˚</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><mo>+</mo><mn>1</mn><mo>/</mo><mi>C</mi></mrow></mfrac><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h3 id="90" name="90" class="anchor-tag">3 CFMK算法</h3>
                <div class="p1">
                    <p id="91">CFMK算法是一种融合支持相关滤波方法和关键点匹配方法的长期目标跟踪算法, 主要由两部分组成:1) 基于支持相关滤波的跟踪器;2) 基于关键点匹配的重检测器。整体跟踪流程如图1所示。</p>
                </div>
                <div class="p1">
                    <p id="92">根据训练好的运动模型对输入帧搜索窗口 (图1中圆点矩形) 进行相关滤波操作, 根据滤波响应值得到预测目标 (实线矩形) 并利用目标表观模型对预测结果进行可信度检测。对可信度[即平均峰值相关能量值 (APCE) <i>R</i><sub>APCE</sub>高于阈值<i>δ</i><sub>APCE</sub>的跟踪结果]进行尺度估计和模型的更新 (图1中点状) 。对于可信度低于阈值的跟踪结果, 激活目标检测器对目标进行重检测 (图1中虚线) 。利用关键点匹配的方法对全局关键点进行匹配分类, 根据目标关键点得到重检测结果 (虚线矩形) , 并将该结果与跟踪结果进行比较, 得到最优跟踪结果, 再进行尺度估计和模型的更新。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93"><b>3.1</b><b>基于多滤波器的目标跟踪与验证</b></h4>
                <div class="p1">
                    <p id="94">为方便比较, 所提算法与主流算法统一采用HOG与颜色名<citation id="176" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation> (CN) 特征描述所有样本, 根据 (7) ～ (9) 式分别对两种特征单独训练支持相关滤波器<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">{</mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow></msub><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow></msub><mo stretchy="false">}</mo></mrow></math></mathml>和<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">{</mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mtext>C</mtext><mtext>Ν</mtext></mrow></msub><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mtext>C</mtext><mtext>Ν</mtext></mrow></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mrow><mtext>C</mtext><mtext>Ν</mtext></mrow></msub><mo stretchy="false">}</mo></mrow></math></mathml>。两个滤波器统称运动模型<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">{</mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>m</mtext></msub><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>m</mtext></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mtext>m</mtext></msub><mo stretchy="false">}</mo></mrow></math></mathml>。检测时, 根据 (4) 式得到两个运动滤波响应向量, 记作<mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow></msub><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mtext>C</mtext><mtext>Ν</mtext></mrow></msub></mrow></math></mathml>。将滤波响应看作目标中心点分布概率, 利用KL (Kullback-Leibler ) 散度<citation id="177" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>计算优化概率分布<mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover><msub><mrow></mrow><mtext>m</mtext></msub></mrow></math></mathml>:</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902032_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 CFMK的长期跟踪框架" src="Detail/GetImg?filename=images/GXXB201902032_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 CFMK的长期跟踪框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902032_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Framework of long-term tracking with CFMK</p>

                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover><msub><mrow></mrow><mtext>m</mtext></msub><mo>=</mo><mi>arg</mi><mspace width="0.25em" /><mi>max</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mo stretchy="false">{</mo><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext><mo>, </mo><mtext>C</mtext><mtext>Ν</mtext><mo stretchy="false">}</mo></mrow></munder><mrow><mstyle displaystyle="true"><mo>∑</mo><mi mathvariant="bold-italic">y</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow><mi>l</mi></msubsup><mi>ln</mi><mfrac><mrow><mi>y</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow><mi>l</mi></msubsup></mrow><mrow><mover accent="true"><mi>y</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="left"><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mtext> </mtext><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></munder><mover accent="true"><mi>y</mi><mo>¯</mo></mover></mstyle><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mo>=</mo><mn>1</mn></mtd></mtr></mtable></mrow></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">式中:<i>l</i>为特征类型;<i>m</i>, <i>n</i>为二维坐标值;<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>y</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub></mrow></math></mathml>为对应位置元素值。利用拉格朗日乘子法求解 (10) 式, 得到</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover><msub><mrow></mrow><mtext>m</mtext></msub><mo>=</mo><mfrac><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow></msub><mo>+</mo><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mtext>C</mtext><mtext>Ν</mtext></mrow></msub></mrow><mn>2</mn></mfrac><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">取最大值作为目标中心点预测, 如图2所示。</p>
                </div>
                <div class="p1">
                    <p id="107">为了判断是否出现跟踪失败, 除了训练区分目标和背景的滤波器 (运动模型) 之外, 还额外训练一个只包含目标的表观滤波模型<mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">{</mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>a</mtext></msub><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>a</mtext></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mtext>a</mtext></msub><mo stretchy="false">}</mo></mrow></math></mathml>, 其目的在于计算跟踪结果与目标表观的滤波响应向量<mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover><msub><mrow></mrow><mtext>a</mtext></msub></mrow></math></mathml>, </p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902032_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 多特征滤波响应融合" src="Detail/GetImg?filename=images/GXXB201902032_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 多特征滤波响应融合  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902032_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Response fusion of multiple features</p>

                </div>
                <div class="p1">
                    <p id="111">并根据响应计算跟踪结果的可信度。当可信度小于阈值时, 激活检测器, 对全局进行检索得到新的重检测结果, 并将其与跟踪结果进行比较, 得到最优预测。所提算法采用文献<citation id="178" type="reference">[<a class="sup">14</a>]</citation>提出的APCE条件计算跟踪可信度:</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">|</mo><mover accent="true"><mi>y</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mover accent="true"><mi>y</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></munder><mo stretchy="false"> (</mo></mstyle><mover accent="true"><mi>y</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mo>-</mo><mover accent="true"><mi>y</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>]</mo></mrow></mrow></mfrac><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">式中:<mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>y</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub></mrow></math></mathml>、<mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>y</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow></math></mathml>分别为得到的表观模型滤波响应<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover><msub><mrow></mrow><mtext>a</mtext></msub></mrow></math></mathml>的最大值和最小值。该条件能很好地反映滤波响应的波动程度和可信度水平。<i>R</i><sub>APCE</sub>值越大, 目标响应峰值相对背景的响应越大, 并且除峰值区域外响应更为平滑, 目标响应可信度更高, 反之响应可信度低。实验中设定阈值<i>δ</i><sub>APCE</sub>, 当滤波跟踪结果大于阈值时, 更新表观滤波模型与检测器, 否则激活目标检测器, 对目标进行全局检测。针对目标的尺度变化问题, 采用文献<citation id="179" type="reference">[<a class="sup">4</a>]</citation>的方法在预测位置处构建目标尺度金字塔, 通过尺度滤波得到最佳的尺度变化率。</p>
                </div>
                <div class="p1">
                    <p id="117">运动模型参数每帧都会更新, 而表观模型和重检测器只在跟踪目标可信度高于阈值时进行更新。为减小运动模糊、噪声干扰和遮挡等条件的影响, 采用线性插值法更新模型:</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msubsup><mrow></mrow><mtext>m</mtext><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>γ</mi><mo stretchy="false">) </mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msubsup><mrow></mrow><mtext>m</mtext><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>m</mtext></msub></mtd></mtr><mtr><mtd columnalign="left"><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><msubsup><mrow></mrow><mtext>m</mtext><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>γ</mi><mo stretchy="false">) </mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><msubsup><mrow></mrow><mtext>m</mtext><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>m</mtext></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>b</mi><msubsup><mrow></mrow><mtext>m</mtext><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>γ</mi><mo stretchy="false">) </mo><mi>b</mi><msubsup><mrow></mrow><mtext>m</mtext><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mi>b</mi><msub><mrow></mrow><mtext>m</mtext></msub></mtd></mtr></mtable></mrow></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">式中:<i>γ</i>为参数学习率;<i>t</i>表示第<i>t</i>帧。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120"><b>3.2</b><b>基于关键点匹配的目标检测器</b></h4>
                <div class="p1">
                    <p id="121">在支持相关滤波跟踪器的基础上, 针对目标消失和严重遮挡的情况, 提出一种基于关键点匹配的目标检测器。检测器通过融合多帧图像中目标与背景关键点的时空信息建立数据库, 在重检测时利用关键点匹配的原则对当前帧关键点进行分类, 再对分类结果进行分析, 从而得到重检测结果。</p>
                </div>
                <div class="p1">
                    <p id="122">关键点数据库<i>D</i>=<i>T</i>∪<i>B</i>包含目标关键点集合<i>T</i>与背景关键点集合<i>B</i>:</p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false"> (</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">}</mo><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow></msubsup><mo>‚</mo><mspace width="0.25em" /><mi>B</mi><mo>=</mo><mo stretchy="false">{</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>B</mi></msub></mrow></msubsup><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="124">关键点选择具有良好尺度和旋转不变性的BRISK<citation id="180" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation> (Binary Robust In variant Scalable Keypoints) 特征, 其中:<i>d</i><sub><i>i</i></sub>∈<b>R</b><sup>512</sup>表示关键点二进制特征描述子, 特征维度为512; <i>r</i><sub><i>i</i></sub> 表示目标关键点与目标中心点的相对位置;<i>N</i><sub><i>T</i></sub>、<i>N</i><sub><i>B</i></sub>为关键点数量。由于剧烈运动或者物体形变, 目标表观特征也随之改变。为了更好地适应特征的变化, 不同于文献<citation id="181" type="reference">[<a class="sup">18</a>]</citation>利用第一帧初始化关键点集合的方式, 提出一种融合多帧图像关键点并实时更新的关键点数据库, 降低由目标表观变化带来的影响。设目标关键点集合<i>T</i>由<i>K</i>帧图像中目标关键点子集<i>V</i>构成:</p>
                </div>
                <div class="p1">
                    <p id="125" class="code-formula">
                        <mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mo>=</mo><mi>V</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mstyle displaystyle="true"><mo>∪</mo><mi>V</mi></mstyle><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msub><mstyle displaystyle="true"><mo>∪</mo><mo>⋯</mo></mstyle><mstyle displaystyle="true"><mo>∪</mo><mi>V</mi></mstyle><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mi>Κ</mi></mrow></msub><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="126">式中:<i>t</i>表示当前帧。只有在可信度满足阈值条件时更新数据库, 所以子集可能来自非连续帧。更新时, 根据关键点时效性, 用当前帧关键点子集<i>V</i><sub><i>t</i></sub>替换时域最远帧的子集<i>V</i><sub><i>t</i>-<i>K</i></sub>。更新关键点与原有关键点存在的冗余会导致重检测匹配时的干扰和额外的时间消耗, 在更新时先对<i>V</i><sub><i>t</i></sub>和<i>T</i>-<i>V</i><sub><i>t</i>-<i>K</i></sub>进行一次匹配, 过滤<i>T</i>-<i>V</i><sub><i>t</i>-<i>K</i></sub>中的重复关键点, 加入目标关键点集合:</p>
                </div>
                <div class="p1">
                    <p id="127" class="code-formula">
                        <mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>V</mi><msub><mrow></mrow><mtext>m</mtext></msub><mo>=</mo><mtext>m</mtext><mtext>a</mtext><mtext>t</mtext><mtext>c</mtext><mtext>h</mtext><mo stretchy="false"> (</mo><mi>V</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>Τ</mi><mo>-</mo><mi>V</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mi>Κ</mi></mrow></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd columnalign="left"><mi>Τ</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>w</mtext></mrow></msub><mo>=</mo><mi>V</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>Τ</mi><mo>-</mo><mi>V</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mi>Κ</mi></mrow></msub><mo>-</mo><mi>V</mi><msub><mrow></mrow><mtext>m</mtext></msub></mtd></mtr></mtable></mrow></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="128">式中:match () 为匹配函数。在需要重检测的视频帧中的全局图像中提取候选关键点<i>P</i>={ (<i>d</i><sub><i>i</i></sub>, <i>a</i><sub><i>i</i></sub>) }<sup><i>Np</i><sub><i>i</i>=1</sub></sup>, 其中<i>a</i><sub><i>i</i></sub>表示关键点在视频帧中的绝对位置, <i>N</i><sub><i>p</i><sub><i>i</i></sub></sub>为候选关键点数量。在进行关键点匹配时, 计算<i>P</i>中关键点<i>p</i><sub><i>i</i></sub>与数据库<i>D</i>中每个关键点的Hamming距离, 并采用最近邻方法得到最近的两个关键点<i>d</i><sup>1st</sup><sub><i>i</i></sub>, <i>d</i><sup>2nd</sup><sub><i>i</i></sub>, 距离分别为<i>h</i><sup>1st</sup>和<i>h</i><sup>2nd</sup>, <i>P</i>中关键点的匹配分类准则为</p>
                </div>
                <div class="p1">
                    <p id="129" class="code-formula">
                        <mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>Τ</mi><mo>, </mo><mspace width="0.25em" /><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mi>d</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mn>1</mn><mtext>s</mtext><mtext>t</mtext></mrow></msubsup><mo>∈</mo><mi>Τ</mi><mspace width="0.25em" /><mo>&amp;</mo><mspace width="0.25em" /><mi>h</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mn>1</mn><mtext>s</mtext><mtext>t</mtext></mrow></msubsup><mo>&lt;</mo><mi>θ</mi><msub><mrow></mrow><mtext>c</mtext></msub><mspace width="0.25em" /><mo>&amp;</mo><mspace width="0.25em" /><mi>h</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mn>1</mn><mtext>s</mtext><mtext>t</mtext></mrow></msubsup><mo>/</mo><mi>h</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mn>2</mn><mtext>n</mtext><mtext>d</mtext></mrow></msubsup><mo>&lt;</mo><mi>θ</mi><msub><mrow></mrow><mtext>r</mtext></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>B</mi><mo>, </mo><mspace width="0.25em" /><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="130">式中:<i>θ</i><sub>c</sub>、<i>θ</i><sub>r</sub>分别为置信阈值和比例阈值;<i>P</i>中分类为目标关键点的<i>p</i><sub><i>i</i></sub>∈<i>T</i>, 加上与其匹配的关键点相对位置<i>r</i><sub><i>i</i></sub>, 组成子集<i>P</i><sup><i>T</i></sup>={ (<i>d</i><sub><i>i</i></sub>, <i>a</i><sub><i>i</i></sub>, <i>r</i><sub><i>i</i></sub>) }。</p>
                </div>
                <div class="p1">
                    <p id="131">根据匹配得到的目标关键点子集<i>P</i><sup><i>T</i></sup>的位置信息, 计算每个点对目标中心点的预测:</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>c</mtext><mtext>e</mtext><mtext>n</mtext><mtext>t</mtext><mtext>e</mtext><mtext>r</mtext></mrow></msubsup><mo>=</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133">考虑到目标的尺度变换, 关键点与中心点的相对距离也会随之改变。根据跟踪时的尺度变换系数<i>s</i>对 (18) 式进行修正, 可得</p>
                </div>
                <div class="p1">
                    <p id="134" class="code-formula">
                        <mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>c</mtext><mtext>e</mtext><mtext>n</mtext><mtext>t</mtext><mtext>e</mtext><mtext>r</mtext></mrow></msubsup><mo>=</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>s</mi><mo>⋅</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="135"><i>P</i><sup><i>T</i></sup>中所有关键点对应的目标中心点预测构成集合<i>M</i>。匹配过程中可能出现错误且位置计算中可能产生误差, 导致预测中心点位置可能在目标轮廓外。为了避免少数错误预测的干扰, 采用自下而上的层次聚类法, 以点之间的欧氏距离为相似度测量, 对<i>M</i>中所有预测中心点进行聚类, 得到子集<i>Q</i><sub>1</sub>, <i>Q</i><sub>2</sub>, …, <i>Q</i><sub><i>n</i></sub>。子集数量<i>n</i>由截断阈值<i>σ</i>决定, 最终选择元素数量最多的子集对中心点进行预测, 并取子集中所有元素的均值作为重检测器的检测结果。</p>
                </div>
                <h3 id="136" name="136" class="anchor-tag">4 实验及结果分析</h3>
                <h4 class="anchor-tag" id="137" name="137"><b>4.1</b><b>实验参数与数据</b></h4>
                <div class="p1">
                    <p id="138">实验环境为Intel i5-4590@3.30 GHz, 算法开发采用Matlab与OpenCV库接口。实验参数设置为:<i>C</i>=10<sup>4</sup>, 样本标签分类的上下限阈值<i>δ</i><sub>u</sub>=0.3, <i>δ</i><sub>l</sub>=0.6;置信度函数尺度参数<i>χ</i>=0.5, 形状参数<i>β</i>=2;模型更新时的参数学习率<i>γ</i>=0.025;跟踪可信度阈值<i>δ</i><sub>APCE</sub>=10;层次聚类截断阈值<i>σ</i>=4。</p>
                </div>
                <div class="p1">
                    <p id="139">选择公开数据库OTB-13<citation id="182" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>和OTB-15<citation id="183" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>进行实验。OTB-13数据库中包含50段视频序列, 可以分为11种不同的视觉跟踪挑战场景属性, 如尺度变化、目标遮挡、光照变化、运动模糊、物体形变等, 每段视频序列可能有多个属性。OTB-15数据库是在OTB-13数据库的基础上增加50组测试视频序列得到的。增加的序列场景属性更为复杂, 跟踪难度更大。</p>
                </div>
                <div class="p1">
                    <p id="140">选择评价准则OPE (One-Pass Evaluation) 计算跟踪准确率和成功率, 以评估跟踪性能。准确率为跟踪中心点位置误差小于阈值的帧数<i>F</i><sub>center</sub>占总体视频帧数<i>F</i><sub>total</sub>的比例, 即<i>F</i><sub>center</sub>/<i>F</i><sub>total</sub>, 成功率则表示跟踪重叠率 (交并比) 大于阈值的帧数<i>F</i><sub>overlap</sub>占比, 即<i>F</i><sub>overlap</sub>/<i>F</i><sub>total</sub>。</p>
                </div>
                <h4 class="anchor-tag" id="141" name="141"><b>4.2</b><b>连续<i>K</i>帧关键点影响分析</b></h4>
                <div class="p1">
                    <p id="142">目标关键点数据集<i>T</i>对目标重检测结果有着直接影响, <i>T</i>是由连续<i>K</i>帧目标关键点子集组成的, 因此<i>K</i>的取值对跟踪结果有一定影响。通过实验比较不同<i>K</i>值下跟踪算法的平均距离精度 (DP) 和平均重叠率精度 (OP) , 即不同位置误差阈值和重叠率阈值下平均跟踪准确率和成功率, 实验结果如表1所示。</p>
                </div>
                <div class="p1">
                    <p id="143">由表1可知, 数据库为OTB-13时, <i>K</i>=1时跟踪准确率和跟踪精度都更高, 当数据库扩展为OTB-15时, 虽然<i>K</i>=1与<i>K</i>=3时跟踪准确率相同且均优于其他取值, 但是由于<i>K</i>=1时OTB-13数据库效果较好, 说明<i>K</i>=1时100个视频帧的平均准确率波动更大, 因此数据库为OTB-15时, 取<i>K</i>=3。</p>
                </div>
                <div class="area_img" id="144">
                    <p class="img_tit">表1 不同<i>K</i>值下所提算法在不同数据库上的平均DP和平均OP <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Mean DP and OP on different databases for proposed algorithm under different values of <i>K</i></p>
                    <p class="img_note"></p>
                    <table id="144" border="1"><tr><td rowspan="2"><br />Value of <i>K</i></td><td colspan="2"><br />Mean DP</td><td rowspan="2"></td><td colspan="2"><br />Mean OP</td></tr><tr><td><br />OTB-13</td><td>OTB-15</td><td><br />OTB-13</td><td>OTB-15</td></tr><tr><td>1</td><td>0.885</td><td>0.796</td><td></td><td>0.642</td><td>0.569</td></tr><tr><td><br />2</td><td>0.871</td><td>0.788</td><td></td><td>0.630</td><td>0.562</td></tr><tr><td><br />3</td><td>0.874</td><td>0.796</td><td></td><td>0.633</td><td>0.569</td></tr><tr><td><br />4</td><td>0.871</td><td>0.795</td><td></td><td>0.632</td><td>0.568</td></tr><tr><td><br />5</td><td>0.871</td><td>0.795</td><td></td><td>0.633</td><td>0.567</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="146" name="146"><b>4.3</b><b>跟踪算法性能比较</b></h4>
                <div class="p1">
                    <p id="147">为验证所提跟踪算法的性能, 选取8种近年来出现的效果良好的跟踪算法CSK<citation id="184" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation> (Circulant Structure of Tracking) 、KCF<citation id="185" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、DSST<citation id="186" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation> (Discriminative Scale Space Tracking) 、SKSCF<citation id="187" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation> (Scale Kernel Support Correlation Filter) 、LCT<citation id="188" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、BACF<citation id="189" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation> (Background-Aware Correlation Filter) 、SRDCF<citation id="190" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation> (Spatially Regularized Correlation Filters) 、STAPLE<citation id="191" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation> (Sum of Template And Pixel-wise LEarners) 进行对比, 这些算法都是基于相关滤波或相关滤波改进的算法。表2列出了9种跟踪算法在相关数据集上运行的平均每秒传输帧数 (FPS) 、平均DP和平均OP。图3则给出了9种算法分别在数据库OTB-13和OTB-15上相应的跟踪成功率曲线和跟踪准确率曲线。</p>
                </div>
                <div class="area_img" id="148">
                    <p class="img_tit">表2 9种跟踪算法在不同数据库上的平均跟踪性能参数 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Mean tracking performance parameters on different databases for nine kinds of tracking algorithms</p>
                    <p class="img_note"></p>
                    <table id="148" border="1"><tr><td rowspan="2">Algorithm</td><td rowspan="2">Mean FPS (OTB-15) / (frame·s<sup>-1</sup>) </td><td colspan="2"><br />Mean DP</td><td rowspan="2"></td><td colspan="2"><br />Mean OP</td></tr><tr><td><br />OTB-13</td><td>OTB-15</td><td><br />OTB-13</td><td>OTB-15</td></tr><tr><td>CSK</td><td>440.41</td><td>0.545</td><td>0.518</td><td></td><td>0.398</td><td>0.382</td></tr><tr><td><br />KCF</td><td>98.49</td><td>0.704</td><td>0.675</td><td></td><td>0.500</td><td>0.470</td></tr><tr><td><br />DSST</td><td>46.44</td><td>0.737</td><td>0.693</td><td></td><td>0.554</td><td>0.520</td></tr><tr><td><br />LCT</td><td>24.18</td><td>0.848</td><td>0.762</td><td></td><td>0.593</td><td>0.527</td></tr><tr><td><br />SKSCF</td><td>39.29</td><td>0.864</td><td>0.772</td><td></td><td>0.623</td><td>0.549</td></tr><tr><td><br />STAPLE</td><td>76.70</td><td>0.782</td><td>0.784</td><td></td><td>0.593</td><td>0.578</td></tr><tr><td><br />SRDCF</td><td>5.42</td><td>0.838</td><td>0.789</td><td></td><td>0.626</td><td>0.598</td></tr><tr><td><br />BACF</td><td>32.12</td><td>0.849</td><td>0.817</td><td></td><td>0.645</td><td>0.616</td></tr><tr><td><br />Proposed</td><td>29.83</td><td>0.885</td><td>0.796</td><td></td><td>0.642</td><td>0.569</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="149">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902032_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 9种算法在不同数据库上的跟踪结果。 OTB-13数据库上的 (a) 跟踪成功率和 (b) 跟踪准确率;OTB-15数据库上的 (c) 跟踪成功率和 (d) 跟踪准确率" src="Detail/GetImg?filename=images/GXXB201902032_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 9种算法在不同数据库上的跟踪结果。 OTB-13数据库上的 (a) 跟踪成功率和 (b) 跟踪准确率;OTB-15数据库上的 (c) 跟踪成功率和 (d) 跟踪准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902032_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Tracking results for nine kinds of trackers on different databases. (a) Tracking success rate and (b) tracking precision for OTB-13 database; (c) tracking success rate and (d) tracking precision for OTB-15 database</p>

                </div>
                <div class="p1">
                    <p id="150">结合表2数据与图3中曲线可知, 在OTB-13数据库上, 所提算法的跟踪成功率接近BACF算法, 且高于其他算法。所提算法的跟踪精度也优于其他8种算法。在OTB-15数据库上, 所提算法的跟踪成功率接近STAPLE算法, 低于针对边界效应问题的BACF、SRDCF算法, 但是在跟踪精度上仅次于BACF算法而优于其他跟踪算法。由表2可知, 在统一的实验环境下, 所提算法在平均跟踪速度上能够达到29.83 frame·s<sup>-1</sup>, 与BACF算法相近, 明显高于SRDCF算法, 能基本满足跟踪实时性要求。</p>
                </div>
                <div class="p1">
                    <p id="151">为充分对比各个算法针对不同场景的性能差异, 将OTB-13数据库中的视频序列按照场景属性分别进行跟踪成功率比较, 结果如图4所示。从图4可知, 所提的长期跟踪算法在OTB-13数据库的大部分场景属性中均有良好表现, 尤其是在目标遮挡[图4 (c) ]、背景杂乱[图4 (h) ]、物体变形[图4 (j) ]和光照变化[图4 (k) ]等场景中。同时, 所提算法的滤波跟踪器基于SKSCF算法, 是尺度自适应的支持相关滤波算法。可以看出, 对跟踪结果进行判断并加入基于关键点匹配的重检测器后, 虽然跟踪速度有所下降, 但是算法的平均准确率和成功率均提升了约2%。图4则显示尤其针对存在出视野[图4 (d) ]、物体变形[图4 (j) ]和目标遮挡[图4 (c) ]属性的视频场景, 较SKSCF算法的平均跟踪成功率分别提高了5.2%、3.5%和3.3%。</p>
                </div>
                <div class="area_img" id="152">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902032_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 11种场景下各算法的跟踪成功率比较。 (a) 平面内旋转; (b) 低分辨率; (c) 目标遮挡; (d) 出视野; (e) 平面外旋转; (f) 尺度变换; (g) 快速运动; (h) 背景杂乱; (i) 运动模糊; (j) 物体变形; (k) 光照变化" src="Detail/GetImg?filename=images/GXXB201902032_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 11种场景下各算法的跟踪成功率比较。 (a) 平面内旋转; (b) 低分辨率; (c) 目标遮挡; (d) 出视野; (e) 平面外旋转; (f) 尺度变换; (g) 快速运动; (h) 背景杂乱; (i) 运动模糊; (j) 物体变形; (k) 光照变化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902032_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Comparison of tracking success rate for each algorithm under eleven kinds of scenes. (a) In-plane rotation; (b) low resolution; (c) object occlusion; (d) out of view; (e) out-of-plane rotation; (f) scale variation; (g) fast motion; (h) background clutter; (i) motion blur; (j) object deformation; (k) illumination variation</p>

                </div>
                <div class="p1">
                    <p id="154">为更加直观地显示不同算法的跟踪效果, 选取了跟踪精度较高的5种算法 (BACF、SRDCF、STAPLE、SKSCF和LCT) 与所提算法在6组视频序列中的跟踪结果进行对比, 如图5所示。序列①存在快速运动以及运动模糊现象, 实验结果显示经历多次快速运动和运动模糊 (0284、0624帧) 后, 只有所提算法和SRDCF算法能长期、准确地跟踪到目标;序列②在跟踪过程中存在密集的相似目标干扰和目标旋转的现象, 结果显示所提算法能排除环境干扰, 持续稳定地跟踪到目标;序列③、④、⑤均存在目标严重遮挡或消失, 序列④、⑤还伴随着人体目标形变的影响。结果显示, 所提算法在三组序列中都能实现稳定跟踪。其余5种算法, 只有BACF、SRDCF和LCT算法能在部分情况下跟踪到目标, 显示了所提算法在目标遮挡和目标消失情况下稳定的跟踪性能; 序列⑥不仅有目标遮挡、运动模糊、物体形变等属性, 还存在着光照变化的影响 (0278帧) , 在这种光照条件下, 所提算法依然能长期、稳定地跟踪到目标, 说明了所提算法对光照条件变化的稳健性。</p>
                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902032_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 6种跟踪算法在部分序列上的跟踪结果图。 (a) BlurOwl; (b) Liquor; (c) Box; (d) Jogging 1; (e) Jogging 2; (f) Tigger" src="Detail/GetImg?filename=images/GXXB201902032_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 6种跟踪算法在部分序列上的跟踪结果图。 (a) BlurOwl; (b) Liquor; (c) Box; (d) Jogging 1; (e) Jogging 2; (f) Tigger  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902032_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Tracking result graphs in partial sequences for six kinds of tracking algorithms. (a) BlurOwl; (b) Liquor; (c) Box; (d) Jogging 1; (e) Jogging 2; (f) Tigger</p>

                </div>
                <h3 id="156" name="156" class="anchor-tag">5 结 论</h3>
                <div class="p1">
                    <p id="157">针对目标遮挡、消失、变形等严重干扰的长期目标跟踪问题, 提出一种融合支持相关滤波和关键点匹配的重检测长期跟踪策略。利用支持相关滤波训练多滤波模型, 减小随机误差, 并对跟踪结果进行验证;同时建立并更新包含目标和背景关键点时空信息的数据库, 当判断目标跟踪失败时, 根据关键点匹配原则对候选关键点进行分类, 对分类结果进行分析得到目标重检测结果。实验结果表明, 所提跟踪算法能有效提高目标跟踪的准确性和稳健性, 尤其对于目标消失等严重的干扰场景具有很好的性能。所提算法的跟踪速度能基本满足实时性要求, 且仍有可提升空间。但所提算法没有考虑相关滤波模型引起的边界效应, 对在视点边缘的目标跟踪有一定影响。下一步工作需要考虑加入边界效应的影响, 从而进一步提高跟踪效果。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061600106283&amp;v=MDY2MzAvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SktGNGRiaHM9TmlmT2ZiSzhIdGZOcVk5Rlplc0pEblE2b0JNVDZUNFBRSA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Vojir T, Noskova J, Matas J. Robust scale-adaptive mean-shift for tracking[J]. Pattern Recognition Letters, 2014, 49: 250-258.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">

                                <b>[2]</b> Bolme D S, Beveridge J R, Draper B A, <i>et al</i>. Visual object tracking using adaptive correlation filters[C]. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010: 2544-2550.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" >
                                    <b>[3]</b>
                                 Henriques J F, Caseiro R, Martins P, <i>et al</i>. High-speed tracking with kernelized correlation filters[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) : 583-596.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative scale space tracking">

                                <b>[4]</b> Danelljan M, Häger G, Khan F S, <i>et al</i>. Discriminative scale space tracking[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 39 (8) : 1561-1575.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A scale adaptive kernel correlation filter tracker with feature integration">

                                <b>[5]</b> Li Y, Zhu J K. A scale adaptive kernel correlation filter tracker with feature integration[C]. European Conference on Computer Vision, 2014: 254-265.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time part-based visual tracking via adaptive correlation filters">

                                <b>[6]</b> Liu T, Wang G, Yang Q X. Real-time part-based visual tracking via adaptive correlation filters[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 4902-4912.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structural Correlation Filter for Robust Visual Tracking">

                                <b>[7]</b> Liu S, Zhang T Z, Cao X C, <i>et al</i>. Structural correlation filter for robust visual tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016: 4312-4320.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Support Correlation Filters for Visual Tracking">

                                <b>[8]</b> Zuo W M, Wu X H, Lin L, <i>et al</i>. Learning support correlation filters for visual tracking[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018: 2829180.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tracking-Learning-Detection">

                                <b>[9]</b> Kalal Z, Mikolajczyk K, Matas J. Tracking-learning-detection[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (7) : 1409-1422.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Self-paced learning for long-term tracking">

                                <b>[10]</b> Supan■i■ III J S, Ramanan D. Self-paced learning for Long-term tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2013: 2379-2386.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond Local Search:Tracking Objects Everywhere with Instance-Specific Proposals">

                                <b>[11]</b> Zhu G, Porikli F, Li H D. Beyond local search: tracking objects everywhere with instance-specific proposals[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016: 943-951.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Collaborative Correlation Tracking[C/OL]">

                                <b>[12]</b> Zhu G B, Wang J Q, Wu Y, <i>et al</i>. Collaborative correlation tracking[C]. Proceedings of the British Machine Vision Conference, 2015: 184.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long-term correlation tracking">

                                <b>[13]</b> Ma C, Yang X K, Zhang C Y, <i>et al</i>. Long-term correlation tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 5388-5396.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large Margin Object Tracking with Circulant Feature Maps">

                                <b>[14]</b> Wang M M, Liu Y, Huang Z Y. Large margin object tracking with circulant feature maps[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2017: 4800-4808.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive color attributes for Real-Time visual tracking">

                                <b>[15]</b> Danelljan M, Khan F S, Felsberg M, <i>et al</i>. Adaptive color attributes for real-time visual tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2014: 1090-1097.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive correlation filters with long-term and short-term memory for object tracking">

                                <b>[16]</b> Ma C, Huang J B, Yang X K, <i>et al</i>. Adaptive correlation filters with long-term and short-term memory for object tracking[J]. International Journal of Computer Vision, 2018, 126 (8) : 771-796.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Brisk:Binary robust invariant scalable keypoints">

                                <b>[17]</b> Leutenegger S, Chli M, Siegwart R Y. BRISK: binary robust invariant scalable keypoints[C]. International Conference on Computer Vision, 2011: 2548-2555.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering of Static-Adaptive Correspondences for Deformable Object Tracking">

                                <b>[18]</b> Nebehay G, Pflugfelder R. Clustering of static-adaptive correspondences for deformable object tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 2784-2791.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online object tracking:A benchmark">

                                <b>[19]</b> Wu Y, Lim J, Yang M H. Online object tracking: a benchmark[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2013: 2411-2418.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">

                                <b>[20]</b> Wu Y, Lim J, Yang M H. Object tracking benchmark[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) : 1834-1848.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">

                                <b>[21]</b> Henriques J F, Caseiro R, Martins P, <i>et al</i>. Exploiting the circulant structure of tracking-by-detection with kernels[M]//Fitzgibbon A, Lazebnik S, Perona P, <i>et al</i>. Computer Vision-ECCV 2012. Heidelberg: Springer, 2012, 7575: 702-715.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Background-Aware Correlation Filters for Visual Tracking">

                                <b>[22]</b> Galoogahi H K, Fagg A, Lucey S. Learning background-aware correlation filters for visual tracking[C]. IEEE International Conference on Computer Vision (ICCV) , 2017: 1144-1152.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">

                                <b>[23]</b> Danelljan M, Häger G, Khan F S, <i>et al</i>. Learning spatially regularized correlation filters for visual tracking[C]. IEEE International Conference on Computer Vision (ICCV) , 2015: 4310-4318.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">

                                <b>[24]</b> Bertinetto L, Valmadre J, Golodetz S, <i>et al</i>. Staple: complementary learners for real-time tracking[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016: 1401-1409.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201902032" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902032&amp;v=MjY3MjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURsV3J2T0lqWFRiTEc0SDlqTXJZOUdab1FLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

