

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135536070412500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201902035%26RESULT%3d1%26SIGN%3d%252fiyL1LpDHbqTuceb0Xi91wTICwk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201902035&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201902035&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902035&amp;v=MDM4NTQ5R1lZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRG1Vci9KSWpYVGJMRzRIOWpNclk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#81" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#86" data-title="2 自监督立体匹配算法 ">2 自监督立体匹配算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#103" data-title="3 损失函数 ">3 损失函数</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#107" data-title="&lt;b&gt;3.1&lt;/b&gt;&lt;b&gt;图像重构损失&lt;/b&gt;"><b>3.1</b><b>图像重构损失</b></a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;3.2&lt;/b&gt;&lt;b&gt;平滑性约束损失&lt;/b&gt;"><b>3.2</b><b>平滑性约束损失</b></a></li>
                                                <li><a href="#135" data-title="&lt;b&gt;3.3&lt;/b&gt;&lt;b&gt;左右一致性损失&lt;/b&gt;"><b>3.3</b><b>左右一致性损失</b></a></li>
                                                <li><a href="#144" data-title="&lt;b&gt;3.4&lt;/b&gt;&lt;b&gt;共同视域的确定&lt;/b&gt;"><b>3.4</b><b>共同视域的确定</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#162" data-title="4 实 验 ">4 实 验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#172" data-title="&lt;b&gt;4.1&lt;/b&gt;&lt;b&gt;自改善能力分析&lt;/b&gt;"><b>4.1</b><b>自改善能力分析</b></a></li>
                                                <li><a href="#178" data-title="&lt;b&gt;4.2&lt;/b&gt;&lt;b&gt;微调能力分析&lt;/b&gt;"><b>4.2</b><b>微调能力分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#189" data-title="5 结 论 ">5 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#89" data-title="图1 自监督立体匹配算法示意图">图1 自监督立体匹配算法示意图</a></li>
                                                <li><a href="#165" data-title="表1 不同视差预测模型的运行时间">表1 不同视差预测模型的运行时间</a></li>
                                                <li><a href="#166" data-title="图2 不同视差预测模型GPU显存占用情况">图2 不同视差预测模型GPU显存占用情况</a></li>
                                                <li><a href="#174" data-title="表2 随机初始化模型训练完成后对不同数据集进行评价的结果对比">表2 随机初始化模型训练完成后对不同数据集进行评价的结果对比</a></li>
                                                <li><a href="#175" data-title="图3 随机初始化模型训练过程中对KITTI2012训练集进行评价的结果对比。 (a) 训练损失; (b) D1; (c) EPE; (d) RPE">图3 随机初始化模型训练过程中对KITTI2012训练集进行评价的结果对比。 (a) 训练损失; (......</a></li>
                                                <li><a href="#180" data-title="表3 预训练模型微调后对不同数据集进行评价的结果对比">表3 预训练模型微调后对不同数据集进行评价的结果对比</a></li>
                                                <li><a href="#181" data-title="图4 预训练模型微调过程中对KITTI2012训练集进行评价的结果对比。 (a) 训练损失; (b) D1; (c) EPE; (d) RPE">图4 预训练模型微调过程中对KITTI2012训练集进行评价的结果对比。 (a) 训练损失; (b)......</a></li>
                                                <li><a href="#186" data-title="图5 KITTI2015训练集中的两组图像。 (a) 第一组图像的左视角图像; (b) 第一组图像的视差预测值; (c) 第二组图像的左视角图像; (d) 第二组图像的视差预测值">图5 KITTI2015训练集中的两组图像。 (a) 第一组图像的左视角图像; (b) 第一组图像的......</a></li>
                                                <li><a href="#187" data-title="图6 预训练模型微调后的实际应用效果对比。 (a) 第一组图像的误差图; (b) 第一组图像的视差预测值; (c) 第二组图像的误差图; (d) 第二组图像的视差预测值">图6 预训练模型微调后的实际应用效果对比。 (a) 第一组图像的误差图; (b) 第一组图像的视差预......</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="9">


                                    <a id="bibliography_1" title=" Liu C, Yuen J, Torralba A. SIFT flow: dense correspondence across scenes and its applications[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) : 978-994." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SIFT flow: Dense correspondence across scenes and its applications">
                                        <b>[1]</b>
                                         Liu C, Yuen J, Torralba A. SIFT flow: dense correspondence across scenes and its applications[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) : 978-994.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_2" title=" Liu Y F, Cai Z J. Binocular stereo vision three-dimensional reconstruction algorithm based on ICP and SFM[J]. Laser and Optoelectronics Progress, 2018, 55 (9) : 091503. 刘一凡, 蔡振江. 基于ICP与SFM的双目立体视觉三维重构算法[J]. 激光与光电子学进展, 2018, 55 (9) : 091503." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201809038&amp;v=MTkxMTE0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURtVXIvSkx5clBaTEc0SDluTXBvOUdiSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Liu Y F, Cai Z J. Binocular stereo vision three-dimensional reconstruction algorithm based on ICP and SFM[J]. Laser and Optoelectronics Progress, 2018, 55 (9) : 091503. 刘一凡, 蔡振江. 基于ICP与SFM的双目立体视觉三维重构算法[J]. 激光与光电子学进展, 2018, 55 (9) : 091503.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_3" title=" Sivaraman S, Trivedi M M. A review of recent developments in vision-based vehicle detection[C]. IEEE Intelligent Vehicles Symposium (IV) , 2013: 310-315." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Review of Recent Developments in Vision-Based Vehicle Detection">
                                        <b>[3]</b>
                                         Sivaraman S, Trivedi M M. A review of recent developments in vision-based vehicle detection[C]. IEEE Intelligent Vehicles Symposium (IV) , 2013: 310-315.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_4" title=" Schmid K, Tomic T, Ruess F, &lt;i&gt;et al&lt;/i&gt;. Stereo vision based indoor/outdoor navigation for flying robots[C]. IEEE/RSJ International Conference on Intelligent Robots and Systems, 2013: 3955-3962." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stereo Vision Based Indoor/Outdoor Navigation for Flying Robots">
                                        <b>[4]</b>
                                         Schmid K, Tomic T, Ruess F, &lt;i&gt;et al&lt;/i&gt;. Stereo vision based indoor/outdoor navigation for flying robots[C]. IEEE/RSJ International Conference on Intelligent Robots and Systems, 2013: 3955-3962.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_5" title=" Scharstein D, Szeliski R, Zabih R. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J]. International Journal of Computer Vision, 2002, 47 (1) : 7-42." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830724&amp;v=MzIwNzVPcDR4Rlkra0xZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZpdmtXN3pKSUY4PU5qN0Jhck80SHRI&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Scharstein D, Szeliski R, Zabih R. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J]. International Journal of Computer Vision, 2002, 47 (1) : 7-42.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_6" title=" Lowe D G. Distinctive image features from scale-invariant keypoints[J]. International Journal of Computer Vision, 2004, 60 (2) : 91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MTg1NjZPNEh0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rml2a1c3ekpJRjg9Tmo3QmFy&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Lowe D G. Distinctive image features from scale-invariant keypoints[J]. International Journal of Computer Vision, 2004, 60 (2) : 91-110.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_7" title=" Dalal N, Triggs B. Histograms of oriented gradients for human detection[C]. IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) , 2005: 886-893." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">
                                        <b>[7]</b>
                                         Dalal N, Triggs B. Histograms of oriented gradients for human detection[C]. IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) , 2005: 886-893.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_8" title=" Hirschmuller H. Stereo processing by semiglobal matching and mutual information[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008, 30 (2) : 328-341." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stereo processing by semiglobal matching and mutual information">
                                        <b>[8]</b>
                                         Hirschmuller H. Stereo processing by semiglobal matching and mutual information[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008, 30 (2) : 328-341.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_9" title=" Liu D W, Han L, Han X Y. High spatial resolution remote sensing image classification based on deep learning[J]. Acta Optica Sinica, 2016, 36 (4) : 0428001. 刘大伟, 韩玲, 韩晓勇. 基于深度学习的高分辨率遥感影像分类研究[J]. 光学学报, 2016, 36 (4) : 0428001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604039&amp;v=MTcxNzBmTXE0OUdiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURtVXIvSklqWFRiTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Liu D W, Han L, Han X Y. High spatial resolution remote sensing image classification based on deep learning[J]. Acta Optica Sinica, 2016, 36 (4) : 0428001. 刘大伟, 韩玲, 韩晓勇. 基于深度学习的高分辨率遥感影像分类研究[J]. 光学学报, 2016, 36 (4) : 0428001.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_10" title=" Hou Y Q Y, Quan J C, Wei Y M. Valid aircraft detection system for remote sensing images based on cognitive models[J]. Acta Optica Sinica, 2018, 38 (1) : 0111005. 侯宇青阳, 全吉成, 魏湧明. 基于认知模型的遥感图像有效飞机检测系统[J]. 光学学报, 2018, 38 (1) : 0111005." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201801018&amp;v=MjgwNjU3cWZadVp0RmlEbVVyL0pJalhUYkxHNEg5bk1ybzlFYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Hou Y Q Y, Quan J C, Wei Y M. Valid aircraft detection system for remote sensing images based on cognitive models[J]. Acta Optica Sinica, 2018, 38 (1) : 0111005. 侯宇青阳, 全吉成, 魏湧明. 基于认知模型的遥感图像有效飞机检测系统[J]. 光学学报, 2018, 38 (1) : 0111005.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_11" title=" Shelhamer E, Long J, Darrell T. Fully convolutional networks for semantic segmentation[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) : 640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[11]</b>
                                         Shelhamer E, Long J, Darrell T. Fully convolutional networks for semantic segmentation[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) : 640-651.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_12" title=" Žbontar J, LeCun Y. Computing the stereo matching cost with a convolutional neural network[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 1592-1599." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Computing the stereo matching cost with a convolutional neural network">
                                        <b>[12]</b>
                                         Žbontar J, LeCun Y. Computing the stereo matching cost with a convolutional neural network[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 1592-1599.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_13" title=" Žbontar J, LeCun Y. Stereo matching by training a convolutional neural network to compare image patches[J]. Journal of Machine Learning Research, 2016, 17 (1) : 2287-2318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stereo matching by training a convolutional neural network to compare image patches">
                                        <b>[13]</b>
                                         Žbontar J, LeCun Y. Stereo matching by training a convolutional neural network to compare image patches[J]. Journal of Machine Learning Research, 2016, 17 (1) : 2287-2318.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_14" title=" Xiao J S, Tian H, Zou W T, &lt;i&gt;et al&lt;/i&gt;. Stereo matching based on convolutional neural network[J]. Acta Optica Sinica, 2018, 38 (8) : 0815017. 肖进胜, 田红, 邹文涛, 等. 基于深度卷积神经网络的双目立体视觉匹配算法[J]. 光学学报, 2018, 38 (8) : 0815017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201808018&amp;v=MjExMzFNcDQ5RWJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRG1Vci9KSWpYVGJMRzRIOW4=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Xiao J S, Tian H, Zou W T, &lt;i&gt;et al&lt;/i&gt;. Stereo matching based on convolutional neural network[J]. Acta Optica Sinica, 2018, 38 (8) : 0815017. 肖进胜, 田红, 邹文涛, 等. 基于深度卷积神经网络的双目立体视觉匹配算法[J]. 光学学报, 2018, 38 (8) : 0815017.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_15" title=" Chen Z Y, Sun X, Wang L, &lt;i&gt;et al&lt;/i&gt;. A deep visual correspondence embedding model for stereo matching costs[C]. IEEE International Conference on Computer Vision, 2015: 972-980." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Deep Visual Correspondence Embedding Model for Stereo Matching Costs">
                                        <b>[15]</b>
                                         Chen Z Y, Sun X, Wang L, &lt;i&gt;et al&lt;/i&gt;. A deep visual correspondence embedding model for stereo matching costs[C]. IEEE International Conference on Computer Vision, 2015: 972-980.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_16" title=" Park H, Lee K M. Look wider to match image patches with convolutional neural networks[J]. IEEE Signal Processing Letters, 2017, 24 (12) : 1788-1792." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Look Wider to Match Image Patches with Convolutional Neural Networks">
                                        <b>[16]</b>
                                         Park H, Lee K M. Look wider to match image patches with convolutional neural networks[J]. IEEE Signal Processing Letters, 2017, 24 (12) : 1788-1792.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_17" title=" G&#252;ney F, Geiger A. Displets: resolving stereo ambiguities using object knowledge[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 4165-4175." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Displets:Resolving stereo ambiguities using object knowledge">
                                        <b>[17]</b>
                                         G&#252;ney F, Geiger A. Displets: resolving stereo ambiguities using object knowledge[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 4165-4175.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_18" title=" Seki A, Pollefeys M. Patch based confidence prediction for dense disparity map[C]. British Machine Vision Conference (BMVC) , 2016: 23." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Patch Based Confidence Prediction for Dense Disparity Map">
                                        <b>[18]</b>
                                         Seki A, Pollefeys M. Patch based confidence prediction for dense disparity map[C]. British Machine Vision Conference (BMVC) , 2016: 23.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_19" title=" Mayer N, Ilg E, H&#228;usser P, &lt;i&gt;et al&lt;/i&gt;. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 4040-4048." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A large dataset to train convolutional networks for disparity,optical flow,and scene flow estimation">
                                        <b>[19]</b>
                                         Mayer N, Ilg E, H&#228;usser P, &lt;i&gt;et al&lt;/i&gt;. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 4040-4048.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_20" title=" Pang J H, Sun W X, Ren J S, &lt;i&gt;et al&lt;/i&gt;. Cascade residual learning: a two-stage convolutional neural network for stereo matching[C]. IEEE International Conference on Computer Vision Workshops, 2017: 887-895." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cascade Residual Learning A Two-Stage Convolutional Neural Network for Stereo Matching">
                                        <b>[20]</b>
                                         Pang J H, Sun W X, Ren J S, &lt;i&gt;et al&lt;/i&gt;. Cascade residual learning: a two-stage convolutional neural network for stereo matching[C]. IEEE International Conference on Computer Vision Workshops, 2017: 887-895.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_21" title=" Liang Z F, Feng Y L, Guo Y L, &lt;i&gt;et al&lt;/i&gt;. Learning deep correspondence through prior and posterior feature constancy[EB/OL]. (2017-12-04) [2018-06-05]. http://cn.arxiv.org/abs/1712.01039." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep correspondence through prior and posterior feature constancy">
                                        <b>[21]</b>
                                         Liang Z F, Feng Y L, Guo Y L, &lt;i&gt;et al&lt;/i&gt;. Learning deep correspondence through prior and posterior feature constancy[EB/OL]. (2017-12-04) [2018-06-05]. http://cn.arxiv.org/abs/1712.01039.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_22" title=" Kendall A, Martirosyan H, Dasgupta S, &lt;i&gt;et al&lt;/i&gt;. End-to-end learning of geometry and context for deep stereo regression[C]. IEEE International Conference on Computer Vision, 2017: 66-75." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Endto-end learning of geometry and context for deep stereo regression">
                                        <b>[22]</b>
                                         Kendall A, Martirosyan H, Dasgupta S, &lt;i&gt;et al&lt;/i&gt;. End-to-end learning of geometry and context for deep stereo regression[C]. IEEE International Conference on Computer Vision, 2017: 66-75.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_23" title=" Yu L D, Wang Y C, Wu Y W, &lt;i&gt;et al&lt;/i&gt;. Deep stereo matching with explicit cost aggregation sub-architecture[EB/OL]. (2018-01-12) [2018-06-05]. http://cn.arxiv.org/abs/1801.04065." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep stereo matching with explicit cost aggregation sub-architecture">
                                        <b>[23]</b>
                                         Yu L D, Wang Y C, Wu Y W, &lt;i&gt;et al&lt;/i&gt;. Deep stereo matching with explicit cost aggregation sub-architecture[EB/OL]. (2018-01-12) [2018-06-05]. http://cn.arxiv.org/abs/1801.04065.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_24" title=" Chang J R, Chen Y S. Pyramid stereo matching network[EB/OL]. (2018-03-23) [2018-06-05]. http://cn.arxiv.org/abs/1803.08669." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pyramid stereo matching network">
                                        <b>[24]</b>
                                         Chang J R, Chen Y S. Pyramid stereo matching network[EB/OL]. (2018-03-23) [2018-06-05]. http://cn.arxiv.org/abs/1803.08669.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_25" title=" Xie J Y, Girshick R, Farhadi A. Deep 3D: fully automatic 2D-to-3D video conversion with deep convolutional neural networks[C]. European Conference on Computer Vision (ECCV) , 2016: 842-857." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks">
                                        <b>[25]</b>
                                         Xie J Y, Girshick R, Farhadi A. Deep 3D: fully automatic 2D-to-3D video conversion with deep convolutional neural networks[C]. European Conference on Computer Vision (ECCV) , 2016: 842-857.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_26" title=" GargR, Vijay K B G, Carneiro G, &lt;i&gt;et al&lt;/i&gt;. Unsupervised CNN for single view depth estimation: Geometry to the rescue[C]. European Conference on Computer Vision (ECCV) , 2016: 740-756." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised CNN for single view depth estimation:Geometry to the rescue">
                                        <b>[26]</b>
                                         GargR, Vijay K B G, Carneiro G, &lt;i&gt;et al&lt;/i&gt;. Unsupervised CNN for single view depth estimation: Geometry to the rescue[C]. European Conference on Computer Vision (ECCV) , 2016: 740-756.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_27" title=" Godard C, Aodha O M, Brostow G J. Unsupervised monocular depth estimation with left-right consistency[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2017: 6602-6611." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised monocular depth estimation with leftright consistency">
                                        <b>[27]</b>
                                         Godard C, Aodha O M, Brostow G J. Unsupervised monocular depth estimation with left-right consistency[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2017: 6602-6611.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_28" title=" Jaderberg M, Simonyan K, Zisserman A, &lt;i&gt;et al&lt;/i&gt;. Spatial transformer networks[EB/OL]. (2015-06-05) [2018-06-05]. http://cn.arxiv.org/abs/1506.02025." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial transformer networks">
                                        <b>[28]</b>
                                         Jaderberg M, Simonyan K, Zisserman A, &lt;i&gt;et al&lt;/i&gt;. Spatial transformer networks[EB/OL]. (2015-06-05) [2018-06-05]. http://cn.arxiv.org/abs/1506.02025.
                                    </a>
                                </li>
                                <li id="65">


                                    <a id="bibliography_29" title=" Ye M L, Johns E, Handa A, &lt;i&gt;et al&lt;/i&gt;. Self-supervised siamese learning on stereo image pairs for depth estimation in robotic surgery[EB/OL]. (2017-05-17) [2018-06-05]. http://cn.arxiv.org/abs/1705.08260." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Self-supervised siamese learning on stereo image pairs for depth estimation in robotic surgery">
                                        <b>[29]</b>
                                         Ye M L, Johns E, Handa A, &lt;i&gt;et al&lt;/i&gt;. Self-supervised siamese learning on stereo image pairs for depth estimation in robotic surgery[EB/OL]. (2017-05-17) [2018-06-05]. http://cn.arxiv.org/abs/1705.08260.
                                    </a>
                                </li>
                                <li id="67">


                                    <a id="bibliography_30" title=" LiuX, Sinha A, Unberath M, &lt;i&gt;et al&lt;/i&gt;. Self-supervised learning for dense depth estimation in monocular endoscopy[C]. European Conference on Computer Vision (ECCV) , 2018, 11041: 128-138." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Self-supervised learning for dense depth estimation in monocular endoscopy">
                                        <b>[30]</b>
                                         LiuX, Sinha A, Unberath M, &lt;i&gt;et al&lt;/i&gt;. Self-supervised learning for dense depth estimation in monocular endoscopy[C]. European Conference on Computer Vision (ECCV) , 2018, 11041: 128-138.
                                    </a>
                                </li>
                                <li id="69">


                                    <a id="bibliography_31" title=" Zhong Y R, Dai Y C, Li H D. Self-supervised learning for stereo matching with self-improving ability[EB/OL]. (2017-09-04) [2018-06-05]. http://cn.arxiv.org/abs/1709.00930." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Self-supervised learning for stereo matching with self-improving ability">
                                        <b>[31]</b>
                                         Zhong Y R, Dai Y C, Li H D. Self-supervised learning for stereo matching with self-improving ability[EB/OL]. (2017-09-04) [2018-06-05]. http://cn.arxiv.org/abs/1709.00930.
                                    </a>
                                </li>
                                <li id="71">


                                    <a id="bibliography_32" >
                                        <b>[32]</b>
                                     Wang Z, Bovik A C, Sheikh H R, &lt;i&gt;et al&lt;/i&gt;. Image quality assessment: from error visibility to structural similarity[J]. IEEE Transactions on Image Processing, 2004, 13 (4) : 600-612.</a>
                                </li>
                                <li id="73">


                                    <a id="bibliography_33" title=" Geiger A, Lenz P, Urtasun R. Are we ready for autonomous driving? The KITTI vision benchmark suite[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2012: 3354-3361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Are we ready for autonomous driving?The KITTI vision benchmark suite,&amp;quot;">
                                        <b>[33]</b>
                                         Geiger A, Lenz P, Urtasun R. Are we ready for autonomous driving? The KITTI vision benchmark suite[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2012: 3354-3361.
                                    </a>
                                </li>
                                <li id="75">


                                    <a id="bibliography_34" title=" Menze M, Geiger A. Object scene flow for autonomous vehicles[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 3061-3070." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object scene flow for autonomous vehicles">
                                        <b>[34]</b>
                                         Menze M, Geiger A. Object scene flow for autonomous vehicles[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 3061-3070.
                                    </a>
                                </li>
                                <li id="77">


                                    <a id="bibliography_35" title=" wyf2017: Pytorch implementation of the several deep stereo matching network [EB/OL]. https://github.com/wyf2017/DSMnet." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=wyf2017: Pytorch implementation of the several deep stereo matching network">
                                        <b>[35]</b>
                                         wyf2017: Pytorch implementation of the several deep stereo matching network [EB/OL]. https://github.com/wyf2017/DSMnet.
                                    </a>
                                </li>
                                <li id="79">


                                    <a id="bibliography_36" title=" Kingma D P, Ba J. Adam: a method for stochastic optimization[EB/OL]. (2014-12-22) [2018-06-05]. http://cn.arxiv.org/abs/1412.6980." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam: a method for stochastic optimization">
                                        <b>[36]</b>
                                         Kingma D P, Ba J. Adam: a method for stochastic optimization[EB/OL]. (2014-12-22) [2018-06-05]. http://cn.arxiv.org/abs/1412.6980.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-10-20 11:54</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(02),287-296 DOI:10.3788/AOS201939.0215004            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于共同视域的自监督立体匹配算法</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E7%8E%89%E9%94%8B&amp;code=41275592&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王玉锋</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%AE%8F%E4%BC%9F&amp;code=20899251&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王宏伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E6%99%A8&amp;code=41275593&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴晨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%AE%87&amp;code=20481306&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A2%81%E6%98%B1%E7%BA%AC&amp;code=42150725&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">袁昱纬</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%85%A8%E5%90%89%E6%88%90&amp;code=41275594&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全吉成</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%B7%E5%86%9B%E8%88%AA%E7%A9%BA%E5%A4%A7%E5%AD%A6%E8%88%AA%E7%A9%BA%E4%BD%9C%E6%88%98%E5%8B%A4%E5%8A%A1%E5%AD%A6%E9%99%A2&amp;code=1701745&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">海军航空大学航空作战勤务学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A9%BA%E5%86%9B%E8%88%AA%E7%A9%BA%E5%A4%A7%E5%AD%A6%E8%88%AA%E7%A9%BA%E4%BD%9C%E6%88%98%E5%8B%A4%E5%8A%A1%E5%AD%A6%E9%99%A2&amp;code=1020414&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空军航空大学航空作战勤务学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A9%BA%E5%86%9B%E8%88%AA%E7%A9%BA%E5%A4%A7%E5%AD%A6%E9%A3%9E%E8%A1%8C%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=1743710&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空军航空大学飞行研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E8%A7%A3%E6%94%BE%E5%86%9B91977%E9%83%A8%E9%98%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国人民解放军91977部队</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出了一种基于共同视域的自监督立体匹配算法, 该算法根据视差的左右一致性来确定双目图像的共同可视区域, 从而抑制被遮挡区域产生的噪声, 为网络模型的学习提供了更加准确的反馈信号。研究结果表明:在没有任何标签数据的前提下, 所提算法的预测误差降低了11%～42%, 且与有监督立体匹配算法的性能相当。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">立体匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自监督学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E7%9B%AE%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双目视觉;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *全吉成, E-mail:jicheng_quan@126.com;;
                                </span>
                                <span>
                                    *王玉锋, E-mail:wangyf_1991@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-20</p>

            </div>
                    <h1>Self-Supervised Stereo Matching Algorithm Based on Common View</h1>
                    <h2>
                    <span>Wang Yufeng</span>
                    <span>Wang Hongwei</span>
                    <span>Wu Chen</span>
                    <span>Liu Yu</span>
                    <span>Yuan Yuwei</span>
                    <span>Quan Jicheng</span>
            </h2>
                    <h2>
                    <span>College of Operation Service on Aviation, University of Naval Aviation</span>
                    <span>College of Operation Service on Aviation, Aviation University of Air Force</span>
                    <span>Flight Institute, Aviation University of Air Force</span>
                    <span>The 91977 Troops of the PLA</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A self-supervised stereo matching algorithm is proposed based on common view. In this algorithm, the common visible region of the binocular images is determined according to the left-right consistency of disparity and thus the noise generated in the occluded region is suppressed, which provides more accurate feedback signals for the network model learning. The research results show that the prediction error of the proposed algorithm can be reduced by 11%-42% without any label data, and the performance of the proposed algorithm is comparable to that of the supervised stereo matching algorithm.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=stereo%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">stereo matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=self-supervised%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">self-supervised learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=binocular%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">binocular vision;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-07-20</p>
                            </div>


        <!--brief start-->
                        <h3 id="81" name="81" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="82">立体匹配就是从已校正的双目图像中获取密集的视差图, 是计算机视觉中的一个核心问题<citation id="191" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 广泛应用于三维重构<citation id="192" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、无人驾驶<citation id="193" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>及机器人导航<citation id="194" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等多种领域。根据Scharstein等<citation id="195" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的研究, 典型的立体匹配方法包含4个部分:匹配损失计算、匹配损失聚合、视差计算和视差微调。传统的立体匹配方法需要设计较好的像素描述子, 如SIFT (Scale-Invariant Feature Transform) <citation id="196" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、HOG (Histogram of Oriented Gradient) <citation id="197" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>等, 使用像素描述子之间的差异来计算匹配损失, 根据局部数据和平滑性约束进行全局优化<citation id="198" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="83">卷积神经网络 (CNN) 可以有效地理解语义信息, 在目标分类<citation id="199" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、目标检测<citation id="200" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和语义分割<citation id="201" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>等任务中取得了优异的性能, 在立体匹配算法中也广受关注<citation id="208" type="reference"><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>。Žbontar等<citation id="209" type="reference"><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>将其应用于计算两个9×9区块之间的相似度, 并通过一些传统的后处理步骤进行优化, 取得了良好的效果。随后, 这种方法不断地被改进, 通过改进表达相似度函数的神经网络结构来提高算法性能<citation id="210" type="reference"><link href="37" rel="bibliography" /><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>, 为进一步消除可靠性低的匹配, 建立自适应的平滑性约束、预测置信度等方法也取得了一定的成效<citation id="211" type="reference"><link href="41" rel="bibliography" /><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>。端到端的学习在全局优化上往往能获得更好的性能, Mayer等<citation id="202" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出一种“编码-解码”网络结构, 并创建了一个大型的合成数据集来进行视差的端到端学习。以此视差预测网络为基础, Pang等<citation id="203" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>通过级联另一个网络进行视差微调来提高精度。为更好地共享特征, Liang等<citation id="204" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提出一种融合立体匹配所有步骤的网络结构, 从多尺度共享特征学习先验和后验的不变特征, 并将CNN与贝叶斯推理相结合进行视差微调。从语义的上下文信息入手, Kendall等<citation id="205" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>使用3D卷积自编码器进行匹配损失计算, 再以可差分的函数进行亚像素的视差预测。在此基础上, Yu等<citation id="206" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>引入一个明确的损失聚合子网络进行优化损失计算, 使用双流网络分别进行损失聚合的提名和选择。Chang等<citation id="207" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>采用金字塔池化来提高对全局特征的抽象能力, 并用堆叠的三维卷积来处理语义的上下文信息。</p>
                </div>
                <div class="p1">
                    <p id="84">目前, 基于学习的视差预测方法大多使用真实视差作为监督信号, 训练过程需要大范围高质量的视差数据, 而这样的数据采集是一个极具挑战性的任务。对于双目图像, 深度预测和视差预测十分相似, 若已知相机内方位元素, 可以用明确的公式表达它们之间的关系。近年来, 在缺乏标签数据的情况下, 可以使用双目图像的重构误差来实现自监督学习, 这种方法利用视角合成技术, 根据预测的深度图和右视角图像生成左视角图像, 从而可以使用图像的重构误差来学习网络参数, 在单目图像的深度预测领域已经有一定的研究成果<citation id="218" type="reference"><link href="57" rel="bibliography" /><link href="59" rel="bibliography" /><link href="61" rel="bibliography" /><sup>[<a class="sup">25</a>,<a class="sup">26</a>,<a class="sup">27</a>]</sup></citation>。Xie等<citation id="212" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>采用一种可差分的方式从右视角图像重构左视角图像, 从而实现自监督的深度预测模型。在此基础上, Garg等<citation id="213" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>进一步引入平滑性约束损失来提高深度预测模型的稳定性。Godard等<citation id="214" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>采用空间变换网络 (STN) <citation id="215" type="reference"><link href="63" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>来实现可差分的图像重构, 并对左右视角图像进行同步预测, 同时引入左右一致性约束损失来提高预测精度。这种自监督的深度预测模型只需要双目图像形成自反馈, 不需要任何标签数据, 很容易从自然场景扩展到其他应用场景, 如医学领域的手术机器人<citation id="216" type="reference"><link href="65" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>、内窥镜检查<citation id="217" type="reference"><link href="67" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>等。</p>
                </div>
                <div class="p1">
                    <p id="85">为方便扩展模型的应用场景, 主要研究自监督立体匹配算法, 这种方法只利用传感器的原始输出数据即可学习到合理的视差预测模型。与所提算法最相似的是文献<citation id="219" type="reference">[<a class="sup">31</a>]</citation>的算法, 但该算法没有处理被遮挡的区域, 而这些区域无法形成有效的反馈, 反而是模型学习的噪声, 降低了算法的整体性能。因此, 所提算法通过检测被遮挡区域, 只在共同视域内计算图像的重构损失以降低模型学习的噪声, 从而提高自监督立体匹配算法的性能。</p>
                </div>
                <h3 id="86" name="86" class="anchor-tag">2 自监督立体匹配算法</h3>
                <div class="p1">
                    <p id="87">有监督立体匹配算法可以利用有标签的数据驱动模型学习, 合理的网络结构和大量的有标签数据是决定模型性能的关键, 而对于自监督立体匹配算法, 如何构建驱动模型学习的损失函数是问题的关键。自监督立体匹配算法基于左右视角图像的一致性, 使用重构误差来构建模型训练的反馈信号。视角合成技术是从其他视角图像重构当前视角图像, 为自监督立体匹配算法的实现提供了可能, 利用观察图像和重构图像之间的差异来驱动模型的学习。空间变换网络 (STN) <citation id="220" type="reference"><link href="63" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>能够实现可差分的图像采样, 是确保模型端到端学习的重要环节。</p>
                </div>
                <div class="p1">
                    <p id="88">如图1所示, 视差预测模型以双目图像为输入, 输出左图像的密集视差图, STN表达的重构函数以右图像和左视差图为输入, 输出重构的左图像。根据左图的观察值和重构值即可构建损失函数并驱动视差预测模型的学习, 这种回环反馈同样适用于右图像, 具体流程如图1所示。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902035_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 自监督立体匹配算法示意图" src="Detail/GetImg?filename=images/GXXB201902035_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 自监督立体匹配算法示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902035_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Schematic of self-supervised stereo matching algorithm</p>

                </div>
                <div class="p1">
                    <p id="90">设<b><i>I</i></b><sub>L</sub>, <b><i>I</i></b><sub>R</sub>分别为左、右视角图像矩阵, 则模型预测的左图像视差矩阵<b><i>D</i></b><sub>L</sub>可表示为</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext><mtext>m</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">θ</mi><mo>;</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>, </mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">式中:<i>F</i><sub>dsm</sub>为视差预测模型函数;<i>θ</i>为<i>F</i><sub>dsm</sub>的模型参数。同理可得右图像视差矩阵为</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>i</mtext><mtext>l</mtext><mtext>p</mtext></mrow></msub><mo stretchy="false">{</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext><mtext>m</mtext></mrow></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">θ</mi><mo>;</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>i</mtext><mtext>l</mtext><mtext>p</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false">) </mo><mo>, </mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>i</mtext><mtext>l</mtext><mtext>p</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">式中<i>F</i><sub>filp</sub>为对图像进行左右翻转的函数。</p>
                </div>
                <div class="p1">
                    <p id="95">使用STN对<b><i>I</i></b><sub>L</sub>, <b><i>I</i></b><sub>R</sub>, <b><i>D</i></b><sub>L</sub>, <b><i>D</i></b><sub>R</sub>进行重构, 得到</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Ι</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msubsup><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>Τ</mtext><mtext>Ν</mtext></mrow></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo>, </mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>r</mtext><mtext>i</mtext><mtext>d</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Ι</mi><msubsup><mrow></mrow><mtext>R</mtext><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msubsup><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>Τ</mtext><mtext>Ν</mtext></mrow></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>, </mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>r</mtext><mtext>i</mtext><mtext>d</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msubsup><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>Τ</mtext><mtext>Ν</mtext></mrow></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo>, </mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>r</mtext><mtext>i</mtext><mtext>d</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mtext>R</mtext><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msubsup><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>Τ</mtext><mtext>Ν</mtext></mrow></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>, </mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>r</mtext><mtext>i</mtext><mtext>d</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mtd></mtr></mtable></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">式中:<i>F</i><sub>STN</sub>为STN图像重构函数;<i>F</i><sub>grid</sub>表示根据视差生成重采样网格的函数;上标wrap表示使用STN得到的对应矩阵变量的重构。</p>
                </div>
                <div class="p1">
                    <p id="98">在没有任何遮挡的情况下, <b><i>I</i></b><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msubsup></mrow></math></mathml>, <b><i>I</i></b><mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mtext>R</mtext><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msubsup></mrow></math></mathml>和<b><i>I</i></b><sub>L</sub>, <b><i>I</i></b><sub>R</sub>应当相同, 以它们之间的差异、平滑性约束和左右一致性可构建损失函数:</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo>=</mo><mi>F</mi><msub><mrow></mrow><mtext>C</mtext></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>, </mo><mi mathvariant="bold-italic">Ι</mi><msubsup><mrow></mrow><mtext>R</mtext><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo>, </mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>, </mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo>, </mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mtext>R</mtext><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msubsup><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">式中<i>F</i><sub>C</sub>表示损失函数。</p>
                </div>
                <h3 id="103" name="103" class="anchor-tag">3 损失函数</h3>
                <div class="p1">
                    <p id="104">对于自监督立体匹配算法, 如何构建驱动模型学习的损失函数是问题的关键, 定义损失函数为</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>C</mi><mo>=</mo><mi>w</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext></mrow><mtext>L</mtext></msubsup><mo>+</mo><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext></mrow><mtext>R</mtext></msubsup><mo stretchy="false">) </mo><mo>+</mo><mi>w</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext></mrow><mtext>L</mtext></msubsup><mo>+</mo><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext></mrow><mtext>R</mtext></msubsup><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>w</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow><mtext>L</mtext></msubsup><mo>+</mo><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow><mtext>R</mtext></msubsup><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">式中:<i>w</i>表示权重, 用于权衡不同类型损失的重要性;下标ap、ds和lr分别为图像重构损失、平滑性约束损失和左右一致性约束损失。</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107"><b>3.1</b><b>图像重构损失</b></h4>
                <div class="p1">
                    <p id="108">结合图像的绝对误差和结构相似度, 定义基于图像颜色的重构损失函数为</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext></mrow></msub><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>v</mtext><mtext>g</mtext></mrow></msub><mrow><mo>[</mo><mrow><mi>α</mi><mfrac><mrow><mn>1</mn><mo>-</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>S</mtext><mtext>Ι</mtext><mtext>Μ</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msup><mo>, </mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">) </mo></mrow><mn>2</mn></mfrac><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>α</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msup><mo>-</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">|</mo><mo>+</mo><mo stretchy="false">|</mo><mo>∇</mo><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msup><mo>-</mo><mo>∇</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">|</mo><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">式中:<i>F</i><sub>avg</sub>为对矩阵中所有元素求均值的函数;<i>F</i><sub>SSIM</sub><citation id="221" type="reference"><link href="71" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>为图像的结构相似度函数;|·|表示对矩阵中各个元素求绝对值;∇表示对图像在空间位置进行一阶差分;<i>α</i>用来权衡两种图像重构损失的作用, <i>α</i>=0.85。</p>
                </div>
                <div class="p1">
                    <p id="111">由于被遮挡的范围不但不能形成有效的反馈, 而且会产生模型学习的噪声, 因此这些区域应当被排除, 设双目图像的共同视域由彩色图像的像素点在共同视域内的概率矩阵<b><i>M</i></b><sub>ap</sub>决定, 则结合共同视域的基于图像颜色的损失函数应为</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext></mrow><mtext>Μ</mtext></msubsup><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>v</mtext><mtext>g</mtext></mrow></msub><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">C</mi><mo>˜</mo></mover><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext></mrow></msub><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">式中<mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">C</mi><mo>˜</mo></mover><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext></mrow></msub></mrow></math></mathml>表示 (6) 式中求均值前的矩阵, 上标M表示只在共同视域内计算相应变量。</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115"><b>3.2</b><b>平滑性约束损失</b></h4>
                <div class="p1">
                    <p id="116">在无纹理或弱纹理的区域, 多种视差的图像重构误差都很小, 通常这些区域内部的视差变化很小, 根据视差的这种平滑性约束构造相应的损失函数, 可使驱动模型学习更加平滑的视差预测, 一定程度上克服纹理一致情况下的视差多值性。</p>
                </div>
                <div class="p1">
                    <p id="117">相对双目相机的焦平面, 相机视域内任意局部平面的深度值随像素位置线性变化, 深度值对像素位置的二阶差分为0, 在深度值突变的位置则为较大的模值。假设视差预测准确, 根据这种特性可以构建自适应的平滑性约束。</p>
                </div>
                <div class="p1">
                    <p id="118">像素点的视差和深度值满足</p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>×</mo><mi>Ζ</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>=</mo><mi>B</mi><mo>×</mo><mi>f</mi><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">式中:<i>p</i>为图像中一点的坐标;<i>d</i> (<i>p</i>) 为<i>p</i>点处的视差值;<i>Z</i> (<i>p</i>) 为<i>p</i>点处的深度值;<i>B</i>为双目相机的基线长度;<i>f</i>为相机焦距。</p>
                </div>
                <div class="p1">
                    <p id="121">对于图像上任意一点<i>p</i>都满足<i>d</i> (<i>p</i>) &gt;0, <i>Z</i> (<i>p</i>) &gt;0, 若<i>p</i>点附近为平面, 则</p>
                </div>
                <div class="p1">
                    <p id="122" class="code-formula">
                        <mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>Ζ</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow><mrow><mi>Ζ</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>Ζ</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow><mrow><mi>Ζ</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>-</mo><mn>2</mn><mo>=</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow><mrow><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow><mrow><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></mfrac><mo>-</mo><mn>2</mn><mo>=</mo><mn>0</mn><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="123">式中:<i>p</i>+1表示点坐标在二维平面内沿某个方向变化1个单位, <i>p</i>-1表示点坐标沿与<i>p</i>+1的相反方向变化1个单位。</p>
                </div>
                <div class="p1">
                    <p id="124">定义基于平面约束的平滑性约束损失为</p>
                </div>
                <div class="p1">
                    <p id="125" class="code-formula">
                        <mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext></mrow><mtext>p</mtext></msubsup><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>|</mo><mrow><mfrac><mrow><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow><mrow><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mrow><mrow><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></mfrac><mo>-</mo><mn>2</mn></mrow><mo>|</mo></mrow><mo>=</mo><mn>0</mn><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="126">式中<i>C</i><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext></mrow><mtext>p</mtext></msubsup></mrow></math></mathml> (p) 为p点处基于平面约束的平滑性约束损失值。</p>
                </div>
                <div class="p1">
                    <p id="128">通常, 在视差变化较大的边缘, 图像纹理的变化也比较明显。因此, 定义自适应的平滑性约束损失值为</p>
                </div>
                <div class="p1">
                    <p id="129" class="code-formula">
                        <mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>=</mo><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext></mrow><mtext>p</mtext></msubsup><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>⋅</mo><mi>exp</mi><mrow><mo>[</mo><mrow><mo>-</mo><mi>β</mi><mfrac><mrow><mo stretchy="false">|</mo><mo>∇</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo stretchy="false">|</mo></mrow><mrow><mi>F</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>v</mtext><mtext>g</mtext></mrow></msub><mo stretchy="false"> (</mo><mo stretchy="false">|</mo><mo>∇</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">|</mo><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>]</mo></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="130">式中:C<sub><i>ds</i></sub> (p) 为p点处自适应的平滑性约束损失值; ∇<b><i>I</i></b> (<i>p</i>) 为∇<b><i>I</i></b>在<i>p</i>点处的RGB均值;<i>β</i>为平滑抑制系数, 表示图像纹理边缘对视差平滑性的抑制强度, <i>β</i>值越大则抑制强度越大, 实验中取<i>β</i>=2。</p>
                </div>
                <div class="p1">
                    <p id="131">整幅图像的平滑性约束损失值为</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>v</mtext><mtext>g</mtext></mrow></msub><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">C</mi><mo>˜</mo></mover><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133"> (12) 式中矩阵<mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">C</mi><mo>˜</mo></mover><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext></mrow></msub></mrow></math></mathml>中<i>p</i>点处的元素值可通过 (11) 式计算得到。</p>
                </div>
                <h4 class="anchor-tag" id="135" name="135"><b>3.3</b><b>左右一致性损失</b></h4>
                <div class="p1">
                    <p id="136">左右一致性检查常用于立体匹配算法的后处理步骤, 从而得到更加合理的视差预测, 这种一致性关系也能用于改善自监督立体匹配方法的性能, 通过惩罚不一致性来引导模型得到更加合理的预测。常采用L1惩罚构造损失函数, 对于左图像视差矩阵<b><i>D</i></b><sub>L</sub>, <i>p</i>点处左右一致性损失可定义为</p>
                </div>
                <div class="p1">
                    <p id="137" class="code-formula">
                        <mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow><mtext>L</mtext></msubsup><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">|</mo><mi>d</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>-</mo><mi>d</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false">[</mo><mi>p</mi><mo>+</mo><mi>d</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">|</mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="138">式中:<i>d</i> (<i>p</i>) 表示相应的视差图<b><i>D</i></b>在<i>p</i>点处的值;<i>p</i>+<i>d</i><sub>L</sub> (<i>p</i>) 表示<i>p</i>点处在图像宽度方向向右平移<i>d</i><sub>L</sub> (<i>p</i>) 个像素之后的坐标。<i>C</i><mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow><mtext>L</mtext></msubsup></mrow></math></mathml>可表示为</p>
                </div>
                <div class="p1">
                    <p id="140" class="code-formula">
                        <mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow><mtext>L</mtext></msubsup><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>v</mtext><mtext>g</mtext></mrow></msub><mo stretchy="false"> (</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msubsup><mo stretchy="false">|</mo><mo stretchy="false">) </mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="141">与图像重构误差相似, 只有在双目图像的共同视域内才能满足这种左右一致性关系, 结合共同视域的左右一致性损失为</p>
                </div>
                <div class="p1">
                    <p id="142" class="code-formula">
                        <mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow><mtext>Μ</mtext></msubsup><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>v</mtext><mtext>g</mtext></mrow></msub><mo stretchy="false"> (</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">D</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msup><mo stretchy="false">|</mo><mo>⋅</mo><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mrow><mi mathvariant="bold">l</mi><mi mathvariant="bold">r</mi></mrow></msub><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="143">式中<b><i>M</i></b><sub>lr</sub>表示预测的视差点在共同视域内的概率矩阵。</p>
                </div>
                <h4 class="anchor-tag" id="144" name="144"><b>3.4</b><b>共同视域的确定</b></h4>
                <div class="p1">
                    <p id="145">自监督立体匹配算法通过左右视角的相关性构建损失函数, 但对于被遮挡的区域并没有这种相关性信息, 在训练过程中若不能排除被遮挡区域, 这些错误反馈将对模型学习产生不利影响。</p>
                </div>
                <div class="p1">
                    <p id="146">双目图像的左右视角图像都有各自视角特有的内容, 如左视角图像的最左侧和右视角图像的最右侧, 由于视场范围限制只能在一个视角内可见, 因此无法形成相关性信息。对于双目图像的共同可视范围, 若视差预测准确, 左右图像预测的视差具有一致性, 被遮挡的区域则不满足这种一致性, 根据这种一致性约束可以对被遮挡区域进行检测。</p>
                </div>
                <div class="p1">
                    <p id="147">假设模型预测的视差<b><i>D</i></b>准确, 相应的重构图为<b><i>D</i></b><sup>wrap</sup>, 将无穷远区域的视差设为<i>δ</i>&gt;0, 若不在共同视角的整体范围, 重构图像相应位置必然为0, 且其他位置均不为0, 因此, 共同视角的整体范围为</p>
                </div>
                <div class="p1">
                    <p id="148" class="code-formula">
                        <mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Μ</mi><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow><mtext>A</mtext></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mrow><mtext>w</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext></mrow></msup><mo>&gt;</mo><mn>0</mn><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="149">式中:<b><i>M</i></b><mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow><mtext>A</mtext></msubsup></mrow></math></mathml>表示共同视域的整体范围在相应视图的区域模板, 矩阵中元素用1和0分别表示相应的视差点在共同视域内和不在共同视域内。</p>
                </div>
                <div class="p1">
                    <p id="151">根据视差的左右一致性, 共同视角内一点<i>p</i>是否被周围物体遮挡可以概率的形式表示为</p>
                </div>
                <div class="p1">
                    <p id="152" class="code-formula">
                        <mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><msub><mrow></mrow><mtext>Ο</mtext></msub><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>0</mn><mo>.</mo><mn>9</mn><mn>8</mn></mtd><mtd columnalign="left"><mtext>Δ</mtext><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>≥</mo><mn>5</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>.</mo><mn>2</mn><mn>4</mn><mn>5</mn><mo>×</mo><mo stretchy="false">[</mo><mtext>Δ</mtext><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn><mo stretchy="false">]</mo></mtd><mtd columnalign="left"><mn>1</mn><mo>≤</mo><mtext>Δ</mtext><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mn>5</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn></mtd><mtd columnalign="left"><mtext>Δ</mtext><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mn>1</mn></mtd></mtr></mtable></mrow></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="153">式中:<i>m</i><sub>O</sub> (<i>p</i>) 表示点<i>p</i>被周围物体遮挡的概率, 下标o是被周围物体遮挡的模板标记, Δ<i>d</i> (<i>p</i>) 是矩阵|<b><i>D</i></b><sup>wrap</sup>-<b><i>D</i></b>|在点<i>p</i>处的元素值。</p>
                </div>
                <div class="p1">
                    <p id="154">对于图像的左右一致性损失, 共同视域在相应视图的区域以概率的形式表示为</p>
                </div>
                <div class="p1">
                    <p id="155" class="code-formula">
                        <mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow></msub><mo>=</mo><mn>1</mn><mo>-</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>i</mtext><mtext>p</mtext></mrow></msub><mo stretchy="false">{</mo><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi mathvariant="bold-italic">Μ</mi><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow><mtext>A</mtext></msubsup><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mtext>Ο</mtext></msub><mo stretchy="false">]</mo><mo>, </mo><mn>0</mn><mo>, </mo><mn>1</mn><mo stretchy="false">}</mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="156">式中矩阵<b><i>M</i></b><sub>O</sub>中<i>p</i>点处的元素值根据 (17) 式计算, <i>F</i><sub>clip</sub> (·, 0, 1) 表示将矩阵中所有元素值限制到范围[0, 1]。</p>
                </div>
                <div class="p1">
                    <p id="157">为使视差预测模型在左边缘能够得到有效的反馈信号, 在重构左图像时, 使用原始的右图像, 而不是已经裁剪的右图像。这样, 对图像进行重构时在左边缘位置也能得到有效反馈, 从而提高视差预测模型在左图像最左侧的预测效果。对于图像的重构损失, 共同视域在相应视图的区域为</p>
                </div>
                <div class="p1">
                    <p id="158" class="code-formula">
                        <mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext></mrow></msub><mo>=</mo><mn>1</mn><mo>-</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>i</mtext><mtext>p</mtext></mrow></msub><mo stretchy="false">{</mo><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi mathvariant="bold-italic">Μ</mi><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext></mrow><mtext>A</mtext></msubsup><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mtext>o</mtext></msub><mo stretchy="false">]</mo><mo>, </mo><mn>0</mn><mo>, </mo><mn>1</mn><mo stretchy="false">}</mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="159">式中<b><i>M</i></b><mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext></mrow><mtext>A</mtext></msubsup></mrow></math></mathml>中的元素<i>M</i><mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext></mrow><mtext>A</mtext></msubsup></mrow></math></mathml> (<i>p</i>) =<i>F</i><sub>or-RGB</sub> (<b><i>I</i></b><sup>wrap</sup>&gt;0) , <i>F</i><sub>or-RGB</sub>表示<b><i>I</i></b><sup>wrap</sup>的RGB值任意一个满足判断条件输出即为1, 否则为0。</p>
                </div>
                <h3 id="162" name="162" class="anchor-tag">4 实 验</h3>
                <div class="p1">
                    <p id="163">为验证算法的有效性, 使用Scene Flow数据集<citation id="222" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>和KITTI数据集<citation id="223" type="reference"><link href="73" rel="bibliography" /><link href="75" rel="bibliography" /><sup>[<a class="sup">33</a>,<a class="sup">34</a>]</sup></citation>对算法进行评价。Scene Flow数据集是仿真环境中生成的数据集, 包含35454对训练图像和4370对测试图像, 为训练一个没有过拟合的模型, 数据量是足够大的。KITTI数据集是在不同天气条件下对真实场景记录的数据集, 包含KITTI2012和KITTI2015两个子集, 前者包含194个训练图像对和195个测试图像对, 后者包含200个训练图像对和200个测试图像对。</p>
                </div>
                <div class="p1">
                    <p id="164">在实际应用中, 需要综合考虑算法精度、运行效率和资源消耗。KITTI数据集为各种算法的性能提供了统一的对比, 但运行各种算法使用的硬件、软件平台各不相同, 并不能对算法的运行效率和资源消耗进行直接对比。在Pytorch上实现5种视差预测模型, 源代码在Github上托管<citation id="224" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>, 在Quadro K6000显卡上对它们的运行效率和GPU占用显存进行对比, 结果如表1和图2所示。两组数据均为测试阶段的数据, 训练阶段通常会消耗更多资源, 其中, 表1是对100对不同测试数据的平均运行时间, 图2是将图片高度设为256 pixel的情况下改变图片宽度时GPU显存占用情况。</p>
                </div>
                <div class="area_img" id="165">
                    <p class="img_tit">表1 不同视差预测模型的运行时间 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Running time of different disparity prediction models</p>
                    <p class="img_note"></p>
                    <table id="165" border="1"><tr><td rowspan="2"><br />Image size /pixel×pixel</td><td colspan="5"><br />Running time /s</td></tr><tr><td><br />DispNet<sup>[19]</sup></td><td>DispNetCorr<sup>[19]</sup></td><td>GC-Net<sup>[22]</sup></td><td>iResNet<sup>[21]</sup></td><td>PSMnet<sup>[24]</sup></td></tr><tr><td>375×1242</td><td>0.074</td><td>0.100</td><td>6.848</td><td>0.375</td><td>3.462</td></tr><tr><td><br />480×960</td><td>0.074</td><td>0.097</td><td>6.527</td><td>0.339</td><td>3.397</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="166">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902035_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同视差预测模型GPU显存占用情况" src="Detail/GetImg?filename=images/GXXB201902035_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 不同视差预测模型GPU显存占用情况  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902035_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 GPU memory consumption of different disparity prediction models</p>

                </div>
                <div class="p1">
                    <p id="167">综合考虑视差预测模型的精度、速度和资源消耗, 实验选用DispNetCorr<citation id="225" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>作为视差预测模型。训练或微调过程中, 均使用Adam<citation id="226" type="reference"><link href="79" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>方法进行优化, 批大小取4, 图片分辨率取256 pixel×512 pixel。数据增强包括空间变换和色彩变换, 空间变换包括裁剪和缩放, 色彩变换包括颜色、对比度和亮度变换。</p>
                </div>
                <div class="p1">
                    <p id="168">在自监督立体匹配方法中, 分别使用文献<citation id="227" type="reference">[<a class="sup">27</a>]</citation>和文献<citation id="228" type="reference">[<a class="sup">31</a>]</citation>的损失函数为基准与所提方法进行对比, 其中损失函数中三种损失的权重初始值<i>w</i><sub>ap</sub>=1, <i>w</i><sub>ds</sub>=0.001, <i>w</i><sub>lr</sub>=0.001, 训练过程中动态调整<i>w</i><sub>ds</sub>和<i>w</i><sub>lr</sub>, 并使用相同的调整函数:</p>
                </div>
                <div class="p1">
                    <p id="169" class="code-formula">
                        <mathml id="169"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><mo>=</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>1</mn><mo>+</mo><mn>0</mn><mo>.</mo><mn>5</mn><mo>×</mo><mi>max</mi><mo stretchy="false"> (</mo><mn>0</mn><mo>, </mo><mi>S</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>Μ</mtext></mrow></msub><mo>-</mo><mn>0</mn><mo>.</mo><mn>7</mn><mn>5</mn><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="170">式中:<i>S</i><sub>IM</sub>=<i>F</i><sub>avg</sub>[<i>F</i><sub>SSIM</sub> (<b><i>I</i></b><sup>wrap</sup>, <b><i>I</i></b>) ], 表示观察图像与重构图像的结构相似度的平均值;max表示求两个变量的最大值。</p>
                </div>
                <div class="p1">
                    <p id="171">使用的主要评价指标包括EPE (end-point error) 、D1和RPE (reconstruction pixel error) 。EPE表示预测视差与实际测量值之间的误差;D1表示每组图像对中第一帧的评价区域错误像素百分比, 其中EPE小于3 pixel或EPE小于实际测量值5%的认为是正确像素, 否则为错误像素。RPE表示重构图像的像素值与观测值之间的误差。</p>
                </div>
                <h4 class="anchor-tag" id="172" name="172"><b>4.1</b><b>自改善能力分析</b></h4>
                <div class="p1">
                    <p id="173">在没有视差数据作为监督信号的情况下, 自监督立体匹配方法以图像重构误差为模型训练提供反馈信号, 从而改善模型的性能。为分析这种自我改善能力, 以KITTI2015训练集对随机初始化的模型进行训练, 学习率取1×10<sup>-4</sup>, 共训练2000周期, 每个周期处理完整个数据集需要50次迭代。训练完成后使用这两个数据集进行性能评价的结果如表2所示, 训练过程中以KITTI2012训练集进行性能评价的结果如图3所示, 其中对训练损失进行了归一化处理, -M表示对应算法只在共同视域内计算损失时的结果。</p>
                </div>
                <div class="area_img" id="174">
                    <p class="img_tit">表2 随机初始化模型训练完成后对不同数据集进行评价的结果对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Comparison of evaluation results on different datasets after training with randomly initializing model</p>
                    <p class="img_note"></p>
                    <table id="174" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="3"><br />KITTI2012 training set</td><td rowspan="2"></td><td colspan="3"><br />KITTI2015 training set</td></tr><tr><td><br />D1 /%</td><td>EPE /pixel</td><td>RPE /pixel</td><td><br />D1 /%</td><td>EPE /pixel</td><td>RPE /pixel</td></tr><tr><td>Method in Ref. [19]</td><td>14.78</td><td>2.37</td><td>13.91</td><td></td><td>4.99</td><td>0.91</td><td>13.40</td></tr><tr><td><br />Method in Ref. [27]</td><td>14.53</td><td>2.90</td><td>11.75</td><td></td><td>11.39</td><td>1.95</td><td>9.92</td></tr><tr><td><br />Method in Ref. [27]-M</td><td>10.40</td><td>2.71</td><td>11.38</td><td></td><td>6.60</td><td>1.46</td><td>9.46</td></tr><tr><td><br />Method in Ref. [31]</td><td>12.98</td><td>2.69</td><td>11.13</td><td></td><td>10.80</td><td>1.89</td><td>9.48</td></tr><tr><td><br />Method in Ref. [31]-M</td><td>10.27</td><td>2.73</td><td>11.24</td><td></td><td>7.55</td><td>1.60</td><td>9.35</td></tr><tr><td><br />Proposed method</td><td>10.13</td><td>2.72</td><td>11.37</td><td></td><td>6.78</td><td>1.64</td><td>9.64</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="175">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902035_175.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 随机初始化模型训练过程中对KITTI2012训练集进行评价的结果对比。 (a) 训练损失; (b) D1; (c) EPE; (d) RPE" src="Detail/GetImg?filename=images/GXXB201902035_175.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 随机初始化模型训练过程中对KITTI2012训练集进行评价的结果对比。 (a) 训练损失; (b) D1; (c) EPE; (d) RPE  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902035_175.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Comparison of evaluation results on KITTI2012 training set with randomly initializing model during training process. (a) Training loss; (b) D1; (c) EPE; (d) RPE</p>

                </div>
                <div class="p1">
                    <p id="176">从表2和图3可以看出:当训练数据集较小时, 有监督学习立体匹配方法在训练集上获得了最佳性能, 但并不能很好地适应新的数据集;与基准算法相比, 所提算法可以明显改善自监督学习立体匹配方法的效果, 在训练集上的D1评价指标降低了30%～42%, 在验证集上的D1评价指标降低了21%～30%。</p>
                </div>
                <div class="p1">
                    <p id="177">从图3也可以看出, 无论采用哪种损失函数, 最终模型训练都能收敛, 并得到合理的视差图, 说明了基于学习的立体匹配方法的有效性, 同时也说明了自监督立体匹配方法具有驱动模型性能改善的能力。</p>
                </div>
                <h4 class="anchor-tag" id="178" name="178"><b>4.2</b><b>微调能力分析</b></h4>
                <div class="p1">
                    <p id="179">以预训练的模型作为初始值进行微调, 能够分析不同损失函数对模型训练的精细调节能力。为使预训练的模型具有较高的准确度, 使用所提方法在Scene Flow数据集上进行预训练, 再将预训练的模型参数作为初始值, 在KITTI2015训练集上进行微调, 微调完成后使用不同数据集进行性能评价的结果如表3所示, 训练过程中以KITTI2012训练集进行性能验证评价的结果如图4所示, 各个参数含义与上一节相同。其中, 在Scene Flow数据集上预训练时, 初始学习率取1×10<sup>-4</sup>, 从第40个周期开始, 每20个周期学习率减半, 共训练120个周期。在KITTI数据集上微调时, 初始学习率取2×10<sup>-5</sup>, 训练400个周期, 调整学习率为1×10<sup>-5</sup>再训练1600个周期。</p>
                </div>
                <div class="area_img" id="180">
                    <p class="img_tit">表3 预训练模型微调后对不同数据集进行评价的结果对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Comparison of evaluation results on different datasets with pre-training model after fine tuning</p>
                    <p class="img_note"></p>
                    <table id="180" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="3"><br />KITTI2012 training set</td><td rowspan="2"></td><td colspan="3"><br />KITTI2015 training set</td></tr><tr><td><br />D1 /%</td><td>EPE /pixel</td><td>RPE /pixel</td><td><br />D1 /%</td><td>EPE /pixel</td><td>RPE /pixel</td></tr><tr><td>Pre-training</td><td>8.07</td><td>1.53</td><td>12.13</td><td></td><td>8.06</td><td>2.27</td><td>11.78</td></tr><tr><td><br />Method in Ref. [19]</td><td>5.71</td><td>1.31</td><td>12.74</td><td></td><td>2.15</td><td>0.69</td><td>12.75</td></tr><tr><td><br />Method in Ref. [27]</td><td>9.26</td><td>2.25</td><td>11.29</td><td></td><td>7.85</td><td>1.73</td><td>9.71</td></tr><tr><td><br />Method in Ref. [27]-M</td><td>7.29</td><td>1.70</td><td>10.84</td><td></td><td>5.91</td><td>1.31</td><td>9.46</td></tr><tr><td><br />Method in Ref. [31]</td><td>8.21</td><td>1.96</td><td>10.74</td><td></td><td>7.28</td><td>1.62</td><td>9.27</td></tr><tr><td><br />Method in Ref. [31]-M</td><td>7.30</td><td>1.59</td><td>10.57</td><td></td><td>5.95</td><td>1.33</td><td>9.21</td></tr><tr><td><br />Proposed method</td><td>6.86</td><td>1.61</td><td>10.64</td><td></td><td>5.96</td><td>1.35</td><td>9.31</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="181">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902035_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 预训练模型微调过程中对KITTI2012训练集进行评价的结果对比。 (a) 训练损失; (b) D1; (c) EPE; (d) RPE" src="Detail/GetImg?filename=images/GXXB201902035_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 预训练模型微调过程中对KITTI2012训练集进行评价的结果对比。 (a) 训练损失; (b) D1; (c) EPE; (d) RPE  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902035_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Comparison of evaluation results on KITTI2012 training set with pre-training model during fine tuning process. (a) Training loss; (b) D1; (c) EPE; (d) RPE</p>

                </div>
                <div class="p1">
                    <p id="183">从表3和图4中也可以看出, 在视差预测的精度上, 所提方法能够明显提高自监督学习方法的性能, 在训练集上的D1评价指标降低了18%～25%, 在验证集上的D1评价指标降低了11%～26%。由于自监督学习方法本质上无法对全部区域产生有效的反馈监督信号, 与有监督学习方法的性能相比仍然有一定差距, 但它不依赖标签数据, 可以大大减小标签数据获取的成本和工作量。</p>
                </div>
                <div class="p1">
                    <p id="184">同时也可以看出, 对于有监督的立体匹配方法, 重构图像的像素误差明显大于自监督的方法, 这说明在KITTI数据集上视差预测越准确, 重构误差不一定越小, 产生这种现象的主要原因有两方面。首先, KITTI数据集并没有提供全部像素的观测视差, 在没有监督信号的区域, 视差的预测误差可能会更大。其次, 在产生遮挡的区域, 使用不准确的视差得到的重构图像的像素误差可能会更小。由于自监督立体匹配方法将重构图像的像素误差作为反馈信号, 因此第二种原因是限制其性能的主要因素。</p>
                </div>
                <div class="p1">
                    <p id="185">对视差预测结果进行定性的对比分析, 可以直观地分析模型的优点和缺陷, 有助于进一步改进模型、提高算法性能。这里选择KITTI2015训练集中的两组图像进行对比分析, 结果如图5和图6所示。图5 (a) 、 (c) 为左视角图像, 图5 (b) 、 (d) 为视差的预测值。图6 (a) 、 (c) 为不同方法预测视差的误差图, 图6 (b) 、 (d) 为不同方法的预测值。可以看出, 汽车玻璃所在的区域, 具有较强的反光, 使得纹理很弱, 自监督立体匹配方法的视差预测效果较差;细长的杆状物体所在的区域, 纹理较明显的区域能够预测较准确的视差, 但会使周围弱纹理的区域的误差增大, 纹理不明显的区域将被误认为是背景。产生这种现象的主要原因是:自监督立体匹配方法的主要反馈信号就是图像的纹理, 当纹理信号弱甚至是不可用时就无法学习到准确的视差, 需要其他有效的约束条件来改善最终结果。</p>
                </div>
                <div class="area_img" id="186">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902035_186.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 KITTI2015训练集中的两组图像。 (a) 第一组图像的左视角图像; (b) 第一组图像的视差预测值; (c) 第二组图像的左视角图像; (d) 第二组图像的视差预测值" src="Detail/GetImg?filename=images/GXXB201902035_186.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 KITTI2015训练集中的两组图像。 (a) 第一组图像的左视角图像; (b) 第一组图像的视差预测值; (c) 第二组图像的左视角图像; (d) 第二组图像的视差预测值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902035_186.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Two sets of images selected from KITTI2015 training set. (a) Left view image in first set. (b) predicted disparity in first set. (c) left view image in second set. (d) predicted disparity in second set</p>

                </div>
                <div class="area_img" id="187">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902035_187.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 预训练模型微调后的实际应用效果对比。 (a) 第一组图像的误差图; (b) 第一组图像的视差预测值; (c) 第二组图像的误差图; (d) 第二组图像的视差预测值" src="Detail/GetImg?filename=images/GXXB201902035_187.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 预训练模型微调后的实际应用效果对比。 (a) 第一组图像的误差图; (b) 第一组图像的视差预测值; (c) 第二组图像的误差图; (d) 第二组图像的视差预测值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902035_187.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Comparison of practical application effects with pre-training model after fine tuning. (a) Error map in first set; (b) predicted disparity in first set; (c) error map in second set; (d) predicted disparity in second set</p>

                </div>
                <h3 id="189" name="189" class="anchor-tag">5 结 论</h3>
                <div class="p1">
                    <p id="190">提出了一种基于共同视域的自监督立体匹配算法, 根据视差的左右一致性特点来判断遮挡区域, 在损失函数中去除遮挡区域, 减少了遮挡区域对模型训练产生的噪声。实验结果表明所提方法不需要任何标签数据就能达到接近有监督立体匹配方法的性能。但对于纹理弱的区域、具有较强反光和细长杆状物的区域, 该算法无法得到有效的训练反馈信息, 仍需要进一步的研究。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="9">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SIFT flow: Dense correspondence across scenes and its applications">

                                <b>[1]</b> Liu C, Yuen J, Torralba A. SIFT flow: dense correspondence across scenes and its applications[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) : 978-994.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201809038&amp;v=MDcyOTA1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEbVVyL0pMeXJQWkxHNEg5bk1wbzlHYklRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Liu Y F, Cai Z J. Binocular stereo vision three-dimensional reconstruction algorithm based on ICP and SFM[J]. Laser and Optoelectronics Progress, 2018, 55 (9) : 091503. 刘一凡, 蔡振江. 基于ICP与SFM的双目立体视觉三维重构算法[J]. 激光与光电子学进展, 2018, 55 (9) : 091503.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Review of Recent Developments in Vision-Based Vehicle Detection">

                                <b>[3]</b> Sivaraman S, Trivedi M M. A review of recent developments in vision-based vehicle detection[C]. IEEE Intelligent Vehicles Symposium (IV) , 2013: 310-315.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stereo Vision Based Indoor/Outdoor Navigation for Flying Robots">

                                <b>[4]</b> Schmid K, Tomic T, Ruess F, <i>et al</i>. Stereo vision based indoor/outdoor navigation for flying robots[C]. IEEE/RSJ International Conference on Intelligent Robots and Systems, 2013: 3955-3962.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830724&amp;v=MDg1MzloNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rml2a1c3ekpJRjg9Tmo3QmFyTzRIdEhPcDR4Rlkra0xZM2s1ekJk&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Scharstein D, Szeliski R, Zabih R. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J]. International Journal of Computer Vision, 2002, 47 (1) : 7-42.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MTc1Njc5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXZrVzd6SklGOD1OajdCYXJPNEh0SE9wNHhGYmVzT1kzazV6QmRoNGo5&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Lowe D G. Distinctive image features from scale-invariant keypoints[J]. International Journal of Computer Vision, 2004, 60 (2) : 91-110.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">

                                <b>[7]</b> Dalal N, Triggs B. Histograms of oriented gradients for human detection[C]. IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) , 2005: 886-893.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stereo processing by semiglobal matching and mutual information">

                                <b>[8]</b> Hirschmuller H. Stereo processing by semiglobal matching and mutual information[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008, 30 (2) : 328-341.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604039&amp;v=MzA1NDJYVGJMRzRIOWZNcTQ5R2JZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRG1Vci9KSWo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Liu D W, Han L, Han X Y. High spatial resolution remote sensing image classification based on deep learning[J]. Acta Optica Sinica, 2016, 36 (4) : 0428001. 刘大伟, 韩玲, 韩晓勇. 基于深度学习的高分辨率遥感影像分类研究[J]. 光学学报, 2016, 36 (4) : 0428001.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201801018&amp;v=MTMxMTlHRnJDVVI3cWZadVp0RmlEbVVyL0pJalhUYkxHNEg5bk1ybzlFYklRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Hou Y Q Y, Quan J C, Wei Y M. Valid aircraft detection system for remote sensing images based on cognitive models[J]. Acta Optica Sinica, 2018, 38 (1) : 0111005. 侯宇青阳, 全吉成, 魏湧明. 基于认知模型的遥感图像有效飞机检测系统[J]. 光学学报, 2018, 38 (1) : 0111005.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[11]</b> Shelhamer E, Long J, Darrell T. Fully convolutional networks for semantic segmentation[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) : 640-651.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Computing the stereo matching cost with a convolutional neural network">

                                <b>[12]</b> Žbontar J, LeCun Y. Computing the stereo matching cost with a convolutional neural network[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 1592-1599.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stereo matching by training a convolutional neural network to compare image patches">

                                <b>[13]</b> Žbontar J, LeCun Y. Stereo matching by training a convolutional neural network to compare image patches[J]. Journal of Machine Learning Research, 2016, 17 (1) : 2287-2318.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201808018&amp;v=MTgyMTJPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRG1Vci9KSWpYVGJMRzRIOW5NcDQ5RWJJUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Xiao J S, Tian H, Zou W T, <i>et al</i>. Stereo matching based on convolutional neural network[J]. Acta Optica Sinica, 2018, 38 (8) : 0815017. 肖进胜, 田红, 邹文涛, 等. 基于深度卷积神经网络的双目立体视觉匹配算法[J]. 光学学报, 2018, 38 (8) : 0815017.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Deep Visual Correspondence Embedding Model for Stereo Matching Costs">

                                <b>[15]</b> Chen Z Y, Sun X, Wang L, <i>et al</i>. A deep visual correspondence embedding model for stereo matching costs[C]. IEEE International Conference on Computer Vision, 2015: 972-980.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Look Wider to Match Image Patches with Convolutional Neural Networks">

                                <b>[16]</b> Park H, Lee K M. Look wider to match image patches with convolutional neural networks[J]. IEEE Signal Processing Letters, 2017, 24 (12) : 1788-1792.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Displets:Resolving stereo ambiguities using object knowledge">

                                <b>[17]</b> Güney F, Geiger A. Displets: resolving stereo ambiguities using object knowledge[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 4165-4175.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Patch Based Confidence Prediction for Dense Disparity Map">

                                <b>[18]</b> Seki A, Pollefeys M. Patch based confidence prediction for dense disparity map[C]. British Machine Vision Conference (BMVC) , 2016: 23.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A large dataset to train convolutional networks for disparity,optical flow,and scene flow estimation">

                                <b>[19]</b> Mayer N, Ilg E, Häusser P, <i>et al</i>. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 4040-4048.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cascade Residual Learning A Two-Stage Convolutional Neural Network for Stereo Matching">

                                <b>[20]</b> Pang J H, Sun W X, Ren J S, <i>et al</i>. Cascade residual learning: a two-stage convolutional neural network for stereo matching[C]. IEEE International Conference on Computer Vision Workshops, 2017: 887-895.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep correspondence through prior and posterior feature constancy">

                                <b>[21]</b> Liang Z F, Feng Y L, Guo Y L, <i>et al</i>. Learning deep correspondence through prior and posterior feature constancy[EB/OL]. (2017-12-04) [2018-06-05]. http://cn.arxiv.org/abs/1712.01039.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Endto-end learning of geometry and context for deep stereo regression">

                                <b>[22]</b> Kendall A, Martirosyan H, Dasgupta S, <i>et al</i>. End-to-end learning of geometry and context for deep stereo regression[C]. IEEE International Conference on Computer Vision, 2017: 66-75.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep stereo matching with explicit cost aggregation sub-architecture">

                                <b>[23]</b> Yu L D, Wang Y C, Wu Y W, <i>et al</i>. Deep stereo matching with explicit cost aggregation sub-architecture[EB/OL]. (2018-01-12) [2018-06-05]. http://cn.arxiv.org/abs/1801.04065.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pyramid stereo matching network">

                                <b>[24]</b> Chang J R, Chen Y S. Pyramid stereo matching network[EB/OL]. (2018-03-23) [2018-06-05]. http://cn.arxiv.org/abs/1803.08669.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks">

                                <b>[25]</b> Xie J Y, Girshick R, Farhadi A. Deep 3D: fully automatic 2D-to-3D video conversion with deep convolutional neural networks[C]. European Conference on Computer Vision (ECCV) , 2016: 842-857.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised CNN for single view depth estimation:Geometry to the rescue">

                                <b>[26]</b> GargR, Vijay K B G, Carneiro G, <i>et al</i>. Unsupervised CNN for single view depth estimation: Geometry to the rescue[C]. European Conference on Computer Vision (ECCV) , 2016: 740-756.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised monocular depth estimation with leftright consistency">

                                <b>[27]</b> Godard C, Aodha O M, Brostow G J. Unsupervised monocular depth estimation with left-right consistency[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2017: 6602-6611.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial transformer networks">

                                <b>[28]</b> Jaderberg M, Simonyan K, Zisserman A, <i>et al</i>. Spatial transformer networks[EB/OL]. (2015-06-05) [2018-06-05]. http://cn.arxiv.org/abs/1506.02025.
                            </a>
                        </p>
                        <p id="65">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Self-supervised siamese learning on stereo image pairs for depth estimation in robotic surgery">

                                <b>[29]</b> Ye M L, Johns E, Handa A, <i>et al</i>. Self-supervised siamese learning on stereo image pairs for depth estimation in robotic surgery[EB/OL]. (2017-05-17) [2018-06-05]. http://cn.arxiv.org/abs/1705.08260.
                            </a>
                        </p>
                        <p id="67">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Self-supervised learning for dense depth estimation in monocular endoscopy">

                                <b>[30]</b> LiuX, Sinha A, Unberath M, <i>et al</i>. Self-supervised learning for dense depth estimation in monocular endoscopy[C]. European Conference on Computer Vision (ECCV) , 2018, 11041: 128-138.
                            </a>
                        </p>
                        <p id="69">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Self-supervised learning for stereo matching with self-improving ability">

                                <b>[31]</b> Zhong Y R, Dai Y C, Li H D. Self-supervised learning for stereo matching with self-improving ability[EB/OL]. (2017-09-04) [2018-06-05]. http://cn.arxiv.org/abs/1709.00930.
                            </a>
                        </p>
                        <p id="71">
                            <a id="bibliography_32" >
                                    <b>[32]</b>
                                 Wang Z, Bovik A C, Sheikh H R, <i>et al</i>. Image quality assessment: from error visibility to structural similarity[J]. IEEE Transactions on Image Processing, 2004, 13 (4) : 600-612.
                            </a>
                        </p>
                        <p id="73">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Are we ready for autonomous driving?The KITTI vision benchmark suite,&amp;quot;">

                                <b>[33]</b> Geiger A, Lenz P, Urtasun R. Are we ready for autonomous driving? The KITTI vision benchmark suite[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2012: 3354-3361.
                            </a>
                        </p>
                        <p id="75">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object scene flow for autonomous vehicles">

                                <b>[34]</b> Menze M, Geiger A. Object scene flow for autonomous vehicles[C]. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015: 3061-3070.
                            </a>
                        </p>
                        <p id="77">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=wyf2017: Pytorch implementation of the several deep stereo matching network">

                                <b>[35]</b> wyf2017: Pytorch implementation of the several deep stereo matching network [EB/OL]. https://github.com/wyf2017/DSMnet.
                            </a>
                        </p>
                        <p id="79">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam: a method for stochastic optimization">

                                <b>[36]</b> Kingma D P, Ba J. Adam: a method for stochastic optimization[EB/OL]. (2014-12-22) [2018-06-05]. http://cn.arxiv.org/abs/1412.6980.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201902035" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902035&amp;v=MDM4NTQ5R1lZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRG1Vci9KSWpYVGJMRzRIOWpNclk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

