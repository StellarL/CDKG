

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134123530752500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201904037%26RESULT%3d1%26SIGN%3d5C%252fGcTbB8NzAVCcwdo40bFP9CsQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201904037&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201904037&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201904037&amp;v=MjE3MjJYVGJMRzRIOWpNcTQ5R1k0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtWN3ZJSWo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="2 相关研究工作 ">2 相关研究工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#73" data-title="3 多边形网格模型的体素化 ">3 多边形网格模型的体素化</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#75" data-title="&lt;b&gt;3.1 体素化&lt;/b&gt;"><b>3.1 体素化</b></a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;3.2 数据扩充&lt;/b&gt;"><b>3.2 数据扩充</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#89" data-title="4 基于深度体素CNN的三维模型识别分类 ">4 基于深度体素CNN的三维模型识别分类</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#91" data-title="&lt;b&gt;4.1 基于卷积网络的三维模型深层特征的提取&lt;/b&gt;"><b>4.1 基于卷积网络的三维模型深层特征的提取</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;4.2 基于Softmax分类器的三维模型分类&lt;/b&gt;"><b>4.2 基于Softmax分类器的三维模型分类</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#119" data-title="5 实验结果与分析 ">5 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#121" data-title="&lt;b&gt;5.1 实验数据集&lt;/b&gt;"><b>5.1 实验数据集</b></a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;5.2 神经网络参数设置与性能评价&lt;/b&gt;"><b>5.2 神经网络参数设置与性能评价</b></a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;5.3 实验结果与分析&lt;/b&gt;"><b>5.3 实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#141" data-title="6 结 论 ">6 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#82" data-title="图1 3D网格模型的体素化。 (a) 渲染后的模型; (b) 网格模型; (c) 体素模型">图1 3D网格模型的体素化。 (a) 渲染后的模型; (b) 网格模型; (c) 体素模型</a></li>
                                                <li><a href="#88" data-title="图2 3D模型的旋转数据扩充。 (a) 马桶模型; (b) 椅子模型">图2 3D模型的旋转数据扩充。 (a) 马桶模型; (b) 椅子模型</a></li>
                                                <li><a href="#94" data-title="图3 卷积操作。 (a) 2D; (b) 3D">图3 卷积操作。 (a) 2D; (b) 3D</a></li>
                                                <li><a href="#96" data-title="图4 CNN结构">图4 CNN结构</a></li>
                                                <li><a href="#111" data-title="图5 3D模型识别分类示意图">图5 3D模型识别分类示意图</a></li>
                                                <li><a href="#130" data-title="表1 不同旋转角度扩充数据集中3D模型识别分类的准确率">表1 不同旋转角度扩充数据集中3D模型识别分类的准确率</a></li>
                                                <li><a href="#133" data-title="表2 不同尺寸卷积核下3D模型识别分类的准确率">表2 不同尺寸卷积核下3D模型识别分类的准确率</a></li>
                                                <li><a href="#135" data-title="表3 不同分辨率下3D模型识别分类的准确率">表3 不同分辨率下3D模型识别分类的准确率</a></li>
                                                <li><a href="#138" data-title="表4 不同算法下3D模型识别分类的准确率">表4 不同算法下3D模型识别分类的准确率</a></li>
                                                <li><a href="#140" data-title="图6 3D模型识别分类结果">图6 3D模型识别分类结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Silberman N, Hoiem D, Kohli P, &lt;i&gt;et al&lt;/i&gt;.Indoor segmentation and support inference from RGBD images[M]∥Silberman N, Hoiem D, Kohli P, &lt;i&gt;et al&lt;/i&gt;.Heidelberg:Computer Vision-ECCV 2012, Springer, 2012:746-760." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Indoor Segmentation and Support Inference from RGBD Images">
                                        <b>[1]</b>
                                         Silberman N, Hoiem D, Kohli P, &lt;i&gt;et al&lt;/i&gt;.Indoor segmentation and support inference from RGBD images[M]∥Silberman N, Hoiem D, Kohli P, &lt;i&gt;et al&lt;/i&gt;.Heidelberg:Computer Vision-ECCV 2012, Springer, 2012:746-760.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Xiao J X, Owens A, Torralba A.SUN&lt;sub&gt;3&lt;/sub&gt;D:a database of big spaces reconstructed using SfM and object labels[C].Sydney:2013 IEEE International Conference on Computer Vision, 2013:1625-1632." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SUN3D:A Database of Big Spaces Reconstructed Using SfM and Object Labels">
                                        <b>[2]</b>
                                         Xiao J X, Owens A, Torralba A.SUN&lt;sub&gt;3&lt;/sub&gt;D:a database of big spaces reconstructed using SfM and object labels[C].Sydney:2013 IEEE International Conference on Computer Vision, 2013:1625-1632.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" Song S R, Lichtenberg S P, Xiao J X.SUN RGB-D:a RGB-D scene understanding benchmark suite[C].2015 IEEE Conference on Computer Vision and Pattern Recognition, 2015:567-576." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SUN RGB-D:A RGB-D scene understanding benchmark suite">
                                        <b>[3]</b>
                                         Song S R, Lichtenberg S P, Xiao J X.SUN RGB-D:a RGB-D scene understanding benchmark suite[C].2015 IEEE Conference on Computer Vision and Pattern Recognition, 2015:567-576.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Chang A X, Funkhouser T, Guibas L, &lt;i&gt;et al&lt;/i&gt;.ShapeNet:an information-rich 3D model repository[J].Computer Science, 2015, 1512:3-12." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ShapeNet: An Information-Rich 3D Model Repository">
                                        <b>[4]</b>
                                         Chang A X, Funkhouser T, Guibas L, &lt;i&gt;et al&lt;/i&gt;.ShapeNet:an information-rich 3D model repository[J].Computer Science, 2015, 1512:3-12.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" Qu L, Wang K R, Chen L L, &lt;i&gt;et al&lt;/i&gt;.Fast road detection based on RGBD images and convolutional neural network[J].Acta Optica Sinica, 2017, 37 (10) :1010003.曲磊, 王康如, 陈利利, 等.基于RGBD图像和卷积神经网络的快速道路检测[J].光学学报, 2017, 37 (10) :1010003." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201710017&amp;v=MTUzNzJyNDlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIa1Y3dklJalhUYkxHNEg5Yk4=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Qu L, Wang K R, Chen L L, &lt;i&gt;et al&lt;/i&gt;.Fast road detection based on RGBD images and convolutional neural network[J].Acta Optica Sinica, 2017, 37 (10) :1010003.曲磊, 王康如, 陈利利, 等.基于RGBD图像和卷积神经网络的快速道路检测[J].光学学报, 2017, 37 (10) :1010003.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Masci J, Boscaini D, Bronstein M M, &lt;i&gt;et al&lt;/i&gt;.Geodesic convolutional neural networks on Riemannian manifolds[C].2015 IEEE International Conference on Computer Vision Workshop, 2015:832-840." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Geodesic Convolutional Neural Networks on Riemannian Manifolds">
                                        <b>[6]</b>
                                         Masci J, Boscaini D, Bronstein M M, &lt;i&gt;et al&lt;/i&gt;.Geodesic convolutional neural networks on Riemannian manifolds[C].2015 IEEE International Conference on Computer Vision Workshop, 2015:832-840.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" Wu Z R, Song S R, Khosla A, &lt;i&gt;et al&lt;/i&gt;.3D ShapeNets:a deep representation for volumetric shapes[C].Boston:2015 IEEE Conference on Computer Vision and Pattern Recognition, 2015:1912-1920." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3d shapenets:A deep representation for volumetric shape modeling">
                                        <b>[7]</b>
                                         Wu Z R, Song S R, Khosla A, &lt;i&gt;et al&lt;/i&gt;.3D ShapeNets:a deep representation for volumetric shapes[C].Boston:2015 IEEE Conference on Computer Vision and Pattern Recognition, 2015:1912-1920.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" Boscaini D, Masci J, Rodol&#224; E, &lt;i&gt;et al&lt;/i&gt;.Learning shape correspondence with anisotropic convolutional neural networks[C].Proceedings of the 30th International Conference on Neural Information Processing Systems, 2016:3197-3205." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning shape correspondence with anisotropic convolutional neural networks">
                                        <b>[8]</b>
                                         Boscaini D, Masci J, Rodol&#224; E, &lt;i&gt;et al&lt;/i&gt;.Learning shape correspondence with anisotropic convolutional neural networks[C].Proceedings of the 30th International Conference on Neural Information Processing Systems, 2016:3197-3205.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" Boscaini D, Masci J, Melzi S, &lt;i&gt;et al&lt;/i&gt;.Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks[J].Computer Graphics Forum, 2015, 34 (5) :13-23." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD15081100000257&amp;v=MzAyNjZLOUh0bk5ybzlGWk9zUERuaytvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lLVjhRYmhJPU5pZmNhcg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Boscaini D, Masci J, Melzi S, &lt;i&gt;et al&lt;/i&gt;.Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks[J].Computer Graphics Forum, 2015, 34 (5) :13-23.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" Bronstein A M, Bronstein M M, Guibas L J, &lt;i&gt;et al&lt;/i&gt;.Shape google[J].ACM Transactions on Graphics, 2011, 30 (1) :1-20." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000004432&amp;v=MzAyMjlSZEdlcnFRVE1ud1plWnVIeWptVWIvSUtWOFFiaEk9TmlmSVk3SzdIdGpOcjQ5RlpPc0xDSDg3b0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Bronstein A M, Bronstein M M, Guibas L J, &lt;i&gt;et al&lt;/i&gt;.Shape google[J].ACM Transactions on Graphics, 2011, 30 (1) :1-20.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" Osada R, Funkhouser T, Chazelle B, &lt;i&gt;et al&lt;/i&gt;.Shape distributions[J].ACM Transactions on Graphics, 2002, 21 (4) :807-832." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000097977&amp;v=MTI4NjlPSUlCWHMrb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JS1Y4UWJoST1OaWZJWTdLN0h0ak5yNDlGWg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Osada R, Funkhouser T, Chazelle B, &lt;i&gt;et al&lt;/i&gt;.Shape distributions[J].ACM Transactions on Graphics, 2002, 21 (4) :807-832.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" Chaudhuri S, Koltun V.Data-driven suggestions for creativity support in 3D modeling[C].ACM Transactions on Graphics, 2010:183-191." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000002893&amp;v=MTM3MzA4UWJoST1OaWZJWTdLN0h0ak5yNDlGWk9zTkJIVTZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lLVg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Chaudhuri S, Koltun V.Data-driven suggestions for creativity support in 3D modeling[C].ACM Transactions on Graphics, 2010:183-191.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" Sun J, Ovsjanikov M, GuibasL.A concise and provably informative multi-scale signature based on heat diffusion[J].Computer Graphics Forum, 2009, 28 (5) :1383-1392." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Concise and Provably Informative Multi-Scale Signature Based on Heat Diffusion">
                                        <b>[13]</b>
                                         Sun J, Ovsjanikov M, GuibasL.A concise and provably informative multi-scale signature based on heat diffusion[J].Computer Graphics Forum, 2009, 28 (5) :1383-1392.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" Bronstein M M, Kokkinos I.Scale-invariant heat kernel signatures for non-rigid shape recognition[C].2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, 2010:1704-1711." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scale-invariant heat kernel signatures for non-rigid shape recognition">
                                        <b>[14]</b>
                                         Bronstein M M, Kokkinos I.Scale-invariant heat kernel signatures for non-rigid shape recognition[C].2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, 2010:1704-1711.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" Knopp J, Prasad M, Willems G, &lt;i&gt;et al&lt;/i&gt;.Hough transform and 3D SURF for robust three dimensional classification[M]//Knopp J, Prasad M, Willems G, &lt;i&gt;et al&lt;/i&gt;.Heidelberg:Springer, 2010:589-602." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hough transform and 3D SURF for robust three dimensional classification">
                                        <b>[15]</b>
                                         Knopp J, Prasad M, Willems G, &lt;i&gt;et al&lt;/i&gt;.Hough transform and 3D SURF for robust three dimensional classification[M]//Knopp J, Prasad M, Willems G, &lt;i&gt;et al&lt;/i&gt;.Heidelberg:Springer, 2010:589-602.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" Kazhdan M, Funkhouser T, Rusinkiewicz S.Rotation invariant spherical harmonic representation of 3D shape descriptors[C].Proceedings of the 2003 Eurographics/ACM SIGGRAPH Symposium on Geometry Processing, 2003:156-164." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rotation invariant spherical harmonic representation of 3D shape descriptors">
                                        <b>[16]</b>
                                         Kazhdan M, Funkhouser T, Rusinkiewicz S.Rotation invariant spherical harmonic representation of 3D shape descriptors[C].Proceedings of the 2003 Eurographics/ACM SIGGRAPH Symposium on Geometry Processing, 2003:156-164.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_17" title=" Chen D Y, Tian X P, Shen Y T, &lt;i&gt;et al&lt;/i&gt;.On visual similarity based 3D model retrieval[J].Computer Graphics Forum, 2003, 22 (3) :223-232." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000017796&amp;v=MDA1NjRGU2psV3I3TUpGND1OaWZjYXJPNEh0SE1yNDVDWStJSlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         Chen D Y, Tian X P, Shen Y T, &lt;i&gt;et al&lt;/i&gt;.On visual similarity based 3D model retrieval[J].Computer Graphics Forum, 2003, 22 (3) :223-232.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_18" title=" Xiao J S, Liu E Y, Zhu L, &lt;i&gt;et al&lt;/i&gt;.Improved image super-resolution algorithm based on convolutional neural network[J].Acta Optica Sinica, 2017, 37 (3) :0318011.肖进胜, 刘恩雨, 朱力, 等.改进的基于卷积神经网络的图像超分辨率算法[J].光学学报, 2017, 37 (3) :0318011." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201703012&amp;v=MjM4NTZxQnRHRnJDVVJMT2VaZVZ1RnlIa1Y3dklJalhUYkxHNEg5Yk1ySTlFWm9RS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Xiao J S, Liu E Y, Zhu L, &lt;i&gt;et al&lt;/i&gt;.Improved image super-resolution algorithm based on convolutional neural network[J].Acta Optica Sinica, 2017, 37 (3) :0318011.肖进胜, 刘恩雨, 朱力, 等.改进的基于卷积神经网络的图像超分辨率算法[J].光学学报, 2017, 37 (3) :0318011.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_19" title=" He M Y, Li B, Chen H H.Multi-scale 3D deep convolutional neural network for hyperspectral image classification[C].2017 IEEE International Conference on Image Processing, 2017:3904-3908." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-scale 3D deep convolutional neural network for hyperspectral image classification">
                                        <b>[19]</b>
                                         He M Y, Li B, Chen H H.Multi-scale 3D deep convolutional neural network for hyperspectral image classification[C].2017 IEEE International Conference on Image Processing, 2017:3904-3908.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_20" title=" Gupta S, Girshick R, Arbel&#225;ez P, &lt;i&gt;et al&lt;/i&gt;.Learning rich features from RGB-D images for object detection and segmentation[M]∥Gupta S, Girshick R, Arbel&#225;ez P, &lt;i&gt;et al&lt;/i&gt;.[S.l.]:Springer International Publishing, 2014:345-360." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning rich features from RGB-D images for object detection and segmentation">
                                        <b>[20]</b>
                                         Gupta S, Girshick R, Arbel&#225;ez P, &lt;i&gt;et al&lt;/i&gt;.Learning rich features from RGB-D images for object detection and segmentation[M]∥Gupta S, Girshick R, Arbel&#225;ez P, &lt;i&gt;et al&lt;/i&gt;.[S.l.]:Springer International Publishing, 2014:345-360.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_21" title=" Maturana D, Scherer S.VoxNet:a 3D convolutional neural network for real-time object recognition[C].2015 IEEE/RSJ International Conference on Intelligent Robots and Systems, Hamburg, 2015:922-928." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=VoxNet:A 3D convolutional neural network for real-time object recognition">
                                        <b>[21]</b>
                                         Maturana D, Scherer S.VoxNet:a 3D convolutional neural network for real-time object recognition[C].2015 IEEE/RSJ International Conference on Intelligent Robots and Systems, Hamburg, 2015:922-928.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_22" title=" Gomez-Donoso F, Garcia-Garcia A, Garcia-Rodriguez J, &lt;i&gt;et al&lt;/i&gt;.LonchaNet:a sliced-based CNN architecture for real-time 3D object recognition[C].2017 International Joint Conference on Neural Networks, 2017:412-418." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LonchaNet:a sliced-based CNN architecture for real-time 3D object recognition">
                                        <b>[22]</b>
                                         Gomez-Donoso F, Garcia-Garcia A, Garcia-Rodriguez J, &lt;i&gt;et al&lt;/i&gt;.LonchaNet:a sliced-based CNN architecture for real-time 3D object recognition[C].2017 International Joint Conference on Neural Networks, 2017:412-418.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_23" title=" Minto L, Zanuttigh P, Pagnutti G.Deep learning for 3D shape classification based on volumetric density and surface approximation clues[C].International Conference on Computer Vision Theory and Applications, 2018:317-324." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning for 3D shape classification based on volumetric density and surface approximation clues">
                                        <b>[23]</b>
                                         Minto L, Zanuttigh P, Pagnutti G.Deep learning for 3D shape classification based on volumetric density and surface approximation clues[C].International Conference on Computer Vision Theory and Applications, 2018:317-324.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_24" title=" Tran D, Bourdev L, Fergus R, &lt;i&gt;et al&lt;/i&gt;.C3D:generic features for video analysis[J].Eprint Arxiv, 2014, 2 (7) :8-17." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=C3D:generic features for video analysis">
                                        <b>[24]</b>
                                         Tran D, Bourdev L, Fergus R, &lt;i&gt;et al&lt;/i&gt;.C3D:generic features for video analysis[J].Eprint Arxiv, 2014, 2 (7) :8-17.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_25" title=" Gwak J Y.3D model classification using convolutional neural network[R].[S.l.]:Stanford University, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D model classification using convolutional neural network">
                                        <b>[25]</b>
                                         Gwak J Y.3D model classification using convolutional neural network[R].[S.l.]:Stanford University, 2016.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_26" title=" Nair V, Hinton G E.Rectified linear units improve restricted Boltzmann machines[C].International Conference on International Conference on Machine Learning, 2010:807-814." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rectified linear units improve restricted boltzmann machines">
                                        <b>[26]</b>
                                         Nair V, Hinton G E.Rectified linear units improve restricted Boltzmann machines[C].International Conference on International Conference on Machine Learning, 2010:807-814.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-27 12:05</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(04),314-324 DOI:10.3788/AOS201939.0415007            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度体素卷积神经网络的三维模型识别分类</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E5%86%9B&amp;code=07917880&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨军</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E9%A1%BA&amp;code=41735331&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王顺</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E9%B9%8F&amp;code=21806151&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周鹏</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%85%B0%E5%B7%9E%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0231149&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">兰州交通大学电子与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%85%B0%E5%B7%9E%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E8%87%AA%E5%8A%A8%E5%8C%96%E4%B8%8E%E7%94%B5%E6%B0%94%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">兰州交通大学自动化与电气工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出一种基于深度体素卷积神经网络的三维 (3D) 模型识别分类算法, 该算法使用体素化技术将3D多边形网格模型转化为体素矩阵, 并通过深度体素卷积神经网络提取该矩阵的深层特征, 以增强特征的表达能力和差异性。在ModelNet40数据集上的实验结果表明:所提算法对3D网格模型识别分类的准确率能够达到87%左右。所构建的深度体素卷积神经网络能够有效地增强3D模型的特征提取和表达能力, 提高对大规模复杂3D网格模型分类识别的准确率, 所提方法优于当前的主流方法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">计算机视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E7%BB%B4%E6%A8%A1%E5%9E%8B%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三维模型识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BD%93%E7%B4%A0%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">体素化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Softmax%E5%88%86%E7%B1%BB%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Softmax分类器;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *杨军, E-mail:yangj@mail.lzjtu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-26</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61862039, 61462059);</span>
                    </p>
            </div>
                    <h1><b>Recognition and Classification for Three-Dimensional Model Based on Deep Voxel Convolution Neural Network</b></h1>
                    <h2>
                    <span>Yang Jun</span>
                    <span>Wang Shun</span>
                    <span>Zhou Peng</span>
            </h2>
                    <h2>
                    <span>School of Electronic and Information Engineering, Lanzhou Jiaotong University</span>
                    <span>School of Automation and Electrical Engineering, Lanzhou Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>An algorithm of recognition and classification of three-dimensional (3 D) model based on deep voxel convolution neural network is proposed. The voxelization technology is used to transform 3 D polygon mesh model into a voxel matrix, and the deep features of the matrix are extracted by the deep voxel convolution neural network to enhance the expressive ability and difference of the features. The experimental results on ModelNet40 dataset show that the accuracy of the algorithm can reach about 87% for recognizing and classifying 3 D mesh model. The constructed deep voxel convolution neural network can effectively enhance the feature extraction and expression ability of 3 D model, as well as improve the classification accuracy of large-scale complex 3 D mesh models, which is better than the current mainstream methods.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=computer%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">computer vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=recognition%20of%20three-dimensional%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">recognition of three-dimensional model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=voxelization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">voxelization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Softmax%20classifier&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Softmax classifier;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-26</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="63" name="63" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="64">继数字声音、数字图像和数字视频之后, 第4代多媒体数据类型——数字几何模型已经成为数字媒体技术新的发展趋势。而作为数字几何研究主体之一的三维 (3D) 模型正被广泛应用于工业设计、影视动画、计算机游戏、虚拟现实和分子生物学等领域。随着3D模型数据的急剧增长, 对3D模型数字几何分析处理方法的研究变得越来越重要。其中的3D模型识别分类是一个重点研究, 因为它在许多视觉系统中具有至关重要的意义, 涵盖了自动驾驶、自动机器人等应用场景。实时定位与地图构建 (SLAM) 技术的应用、激光探测与测量 (LiDAR) 传感器, 以及RGB-D相机的最新进展进一步促进了3D数据的实用性<citation id="143" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>, 并有效构建了3D模型的通用数据表示方法。目前, 如何利用深度学习方法解决3D模型识别分类是现今机器视觉领域的重点研究方向。</p>
                </div>
                <div class="p1">
                    <p id="65">作为深度学习的一种应用技术, 卷积神经网络 (CNN) 可以自动提取对象特征, 从而省去了人工提取特征的步骤。CNN是一种专门用于处理具有类似网状结构数据的神经网络, 其最主要的特点是卷积操作运算, 因此在诸多应用领域, 特别是在处理与图像相关的任务 (如图像分类、图像语义分割、图像检索、目标检测等计算机视觉问题) 上表现优异。</p>
                </div>
                <div class="p1">
                    <p id="66">随着CNN研究的深入, CNN的应用已从二维 (2D) 图像处理扩展到3D模型处理。然而, 将用于处理规则采样的2D图像的CNN应用于一个由不规则多边形网格或点云建模的3D形状是一项具有挑战性的任务。例如, 在2D图像中, 每个像素都包含一个明确的位置和被分配的色彩数值, 而3D模型则多由点云或多边形网格数据构成, 这些数据仅对3D模型的表面形状进行描述。文献<citation id="144" type="reference">[<a class="sup">6</a>]</citation>提出了测地CNN (GCNN) , 通过定义一个基于极坐标的测地距离函数实现了对模型特征信息的提取, 并通过滤波器系数对目标函数进行优化。然而, 对于表面存在纹理特征的模型, 该网络结构并不能通过训练得到理想的特征描述符。文献<citation id="145" type="reference">[<a class="sup">7</a>]</citation>通过把3D形状光栅转化为在密集体素上采样的距离函数, 将3D形状转换为规则采样表示, 并在整个3D体积上应用3D CNN。随着体素分辨率增加, 存储器和计算成本增加, 因此该方法对于高分辨率体素的计算开销非常大。文献<citation id="146" type="reference">[<a class="sup">8</a>,<a class="sup">9</a>]</citation>将卷积推广到非欧几里得空间, 通过卷积核与非线性算子的堆叠形成深度CNN, 并在此基础上构建3D网格数据的特征表示。其网络系数由最小化任务特定成本的学习过程确定, 并最终用于建立形状之间的对应关系。这类方法要求将平滑的流形曲面作为输入, 并且对噪声和模型失真较为敏感, 这使得它们不适合用于非流形3D模型。</p>
                </div>
                <div class="p1">
                    <p id="67">综上所述, CNN在3D模型数字几何分析任务中的应用越来越广泛, 然而将3D多边形网格模型直接应用于CNN网络需要建立网格数据与深度学习网络之间复杂的对应关系, 因此, 直接利用CNN网络提取由多边形网格数据构成的3D模型的理想特征描述符, 进而实现3D模型的识别分类任务将变得非常困难。本文提出了一种深度体素CNN, 并用它来处理体素数据, 以得到模型的深层特征信息, 实现3D网格模型的识别分类。本研究的主要创新点和贡献有:1) 在3D模型预处理阶段, 使用体素化技术将3D多边形网格模型离散为规则二值化的3D体素数据, 并且通过旋转因子将体素数据进行旋转扩充来有效增加训练集的数据量, 从而在训练过程中有效增强网络的泛化能力, 提高网络对不同方位3D模型的识别能力。2) 在模型特征的提取过程中, 使用堆叠小卷积核的深度CNN, 以便更加有效地挖掘模型内部的隐含信息, 提取体素矩阵的深层特征, 增强特征的表达能力和差异性, 提高大规模复杂3D模型分类识别的准确率。</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag">2 相关研究工作</h3>
                <div class="p1">
                    <p id="69">3D模型识别分类的关键问题是提取模型的特征描述符, 然后通过比较其特征描述符的相似性来实现模型的识别分类。致力于计算机视觉研究领域的学者已经开发了许多用以表示3D模型的形状描述符, 例如:由表面法线和曲率构成的直方图或特征袋 (BoF) 模型描述符<citation id="147" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 基于距离、角度、三角形区域或四面体体积的模型描述符<citation id="148" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 通过测量密集采样表面点来获取局部形状直径函数的描述符<citation id="149" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。此外, 还有热核签名描述符<citation id="153" type="reference"><link href="35" rel="bibliography" /><link href="37" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>, 以及将尺度不变特征变换 (SIFT) 和加速稳健特征 (SURF) 扩展到3D体素网格的描述符<citation id="150" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。球谐函数描述符 (SPH) <citation id="151" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>和光场描述符 (LFD) <citation id="152" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>是最近流行的特征描述符, 并在模型识别分类任务中有着良好的表现。SPH是基于球谐函数的数学工具, 该方法的关键思想是根据模型在不同频率下包含的能量来描述球谐函数。由于这些函数值在旋转时不会改变, 因此可以将旋转相关的球形和体素形状描述符转换为旋转不变的特征描述符, 以提高模型的匹配性能。此外, 它还降低了描述符的维度, 提供了更加紧凑的模型特征描述符。LFD通过在光场中将相机均匀地布置在正十二面体的顶点位置上, 从正十二面体上半部的不同视点渲染对象轮廓的10个特征图中提取几何和傅里叶描述符, 该描述符对于模型平移、旋转、缩放、噪声等具有很强的稳健性。与近期采用深度学习提取特征的技术相比, 以上特征在提取过程中需要建立复杂的数学模型, 主要依靠设计者的先验知识, 并且需要手工调节参数。此外, 这些特征描述符由于特征之间的区分度不强, 只能针对特定的问题, 不能很好地适用于不同的领域, 通用性较差。</p>
                </div>
                <div class="p1">
                    <p id="70">2.5维 (2.5D) 深度传感器的广泛应用使得深度通道可用来提供模型深度附加信息, 并用于常见的CNN体系结构中。文献<citation id="154" type="reference">[<a class="sup">18</a>]</citation>提出了一种融合彩色图像和视差图像的基于9层CNN的快速道路检测算法。该算法通过数据输入层预处理方法, 将视差图变换为视差梯度图, 以强化地面特征, 降低对网络深度的要求。文献<citation id="155" type="reference">[<a class="sup">19</a>]</citation>将卷积和递归神经网络结合起来用于学习特征和对RGB-D图像进行分类。文献<citation id="156" type="reference">[<a class="sup">20</a>]</citation>提出了一种区域CNN (RCNN) , 并采用它对RGB-D图像进行识别检测, 该算法除利用水平像差之外, 还利用地心深度图像对地物高度进行编码, 并对每个像素的重力角度进行编码。综合这些编码信息, 利用CNN就可获得比使用原始深度图像更加有效的特征表示。文献<citation id="157" type="reference">[<a class="sup">7</a>]</citation>利用3D ShapeNets方法将2.5D深度图像处理为3D二进制体素网格, 该算法是第一个将CNN应用于3D模型分析的方法。文献<citation id="158" type="reference">[<a class="sup">21</a>]</citation>提出了VoxNet, 使用二进制体素网格和相应的3D CNN体系结构对3D模型进行数字几何分析。以上这些方法的优势在于它们可以处理不同来源的3D数据, 包括LiDAR点云、RGB-D深度图和多边形网格数据模型, 但计算开销会随着处理数据分辨率的提高而增加, 而且对低分辨率模型的识别分类精度不高。</p>
                </div>
                <div class="p1">
                    <p id="71">文献<citation id="159" type="reference">[<a class="sup">22</a>]</citation>提出了一种新的3D物体识别架构LonchaNet, 它将3D点云数据投影到一个平面上, 生成3个2D图像, 然后将它们输入到神经网络中。该网络架构由3个独立的GoogLeNet分支组成, 这些分支被激活连接, 并被输入到完全连接的层, 而且能学习切片的特定特征。该方法在3D形状识别分类中具有良好的表现, 但是由于网络结构复杂, 导致其泛化能力不强。文献<citation id="160" type="reference">[<a class="sup">23</a>]</citation>提出了一种对3D对象分类的深度学习框架。该框架使用多分支网络结构从3D对象中提取表面曲率、深度数据以及体素数据的不同表示, 所有三种数据表示都被输入到多分支CNN中。每个分支处理不同的数据源, 并通过使用逐渐降低分辨率的卷积层产生特征向量。提取的特征向量被输入到线性分类器中, 组合后输出, 获得最终的分类结果。但是该网络结构需要多种数据作为输入, 而且数据处理过程繁琐复杂。</p>
                </div>
                <div class="p1">
                    <p id="72">综上所述, CNN可以从原始数据中通过卷积、池化和非线性激活函数映射等一系列操作的层层堆叠, 将高级语义信息逐层由原始数据输入层中抽取出来, 最后生成一个有效的特征。本课题组在文献<citation id="161" type="reference">[<a class="sup">7</a>]</citation>的基础上, 将3D体素数据作为输入, 通过3D ShapeNets将CNN的应用从2D图像提升到3D模型, 并结合文献<citation id="162" type="reference">[<a class="sup">24</a>]</citation>在3D ConvNets中通过堆叠使用小卷积核来提高识别准确度的思想, 提出了一种处理分析体素数据的深度体素CNN (DVCNN) 算法, 旨在提高大规模、多类别、复杂3D模型的识别分类精度。首先, 将3D多边形网格模型离散为规则二值化的3D体素数据, 即用体素单元表示3D网格数据;然后通过深度体素CNN的卷积、池化等操作提取模型的深层特征, 并使用Softmax分类器对模型进行分类预测。</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag">3 多边形网格模型的体素化</h3>
                <div class="p1">
                    <p id="74">多边形网格是计算机图形学中表示多面体形状的顶点与多边形的集合, 这些网格通常是由三角形、四边形或其他简单的凸多边形组成的非结构网格。多边形网格组成的3D模型不仅渲染过程简单, 而且可以很好地控制网格的密度, 得到不同稀疏程度的模型。本研究提出采用深度体素CNN对网格模型进行识别分类。相对于传统的多边形网格表示, 3D模型体素化的意义在于以体素的方式描述几何物体具有简单、稳定及不需要拓扑结构信息等特点, 而且还可以将网格模型数据转化为规则信号表示, 能够作为深度CNN的有效输入, 从而有利于高效特征描述符的提取。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>3.1 体素化</b></h4>
                <div class="p1">
                    <p id="76">体素是体图形学中描述体模型的基本数据单元。体素化是指在保证精度的前提下, 将由三角面片或其他边界形式组成的几何模型转化为离散的体素集合表示的过程。</p>
                </div>
                <div class="p1">
                    <p id="77">本研究采用一种简单有效的体素化操作方法快速地对3D网格模型进行体素化。首先判断模型表面的体素化单元是否被标记, 以实现模型的体素化, 然后计算细分过程中所有网格细分顶点与体素单元的对应关系。为了建立网格细分采样顶点与体素的对应关系, 需要通过刚性缩放变换将模型的实际尺寸转换至大小统一的轴对齐包围盒 (AABB) 。多边形网格模型顶点坐标的最小值和最大值分别记为 (<i>x</i><sub>min</sub>, <i>x</i><sub>max</sub>) 、 (<i>y</i><sub>min</sub>, <i>y</i><sub>max</sub>) 、 (<i>z</i><sub>min</sub>, <i>z</i><sub>max</sub>) , 则所建立的模型AABB包围盒是以点 (<i>x</i><sub>min</sub>, <i>y</i><sub>mix</sub>, <i>z</i><sub>min</sub>) 和点 (<i>x</i><sub>max</sub>, <i>y</i><sub>max</sub>, <i>z</i><sub>max</sub>) 为对角顶点构成的立方体。 (<i>x</i><sub>min</sub>, <i>y</i><sub>mix</sub>, <i>z</i><sub>min</sub>) 为体素单元坐标系的原点, 体素单元坐标系的坐标轴方向与原有坐标轴方向平行, 将模型的AABB包围盒与<i>X</i>轴、<i>Y</i>轴、<i>Z</i>轴平行的边分别进行等距离平分。若等分过程中边长不足则向上取整。过等分点用垂直于等分边的平面将AABB包围盒分割成<i>N</i><sub><i>x</i></sub>×<i>N</i><sub><i>y</i></sub>×<i>N</i><sub><i>z</i></sub>个正方体。AABB包围盒沿<i>X</i>轴、<i>Y</i>轴、<i>Z</i>轴分割的行<i>N</i><sub><i>x</i></sub>、列<i>N</i><sub><i>y</i></sub>、层数<i>N</i><sub><i>z</i></sub>分别为</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ν</mi><msub><mrow></mrow><mi>x</mi></msub><mo>=</mo><mtext>c</mtext><mtext>e</mtext><mtext>i</mtext><mtext>l</mtext><mrow><mo> (</mo><mrow><mfrac><mrow><mi>x</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>u</mtext></mrow></msub></mrow></mfrac></mrow><mo>) </mo></mrow><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Ν</mi><msub><mrow></mrow><mi>y</mi></msub><mo>=</mo><mtext>c</mtext><mtext>e</mtext><mtext>i</mtext><mtext>l</mtext><mrow><mo> (</mo><mrow><mfrac><mrow><mi>y</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>u</mtext></mrow></msub></mrow></mfrac></mrow><mo>) </mo></mrow><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Ν</mi><msub><mrow></mrow><mi>z</mi></msub><mo>=</mo><mtext>c</mtext><mtext>e</mtext><mtext>i</mtext><mtext>l</mtext><mrow><mo> (</mo><mrow><mfrac><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>z</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>u</mtext></mrow></msub></mrow></mfrac></mrow><mo>) </mo></mrow><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">式中:<i>L</i><sub>cu</sub>为正方体体素单元的边长;ceil (·) 为向上取整函数, 用以保证体素单元的存储空间能够描述模型的AABB包围盒。通过 (4) 、 (5) 、 (6) 式可以计算坐标点 (<i>x</i>, <i>y</i>, <i>z</i>) 在体素存储空间中对应的体素单元, 并将位于第<i>N</i><sub>ro</sub>行、第<i>N</i><sub>co</sub>列、第<i>N</i><sub>la</sub>层的体素单元进行标记。<i>N</i><sub>ro</sub>、<i>N</i><sub>co</sub>、<i>N</i><sub>la</sub>的表达式分别为</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>o</mtext></mrow></msub><mo>=</mo><mtext>c</mtext><mtext>e</mtext><mtext>i</mtext><mtext>l</mtext><mrow><mo> (</mo><mrow><mfrac><mrow><mi>x</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>u</mtext></mrow></msub></mrow></mfrac></mrow><mo>) </mo></mrow><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext></mrow></msub><mo>=</mo><mtext>c</mtext><mtext>e</mtext><mtext>i</mtext><mtext>l</mtext><mrow><mo> (</mo><mrow><mfrac><mrow><mi>y</mi><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>u</mtext></mrow></msub></mrow></mfrac></mrow><mo>) </mo></mrow><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>a</mtext></mrow></msub><mo>=</mo><mtext>c</mtext><mtext>e</mtext><mtext>i</mtext><mtext>l</mtext><mrow><mo> (</mo><mrow><mfrac><mrow><mi>z</mi><mo>-</mo><mi>z</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>u</mtext></mrow></msub></mrow></mfrac></mrow><mo>) </mo></mrow><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">由AABB求交运算可以得到这些坐标点能影响到的体素单元, 将这些体素单元作为待判断是否被体素化的基本对象。体素单元的值被映射到集合{0, 1}中, 将被赋值为“1”的体素称为“黑色”或“非空”体素, 而将被赋值为“0”的体素称为“白色”或“空”体素, 这样就完成了对3D网格模型的体素化操作。为了能够使体素化处理的网格模型经卷积、池化等操作之后输出的特征图分辨率大小为偶数, 并结合计算机的处理能力等因素, 将多边形网格模型体素化至分辨率为32×32×32的体素矩阵中。图1为4个多边形网格模型的体素化结果。图1 (a) 为渲染后的4个3D模型, 图1 (b) 为对应的3D网格模型, 图1 (c) 是经过体素化处理后的体素模型。</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904037_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 3D网格模型的体素化。 (a) 渲染后的模型; (b) 网格模型; (c) 体素模型" src="Detail/GetImg?filename=images/GXXB201904037_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 3D网格模型的体素化。 (a) 渲染后的模型; (b) 网格模型; (c) 体素模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904037_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Voxelization of 3D mesh models. (a) Rendered models; (b) mesh models; (c) voxelization models</p>

                </div>
                <div class="p1">
                    <p id="83">文献<citation id="163" type="reference">[<a class="sup">7</a>]</citation>将3D形状光栅转化为在密集体素上采样的距离函数, 将3D几何形状的深度图表示为2D变量在3D体素网格上的概率分布, 从而完成体素化。文献<citation id="164" type="reference">[<a class="sup">21</a>]</citation>计算了射线对空间点云数据区域命中率与穿透率的比值, 之后通过这一比值来判断该区域的点云覆盖概率, 并将这一思想作为空间占用网格模型的计算依据;然后, 采用3D射线对每个网格单元的命中状态和穿透状态进行计算, 从而完成点云数据的体素化过程。与以上方法不同, 本研究采用的体素化方法是将3D网格模型按照需要的体素分辨率进行网格划分, 然后通过求交运算来判断模型的坐标点与网格是否有交集, 得到坐标点所能影响到的体素单元, 然后将这些体素单元进行渲染, 最终完成体素化。</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84"><b>3.2 数据扩充</b></h4>
                <div class="p1">
                    <p id="85">CNN自身拥有强大的表达能力, 但网络本身需要大量甚至海量数据来驱动模型训练, 否则便有极大可能陷入过拟合的窘境。实际上, 并不是所有的数据集或真实任务都能提供像ImageNet数据集一样的海量训练样本, 因此, 数据扩充便成为深度学习模型训练的第一步。有效的数据扩充不仅能扩充训练样本的数量, 还能增加训练样本的多样性, 从而可以避免过拟合, 同时也可以使机器学习模型拥有更强的泛化能力, 提升学习模型的性能。数据扩充常用的简单方法有图像水平翻转、随机抠取、尺度变换和旋转等。文献<citation id="165" type="reference">[<a class="sup">25</a>]</citation>采用对体素施加随机变换的方法扩充数据, 首先通过随机选择旋转角度、平移量以及比例参数对原始数据集进行随机增加, 之后对其进行体素化。而所提方法则是将体素化后的3D网格模型按照特定的角度进行有规律的旋转, 从而得到相应倍数的体素模型增强数据集。具体方法为:首先设置旋转角度<i>θ</i>来确定生成的模型旋转副本的个数<i>n</i>;然后通过旋转变换矩阵<b><i>R</i></b>将每个模型绕<i>Z</i>轴均匀旋转来为每个模型生成<i>n</i>个新的姿态, 并将其增加到模型数据集中。旋转副本个数<i>n</i>与旋转变换的计算公式分别为</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>n</mi><mo>=</mo><mfrac><mrow><mn>3</mn><mn>6</mn><mn>0</mn><mo>°</mo></mrow><mi>θ</mi></mfrac><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><msup><mi mathvariant="bold-italic">Μ</mi><mo>′</mo></msup><mo>=</mo><mi mathvariant="bold-italic">R</mi><mo>⋅</mo><mi mathvariant="bold-italic">Μ</mi><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">R</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>cos</mi><mspace width="0.25em" /><mi>θ</mi></mtd><mtd><mo>-</mo><mi>sin</mi><mspace width="0.25em" /><mi>θ</mi></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mi>sin</mi><mspace width="0.25em" /><mi>θ</mi></mtd><mtd><mi>cos</mi><mspace width="0.25em" /><mi>θ</mi></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">式中:<b><i>M</i></b>为源模型;<b><i>M</i></b>′为旋转后的目标模型副本。图2展示了数据集中马桶和椅子模型通过30°旋转后的数据扩充, 每个模型生成了12个副本。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 3D模型的旋转数据扩充。 (a) 马桶模型; (b) 椅子模型" src="Detail/GetImg?filename=images/GXXB201904037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 3D模型的旋转数据扩充。 (a) 马桶模型; (b) 椅子模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Data expansion of 3D models by rotation transformation. (a) Toilet models; (b) chair models</p>

                </div>
                <h3 id="89" name="89" class="anchor-tag">4 基于深度体素CNN的三维模型识别分类</h3>
                <div class="p1">
                    <p id="90">对象识别分类的目标是将类别信息分配给每个对象, 这是理解3D形状的基本任务。本研究提出了一种针对体素化模型的深度体素CNN, 用以实现3D模型的识别分类任务。首先通过深度体素CNN提取体素模型的深层特征信息, 然后通过Softmax分类器对提取到的特征信息进行概率计算, 以实现对3D模型的识别分类。</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91"><b>4.1 基于卷积网络的三维模型深层特征的提取</b></h4>
                <div class="p1">
                    <p id="92">CNN是第一个真正成功训练多层网络结构的深度学习算法, 被广泛用于解决如何学习和提取图像数据的深层特征问题。其基本思想是将图像的局部感受野作为网络的输入, 信息依次被传输到不同的层, 并通过一个数字滤波器获取对平移、旋转和尺度变换具有不变性的显著特征。因为权值共享和池化可以大大减少模型参数的个数, 所以将提取图像深层特征的2D CNN推广至3D的方法, 可以被用来进行有效的3D数据的特征提取。</p>
                </div>
                <div class="p1">
                    <p id="93">如图3所示, 2D CNN应用于图像上的2D卷积操作将输出一幅2D图像, 而3D CNN在3D数据上进行卷积操作则会输出另外一个3D数据。在图3 (a) 中, <i>H</i>、<i>W</i>分别为2D图像数据的高和宽, <i>k</i>为卷积核的大小;图3 (b) 中的<i>H</i>、<i>W</i>、<i>L</i>分别表示3D模型数据的高、宽和长, 其中卷积核大小由2D中的<i>k</i>×<i>k</i>更改为<i>k</i>×<i>k</i>×<i>d</i>。<i>d</i>&lt;<i>L</i>表示卷积核的长度应小于3D模型的长度。图3 (a) 、 (b) 中的折线分别表示进行2D卷积操作和3D卷积操作时卷积核移动的方向。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904037_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 卷积操作。 (a) 2D; (b) 3D" src="Detail/GetImg?filename=images/GXXB201904037_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 卷积操作。 (a) 2D; (b) 3D  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904037_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Convolution operations. (a) 2D; (b) 3D</p>

                </div>
                <div class="p1">
                    <p id="95">图4所示为本研究所构建的用于提取3D体素数据深层特征描述符的深度体素CNN结构, 该结构共包含8个卷积层和4个池化层, 网络的输入是分辨率大小为32×32×32的体素数据, 输出为16384×1的列向量, 即体素模型的深层特征。图中的Conv为卷积层, pool为池化层, FL为平整层。卷积层中的数字表示每层所拥有的卷积核个数, 例如:Conv1a 32中的数字32表示第1层卷积核的个数为32, Conv4a 256中的数字256表示第4层卷积核的个数为256。平整层将经过处理后的特征图中的元素按照行的顺序进行排序, 并将所有特征图中排列好的元素存储在一个列向量中。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904037_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 CNN结构" src="Detail/GetImg?filename=images/GXXB201904037_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 CNN结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904037_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Structure of convolutional neural network</p>

                </div>
                <div class="p1">
                    <p id="97">在DVCNN中, 定义3D张量<b><i>x</i></b><sup><i>l</i></sup>∈<b>R</b><sup><i>H</i><sup><i>l</i></sup>×<i>W</i><sup><i>l</i></sup>×<i>L</i><sup><i>l</i></sup></sup>为CNN第<i>l</i>层的输入, 用三元组 (<i>i</i><sup><i>l</i></sup>, <i>j</i><sup><i>l</i></sup>, <i>k</i><sup><i>l</i></sup>) 来指示该张量对应第<i>i</i><sup><i>l</i></sup>行、第<i>j</i><sup><i>l</i></sup>列、第<i>k</i><sup><i>l</i></sup>层位置的元素, 其中0≤<i>i</i><sup><i>l</i></sup>&lt;<i>H</i><sup><i>l</i></sup>, 0≤<i>j</i><sup><i>l</i></sup>&lt;<i>W</i><sup><i>l</i></sup>, 0≤<i>k</i><sup><i>l</i></sup>&lt;<i>L</i><sup><i>l</i></sup>。<b><i>x</i></b><sup><i>l</i></sup>经过第<i>l</i>层的操作处理后可得<b><i>x</i></b><sup><i>l</i>+1</sup>, 为了书写方便, 特将此简写为<b><i>y</i></b>, 作为第<i>l</i>层对应的输出, 即:<b><i>y</i></b>=<b><i>x</i></b><sup><i>l</i>+1</sup>∈<b>R</b><sup><i>H</i><sup><i>l</i>+1</sup>×<i>W</i><sup><i>l</i>+1</sup>×<i>L</i><sup><i>l</i>+1</sup></sup>。</p>
                </div>
                <div class="p1">
                    <p id="98">在卷积层, 第<i>l</i>层与可自学习的卷积核<i>K</i><sub><i>i</i>, <i>j</i>, <i>k</i></sub>进行卷积, 卷积的结果经过非线性函数<i>g</i> (·) 后生成这一层的特征图<b><i>y</i></b>, 具体形式为</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">y</mi><mo>=</mo><mi>g</mi><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>Η</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>W</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>L</mi></munderover><mi mathvariant="bold-italic">Κ</mi></mstyle></mrow></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>k</mi></mrow></msub><mo>⊗</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><msup><mrow></mrow><mi>l</mi></msup><mo>, </mo><mi>j</mi><msup><mrow></mrow><mi>l</mi></msup><mo>, </mo><mi>k</mi><msup><mrow></mrow><mi>l</mi></msup></mrow><mi>l</mi></msubsup><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>k</mi></mrow></msub></mrow><mo>) </mo></mrow><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">式中:⨂为卷积运算;<b><i>b</i></b><sub><i>i</i>, <i>j</i>, <i>k</i></sub>为偏置;卷积核<b><i>K</i></b><sub><i>i</i>, <i>j</i>, <i>k</i></sub>可与前一层的一个或者多个特征图进行卷积运算。常用的非线性激活函数通常包括双曲正切函数、Sigmoid函数和修正线性单元 (ReLU) , 它们分别表示为</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>g</mi><msub><mrow></mrow><mtext>Τ</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mi>tanh</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>g</mi><msub><mrow></mrow><mtext>S</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>+</mo><mi>exp</mi><mo stretchy="false"> (</mo><mo>-</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow></mfrac><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>g</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mi>max</mi><mo stretchy="false"> (</mo><mn>0</mn><mo>, </mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mi>x</mi><mo>, </mo></mtd><mtd><mtext>i</mtext><mtext>f</mtext></mtd><mtd><mi>x</mi><mo>≥</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo></mtd><mtd><mtext>i</mtext><mtext>f</mtext></mtd><mtd><mi>x</mi><mo>&lt;</mo><mn>0</mn></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">相比于双曲正切函数和Sigmoid函数, ReLU能够有效提高训练效率<citation id="166" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>。本研究使用ReLU作为网络的激活函数。卷积层每层生成的特征图大小可表示为</p>
                </div>
                <div class="p1">
                    <p id="103" class="code-formula">
                        <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mrow><mi>m</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></msub><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mrow><mi>m</mi><msub><mrow></mrow><mi>l</mi></msub></mrow></msub><mo>+</mo><mn>2</mn><mo>×</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>l</mi></msub><mo>-</mo><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi>l</mi></msub></mrow><mi>λ</mi></mfrac><mo>+</mo><mn>1</mn><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="104">式中:<b><i>F</i></b><sub><i>m</i><sub><i>l</i>+1</sub></sub>为第<i>l</i>+1层特征图的大小;<b><i>F</i></b><sub><i>m</i><sub><i>l</i></sub></sub>为第<i>l</i>层特征图的大小;<b><i>K</i></b><sub><i>l</i></sub>为第<i>l</i>层卷积核的大小;<i>λ</i>为卷积核移动的步长;<b><i>P</i></b><sub><i>l</i></sub>为在进行卷积运算时对前一层特征图边缘填充补零的列数。在本研究中, 卷积层采用的卷积核大小均为3×3×3, 步长为1, 填充补零列数为1, 该参数设置可以保证在每层卷积运算完成之后, 前一层特征图的大小与当前层特征图的大小一致。通过堆叠使用3×3×3大小的卷积核, 可以提取到更深层次的特征信息。</p>
                </div>
                <div class="p1">
                    <p id="105">在池化层, 本研究采用最大值池化操作。在每次运算时, 将池化核覆盖区域中所有值的最大值作为池化结果, 即</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">y</mi><mo>=</mo><mi>max</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><msup><mrow></mrow><mi>l</mi></msup><mo>, </mo><mi>j</mi><msup><mrow></mrow><mi>l</mi></msup><mo>, </mo><mi>k</mi><msup><mrow></mrow><mi>l</mi></msup></mrow><mi>l</mi></msubsup><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">式中:max (·) 为取最大值操作。池化层没有需要学习的参数, 使用时只需指定池化类型、池化核大小以及池化操作的步长。池化操作一方面可以增强特征不变性, 使网络更关注是否存在某些特征而不是特征的具体位置;另一方面, 由于池化的降采样作用, 相当于在空间范围内做了维度约减, 从而使网络可以抽取更广范围的特征, 同时减少了下一层输入的大小, 进而减小计算量和减少参数个数。本研究采用的池化核大小为2×2×2, 池化步长为2, 该参数设置可以使每次池化后的特征图的大小缩减为原来的1/2。</p>
                </div>
                <div class="p1">
                    <p id="108">平整层将之前卷积层和池化层提取的特征图进行向量化, 使之呈列向量的形式, 即把多维输入一维化。通过卷积和池化操作将大小为32×32×32的体素数据运算为256个4×4×4大小的特征图, 经过平整层之后变为一个16384×1的列向量, 即为算法最终提取到的体素模型数据的深层特征信息。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109"><b>4.2 基于Softmax分类器的三维模型分类</b></h4>
                <div class="p1">
                    <p id="110">为了执行3D模型识别分类这一任务, 需要将CNN提取的3D模型特征进行概率计算, 因此在平整层之后还应该连接全连接层 (FC) 。全连接层可以将学习到的特征表示映射到样本的标记空间中。图5所示为3D模型识别分类的示意图, 设计了两个全连接层、两个dropout层和一个Softmax层。</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904037_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 3D模型识别分类示意图" src="Detail/GetImg?filename=images/GXXB201904037_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 3D模型识别分类示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904037_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Schematic of 3D model recognition and classification</p>

                </div>
                <div class="p1">
                    <p id="112">在图5中, FC为全连接层, 第一个全连接层包含了128个神经元, 第二个全连接层包含了<i>C</i>个神经元, <i>C</i>为3D模型识别分类任务的类别数。dropout层可以缓解神经元之间复杂的协同适应, 降低神经元的依赖, 避免网络发生过拟合。此外, dropout层还可以降低模型的复杂度, 增强模型的泛化能力, 显著降低运算量。将隐含节点的dropout丢弃率设置为0.5, 主要因为在该参数下dropout随机生成的网络结构最多, 此时带来的效果也最好。在训练阶段, 对于某层的神经元以概率<i>p</i>随机将该神经元权重置0, 测试阶段所有神经元均呈激活状态, 但其权重需乘以 (1-<i>p</i>) 来保证训练和测试阶段各自的权重拥有相同的期望。</p>
                </div>
                <div class="p1">
                    <p id="113">Softmax在机器学习和深度学习中有着非常广泛的应用, 尤其是在处理多分类问题时, 分类器最后的输出单元需要用Softmax函数进行数值处理, 将多分类的输出数值转化为相对概率。Softmax函数的定义为</p>
                </div>
                <div class="p1">
                    <p id="114" class="code-formula">
                        <mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mrow><mtext>e</mtext><mtext>x</mtext><mtext>p</mtext></mrow><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="115">式中:<i>v</i><sub><i>i</i></sub>为分类器前级输出单元的输出, 其中的<i>i</i>为类别索引标号;<i>S</i><sub><i>i</i></sub>为分类器前级输出单元输出元素的指数与所有元素指数之和的比值。</p>
                </div>
                <div class="p1">
                    <p id="116">使用交叉熵损失函数作为分类目标函数, 其又称为Softmax损失函数, 定义为</p>
                </div>
                <div class="p1">
                    <p id="117" class="code-formula">
                        <mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ℓ</mi><msub><mrow></mrow><mrow><mtext>C</mtext><mtext>E</mtext><mtext>L</mtext></mrow></msub><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mtext>l</mtext></mstyle><mtext>n</mtext><mrow><mo>[</mo><mrow><mfrac><mrow><mrow><mtext>e</mtext><mtext>x</mtext><mtext>p</mtext></mrow><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>]</mo></mrow><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="118">式中:<i>m</i>为训练样本集中样本的个数;<i>y</i><sub><i>j</i></sub>为网络的输入样本;<i>j</i>为输入的样本标号;<i>v</i><sub><i>y</i><sub><i>j</i></sub></sub>为分类器前级输出单元对应的输入样本的真实分类值。得到目标函数之后使用误差反向传播 (BP) 算法对参数权重进行调节优化。</p>
                </div>
                <h3 id="119" name="119" class="anchor-tag">5 实验结果与分析</h3>
                <div class="p1">
                    <p id="120">本算法的实验环境为Linux Ubuntu 16.04操作系统, 硬件环境为Intel Xeon E5-2620 v4 CPU和NVIDIA Quadro M4000 GPU (8 GB内存) 处理器, 编程开发环境为CUDA-Toolkit 8.0, 编程语言为Python 2.7, 机器学习框架为Theano。</p>
                </div>
                <h4 class="anchor-tag" id="121" name="121"><b>5.1 实验数据集</b></h4>
                <div class="p1">
                    <p id="122">本研究选用ModelNet数据集进行实验, 该数据集中包括来自622类的127915个3D模型, 其子集ModelNet10数据集包括10类4899个3D模型, 子集ModelNet40数据集包括40类12311个3D模型。由于实验中选用的网络需要大量的数据进行训练, 因此选用ModelNet40数据集进行实验。</p>
                </div>
                <div class="p1">
                    <p id="123">在进行神经网络训练前需要对模型数据进行预处理, 一方面采用2.1节所述方法将3D网格模型体素化至32×32×32分辨率的体素单元中;另一方面通过2.2节所述方法将体素化后的网格模型进行旋转变换来扩充训练数据集, 实验中选择4个旋转角度, 分别为120°、60°、40°、30°, 从而可以获得3倍、6倍、9倍、12倍的4个增强数据集。</p>
                </div>
                <h4 class="anchor-tag" id="124" name="124"><b>5.2 神经网络参数设置与性能评价</b></h4>
                <div class="p1">
                    <p id="125">实验中使用的网络结构如第4节所示。通过基于动量的随机梯度下降 (SGD) 来优化网络, 设置动量因子为0.9, 权重衰减为0.0005, 初始学习率为0.001, 学习衰减率为0.01, dropout丢弃率为0.5。使用随机初始化方式初始化网络参数, 随机参数均服从均值为0, 方差为1的标准高斯分布。训练结束后将得到的网络超参数固定在深度CNN当中, 并将测试数据集在3D模型识别分类任务中进行测试。3D模型识别分类性能指标选择准确率, 其定义为</p>
                </div>
                <div class="p1">
                    <p id="126" class="code-formula">
                        <mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>c</mtext><mtext>c</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mtext>c</mtext></msub></mrow><mrow><mi>S</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo>+</mo><mi>S</mi><msub><mrow></mrow><mtext>e</mtext></msub></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="127">式中:<i>S</i><sub>c</sub>为被正确分类的样本个数;<i>S</i><sub>e</sub>为被错误分类的样本个数。</p>
                </div>
                <h4 class="anchor-tag" id="128" name="128"><b>5.3 实验结果与分析</b></h4>
                <div class="p1">
                    <p id="129">为验证训练数据集样本个数对网络的影响, 本研究分别在无旋转、旋转120°、旋转60°、旋转40°和旋转30°等5个扩充数据集上分别对网络性能进行训练并测试, 实验结果如表1所示。可以看出:随着旋转角度减小, 3D模型识别分类的准确率逐渐增大。也就是说, 3D模型识别分类的准确率随训练数据集样本个数的增多而增加, 因为更多的训练样本增加了训练样本的多样性, 使得网络可以学习更多的模型特征。此外, 从表1中的数据还可以看出, 旋转40°扩充的9倍数据集与旋转30°扩充的12倍数据集的模型识别分类准确率相差在1%之内。原因在于, 随着旋转角度的减小, 通过旋转变换扩充的数据集获得的模型间差异会越来越小, 网络能够提取到的有效特征信息也会越来越少, 因此通过旋转变换获得的扩充数据集对模型识别分类准确率带来的提高也会逐步收敛。</p>
                </div>
                <div class="area_img" id="130">
                    <p class="img_tit">表1 不同旋转角度扩充数据集中3D模型识别分类的准确率 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Accuracy rate for recognition and classification of 3D models in expanded dataset with different rotation angles</p>
                    <p class="img_note"></p>
                    <table id="130" border="1"><tr><td><br />Rotation angle / (°) </td><td>Accuracy rate /%</td></tr><tr><td><br />0</td><td>70.6</td></tr><tr><td><br />120</td><td>77.1</td></tr><tr><td><br />60</td><td>83.5</td></tr><tr><td><br />40</td><td>87.1</td></tr><tr><td><br />30</td><td>87.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="132">此外, 还探究了卷积核大小对模型性能的影响。表2为网络结构中采用大小为3×3×3的卷积核与5×5×5的卷积核的实验结果, 实验中均采用旋转30°扩充的数据集进行网络训练。可以看出:通过使用堆叠多个3×3×3卷积核的模型分类识别的准确率高于使用5×5×5卷积核的结果。因为小卷积核卷积层与非线性激活层交替的结构, 比大卷积核卷积层的结构更能提取出深层的特征。其次, 小卷积核加深了网络深度, 进而增大了网络容量和复杂度, 同时减少了参数个数。唯一的不足是, 在进行反向传播时, 中间的卷积层可能会导致占用更多的内存。</p>
                </div>
                <div class="area_img" id="133">
                    <p class="img_tit">表2 不同尺寸卷积核下3D模型识别分类的准确率 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Accuracy rate for recognition and classification of 3D models obtained at different sizes of convolution kernel</p>
                    <p class="img_note"></p>
                    <table id="133" border="1"><tr><td><br />Kernel size</td><td>Accuracy rate /%</td></tr><tr><td><br />5×5×5</td><td>84.3</td></tr><tr><td><br />3×3×3</td><td>87.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="134">此外, 本课题组还探究了体素分辨率对识别分类准确率的影响。在采用12倍增强数据集以及3×3×3卷积核参数条件下对不同分辨率的体素模型进行实验。表3是分辨率为24×24×24和32×32×32的对比实验结果, 可以看出:3D模型识别分辨率的准确率随着体素模型分辨率的增加而增大, 因为高分辨率的体素模型更能还原3D模型的真实面貌, 但其计算时间会随之增加。</p>
                </div>
                <div class="area_img" id="135">
                    <p class="img_tit">表3 不同分辨率下3D模型识别分类的准确率 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Accuracy rate for recognition and classification of 3D models obtained at different resolutions</p>
                    <p class="img_note"></p>
                    <table id="135" border="1"><tr><td><br />Resolution</td><td>Recognition accuracy rate /%</td></tr><tr><td><br />24×24×24</td><td>81.1</td></tr><tr><td><br />32×32×32</td><td>87.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="136">为了验证所提算法的优越性, 将其分别与文献<citation id="167" type="reference">[<a class="sup">15</a>]</citation>、文献<citation id="168" type="reference">[<a class="sup">16</a>]</citation>和文献<citation id="169" type="reference">[<a class="sup">7</a>]</citation>中采用的主流算法在ModelNet40数据集上比较了对3D模型识别分类的准确率。文献<citation id="170" type="reference">[<a class="sup">15</a>]</citation>和文献<citation id="171" type="reference">[<a class="sup">16</a>]</citation>采用的是传统人工特征描述符, 即SPH和LFD。SPH采用基于球谐函数的替代方法, 可以获得旋转不变的特征描述符, 从而可以提高匹配性能。LFD从几个不同视点渲染的对象轮廓中提取几何和傅里叶描述符, 并且可以直接应用于形状分类任务。文献<citation id="172" type="reference">[<a class="sup">7</a>]</citation>提出了3D ShapeNets神经网络, 该网络是一个具有5层结构的CNN。通过深度卷积置信网络提取体素矩阵的特征, 根据输入数据的标签与网络预测标签的联合分布进行模型的识别分类。本研究堆叠使用小卷积核的卷积层, 在减少网络参数的同时增加了网络容量, 网络的深度达到了10层, 可以挖掘3D模型更多内在的隐含信息, 提取更加有效的深层特征, 从而提高了模型识别分类的准确率。从表4中数据可以看出, 所提算法明显优于其他几种算法。</p>
                </div>
                <div class="p1">
                    <p id="137">图6为深度体素CNN对ModelNet40数据集中部分模型进行识别分类的结果。其中:Instance表示模型在测试数据集中的位置, 包含了模型的类别和序号;Predicted label为模型的预测标签;True label为模型的真实标签。若True label与Predicted label一致, 则表明识别分类为正确, 否则认为识别分类错误。图中用方框标识了错误的识别分类结果。识别分类错误的原因在于体素数据只能表示真实网格模型的近似模样, 对于比较相像的两个体素模型则会出现识别分类错误的情况。</p>
                </div>
                <div class="area_img" id="138">
                    <p class="img_tit">表4 不同算法下3D模型识别分类的准确率 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Accuracy rate for recognition and classification of 3D models obtained with different algorithms</p>
                    <p class="img_note"></p>
                    <table id="138" border="1"><tr><td><br />Algorithm</td><td>Recognition accuracy rate /%</td></tr><tr><td><br />SPH<sup>[15]</sup></td><td>68.2</td></tr><tr><td><br />LFD<sup>[16]</sup></td><td>75.5</td></tr><tr><td><br />3D ShapeNets<sup>[7]</sup></td><td>77.3</td></tr><tr><td><br />Proposed algorithm</td><td>87.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904037_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 3D模型识别分类结果" src="Detail/GetImg?filename=images/GXXB201904037_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 3D模型识别分类结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904037_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Recognition and classification for 3D models</p>

                </div>
                <h3 id="141" name="141" class="anchor-tag">6 结 论</h3>
                <div class="p1">
                    <p id="142">3D模型识别分类问题是计算机图形学和计算机视觉领域的一个重要研究课题, 更是一个难点问题。本研究提出了一种基于深度体素CNN的3D模型分类识别算法。首先使用体素化技术将3D网格模型离散为规则的体素单元, 然后通过深度体素CNN提取体素模型的深层特征, 充分利用神经网络的学习能力实现对3D模型精确识别分类的目的。在ModelNet40数据集上, 所提算法通过深度体素CNN学习模型分类任务的深层特征, 使得3D模型的识别分类准确率达到了87.7%。然而, 所提算法仍存在改进的空间。一方面, 体素分辨率的大小对识别分类结果具有重要影响, 但是高分辨率的体素数据会耗费大量的计算成本, 因此可以采用更加高效的数据结构来降低网络计算量;另一方面, 深度CNN的结构也会影响3D模型识别分类的准确率。因此, 在网络结构上仍需要进一步探索, 这也是今后需要继续研究的问题。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Indoor Segmentation and Support Inference from RGBD Images">

                                <b>[1]</b> Silberman N, Hoiem D, Kohli P, <i>et al</i>.Indoor segmentation and support inference from RGBD images[M]∥Silberman N, Hoiem D, Kohli P, <i>et al</i>.Heidelberg:Computer Vision-ECCV 2012, Springer, 2012:746-760.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SUN3D:A Database of Big Spaces Reconstructed Using SfM and Object Labels">

                                <b>[2]</b> Xiao J X, Owens A, Torralba A.SUN<sub>3</sub>D:a database of big spaces reconstructed using SfM and object labels[C].Sydney:2013 IEEE International Conference on Computer Vision, 2013:1625-1632.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SUN RGB-D:A RGB-D scene understanding benchmark suite">

                                <b>[3]</b> Song S R, Lichtenberg S P, Xiao J X.SUN RGB-D:a RGB-D scene understanding benchmark suite[C].2015 IEEE Conference on Computer Vision and Pattern Recognition, 2015:567-576.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ShapeNet: An Information-Rich 3D Model Repository">

                                <b>[4]</b> Chang A X, Funkhouser T, Guibas L, <i>et al</i>.ShapeNet:an information-rich 3D model repository[J].Computer Science, 2015, 1512:3-12.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201710017&amp;v=MzE3NjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhrVjd2SUlqWFRiTEc0SDliTnI0OUVZNFFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Qu L, Wang K R, Chen L L, <i>et al</i>.Fast road detection based on RGBD images and convolutional neural network[J].Acta Optica Sinica, 2017, 37 (10) :1010003.曲磊, 王康如, 陈利利, 等.基于RGBD图像和卷积神经网络的快速道路检测[J].光学学报, 2017, 37 (10) :1010003.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Geodesic Convolutional Neural Networks on Riemannian Manifolds">

                                <b>[6]</b> Masci J, Boscaini D, Bronstein M M, <i>et al</i>.Geodesic convolutional neural networks on Riemannian manifolds[C].2015 IEEE International Conference on Computer Vision Workshop, 2015:832-840.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3d shapenets:A deep representation for volumetric shape modeling">

                                <b>[7]</b> Wu Z R, Song S R, Khosla A, <i>et al</i>.3D ShapeNets:a deep representation for volumetric shapes[C].Boston:2015 IEEE Conference on Computer Vision and Pattern Recognition, 2015:1912-1920.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning shape correspondence with anisotropic convolutional neural networks">

                                <b>[8]</b> Boscaini D, Masci J, Rodolà E, <i>et al</i>.Learning shape correspondence with anisotropic convolutional neural networks[C].Proceedings of the 30th International Conference on Neural Information Processing Systems, 2016:3197-3205.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD15081100000257&amp;v=MzEyNjFlWnVIeWptVWIvSUtWOFFiaEk9TmlmY2FySzlIdG5Ocm85RlpPc1BEbmsrb0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Boscaini D, Masci J, Melzi S, <i>et al</i>.Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks[J].Computer Graphics Forum, 2015, 34 (5) :13-23.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000004432&amp;v=MjkwMTdlWnVIeWptVWIvSUtWOFFiaEk9TmlmSVk3SzdIdGpOcjQ5RlpPc0xDSDg3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Bronstein A M, Bronstein M M, Guibas L J, <i>et al</i>.Shape google[J].ACM Transactions on Graphics, 2011, 30 (1) :1-20.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000097977&amp;v=Mjk1MDdRYmhJPU5pZklZN0s3SHRqTnI0OUZaT0lJQlhzK29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUtWOA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Osada R, Funkhouser T, Chazelle B, <i>et al</i>.Shape distributions[J].ACM Transactions on Graphics, 2002, 21 (4) :807-832.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000002893&amp;v=MTczMDRRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JS1Y4UWJoST1OaWZJWTdLN0h0ak5yNDlGWk9zTkJIVTZvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Chaudhuri S, Koltun V.Data-driven suggestions for creativity support in 3D modeling[C].ACM Transactions on Graphics, 2010:183-191.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Concise and Provably Informative Multi-Scale Signature Based on Heat Diffusion">

                                <b>[13]</b> Sun J, Ovsjanikov M, GuibasL.A concise and provably informative multi-scale signature based on heat diffusion[J].Computer Graphics Forum, 2009, 28 (5) :1383-1392.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scale-invariant heat kernel signatures for non-rigid shape recognition">

                                <b>[14]</b> Bronstein M M, Kokkinos I.Scale-invariant heat kernel signatures for non-rigid shape recognition[C].2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, 2010:1704-1711.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hough transform and 3D SURF for robust three dimensional classification">

                                <b>[15]</b> Knopp J, Prasad M, Willems G, <i>et al</i>.Hough transform and 3D SURF for robust three dimensional classification[M]//Knopp J, Prasad M, Willems G, <i>et al</i>.Heidelberg:Springer, 2010:589-602.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rotation invariant spherical harmonic representation of 3D shape descriptors">

                                <b>[16]</b> Kazhdan M, Funkhouser T, Rusinkiewicz S.Rotation invariant spherical harmonic representation of 3D shape descriptors[C].Proceedings of the 2003 Eurographics/ACM SIGGRAPH Symposium on Geometry Processing, 2003:156-164.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000017796&amp;v=MjcwMTlNcjQ1Q1krSUpZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTamxXcjdNSkY0PU5pZmNhck80SHRI&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> Chen D Y, Tian X P, Shen Y T, <i>et al</i>.On visual similarity based 3D model retrieval[J].Computer Graphics Forum, 2003, 22 (3) :223-232.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201703012&amp;v=MTAwNTh0R0ZyQ1VSTE9lWmVWdUZ5SGtWN3ZJSWpYVGJMRzRIOWJNckk5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Xiao J S, Liu E Y, Zhu L, <i>et al</i>.Improved image super-resolution algorithm based on convolutional neural network[J].Acta Optica Sinica, 2017, 37 (3) :0318011.肖进胜, 刘恩雨, 朱力, 等.改进的基于卷积神经网络的图像超分辨率算法[J].光学学报, 2017, 37 (3) :0318011.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-scale 3D deep convolutional neural network for hyperspectral image classification">

                                <b>[19]</b> He M Y, Li B, Chen H H.Multi-scale 3D deep convolutional neural network for hyperspectral image classification[C].2017 IEEE International Conference on Image Processing, 2017:3904-3908.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning rich features from RGB-D images for object detection and segmentation">

                                <b>[20]</b> Gupta S, Girshick R, Arbeláez P, <i>et al</i>.Learning rich features from RGB-D images for object detection and segmentation[M]∥Gupta S, Girshick R, Arbeláez P, <i>et al</i>.[S.l.]:Springer International Publishing, 2014:345-360.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=VoxNet:A 3D convolutional neural network for real-time object recognition">

                                <b>[21]</b> Maturana D, Scherer S.VoxNet:a 3D convolutional neural network for real-time object recognition[C].2015 IEEE/RSJ International Conference on Intelligent Robots and Systems, Hamburg, 2015:922-928.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LonchaNet:a sliced-based CNN architecture for real-time 3D object recognition">

                                <b>[22]</b> Gomez-Donoso F, Garcia-Garcia A, Garcia-Rodriguez J, <i>et al</i>.LonchaNet:a sliced-based CNN architecture for real-time 3D object recognition[C].2017 International Joint Conference on Neural Networks, 2017:412-418.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning for 3D shape classification based on volumetric density and surface approximation clues">

                                <b>[23]</b> Minto L, Zanuttigh P, Pagnutti G.Deep learning for 3D shape classification based on volumetric density and surface approximation clues[C].International Conference on Computer Vision Theory and Applications, 2018:317-324.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=C3D:generic features for video analysis">

                                <b>[24]</b> Tran D, Bourdev L, Fergus R, <i>et al</i>.C3D:generic features for video analysis[J].Eprint Arxiv, 2014, 2 (7) :8-17.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D model classification using convolutional neural network">

                                <b>[25]</b> Gwak J Y.3D model classification using convolutional neural network[R].[S.l.]:Stanford University, 2016.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rectified linear units improve restricted boltzmann machines">

                                <b>[26]</b> Nair V, Hinton G E.Rectified linear units improve restricted Boltzmann machines[C].International Conference on International Conference on Machine Learning, 2010:807-814.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201904037" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201904037&amp;v=MjE3MjJYVGJMRzRIOWpNcTQ5R1k0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtWN3ZJSWo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

