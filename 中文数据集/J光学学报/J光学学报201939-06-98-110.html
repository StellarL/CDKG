

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133882530596250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201906012%26RESULT%3d1%26SIGN%3dXfXaGhXGKoHqNTBVmHs7rbSN06I%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201906012&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201906012&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201906012&amp;v=MTkzODFqWFRiTEc0SDlqTXFZOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeWptVnIvSUk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#79" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#83" data-title="2 方  法 ">2 方  法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#84" data-title="&lt;b&gt;2.1 解析字典学习&lt;/b&gt;"><b>2.1 解析字典学习</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;2.2 seen类场景的图像特征融合&lt;/b&gt;"><b>2.2 seen类场景的图像特征融合</b></a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;2.3 unseen类场景的图像特征融合及分类&lt;/b&gt;"><b>2.3 unseen类场景的图像特征融合及分类</b></a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;2.4 算法步骤&lt;/b&gt;"><b>2.4 算法步骤</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#135" data-title="&lt;b&gt;3.1 数据集及实验设置&lt;/b&gt;"><b>3.1 数据集及实验设置</b></a></li>
                                                <li><a href="#143" data-title="&lt;b&gt;3.2 图像特征融合效果分析&lt;/b&gt;"><b>3.2 图像特征融合效果分析</b></a></li>
                                                <li><a href="#156" data-title="&lt;b&gt;3.3 算法收敛性分析&lt;/b&gt;"><b>3.3 算法收敛性分析</b></a></li>
                                                <li><a href="#158" data-title="&lt;b&gt;3.4 计算效率分析&lt;/b&gt;"><b>3.4 计算效率分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#167" data-title="4 结  论 ">4 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#131" data-title="图1 所提算法的整体框架图">图1 所提算法的整体框架图</a></li>
                                                <li><a href="#138" data-title="图2 所提算法运算流程图">图2 所提算法运算流程图</a></li>
                                                <li><a href="#140" data-title="图3 UCM数据集中的若干类样本">图3 UCM数据集中的若干类样本</a></li>
                                                <li><a href="#141" data-title="图4 AID数据集中的若干类样本">图4 AID数据集中的若干类样本</a></li>
                                                <li><a href="#150" data-title="表1 UCM数据集上ZSC算法的相同层次图像特征融合效果OA值">表1 UCM数据集上ZSC算法的相同层次图像特征融合效果OA值</a></li>
                                                <li><a href="#154" data-title="表2 AID数据集上ZSC算法的相同层次图像特征融合效果OA值">表2 AID数据集上ZSC算法的相同层次图像特征融合效果OA值</a></li>
                                                <li><a href="#160" data-title="表3 不同ZSC算法在UCM数据集上的不同层次图像特征融合OA值">表3 不同ZSC算法在UCM数据集上的不同层次图像特征融合OA值</a></li>
                                                <li><a href="#161" data-title="表4 不同ZSC算法在AID数据集上不同层次图像特征融合OA值">表4 不同ZSC算法在AID数据集上不同层次图像特征融合OA值</a></li>
                                                <li><a href="#162" data-title="图5 所提算法在UCM上融合时的各unseen类的分类准确度">图5 所提算法在UCM上融合时的各unseen类的分类准确度</a></li>
                                                <li><a href="#163" data-title="图6 所提算法在AID上融合时的各unseen类的分类准确度">图6 所提算法在AID上融合时的各unseen类的分类准确度</a></li>
                                                <li><a href="#164" data-title="图7 所提算法在UCM场景集上融合时的总体损失曲线和测试准确度折线">图7 所提算法在UCM场景集上融合时的总体损失曲线和测试准确度折线</a></li>
                                                <li><a href="#165" data-title="图8 所提算法在AID场景集上融合时的总体损失曲线和测试准确度折线">图8 所提算法在AID场景集上融合时的总体损失曲线和测试准确度折线</a></li>
                                                <li><a href="#166" data-title="表5 各ZSC算法在AID上的GoogLeNet特征运算耗时">表5 各ZSC算法在AID上的GoogLeNet特征运算耗时</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="13">


                                    <a id="bibliography_1" title=" Liu D W, Han L, Han X Y.High spatial resolution remote sensing image classification based on deep learning[J].Acta Optica Sinica, 2016, 36 (4) :0428001.刘大伟, 韩玲, 韩晓勇.基于深度学习的高分辨率遥感影像分类研究[J].光学学报, 2016, 36 (4) :0428001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604039&amp;v=MTU1OTlJSWpYVGJMRzRIOWZNcTQ5R2JZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5am1Wci8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Liu D W, Han L, Han X Y.High spatial resolution remote sensing image classification based on deep learning[J].Acta Optica Sinica, 2016, 36 (4) :0428001.刘大伟, 韩玲, 韩晓勇.基于深度学习的高分辨率遥感影像分类研究[J].光学学报, 2016, 36 (4) :0428001.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_2" title=" Chen S Z, Tian Y L.Pyramid of spatial relations for scene-level land use classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (4) :1947-1957." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pyramid of Spatial Relatons for Scene-Level Land Use Classification">
                                        <b>[2]</b>
                                         Chen S Z, Tian Y L.Pyramid of spatial relations for scene-level land use classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (4) :1947-1957.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_3" title=" Zhang L P, Zhang L F, Du B.Deep learning for remote sensing data:a technical tutorial on the state of the art[J].IEEE Geoscience and Remote Sensing Magazine, 2016, 4 (2) :22-40." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning for remote sensing data:A technical tutorial on the state of the art">
                                        <b>[3]</b>
                                         Zhang L P, Zhang L F, Du B.Deep learning for remote sensing data:a technical tutorial on the state of the art[J].IEEE Geoscience and Remote Sensing Magazine, 2016, 4 (2) :22-40.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_4" title=" Li A X, Lu Z W, Wang L W, &lt;i&gt;et al&lt;/i&gt;.Zero-shot scene classification for high spatial resolution remote sensing images[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :4157-4167." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot scene classification for high spatial resolution remote sensing images">
                                        <b>[4]</b>
                                         Li A X, Lu Z W, Wang L W, &lt;i&gt;et al&lt;/i&gt;.Zero-shot scene classification for high spatial resolution remote sensing images[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :4157-4167.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_5" title=" Xian Y Q, Akata Z, Sharma G, &lt;i&gt;et al&lt;/i&gt;.Latent embeddings for zero-shot classification[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:69-77." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Latent embeddings for zero-shot classification">
                                        <b>[5]</b>
                                         Xian Y Q, Akata Z, Sharma G, &lt;i&gt;et al&lt;/i&gt;.Latent embeddings for zero-shot classification[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:69-77.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_6" title=" Wang D, Li Y, Lin Y, &lt;i&gt;et al&lt;/i&gt;.Relational knowledge transfer for zero-shot learning[C]∥Thirtieth AAAI Conference in Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA.California:AAAI Press, 2016:2145-2151." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Relational knowledge transfer for zero-shot learning">
                                        <b>[6]</b>
                                         Wang D, Li Y, Lin Y, &lt;i&gt;et al&lt;/i&gt;.Relational knowledge transfer for zero-shot learning[C]∥Thirtieth AAAI Conference in Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA.California:AAAI Press, 2016:2145-2151.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_7" title=" Zhang Z M, Saligrama V.Zero-shot learning via joint latent similarity embedding[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:6034-6042." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning via joint latent similarity embedding">
                                        <b>[7]</b>
                                         Zhang Z M, Saligrama V.Zero-shot learning via joint latent similarity embedding[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:6034-6042.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_8" title=" Zhang Z M, Saligrama V.Zero-shot learning via semantic similarity embedding[C]∥2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:4166-4174." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning via semantic similarity embedding">
                                        <b>[8]</b>
                                         Zhang Z M, Saligrama V.Zero-shot learning via semantic similarity embedding[C]∥2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:4166-4174.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_9" title=" Wang Q, Chen K.Zero-shot visual recognition via bidirectional latent embedding[J].International Journal of Computer Vision, 2017, 124 (3) :356-383." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDD31F20253A10A4B4D5AE16DA35AA17A9&amp;v=MTk2MzV3Q01PVlFzdVdDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh4THU2d0tnPU5qN0Jhc2U3SDZmT3I0MUFaNW9PREEwOXZSSm43MDRJU1htVzNSRQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Wang Q, Chen K.Zero-shot visual recognition via bidirectional latent embedding[J].International Journal of Computer Vision, 2017, 124 (3) :356-383.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_10" title=" Li Y N, Wang D H, Hu H H, &lt;i&gt;et al&lt;/i&gt;.Zero-shot recognition using dual visual-semantic mapping paths[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:5207-5215." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot recognition using dual visual-semantic mapping paths">
                                        <b>[10]</b>
                                         Li Y N, Wang D H, Hu H H, &lt;i&gt;et al&lt;/i&gt;.Zero-shot recognition using dual visual-semantic mapping paths[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:5207-5215.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_11" title=" Kodirov E, Xiang T, Gong S G.Semantic autoencoder for zero-shot learning[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4447-4456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic Autoencoder for Zero-Shot Learning">
                                        <b>[11]</b>
                                         Kodirov E, Xiang T, Gong S G.Semantic autoencoder for zero-shot learning[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4447-4456.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_12" title=" Fernando B, Fromont E, Muselet D, &lt;i&gt;et al&lt;/i&gt;.Discriminative feature fusion for image classification[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:3434-3441." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative feature fusion for image classific-ation">
                                        <b>[12]</b>
                                         Fernando B, Fromont E, Muselet D, &lt;i&gt;et al&lt;/i&gt;.Discriminative feature fusion for image classification[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:3434-3441.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_13" title=" Bao C L, Ji H, Quan Y H, &lt;i&gt;et al&lt;/i&gt;.Dictionary learning for sparse coding:algorithms and convergence analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (7) :1356-1369." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dictionary learning for sparse coding:algorithms and convergence analysis">
                                        <b>[13]</b>
                                         Bao C L, Ji H, Quan Y H, &lt;i&gt;et al&lt;/i&gt;.Dictionary learning for sparse coding:algorithms and convergence analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (7) :1356-1369.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_14" title=" Yang M, Chang H Y, Luo W X.Discriminative analysis-synthesis dictionary learning for image classification[J].Neurocomputing, 2017, 219:404-411." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative analysis-synthesis dictionary learning for image classification">
                                        <b>[14]</b>
                                         Yang M, Chang H Y, Luo W X.Discriminative analysis-synthesis dictionary learning for image classification[J].Neurocomputing, 2017, 219:404-411.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_15" title=" Wang J J, Guo Y Q, Guo J, &lt;i&gt;et al&lt;/i&gt;.Synthesis linear classifier based analysis dictionary learning for pattern classification[J].Neurocomputing, 2017, 238:103-113." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES579AE40DE8831392A28EB8D29F9C9B3D&amp;v=MTkxMzZOaWZPZmJhL0Y2QzVxNDh4RWVNSEQzMDZ4aFJpNkRjSU9uZVdyaHREY01HZE43bnJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh4THU2d0tnPQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Wang J J, Guo Y Q, Guo J, &lt;i&gt;et al&lt;/i&gt;.Synthesis linear classifier based analysis dictionary learning for pattern classification[J].Neurocomputing, 2017, 238:103-113.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_16" title=" Ravishankar S, Bresler Y.Sparsifying transform learning with efficient optimal updates and convergence guarantees[J].IEEE Transactions on Signal Processing, 2015, 63 (9) :2389-2404." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=L0 sparsifying transform learning with efficient optimal updates and convergence guarantees">
                                        <b>[16]</b>
                                         Ravishankar S, Bresler Y.Sparsifying transform learning with efficient optimal updates and convergence guarantees[J].IEEE Transactions on Signal Processing, 2015, 63 (9) :2389-2404.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_17" title=" Xia G S, Hu J W, Hu F, &lt;i&gt;et al&lt;/i&gt;.AID:a benchmark data set for performance evaluation of aerial scene classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :3965-3981." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=AID:a benchmark data set for performance evaluation of aerial scene classification">
                                        <b>[17]</b>
                                         Xia G S, Hu J W, Hu F, &lt;i&gt;et al&lt;/i&gt;.AID:a benchmark data set for performance evaluation of aerial scene classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :3965-3981.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_18" title=" Swain M J, Ballard D H.Color indexing[J].International Journal of Computer Vision, 1991, 7 (1) :11-32." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830225&amp;v=MDAzODFGND1OajdCYXJPNEh0SE9wNHhGWnVrS1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RlNqbFU3ek5J&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Swain M J, Ballard D H.Color indexing[J].International Journal of Computer Vision, 1991, 7 (1) :11-32.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_19" title=" Ojala T, Pietikainen M, Maenpaa T.Multiresolution gray-scale and rotation invariant texture classification with local binary patterns[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002, 24 (7) :971-987." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiresolution gray-scale and rotation invariant texture classification with local binary patterns">
                                        <b>[19]</b>
                                         Ojala T, Pietikainen M, Maenpaa T.Multiresolution gray-scale and rotation invariant texture classification with local binary patterns[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002, 24 (7) :971-987.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_20" title=" Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjQ1MjRwNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RlNqbFU3ek5JRjQ9Tmo3QmFyTzRIdEhP&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_21" title=" Oliva A, Torralba A.Modeling the shape of the scene:a holistic representation of the spatial envelope[J].International Journal of Computer Vision, 2001, 42 (3) :145-175." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830679&amp;v=MTU4MDc9Tmo3QmFyTzRIdEhPcDR4Rll1d0dZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTamxVN3pOSUY0&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         Oliva A, Torralba A.Modeling the shape of the scene:a holistic representation of the spatial envelope[J].International Journal of Computer Vision, 2001, 42 (3) :145-175.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_22" title=" Yang Y, Newsam S.Bag-of-visual-words and spatial extensions for land-use classification[C]//Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems, November 2-5, 2010, San Jose, California.New York:ACM, 2010:270-279." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bag-of-visual-words and spatial extensions for land-use classification">
                                        <b>[22]</b>
                                         Yang Y, Newsam S.Bag-of-visual-words and spatial extensions for land-use classification[C]//Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems, November 2-5, 2010, San Jose, California.New York:ACM, 2010:270-279.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_23" title=" Perronnin F, S&#225;nchez J, Mensink T.Improving the fisher kernel for large-scale image classification[M]//Daniilidis K, Maragos P, Paragios N.Computer Vision-ECCV 2010.Berlin, Heidelberg:Springer, 2010:143-156." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving the fisher kernel for large-scale image classification">
                                        <b>[23]</b>
                                         Perronnin F, S&#225;nchez J, Mensink T.Improving the fisher kernel for large-scale image classification[M]//Daniilidis K, Maragos P, Paragios N.Computer Vision-ECCV 2010.Berlin, Heidelberg:Springer, 2010:143-156.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_24" >
                                        <b>[24]</b>
                                     Blei D M, Ng A Y, Jordan M I.Latent dirichl location[J].Journal of Machine Learning research, 2003, 3:993-1022.</a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_25" >
                                        <b>[25]</b>
                                     Jia Y Q, Shelhamer E, Donahue J, &lt;i&gt;et al&lt;/i&gt;.Caffe:convolutional architecture for fast feature embedding[C]//Proceedings of the 22nd ACM international conference on Multimedia, November 3-7, 2014, Orlando, Florida, USA.New York:ACM, 2014:675-678.</a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_26" title=" Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2018-12-25].https://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">
                                        <b>[26]</b>
                                         Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2018-12-25].https://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                                <li id="65">


                                    <a id="bibliography_27" title=" Szegedy C, Liu W, Jia Y Q, &lt;i&gt;et al&lt;/i&gt;.Going deeper with convolutions[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[27]</b>
                                         Szegedy C, Liu W, Jia Y Q, &lt;i&gt;et al&lt;/i&gt;.Going deeper with convolutions[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:1-9.
                                    </a>
                                </li>
                                <li id="67">


                                    <a id="bibliography_28" >
                                        <b>[28]</b>
                                     He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.Deep residual learning for image recognition[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778.</a>
                                </li>
                                <li id="69">


                                    <a id="bibliography_29" title=" Wang J J, Yang J C, Yu K, &lt;i&gt;et al&lt;/i&gt;.Locality-constrained linear coding for image classification[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) , June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:3360-3367." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locality-constrained linear coding for image classification">
                                        <b>[29]</b>
                                         Wang J J, Yang J C, Yu K, &lt;i&gt;et al&lt;/i&gt;.Locality-constrained linear coding for image classification[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) , June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:3360-3367.
                                    </a>
                                </li>
                                <li id="71">


                                    <a id="bibliography_30" title=" Bosch A, Zisserman A, Muňoz X.Scene classification via pLSA[M]//Leonardis A, Bischof H, Pinz A.Computer Vision-ECCV 2006.Berlin, Heidelberg:Springer, 2006:517-530." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scene classification via pLSA">
                                        <b>[30]</b>
                                         Bosch A, Zisserman A, Muňoz X.Scene classification via pLSA[M]//Leonardis A, Bischof H, Pinz A.Computer Vision-ECCV 2006.Berlin, Heidelberg:Springer, 2006:517-530.
                                    </a>
                                </li>
                                <li id="73">


                                    <a id="bibliography_31" title=" Lazebnik S, Schmid C, Ponce J.Beyond bags of features:spatial pyramid matching for recognizing natural scene categories[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) , June 17-22, 2006, New York, USA.New York:IEEE, 2006:2169-2178." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond bags of features: spatial pyramid matching for recognizing natural scene categories">
                                        <b>[31]</b>
                                         Lazebnik S, Schmid C, Ponce J.Beyond bags of features:spatial pyramid matching for recognizing natural scene categories[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) , June 17-22, 2006, New York, USA.New York:IEEE, 2006:2169-2178.
                                    </a>
                                </li>
                                <li id="75">


                                    <a id="bibliography_32" title=" Jegou H, Perronnin F, Douze M, &lt;i&gt;et al&lt;/i&gt;.Aggregating local image descriptors into compact codes[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (9) :1704-1716." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Aggregating Local Image Descriptors into Compact Codes">
                                        <b>[32]</b>
                                         Jegou H, Perronnin F, Douze M, &lt;i&gt;et al&lt;/i&gt;.Aggregating local image descriptors into compact codes[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (9) :1704-1716.
                                    </a>
                                </li>
                                <li id="77">


                                    <a id="bibliography_33" title=" Rupnik J, Shawe-Taylor J.Multi-view canonical correlation analysis[C]∥Proceedings of the 13th Multiconference on Information Society, IS, Ljubljana, Slovenia.[S.l.:s.n.], 2010:201-204." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-view canonical correlation analysis">
                                        <b>[33]</b>
                                         Rupnik J, Shawe-Taylor J.Multi-view canonical correlation analysis[C]∥Proceedings of the 13th Multiconference on Information Society, IS, Ljubljana, Slovenia.[S.l.:s.n.], 2010:201-204.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-03-19 09:08</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(06),98-110 DOI:10.3788/AOS201939.0610002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于图像特征融合的遥感场景零样本分类算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E6%99%A8&amp;code=41275593&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴晨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%AE%8F%E4%BC%9F&amp;code=20899251&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王宏伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A2%81%E6%98%B1%E7%BA%AC&amp;code=42150725&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">袁昱纬</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%BF%97%E5%BC%BA&amp;code=20250775&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王志强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%AE%87&amp;code=20481306&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A8%8B%E7%BA%A2&amp;code=20025334&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">程红</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%85%A8%E5%90%89%E6%88%90&amp;code=20534592&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全吉成</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%B7%E5%86%9B%E8%88%AA%E7%A9%BA%E5%A4%A7%E5%AD%A6&amp;code=1701745&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">海军航空大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A9%BA%E5%86%9B%E8%88%AA%E7%A9%BA%E5%A4%A7%E5%AD%A6&amp;code=1020414&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空军航空大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=91977%E9%83%A8%E9%98%9F&amp;code=1743710&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">91977部队</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>利用不同图像特征之间的互补性, 可提升遥感场景零样本分类性能。将图像特征的融合与零样本分类结合, 提出一种基于图像特征融合的遥感场景零样本分类算法。采用解析字典学习方法, 计算各图像特征的稀疏系数, 并串接起来作为融合后图像特征, 以减少冗余信息且保留各图像特征自身特点;引入监督信息, 提高融合特征的鉴别性;将融合特征与场景类别词向量进行结构对齐, 提升对新类别场景的迁移识别效果。在UC-Merced和航拍图像数据集两种遥感场景集上, 对相同层次及不同层次的场景图像特征分别进行融合实验。实验结果表明:对于总体分类准确度和运算耗时, 所提算法均优于其他零样本分类算法及通用的特征融合算法, 证明了方法的有效性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%A5%E6%84%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遥感;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像特征融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%A3%E6%9E%90%E5%AD%97%E5%85%B8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">解析字典学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%A5%E6%84%9F%E5%9C%BA%E6%99%AF%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遥感场景分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">零样本分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%89%B4%E5%88%AB%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">鉴别性;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%93%E6%9E%84%E5%AF%B9%E9%BD%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">结构对齐;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    全吉成 E-mail:jicheng_quan@126.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-02</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家青年自然科学基金 (61301233);</span>
                    </p>
            </div>
                    <h1><b>Image Feature Fusion Based Remote Sensing Scene Zero-Shot Classification Algorithm</b></h1>
                    <h2>
                    <span>Wu Chen</span>
                    <span>Wang Hongwei</span>
                    <span>Yuan Yuwei</span>
                    <span>Wang Zhiqiang</span>
                    <span>Liu Yu</span>
                    <span>Cheng Hong</span>
                    <span>Quan Jicheng</span>
            </h2>
                    <h2>
                    <span>Naval Aviation University</span>
                    <span>Aviation University of Air Force</span>
                    <span>The 91977 of PLA</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to improve the performance of remote sensing scene zero-shot classification by utilizing the complementarity among different image features, a zero-shot scene classification algorithm based on the fusion of image features is proposed, which combines the fusion of image features with zero-shot classification. The analysis dictionary learning is exploited to obtain the sparse coefficients of image features. The obtained sparse coefficients are concatenated as the fused image feature to reduce the redundant information and retain the characteristics of different image features. Supervised information is introduced to improve the discriminability of the model. The fused image feature is structurally aligned with the semantic word vectors to improve the transfer capability for the unseen class scenes. The fusions of image features at the same level and the different levels are carried out on UC-Merced (UCM) and Aerial Image Dataset (AID) remote sensing scenes datasets, respectively. The experimental results show that the overall classification accuracy and time-consuming of our method are superior to those of other zero-shot classification algorithms and general feature fusion algorithms, which proves the effectiveness of our method.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=remote%20sensing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">remote sensing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20feature%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image feature fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=analysis%20dictionary%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">analysis dictionary learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=remote%20sensing%20scenes%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">remote sensing scenes classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=zero-shot%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">zero-shot classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=discriminability&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">discriminability;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=structural%20alignment&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">structural alignment;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-02</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="79" name="79" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="80">随着遥感技术的进步和广泛应用, 人们获取了海量的高空间分辨率遥感图像, 研究出了基于场景的遥感图像分类方法<citation id="171" type="reference"><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。作为具有某种概念的遥感图像块, 场景已成为海量遥感图像分析的基本单元<citation id="169" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。目前, 遥感图像场景分类主要采用监督分类方法, 这需要标注大量样本, 限制了遥感场景分类算法的实际应用。针对该问题, Li等<citation id="170" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>将遥感场景分类与零样本分类算法相结合, 提出一种遥感场景零样本分类算法。</p>
                </div>
                <div class="p1">
                    <p id="81">作为一种特殊的无监督分类方法, 零样本分类 (ZSC) 算法使用类别名称的语义词向量提供类别之间的距离关系信息, 通过迁移已知 (seen) 类标注样本中的知识, 来推断识别新 (unseen) 类别的样本。由于不需要标注unseen类样本, 近年来ZSC受到广泛关注<citation id="179" type="reference"><link href="21" rel="bibliography" /><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>。Xian等<citation id="172" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出的隐式嵌入 (LatEm) 算法在兼容函数学习过程中引入隐式变量模型, 进行细粒度ZSC。针对映射函数的泛化能力不足问题, Wang等<citation id="173" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出关系知识迁移 (RKT) 算法, 通过语义映射还原unseen类的流形结构。Zhang等<citation id="174" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出联合隐式相似性嵌入 (JLSE) 方法及语义相似性嵌入 (SSE) 方法<citation id="175" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 将源域或目标域数据视为训练类组合。Wang等<citation id="176" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出双向隐式嵌入 (BiDiLEL) 方法, 将图像特征和语义词向量分别映射到公共空间。Li等<citation id="177" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>基于语义空间流形和视觉语义映射迁移能力之间的关系, 提出双视觉语义映射 (DMaP) 方法。Kodirov等<citation id="178" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出语义自编码方法 (SAE) , 并采用基于编码-解码的架构, 其中, 编码项将图像特征嵌入到语义词向量空间, 解码项则用来重建图像特征, 以增强对unseen类的泛化识别能力。</p>
                </div>
                <div class="p1">
                    <p id="82">虽然已有多种场景图像特征, 但现有的ZSC算法均采用单一图像特征, 而单一图像特征一般仅针对图像的某一方面, 其表达能力有限, 制约遥感场景ZSC性能的提升。不同图像特征的表达侧重点不同, 因此存在一定的互补性。现有研究已证明, 通过融合不同的图像特征, 可获得表达能力更强的图像特征<citation id="180" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>;串接、典型关联分析等常用的特征融合方法, 虽然能利用不同特征的互补性, 但由于不是针对ZSC分类任务, 其对遥感场景ZSC算法的适用性不足, 很难保证遥感场景ZSC性能的提升。针对上述问题, 利用不同类型图像特征之间的互补性, 将各图像特征的稀疏系数串接作为融合图像特征, 并与seen类遥感场景样本标签信息结合, 提升unseen类遥感场景零样本分类性能。</p>
                </div>
                <h3 id="83" name="83" class="anchor-tag">2 方  法</h3>
                <h4 class="anchor-tag" id="84" name="84"><b>2.1 解析字典学习</b></h4>
                <div class="p1">
                    <p id="85">稀疏表示可减少冗余信息, 提高分类效果<citation id="181" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。字典学习 (DL) 是获取输入样本稀疏系数的典型方法。字典学习方法分为两类:合成字典学习 (SDL) 和解析字典学习 (ADL) 。虽然SDL方法应用广泛, 但计算复杂度较高。ADL通常具有闭式解和良好的编码能力, 计算效率较高, 近年来受到广泛关注<citation id="182" type="reference"><link href="39" rel="bibliography" /><link href="41" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>。ADL的基本公式为</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>arg</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ζ</mi><mo>, </mo><mi mathvariant="bold-italic">Ω</mi></mrow></munder><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><mi mathvariant="bold-italic">X</mi></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">Ω</mi><mo>∈</mo><mi>Γ</mi><mo>, </mo><mspace width="0.25em" /><mrow><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>l</mi></msub></mrow><mo>|</mo></mrow></mrow><msub><mrow></mrow><mn>0</mn></msub><mo>≤</mo><mi>Τ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">式中:<b><i>X</i></b>=[<i>x</i><sub>1</sub><i>x</i><sub>2</sub> … <i>x</i><sub><i>n</i></sub>]∈<b>R</b><sup><i>m</i>×<i>n</i></sup>为<i>n</i>个输入样本的特征矩阵 (<i>m</i>维) ;<i>x</i><sub><i>l</i></sub>∈<b>R</b><sup><i>m</i></sup>为第<i>l</i>个样本特征;<i>z</i><sub><i>l</i></sub>∈<b>R</b><sup><i>m</i></sup>为<i>x</i><sub><i>l</i></sub>的稀疏系数, 其稀疏性采用L0范数及参数<i>T</i><sub>0</sub>实现;<b><i>Z</i></b>=[<i>z</i><sub>1</sub><i>z</i><sub>2</sub> … <i>z</i><sub><i>n</i></sub>]∈<b>R</b><sup><i>m</i>×<i>n</i></sup>为<b><i>X</i></b>的稀疏系数矩阵;<i>Ω</i>为解析字典;<i>Γ</i>是为避免出现平凡解, 对<i>Ω</i>限制的log-det条件<citation id="183" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>;<mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>|</mo><mo>⋅</mo><mo>|</mo></mrow></mrow><msub><mrow></mrow><mtext>F</mtext></msub></mrow></math></mathml>为矩阵的<i>F</i>范数。 <i>ADL</i>的任务是给定输入特征矩阵<b><i>X</i></b>, 寻找最佳稀疏系数矩阵<b><i>Z</i></b>及解析字典<i>Ω</i>。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89"><b>2.2 seen类场景的图像特征融合</b></h4>
                <div class="p1">
                    <p id="90">现有场景分类方法所用的图像特征, 按语义程度分为3类:低层图像特征、中层图像特征和高层图像特征<citation id="184" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。低层图像特征主要针对图像的颜色、纹理等局部或全部信息, 其所含语义信息较少, 如:颜色直方图 (CH) <citation id="185" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、局部二进制模式 (LBP) <citation id="186" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、尺度不变特征变换 (SIFT) <citation id="187" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>及全局信息场景变换 (GIST) <citation id="188" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>等特征;中层图像特征一般通过对低层图像特征的编码获得, 其抽象能力较强, 具有一定语义信息, 如词袋模型 (BoVW) <citation id="189" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、改进的费舍尔核 (IFK) <citation id="190" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、隐狄利克雷分配 (LDA) <citation id="191" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>等。近年来, 通过深度学习模型提取的图像特征, 具有更丰富的语义信息, 称为高层图像特征。深度卷积模型 (DCNN) 是目前应用最广泛的高层图像特征提取器, 如CaffeNet<citation id="192" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、VGGNet<citation id="193" type="reference"><link href="63" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、GoogLeNet<citation id="194" type="reference"><link href="65" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>、ResNet<citation id="195" type="reference"><link href="67" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>等模型。基于模拟人眼视觉信息处理的多层结构, DCNN模型可提取具有丰富语义信息的图像特征。</p>
                </div>
                <div class="p1">
                    <p id="91">图像特征中存在的大量冗余信息降低了不同类别场景图像在特征空间中的区分度, 因此需要减少冗余信息, 提升场景图像之间的区分度, 以增强分类性能。由于ADL方法具有较高的稀疏编码效率, 可以有效减少冗余信息, 提升样本之间的区分度, 因此采用ADL方法提取各图像特征的稀疏系数。同时, 为利用不同图像特征之间的互补性, 需要保留其各自的特点。串接是一种可以保留特征向量自身特点的简单方法, 因此本文将各图像特征的稀疏系数串接, 作为场景图像的融合特征。</p>
                </div>
                <div class="p1">
                    <p id="92">此外, 没有引入场景图像的类别信息会导致融合后的图像特征的鉴别性不足, 即类内样本没有互相聚集、类间样本没有互相远离, 这样就降低了模型的分类效果。为增强融合后图像特征的鉴别性, 引入场景图像的类别信息, 将融合特征映射到场景的类别标签向量空间, 强制同类场景互相聚集、异类样本互相远离, 从而提升融合特征的鉴别性。因此, seen类场景图像特征融合的目标损失函数由稀疏编码项和鉴别项组成, 即</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mo>=</mo><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mi mathvariant="bold-italic">W</mi></mrow></munder><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>v</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">F</mi></mrow><mo>|</mo></mrow></mrow></mstyle><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mspace width="0.25em" /><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>∈</mo><mtext>Γ</mtext><mo>, </mo><mspace width="0.25em" /><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mo>⋅</mo><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow><msub><mrow></mrow><mn>0</mn></msub><mo>≤</mo><mi>Τ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">式中:<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mrow></math></mathml>为稀疏编码项;<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">F</mi></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mrow></math></mathml>为鉴别项。其中, 采用<i>v</i>种不同图像特征, 对应<i>v</i>个不同图像特征矩阵<b><i>X</i></b><sup> (<i>i</i>) </sup><sub>s</sub>∈<b>R</b><sup><i>d</i><sub><i>i</i></sub>×<i>N</i><sub>s</sub></sup> (<i>i</i>=1, 2, …, <i>v</i>) , 其中下角标s为seen类别, 上角标<i>i</i>为第<i>i</i>种类型的图像特征, <i>d</i><sub><i>i</i></sub>为第<i>i</i>种图像特征的维数, <i>N</i><sub>s</sub>为seen类场景图像个数;上角标c为串接后的稀疏系数;<b><i>Z</i></b><sup> (<i>i</i>) </sup><sub>s</sub>∈<b>R</b><sup><i>d</i><sub><i>i</i></sub>×<i>N</i><sub><i>s</i></sub></sup>、<i>Ω</i><sup> (<i>i</i>) </sup>∈<b>R</b><sup><i>d</i><sub><i>i</i></sub>×<i>d</i><sub><i>i</i></sub></sup>分别为图像特征矩阵<b><i>X</i></b><sup> (<i>i</i>) </sup><sub>s</sub>的稀疏系数矩阵和解析字典;<b><i>Z</i></b><sup>c</sup><sub>s</sub>∈<b>R</b><sup><i>d</i>×<i>N</i><sub>s</sub></sup>为串接<b><i>Z</i></b><sup> (<i>i</i>) </sup><sub>s</sub>后的融合图像特征, 其中<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>v</mi></munderover><mi>d</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>为融合图像特征<b><i>Z</i></b><mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup></mrow></math></mathml>的维数;<b><i>F</i></b>∈<b>R</b><sup><i>n</i><sub>s</sub>×<i>N</i><sub>s</sub></sup>为场景图像的类别标签one-hot向量组成的矩阵;<i>n</i><sub>s</sub>为seen场景类别个数;<b><i>W</i></b>∈<b>R</b><sup><i>n</i><sub>s</sub>×<i>d</i></sup>为鉴别项的映射矩阵。one-hot向量中只有类别位置的元素为1, 其余位置均为0, 因此属于不同类别的样本对应的one-hot向量彼此正交, 从而使得<b><i>F</i></b>具有最高的鉴别性。本文以<b><i>F</i></b>作为监督信息, 将融合特征映射到该空间中并进行对齐, 提升融合特征的鉴别性。</p>
                </div>
                <div class="p1">
                    <p id="99">在ZSC方法中, 每个场景类别均对应一个语义词向量, 通常认为词向量空间中的场景类间距离与图像特征空间的场景类间距离一致, 将监督分类模型迁移至unseen类场景, 可实现对unseen场景图像的零样本分类。然而, 词向量与遥感图像特征的来源不同会导致两种空间中的类间距离并不一致, 同时, 直接迁移会导致分类效果下降, 因此需要进行距离结构对齐, 以降低两种空间中的类间距离不一致性。由于融合特征是通过稀疏系数串接得到的, 对齐矩阵需要具有对词向量的稀疏编码能力。ADL具有良好的稀疏编码能力, 可将词向量嵌入到融合特征空间, 从而实现两种空间的对齐, 因此本文采用ADL对词向量和融合特征进行结构对齐处理。seen类场景图像特征融合的总体目标损失函数为</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">D</mi></mrow></munder><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>v</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mo>|</mo></mrow></mrow></mstyle><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">F</mi></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mtext>s</mtext><mo>⊗</mo></msubsup></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>∈</mo><mtext>Γ</mtext><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">D</mi><mo>∈</mo><mtext>Γ</mtext><mo>, </mo><mspace width="0.25em" /><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mo>⋅</mo><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow><msub><mrow></mrow><mn>0</mn></msub><mo>≤</mo><mi>Τ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">式中:<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mtext>s</mtext><mo>⊗</mo></msubsup></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mrow></math></mathml>为结构对齐项;<b><i>C</i></b><sup>⨂</sup><sub>s</sub>∈<b>R</b><sup><i>d</i><sub>c</sub>×<i>N</i><sub>s</sub></sup>为由全体seen类场景图像对应的类别词向量组成的矩阵;<i>d</i><sub>c</sub>为词向量的维数;<b><i>D</i></b>∈<b>R</b><sup><i>d</i>×<i>d</i><sub>c</sub></sup>为<b><i>C</i></b><sup>⨂</sup><sub>s</sub>的解析字典。由于 (3) 式对于全体变量为非凸函数, 因此难以直接求解, 可采用循环方式逐个求解。<b><i>W</i></b>和<b><i>D</i></b>的求解依赖于<b><i>Z</i></b><sup>c</sup><sub>s</sub>, <b><i>Z</i></b><sup>c</sup><sub>s</sub>可由<b><i>Z</i></b><sup> (<i>i</i>) </sup><sub>s</sub>串接得到, 需要先求解<i>Ω</i><sup> (<i>i</i>) </sup>, 更新<b><i>Z</i></b><sup> (<i>i</i>) </sup><sub>s</sub>和<b><i>Z</i></b><sup>c</sup><sub>s</sub>, 然后更新<b><i>W</i></b>和<b><i>D</i></b>。初始化<b><i>Z</i></b><sup> (<i>i</i>) </sup><sub>s</sub>为one-hot向量, 本文方法的具体求解过程如下:</p>
                </div>
                <div class="p1">
                    <p id="103">1) 固定<b><i>Z</i></b><sup> (<i>i</i>) </sup><sub>s</sub>、<b><i>W</i></b>和<b><i>D</i></b>, 更新解析字典<i>Ω</i><sup> (<i>i</i>) </sup>。为避免出现平凡解, 引入对解析字典<i>Ω</i><sup> (<i>i</i>) </sup>的log-det条件<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>-</mo><mrow><mi>lg</mi></mrow><mrow><mo>|</mo><mrow><mi>det</mi><mspace width="0.25em" /><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup></mrow><mo>|</mo></mrow></mrow></math></mathml>, 其中<mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>lg</mi></mrow><mrow><mo>|</mo><mrow><mi>det</mi><mspace width="0.25em" /><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup></mrow><mo>|</mo></mrow></mrow></math></mathml>抑制出现无效解, 避免<i>Ω</i><sup> (<i>i</i>) </sup>中的行重复或全0。因此, 更新<i>Ω</i><sup> (<i>i</i>) </sup>的目标函数:</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup></mrow></munder><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>v</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>-</mo><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mo>|</mo></mrow></mrow></mstyle><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>α</mi><mi>R</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">式中:<i>α</i>&gt;0为正则项系数。 (4) 式的求解步骤<citation id="196" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>为:首先, <b><i>X</i></b><sup> (<i>i</i>) </sup><sub>s</sub> (<b><i>X</i></b><sup> (<i>i</i>) </sup><sub>s</sub>) <sup>T</sup>+<i>α</i><b><i>I</i></b>矩阵可分解为<b><i>LL</i></b><sup>T</sup>;其次, 对<b><i>L</i></b><sup>-1</sup><b><i>X</i></b><sup> (<i>i</i>) </sup><sub>s</sub><b><i>Z</i></b><sup> (<i>i</i>) </sup><sub>s</sub>进行SVD分解, 得到<b><i>QMR</i></b><sup>T</sup>;最后得到<i>Ω</i><sup> (<i>i</i>) </sup>=0.5<b><i>R</i></b>[<b><i>M</i></b>+ (<b><i>M</i></b><sup>2</sup>+2<i>α</i><b><i>I</i></b>) <sup>1/2</sup>]<b><i>Q</i></b><sup>T</sup><b><i>L</i></b><sup>-1</sup>。</p>
                </div>
                <div class="p1">
                    <p id="108">2) 固定<b><i>W</i></b>、<i>Ω</i><sup> (<i>i</i>) </sup>和<b><i>D</i></b>, 更新<b><i>Z</i></b><sup> (<i>i</i>) </sup><sub>s</sub>:</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mi mathvariant="bold-italic">Ω</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">并对<b><i>Z</i></b><sup> (<i>i</i>) </sup><sub>s</sub>进行稀疏化处理, 使<mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mo>⋅</mo><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow><msub><mrow></mrow><mn>0</mn></msub><mo>≤</mo><mi>Τ</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></math></mathml>。串接稀疏化后的<b><i>Z</i></b><sup> (<i>i</i>) </sup><sub>s</sub>, 得到<b><i>Z</i></b><sup>c</sup><sub>s</sub>。</p>
                </div>
                <div class="p1">
                    <p id="112">3) 固定<b><i>Z</i></b><sup> (<i>i</i>) </sup><sub>s</sub>、<i>Ω</i><sup> (<i>i</i>) </sup>、<b><i>D</i></b>, 更新<b><i>W</i></b>。为避免过拟合, 引入正则项<mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">W</mi><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mrow></math></mathml>, 此时的目标损失函数为</p>
                </div>
                <div class="p1">
                    <p id="114" class="code-formula">
                        <mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">W</mi></munder><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">F</mi></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">W</mi><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="115">得到更新<b><i>W</i></b>的解析式为</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><mo>=</mo><mi mathvariant="bold-italic">F</mi><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mrow><mo stretchy="false">[</mo><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>+</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">]</mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">进而更新<b><i>Z</i></b><sup>c</sup><sub>s</sub>:</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mo>=</mo><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">W</mi><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">F</mi><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">拆分<b><i>Z</i></b><sup>c</sup><sub>s</sub>为<b><i>Z</i></b><sup> (<i>i</i>) </sup><sub>s</sub>并进行稀疏化处理。</p>
                </div>
                <div class="p1">
                    <p id="120">4) 固定<b><i>Z</i></b><sup> (<i>i</i>) </sup><sub>s</sub>、<i>Ω</i><sup> (<i>i</i>) </sup>和<b><i>W</i></b>, 更新<b><i>D</i></b>。此时的目标损失函数为</p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula">
                        <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">D</mi></munder><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mtext>s</mtext><mo>⊗</mo></msubsup></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">D</mi><mo>∈</mo><mtext>Γ</mtext><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="122">与<i>Ω</i><sup> (<i>i</i>) </sup>∈<b>R</b><sup><i>d</i><sub><i>i</i></sub>×<i>d</i><sub><i>i</i></sub></sup>不同, <b><i>D</i></b>∈<b>R</b><sup><i>d</i>×<i>d</i><sub>c</sub></sup>的行列数不相等, 因此采用不同的求解方法, 新的正则项<i>R</i> (<b><i>D</i></b>) 为</p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">D</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">D</mi><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>-</mo><mi>log</mi><mrow><mo>|</mo><mrow><mi>det</mi><mspace width="0.25em" /><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">D</mi></mrow><mo>|</mo></mrow></mtd><mtd><mi>d</mi><mo>≥</mo><mi>d</mi><msub><mrow></mrow><mtext>c</mtext></msub></mtd></mtr><mtr><mtd><mrow><mo>|</mo><mi mathvariant="bold-italic">D</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>-</mo><mi>log</mi><mrow><mo>|</mo><mrow><mi>det</mi><mspace width="0.25em" /><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow><mo>|</mo></mrow></mtd><mtd><mi>d</mi><mo>&lt;</mo><mi>d</mi><msub><mrow></mrow><mtext>c</mtext></msub></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mtext> </mtext><mtext> </mtext><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="124">因此, 更新<b><i>D</i></b>的目标函数:</p>
                </div>
                <div class="p1">
                    <p id="125" class="code-formula">
                        <mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">D</mi></munder><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mtext>s</mtext><mo>⊗</mo></msubsup></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>α</mi><mi>R</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">D</mi><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="126">式中:<i>α</i>&gt;0为正则项<i>R</i> (<b><i>D</i></b>) 的重要性系数。利用 (11) 式难以直接求解字典<b><i>D</i></b>, 采用梯度下降方法进行求解。其中<mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mtext>s</mtext><mo>⊗</mo></msubsup></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mrow></math></mathml>和<i>R</i> (<b><i>D</i></b>) 对字典<b><i>D</i></b>的梯度分别为<mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∇</mo><msub><mrow></mrow><mtext>D</mtext></msub><mo stretchy="false"> (</mo><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mtext>s</mtext><mo>⊗</mo></msubsup></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo stretchy="false">) </mo><mo>=</mo><mn>2</mn><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mtext>s</mtext><mo>⊗</mo></msubsup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mtext>s</mtext><mo>⊗</mo></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>-</mo><mn>2</mn><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mtext>s</mtext><mo>⊗</mo></msubsup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mtext>s</mtext><mtext>c</mtext></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>, </mo><mo>∇</mo><msub><mrow></mrow><mtext>D</mtext></msub><mo stretchy="false">[</mo><mi>R</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">D</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>=</mo><mo>-</mo><mn>2</mn><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mo>†</mo></msup><mo>, </mo><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mo>†</mo></msup></mrow></math></mathml>为字典<b><i>D</i></b>的伪逆矩阵。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129"><b>2.3 unseen类场景的图像特征融合及分类</b></h4>
                <div class="p1">
                    <p id="130">将第2.2节得到的解析字典<i>Ω</i><sup> (<i>i</i>) </sup>、<b><i>D</i></b>以及鉴别项映射矩阵<b><i>W</i></b>, 迁移应用到unseen类场景图像上, 实现在无标注样本情形下的unseen类场景图像分类, 即零样本分类。首先, 采用各图像特征的解析字典项<i>Ω</i><sup> (<i>i</i>) </sup>, 提取unseen类场景图像特征的稀疏系数, 并将提取的稀疏系数串接, 组成融合特征。然后, 采用词向量的解析字典<b><i>D</i></b>, 将unseen类的词向量嵌入到融合特征空间。最后, 采用最近邻分类器 (KNN) , 根据融合特征空间中unseen类场景图像与unseen类词向量的距离, 推断unseen场景图像的类别标签。所提算法的整体框架如图1所示。</p>
                </div>
                <div class="area_img" id="131">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906012_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 所提算法的整体框架图" src="Detail/GetImg?filename=images/GXXB201906012_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 所提算法的整体框架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906012_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Whole framework of proposed method</p>

                </div>
                <h4 class="anchor-tag" id="132" name="132"><b>2.4 算法步骤</b></h4>
                <div class="p1">
                    <p id="133">所提算法在运算时, 输入seen类词向量<b><i>C</i></b><sub>s</sub>∈<b>R</b><sup><i>d</i><sub>c</sub>×<i>n</i><sub>s</sub></sup> (<i>d</i><sub>c</sub>为词向量维数, <i>n</i><sub>s</sub>为seen类个数) , 涉及<i>v</i>种不同的图像特征 (其中seen类图像特征<b><i>X</i></b><sup> (<i>i</i>) </sup><sub>s</sub>∈<b>R</b><sup><i>d</i><sub><i>i</i></sub>×<i>N</i><sub>s</sub></sup>, unseen类图像特征<b><i>X</i></b><sup> (<i>i</i>) </sup><sub>u</sub>∈<b>R</b><sup><i>d</i><sub><i>i</i></sub>×<i>N</i><sub>u</sub></sup>, <i>i</i>=1, 2, …, <i>v</i>, 下角标u代表unseen类别) , unseen类词向量为<b><i>C</i></b><sub>u</sub>∈<b>R</b><sup><i>d</i><sub>c</sub>×<i>n</i><sub>u</sub></sup> (<i>n</i><sub>u</sub>为unseen类个数) , 最大迭代次数为MaxIterNum。按照图2所示的运算流程, 循环运算直到达到最大迭代次数MaxIterNum为止。</p>
                </div>
                <h3 id="134" name="134" class="anchor-tag">3 实验及结果分析</h3>
                <h4 class="anchor-tag" id="135" name="135"><b>3.1 数据集及实验设置</b></h4>
                <div class="p1">
                    <p id="136">实验采用两种公开的遥感场景集:UC-Merced (UCM) 数据集<citation id="197" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>和航空图像数据集 (AID) <citation id="198" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。UCM是使用最广泛的遥感场景集, 有21类场景, 每类场景有100张图像, 共2100张遥感场景图像, 图像大小为256 pixel×256 pixel, 空间分辨率为0.3 m。UCM数据集的若干样本如图3所示;AID共有30类场景, 每类场景有220～420张图像, 共10000张场景图像, 图像大小为600 pixel×600 pixel, 空间分辨率为0.5～8 m, AID数据集的若干样本如图4所示。</p>
                </div>
                <div class="p1">
                    <p id="137">实验涉及3种层次的场景图像特征:低层场景图像特征、中层场景图像特征和高层场景图像特征。低层场景图像特征包括CH、SIFT、GIST和LBP。其中, CH场景图像特征为:在RGB三通道上, 分别按32个区间提取, 然后串接为96维的图像特征向量;SIFT场景图像特征为:采用4×4空间网格, 梯度量化为8个区间, 最后串接为128维特征向量;GIST场景图像特征为:采用4×4空间网格, 以及4个尺度和8个方向, 最后的特征向量为512维;LBP场景图像特征共256维, 提供了场景图像的纹理信息。将场景图像的CH特征的7种不同编码作为中层场景图像特征, 这7种不同编码模型分别为:BoVW、IFK、LDA、局部线性编码 (LLC) <citation id="199" type="reference"><link href="69" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>、概率隐式语义分析 (pLSA) <citation id="200" type="reference"><link href="71" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>、空间金字塔匹配 (SPM) <citation id="201" type="reference"><link href="73" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>和局部集成描述器向量 (VLAD) <citation id="202" type="reference"><link href="75" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>, 均采用16×16、步长为8 pixel的滑动窗口来提取CH特征;词袋大小具体如下:IFK和VLAD为128, SPM为256, pLSA为1024, BoVW和LLC为4096。此外, SPM的金字塔层数设定为2;pLSA的主题词数为64。高层场景图像特征采用在ImageNet大规模数据集上训练的4种DCNN模型 (CaffeNet、VGGNet、GoogLeNet和ResNet) , 以最后全连接层输出作为场景图像特征。VGGNet采用16层结构, ResNet采用152层结构。CaffeNet和VGGNet特征均为4096维, GoogLeNet和ResNet特征分别为1024和2048维。</p>
                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906012_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 所提算法运算流程图" src="Detail/GetImg?filename=images/GXXB201906012_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 所提算法运算流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906012_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Flow chart of proposed algorithm</p>

                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906012_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 UCM数据集中的若干类样本" src="Detail/GetImg?filename=images/GXXB201906012_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 UCM数据集中的若干类样本  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906012_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Images of several UCM classes</p>
                                <p class="img_note"> (a) 农田; (b) 飞机; (c) 棒球场; (d) 密集住宅; (e) 高速公路; (f) 海港; (g) 储罐; (h) 网球场; (i) 立交桥; (j) 高尔夫球场</p>
                                <p class="img_note"> (a) Agricultural; (b) airplane; (c) baseball diamond; (d) dense residential; (e) freeway; (f) harbor; (g) storage tanks; (h) tennis court; (i) overpass; (j) golf course</p>

                </div>
                <div class="area_img" id="141">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906012_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 AID数据集中的若干类样本" src="Detail/GetImg?filename=images/GXXB201906012_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 AID数据集中的若干类样本  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906012_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Images of several AID classes</p>
                                <p class="img_note"> (a) 机场; (b) 贫瘠地; (c) 海滩; (d) 桥梁; (e) 商业区; (f) 运动场; (g) 池塘; (h) 火车站; (i) 体育场; (j) 立交桥</p>
                                <p class="img_note">. (a) Airport; (b) bareland; (c) beach; (d) bridge; (e) commercial; (f) playgound; (g) pond; (h) railway station; (i) stadium; (j) viaduct</p>

                </div>
                <div class="p1">
                    <p id="142">采用在维基百科语料上训练的Word2Vec词向量, 作为场景类别的语义词向量, 维数为300。MaxIterNum设置为60。实验采用总体分类准确度 (OA) 作为评价指标, 其值为重复10次实验的unseen场景图像的分类准确度的均值。UCM和AID上的seen/unseen类划分, 分别采用16/5和25/5划分, 超参数<i>α</i>的取值范围为{10<sup>-3</sup>, 10<sup>-2</sup>, 10<sup>-1</sup>, 1, 10}, 从中选择最佳测试准确度。</p>
                </div>
                <h4 class="anchor-tag" id="143" name="143"><b>3.2 图像特征融合效果分析</b></h4>
                <div class="p1">
                    <p id="144">为了对比所提算法与其他ZSC算法的性能, 分别对相同层次及不同层次的场景图像特征进行融合。</p>
                </div>
                <h4 class="anchor-tag" id="145" name="145">3.2.1 相同层次图像特征融合</h4>
                <div class="p1">
                    <p id="146">对3种层次的图像特征分别进行融合, 对比所提算法及7种典型ZSC算法的效果。表1和表2中的行表示场景图像特征, 包括未融合的单图像特征和4种融合特征 (Fusion_CAT、Fusion_CCA、Fusion_ADL和Fusion_Ours) 。其中, Fusion_CAT为采用串接方法得到的融合图像特征, Fusion_CCA为采用多视角典型关联分析方法<citation id="203" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>得到的融合图像特征, Fusion_ADL为采用 (2) 式 (即没有结构对齐项的本文方法) 得到的融合图像特征, Fusion_Ours为采用本文方法得到的融合图像特征。表1和表2中的列表示ZSC方法及所提算法。每个OA值均为对应的ZSC方法在相应的图像特征上的实验结果。按如下步骤进行讨论:1) 在行方向进行比较, 即输入相同的图像特征 (未融合的图像特征、融合后的图像特征) 时, 对比分析本文算法与7种典型ZSC方法的OA值;2) 在列方向进行比较, 分析所提算法得到的融合图像特征是否优于未融合的单图像特征及通用性融合图像特征 (Fusion_CAT、Fusion_CCA、Fusion_ADL) 。</p>
                </div>
                <div class="p1">
                    <p id="147">由表1和表2的行方向数据可知, 不论是UCM还是AID数据集, 在3种层次图像特征 (高层图像特征、中层图像特征和低层图像特征) 上, 输入相同图像特征 (包括未融合的单图像特征及融合后的图像特征) 时, 本文方法的OA值均优于7种典型ZSC方法的OA值。其中, 在UCM数据集, 通过对4种高层图像特征的融合, 本文方法取得61.41%的最大OA值, 超过SSE算法OA (49.21%) 值12.20%, 超过SAE算法OA (44.80%) 值16.61%。而在AID数据集, 对4种高层图像特征进行融合, 本文方法同样取得68.34%的最大OA值, 超过SSE算法OA (55.38%) 值12.96%, 超过SAE算法OA (54.90%) 值13.44%。其主要原因在于本文方法综合利用鉴别项和结构对齐项, 提升了对unseen类场景的迁移识别效果, 而SSE等算法均未综合考虑这两个方面。</p>
                </div>
                <div class="p1">
                    <p id="148">由表1和表2可知, 对于相同层次图像特征的融合效果, 本文方法不仅超过所有未融合的单图像特征OA值, 也超过3种对比的融合特征 (Fusion_CAT、Fusion_CCA、Fusion_ADL) 的OA值, 例如, 在UCM数据集上, 本文方法对7种中层图像特征融合得到59.41%的OA值, 分别超过获得最大OA值的单图像特征 (IFK, 49.22%) 以及融合图像特征Fusion_CAT (34.83%) 、Fusion_CCA (37.61%) 和Fusion_ADL (44.04%) 的OA值10.19%、24.58%、21.80%和15.37%。</p>
                </div>
                <div class="p1">
                    <p id="149">上述结果表明:本文算法通过对相同层次的图像特征进行融合, 可以显著提升ZSC效果, 且融合效果优于串接、典型关联分析及ADL等常用的特征融合方法。其主要原因是:串接、典型关联分析及ADL等常用的特征融合方法, 仅考虑了特征间的关联性, 却没有考虑ZSC任务特点, 而所提算法通过将图像特征融合与ZSC任务结合, 能够适应ZSC任务特点。Fusion_ADL没有采用结构对齐项, 导致其ZSC效果低于本文方法的图像特征融合效果, 证明了所提算法中结构对齐项的必要性。</p>
                </div>
                <div class="area_img" id="150">
                    <p class="img_tit">表1 UCM数据集上ZSC算法的相同层次图像特征融合效果OA值 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 OA values of different ZSC algorithms on UCM dataset for fusion of the same level features</p>
                    <p class="img_note">%</p>
                    <table id="150" border="1"><tr><td rowspan="2" colspan="2"><br />Features</td><td colspan="8"><br />OA</td></tr><tr><td><br />LatEm</td><td>BiDiLEL</td><td>JLSE</td><td>SSE</td><td>DMaP</td><td>SAE</td><td>RKT</td><td>Ours</td></tr><tr><td rowspan="8"><br />High-level <br />features</td><td>CaffeNet</td><td>18.96</td><td>35.81</td><td>31.28</td><td>30.81</td><td>28.52</td><td>36.60</td><td>32.02</td><td><b>42.63</b></td></tr><tr><td><br />VGGNet</td><td>20.06</td><td>32.83</td><td>35.04</td><td>26.23</td><td>28.61</td><td>45.60</td><td>34.24</td><td><b>46.82</b></td></tr><tr><td><br />GoogLeNet</td><td>15.68</td><td>37.04</td><td>35.98</td><td>34.24</td><td>25.44</td><td>44.20</td><td>28.44</td><td><b>48.04</b></td></tr><tr><td><br />ResNet</td><td>20.00</td><td>35.01</td><td>22.10</td><td>19.58</td><td>25.01</td><td>24.24</td><td>18.03</td><td><b>38.74</b></td></tr><tr><td><br />Fusion_CAT</td><td>20.20</td><td>34.44</td><td>31.88</td><td>28.02</td><td>22.42</td><td>43.20</td><td>32.01</td><td><b>44.42</b></td></tr><tr><td><br />Fusion_CCA</td><td>20.54</td><td>35.46</td><td>21.34</td><td>20.44</td><td>27.41</td><td>30.06</td><td>24.44</td><td><b>37.24</b></td></tr><tr><td><br />Fusion_ADL</td><td>22.86</td><td>31.84</td><td>29.54</td><td>44.83</td><td>26.81</td><td>36.40</td><td>29.62</td><td><b>45.63</b></td></tr><tr><td><br />Fusion_Ours</td><td>23.20</td><td>35.63</td><td>37.80</td><td>49.21</td><td>31.83</td><td>44.80</td><td>34.41</td><td><b>61.41</b></td></tr><tr><td rowspan="11"><br />Middle-level <br />features</td><td>BoVW</td><td>20.80</td><td>36.83</td><td>20.72</td><td>26.64</td><td>18.84</td><td>25.80</td><td>29.83</td><td><b>37.24</b></td></tr><tr><td><br />IFK</td><td>20.74</td><td>47.04</td><td>27.34</td><td>19.24</td><td>26.76</td><td>39.20</td><td>26.04</td><td><b>49.22</b></td></tr><tr><td><br />LDA</td><td>21.92</td><td>38.03</td><td>27.56</td><td>27.83</td><td>31.22</td><td>29.40</td><td>33.59</td><td><b>39.23</b></td></tr><tr><td><br />LLC</td><td>20.82</td><td>45.37</td><td>26.66</td><td>18.21</td><td>33.44</td><td>28.20</td><td>27.18</td><td><b>47.42</b></td></tr><tr><td><br />pLSA</td><td>19.64</td><td>39.19</td><td>29.78</td><td>22.64</td><td>31.63</td><td>29.20</td><td>26.21</td><td><b>40.81</b></td></tr><tr><td><br />SPM</td><td>21.94</td><td>45.37</td><td>28.36</td><td>21.03</td><td>23.04</td><td>32.40</td><td>27.82</td><td><b>46.84</b></td></tr><tr><td><br />VLAD</td><td>20.38</td><td>42.63</td><td>26.62</td><td>24.04</td><td>30.02</td><td>39.00</td><td>29.83</td><td><b>44.81</b></td></tr><tr><td><br />Fusion_CAT</td><td>21.70</td><td>33.64</td><td>18.62</td><td>23.01</td><td>22.41</td><td>28.60</td><td>28.76</td><td><b>34.83</b></td></tr><tr><td><br />Fusion_CCA</td><td>21.48</td><td>35.04</td><td>20.40</td><td>29.62</td><td>20.63</td><td>34.80</td><td>28.37</td><td><b>37.61</b></td></tr><tr><td><br />Fusion_ADL</td><td>20.40</td><td>40.64</td><td>32.34</td><td>23.44</td><td>35.57</td><td>37.80</td><td>29.64</td><td><b>44.04</b></td></tr><tr><td><br />Fusion_Ours</td><td>23.70</td><td>46.02</td><td>33.28</td><td>40.61</td><td>37.19</td><td>39.80</td><td>34.03</td><td><b>59.41</b></td></tr><tr><td rowspan="8"><br />Low-level <br />features</td><td>CH</td><td>19.76</td><td>25.84</td><td>21.24</td><td>21.64</td><td>15.60</td><td>20.60</td><td>16.21</td><td><b>26.21</b></td></tr><tr><td><br />SIFT</td><td>20.80</td><td>43.24</td><td>21.22</td><td>21.38</td><td>41.82</td><td>28.40</td><td>20.02</td><td><b>44.63</b></td></tr><tr><td><br />GIST</td><td>21.98</td><td>37.43</td><td>20.86</td><td>18.19</td><td>31.61</td><td>21.20</td><td>20.04</td><td><b>39.84</b></td></tr><tr><td><br />LBP</td><td>20.28</td><td>44.41</td><td>31.50</td><td>26.41</td><td>39.24</td><td>37.00</td><td>26.24</td><td><b>45.82</b></td></tr><tr><td><br />Fusion_CAT</td><td>20.20</td><td>47.04</td><td>22.64</td><td>21.43</td><td>41.03</td><td>39.40</td><td>20.03</td><td><b>47.81</b></td></tr><tr><td><br />Fusion_CCA</td><td>20.22</td><td>44.83</td><td>25.80</td><td>28.84</td><td>38.81</td><td>34.80</td><td>32.84</td><td><b>45.84</b></td></tr><tr><td><br />Fusion_ADL</td><td>23.06</td><td>47.04</td><td>30.96</td><td>27.81</td><td>41.61</td><td>37.60</td><td>30.03</td><td><b>47.43</b></td></tr><tr><td><br />Fusion_Ours</td><td>23.20</td><td>47.24</td><td>32.40</td><td>34.24</td><td>43.24</td><td>41.20</td><td>38.19</td><td><b>54.47</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="151" name="151">3.2.2 不同层次图像特征融合</h4>
                <div class="p1">
                    <p id="152">在3种层次图像特征中, 分别选择取得最高OA结果的图像特征, 进行不同层次图像特征融合实验, 实验结果如表3和表4所示。本节通过对不同层次图像特征的融合效果进行比较, 分析本文方法是否优于典型ZSC方法及通用的特征融合方法。</p>
                </div>
                <div class="p1">
                    <p id="153">表3和表4中的行表示不同的ZSC方法, 列表示不同的图像特征 (未融合的单图像特征和融合后图像特征) 。由表3和表4可知, 对不同层次图像特征进行融合, 所提算法同样可以显著提升ZSC效果, 且相比其他ZSC方法, 能够获得更高的OA值。例如:在UCM数据集上, 对3种不同层次的图像特征GoogLeNet、IFK和LBP进行融合, 所提算法获得61.43%的最高OA值, 超过SAE方法OA值 (49.20%) 12.23%, 且超过BiDiLEL方法OA值 (47.83%) 13.60%;在AID数据集上, 对3种不同层次的图像特征GoogLeNet、BoVW和CH进行融合, 所提算法获得66.82%的最高OA值, 分别超过RKT (52.43%) 、BiDiLEL (54.55%) 方法14.39%、12.27%。因此, 对不同层次的图像特征进行融合, 所提算法优于其他ZSC方法。综上表明:所提算法在相同以及不同层次图像特征的融合上优于其他ZSC方法和通用特征融合方法。</p>
                </div>
                <div class="area_img" id="154">
                    <p class="img_tit">表2 AID数据集上ZSC算法的相同层次图像特征融合效果OA值 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 OA values of different ZSC algorithms on AID dataset for fusion of the same level features</p>
                    <p class="img_note">%</p>
                    <table id="154" border="1"><tr><td rowspan="2" colspan="2"><br />Features</td><td colspan="8"><br />OA</td></tr><tr><td><br />LatEm</td><td>BiDiLEL</td><td>JLSE</td><td>SSE</td><td>DMaP</td><td>SAE</td><td>RKT</td><td>Ours</td></tr><tr><td rowspan="8"><br />High-level <br />features</td><td>CaffeNet</td><td>19.48</td><td>51.30</td><td>40.63</td><td>49.05</td><td>41.54</td><td>36.30</td><td>45.55</td><td><b>52.23</b></td></tr><tr><td><br />VGGNet</td><td>19.93</td><td>49.94</td><td>44.37</td><td>44.56</td><td>41.30</td><td>33.40</td><td>44.73</td><td><b>52.09</b></td></tr><tr><td><br />GoogLeNet</td><td>20.12</td><td>51.59</td><td>44.80</td><td>48.76</td><td>45.21</td><td>43.30</td><td>51.18</td><td><b>53.27</b></td></tr><tr><td><br />ResNet</td><td>19.98</td><td>37.69</td><td>17.95</td><td>29.47</td><td>36.39</td><td>32.20</td><td>17.87</td><td><b>41.65</b></td></tr><tr><td><br />Fusion_CAT</td><td>19.99</td><td>52.99</td><td>39.74</td><td>29.76</td><td>35.80</td><td>38.90</td><td>49.64</td><td><b>53.41</b></td></tr><tr><td><br />Fusion_CCA</td><td>18.86</td><td>51.03</td><td>20.70</td><td>45.86</td><td>38.70</td><td>45.10</td><td>35.50</td><td><b>51.96</b></td></tr><tr><td><br />Fusion_ADL</td><td>20.42</td><td>52.77</td><td>43.83</td><td>55.15</td><td>46.21</td><td>43.90</td><td>52.60</td><td><b>55.52</b></td></tr><tr><td><br />Fusion_Ours</td><td>21.02</td><td>53.09</td><td>47.22</td><td>55.38</td><td>45.62</td><td>54.90</td><td>50.12</td><td><b>68.34</b></td></tr><tr><td rowspan="11"><br />Middle-level <br />features</td><td>BoVW</td><td>20.04</td><td>36.04</td><td>40.33</td><td>51.83</td><td>29.70</td><td>44.00</td><td>35.56</td><td><b>52.76</b></td></tr><tr><td><br />IFK</td><td>19.75</td><td>50.01</td><td>45.22</td><td>30.89</td><td>34.67</td><td>43.10</td><td>28.17</td><td><b>51.89</b></td></tr><tr><td><br />LDA</td><td>20.01</td><td>36.12</td><td>42.35</td><td>47.10</td><td>34.44</td><td>38.90</td><td>36.21</td><td><b>48.67</b></td></tr><tr><td><br />LLC</td><td>19.72</td><td>44.58</td><td>37.61</td><td>44.85</td><td>30.00</td><td>41.30</td><td>47.99</td><td><b>48.51</b></td></tr><tr><td><br />pLSA</td><td>20.15</td><td>35.37</td><td>41.36</td><td>49.53</td><td>34.44</td><td>36.30</td><td>44.73</td><td><b>50.93</b></td></tr><tr><td><br />SPM</td><td>20.04</td><td>43.36</td><td>38.78</td><td>37.46</td><td>35.15</td><td>37.20</td><td>33.43</td><td><b>46.80</b></td></tr><tr><td><br />VLAD</td><td>20.13</td><td>36.39</td><td>35.47</td><td>41.60</td><td>29.17</td><td>35.60</td><td>33.37</td><td><b>46.56</b></td></tr><tr><td><br />Fusion_CAT</td><td>20.15</td><td>35.18</td><td>40.67</td><td>41.78</td><td>37.34</td><td>43.80</td><td>34.50</td><td><b>46.37</b></td></tr><tr><td><br />Fusion_CCA</td><td>21.76</td><td>36.35</td><td>19.12</td><td>15.74</td><td>23.61</td><td>27.40</td><td>31.66</td><td><b>46.04</b></td></tr><tr><td><br />Fusion_ADL</td><td>20.11</td><td>44.13</td><td>37.34</td><td>38.76</td><td>34.73</td><td>33.80</td><td>33.79</td><td><b>44.18</b></td></tr><tr><td><br />Fusion_Ours</td><td>20.91</td><td>45.17</td><td>38.28</td><td>42.49</td><td>40.53</td><td>36.90</td><td>32.43</td><td><b>66.05</b></td></tr><tr><td rowspan="8"><br />Low-level <br />features</td><td>CH</td><td>20.00</td><td>40.87</td><td>35.00</td><td>30.53</td><td>45.09</td><td>26.00</td><td>18.82</td><td><b>46.07</b></td></tr><tr><td><br />SIFT</td><td>19.98</td><td>25.19</td><td>13.59</td><td>17.81</td><td>28.64</td><td>19.20</td><td>15.38</td><td><b>30.28</b></td></tr><tr><td><br />GIST</td><td>19.81</td><td>27.34</td><td>29.17</td><td>17.51</td><td>39.70</td><td>26.70</td><td>15.50</td><td><b>40.68</b></td></tr><tr><td><br />LBP</td><td>19.89</td><td>31.07</td><td>15.45</td><td>21.66</td><td>26.75</td><td>32.40</td><td>15.38</td><td><b>34.40</b></td></tr><tr><td><br />Fusion_CAT</td><td>19.76</td><td>36.87</td><td>38.65</td><td>31.30</td><td>40.00</td><td>35.50</td><td>15.38</td><td><b>43.19</b></td></tr><tr><td><br />Fusion_CCA</td><td>19.84</td><td>35.77</td><td>40.21</td><td>31.66</td><td>40.47</td><td>24.30</td><td>43.25</td><td><b>46.08</b></td></tr><tr><td><br />Fusion_ADL</td><td>20.03</td><td>34.72</td><td>40.04</td><td>28.17</td><td>38.11</td><td>36.50</td><td>36.39</td><td><b>44.15</b></td></tr><tr><td><br />Fusion_Ours</td><td>20.37</td><td>46.60</td><td>41.77</td><td>32.31</td><td>46.04</td><td>36.60</td><td>46.86</td><td><b>53.91</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="155">此外, 图5和图6分别为UCM和AID上相同及不同层次图像特征融合时, 所提算法的各unseen类的分类准确度。由图5和图6可知, 所提算法融合特征的各unseen类的分类准确度, 均优于未融合的单图像特征和三种通用融合图像特征, 这进一步证明所提算法能够获得适应ZSC任务的融合图像特征, 从而显著提升ZSC的性能。</p>
                </div>
                <h4 class="anchor-tag" id="156" name="156"><b>3.3 算法收敛性分析</b></h4>
                <div class="p1">
                    <p id="157">采用迭代过程中的总体损失和测试准确度作为观察指标, 以验证所提算法是否能够快速稳定收敛。在每种数据集上观察相同层次以及不同层次 (高层、中层和低层) 图像特征的4种融合情形。图7和图8分别为UCM和AID数据集上的总体损失曲线和测试准确度折线图。由图7和图8可知, 在UCM和AID数据集上, 4种图像特征融合情形的总体损失曲线均持续下降, 测试准确度折线均逐渐提升, 且都迅速稳定, 说明所提算法能够快速达到稳定的收敛状态, 具有良好的收敛性。</p>
                </div>
                <h4 class="anchor-tag" id="158" name="158"><b>3.4 计算效率分析</b></h4>
                <div class="p1">
                    <p id="159">为比较所提算法与其他ZSC算法的计算效率, 在AID数据集上计算各ZSC方法对高层图像特征GoogLeNet的运算耗时, 结果如表5所示。根据表5, RKT方法耗时最长, 为485.57 s, 其次为BiDiLEL方法, 耗时为252.22 s, 而所提算法耗时最短, 为71.59 s。这表明所提算法的运算效率优于其他7种典型ZSC方法。</p>
                </div>
                <div class="area_img" id="160">
                    <p class="img_tit">表3 不同ZSC算法在UCM数据集上的不同层次图像特征融合OA值 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 OA values of different ZSC algorithms on UCM dataset for fusion of different level features</p>
                    <p class="img_note">%</p>
                    <table id="160" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="7"><br />OA</td></tr><tr><td><br />High-level <br />feature</td><td>Middle-level <br />feature</td><td>Low-level <br />feature</td><td>Fusion_CAT</td><td>Fusion_CCA</td><td>Fusion_ADL</td><td>Fusion_Ours</td></tr><tr><td>LatEm</td><td>20.06<br />VGGNet</td><td>21.94<br />SPM</td><td>21.98<br />GIST</td><td>20.96</td><td>20.87</td><td>21.6</td><td>27.46</td></tr><tr><td><br />BiDiLEL</td><td>37.04<br />GoogLeNet</td><td>47.04<br />IFK</td><td>44.41<br />LBP</td><td>36.82</td><td>32.84</td><td>39.42</td><td>47.83</td></tr><tr><td><br />JLSE</td><td>35.98<br />GoogLeNet</td><td>29.78<br />pLSA</td><td>31.50<br />LBP</td><td>34.24</td><td>35.12</td><td>37.20</td><td>38.52</td></tr><tr><td><br />SSE</td><td>34.24<br />GoogLeNet</td><td>27.83<br />LDA</td><td>26.41<br />LBP</td><td>31.64</td><td>34.80</td><td>38.05</td><td>39.83</td></tr><tr><td><br />DMaP</td><td>28.61<br />VGGNet</td><td>33.44<br />LLC</td><td>41.82<br />SIFT</td><td>30.62</td><td>39.21</td><td>38.83</td><td>42.44</td></tr><tr><td><br />SAE</td><td>45.60<br />VGGNet</td><td>39.20<br />IFK</td><td>37.00<br />LBP</td><td>45.60</td><td>47.60</td><td>48.90</td><td>49.20</td></tr><tr><td><br />RKT</td><td>34.24<br />VGGNet</td><td>33.59<br />LDA</td><td>26.24<br />LBP</td><td>35.64</td><td>34.81</td><td>33.43</td><td>38.54</td></tr><tr><td><br />Ours</td><td>48.04<br />GoogLeNet</td><td>49.22<br />IFK</td><td>45.82<br />LBP</td><td>46.44</td><td>47.59</td><td>48.24</td><td><b>61.43</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="161">
                    <p class="img_tit">表4 不同ZSC算法在AID数据集上不同层次图像特征融合OA值 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 OA values of different ZSC algorithms on AID dataset for fusion of the different levels features</p>
                    <p class="img_note">%</p>
                    <table id="161" border="1"><tr><td rowspan="2"><br />Methods</td><td colspan="7"><br />OA</td></tr><tr><td><br />High-level <br />feature</td><td>Middle-level <br />feature</td><td>Low-level <br />feature</td><td>Fusion_CAT</td><td>Fusion_CCA</td><td>Fusion_ADL</td><td>Fusion_Ours</td></tr><tr><td>LatEm</td><td>20.12<br />GoogLeNet</td><td>20.15<br />pLSA</td><td>20.00<br />CH</td><td>20.08</td><td>20.13</td><td>20.33</td><td>21.16</td></tr><tr><td><br />BiDiLEL</td><td>51.59<br />GoogLeNet</td><td>50.01<br />IFK</td><td>40.87<br />CH</td><td>53.50</td><td>47.51</td><td>52.58</td><td>54.55</td></tr><tr><td><br />JLSE</td><td>44.80<br />GoogLeNet</td><td>45.22<br />IFK</td><td>35.00<br />CH</td><td>45.80</td><td>43.61</td><td>47.57</td><td>49.40</td></tr><tr><td><br />SSE</td><td>49.05<br />CaffeNet</td><td>51.83<br />BoVW</td><td>30.53<br />CH</td><td>38.39</td><td>33.24</td><td>42.31</td><td>45.27</td></tr><tr><td><br />DMaP</td><td>45.21<br />GoogLeNet</td><td>35.15<br />SPM</td><td>45.09<br />CH</td><td>40.47</td><td>45.03</td><td>45.86</td><td>47.37</td></tr><tr><td><br />SAE</td><td>43.30<br />GoogLeNet</td><td>44.00<br />BoVW</td><td>32.40<br />LBP</td><td>28.20</td><td>37.4</td><td>39.5</td><td>42.20</td></tr><tr><td><br />RKT</td><td>51.18<br />GoogLeNet</td><td>47.99<br />LLC</td><td>18.82<br />CH</td><td>51.49</td><td>47.75</td><td>50.71</td><td>52.43</td></tr><tr><td><br />Ours</td><td>52.23<br />GoogLeNet</td><td>52.76<br />BoVW</td><td>46.07<br />CH</td><td>50.85</td><td>53.58</td><td>55.48</td><td><b>66.82</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="162">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906012_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 所提算法在UCM上融合时的各unseen类的分类准确度" src="Detail/GetImg?filename=images/GXXB201906012_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 所提算法在UCM上融合时的各unseen类的分类准确度  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906012_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Test accuracies of unseen classes of our algorithm′s fusion on UCM dataset</p>
                                <p class="img_note"> (a) 高层图像特征融合; (b) 中层图像特征融合; (c) 低层图像特征融合; (d) 不同层图像特征融合</p>
                                <p class="img_note"> (a) High-level feature fusion; (b) middle-level feature fusion; (c) low-level feature fusion; (d) different-level feature fusion</p>

                </div>
                <div class="area_img" id="163">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906012_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 所提算法在AID上融合时的各unseen类的分类准确度" src="Detail/GetImg?filename=images/GXXB201906012_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 所提算法在AID上融合时的各unseen类的分类准确度  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906012_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Test accuracies of unseen classes of our algorithm′s fusion on AID dataset</p>
                                <p class="img_note"> (a) 高层图像特征融合; (b) 中层图像特征融合; (c) 低层图像特征融合; (d) 不同层图像特征融合</p>
                                <p class="img_note"> (a) High-level feature fusion; (b) middle-level feature fusion; (c) low-level feature fusion; (d) different-level feature fusion</p>

                </div>
                <div class="area_img" id="164">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906012_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 所提算法在UCM场景集上融合时的总体损失曲线和测试准确度折线" src="Detail/GetImg?filename=images/GXXB201906012_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 所提算法在UCM场景集上融合时的总体损失曲线和测试准确度折线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906012_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Overall loss and test accuracy of our algorithm′s fusion on UCM dataset</p>
                                <p class="img_note"> (a) 高层图像特征融合; (b) 中层图像特征融合; (c) 低层图像特征融合; (d) 不同层图像特征融合</p>
                                <p class="img_note">. (a) High-level feature fusion; (b) middle-level feature fusion; (c) low-level feature fusion; (d) different-level feature fusion</p>

                </div>
                <div class="area_img" id="165">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906012_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 所提算法在AID场景集上融合时的总体损失曲线和测试准确度折线" src="Detail/GetImg?filename=images/GXXB201906012_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 所提算法在AID场景集上融合时的总体损失曲线和测试准确度折线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906012_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Overall loss and test accuracy of our algorithm′s fusion on AID dataset</p>
                                <p class="img_note"> (a) 高层图像特征融合; (b) 中层图像特征融合; (c) 低层图像特征融合; (d) 不同层图像特征融合</p>
                                <p class="img_note"> (a) High-level feature fusion; (b) middle-level feature fusion; (c) low-level feature fusion; (d) different-level feature fusion</p>

                </div>
                <div class="area_img" id="166">
                    <p class="img_tit">表5 各ZSC算法在AID上的GoogLeNet特征运算耗时 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Computing time of different ZSC algorithms on AID dataset for GoogLeNet feature</p>
                    <p class="img_note"></p>
                    <table id="166" border="1"><tr><td><br />Method</td><td>LatEm</td><td>BiDiLEL</td><td>JLSE</td><td>SSE</td><td>DMaP</td><td>SAE</td><td>RKT</td><td>Ours</td></tr><tr><td><br />Time /s</td><td>85.65</td><td>252.22</td><td>83.39</td><td>172.19</td><td>73.68</td><td>75.46</td><td>485.57</td><td>71.59</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="167" name="167" class="anchor-tag">4 结  论</h3>
                <div class="p1">
                    <p id="168">现有ZSC算法仅使用单一图像特征而未利用不同图像特征间的互补性, 针对该问题, 提出一种基于图像特征融合的遥感场景零样本分类算法。该算法以解析字典学习作为稀疏编码提取器, 将串接后的稀疏编码系数作为融合图像特征, 并以seen类别场景样本的类别标签作为监督信息, 提升对unseen类场景样本的迁移识别能力。实验结果表明, 在相同层次及不同层次图像特征的融合上, 所提算法均取得最佳的总体分类准确度, 且计算效率最高, 所提算法能够有效地利用不同图像特征间的互补性, 显著提升遥感场景零样本分类性能。但是所提算法未考虑场景图像之间的近邻关系, 未来将在稀疏编码空间中保持图像特征空间中的几何近邻关系, 以进一步提升图像特征融合效果和遥感场景零样本分类性能。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="13">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604039&amp;v=MzA3MTBWdUZ5am1Wci9JSWpYVGJMRzRIOWZNcTQ5R2JZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Liu D W, Han L, Han X Y.High spatial resolution remote sensing image classification based on deep learning[J].Acta Optica Sinica, 2016, 36 (4) :0428001.刘大伟, 韩玲, 韩晓勇.基于深度学习的高分辨率遥感影像分类研究[J].光学学报, 2016, 36 (4) :0428001.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pyramid of Spatial Relatons for Scene-Level Land Use Classification">

                                <b>[2]</b> Chen S Z, Tian Y L.Pyramid of spatial relations for scene-level land use classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (4) :1947-1957.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning for remote sensing data:A technical tutorial on the state of the art">

                                <b>[3]</b> Zhang L P, Zhang L F, Du B.Deep learning for remote sensing data:a technical tutorial on the state of the art[J].IEEE Geoscience and Remote Sensing Magazine, 2016, 4 (2) :22-40.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot scene classification for high spatial resolution remote sensing images">

                                <b>[4]</b> Li A X, Lu Z W, Wang L W, <i>et al</i>.Zero-shot scene classification for high spatial resolution remote sensing images[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :4157-4167.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Latent embeddings for zero-shot classification">

                                <b>[5]</b> Xian Y Q, Akata Z, Sharma G, <i>et al</i>.Latent embeddings for zero-shot classification[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:69-77.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Relational knowledge transfer for zero-shot learning">

                                <b>[6]</b> Wang D, Li Y, Lin Y, <i>et al</i>.Relational knowledge transfer for zero-shot learning[C]∥Thirtieth AAAI Conference in Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA.California:AAAI Press, 2016:2145-2151.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning via joint latent similarity embedding">

                                <b>[7]</b> Zhang Z M, Saligrama V.Zero-shot learning via joint latent similarity embedding[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:6034-6042.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning via semantic similarity embedding">

                                <b>[8]</b> Zhang Z M, Saligrama V.Zero-shot learning via semantic similarity embedding[C]∥2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:4166-4174.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDD31F20253A10A4B4D5AE16DA35AA17A9&amp;v=MjA3NTF2RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh4THU2d0tnPU5qN0Jhc2U3SDZmT3I0MUFaNW9PREEwOXZSSm43MDRJU1htVzNSRXdDTU9WUXN1V0NPTg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Wang Q, Chen K.Zero-shot visual recognition via bidirectional latent embedding[J].International Journal of Computer Vision, 2017, 124 (3) :356-383.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot recognition using dual visual-semantic mapping paths">

                                <b>[10]</b> Li Y N, Wang D H, Hu H H, <i>et al</i>.Zero-shot recognition using dual visual-semantic mapping paths[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:5207-5215.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic Autoencoder for Zero-Shot Learning">

                                <b>[11]</b> Kodirov E, Xiang T, Gong S G.Semantic autoencoder for zero-shot learning[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4447-4456.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative feature fusion for image classific-ation">

                                <b>[12]</b> Fernando B, Fromont E, Muselet D, <i>et al</i>.Discriminative feature fusion for image classification[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 16-21, 2012, Providence, RI, USA.New York:IEEE, 2012:3434-3441.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dictionary learning for sparse coding:algorithms and convergence analysis">

                                <b>[13]</b> Bao C L, Ji H, Quan Y H, <i>et al</i>.Dictionary learning for sparse coding:algorithms and convergence analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (7) :1356-1369.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative analysis-synthesis dictionary learning for image classification">

                                <b>[14]</b> Yang M, Chang H Y, Luo W X.Discriminative analysis-synthesis dictionary learning for image classification[J].Neurocomputing, 2017, 219:404-411.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES579AE40DE8831392A28EB8D29F9C9B3D&amp;v=MjUyNzBwbWFCdUhZZk9HUWxmQ3BiUTM1ZGhoeEx1NndLZz1OaWZPZmJhL0Y2QzVxNDh4RWVNSEQzMDZ4aFJpNkRjSU9uZVdyaHREY01HZE43bnJDT052RlNpV1dyN0pJRg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Wang J J, Guo Y Q, Guo J, <i>et al</i>.Synthesis linear classifier based analysis dictionary learning for pattern classification[J].Neurocomputing, 2017, 238:103-113.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=L0 sparsifying transform learning with efficient optimal updates and convergence guarantees">

                                <b>[16]</b> Ravishankar S, Bresler Y.Sparsifying transform learning with efficient optimal updates and convergence guarantees[J].IEEE Transactions on Signal Processing, 2015, 63 (9) :2389-2404.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=AID:a benchmark data set for performance evaluation of aerial scene classification">

                                <b>[17]</b> Xia G S, Hu J W, Hu F, <i>et al</i>.AID:a benchmark data set for performance evaluation of aerial scene classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :3965-3981.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830225&amp;v=MzEzMjJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTamxVN3pOSUY0PU5qN0Jhck80SHRIT3A0eEZadWtLWTNrNXpC&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Swain M J, Ballard D H.Color indexing[J].International Journal of Computer Vision, 1991, 7 (1) :11-32.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiresolution gray-scale and rotation invariant texture classification with local binary patterns">

                                <b>[19]</b> Ojala T, Pietikainen M, Maenpaa T.Multiresolution gray-scale and rotation invariant texture classification with local binary patterns[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002, 24 (7) :971-987.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MDkwNDFlc09ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTamxVN3pOSUY0PU5qN0Jhck80SHRIT3A0eEZi&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830679&amp;v=MDYwNjlTamxVN3pOSUY0PU5qN0Jhck80SHRIT3A0eEZZdXdHWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRG&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> Oliva A, Torralba A.Modeling the shape of the scene:a holistic representation of the spatial envelope[J].International Journal of Computer Vision, 2001, 42 (3) :145-175.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bag-of-visual-words and spatial extensions for land-use classification">

                                <b>[22]</b> Yang Y, Newsam S.Bag-of-visual-words and spatial extensions for land-use classification[C]//Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems, November 2-5, 2010, San Jose, California.New York:ACM, 2010:270-279.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving the fisher kernel for large-scale image classification">

                                <b>[23]</b> Perronnin F, Sánchez J, Mensink T.Improving the fisher kernel for large-scale image classification[M]//Daniilidis K, Maragos P, Paragios N.Computer Vision-ECCV 2010.Berlin, Heidelberg:Springer, 2010:143-156.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_24" >
                                    <b>[24]</b>
                                 Blei D M, Ng A Y, Jordan M I.Latent dirichl location[J].Journal of Machine Learning research, 2003, 3:993-1022.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_25" >
                                    <b>[25]</b>
                                 Jia Y Q, Shelhamer E, Donahue J, <i>et al</i>.Caffe:convolutional architecture for fast feature embedding[C]//Proceedings of the 22nd ACM international conference on Multimedia, November 3-7, 2014, Orlando, Florida, USA.New York:ACM, 2014:675-678.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">

                                <b>[26]</b> Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2018-12-25].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
                        <p id="65">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[27]</b> Szegedy C, Liu W, Jia Y Q, <i>et al</i>.Going deeper with convolutions[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:1-9.
                            </a>
                        </p>
                        <p id="67">
                            <a id="bibliography_28" >
                                    <b>[28]</b>
                                 He K M, Zhang X Y, Ren S Q, <i>et al</i>.Deep residual learning for image recognition[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="69">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locality-constrained linear coding for image classification">

                                <b>[29]</b> Wang J J, Yang J C, Yu K, <i>et al</i>.Locality-constrained linear coding for image classification[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) , June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:3360-3367.
                            </a>
                        </p>
                        <p id="71">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scene classification via pLSA">

                                <b>[30]</b> Bosch A, Zisserman A, Muňoz X.Scene classification via pLSA[M]//Leonardis A, Bischof H, Pinz A.Computer Vision-ECCV 2006.Berlin, Heidelberg:Springer, 2006:517-530.
                            </a>
                        </p>
                        <p id="73">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond bags of features: spatial pyramid matching for recognizing natural scene categories">

                                <b>[31]</b> Lazebnik S, Schmid C, Ponce J.Beyond bags of features:spatial pyramid matching for recognizing natural scene categories[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) , June 17-22, 2006, New York, USA.New York:IEEE, 2006:2169-2178.
                            </a>
                        </p>
                        <p id="75">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Aggregating Local Image Descriptors into Compact Codes">

                                <b>[32]</b> Jegou H, Perronnin F, Douze M, <i>et al</i>.Aggregating local image descriptors into compact codes[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (9) :1704-1716.
                            </a>
                        </p>
                        <p id="77">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-view canonical correlation analysis">

                                <b>[33]</b> Rupnik J, Shawe-Taylor J.Multi-view canonical correlation analysis[C]∥Proceedings of the 13th Multiconference on Information Society, IS, Ljubljana, Slovenia.[S.l.:s.n.], 2010:201-204.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201906012" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201906012&amp;v=MTkzODFqWFRiTEc0SDlqTXFZOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeWptVnIvSUk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

