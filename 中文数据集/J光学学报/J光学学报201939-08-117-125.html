

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133833693096250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201908015%26RESULT%3d1%26SIGN%3ddppsNm5%252bLD5itDGLA0r08qFGU6Q%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201908015&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201908015&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201908015&amp;v=Mjg0MzN5bmtVTHJPSWpYVGJMRzRIOWpNcDQ5RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#65" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#69" data-title="2 SSGAN ">2 SSGAN</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="&lt;b&gt;2.1 GAN&lt;/b&gt;"><b>2.1 GAN</b></a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;2.2 SSGAN&lt;/b&gt;"><b>2.2 SSGAN</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#84" data-title="3 基于GAN的半监督X影像分类 ">3 基于GAN的半监督X影像分类</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#86" data-title="&lt;b&gt;3.1 数据及预处理&lt;/b&gt;"><b>3.1 数据及预处理</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;3.2 SSGAN模型训练&lt;/b&gt;"><b>3.2 SSGAN模型训练</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#124" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#137" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#74" data-title="图1 GAN流程图">图1 GAN流程图</a></li>
                                                <li><a href="#77" data-title="图2 SSGAN结构">图2 SSGAN结构</a></li>
                                                <li><a href="#81" data-title="图3 SSGAN生成网络结构图">图3 SSGAN生成网络结构图</a></li>
                                                <li><a href="#83" data-title="图4 SSGAN判别器D结构图">图4 SSGAN判别器D结构图</a></li>
                                                <li><a href="#89" data-title="图5 训练样本图像">图5 训练样本图像</a></li>
                                                <li><a href="#90" data-title="图6 直方图均衡化图像对比图。">图6 直方图均衡化图像对比图。</a></li>
                                                <li><a href="#126" data-title="图7 损失函数变化趋势。">图7 损失函数变化趋势。</a></li>
                                                <li><a href="#129" data-title="表1 不同数量标注样本的平均分类准确率">表1 不同数量标注样本的平均分类准确率</a></li>
                                                <li><a href="#130" data-title="图8 不同数量标注样本分类结果趋势图">图8 不同数量标注样本分类结果趋势图</a></li>
                                                <li><a href="#133" data-title="表2 1000个标注样本分类准确率">表2 1000个标注样本分类准确率</a></li>
                                                <li><a href="#135" data-title="图9 SSGAN生成图像与原始图像。">图9 SSGAN生成图像与原始图像。</a></li>
                                                <li><a href="#136" data-title="表3 各种模型生成样本的inception分值">表3 各种模型生成样本的inception分值</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]∥25th International Conference on Neural Information Processing Systems, December 3-6, 2012, Lake Tahoe, Nevada.USA:Curran Associates Inc., 2012, 1:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">
                                        <b>[1]</b>
                                         Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]∥25th International Conference on Neural Information Processing Systems, December 3-6, 2012, Lake Tahoe, Nevada.USA:Curran Associates Inc., 2012, 1:1097-1105.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Ren S Q, He K M, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Faster R-CNN:towards real-time object detection with region proposal networks[C]//28th International Conference on Neural Information Processing Systems, December 7-12, 2015, Montreal, Canada.Cambridge:MIT Press, 2015, 1:91-99." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">
                                        <b>[2]</b>
                                         Ren S Q, He K M, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Faster R-CNN:towards real-time object detection with region proposal networks[C]//28th International Conference on Neural Information Processing Systems, December 7-12, 2015, Montreal, Canada.Cambridge:MIT Press, 2015, 1:91-99.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" Long J, Shelhamer E, Darrell T.Fully convolutional networks for semantic segmentation[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[3]</b>
                                         Long J, Shelhamer E, Darrell T.Fully convolutional networks for semantic segmentation[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:3431-3440.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Ciresan D, Giusti A, Gambardella L M, &lt;i&gt;et al&lt;/i&gt;.Deep neural networks segment neuronal membranes in electron microscopy images[C]∥25th International Conference on Neural Information Processing Systems, December 3-6, 2012, Lake Tahoe, Nevada.USA:Curran Associates Inc., 2012, 2:2843-2851." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep neural networks segment neuronal membranes in electron microscopy images">
                                        <b>[4]</b>
                                         Ciresan D, Giusti A, Gambardella L M, &lt;i&gt;et al&lt;/i&gt;.Deep neural networks segment neuronal membranes in electron microscopy images[C]∥25th International Conference on Neural Information Processing Systems, December 3-6, 2012, Lake Tahoe, Nevada.USA:Curran Associates Inc., 2012, 2:2843-2851.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" Esteva A, Kuprel B, Novoa R A, &lt;i&gt;et al&lt;/i&gt;.Dermatologist-level classification of skin cancer with deep neural networks[J].Nature, 2017, 542 (7639) :115-118." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dermatologist-level classification of skin cancer with deep neural networks">
                                        <b>[5]</b>
                                         Esteva A, Kuprel B, Novoa R A, &lt;i&gt;et al&lt;/i&gt;.Dermatologist-level classification of skin cancer with deep neural networks[J].Nature, 2017, 542 (7639) :115-118.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Platania R, Shams S, Yang S, &lt;i&gt;et al&lt;/i&gt;.Automated breast cancer diagnosis using deep learning and region of interest detection (BC-DROID) [C]∥8th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatic, August 20-23, 2017, Boston.New York:ACM, 2017:536-543." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automated breast cancer diagnosis using deep learning and region of interest detection (BC-DROID)">
                                        <b>[6]</b>
                                         Platania R, Shams S, Yang S, &lt;i&gt;et al&lt;/i&gt;.Automated breast cancer diagnosis using deep learning and region of interest detection (BC-DROID) [C]∥8th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatic, August 20-23, 2017, Boston.New York:ACM, 2017:536-543.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" Tseng K L, Lin Y L, Hsu W, &lt;i&gt;et al&lt;/i&gt;.Joint sequence learning and cross-modality convolution for 3D biomedical segmentation[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI.New York:IEEE, 2017:3739-3746." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint Sequence Learning and CrossModality Convolution for 3D Biomedical Segmentation">
                                        <b>[7]</b>
                                         Tseng K L, Lin Y L, Hsu W, &lt;i&gt;et al&lt;/i&gt;.Joint sequence learning and cross-modality convolution for 3D biomedical segmentation[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI.New York:IEEE, 2017:3739-3746.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" Rajpurkar P, Irvin J, Zhu K, &lt;i&gt;et al&lt;/i&gt;.Chexnet:radiologist-level pneumonia detection on chest X-rays with deep learning[J/OL]. (2017-12-25) [2019-01-25].https://arxiv.org/abs/1711.05225." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Chexnet:radiologist-level pneumonia detection on chest X-rays with deep learning">
                                        <b>[8]</b>
                                         Rajpurkar P, Irvin J, Zhu K, &lt;i&gt;et al&lt;/i&gt;.Chexnet:radiologist-level pneumonia detection on chest X-rays with deep learning[J/OL]. (2017-12-25) [2019-01-25].https://arxiv.org/abs/1711.05225.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" Rosenberg C, Hebert M, Schneiderman H.Semi-supervised self-training of object detection models[C]//2005 Seventh IEEE Workshops on Applications of Computer Vision, January 5-7, 2005, Breckenridge, CO.New York:IEEE, 2005:29-36." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised self-training of object detection models">
                                        <b>[9]</b>
                                         Rosenberg C, Hebert M, Schneiderman H.Semi-supervised self-training of object detection models[C]//2005 Seventh IEEE Workshops on Applications of Computer Vision, January 5-7, 2005, Breckenridge, CO.New York:IEEE, 2005:29-36.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" Rasmus A, Berglund M, Honkala M, &lt;i&gt;et al&lt;/i&gt;.Semi-supervised learning with ladder networks[J/OL]. (2015-11-24) [2019-01-25].https://arxiv.org/abs/1507.02672." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning with ladder networks">
                                        <b>[10]</b>
                                         Rasmus A, Berglund M, Honkala M, &lt;i&gt;et al&lt;/i&gt;.Semi-supervised learning with ladder networks[J/OL]. (2015-11-24) [2019-01-25].https://arxiv.org/abs/1507.02672.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" Kingma D P, Mohamed S, Rezende D J, &lt;i&gt;et al&lt;/i&gt;.Semi-supervised learning with deep generative models[C]∥ 27th International Conference on Neural Information Processing Systems, December 8-13, 2014, Montreal, Canda.Cambridge:MIT Press, 2014:3581-3589." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised Learning with Deep Generative Models">
                                        <b>[11]</b>
                                         Kingma D P, Mohamed S, Rezende D J, &lt;i&gt;et al&lt;/i&gt;.Semi-supervised learning with deep generative models[C]∥ 27th International Conference on Neural Information Processing Systems, December 8-13, 2014, Montreal, Canda.Cambridge:MIT Press, 2014:3581-3589.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" Bennett K P, Demiriz A.Semi-supervised support vector machines[C]//1998 Conference on Advances in Neural Information Processing Systems.Cambridge:MIT Press, 1999:368-374." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised support vector machines">
                                        <b>[12]</b>
                                         Bennett K P, Demiriz A.Semi-supervised support vector machines[C]//1998 Conference on Advances in Neural Information Processing Systems.Cambridge:MIT Press, 1999:368-374.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" Grandvalet Y, Bengio Y.Semi-supervised learning by entropy minimization[C]//17th International Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada.Cambridge:MIT Press, 2004:529-536." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning by entropyminimization">
                                        <b>[13]</b>
                                         Grandvalet Y, Bengio Y.Semi-supervised learning by entropy minimization[C]//17th International Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada.Cambridge:MIT Press, 2004:529-536.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" Zhu X J.Semi-supervised learning with graphs[D].Pittsburgh:Carnegie Mellon University, 2005:5-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning with graphs">
                                        <b>[14]</b>
                                         Zhu X J.Semi-supervised learning with graphs[D].Pittsburgh:Carnegie Mellon University, 2005:5-7.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" Wang K F, Gou C, Duan Y J, &lt;i&gt;et al&lt;/i&gt;.Generative adversarial networks:the state of the art and beyond[J].Acta Automatica Sinica, 2017, 43 (3) :321-332.王坤峰, 苟超, 段艳杰, 等.生成式对抗网络GAN的研究进展与展望[J].自动化学报, 2017, 43 (3) :321-332." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201703001&amp;v=Mjk4NjllVnVGeW5rVUxyQktDTGZZYkc0SDliTXJJOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Wang K F, Gou C, Duan Y J, &lt;i&gt;et al&lt;/i&gt;.Generative adversarial networks:the state of the art and beyond[J].Acta Automatica Sinica, 2017, 43 (3) :321-332.王坤峰, 苟超, 段艳杰, 等.生成式对抗网络GAN的研究进展与展望[J].自动化学报, 2017, 43 (3) :321-332.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" Goodfellow I J, Pouget-Abadie J, Mirza M, &lt;i&gt;et al&lt;/i&gt;.Generative adversarial net[C]∥27th International Conference on Neural Information Processing Systems, December 8-13, 2014, Montreal, Canda.Cambridge:MIT Press, 2014, 2:2672-2680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial net">
                                        <b>[16]</b>
                                         Goodfellow I J, Pouget-Abadie J, Mirza M, &lt;i&gt;et al&lt;/i&gt;.Generative adversarial net[C]∥27th International Conference on Neural Information Processing Systems, December 8-13, 2014, Montreal, Canda.Cambridge:MIT Press, 2014, 2:2672-2680.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_17" title=" Zhang Q B, Zhang X H, Han H W.Backscattered light repairing method for underwater laser image based on improved generative adversarial network[J].Laser &amp;amp; Optoelectronics Progress, 2019, 56 (4) :041004.张清博, 张晓晖, 韩宏伟.基于改进生成对抗网络的水下激光图像后向散射光修复方法[J].激光与光电子学进展, 2019, 56 (4) :041004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201904011&amp;v=MjAxNDJDVVJMT2VaZVZ1Rnlua1VMckJMeXJQWkxHNEg5ak1xNDlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         Zhang Q B, Zhang X H, Han H W.Backscattered light repairing method for underwater laser image based on improved generative adversarial network[J].Laser &amp;amp; Optoelectronics Progress, 2019, 56 (4) :041004.张清博, 张晓晖, 韩宏伟.基于改进生成对抗网络的水下激光图像后向散射光修复方法[J].激光与光电子学进展, 2019, 56 (4) :041004.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_18" title=" Tang X L, Du Y M, Liu Y W, &lt;i&gt;et al&lt;/i&gt;.Image recognition with conditional deep convolutional generative adversarial networks[J].Acta Automatica Sinica, 2018, 44 (5) :855-864.唐贤伦, 杜一铭, 刘雨微, 等.基于条件深度卷积生成对抗网络的图像识别方法[J].自动化学报, 2018, 44 (5) :855-864." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805009&amp;v=MjM4NDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlua1VMckJLQ0xmWWJHNEg5bk1xbzlGYllRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Tang X L, Du Y M, Liu Y W, &lt;i&gt;et al&lt;/i&gt;.Image recognition with conditional deep convolutional generative adversarial networks[J].Acta Automatica Sinica, 2018, 44 (5) :855-864.唐贤伦, 杜一铭, 刘雨微, 等.基于条件深度卷积生成对抗网络的图像识别方法[J].自动化学报, 2018, 44 (5) :855-864.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_19" title=" Sheng K, Liu Z, Zhou D C, &lt;i&gt;et al&lt;/i&gt;.A multi-class semi-supervised classification algorithm based on evidence theory[J].Acta Electronica Sinica, 2018, 46 (11) :2642-2649.盛凯, 刘忠, 周德超, 等.一种基于证据理论的多类半监督分类算法[J].电子学报, 2018, 46 (11) :2642-2649." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201811011&amp;v=MDE0MTZxQnRHRnJDVVJMT2VaZVZ1Rnlua1VMckJJVGZUZTdHNEg5bk5ybzlFWllRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         Sheng K, Liu Z, Zhou D C, &lt;i&gt;et al&lt;/i&gt;.A multi-class semi-supervised classification algorithm based on evidence theory[J].Acta Electronica Sinica, 2018, 46 (11) :2642-2649.盛凯, 刘忠, 周德超, 等.一种基于证据理论的多类半监督分类算法[J].电子学报, 2018, 46 (11) :2642-2649.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_20" title=" Radford A, Metz L, Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[J/OL]. (2015-11-27) [2019-01-25].https://arxiv.org/abs/1511.06434." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks">
                                        <b>[20]</b>
                                         Radford A, Metz L, Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[J/OL]. (2015-11-27) [2019-01-25].https://arxiv.org/abs/1511.06434.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_21" title=" Xu B, Wang N, Chen T, &lt;i&gt;et al&lt;/i&gt;.Empirical evaluation of rectified activations in convolutional network[J/OL]. (2015-11-27) [2019-01-25].https://arxiv.org/abs/1505.00853." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Empirical evaluation of rectified activations in convolutional network">
                                        <b>[21]</b>
                                         Xu B, Wang N, Chen T, &lt;i&gt;et al&lt;/i&gt;.Empirical evaluation of rectified activations in convolutional network[J/OL]. (2015-11-27) [2019-01-25].https://arxiv.org/abs/1505.00853.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_22" title=" Xie J R, Li F M, Wei H, &lt;i&gt;et al&lt;/i&gt;.Infrared target simulation method based on generative adversarial neural networks[J].Acta Optica Sinica, 2019, 39 (3) :0311002.谢江荣, 李范鸣, 卫红, 等.基于生成对抗式神经网络的红外目标仿真方法[J].光学学报, 2019, 39 (3) :0311002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201903017&amp;v=MjE4ODFyQ1VSTE9lWmVWdUZ5bmtVTHJCSWpYVGJMRzRIOWpNckk5RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         Xie J R, Li F M, Wei H, &lt;i&gt;et al&lt;/i&gt;.Infrared target simulation method based on generative adversarial neural networks[J].Acta Optica Sinica, 2019, 39 (3) :0311002.谢江荣, 李范鸣, 卫红, 等.基于生成对抗式神经网络的红外目标仿真方法[J].光学学报, 2019, 39 (3) :0311002.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_23" title=" Wang X S, Peng Y F, Lu L, &lt;i&gt;et al&lt;/i&gt;.Chest X-ray 8:hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI.New York:IEEE, 2017:3462-3471." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ChestX-Ray8:Hospital-Scale Chest X-Ray Database and Benchmarks On Weakly-Supervised Classification and Localization of Common Thorax Diseases">
                                        <b>[23]</b>
                                         Wang X S, Peng Y F, Lu L, &lt;i&gt;et al&lt;/i&gt;.Chest X-ray 8:hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI.New York:IEEE, 2017:3462-3471.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_24" title=" Salimans T, Goodfellow I, Zaremba W, &lt;i&gt;et al&lt;/i&gt;.Improved techniques for training GANs[J/OL]. (2016-06-10) [2019-01-25].https://arxiv.org/abs/1606.03498." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved Techniques for Training Gans">
                                        <b>[24]</b>
                                         Salimans T, Goodfellow I, Zaremba W, &lt;i&gt;et al&lt;/i&gt;.Improved techniques for training GANs[J/OL]. (2016-06-10) [2019-01-25].https://arxiv.org/abs/1606.03498.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_25" >
                                        <b>[25]</b>
                                     Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J/OL]. (2015-04-10) [2019-01-25].https://arxiv.org/abs/1409.1556.</a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_26" title=" Li Y Q, Guan C T, Li H Q, &lt;i&gt;et al&lt;/i&gt;.A self-training semi-supervised SVM algorithm and its application in an EEG-based brain computer interface speller system[J].Pattern Recognition Letters, 2008, 29 (9) :1285-1294." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300414315&amp;v=MzE5OThkR2VycVFUTW53WmVadUh5am1VYi9JSVY4WGJ4cz1OaWZPZmJLN0h0RE9ySTlGWU9vTEQzMDhvQk1UNlQ0UFFIL2lyUg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         Li Y Q, Guan C T, Li H Q, &lt;i&gt;et al&lt;/i&gt;.A self-training semi-supervised SVM algorithm and its application in an EEG-based brain computer interface speller system[J].Pattern Recognition Letters, 2008, 29 (9) :1285-1294.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_27" title=" Kingma D P, Welling M.Auto-encoding variational Bayes[J/OL]. (2014-05-01) [2019-01-25].https://arxiv.org/abs/1312.6114." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Auto-encoding variational Bayes">
                                        <b>[27]</b>
                                         Kingma D P, Welling M.Auto-encoding variational Bayes[J/OL]. (2014-05-01) [2019-01-25].https://arxiv.org/abs/1312.6114.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-25 11:38</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(08),117-125 DOI:10.3788/AOS201939.0810003            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于半监督生成对抗网络X光图像分类算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%9D%A4&amp;code=24532408&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘坤</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%85%B8&amp;code=39917041&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王典</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%8D%A3%E6%A2%A6%E5%AD%A6&amp;code=42580135&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">荣梦学</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E6%B5%B7%E4%BA%8B%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0121742&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海海事大学信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>利用半监督学习体系结构中的生成对抗性网络, 围绕标注数据稀缺性的问题进行研究, 在传统无监督生成对抗网络的基础上用softmax替代最后的输出层, 使其扩展为半监督生成对抗网络。对生成样本定义额外的类别标签, 用于引导训练, 采用半监督训练方式对网络参数进行优化, 并将训练得到的判别网络运用于X光图像分类中。对于胸部X光图像, 结合自动化分类诊断选取了6种肺部疾病的X光前视图进行实验, 结果表明:所提算法提高了利用标注数据的监督学习性能, 与其他半监督分类方法相比具有优越的性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=X%E5%85%89%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">X光图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">半监督学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%87%E6%B3%A8%E6%95%B0%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">标注数据;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *王典 E-mail:wangdian687@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-06</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61271446);</span>
                                <span>航空科学基金 (2013ZC15005);</span>
                    </p>
            </div>
                    <h1><b>X-Ray Image Classification Algorithm Based on Semi-Supervised Generative Adversarial Networks</b></h1>
                    <h2>
                    <span>Liu Kun</span>
                    <span>Wang Dian</span>
                    <span>Rong Mengxue</span>
            </h2>
                    <h2>
                    <span>College of Information Engineering, Shanghai Maritime University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A generative adversarial network (GAN) in the semi-supervised learning architecture was used to address the problem of the scarcity of labeled data in X-ray image classification. Initially, we used a softmax layer to replace the output layer of an unsupervised GAN, extending it to a semi-supervised GAN. In addition, we defined additional labels for the GAN-synthesized samples to guide the training process and optimized the network parameters using a semi-supervised training strategy. Then, the discriminator network obtained by the training was used for X-ray image classification. From tested front-view chest X-ray images of six lung diseases, we find that the proposed method substantially enhances the supervised learning with limited labeled data. Further, the proposed method demonstrates superior classification performance compared with other semi-supervised methods.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=X-ray%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">X-ray image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=generative%20adversarial%20networks&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">generative adversarial networks;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semi-supervised%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semi-supervised learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=labeled%20data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">labeled data;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-06</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="65" name="65" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="66">近年来, 随着大量标注的自然图像数据集的出现, 以及深度学习技术在计算机视觉方面的突破性进展, 图像分类、目标检测和实例分割等自然图像计算机视觉任务的性能得到显著提高<citation id="145" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。因此, 已有学者基于监督学习的计算机视觉方法对早期疾病进行检测和诊断, 其中Ciresan等<citation id="139" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>使用一种特别的深度人造神经网络作为一个像素分类器对生物神经元膜进行分割, 该方法首次成功地将深层神经网络应用于生物医学影像。而后在皮肤癌分类<citation id="140" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、乳腺癌诊断<citation id="141" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、脑肿瘤分割<citation id="142" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、肺炎检测<citation id="143" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等方面的应用中, 深度学习方法也都表现出良好的性能。如文献<citation id="144" type="reference">[<a class="sup">8</a>]</citation>在ChestX-ray14数据库的基础上进行肺炎检测, 其训练的CheXNet深度模型在肺炎检测的敏感度与专业性上都超过了专业医生的平均水平。深度学习在医学影像领域取得的显著性进展, 一方面是由于深度学习在数据分层特征提取和特征学习方面具有的优越性能;另一方面要归功于大规模专业标注数据集的发展。但在实际问题中, 手工标注医学图像需要医学和临床专业知识, 以及受到隐私权、医疗行业标准和医疗信息系统集成不足等方面的限制, 构建标注密集的大型医学图像数据无疑会消耗大量的人力、物力成本。因此, 在图像标注样本较少的情况下提高分类、检测的准确率已成为目前的研究热点。</p>
                </div>
                <div class="p1">
                    <p id="67">半监督学习的主要思想是利用标注数据和未标注数据所提供的信息, 在标注数据较少的情况下, 集合大量未标注的数据来改善学习性能。其中, 标注数据提供数据和标签联合分布信息, 未标注数据提供数据分布信息。如文献<citation id="146" type="reference">[<a class="sup">9</a>]</citation>中提出利用基于自训练半监督学习方法进行目标检测, 这种方式训练的模型可以获得与使用更大标注数据集的传统方式训练模型相当的效果;文献<citation id="147" type="reference">[<a class="sup">10</a>]</citation>中提出了利用自编码网络性质构造的阶梯网络方法, 通过添加横向连接, 在正常的前馈网络中加入噪声编码器和相应的去噪解码器, 用于学习未标注的训练数据。目前, 半监督学习的方法主要包括:生成式模型<citation id="148" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、自训练模型<citation id="149" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation> 、半监督SVM<citation id="150" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation> 、熵正则化模型<citation id="151" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和基于图论模型<citation id="152" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>等。已有的研究结果表明, 半监督学习与仅利用标注数据的监督学习相比, 学习性能得到了提高。</p>
                </div>
                <div class="p1">
                    <p id="68">针对肺部X光图像数据, 本文提出一种基于生成对抗网络 (GAN) <citation id="153" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>的半监督图像分类方法, 利用少量标注数据和未标注数据, 使GAN的判别器网络可以输出数据类别标签, 实现了基于少量标注样本的半监督分类。比较监督学习和半监督阶梯网络的实验性能发现, 半监督生成对抗网络 (SSGAN) 提高了只利用标注数据的监督学习的性能, 且性能表现优于其他半监督网络。该方法可应用于医学诊断、异常检测以及图像识别等领域。</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag">2 SSGAN</h3>
                <h4 class="anchor-tag" id="70" name="70"><b>2.1 GAN</b></h4>
                <div class="p1">
                    <p id="71">GAN是由Goodfellow等<citation id="154" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>于2014年提出的一种基于博弈理论的无监督生成模型, 其流程如图1所示, 该模型主要由两部分构成:生成器G和判别器D。生成器G将符合特定分布<i>P</i><sub><b><i>z</i></b></sub> (如高斯分布、均匀分布等) 的随机噪声<b><i>z</i></b>映射到目标域, 用于学习真实数据的概率分布<i>P</i><sub>data</sub>, 使其生成尽可能服从真实数据分布<i>P</i><sub>data</sub> (<b><i>x</i></b>) 的样本<i>G</i> (<b><i>z</i></b>) <citation id="155" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。判别器D对输入样本到底来自于真实数据<b><i>x</i></b>还是生成数据<i>G</i> (<b><i>z</i></b>) 进行判断, 输出一个属于真实数据的概率值<i>D</i> (·) 。GAN的训练过程本质上是训练判别器D来最大化真实样本和生成样本判别的正确率, 同时训练生成器G, 即最小化lb[1-<i>D</i> (<i>G</i> (<b><i>z</i></b>) ) ]<citation id="156" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。因此, 生成器G和判别器D的训练是一个二元极小极大博弈问题, 目标函数为</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtable columnalign="left"><mtr><mtd><mi>J</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>D</mtext><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mo>-</mo><mi>E</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo></mrow></msub><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>-</mo></mtd></mtr><mtr><mtd><mi>E</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">z</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">z</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo></mrow></msub><mtext>l</mtext><mtext>b</mtext><mo stretchy="false">{</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false">[</mo><mi>G</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></mtd></mtr><mtr><mtd><mi>J</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>G</mtext><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mo>-</mo><mi>E</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">z</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">z</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo></mrow></msub><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mi>D</mi><mo stretchy="false">[</mo><mi>G</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">式中:<b><i>x</i></b>～<i>P</i><sub>data</sub> (<b><i>x</i></b>) 表示数据<b><i>x</i></b>服从<i>P</i><sub>data</sub>分布;<b><i>z</i></b>～<i>P</i><sub><b><i>z</i></b></sub> (<b><i>z</i></b>) 表示数据<b><i>z</i></b>服从随机噪声<i>P</i><sub><b><i>z</i></b></sub>分布。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908015_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 GAN流程图" src="Detail/GetImg?filename=images/GXXB201908015_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 GAN流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908015_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Flow chart of GAN</p>

                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>2.2 SSGAN</b></h4>
                <div class="p1">
                    <p id="76">在GAN的基础上, 提出了一种基于SSGAN的肺部X光影像分类模型。该模型在GAN网络的基础上, 将来自GAN生成器G的生成图像样本添加到数据库图像中。假设对于一个<i>K</i>类分类问题, 用新生成的类<i>y</i>=<i>K</i>+1标注已生成的图像样本, 相应地将输出softmax分类器的维数从<i>K</i>扩展到<i>K</i>+1, 并采用监督损失和无监督的GAN网络损失函数相结合的半监督训练方式, 对大量的未标注样本数据辅助少量标注样本数据进行学习<citation id="157" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, 提高半监督分类的准确率, SSGAN网络结构如图2所示。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908015_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 SSGAN结构" src="Detail/GetImg?filename=images/GXXB201908015_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 SSGAN结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908015_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Framework of SSGAN</p>

                </div>
                <div class="p1">
                    <p id="78">将符合特定分布 (如高斯分布、均匀分布等) 的噪声<b><i>z</i></b>送入到生成器网络G中, 生成尽可能符合真实数据分布的生成图像样本<i>G</i> (<b><i>z</i></b>) ;将生成图像样本<i>G</i> (<b><i>z</i></b>) 和数据库图像样本一起输入到判别器网络D, 其中数据库样本数据中包括少量标注样本数据和大量未标注样本数据, 判别器D由多个卷积层和全连接层构成;最后利用一个softmax输出表征一个不同类别的归一化相对概率。训练初期, 生成器G和判别器D的收敛性都不佳。利用不断的交替迭代训练, 生成器G逐步拟合数据库图像样本的分布, 生成逼真的图像样本, 同时判别器D对输入样本类别的分类判别性能也不断提升。</p>
                </div>
                <div class="p1">
                    <p id="79">为了提取更深层次的特征, 提高模型训练过程的稳定性, 借鉴改进后的深度卷积对抗网络 (DCGAN) 结构<citation id="158" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, 在生成器G和判别器D中对网络结构进行约束和调整, 以提高模型训练过程的稳定性。具体调整如下:1) 判别器网络使用卷积替代池化层, 生成模型使用反卷积替代池化层;2) 在判别网络和生成网络使用批量归一化处理 (BatchNorm) ; 3) 生成网络在输出层使用Tanh激活函数, 其余层使用 ReLU激活函数;4) 对于判别模型, 每一层都使用LeakyReLU<citation id="159" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>激活函数, 最后输出层使用softmax函数。</p>
                </div>
                <div class="p1">
                    <p id="80">生成器G使用5层反卷积网络对随机噪声进行上采样, 生成指定大小的仿真图像。网络参数如图3所示。先将符合特定分布 (如高斯分布、均匀分布等) 的100维随机噪声<b><i>z</i></b>输入到全连接网络, 通过维度变换 (reshape) 使其成为三维张量;将维度变换后的张量输入到卷积核<i>w</i>大小为5×5、步长stride为2的5层反卷积层, 每次完成反卷积之后都进行BatchNorm, 除了最后输出层使用Tanh激活函数以外, 其余层都采用ReLU作为激活函数, 最后输出层输出大小为 (128, 128, 1) 的张量, 即为生成图像样本<citation id="160" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908015_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 SSGAN生成网络结构图" src="Detail/GetImg?filename=images/GXXB201908015_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 SSGAN生成网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908015_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Generation network structure of SSGAN</p>

                </div>
                <div class="p1">
                    <p id="82">判别器D由17个卷积层, 3个全连接层串连组成, 如图4所示。将输入图像样本输入到17层采用卷积核为3×3的卷积层中, 每次完成卷积之后都进行BatchNorm, 激活函数采用LeakReLU函数。不同于传统的ReLU激活函数, LeakReLU在负半轴保留了很小的斜率 (本文实验设置为0.2) , 避免了训练过程中梯度消失。最后经全连接层输出逻辑向量, 通过softmax输出归一化类别概率。</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908015_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 SSGAN判别器D结构图" src="Detail/GetImg?filename=images/GXXB201908015_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 SSGAN判别器D结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908015_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Structure of discriminator D in SSGAN</p>

                </div>
                <h3 id="84" name="84" class="anchor-tag">3 基于GAN的半监督X影像分类</h3>
                <div class="p1">
                    <p id="85">基于上述网络结构模型, 针对肺部X影像分类中标注样本稀缺的问题, 提出了一种基于GAN模型的半监督X光影像分类算法。采用监督损失和无监督损失相结合的训练方式共同调整网络参数, 利用特征匹配来提高GAN的学习能力, 在对抗过程中提高判别图像分类的准确率。当模型收敛时, 网络可对描绘肺部疾病实例的图像进行分类。该方法主要有以下三个步骤。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86"><b>3.1 数据及预处理</b></h4>
                <div class="p1">
                    <p id="87">本研究使用的是Wang等<citation id="161" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>发布的肺部X光影像ChestX-ray14数据集, 该数据库包含30805名患者的112120幅前视X光图像。Wang等根据放射学报告, 运用数据信息挖掘自动提取方法, 使用多达14种 (atelectasis、cardiomegaly、effusion、infiltration、mass、nodule、pneumonia、pneumothorax、consolidation、edema、emphysema、fibrosis、pleural thickening、hernia) 不同的胸腔病理标签标注每张图片。</p>
                </div>
                <div class="p1">
                    <p id="88">首先, 选取了其中36800张肺部X光图像作为实验数据, 其中包括肺部X光片中观察到的6种发病率较高的常见胸部疾病;同时考虑到在自动化分类诊断中的实际情况, 加入了不含肺部疾病的正常类和除此6种高发病率的肺部疾病外的其他类, 共8类样本数据, 如图5所示。其次, 为了保证图像输入大小一致, 将图像统一缩放为128 pixel×128 pixel, 并针对X光图像中出现的灰雾现象采用直方图均衡化进行处理, 结果如图6所示。最后, 将选取的数据按照7∶1∶2划分为训练集、验证集和测试集。其中训练数据分为标注样本数据和未标注样本数据, 并且标注样本数据量远远小于未标注样本数据量。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908015_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 训练样本图像" src="Detail/GetImg?filename=images/GXXB201908015_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 训练样本图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908015_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Training sample images</p>

                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908015_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 直方图均衡化图像对比图。" src="Detail/GetImg?filename=images/GXXB201908015_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 直方图均衡化图像对比图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908015_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Comparison of histogram equalization images. </p>
                                <p class="img_note"> (a) (b) (c) 原始X光图像; (d) (e) (f) 直方图均衡化X光图像</p>
                                <p class="img_note"> (a) (b) (c) Original X-ray images; (d) (e) (f) histogram equalization X-ray images</p>

                </div>
                <h4 class="anchor-tag" id="91" name="91"><b>3.2 SSGAN模型训练</b></h4>
                <div class="p1">
                    <p id="92">在SSGAN中, 对于<i>K</i>类的数据问题, 将生成器G的生成图像样本添加到数据库图像中, 并标注为第<i>K</i>+1类, 判别器接收样本<b><i>x</i></b>作为输入时, 输出一个<i>K</i>+1维的逻辑向量{<i>l</i><sub>1</sub>, <i>l</i><sub>2</sub>, …, <i>l</i><sub><i>K</i>+1</sub>}, 再由softmax函数得归一化类概率:</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>y</mi><mo>=</mo><mi>i</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Κ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94"> (3) 式中前<i>K</i>维对应原有<i>K</i>个类, 最后一维对应“伪样本”类, 即生成图像样本类。<i>P</i><sub>model</sub>的最大值位置对应预测的类别标签。</p>
                </div>
                <div class="p1">
                    <p id="95">网络的训练过程实质上是对损失函数的优化过程。在判别器D中, 每种类型的输入数据 (标注数据、未标注数据、生成数据) 分别对应每种类型的损失, 即标注样本损失<i>L</i><sub>label</sub>、未标注样本损失<i>L</i><sub>unlabel</sub>、生成样本损失<i>L</i><sub>gen</sub>。</p>
                </div>
                <div class="p1">
                    <p id="96">标注样本损失, 即真实类标签分布和预测类标签的交叉熵损失, 表达式为</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>a</mtext><mtext>b</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo>=</mo><mo>-</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>y</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">|</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo>&lt;</mo><mi>Κ</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">未标注样本损失, 即将未标注样本判断为真实样本的损失, 此时<i>y</i>≠<i>K</i>+1, 表达式为</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>u</mtext><mtext>n</mtext><mtext>l</mtext><mtext>a</mtext><mtext>b</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo>=</mo><mo>-</mo><mi>E</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub></mrow></msub><mo stretchy="false">{</mo><mtext>l</mtext><mtext>b</mtext><mo stretchy="false">[</mo><mn>1</mn><mo>-</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>y</mi><mo>=</mo><mi>Κ</mi><mo>+</mo><mn>1</mn><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">生成样本损失, 即将生成样本判断为假样本的损失, 此时<i>y</i>=<i>K</i>+1, 表达式为</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>e</mtext><mtext>n</mtext></mrow></msub><mo>=</mo><mo>-</mo><mi>E</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><mo>∼</mo><mi>G</mi></mrow></msub><mo stretchy="false">[</mo><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>y</mi><mo>=</mo><mi>Κ</mi><mo>+</mo><mn>1</mn><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">式中:<b><i>x</i></b>为采样样本图像;<i>y</i>为标注样本数据对应的标签, 即<i>y</i>∈{1, 2, …, <i>K</i>, <i>K</i>+1};<b><i>x</i></b>, <i>y</i>～<i>P</i><sub>data</sub>表示输入图像<b><i>x</i></b>带有标签<i>y</i>, 并且取自真实数据分布;<b><i>x</i></b>～<i>P</i><sub>data</sub>表示<b><i>x</i></b>是未标注样本, 且取自真实数据分布;<b><i>x</i></b>～<i>G</i>表示<b><i>x</i></b>取自生成样本;<mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false"> (</mo><mo>⋅</mo><mo stretchy="false">|</mo><mo>⋅</mo><mo stretchy="false">) </mo></mrow></math></mathml>为预测类的概率。</p>
                </div>
                <div class="p1">
                    <p id="104">在判别器D网络损失中, 对于标注样本损失<i>L</i><sub>label</sub>, 其训练过程相当于一个标准<i>K</i>类分类的监督学习过程, 通过最小化标注样本数据标签和模型预测分布<mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo></mrow></math></mathml>之间的交叉熵损失来训练并优化网络参数。对于未标注样本损失L<sub><i>unlabel</i></sub>, 其中<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>1</mn><mo>-</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>y</mi><mo>=</mo><mi>Κ</mi><mo>+</mo><mn>1</mn><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo></mrow></math></mathml>为预测真实样本的概率, 训练过程中尽可能最大化未标注样本为真实样本的概率;对于生成样本损失L<sub><i>gen</i></sub>, 其中<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>y</mi><mo>=</mo><mi>Κ</mi><mo>+</mo><mrow><mn>1</mn><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>, </mo></mrow></mrow></math></mathml>表示<b><i>x</i></b>预测为生成数据的概率, 训练过程中尽可能最大化被判别为生成样本的概率。</p>
                </div>
                <div class="p1">
                    <p id="108">判别器D网络的训练过程实质上是对总的损失函数的优化过程。其中, 对标注样本数据进行监督学习, 而对未标注样本数据和生成样本进行类似GAN的无监督学习。因此, 总的判别器损失函数<i>L</i><sub>D</sub>可以分解成标准监督损失函数<i>L</i><sub>supervised</sub>和无监督损失函数<i>L</i><sub>unsupervised</sub>, 即</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mtext>D</mtext></msub><mo>=</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>a</mtext><mtext>b</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo>+</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>u</mtext><mtext>n</mtext><mtext>l</mtext><mtext>a</mtext><mtext>b</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo>+</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>e</mtext><mtext>n</mtext></mrow></msub><mo>=</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>u</mtext><mtext>p</mtext><mtext>e</mtext><mtext>r</mtext><mtext>v</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub><mo>+</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>u</mtext><mtext>n</mtext><mtext>s</mtext><mtext>u</mtext><mtext>p</mtext><mtext>e</mtext><mtext>r</mtext><mtext>v</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">式中:</p>
                </div>
                <div class="p1">
                    <p id="111" class="code-formula">
                        <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>u</mtext><mtext>p</mtext><mtext>e</mtext><mtext>r</mtext><mtext>v</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub><mo>=</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>a</mtext><mtext>b</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>L</mi><msub><mrow></mrow><mrow><mtext>u</mtext><mtext>n</mtext><mtext>s</mtext><mtext>u</mtext><mtext>p</mtext><mtext>e</mtext><mtext>r</mtext><mtext>v</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub><mo>=</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>u</mtext><mtext>n</mtext><mtext>l</mtext><mtext>a</mtext><mtext>b</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo>+</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>e</mtext><mtext>n</mtext></mrow></msub></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="112">在最小化判别器损失函数以获得更优性能判别器的同时, 需要训练生成器G来近似拟合真实数据的分布。这里并没有采用原始GAN中最大化生成样本来自真实数据的概率<i>D</i>[<i>G</i> (<b><i>z</i></b>) ], 而是采用要求生成器生成与真实数据的统计分布相匹配的特征匹配<citation id="162" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>方法定义生成器损失函数<i>L</i><sub>G</sub>, 其定义为</p>
                </div>
                <div class="area_img" id="113">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201908015_11300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="114">式中:<i>f</i> (·) 表示鉴别器中间层的特征值;<mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mo>⋅</mo><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub></mrow></math></mathml>表示2-范数。在训练过程中, 将特征匹配结果作为生成器网络G的损失函数, 通过最小化损失函数来匹配生成样本和真实样本, 实现生成器G最大化拟合真实数据的分布。</p>
                </div>
                <div class="p1">
                    <p id="116">SSGAN模型采用监督损失和无监督损失相结合的训练方式共同调整判别器网络参数, 能够进一步提高模型的学习能力。具体地, 在半监督训练时, 判别器D和生成器G交替迭代训练, 固定G和D的一方, 更新另一方的权重参数。训练判别网络时, 含有标签真实样本, 通过最小化标注样本数据标签与模型预测概率分布的交叉熵损失来更新网络模型参数, 而利用GAN 对抗训练原理来调整无标签的真实样本和生成样本的网络模型参数。在训练生成网络时, 采用特征匹配使生成样本尽可能拟合真实数据分布。通过上述联合训练方式实现具有半监督分类功能的SSGAN网络。</p>
                </div>
                <div class="p1">
                    <p id="117">提出的SSGAN模型具体训练步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="118">1) 在高斯分布中随机采样生成随机向量<b><i>z</i></b>, 并将随机向量输入至生成器网络G, 得到仿真图像<i>G</i> (<b><i>z</i></b>) ;</p>
                </div>
                <div class="p1">
                    <p id="119">2) 将标注和未标注的真实图像<b><i>x</i></b>以及仿真图像<i>G</i> (<b><i>z</i></b>) 按批次输入到判别器网络D, 通过softmax输出归一化的概率值<i>D</i> (<b><i>x</i></b>) 和<i>D</i>[<i>G</i> (<b><i>z</i></b>) ];</p>
                </div>
                <div class="p1">
                    <p id="120">3) 固定生成器网络G的参数, 若真实图像<b><i>x</i></b>未被标注, 则将<i>L</i><sub>unlabel</sub>作为损失函数;若真实图像<b><i>x</i></b>已被标注, 则将<i>L</i><sub>label</sub>作为损失函数;若输入图像为仿真图像, 将<i>L</i><sub>gen</sub>作为损失函数。利用Adam梯度下降法来调整判别器网络D的参数;</p>
                </div>
                <div class="p1">
                    <p id="121">4) 固定判别器网络D的参数, 选取全连接层输出作为中间层特征, 对真实图像<b><i>x</i></b>和仿真图像<i>G</i> (<b><i>z</i></b>) 进行特征匹配操作, 并利用特征匹配调整生成器网络G的参数;</p>
                </div>
                <div class="p1">
                    <p id="122">5) 重复1) ～4) 步操作, 直至达到迭代次数;</p>
                </div>
                <div class="p1">
                    <p id="123">6) 将测试样本图像输入到判别器D中, 输出图像类别。</p>
                </div>
                <h3 id="124" name="124" class="anchor-tag">4 实验结果与分析</h3>
                <div class="p1">
                    <p id="125">实验在内存为16 GB的GPU (NVIDIA, GeForce GTX 1080Ti) 工作站上进行基于SSGAN的肺部X光影像分类计算, 实现少量标注样本和未标注样本的肺部X光图像的有效分类。实验采用Adam算法优化损失函数, 网络学习率 (learning rate) 设置为0.0003, 动量 (momentum) 设置为0.5, 每个批次 (batch) 有64个样本, 迭代次数与损失函数的变化关系如图7所示。图7 (a) 中判别网络损失函数在训练前期振荡现象较为明显, 后期较为平滑, 总体呈现下降趋势。图7 (b) 中生成器网络损失函数先是迅速下降, 后又缓慢上升。以上现象说明在两个网络对抗过程中, 判别器网络以微弱的优势胜过生成器网络。</p>
                </div>
                <div class="area_img" id="126">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908015_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 损失函数变化趋势。" src="Detail/GetImg?filename=images/GXXB201908015_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 损失函数变化趋势。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908015_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Variaiton of loss function. </p>
                                <p class="img_note"> (a) 判别器网络损失; (b) 生成器网络损失</p>
                                <p class="img_note"> (a) Loss of discriminator network; (b) loss of generator network</p>

                </div>
                <div class="p1">
                    <p id="127">为了测试SSGAN在标注数据稀缺时图像分类任务的性能, 在一个相对较小的图像数据集上对SSGAN进行训练, 同时算法与监督学习的卷积神经网络 (CNN) <citation id="163" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、半监督阶梯网络 (Ladder Network) <citation id="164" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>以及基于PCA和SVM的半监督自训练<citation id="165" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>进行实验对比。定义了8种不同大小的标注数据集, 每类标注样本的数量分别为25, 50, 100, 250, 500, 1000, 1500, 2000。表1为在不同数量标注样本情况下4种算法的分类平均准确率。图8为不同数量标注样本的平均分类准确率的趋势图。</p>
                </div>
                <div class="p1">
                    <p id="128">从上述数据结果中可以看出, SSGAN仅需要较少的标注数据就可以达到和CNN相同的分类性能。如:SSGAN每类只需要50个标注样本就可以达到 (62.47±4.7) %的平均准确率, 而CNN则需要100～250个标注样本才可以达到相当的分类准确率。在标注数据较少的情况下, SSGAN对分类性能的提升较为明显, 但随着标注样本数量的增加, 分类性能的提升作用逐渐减小, 但总体优于CNN。而对比另外两种半监督模型, 即半监督梯形网络和PCA+SVM自训练模型, SSGAN都表现出优越的性能。</p>
                </div>
                <div class="area_img" id="129">
                    <p class="img_tit">表1 不同数量标注样本的平均分类准确率 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Average classification accuracies of samples with different number of labeled data</p>
                    <p class="img_note"></p>
                    <table id="129" border="1"><tr><td rowspan="2"><br />Labeled <br />data</td><td colspan="4"><br />Average accuracy /%</td></tr><tr><td><br />PCA+SVM</td><td>Ladder</td><td>CNN</td><td>SSGAN</td></tr><tr><td>25</td><td>52.63±4.6</td><td>53.12±6.4</td><td>48.47±5.2</td><td>58.69±7.1</td></tr><tr><td><br />50</td><td>58.94±6.3</td><td>57.60±4.1</td><td>55.62±6.2</td><td>62.47±4.7</td></tr><tr><td><br />100</td><td>63.20±4.1</td><td>64.27±3.7</td><td>61.64±5.5</td><td>68.71±4.3</td></tr><tr><td><br />250</td><td>68.84±3.4</td><td>70.79±4.4</td><td>71.75±5.1</td><td>75.94±3.5</td></tr><tr><td><br />500</td><td>70.98±5.9</td><td>74.17±3.2</td><td>76.10±3.4</td><td>78.87±4.8</td></tr><tr><td><br />1000</td><td>73.41±6.6</td><td>76.39±3.8</td><td>81.47±5.6</td><td>83.20±4.6</td></tr><tr><td><br />1500</td><td>75.24±4.2</td><td>77.83±2.5</td><td>82.92±4.3</td><td>84.57±4.1</td></tr><tr><td><br />2000</td><td>76.07±3.7</td><td>78.32±3.4</td><td>84.15±2.9</td><td>85.10±2.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908015_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 不同数量标注样本分类结果趋势图" src="Detail/GetImg?filename=images/GXXB201908015_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 不同数量标注样本分类结果趋势图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908015_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Trend of classification results for samples with different number of labeled data</p>

                </div>
                <div class="p1">
                    <p id="131">同时针对6种高发类肺部疾病, 将文献<citation id="166" type="reference">[<a class="sup">23</a>]</citation>中的基准分类方法 (Benchmark) 与3种半监督分类方法进行对比, 3种半监督分类方法为自训练PCA+SVM、半监督梯形网络和半监督生成对抗网络。每类采用1000个标注数据进行训练, 实验数据如表2所示。从表2可以看到, SSGAN优于基准分类方法、自训练PCA+SVM和半监督阶梯网络, 并且总体要优于分类性能, 优于或接近基准。</p>
                </div>
                <div class="p1">
                    <p id="132">在实验中, 生成器通过匹配真实数据分布, 生成的图像如图9 (a) 所示, 与图9 (b) 所示的原数据库图像进行对比, 发现生成样本轮廓清晰、结构合理。通过引入inception得分值<citation id="167" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>来评价生成图像质量, inception得分值是一种依据生成图像的多样性和质量评价其模型表现的一项重要指标。表3所示为生成样本的inception分值, 并与变分自编码器 (VAE) <citation id="168" type="reference"><link href="63" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>和DCGAN两种模型进行比对。生成图像与原数据库图像的inception得分值最为接近, 说明网络训练过程中同时实现了对数据的拟合。</p>
                </div>
                <div class="area_img" id="133">
                    <p class="img_tit">表2 1000个标注样本分类准确率 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Classification accuracy of 1000 labeled samples</p>
                    <p class="img_note"></p>
                    <table id="133" border="1"><tr><td rowspan="2"><br />Label</td><td colspan="4"><br />Accuracy /%</td></tr><tr><td><br />Benchmark</td><td>PCA+SVM</td><td>Ladder</td><td>SSGAN</td></tr><tr><td>Atelectasis</td><td>71.62</td><td>67.96</td><td>71.21</td><td>81.97</td></tr><tr><td><br />Effusion</td><td>78.47</td><td>73.61</td><td>76.94</td><td>82.75</td></tr><tr><td><br />Infiltration</td><td>60.95</td><td>65.95</td><td>61.73</td><td>71.62</td></tr><tr><td><br />Mass</td><td>70.64</td><td>63.88</td><td>61.18</td><td>85.32</td></tr><tr><td><br />Nodule</td><td>67.13</td><td>61.26</td><td>63.63</td><td>77.38</td></tr><tr><td><br />Pneumothorax</td><td>80.65</td><td>67.53</td><td>67.16</td><td>83.41</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="135">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908015_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 SSGAN生成图像与原始图像。" src="Detail/GetImg?filename=images/GXXB201908015_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 SSGAN生成图像与原始图像。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908015_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Generated images of SSGAN and original images. </p>
                                <p class="img_note"> (a) 生成图像; (b) 原始图像</p>
                                <p class="img_note"> (a) Generated images; (b) original images</p>

                </div>
                <div class="area_img" id="136">
                    <p class="img_tit">表3 各种模型生成样本的inception分值 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Inception scores for samples generated by various models</p>
                    <p class="img_note"></p>
                    <table id="136" border="1"><tr><td><br />Real data</td><td>SSGAN</td><td>VAE</td><td>DCGAN</td></tr><tr><td><br />4.54±0.15</td><td>3.48±0.22</td><td>2.87±0.12</td><td>3.15±0.17</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="137" name="137" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="138">为了解决医学图像分类中标注数据不足的问题, 提出了基于SSGAN的图像分类模型, 通过对半监督损失函数的定义和联合训练, 利用模型判别器对肺部X光图像进行分类。在ChestX-ray数据集下, 针对标注数据稀缺性问题研究不同数量标注数据的分类识别率, 结果表明, 相比于监督学习的CNN, 在标注数据数量受限情况下, SSGAN能显著提高分类准确率。对比其他半监督分类模型, 所提方法表现出更好的性能。下一步将针对生成样本多样性对判别器分类结果的影响做进一步研究。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">

                                <b>[1]</b> Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]∥25th International Conference on Neural Information Processing Systems, December 3-6, 2012, Lake Tahoe, Nevada.USA:Curran Associates Inc., 2012, 1:1097-1105.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">

                                <b>[2]</b> Ren S Q, He K M, Girshick R, <i>et al</i>.Faster R-CNN:towards real-time object detection with region proposal networks[C]//28th International Conference on Neural Information Processing Systems, December 7-12, 2015, Montreal, Canada.Cambridge:MIT Press, 2015, 1:91-99.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[3]</b> Long J, Shelhamer E, Darrell T.Fully convolutional networks for semantic segmentation[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:3431-3440.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep neural networks segment neuronal membranes in electron microscopy images">

                                <b>[4]</b> Ciresan D, Giusti A, Gambardella L M, <i>et al</i>.Deep neural networks segment neuronal membranes in electron microscopy images[C]∥25th International Conference on Neural Information Processing Systems, December 3-6, 2012, Lake Tahoe, Nevada.USA:Curran Associates Inc., 2012, 2:2843-2851.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dermatologist-level classification of skin cancer with deep neural networks">

                                <b>[5]</b> Esteva A, Kuprel B, Novoa R A, <i>et al</i>.Dermatologist-level classification of skin cancer with deep neural networks[J].Nature, 2017, 542 (7639) :115-118.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automated breast cancer diagnosis using deep learning and region of interest detection (BC-DROID)">

                                <b>[6]</b> Platania R, Shams S, Yang S, <i>et al</i>.Automated breast cancer diagnosis using deep learning and region of interest detection (BC-DROID) [C]∥8th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatic, August 20-23, 2017, Boston.New York:ACM, 2017:536-543.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint Sequence Learning and CrossModality Convolution for 3D Biomedical Segmentation">

                                <b>[7]</b> Tseng K L, Lin Y L, Hsu W, <i>et al</i>.Joint sequence learning and cross-modality convolution for 3D biomedical segmentation[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI.New York:IEEE, 2017:3739-3746.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Chexnet:radiologist-level pneumonia detection on chest X-rays with deep learning">

                                <b>[8]</b> Rajpurkar P, Irvin J, Zhu K, <i>et al</i>.Chexnet:radiologist-level pneumonia detection on chest X-rays with deep learning[J/OL]. (2017-12-25) [2019-01-25].https://arxiv.org/abs/1711.05225.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised self-training of object detection models">

                                <b>[9]</b> Rosenberg C, Hebert M, Schneiderman H.Semi-supervised self-training of object detection models[C]//2005 Seventh IEEE Workshops on Applications of Computer Vision, January 5-7, 2005, Breckenridge, CO.New York:IEEE, 2005:29-36.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning with ladder networks">

                                <b>[10]</b> Rasmus A, Berglund M, Honkala M, <i>et al</i>.Semi-supervised learning with ladder networks[J/OL]. (2015-11-24) [2019-01-25].https://arxiv.org/abs/1507.02672.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised Learning with Deep Generative Models">

                                <b>[11]</b> Kingma D P, Mohamed S, Rezende D J, <i>et al</i>.Semi-supervised learning with deep generative models[C]∥ 27th International Conference on Neural Information Processing Systems, December 8-13, 2014, Montreal, Canda.Cambridge:MIT Press, 2014:3581-3589.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised support vector machines">

                                <b>[12]</b> Bennett K P, Demiriz A.Semi-supervised support vector machines[C]//1998 Conference on Advances in Neural Information Processing Systems.Cambridge:MIT Press, 1999:368-374.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning by entropyminimization">

                                <b>[13]</b> Grandvalet Y, Bengio Y.Semi-supervised learning by entropy minimization[C]//17th International Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada.Cambridge:MIT Press, 2004:529-536.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning with graphs">

                                <b>[14]</b> Zhu X J.Semi-supervised learning with graphs[D].Pittsburgh:Carnegie Mellon University, 2005:5-7.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201703001&amp;v=MTAzMjJMZlliRzRIOWJNckk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmtVTHJCS0M=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Wang K F, Gou C, Duan Y J, <i>et al</i>.Generative adversarial networks:the state of the art and beyond[J].Acta Automatica Sinica, 2017, 43 (3) :321-332.王坤峰, 苟超, 段艳杰, 等.生成式对抗网络GAN的研究进展与展望[J].自动化学报, 2017, 43 (3) :321-332.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial net">

                                <b>[16]</b> Goodfellow I J, Pouget-Abadie J, Mirza M, <i>et al</i>.Generative adversarial net[C]∥27th International Conference on Neural Information Processing Systems, December 8-13, 2014, Montreal, Canda.Cambridge:MIT Press, 2014, 2:2672-2680.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201904011&amp;v=MTg2NTF1Rnlua1VMckJMeXJQWkxHNEg5ak1xNDlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> Zhang Q B, Zhang X H, Han H W.Backscattered light repairing method for underwater laser image based on improved generative adversarial network[J].Laser &amp; Optoelectronics Progress, 2019, 56 (4) :041004.张清博, 张晓晖, 韩宏伟.基于改进生成对抗网络的水下激光图像后向散射光修复方法[J].激光与光电子学进展, 2019, 56 (4) :041004.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805009&amp;v=MTAwMjFDTGZZYkc0SDluTXFvOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5rVUxyQks=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Tang X L, Du Y M, Liu Y W, <i>et al</i>.Image recognition with conditional deep convolutional generative adversarial networks[J].Acta Automatica Sinica, 2018, 44 (5) :855-864.唐贤伦, 杜一铭, 刘雨微, 等.基于条件深度卷积生成对抗网络的图像识别方法[J].自动化学报, 2018, 44 (5) :855-864.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201811011&amp;v=MDU2OTlCSVRmVGU3RzRIOW5Ocm85RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmtVTHI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> Sheng K, Liu Z, Zhou D C, <i>et al</i>.A multi-class semi-supervised classification algorithm based on evidence theory[J].Acta Electronica Sinica, 2018, 46 (11) :2642-2649.盛凯, 刘忠, 周德超, 等.一种基于证据理论的多类半监督分类算法[J].电子学报, 2018, 46 (11) :2642-2649.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks">

                                <b>[20]</b> Radford A, Metz L, Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[J/OL]. (2015-11-27) [2019-01-25].https://arxiv.org/abs/1511.06434.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Empirical evaluation of rectified activations in convolutional network">

                                <b>[21]</b> Xu B, Wang N, Chen T, <i>et al</i>.Empirical evaluation of rectified activations in convolutional network[J/OL]. (2015-11-27) [2019-01-25].https://arxiv.org/abs/1505.00853.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201903017&amp;v=MDM1NTc0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmtVTHJCSWpYVGJMRzRIOWpNckk5RVk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> Xie J R, Li F M, Wei H, <i>et al</i>.Infrared target simulation method based on generative adversarial neural networks[J].Acta Optica Sinica, 2019, 39 (3) :0311002.谢江荣, 李范鸣, 卫红, 等.基于生成对抗式神经网络的红外目标仿真方法[J].光学学报, 2019, 39 (3) :0311002.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ChestX-Ray8:Hospital-Scale Chest X-Ray Database and Benchmarks On Weakly-Supervised Classification and Localization of Common Thorax Diseases">

                                <b>[23]</b> Wang X S, Peng Y F, Lu L, <i>et al</i>.Chest X-ray 8:hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI.New York:IEEE, 2017:3462-3471.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved Techniques for Training Gans">

                                <b>[24]</b> Salimans T, Goodfellow I, Zaremba W, <i>et al</i>.Improved techniques for training GANs[J/OL]. (2016-06-10) [2019-01-25].https://arxiv.org/abs/1606.03498.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_25" >
                                    <b>[25]</b>
                                 Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J/OL]. (2015-04-10) [2019-01-25].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300414315&amp;v=Mjg0NzZIeWptVWIvSUlWOFhieHM9TmlmT2ZiSzdIdERPckk5RllPb0xEMzA4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> Li Y Q, Guan C T, Li H Q, <i>et al</i>.A self-training semi-supervised SVM algorithm and its application in an EEG-based brain computer interface speller system[J].Pattern Recognition Letters, 2008, 29 (9) :1285-1294.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Auto-encoding variational Bayes">

                                <b>[27]</b> Kingma D P, Welling M.Auto-encoding variational Bayes[J/OL]. (2014-05-01) [2019-01-25].https://arxiv.org/abs/1312.6114.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201908015" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201908015&amp;v=Mjg0MzN5bmtVTHJPSWpYVGJMRzRIOWpNcDQ5RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

