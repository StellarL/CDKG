

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133882357627500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201906011%26RESULT%3d1%26SIGN%3dIAGjwCnd%252fCYvGAJLODzuJ%252bvsT58%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201906011&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201906011&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201906011&amp;v=MTQ3MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlqbVVidk5JalhUYkxHNEg5ak1xWTlFWlk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#59" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="2 基于多尺度递归网络的SR算法 ">2 基于多尺度递归网络的SR算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="&lt;b&gt;2.1 网络结构概述&lt;/b&gt;"><b>2.1 网络结构概述</b></a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;2.2 多尺度特征映射单元&lt;/b&gt;"><b>2.2 多尺度特征映射单元</b></a></li>
                                                <li><a href="#83" data-title="&lt;b&gt;2.3 亚像素卷积重构层&lt;/b&gt;"><b>2.3 亚像素卷积重构层</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#89" data-title="3 实验设置 ">3 实验设置</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#90" data-title="&lt;b&gt;3.1 实验环境&lt;/b&gt;"><b>3.1 实验环境</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;3.2 图像集和参数设置&lt;/b&gt;"><b>3.2 图像集和参数设置</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#95" data-title="4 结果分析 ">4 结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#97" data-title="&lt;b&gt;4.1 主观效果&lt;/b&gt;"><b>4.1 主观效果</b></a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;4.2 客观指标&lt;/b&gt;"><b>4.2 客观指标</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#118" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#68" data-title="图1 所提算法网络结构图">图1 所提算法网络结构图</a></li>
                                                <li><a href="#78" data-title="图2 多尺度特征映射单元结构图">图2 多尺度特征映射单元结构图</a></li>
                                                <li><a href="#88" data-title="图3 亚像素卷积重构层结构图">图3 亚像素卷积重构层结构图</a></li>
                                                <li><a href="#100" data-title="图4 两倍放大因子下不同算法处理后的butterfly图像的超分辨率结果">图4 两倍放大因子下不同算法处理后的butterfly图像的超分辨率结果</a></li>
                                                <li><a href="#102" data-title="图5 三倍放大因子下不同算法处理后302008图像的超分辨率结果">图5 三倍放大因子下不同算法处理后302008图像的超分辨率结果</a></li>
                                                <li><a href="#115" data-title="表1 不同测试集经过不同算法处理后图像PSNR的平均值">表1 不同测试集经过不同算法处理后图像PSNR的平均值</a></li>
                                                <li><a href="#116" data-title="表2 不同测试集经过不同算法处理后图像SSIM的平均值">表2 不同测试集经过不同算法处理后图像SSIM的平均值</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Park S C, Park M K, Kang M G.Super-resolution image reconstruction:a technical overview[J].IEEE Signal Processing Magazine, 2003, 20 (3) :21-36." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Super-resolution image reconstruction: a technical overview">
                                        <b>[1]</b>
                                         Park S C, Park M K, Kang M G.Super-resolution image reconstruction:a technical overview[J].IEEE Signal Processing Magazine, 2003, 20 (3) :21-36.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Wang M, Liu K X, Liu L, &lt;i&gt;et al&lt;/i&gt;.Super-resolution reconstruction of image based on optimized convolution neural network[J].Laser &amp;amp; Optoelectronics Progress, 2017, 54 (11) :111005.王民, 刘可心, 刘利, 等.基于优化卷积神经网络的图像超分辨率重建[J].激光与光电子学进展, 2017, 54 (11) :111005." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201711015&amp;v=MDU1NDg2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeWptVWJ2Tkx5clBaTEc0SDliTnJvOUVZWVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Wang M, Liu K X, Liu L, &lt;i&gt;et al&lt;/i&gt;.Super-resolution reconstruction of image based on optimized convolution neural network[J].Laser &amp;amp; Optoelectronics Progress, 2017, 54 (11) :111005.王民, 刘可心, 刘利, 等.基于优化卷积神经网络的图像超分辨率重建[J].激光与光电子学进展, 2017, 54 (11) :111005.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" Li X, Orchard M T.New edge-directed interpolation[J].IEEE Transactions on Image Processing, 2001, 10 (10) :1521-1527." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=New edge-directed interpolation">
                                        <b>[3]</b>
                                         Li X, Orchard M T.New edge-directed interpolation[J].IEEE Transactions on Image Processing, 2001, 10 (10) :1521-1527.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Unser M, Aldroubi A, Eden M.Fast B-spline transforms for continuous image representation and interpolation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1991, 13 (3) :277-285." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast B-spline transforms for continuous image representation and interpolation">
                                        <b>[4]</b>
                                         Unser M, Aldroubi A, Eden M.Fast B-spline transforms for continuous image representation and interpolation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1991, 13 (3) :277-285.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" Schultz R R, Stevenson R L.Extraction of high-resolution frames from video sequences[J].IEEE Transactions on Image Processing, 1996, 5 (6) :996-1011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extraction of high-resolution frames from video sequences">
                                        <b>[5]</b>
                                         Schultz R R, Stevenson R L.Extraction of high-resolution frames from video sequences[J].IEEE Transactions on Image Processing, 1996, 5 (6) :996-1011.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Sun J, Xu Z B, Shum H Y.Image super-resolution using gradient profile prior[C]//2008 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2008, Anchorage, AK, USA.New York:IEEE, 2008:4587659." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using gradient profile prior">
                                        <b>[6]</b>
                                         Sun J, Xu Z B, Shum H Y.Image super-resolution using gradient profile prior[C]//2008 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2008, Anchorage, AK, USA.New York:IEEE, 2008:4587659.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" Irani M, Peleg S.Improving resolution by image registration[J].CVGIP:Graphical Models and Image Processing, 1991, 53 (3) :231-239." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving resolution by image registration">
                                        <b>[7]</b>
                                         Irani M, Peleg S.Improving resolution by image registration[J].CVGIP:Graphical Models and Image Processing, 1991, 53 (3) :231-239.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" Stark H, Oskoui P.High-resolution image recovery from image-plane arrays, using convex projections[J].Journal of the Optical Society of America A, 1989, 6 (11) :1715-1726." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-resolution image recovery from image-plane arrays, using convex projections">
                                        <b>[8]</b>
                                         Stark H, Oskoui P.High-resolution image recovery from image-plane arrays, using convex projections[J].Journal of the Optical Society of America A, 1989, 6 (11) :1715-1726.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" Timofte R, De V, Gool L V.Anchored neighborhood regression for fast example-based super-resolution[C]∥2013 IEEE International Conference on Computer Vision, December 1-8, 2013, Sydney, NSW, Australia.New York:IEEE, 2013:1920-1927." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Anchored Neighborhood Regression for Fast Example-Based Super Resolution">
                                        <b>[9]</b>
                                         Timofte R, De V, Gool L V.Anchored neighborhood regression for fast example-based super-resolution[C]∥2013 IEEE International Conference on Computer Vision, December 1-8, 2013, Sydney, NSW, Australia.New York:IEEE, 2013:1920-1927.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" Chang H, Yeung D Y, Xiong Y M.Super-resolution through neighbor embedding[C]//Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004, June 27-July 2, 2004, Washington D.C., USA.New York:IEEE, 2004:1315043." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Super-Resolution Through Neighbor Embed- ding">
                                        <b>[10]</b>
                                         Chang H, Yeung D Y, Xiong Y M.Super-resolution through neighbor embedding[C]//Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004, June 27-July 2, 2004, Washington D.C., USA.New York:IEEE, 2004:1315043.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" Huang J B, Singh A, Ahuja N.Single image super-resolution from transformed self-exemplars[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:5197-5206." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Singleimage super-resolution from transformedself-exemplars">
                                        <b>[11]</b>
                                         Huang J B, Singh A, Ahuja N.Single image super-resolution from transformed self-exemplars[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:5197-5206.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" Freeman W T, Jones T R, Pasztor E C.Example-based super-resolution[J].IEEE Computer Graphics and Applications, 2002, 22 (2) :56-65." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Example-based super-resolution">
                                        <b>[12]</b>
                                         Freeman W T, Jones T R, Pasztor E C.Example-based super-resolution[J].IEEE Computer Graphics and Applications, 2002, 22 (2) :56-65.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" Yang J C, Wright J, Huang T, &lt;i&gt;et al&lt;/i&gt;.Image super-resolution as sparse representation of raw image patches[C]∥2008 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2008, Anchorage, AK, USA.New York:IEEE, 2008:14587647." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Super-Resolution as Sparse Representation of Raw Image Patches">
                                        <b>[13]</b>
                                         Yang J C, Wright J, Huang T, &lt;i&gt;et al&lt;/i&gt;.Image super-resolution as sparse representation of raw image patches[C]∥2008 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2008, Anchorage, AK, USA.New York:IEEE, 2008:14587647.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" Dong C, Loy C C, He K M, &lt;i&gt;et al&lt;/i&gt;.Learning a deep convolutional network for image super-resolution[M]//Fleet D, Pajdla T, Schiele B, &lt;i&gt;et al&lt;/i&gt;.Computer Vision-ECCV 2014.Cham:Springer, 2014, 8692:184-199." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a Deep Convolutional Network for Image SuperResolution">
                                        <b>[14]</b>
                                         Dong C, Loy C C, He K M, &lt;i&gt;et al&lt;/i&gt;.Learning a deep convolutional network for image super-resolution[M]//Fleet D, Pajdla T, Schiele B, &lt;i&gt;et al&lt;/i&gt;.Computer Vision-ECCV 2014.Cham:Springer, 2014, 8692:184-199.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" Dong C, Loy C C, Tang X O.Accelerating the super-resolution convolutional neural network[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Computer Vision-ECCV 2016.Cham:Springer, 2016, 9906:391-407." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accelerating the Super-Resolution Convolutional Neural Network">
                                        <b>[15]</b>
                                         Dong C, Loy C C, Tang X O.Accelerating the super-resolution convolutional neural network[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Computer Vision-ECCV 2016.Cham:Springer, 2016, 9906:391-407.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" Kim J, Lee J K, Lee K M.Accurate image super-resolution using very deep convolutional networks[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1646-1654." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate image super-resolution using very deep convolutional networks">
                                        <b>[16]</b>
                                         Kim J, Lee J K, Lee K M.Accurate image super-resolution using very deep convolutional networks[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1646-1654.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_17" title=" Shi W Z, Caballero J, Husz&#225;r F, &lt;i&gt;et al&lt;/i&gt;.Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1874-1883." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network">
                                        <b>[17]</b>
                                         Shi W Z, Caballero J, Husz&#225;r F, &lt;i&gt;et al&lt;/i&gt;.Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1874-1883.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_18" title=" Xiao J S, Liu E Y, Zhu L, &lt;i&gt;et al&lt;/i&gt;.Improved image super-resolution algorithm based on convolutional neural network[J].Acta Optica Sinica, 2017, 37 (3) :0318011.肖进胜, 刘恩雨, 朱力, 等.改进的基于卷积神经网络的图像超分辨率算法[J].光学学报, 2017, 37 (3) :0318011." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201703012&amp;v=MjE0NjhaZVZ1RnlqbVVidk5JalhUYkxHNEg5Yk1ySTlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Xiao J S, Liu E Y, Zhu L, &lt;i&gt;et al&lt;/i&gt;.Improved image super-resolution algorithm based on convolutional neural network[J].Acta Optica Sinica, 2017, 37 (3) :0318011.肖进胜, 刘恩雨, 朱力, 等.改进的基于卷积神经网络的图像超分辨率算法[J].光学学报, 2017, 37 (3) :0318011.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_19" title=" Du B, Xiong W, Wu J, &lt;i&gt;et al&lt;/i&gt;.Stacked convolutional denoising auto-encoders for feature representation[J].IEEE Transactions on Cybernetics, 2017, 47 (4) :1017-1027." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Stacked Convolutional Denoising Auto-encoders for Feature Representation,&amp;quot;">
                                        <b>[19]</b>
                                         Du B, Xiong W, Wu J, &lt;i&gt;et al&lt;/i&gt;.Stacked convolutional denoising auto-encoders for feature representation[J].IEEE Transactions on Cybernetics, 2017, 47 (4) :1017-1027.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_20" title=" Sun C, L&#252; J W, Li J W, &lt;i&gt;et al&lt;/i&gt;.Method of rapid image super-resolution based on deconvolution[J].Acta Optica Sinica, 2017, 37 (12) :1210004.孙超, 吕俊伟, 李健伟, 等.基于去卷积的快速图像超分辨率方法[J].光学学报, 2017, 37 (12) :1210004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201712017&amp;v=MjMwNTc0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5am1VYnZOSWpYVGJMRzRIOWJOclk5RVk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         Sun C, L&#252; J W, Li J W, &lt;i&gt;et al&lt;/i&gt;.Method of rapid image super-resolution based on deconvolution[J].Acta Optica Sinica, 2017, 37 (12) :1210004.孙超, 吕俊伟, 李健伟, 等.基于去卷积的快速图像超分辨率方法[J].光学学报, 2017, 37 (12) :1210004.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_21" title=" Deng C, Xu J, Zhang K B, &lt;i&gt;et al&lt;/i&gt;.Similarity constraints-based structured output regression machine:an approach to image super-resolution[J].IEEE Transactions on Neural Networks and Learning Systems, 2016, 27 (12) :2472-2485." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Similarity constraints-based structured output regression machine:An approach to image super-resolution">
                                        <b>[21]</b>
                                         Deng C, Xu J, Zhang K B, &lt;i&gt;et al&lt;/i&gt;.Similarity constraints-based structured output regression machine:an approach to image super-resolution[J].IEEE Transactions on Neural Networks and Learning Systems, 2016, 27 (12) :2472-2485.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_22" title=" Martin D, Fowlkes C, Tal D, &lt;i&gt;et al&lt;/i&gt;.A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics[C]∥Proceedings Eighth IEEE International Conference on Computer Vision, July 7-14, 2001, Vancouver, BC, Canada.New York:IEEE, 2001:416-423." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics">
                                        <b>[22]</b>
                                         Martin D, Fowlkes C, Tal D, &lt;i&gt;et al&lt;/i&gt;.A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics[C]∥Proceedings Eighth IEEE International Conference on Computer Vision, July 7-14, 2001, Vancouver, BC, Canada.New York:IEEE, 2001:416-423.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_23" title=" Fan X X, Yang Y H, Deng C, &lt;i&gt;et al&lt;/i&gt;.Compressed multi-scale feature fusion network for single image super-resolution[J].Signal Processing, 2018, 146:50-60." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESAFD85E126BF19115A2A7494148D56B77&amp;v=MDY3NDJOaTZFNTZUSGJtclJZOURiZVNONzJZQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGhoeEx1OXhLMD1OaWZPZmNMT2F0bkoybzVIWXBsNURYVTR6aA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         Fan X X, Yang Y H, Deng C, &lt;i&gt;et al&lt;/i&gt;.Compressed multi-scale feature fusion network for single image super-resolution[J].Signal Processing, 2018, 146:50-60.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_24" title=" Su H, Zhou J, Zhang Z H.Survey of super-resolution image reconstruction methods[J].Acta Automatica Sinica, 2013, 39 (8) :1202-1213.苏衡, 周杰, 张志浩.超分辨率图像重建方法综述[J].自动化学报, 2013, 39 (8) :1202-1213." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201308005&amp;v=MjQ5MDNmWWJHNEg5TE1wNDlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlqbVVidk5LQ0w=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                         Su H, Zhou J, Zhang Z H.Survey of super-resolution image reconstruction methods[J].Acta Automatica Sinica, 2013, 39 (8) :1202-1213.苏衡, 周杰, 张志浩.超分辨率图像重建方法综述[J].自动化学报, 2013, 39 (8) :1202-1213.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-03-09 18:02</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(06),90-97 DOI:10.3788/AOS201939.0610001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于多尺度递归网络的图像超分辨率重建</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="javascript:;">吴磊</a>
                                <a href="javascript:;">吕国强</a>
                                <a href="javascript:;">薛治天</a>
                                <a href="javascript:;">盛杰超</a>
                                <a href="javascript:;">冯奇斌</a>
                </h2>
                    <h2>

                    <span>合肥工业大学电子科学与应用物理学院</span>
                    <span>合肥工业大学特种显示技术国家工程实验室现代显示技术省部共建国家重点实验室光电技术研究院</span>
                    <span>合肥工业大学仪器科学与光电工程学院</span>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出了一种基于多尺度递归网络的图像超分辨率网络模型, 该模型主要由多个多尺度特征映射单元级联而成, 每个单元分别包含一组不同尺度的特征提取层、一个融合层以及一个特征映射层。特征提取直接在原始低分辨率图像上进行, 最后采用亚像素卷积重构高分辨率图像。训练阶段使用自适应矩估计优化方法加速网络模型的收敛。实验结果表明, 所提算法取得了较好的超分辨率结果, 图像纹理清晰、边缘锐利, 视觉效果明显得到增强。在Set5、Set14、BSD100以及Urban100等常用测试集上的客观评价指标 (PSNR和SSIM) 均高于现有的几种主流算法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">超分辨率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%9A%E5%83%8F%E7%B4%A0%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">亚像素卷积;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    冯奇斌 E-mail:fengqibin@hfut.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>安徽省科技重大专项 (17030901053);</span>
                    </p>
            </div>
                    <h1><b>Super-Resolution Reconstruction of Images Based on Multi-Scale Recursive Network</b></h1>
                    <h2>
                    <span>Wu Lei</span>
                    <span>Lü Guoqiang</span>
                    <span>Xue Zhitian</span>
                    <span>Sheng Jiechao</span>
                    <span>Feng Qibin</span>
            </h2>
                    <h2>
                    <span>School of Electronic Science & Applied Physics, Hefei University of Technology</span>
                    <span>National Engineering Laboratory of Special Display Technology, National Key Laboratory of Advanced Display Technology, Academy of Photoelectric Technology, Hefei University of Technology</span>
                    <span>School of Instrument Science & Opto-Electronics Engineering, Hefei University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>An image super-resolution network model is proposed based on a multi-scale recursive network herein. The proposed model mainly comprises a plurality of multi-scale feature mapping units, each of which includes a set of feature extraction layers with different scales, a fusion layer, and a mapping layer. The network performs feature extraction directly on an original low-resolution image, which is then reconstructed into a high-resolution image via sub-pixel convolution. In the training phase, the adaptive optimization method is used to accelerate the convergence of the network model. The experimental results show that the proposed algorithm achieves better super-resolution results, significantly improves the subjective visual effects, and sharpens the image texture. The objective evaluation indicators (PSNR and SSIM) of the proposed algorithm on the common test sets such as Set5, Set14, BSD100, and Urban100 are higher than those of the existing mainstream algorithms.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=super-resolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">super-resolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale%20features&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale features;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20networks&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural networks;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sub-pixel%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sub-pixel convolution;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-12-19</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="59" name="59" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="60">超分辨率 (SR) 重建技术一直以来都是图像处理领域的研究热点, 是一种仅从算法层面入手, 利用一幅或多幅低分辨率 (LR) 图像估计高分辨率 (HR) 图像的方法。该技术廉价、高效, 现已广泛应用于城市安防、医学成像和卫星遥感等领域<citation id="120" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="61">随着众多学者的深入研究, 目前的单帧图像超分辨率重建技术可以分为基于插值、基于重建和基于学习三种方法。双线性插值和双三次插值等是经典的基于插值<citation id="129" type="reference"><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>的算法, 这类算法简单直观, 但重建图像存在锯齿伪影和纹理模糊等问题。基于重建<citation id="130" type="reference"><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>的方法, 如迭代反投影法<citation id="121" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和凸集投影法<citation id="122" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等, 都是基于信号处理理论实现成像原理的逆过程, 恢复在成像过程中丢失的高频信息。该类算法可以恢复比较清晰的HR图像, 但通常也会忽略一些细节信息。近年来, 基于学习的方法<citation id="131" type="reference"><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>备受关注, Freeman等<citation id="123" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出了基于样例学习的超分辨率算法, 率先指出在图像局部空间邻域中存在大量的自相似块;Yang等<citation id="124" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>利用稀疏编码理论, 构建高低分辨率图像字典, 通过学习字典之间的映射关系重构HR图像。随着Dong等<citation id="125" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>率先将卷积神经网络应用于图像超分辨率 (SRCNN) , 基于深度学习的方法一跃成为超分辨率领域的研究热点。为了提高图像重建速度, Dong等<citation id="126" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>又在自己算法的基础上进行改进, 提出基于反卷积的快速图像超分辨率重建 (FSRCNN) 算法, 该算法直接在LR图像上进行特征提取, 在网络末端引入反卷积层以重构高分辨率图像。Kim等<citation id="127" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>利用深度卷积网络实现精确的图像超分辨率 (VDSR) 算法, 加深了网络层数, 利用深层残差网络加快网络学习速度, 构建了一个支持不同倍数放大图像的网络模型。Shi等<citation id="128" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>基于高效的亚像素卷积神经网络的实时单图像和视频超分辨率 (ESPCN) 算法, 提出采用亚像素卷积的上采样方式, 将从LR图像中提取到的特征图重新排列成HR图像, 该方法加快了算法的运算速度, 在保证较好视觉效果的前提下, 满足了实时视频处理的要求。</p>
                </div>
                <div class="p1">
                    <p id="62">虽然上述方法取得了较好的重建效果, 但仍然存在一些不足:1) VDSR算法<citation id="132" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>需要先对输入图像进行双三插值放大处理, 这增加了网络的计算量, 20层的网络层数更是增加了算法的复杂度;2) ESPCN算法<citation id="133" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>的网络模型相对单一, 仅有的三层卷积以及不使用边界零填充使得该模型无法充分恢复图像的高频信息;3) 上述网络模型都只在单一尺度下提取图像特征, 忽略了不同尺度下的图像细节信息。</p>
                </div>
                <div class="p1">
                    <p id="63">为了解决上述问题, 本文提出一种多尺度递归网络模型, 该模型利用多尺度特征提取单元, 递归提取并映射图像特征信息, 然后采用亚像素卷积的方式对特征图进行重新排列, 重建HR图像。网络训练使用Adam (自适应矩估计) 优化方法来加速模型的收敛。卷积操作在边界零填充的前提下进行, 高质量地恢复了图像的纹理信息, 增强了重建图像的视觉效果。</p>
                </div>
                <h3 id="64" name="64" class="anchor-tag">2 基于多尺度递归网络的SR算法</h3>
                <div class="p1">
                    <p id="65">基于卷积神经网络的SR算法通常可分为以下4个步骤:1) 预处理外部图像集, 构造训练数据集;2) 搭建合适的网络模型;3) 利用数据集训练网络优化网络参数;4) 利用训练好的网络模型重建HR图像<citation id="134" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66"><b>2.1 网络结构概述</b></h4>
                <div class="p1">
                    <p id="67">针对现有算法特征提取尺度单一和重建图像纹理模糊等问题, 提出了多尺度特征递归提取网络, 即将前一个特征映射单元的输出直接作为下一个单元的输入, 递归提取并融合多尺度特征信息, 网络结构如图1所示。图中Conv表示卷积操作, PReLU表示参数化线性修正单元。在网络确定前期选取了多组卷积核进行预实验, 测试结果显示:当采用1×1、3×3和5×5三种不同尺度的卷积核组合时, 图像</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906011_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 所提算法网络结构图" src="Detail/GetImg?filename=images/GXXB201906011_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 所提算法网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906011_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Network structure of proposed algorithm</p>

                </div>
                <div class="p1">
                    <p id="69">特征提取更充分;当级联11个特征映射单元时, 该网络的超分辨率效果最好。因此所提模型采用11个三尺度特征映射单元级联组成递归学习网络。该网络首先在原始LR图像上进行特征提取, 避免通过双三次插值放大LR图像带来无用信息的干扰, 在减少算法复杂度的同时, 可以充分保留图像的结构和背景信息;再将这些信息融合映射构成特征图;最后采用亚像素卷积的方式将这些特征图进行重新排列, 重建视觉效果优良的HR图像。</p>
                </div>
                <div class="p1">
                    <p id="70">随着网络层数的加深, 收敛速度会有所减慢<citation id="135" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, 为了减小深层网络对收敛速度的影响, 在训练过程中采用Adam优化方法优化网络参数。相较于传统的随机梯度下降法, Adam优化算法能够做到基于训练数据迭代更新网络权重, 通过计算梯度的一阶矩估计和二阶矩估计, 可为不同的参数设计独立的自适应性学习率, 从而实现网络模型的快速收敛。计算过程可表示为</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>ρ</mi><msub><mrow></mrow><mn>1</mn></msub><mi>s</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>ρ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mi>g</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>r</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>ρ</mi><msub><mrow></mrow><mn>2</mn></msub><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>ρ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mi>g</mi><msubsup><mrow></mrow><mi>t</mi><mn>2</mn></msubsup><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mover accent="true"><mi>s</mi><mo>^</mo></mover><mo>=</mo><mfrac><mi>s</mi><mrow><mn>1</mn><mo>-</mo><mi>ρ</mi><msubsup><mrow></mrow><mn>1</mn><mi>t</mi></msubsup></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo>=</mo><mfrac><mi>r</mi><mrow><mn>1</mn><mo>-</mo><mi>ρ</mi><msubsup><mrow></mrow><mn>2</mn><mi>t</mi></msubsup></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><mi>Δ</mi><mi>θ</mi><mo>=</mo><mo>-</mo><mi>ε</mi><mfrac><mover accent="true"><mi>s</mi><mo>^</mo></mover><mrow><msqrt><mover accent="true"><mi>r</mi><mo>^</mo></mover></msqrt><mo>+</mo><mi>δ</mi></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><mi>θ</mi><mo>=</mo><msup><mi>θ</mi><mo>′</mo></msup><mo>+</mo><mi>Δ</mi><mi>θ</mi><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">式中:<i>s</i><sub><i>t</i></sub>和<i>r</i><sub><i>t</i></sub>分别表示一、二阶矩估计;<i>t</i>表示时间序列;<i>g</i><sub><i>t</i></sub>表示第<i>t</i>步时的梯度;<mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>s</mi><mo>^</mo></mover></math></mathml>和<mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>r</mi><mo>^</mo></mover></math></mathml>分别表示修正的一、二阶矩估计的偏差;θ′表示初始参数;Δθ表示计算的更新值;θ表示更新后的参数;步长ε取0.001;δ表示用于数值稳定的小常数, 这里取10<sup>-8</sup>。初始一、二阶矩变量都为0。在此, <i>Adam</i>参数设置采用默认设置, 即矩估计指数衰减速率ρ<sub>1</sub>=0.9, ρ<sub>2</sub>=0.999。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>2.2 多尺度特征映射单元</b></h4>
                <div class="p1">
                    <p id="76">在图像超分辨率重建中, 需要在不同的尺度上处理特征信息, 大多数现有算法几乎都只使用较小或较大的感受野, 并在一个维度上进行学习。但实际上, 图像的结构信息往往是不同尺度的, 单一尺度的特征提取不足以完全恢复图像的高频纹理区域。实验证明不同尺度的卷积核能够提取到图像的多种特征, 对这些特征信息进行融合映射可使网络充分学习到<i>LR</i>图像与<i>HR</i>图像之间的对应关系, 从而保证重建图像的细节清晰性。</p>
                </div>
                <div class="p1">
                    <p id="77">如图2所示, 多尺度特征映射单元由多尺度特征提取层、特征融合层和特征映射层连接而成。根据2.1节所述, 特征提取层由1×1、3×3和5×5三种不同尺度的卷积核并列组成, 同时对图像进行特征提取, 分别得到16, 64, 48幅特征图;融合层采用级联的方式将这些包含多尺度信息的特征图进行合并重组, 以构成新的特征图;特征映射层采用尺寸为1×1的卷积核对新的特征图进行特征映射, 再次得到16幅特征图。</p>
                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906011_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 多尺度特征映射单元结构图" src="Detail/GetImg?filename=images/GXXB201906011_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 多尺度特征映射单元结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906011_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Structural diagram of multi-scale feature mapping unit</p>

                </div>
                <div class="p1">
                    <p id="79">在特征提取层后设置激活层, 在此激活函数选用PRelu函数, 它在负数区域内有一个小斜率, 避免了输入是负数时Relu函数不激活的情况<citation id="136" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。为了更充分地恢复图像的高频信息, 所提算法不进行池化操作。同时为了避免卷积过程中图像边缘信息的丢失, 在卷积运算前对图像进行边界填充零操作, 保证了输出图像与输入图像尺寸的一致性, 其表达式为</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mtext>ϕ</mtext></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mn>1</mn><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>L</mtext><mtext>R</mtext></mrow></msup><mo>+</mo><mi mathvariant="bold-italic">B</mi><msub><mrow></mrow><mrow><mn>1</mn><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /></mtd></mtr><mtr><mtd columnalign="left"><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mn>2</mn><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi mathvariant="bold-italic">B</mi><msub><mrow></mrow><mrow><mn>2</mn><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>, </mo></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi mathvariant="bold-italic">F</mi><msubsup><mrow></mrow><mn>1</mn><mi>m</mi></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mtext>ϕ</mtext></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mn>1</mn><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">F</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>+</mo><mi mathvariant="bold-italic">B</mi><msub><mrow></mrow><mrow><mn>1</mn><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /></mtd></mtr><mtr><mtd columnalign="left"><mi mathvariant="bold-italic">F</mi><msubsup><mrow></mrow><mn>2</mn><mi>m</mi></msubsup><mo>=</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mn>2</mn><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">F</mi><msubsup><mrow></mrow><mn>1</mn><mi>m</mi></msubsup><mo>+</mo><mi mathvariant="bold-italic">B</mi><msub><mrow></mrow><mrow><mn>2</mn><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>, </mo></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext>ϕ</mtext><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>max</mi></mrow><mo stretchy="false"> (</mo><mi>a</mi><mi>x</mi><mo>, </mo><mi>x</mi><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">式中:<b><i>F</i></b><sub>1</sub>和<b><i>F</i></b><sub>2</sub>分别表示第1个单元的特征提取融合和映射操作;<b><i>F</i></b><mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>m</mi></msubsup></mrow></math></mathml>和<b><i>F</i></b><sup><i>m</i></sup><sub>2</sub>分别表示第<i>m</i>个单元的特征提取融合和映射操作;<i>n</i>表示多尺度卷积核的数量, 在此<i>n</i>=3;<b><i>W</i></b><sub>1<sub><i>i</i></sub></sub>、<b><i>W</i></b><sub>2<sub><i>i</i></sub></sub>和<b><i>B</i></b><sub>1<sub><i>i</i></sub></sub>、<b><i>B</i></b><sub>2<sub><i>i</i></sub></sub>分别表示可学习的网络权重和偏差, <i>i</i>为卷积核序号;*表示卷积操作;<b><i>I</i></b><sup>LR</sup>表示原始LR图像;ϕ (<i>x</i>) 表示激活函数, 在此选用PRelu函数;<i>x</i>为自变量;参数<i>a</i>取0～1之间的小数。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83"><b>2.3 亚像素卷积重构层</b></h4>
                <div class="p1">
                    <p id="84">在网络末端引入亚像素卷积层, 亚像素卷积重构层结构如图3所示。针对不同的放大倍数<i>r</i>, 在最后一个多尺度映射单元输出的特征图上, 首先使用尺寸为<i>r</i><sup>2</sup>×3×3的卷积核进行卷积操作, <i>r</i><sup>2</sup>表示特征图数量;然后将卷积生成的<i>r</i><sup>2</sup>个特征图重新进行排列组合, 重构放大<i>r</i>倍的HR图像。具体可表示为</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>Η</mtext><mtext>R</mtext></mrow></msup><mo>=</mo><mtext>Ρ</mtext><mtext>S</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>3</mn></msub><mo>*</mo><mi mathvariant="bold-italic">F</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mn>1</mn><mn>1</mn></mrow></msubsup><mo>+</mo><mi mathvariant="bold-italic">B</mi><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">式中:<b><i>I</i></b><sup>HR</sup>表示输出的HR图像;<b><i>F</i></b><mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mrow><mn>1</mn><mn>1</mn></mrow></msubsup></mrow></math></mathml>表示最后一个映射单元输出的特征图;<b><i>W</i></b><sub>3</sub>和<b><i>B</i></b><sub>3</sub>表示可学习的网络权重和偏差;PS表示一个周期变换算子, 它将一个<i>H</i>×<i>W</i>×<i>Cr</i><sup>2</sup>的张量重新排列成一个形状为<i>rH</i>×<i>rW</i>×<i>C</i>的张量, 从而实现图像的上采样, 其中<i>H</i>、<i>W</i>和<i>C</i>分别表示张量的长、宽和维度。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906011_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 亚像素卷积重构层结构图" src="Detail/GetImg?filename=images/GXXB201906011_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 亚像素卷积重构层结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906011_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Structural diagram of sub-pixel convolution reconstruction layer</p>

                </div>
                <h3 id="89" name="89" class="anchor-tag">3 实验设置</h3>
                <h4 class="anchor-tag" id="90" name="90"><b>3.1 实验环境</b></h4>
                <div class="p1">
                    <p id="91">所提算法实验测试的硬件环境为搭载Intel Core i7-3770U @3.40 GHz CPU, 配置NVIDIA GeForce GTX 1060 GPU, 内存为16 GB的计算机, 软件环境为64位Windows 7操作系统、caffe框架、CUDA Tookit 8.0、Cudnn 5.0和Matlab R2016a。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92"><b>3.2 图像集和参数设置</b></h4>
                <div class="p1">
                    <p id="93">在此选用公开的标准数据集训练网络模型, 该数据集包含291幅自然图像, 其中91幅来自已有文献<citation id="137" type="reference">[<a class="sup">21</a>]</citation>, 另外200幅来自Berkeley dataset (BSD) <citation id="138" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>训练集。研究发现<citation id="139" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>, 网络模型性能的好坏与训练数据集的大小密切相关, 大规模的数据集可以提升网络的整体性能。为了扩充训练数据, 对标准数据集中的每个图像进行旋转缩放处理, 在分别旋转90°、180°和270°的基础上, 按照0.9, 0.8, 0.7, 0.6的系数缩放, 保存每一步操作生成的图像, 共生成5820幅训练图像。对这5820幅图像先进行<i>r</i>倍下采样以获取LR图像<b><i>I</i></b><sup>LR</sup>, 为了增大网络的感受野, 充分利用较大图像区域的上下文信息, 将其不重叠地裁剪成41×41的子图像块<citation id="140" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 再与裁剪后的原始HR图像块对应组成LR-HR训练数据, 训练验证集选用Set5。由于人眼视觉系统对图像亮度分量较为敏感, 因此该网络只在图像亮度分量上进行训练。</p>
                </div>
                <div class="p1">
                    <p id="94">网络权重采取高斯初始化, 训练开始阶段, 将学习率设置为10<sup>-4</sup>, 随着训练的进行, 观察损失函数的同时动态微调学习率, 其最低可达到10<sup>-5</sup>, 网络的动量和权值衰减参数分别设置为0.9和10<sup>-4</sup>。</p>
                </div>
                <h3 id="95" name="95" class="anchor-tag">4 结果分析</h3>
                <div class="p1">
                    <p id="96">为了验证所提算法的超分辨率重建性能, 实验选用超分辨率领域通用的测试集Set5、Set14、BSD100和Urban100进行测试, 并将测试结果与Bicubic、Self-Ex<citation id="141" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、ScSR<citation id="142" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、SRCNN<citation id="143" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、FSRCNN<citation id="144" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、VDSR<citation id="145" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>和ESPCN<citation id="146" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>等算法在主观效果或客观指标等方面进行对比。为了保证客观准确的对比, 所有网络模型均采用处理后的291幅通用自然图像重新进行训练, 且均在放大因子为2, 3, 4的比例下进行实验测试, 加之测试平台配置不同, 因此所得测试结果与原文献数据略有不同。</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97"><b>4.1 主观效果</b></h4>
                <div class="p1">
                    <p id="98">将所提算法与已有算法进行对比。图4和图5分别展示了不同算法在不同放大因子下对两幅通用测试图像进行处理的结果, 为了突显细节纹理, 在此对图像局部区域进行了放大对比。</p>
                </div>
                <div class="p1">
                    <p id="99">图4为2倍放大因子下不同算法对Set5测试集中butterfly图像处理后得到的超分辨率结果, 从图中可以看出, Bicubic算法的处理效果最为模糊, Self-Ex<citation id="147" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、SRCNN<citation id="148" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>和FSRCNN<citation id="149" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>算法在蝴蝶翅膀细节区域有所欠缺, 所提算法的处理结果要优于ESPCN算法<citation id="150" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 与VDSR算法<citation id="151" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>效果相当。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906011_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 两倍放大因子下不同算法处理后的butterfly图像的超分辨率结果" src="Detail/GetImg?filename=images/GXXB201906011_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 两倍放大因子下不同算法处理后的butterfly图像的超分辨率结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906011_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Super-resolution results of butterfly image processed by different algorithms under twice magnification factor</p>
                                <p class="img_note"> (a) 原图; (b) Bicubic算法; (c) Self-Ex算法; (d) SRCNN算法; (e) FSRCNN算法; (f) ESPCN算法; (g) VDSR算法; (h) 所提算法</p>
                                <p class="img_note"> (a) Original image; (b) Bicubic algorithm; (c) Self-Ex algorithm; (d) SRCNN algorithm; (e) FSRCNN algorithm; (f) ESPCN algorithm; (g) VDSR algorithm; (h) proposed algorithm</p>

                </div>
                <div class="p1">
                    <p id="101">图5为3倍放大因子下不同算法对BSD100测试集中302008图像进行处理得到的超分辨率结果, 可以看出在人的衣领细节区域, 所提算法处理结果要明显优于传统的插值算法, 同时相较于其他基于深度学习的算法, 所提算法处理得到的图像条纹边缘更锐利, 细节更清晰。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906011_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 三倍放大因子下不同算法处理后302008图像的超分辨率结果" src="Detail/GetImg?filename=images/GXXB201906011_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 三倍放大因子下不同算法处理后302008图像的超分辨率结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906011_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Super-resolution results of 302008 image processed by different algorithms under triple magnification factor</p>
                                <p class="img_note"> (a) 原图; (b) Bicubic算法; (c) Self-Ex算法; (d) SRCNN算法; (e) FSRCNN算法; (f) ESPCN算法; (g) VDSR算法; (h) 所提算法</p>
                                <p class="img_note"> (a) Original image; (b) Bicubic algorithm; (c) Self-Ex algorithm; (d) SRCNN algorithm; (e) FSRCNNalgorithm; (f) ESPCN algorithm; (g) VDSR algorithm; (h) proposed algorithm</p>

                </div>
                <h4 class="anchor-tag" id="103" name="103"><b>4.2 客观指标</b></h4>
                <div class="p1">
                    <p id="104">采用峰值信噪比 (PSNR) 和结构相似性 (SSIM) 作为客观评价指标<citation id="152" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>, 从而更准确地表明所提算法的优越性。</p>
                </div>
                <div class="p1">
                    <p id="105">PSNR反映的是两幅图像对应像素点间的误差, 其值越高表明输出图像失真越少, 图像重建质量越好。SSIM是表示两幅图像相似度的评价指标, 其值越接近于1, 代表输出图像越接近于原始HR图像, 即重建效果越好。PSNR的计算公式为</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>S</mtext><mtext>Ν</mtext><mtext>R</mtext></mrow></msub><mo>=</mo><mn>1</mn><mn>0</mn><mrow><mi>lg</mi></mrow><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mn>2</mn><msup><mrow></mrow><mi>k</mi></msup><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mi>f</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>S</mtext><mtext>E</mtext></mrow></msub><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">式中:<i>f</i><sub>MSE</sub>表示均方误差, 即网络的损失函数;<i>k</i>表示图像位数, 在此取8。</p>
                </div>
                <div class="p1">
                    <p id="108">SSIM的计算公式为</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>S</mtext><mtext>Ι</mtext><mtext>Μ</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><mo>, </mo><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>Η</mtext><mtext>R</mtext></mrow></msup><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mn>2</mn><mi>μ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ι</mi></msub><mi>μ</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>Η</mtext><mtext>R</mtext></mrow></msup></mrow></msub><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mn>2</mn><mi>δ</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">Ι</mi><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>Η</mtext><mtext>R</mtext></mrow></msup></mrow></msub><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow><mrow><mo stretchy="false"> (</mo><mi>μ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Ι</mi><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>Η</mtext><mtext>R</mtext></mrow></msup></mrow><mn>2</mn></msubsup><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>δ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Ι</mi><mn>2</mn></msubsup><mo>+</mo><mi>δ</mi><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>Η</mtext><mtext>R</mtext></mrow></msup></mrow><mn>2</mn></msubsup><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">其中</p>
                </div>
                <div class="p1">
                    <p id="111" class="code-formula">
                        <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mrow><mo stretchy="false"> (</mo><mi>k</mi><msub><mrow></mrow><mn>1</mn></msub><mi>L</mi><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mspace width="0.25em" /><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mrow><mo stretchy="false"> (</mo><mi>k</mi><msub><mrow></mrow><mn>2</mn></msub><mi>L</mi><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="112">式中:<b><i>I</i></b>表示原始HR图像;<b><i>I</i></b><sup>HR</sup>表示重建后的HR图像;<i>μ</i><sub><b><i>I</i></b></sub>和<i>μ</i><sub><b><i>I</i></b><sup>HR</sup></sub>分别为原始HR图像和重建HR图像的均值;<i>δ</i><sup>2</sup><sub><b><i>I</i></b></sub>表示<b><i>I</i></b>的方差;<i>δ</i><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>Η</mtext><mtext>R</mtext></mrow></msup></mrow><mn>2</mn></msubsup></mrow></math></mathml>表示<b><i>I</i></b><sup>HR</sup>的方差;<i>δ</i><sub><b><i>II</i></b><sup>HR</sup></sub>表示<b><i>I</i></b>和<b><i>I</i></b><sup>HR</sup>的协方差;<i>c</i><sub>1</sub>、<i>c</i><sub>2</sub>是用来维持稳定的常数;<i>L</i>表示像素值的动态范围;系数<i>k</i><sub>1</sub>=0.01, <i>k</i><sub>2</sub>=0.03。</p>
                </div>
                <div class="p1">
                    <p id="114">采用上述两种指标对算法进行客观评价, 得到了如表1和表2分别所示的在2, 3, 4倍放大因子下, Set5、Set14、BSD100和Urban100等测试集中所有图像分别经过不同算法处理后PSNR和SSIM的平均值, 对比表中数据可以看出, 所提算法相较于传统的Bicubic算法和基于稀疏表示的算法, 两个指标数据均有大幅提升, 同时也优于其他基于深度学习的主流算法, 4个测试集的PSNR相对于ESPCN算法<citation id="153" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>的PSNR平均提升了0.5 dB左右, SSIM相对提高了0.02左右。</p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit">表1 不同测试集经过不同算法处理后图像PSNR的平均值 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Average PSNR values of images for different test sets processed by different algorithms</p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td>Dataset</td><td>Scale</td><td>Bicubic <br />algorithm</td><td>ScSR <br />algorithm</td><td>Self-Ex <br />algorithm</td><td>SRCNN <br />algorithm</td><td>ESPCN <br />algorithm</td><td>Proposed <br />algorithm</td></tr><tr><td><br /></td><td>×2</td><td>33.64</td><td>35.78</td><td>36.49</td><td>36.69</td><td>36.39</td><td>36.88</td></tr><tr><td><br />Set5</td><td>×3</td><td>30.39</td><td>31.34</td><td>32.58</td><td>32.76</td><td>32.75</td><td>33.59</td></tr><tr><td><br /></td><td>×4</td><td>28.42</td><td>29.07</td><td>30.31</td><td>30.49</td><td>30.21</td><td>30.65</td></tr><tr><td><br /></td><td>×2</td><td>30.00</td><td>31.64</td><td>32.22</td><td>32.47</td><td>32.21</td><td>32.36</td></tr><tr><td><br />Set14</td><td>×3</td><td>27.33</td><td>28.19</td><td>29.16</td><td>29.30</td><td>29.10</td><td>29.37</td></tr><tr><td><br /></td><td>×4</td><td>25.79</td><td>26.40</td><td>27.40</td><td>27.51</td><td>27.17</td><td>27.33</td></tr><tr><td><br /></td><td>×2</td><td>29.56</td><td>30.77</td><td>31.18</td><td>31.37</td><td>30.93</td><td>31.53</td></tr><tr><td><br />BSD100</td><td>×3</td><td>27.21</td><td>27.72</td><td>28.29</td><td>28.42</td><td>28.16</td><td>28.71</td></tr><tr><td><br /></td><td>×4</td><td>25.98</td><td>26.61</td><td>26.84</td><td>26.91</td><td>26.59</td><td>27.01</td></tr><tr><td><br /></td><td>×2</td><td>26.86</td><td>28.26</td><td>29.54</td><td>29.51</td><td>29.31</td><td>29.77</td></tr><tr><td><br />Urban100</td><td>×3</td><td>24.46</td><td>25.69</td><td>26.44</td><td>26.24</td><td>25.94</td><td>26.74</td></tr><tr><td><br /></td><td>×4</td><td>23.14</td><td>24.02</td><td>24.79</td><td>24.52</td><td>24.26</td><td>24.43</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit">表2 不同测试集经过不同算法处理后图像SSIM的平均值 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Average SSIM values of images for different test sets processed by different algorithms</p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td>Dataset</td><td>Scale</td><td>Bicubic <br />algorithm</td><td>ScSR <br />algorithm</td><td>Self-Ex <br />algorithm</td><td>SRCNN <br />algorithm</td><td>ESPCN <br />algorithm</td><td>Proposed <br />algorithm</td></tr><tr><td><br /></td><td>×2</td><td>0.9376</td><td>0.9485</td><td>0.9537</td><td>0.9612</td><td>0.9568</td><td>0.9622</td></tr><tr><td><br />Set5</td><td>×3</td><td>0.8795</td><td>0.8869</td><td>0.9093</td><td>0.9198</td><td>0.9183</td><td>0.9303</td></tr><tr><td><br /></td><td>×4</td><td>0.8238</td><td>0.8263</td><td>0.8619</td><td>0.8769</td><td>0.8578</td><td>0.8849</td></tr><tr><td><br /></td><td>×2</td><td>0.9412</td><td>0.8940</td><td>0.9034</td><td>0.9655</td><td>0.9598</td><td>0.9662</td></tr><tr><td><br />Set14</td><td>×3</td><td>0.8589</td><td>0.7977</td><td>0.8196</td><td>0.8956</td><td>0.8859</td><td>0.9008</td></tr><tr><td><br /></td><td>×4</td><td>0.7944</td><td>0.7218</td><td>0.7518</td><td>0.8291</td><td>0.8225</td><td>0.8439</td></tr><tr><td><br /></td><td>×2</td><td>0.8428</td><td>0.8744</td><td>0.8855</td><td>0.8884</td><td>0.8832</td><td>0.8914</td></tr><tr><td><br />BSD100</td><td>×3</td><td>0.7391</td><td>0.7647</td><td>0.7840</td><td>0.7867</td><td>0.7808</td><td>0.7963</td></tr><tr><td><br /></td><td>×4</td><td>0.6687</td><td>0.6983</td><td>0.7106</td><td>0.7107</td><td>0.6942</td><td>0.7193</td></tr><tr><td><br /></td><td>×2</td><td>0.9865</td><td>0.8828</td><td>0.8967</td><td>0.9944</td><td>0.9921</td><td>0.9946</td></tr><tr><td><br />Urban100</td><td>×3</td><td>0.9409</td><td>0.7831</td><td>0.8088</td><td>0.9657</td><td>0.9629</td><td>0.9709</td></tr><tr><td><br /></td><td>×4</td><td>0.9022</td><td>0.7024</td><td>0.7374</td><td>0.9396</td><td>0.9413</td><td>0.9423</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="117">此外, 选取了数张尺寸为512 pixel×512 pixel和256 pixel×256 pixel的图像对算法进行重建速度对比, 结果显示所提算法的重建速度相较Self-Ex<citation id="154" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、ScSR<citation id="155" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和SRCNN<citation id="156" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>等算法的图像重建速度均有明显提升。</p>
                </div>
                <h3 id="118" name="118" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="119">提出了一种基于多尺度递归网络的超分辨率重建算法, 用以解决现有的基于深度学习的超分辨率算法对原始LR图像细节特征提取不够充分和图像纹理区域重建不够清晰等问题。所提网络模型通过级联多尺度特征映射单元形成递归网络, 直接在LR图像上进行多尺度底层特征提取, 最后采用亚像素卷积对多尺度特征图进行上采样, 重建HR图像。通过最终的主观效果和客观数据对比, 可以看出所提算法能够更全面地提取LR图像特征, 更充分地恢复图像的纹理信息。但该算法精度仍有较大提高空间, 因此在接下来的优化实验中可以考虑引入多尺度残差学习网络, 在加速模型收敛的同时, 进一步提升图像重建质量。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Super-resolution image reconstruction: a technical overview">

                                <b>[1]</b> Park S C, Park M K, Kang M G.Super-resolution image reconstruction:a technical overview[J].IEEE Signal Processing Magazine, 2003, 20 (3) :21-36.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201711015&amp;v=MTMyODY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5am1VYnZOTHlyUFpMRzRIOWJOcm85RVlZUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Wang M, Liu K X, Liu L, <i>et al</i>.Super-resolution reconstruction of image based on optimized convolution neural network[J].Laser &amp; Optoelectronics Progress, 2017, 54 (11) :111005.王民, 刘可心, 刘利, 等.基于优化卷积神经网络的图像超分辨率重建[J].激光与光电子学进展, 2017, 54 (11) :111005.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=New edge-directed interpolation">

                                <b>[3]</b> Li X, Orchard M T.New edge-directed interpolation[J].IEEE Transactions on Image Processing, 2001, 10 (10) :1521-1527.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast B-spline transforms for continuous image representation and interpolation">

                                <b>[4]</b> Unser M, Aldroubi A, Eden M.Fast B-spline transforms for continuous image representation and interpolation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1991, 13 (3) :277-285.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extraction of high-resolution frames from video sequences">

                                <b>[5]</b> Schultz R R, Stevenson R L.Extraction of high-resolution frames from video sequences[J].IEEE Transactions on Image Processing, 1996, 5 (6) :996-1011.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using gradient profile prior">

                                <b>[6]</b> Sun J, Xu Z B, Shum H Y.Image super-resolution using gradient profile prior[C]//2008 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2008, Anchorage, AK, USA.New York:IEEE, 2008:4587659.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving resolution by image registration">

                                <b>[7]</b> Irani M, Peleg S.Improving resolution by image registration[J].CVGIP:Graphical Models and Image Processing, 1991, 53 (3) :231-239.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-resolution image recovery from image-plane arrays, using convex projections">

                                <b>[8]</b> Stark H, Oskoui P.High-resolution image recovery from image-plane arrays, using convex projections[J].Journal of the Optical Society of America A, 1989, 6 (11) :1715-1726.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Anchored Neighborhood Regression for Fast Example-Based Super Resolution">

                                <b>[9]</b> Timofte R, De V, Gool L V.Anchored neighborhood regression for fast example-based super-resolution[C]∥2013 IEEE International Conference on Computer Vision, December 1-8, 2013, Sydney, NSW, Australia.New York:IEEE, 2013:1920-1927.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Super-Resolution Through Neighbor Embed- ding">

                                <b>[10]</b> Chang H, Yeung D Y, Xiong Y M.Super-resolution through neighbor embedding[C]//Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004, June 27-July 2, 2004, Washington D.C., USA.New York:IEEE, 2004:1315043.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Singleimage super-resolution from transformedself-exemplars">

                                <b>[11]</b> Huang J B, Singh A, Ahuja N.Single image super-resolution from transformed self-exemplars[C]∥2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:5197-5206.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Example-based super-resolution">

                                <b>[12]</b> Freeman W T, Jones T R, Pasztor E C.Example-based super-resolution[J].IEEE Computer Graphics and Applications, 2002, 22 (2) :56-65.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Super-Resolution as Sparse Representation of Raw Image Patches">

                                <b>[13]</b> Yang J C, Wright J, Huang T, <i>et al</i>.Image super-resolution as sparse representation of raw image patches[C]∥2008 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2008, Anchorage, AK, USA.New York:IEEE, 2008:14587647.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a Deep Convolutional Network for Image SuperResolution">

                                <b>[14]</b> Dong C, Loy C C, He K M, <i>et al</i>.Learning a deep convolutional network for image super-resolution[M]//Fleet D, Pajdla T, Schiele B, <i>et al</i>.Computer Vision-ECCV 2014.Cham:Springer, 2014, 8692:184-199.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accelerating the Super-Resolution Convolutional Neural Network">

                                <b>[15]</b> Dong C, Loy C C, Tang X O.Accelerating the super-resolution convolutional neural network[M]//Leibe B, Matas J, Sebe N, <i>et al</i>.Computer Vision-ECCV 2016.Cham:Springer, 2016, 9906:391-407.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate image super-resolution using very deep convolutional networks">

                                <b>[16]</b> Kim J, Lee J K, Lee K M.Accurate image super-resolution using very deep convolutional networks[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1646-1654.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network">

                                <b>[17]</b> Shi W Z, Caballero J, Huszár F, <i>et al</i>.Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1874-1883.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201703012&amp;v=MTg4NTVFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlqbVVidk5JalhUYkxHNEg5Yk1ySTk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Xiao J S, Liu E Y, Zhu L, <i>et al</i>.Improved image super-resolution algorithm based on convolutional neural network[J].Acta Optica Sinica, 2017, 37 (3) :0318011.肖进胜, 刘恩雨, 朱力, 等.改进的基于卷积神经网络的图像超分辨率算法[J].光学学报, 2017, 37 (3) :0318011.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Stacked Convolutional Denoising Auto-encoders for Feature Representation,&amp;quot;">

                                <b>[19]</b> Du B, Xiong W, Wu J, <i>et al</i>.Stacked convolutional denoising auto-encoders for feature representation[J].IEEE Transactions on Cybernetics, 2017, 47 (4) :1017-1027.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201712017&amp;v=MDUwNDZHNEg5Yk5yWTlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlqbVVidk5JalhUYkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> Sun C, Lü J W, Li J W, <i>et al</i>.Method of rapid image super-resolution based on deconvolution[J].Acta Optica Sinica, 2017, 37 (12) :1210004.孙超, 吕俊伟, 李健伟, 等.基于去卷积的快速图像超分辨率方法[J].光学学报, 2017, 37 (12) :1210004.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Similarity constraints-based structured output regression machine:An approach to image super-resolution">

                                <b>[21]</b> Deng C, Xu J, Zhang K B, <i>et al</i>.Similarity constraints-based structured output regression machine:an approach to image super-resolution[J].IEEE Transactions on Neural Networks and Learning Systems, 2016, 27 (12) :2472-2485.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics">

                                <b>[22]</b> Martin D, Fowlkes C, Tal D, <i>et al</i>.A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics[C]∥Proceedings Eighth IEEE International Conference on Computer Vision, July 7-14, 2001, Vancouver, BC, Canada.New York:IEEE, 2001:416-423.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESAFD85E126BF19115A2A7494148D56B77&amp;v=MTQwODNPYXRuSjJvNUhZcGw1RFhVNHpoTmk2RTU2VEhibXJSWTlEYmVTTjcyWUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRoaHhMdTl4SzA9TmlmT2ZjTA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> Fan X X, Yang Y H, Deng C, <i>et al</i>.Compressed multi-scale feature fusion network for single image super-resolution[J].Signal Processing, 2018, 146:50-60.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201308005&amp;v=MDc3NzVxcUJ0R0ZyQ1VSTE9lWmVWdUZ5am1VYnZOS0NMZlliRzRIOUxNcDQ5RllZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b> Su H, Zhou J, Zhang Z H.Survey of super-resolution image reconstruction methods[J].Acta Automatica Sinica, 2013, 39 (8) :1202-1213.苏衡, 周杰, 张志浩.超分辨率图像重建方法综述[J].自动化学报, 2013, 39 (8) :1202-1213.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201906011" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201906011&amp;v=MTQ3MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlqbVVidk5JalhUYkxHNEg5ak1xWTlFWlk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

