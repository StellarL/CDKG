

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134131875440000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201903002%26RESULT%3d1%26SIGN%3dmUJg3%252bsAplBOJbu9tPEr2ztBiUc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201903002&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201903002&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201903002&amp;v=MzA1ODZPZVplVnVGeUhsVzdyTUlqWFRiTEc0SDlqTXJJOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#58" data-title="2 原理方法 ">2 原理方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title="&lt;b&gt;2.1 VGG-19特征提取器&lt;/b&gt;"><b>2.1 VGG-19特征提取器</b></a></li>
                                                <li><a href="#65" data-title="&lt;b&gt;2.2 局部特征的表达&lt;/b&gt;"><b>2.2 局部特征的表达</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;2.3 全局特征的提取&lt;/b&gt;"><b>2.3 全局特征的提取</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;2.4 融合特征的提取及分类&lt;/b&gt;"><b>2.4 融合特征的提取及分类</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#100" data-title="3 实验与分析 ">3 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#101" data-title="&lt;b&gt;3.1 实验设置及数据集&lt;/b&gt;"><b>3.1 实验设置及数据集</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;3.2 局部特征表达能力分析&lt;/b&gt;"><b>3.2 局部特征表达能力分析</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;3.3 融合策略有效性分析&lt;/b&gt;"><b>3.3 融合策略有效性分析</b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;3.4 GLDFB实验结果及分析&lt;/b&gt;"><b>3.4 GLDFB实验结果及分析</b></a></li>
                                                <li><a href="#131" data-title="&lt;b&gt;3.5 迁移实验&lt;/b&gt;"><b>3.5 迁移实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="4 结 论 ">4 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="图1 GLDFB流程图">图1 GLDFB流程图</a></li>
                                                <li><a href="#64" data-title="图2 VGG-19的网络结构">图2 VGG-19的网络结构</a></li>
                                                <li><a href="#67" data-title="表1 VGG-19卷积层的输出特征维度">表1 VGG-19卷积层的输出特征维度</a></li>
                                                <li><a href="#74" data-title="图3 卷积层特征的重组和编码">图3 卷积层特征的重组和编码</a></li>
                                                <li><a href="#104" data-title="图4 遥感场景图像示例。">图4 遥感场景图像示例。</a></li>
                                                <li><a href="#110" data-title="表2 三种类型卷积层特征在不同&lt;i&gt;K&lt;/i&gt;值下的平均分类精度比较">表2 三种类型卷积层特征在不同<i>K</i>值下的平均分类精度比较</a></li>
                                                <li><a href="#111" data-title="表3 其他几种特征的分类精度">表3 其他几种特征的分类精度</a></li>
                                                <li><a href="#112" data-title="图5 12层卷积层特征在不同&lt;i&gt;K&lt;/i&gt;值下的聚类过程中单次迭代的时间耗费。">图5 12层卷积层特征在不同<i>K</i>值下的聚类过程中单次迭代的时间耗费。</a></li>
                                                <li><a href="#113" data-title="图6 12层卷积层特征在不同&lt;i&gt;K&lt;/i&gt;值下的分类精度。">图6 12层卷积层特征在不同<i>K</i>值下的分类精度。</a></li>
                                                <li><a href="#116" data-title="表4 多种特征的分类精度对比">表4 多种特征的分类精度对比</a></li>
                                                <li><a href="#121" data-title="表5 UCM数据集上的分类精度比较">表5 UCM数据集上的分类精度比较</a></li>
                                                <li><a href="#124" data-title="表6 SIRI数据集上的分类精度比较">表6 SIRI数据集上的分类精度比较</a></li>
                                                <li><a href="#126" data-title="图7 GLDFB在UCM数据集上的分类混淆矩阵">图7 GLDFB在UCM数据集上的分类混淆矩阵</a></li>
                                                <li><a href="#127" data-title="图8 两大类易混淆场景。">图8 两大类易混淆场景。</a></li>
                                                <li><a href="#128" data-title="图9 GLDFB在SIRI数据集上的分类混淆矩阵">图9 GLDFB在SIRI数据集上的分类混淆矩阵</a></li>
                                                <li><a href="#130" data-title="图10 GLDFB结果。">图10 GLDFB结果。</a></li>
                                                <li><a href="#133" data-title="表7 其他预训练CNN下GLDFB的分类结果">表7 其他预训练CNN下GLDFB的分类结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Cheriyadat A M. Unsupervised feature learning for aerial scene classification[J]. IEEE Transactions on Geoscience and Remote Sensing, 2014, 52 (1) : 439-451." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised feature learning for aerial scene classification">
                                        <b>[1]</b>
                                         Cheriyadat A M. Unsupervised feature learning for aerial scene classification[J]. IEEE Transactions on Geoscience and Remote Sensing, 2014, 52 (1) : 439-451.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Csurka G. Visual categorization with bags of keypoints[C]. European Conference on Computer Vision, 2004: 1-22." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual categorization with bags of keypoints">
                                        <b>[2]</b>
                                         Csurka G. Visual categorization with bags of keypoints[C]. European Conference on Computer Vision, 2004: 1-22.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" Lazebnik S, Schmid C, Ponce J. Beyond bags of features: spatial pyramid matching for recognizing natural scene categories[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2006: 2169-2178." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond bags of features:Spatial pyramid matching for recognizing natural scene categories">
                                        <b>[3]</b>
                                         Lazebnik S, Schmid C, Ponce J. Beyond bags of features: spatial pyramid matching for recognizing natural scene categories[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2006: 2169-2178.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Yang Y, Newsam S. Spatial pyramid co-occurrence for image classification[C]. International Conference on Computer Vision, 2011: 1465-1472." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid co-occurrence for image classification">
                                        <b>[4]</b>
                                         Yang Y, Newsam S. Spatial pyramid co-occurrence for image classification[C]. International Conference on Computer Vision, 2011: 1465-1472.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" Fang X, Wang G H, Yang H C, &lt;i&gt;et al&lt;/i&gt;. High resolution remote sensing image classification combining with mean-shift segmentation and fully convolution neural network[J]. Laser &amp;amp; Optoelectronics Progress, 2018, 55 (2) : 022802.  方旭, 王光辉, 杨化超, 等. 结合均值漂移分割与全卷积神经网络的高分辨遥感影像分类[J]. 激光与光电子学进展, 2018, 55 (2) : 022802." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201802058&amp;v=MDIxMzh0R0ZyQ1VSTE9lWmVWdUZ5SGxXN3JQTHlyUFpMRzRIOW5Nclk5QWJJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Fang X, Wang G H, Yang H C, &lt;i&gt;et al&lt;/i&gt;. High resolution remote sensing image classification combining with mean-shift segmentation and fully convolution neural network[J]. Laser &amp;amp; Optoelectronics Progress, 2018, 55 (2) : 022802.  方旭, 王光辉, 杨化超, 等. 结合均值漂移分割与全卷积神经网络的高分辨遥感影像分类[J]. 激光与光电子学进展, 2018, 55 (2) : 022802.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Liu D W, Han L, Han X Y. High spatial resolution remote sensing image classification based on deep learning[J]. Acta Optica Sinica, 2016, 36 (4) : 0428001.  刘大伟, 韩玲, 韩晓勇. 基于深度学习的高分辨率遥感影像分类研究[J]. 光学学报, 2016, 36 (4) : 0428001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604039&amp;v=MTA3ODhaZVZ1RnlIbFc3clBJalhUYkxHNEg5Zk1xNDlHYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Liu D W, Han L, Han X Y. High spatial resolution remote sensing image classification based on deep learning[J]. Acta Optica Sinica, 2016, 36 (4) : 0428001.  刘大伟, 韩玲, 韩晓勇. 基于深度学习的高分辨率遥感影像分类研究[J]. 光学学报, 2016, 36 (4) : 0428001.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" Liu F, Lu L X, Huang G W, &lt;i&gt;et al&lt;/i&gt;. Landform image classification based on discrete cosine transformation and deep network [J]. Acta Optica Sinica, 2018, 38 (6) : 0620001. 刘芳, 路丽霞, 黄光伟, 等. 基于离散余弦变换和深度网络的地貌图像分类[J]. 光学学报, 2018, 38 (6) : 0620001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806037&amp;v=Mjg1OTMzenFxQnRHRnJDVVJMT2VaZVZ1RnlIbFc3clBJalhUYkxHNEg5bk1xWTlHWTRRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Liu F, Lu L X, Huang G W, &lt;i&gt;et al&lt;/i&gt;. Landform image classification based on discrete cosine transformation and deep network [J]. Acta Optica Sinica, 2018, 38 (6) : 0620001. 刘芳, 路丽霞, 黄光伟, 等. 基于离散余弦变换和深度网络的地貌图像分类[J]. 光学学报, 2018, 38 (6) : 0620001.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" Castelluccio M, Poggi G, Sansone C, &lt;i&gt;et al&lt;/i&gt;. Land use classification in remote sensing images by convolutional neural networks[J]. Acta Ecologica Sinica, 2015, 28 (2) : 627-635." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Land use classification in remote sensing images by convolutional neural networks">
                                        <b>[8]</b>
                                         Castelluccio M, Poggi G, Sansone C, &lt;i&gt;et al&lt;/i&gt;. Land use classification in remote sensing images by convolutional neural networks[J]. Acta Ecologica Sinica, 2015, 28 (2) : 627-635.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" Zhong Y F, Fei F, Zhang L P. Large patch convolutional neural networks for the scene classification of high spatial resolution imagery[J]. Journal of Applied Remote Sensing, 2016, 10 (2) : 025006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large patch convolutional neural networks for the scene classification of high spatial resolution imagery">
                                        <b>[9]</b>
                                         Zhong Y F, Fei F, Zhang L P. Large patch convolutional neural networks for the scene classification of high spatial resolution imagery[J]. Journal of Applied Remote Sensing, 2016, 10 (2) : 025006.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" Chen Y, Fan R S, Wang J X, &lt;i&gt;et al&lt;/i&gt;. High resolution image classification method combining with minimum noise fraction rotation and convolution neural network[J]. Laser &amp;amp; Optoelectronics Progress, 2017, 54 (10) : 102801.  陈洋, 范荣双, 王竞雪, 等. 结合最小噪声分离变换和卷积神经网络的高分辨影像分类方法[J]. 激光与光电子学进展, 2017, 54 (10) : 102801." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201710056&amp;v=MjYwNDJyUFpMRzRIOWJOcjQ5QVlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGxXN3JQTHk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Chen Y, Fan R S, Wang J X, &lt;i&gt;et al&lt;/i&gt;. High resolution image classification method combining with minimum noise fraction rotation and convolution neural network[J]. Laser &amp;amp; Optoelectronics Progress, 2017, 54 (10) : 102801.  陈洋, 范荣双, 王竞雪, 等. 结合最小噪声分离变换和卷积神经网络的高分辨影像分类方法[J]. 激光与光电子学进展, 2017, 54 (10) : 102801.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" Penatti O A B, Nogueira K, dos Santos J A. Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?[C]. IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2015: 44-51." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?">
                                        <b>[11]</b>
                                         Penatti O A B, Nogueira K, dos Santos J A. Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?[C]. IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2015: 44-51.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" Hu F, Xia G S, Hu J W, &lt;i&gt;et al&lt;/i&gt;. Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery[J]. Remote Sensing, 2015, 7 (11) : 14680-14707." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery">
                                        <b>[12]</b>
                                         Hu F, Xia G S, Hu J W, &lt;i&gt;et al&lt;/i&gt;. Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery[J]. Remote Sensing, 2015, 7 (11) : 14680-14707.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" Yandex A B, Lempitsky V. Aggregating local deep features for image retrieval[C]. IEEE International Conference on Computer Vision, 2015: 1269-1277." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Aggregating local deep features for image retrieval">
                                        <b>[13]</b>
                                         Yandex A B, Lempitsky V. Aggregating local deep features for image retrieval[C]. IEEE International Conference on Computer Vision, 2015: 1269-1277.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition [EB/OL]. (2015-04-10) [2018-09-28].https://arxiv.org/abs/1409.1556</a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" Yang C, Lu X, Lin Z, &lt;i&gt;et al&lt;/i&gt;. High-resolution image inpainting using multi-scale neural patch synthesis[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2017: 4076-4084." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-Resolution Image Inpainting Using Multi-scale Neural Patch Synthesis">
                                        <b>[15]</b>
                                         Yang C, Lu X, Lin Z, &lt;i&gt;et al&lt;/i&gt;. High-resolution image inpainting using multi-scale neural patch synthesis[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2017: 4076-4084.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" Gong Y C, Wang L W, Guo R Q, &lt;i&gt;et al&lt;/i&gt;. Multi-scale orderless pooling of deep convolutional activation features[C]. European Conference on Computer Vision, 2014:392-407." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-scale Orderless Pooling of Deep Convolutional Activation Features">
                                        <b>[16]</b>
                                         Gong Y C, Wang L W, Guo R Q, &lt;i&gt;et al&lt;/i&gt;. Multi-scale orderless pooling of deep convolutional activation features[C]. European Conference on Computer Vision, 2014:392-407.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_17" title=" Zhao B, Zhong Y F, Xia G S, &lt;i&gt;et al&lt;/i&gt;. Dirichlet-derived multiple topic scene classification model for high spatial resolution remote sensing imagery[J]. IEEE Transactions on Geoscience and Remote Sensing, 2016, 54 (4) : 2108-2123." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dirichlet-derived multiple topic scene classification model for high spatial resolution remote sensing imagery">
                                        <b>[17]</b>
                                         Zhao B, Zhong Y F, Xia G S, &lt;i&gt;et al&lt;/i&gt;. Dirichlet-derived multiple topic scene classification model for high spatial resolution remote sensing imagery[J]. IEEE Transactions on Geoscience and Remote Sensing, 2016, 54 (4) : 2108-2123.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_18" title=" Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks [C]. International Conference on Neural Information Processing Systems, 2012: 1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">
                                        <b>[18]</b>
                                         Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks [C]. International Conference on Neural Information Processing Systems, 2012: 1097-1105.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_19" title=" Jia Y, Shelhamer E, Donahue J, &lt;i&gt;et al&lt;/i&gt;. Caffe: Convolutional architecture for fast feature embedding[C]. The ACM International Conference on Multimedia, 2014: 675-678." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffe:Convolutional architecture for fast feature embedding">
                                        <b>[19]</b>
                                         Jia Y, Shelhamer E, Donahue J, &lt;i&gt;et al&lt;/i&gt;. Caffe: Convolutional architecture for fast feature embedding[C]. The ACM International Conference on Multimedia, 2014: 675-678.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_20" title=" Chatfield K, Simonyan K, Vedaldi A, &lt;i&gt;et al&lt;/i&gt;. Return of the devil in the details: delving deep into convolutional nets[EB/OL]. (2014-11-05) [2018-09-28]. https://arxiv.org/abs/1405.3531" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Return of the Devil in the Details:Delving Deep into Convolutional Nets">
                                        <b>[20]</b>
                                         Chatfield K, Simonyan K, Vedaldi A, &lt;i&gt;et al&lt;/i&gt;. Return of the devil in the details: delving deep into convolutional nets[EB/OL]. (2014-11-05) [2018-09-28]. https://arxiv.org/abs/1405.3531
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_21" title=" He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;. Deep residual learning for image recognition[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[21]</b>
                                         He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;. Deep residual learning for image recognition[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 770-778.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-10-29 06:35</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(03),19-29 DOI:10.3788/AOS201939.0301002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>融合全局和局部深度特征的高分辨率遥感影像场景分类方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BE%9A%E5%B8%8C&amp;code=33439244&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">龚希</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E4%BA%AE&amp;code=24062402&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴亮</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%A2%E5%BF%A0&amp;code=24062623&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谢忠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E5%8D%A0%E9%BE%99&amp;code=24438862&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈占龙</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E8%A2%81%E7%BC%98&amp;code=36915870&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘袁缘</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BF%9E%E4%BE%83&amp;code=33510079&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">俞侃</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E5%9C%B0%E8%B4%A8%E5%A4%A7%E5%AD%A6(%E6%AD%A6%E6%B1%89)%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0000038&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国地质大学(武汉)信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%BD%E5%AE%B6%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=1699370&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">国家地理信息系统工程技术研究中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%96%87%E5%8D%8E%E5%AD%A6%E9%99%A2%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%83%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文华学院信息科学与技术学部</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出了一种融合全局和局部深度特征 (GLDFB) 的视觉词袋模型。通过视觉词袋模型将深度卷积神经网络提取的多个层次的高层特征进行重组编码并融合, 利用支持向量机对融合特征进行分类。充分利用包含场景局部细节信息的卷积层特征和包含场景全局信息的全连接层特征, 完成对遥感影像场景的高效表达。通过对两个不同规模的遥感图像场景数据集的实验研究表明, 相比现有方法, 所提方法在高层特征表达能力和分类精度方面具有显著优势。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%A5%E6%84%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遥感;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉词袋模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E9%81%A5%E6%84%9F%E5%BD%B1%E5%83%8F%E5%9C%BA%E6%99%AF%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高分辨率遥感影像场景分类;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *刘袁缘 E-mail:liuyy@cug.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-29</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61602429, 41671400, 41701446, 41871305, 41874009);</span>
                                <span>国家重点研发计划 (2017YFB0503600, 2017YFC0602204, 2018YFB0505500, 2017YFC0602204);</span>
                                <span>湖北自然科学基金 (2015CFA012);</span>
                    </p>
            </div>
                    <h1><b>Classification Method of High-Resolution Remote Sensing Scenes Based on Fusion of Global and Local Deep Features</b></h1>
                    <h2>
                    <span>Gong Xi</span>
                    <span>Wu Liang</span>
                    <span>Xie Zhong</span>
                    <span>Chen Zhanlong</span>
                    <span>Liu Yuanyuan</span>
                    <span>Yu Kan</span>
            </h2>
                    <h2>
                    <span>Department of Information Engineering, China University of Geosciences</span>
                    <span>National Engineering Research Center of Geographic Information System</span>
                    <span>Department of Information Science and Technology, Wenhua College</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A global and local deep feature based (GLDFB) bag-of-visual-words (BoVW) model is proposed. The high-level features extracted from the deep convolutional neural network are reorganized and encoded by the BoVW model and the fusion features are classified by the support vector machine. The features from the convolutional layer containing the local details and the fully-connected layer containing the global information of scenes are fully used and thus the efficient expressions of the remote sensing image scenes are formed. The experimental results on two remote sensing image scene datasets with different scales show that, compared with the existing methods, the proposed method possesses unique advantages in the representation ability and the classification accuracy of high-level features.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=remote%20sensing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">remote sensing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep feature;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=bag-of-visual-words&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">bag-of-visual-words;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=high-resolution%20remote%20sensing%20scene%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">high-resolution remote sensing scene classification;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-08-29</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="53" name="53" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="54">随着遥感传感器技术和制图技术的发展, 大量高分辨率遥感影像被应用于国土规划、工程建设和抢险救灾等领域。高分辨率遥感影像包含丰富的场景语义信息, 但其组成地物的多样性和空间分布的复杂性, 造成语义信息难以被有效提取。遥感影像场景分类是对高分辨率遥感影像的有效解译, 场景分类的核心是场景特征的提取。如何有效地对高分辨率遥感影像场景进行表达及识别是当前面临的极具挑战的课题<citation id="137" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="55">遥感影像场景分类研究已取得一定的发展。早期主要通过全局统计信息 (如全局纹理或颜色直方图) 对场景进行表达, 这类基于低层特征的方法易于计算, 但其精度较低、使用范围小。后来出现了基于中层语义的模型, 如经典的视觉词袋 (bag-of-visual-words, BoVW) 模型<citation id="138" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 以及很多基于BoVW的模型, 如空间金字塔匹配核 (SPMK) <citation id="139" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和空间金字塔共线性核 (SPCK) <citation id="140" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等。但这类方法采用的低层局部特征会丢失信息, 导致分类效果存在一定的局限性。</p>
                </div>
                <div class="p1">
                    <p id="56">近年来深度学习在包括遥感影像分类<citation id="146" type="reference"><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>等诸多领域表现突出。如文献<citation id="141" type="reference">[<a class="sup">7</a>]</citation>提出了离散余弦变换 (DCT) 与深度卷积神经网络 (CNN) 相结合的DCT-CNN方法, 该方法可更高效地提取陆地地貌图像场景中主要目标特征信息的深度特征并对其进行分类;文献<citation id="142" type="reference">[<a class="sup">8</a>]</citation>通过不同的CNN框架证明了CNN对遥感影像场景分类的有效性, 但建立抽象性能优良的CNN需要大量的标签数据<citation id="147" type="reference"><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>, 由于遥感图像人工标记的代价很大, 因此遥感场景样本量往往较小。针对遥感场景下的小样本数据, 一种简单高效的解决办法是借助在自然场景数据集ImageNet上预训练的CNN提取图像特征并对其进行分类。预训练CNN的全连接层特征在高分辨率遥感场景分类中有着突出表现<citation id="148" type="reference"><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>, 它是对图像场景全局信息的高度抽象和高效表达<citation id="143" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 但对局部信息表达却不足。卷积层特征则相反, 它是针对上层输入特征图在感受野范围内通过滑动窗口卷积得到的特征图, 每个元素是图像局部信息抽象的结果<citation id="144" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。不同于自然场景, 遥感场景中的地物对象往往不集中在图像的中间区域而是分散分布, 局部特征对遥感图像场景表达的意义十分重大<citation id="145" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。基于预训练模型提取特征的方法大多侧重于全局信息而忽略了局部信息, 导致对场景的表达能力受限。</p>
                </div>
                <div class="p1">
                    <p id="57">为克服现有方法对小样本高分辨率遥感场景表达的不足, 提升其对高分辨率遥感影像场景的表达能力, 本文提出了一种同时兼顾局部信息和全局信息的高分辨率遥感影像场景分类方法——融合全局和局部深度特征的视觉词袋模型 (Global and Local Deep Features based Bag-of-visual-words, GLDFB) , 通过BoVW将深度卷积神经网络提取的包含场景局部细节信息的卷积层深度特征和包含场景全局信息的全连接层深度特征进行重组并融合, 充分挖掘并运用卷积神经网络特征, 形成对遥感图像场景的高效表达。在获取对图像高效表达的融合特征后, 通过支持向量机 (SVM) 得到最终的分类结果。</p>
                </div>
                <h3 id="58" name="58" class="anchor-tag">2 原理方法</h3>
                <div class="p1">
                    <p id="59">融合特征是由卷积层特征和全连接层特征共同组成的。如图1所示, 由于两种特征的维度不同, 在获取卷积层特征后, 需对其进行重组并通过BoVW编码得到局部特征的分布直方图, 再与全连接层特征进行融合, 从而得到融合特征并对其进行分类。本文以深度卷积神经网络VGG-19 (Visual Geometry Group-19) <citation id="149" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>作为特征提取器为例进行研究。VGG-19的卷积层特征中的图像纹理细节有很强的不变性, 被广泛运用于特征提取<citation id="150" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903002_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 GLDFB流程图" src="Detail/GetImg?filename=images/GXXB201903002_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 GLDFB流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903002_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Flow chart of GLDFB</p>

                </div>
                <div class="p1">
                    <p id="61">文献<citation id="151" type="reference">[<a class="sup">12</a>]</citation>证明在遥感影像场景分类任务中, 包括VGG-19在内的多种卷积神经网络均有第一个全连接层特征的表达效果优于第二个全连接层特征, 因而选取VGG-19的第一个全连接层FC6作为全局特征提取器;而卷积层特征在高分辨率遥感影像场景分类中的研究较少, 先对VGG-19的多个卷积层特征进行对比分析, 当验证卷积层特征的有效性之后, 选定综合表现最优的卷积层作为局部特征提取器。此外, GLDFB是从同一卷积神经网络中提取卷积层特征和全连接层特征, 一次正向运算就可实现两种层次特征的提取, 在充分挖掘并运用卷积神经网络多种特征的同时保证了提取时间最短和计算耗费最少。</p>
                </div>
                <h4 class="anchor-tag" id="62" name="62"><b>2.1 VGG-19特征提取器</b></h4>
                <div class="p1">
                    <p id="63">VGG-19是一种典型的深度卷积神经网络。如图2所示, 它包含16个卷积层、5个最大池化层、3个全连接层和1个softmax分类层。VGG-19采用修正线性单元 (ReLU) 作为激活函数, ReLU保证输出元素值均非负, 经过ReLU激活的特征具有更好的表达效果<citation id="152" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 因此本文使用的特征都经过ReLU函数激活。</p>
                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903002_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 VGG-19的网络结构" src="Detail/GetImg?filename=images/GXXB201903002_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 VGG-19的网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903002_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Network structure of VGG-19</p>

                </div>
                <h4 class="anchor-tag" id="65" name="65"><b>2.2 局部特征的表达</b></h4>
                <div class="p1">
                    <p id="66">表1为VGG-19卷积层的输出特征维度, 其中第<i>l</i>个卷积层CONV<sub><i>l</i></sub>的输出特征维度为<i>d</i><sup><i>l</i></sup>×<i>n</i><sup><i>l</i></sup>×<i>n</i><sup><i>l</i></sup>, <i>n</i><sup><i>l</i></sup>×<i>n</i><sup><i>l</i></sup>为单个特征图的大小, <i>d</i><sup><i>l</i></sup>为特征图数目。这种高维度特征在存储和计算中耗费巨大。通过BoVW对卷积层特征进行编码, 得到图像的视觉词频分布直方图, 从而降低特征维度, 达到对图像局部特征进行表达的目的。实现过程主要有三步:1) 视觉词汇的提取;2) 视觉词典生成;3) 图像局部特征的表达。</p>
                </div>
                <div class="area_img" id="67">
                    <p class="img_tit">表1 VGG-19卷积层的输出特征维度 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Output feature dimensions of VGG-19 convolutional layers</p>
                    <p class="img_note"></p>
                    <table id="67" border="1"><tr><td><br />No.</td><td>Layer name</td><td>Feature size</td></tr><tr><td><br />1</td><td>conv1_1</td><td>64×224× 224</td></tr><tr><td><br />2</td><td>conv1_2</td><td>64×224× 224</td></tr><tr><td><br />3</td><td>conv2_1</td><td>128×112×112</td></tr><tr><td><br />4</td><td>conv2_2</td><td>128×112×112</td></tr><tr><td><br />5</td><td>conv3_1</td><td>256×56×56</td></tr><tr><td><br />6</td><td>conv3_2</td><td>256×56×56</td></tr><tr><td><br />7</td><td>conv3_3</td><td>256×56×56</td></tr><tr><td><br />8</td><td>conv3_4</td><td>256×56×56</td></tr><tr><td><br />9</td><td>conv4_1</td><td>512×28×28</td></tr><tr><td><br />10</td><td>conv4_2</td><td>512×28×28</td></tr><tr><td><br />11</td><td>conv4_3</td><td>512×28×28</td></tr><tr><td><br />12</td><td>conv4_4</td><td>512×28×28</td></tr><tr><td><br />13</td><td>conv5_1</td><td>512×14×14</td></tr><tr><td><br />14</td><td>conv5_2</td><td>512×14×14</td></tr><tr><td><br />15</td><td>conv5_3</td><td>512×14×14</td></tr><tr><td><br />16</td><td>conv5_4</td><td>512×14×14<br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="69" name="69">1) 视觉词汇的提取</h4>
                <div class="p1">
                    <p id="70">由于特征图的每一个元素都是对前一层的一个局部感受野的卷积结果, 因此不同特征图同一位置的元素可视为对输入图像同一局部区域的不同抽象, 将不同特征图同一位置的元素进行抽取排列, 即得到图像局部区域的特征表达, 如图3所示。</p>
                </div>
                <div class="p1">
                    <p id="71">记卷积层CONV<sub><i>l</i></sub>的第<i>k</i> (1≤<i>k</i>≤<i>d</i><sup><i>l</i></sup>) 个特征图为<i>f</i><mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>l</mi></msubsup></mrow></math></mathml>, 该特征图第i行第j列元素为f<mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>l</mi></msubsup></mrow></math></mathml> (i, j) (1≤i≤n<sup>l</sup>, 1≤j≤n<sup>l</sup>) , 则卷积层<i>CONV</i><sub>l</sub>的所有特征图在 (i, j) 位置的元素可重组为如下特征向量:</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903002_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 卷积层特征的重组和编码" src="Detail/GetImg?filename=images/GXXB201903002_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 卷积层特征的重组和编码  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903002_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Reconstruction and coding of convolutional layer features</i></p>

                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">f</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">{</mo><mi>g</mi><msub><mrow></mrow><mrow><mtext>R</mtext><mtext>e</mtext><mtext>L</mtext><mtext>U</mtext></mrow></msub><mo stretchy="false">[</mo><mi>f</mi><msubsup><mrow></mrow><mn>1</mn><mi>l</mi></msubsup><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>, </mo><mi>f</mi><msubsup><mrow></mrow><mn>2</mn><mi>l</mi></msubsup><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>f</mi><msubsup><mrow></mrow><mrow><mi>d</mi><msup><mrow></mrow><mi>l</mi></msup></mrow><mi>l</mi></msubsup><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>, </mo><mspace width="0.25em" /><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">式中:g<sub><i>ReLU</i></sub>为<i>ReLU</i>激活函数;<i>T</i>为转置运算;<b><i>f</i></b><sup><i>l</i></sup> (<i>i</i>, <i>j</i>) 为<i>d</i><sup><i>l</i></sup>维列向量。对卷积层CONV<sub><i>l</i></sub>的所有特征图元素进行重组后得到的特征向量的集合可表示为</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">F</mi><msup><mrow></mrow><mi>l</mi></msup><mo>=</mo><mo stretchy="false">{</mo><mi mathvariant="bold-italic">f</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false"> (</mo><mn>1</mn><mo>, </mo><mn>1</mn><mo stretchy="false">) </mo><mo>, </mo><mi mathvariant="bold-italic">f</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false"> (</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">f</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false"> (</mo><mi>n</mi><msup><mrow></mrow><mi>l</mi></msup><mo>, </mo><mi>n</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">记<i>N</i><sup><i>l</i></sup>=<i>n</i><sup><i>l</i></sup>×<i>n</i><sup><i>l</i></sup>, 则<b><i>F</i></b><sup><i>l</i></sup>∈<b>R</b><sup><i>d</i><sup><i>l</i></sup>×<i>N</i><sup><i>l</i></sup></sup>包含<i>N</i><sup><i>l</i></sup>个<i>d</i><sup><i>l</i></sup>维列向量, 其中<b>R</b>为实数集。将每个列向量作为一个视觉词汇, 则<b><i>F</i></b><sup><i>l</i></sup>为<i>N</i><sup><i>l</i></sup>个视觉词汇的集合。包含<i>m</i>张遥感场景图像的集合<b><i>S</i></b>={<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>m</i></sub>}, 其中<i>s</i><sub><i>i</i></sub> (1≤<i>i</i>≤<i>m</i>) 为<b><i>S</i></b>中第<i>i</i>幅图像, 可通过CONV<sub><i>l</i></sub>层提取<i>N</i><sup><i>l</i></sup>×<i>m</i>个视觉词汇, 记为视觉词汇集<b><i>F</i></b><sup><i>l</i></sup><sub><i>S</i></sub>={<b><i>F</i></b><sup><i>l</i></sup><sub>s<sub>1</sub></sub>, <b><i>F</i></b><sup><i>l</i></sup><sub>s<sub>2</sub></sub>, …, <b><i>F</i></b><sup><i>l</i></sup><sub>s<sub><i>m</i></sub></sub>}, 其中<b><i>F</i></b><sup><i>l</i></sup><sub>s<sub><i>i</i></sub></sub>为在场景图像s<sub><i>i</i></sub>上获取的视觉词汇集合。</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80">2) 视觉词典生成</h4>
                <div class="p1">
                    <p id="81">通过视觉词典可完成图像的视觉词汇集从高维到低维的转换。视觉词典由聚类算法对所有视觉词汇的集合进行聚类产生的聚类中心构成, BoVW模型中常用k-means作为聚类方法, 该方法简单快速且适用性强。k-means对卷积层提取的视觉词汇进行聚类产生<i>K</i>个聚类中心, 将其视为<i>K</i>个视觉单词, 从而构成视觉词典<b><i>D</i></b>={<b><i>C</i></b><sub>1</sub>, <b><i>C</i></b><sub>2</sub>, …, <b><i>C</i></b><sub><i>K</i></sub>}, 其中<b><i>C</i></b><sub><i>i</i></sub>∈<b>R</b><sup><i>d</i><sup><i>l</i></sup></sup> (1≤<i>i</i>≤<i>K</i>) 为第<i>i</i>个视觉单词。视觉词汇间的相似度通过欧几里得距离度量, 卷积层CONV<sub><i>l</i></sub>在 (<i>i</i>, <i>j</i>) 位置的视觉词汇<b><i>f</i></b><sup><i>l</i></sup> (<i>i</i>, <i>j</i>) 与在 (<i>a</i>, <i>b</i>) 位置的视觉词汇<b><i>f</i></b><sup><i>l</i></sup> (<i>a</i>, <i>b</i>) 之间的距离可表示为</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mo stretchy="false">[</mo><mi mathvariant="bold-italic">f</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>, </mo><mi mathvariant="bold-italic">f</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false"> (</mo><mi>a</mi><mo>, </mo><mi>b</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>=</mo><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>d</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></munderover><mo stretchy="false">[</mo></mstyle><mi>f</mi><msubsup><mrow></mrow><mi>k</mi><mi>l</mi></msubsup><mo stretchy="false"> (</mo><mi>a</mi><mo>, </mo><mi>b</mi><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><msubsup><mrow></mrow><mi>k</mi><mi>l</mi></msubsup><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">距离越小, 相似度越大, 越有可能聚为一类。</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84">3) 图像局部特征的表达</h4>
                <div class="p1">
                    <p id="85">根据视觉词典对图像的视觉词汇进行编码, 即可通过视觉单词对图像进行表达。编码过程如下:对于任一视觉词汇, 计算它与视觉词典中所有视觉单词的距离, 找到与其距离最小的视觉单词替代当前视觉词汇, 将图像的视觉词汇替换为视觉单词, 统计每个视觉单词出现的频率, 即可得到表示该图像的视觉单词词频分布直方图, 并表达为<i>K</i>维特征向量, 可表示为</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><mo>=</mo><mo stretchy="false"> (</mo><mi>h</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>h</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>h</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mo> (</mo><mrow><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub></mrow><mrow><mi>Ν</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></mfrac><mo>, </mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></msub></mrow><mrow><mi>Ν</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></mfrac><mo>, </mo><mo>⋯</mo><mo>, </mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>Κ</mi></msub></mrow></msub></mrow><mrow><mi>Ν</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></mfrac></mrow><mo>) </mo></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">式中:<i>h</i><sub><i>i</i></sub>为第<i>i</i>个视觉单词<b><i>C</i></b><sub><i>i</i></sub>出现的频率比例;<i>n</i><sub><b><i>C</i></b><sub><i>i</i></sub></sub>∈[1, <i>N</i><sup><i>l</i></sup>] (1≤<i>i</i>≤<i>K</i>) 为视觉单词<b><i>C</i></b><sub><i>i</i></sub>出现的频率;<i>N</i><sup><i>l</i></sup>为单张图像的视觉词汇总数。其中, <i>K</i>值越大则产生的视觉单词越多, 对图像的表达越细腻, 但过大的<i>K</i>值会导致对场景的过度解析, 造成过拟合, 使得测试精度降低, 同时计算和时间耗费也会大幅增加。对于不同复杂程度的数据集, 能高效表达的<i>K</i>值才是合适的。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88"><b>2.3 全局特征的提取</b></h4>
                <div class="p1">
                    <p id="89">图像全局特征通过全连接层进行提取。全连接层的输出为一个<i>N</i>维特征向量, 可视为由<i>N</i>个1×1的特征图组成:</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Y</mi><mo>=</mo><mi>g</mi><msub><mrow></mrow><mrow><mtext>R</mtext><mtext>e</mtext><mtext>L</mtext><mtext>U</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">X</mi><mo>+</mo><mi mathvariant="bold-italic">b</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">式中:<b><i>Y</i></b>∈<b>R</b><sup><i>N</i>×1×1</sup>为输出特征向量;<b><i>X</i></b>∈<b>R</b><sup><i>d</i><sup><i>l</i></sup>×<i>n</i><sup><i>l</i></sup>×<i>n</i><sup><i>l</i></sup></sup>为输入特征图;<b><i>W</i></b>∈<b>R</b><sup> (<i>d</i><sup><i>l</i></sup>×<i>n</i><sup><i>l</i></sup>×<i>n</i><sup><i>l</i></sup>) ×<i>N</i></sup>为权重;<b><i>b</i></b>∈<b>R</b><sup><i>N</i></sup>为偏置项。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92"><b>2.4 融合特征的提取及分类</b></h4>
                <div class="p1">
                    <p id="93">已提取的局部特征<b><i>H</i></b>和全局特征<b><i>Y</i></b>分别为<i>K</i>维和<i>N</i>维特征向量。在特征融合之前, 需对特征进行规范化。由于<i>H</i>为视觉单直方图, 每个元素表示相应单词的出现词频, 因此对任意元素<i>h</i><sub><i>i</i></sub> (1≤<i>i</i>≤<i>K</i>) 有<i>h</i><sub><i>i</i></sub>∈[0, 1]且sum (<i>h</i><sub>1</sub>, <i>h</i><sub>2</sub>, …, <i>h</i><sub><i>K</i></sub>) =1, 其中sum为求和函数, 而<b><i>Y</i></b>是经过ReLU激活函数处理后的输出, 其任意元素<i>y</i><sub><i>j</i></sub> (1≤<i>j</i>≤<i>N</i>) 满足<i>y</i><sub><i>j</i></sub>∈[0, +∞], 对<b><i>Y</i></b>进行如下规范化:</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ζ</mi><mo>=</mo><mo stretchy="false"> (</mo><mi>z</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>z</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mi mathvariant="bold-italic">Y</mi><mrow><mstyle displaystyle="true"><mo>∑</mo><mi>y</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mrow><mstyle displaystyle="true"><mo>∑</mo><mi>y</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">式中:<b><i>Z</i></b>为输出特征向量, <b><i>Z</i></b>中任意元素<i>z</i><sub><i>j</i></sub> (1≤<i>j</i>≤<i>N</i>) 都有<i>z</i><sub><i>j</i></sub>∈[0, 1]且sum (<i>z</i><sub>1</sub>, <i>z</i><sub>2</sub>, …, <i>z</i><sub><i>N</i></sub>) =1。将<b><i>Z</i></b>作为全局特征表达直方图, 其与局部特征表达直方图<b><i>H</i></b>在直方图层面通过连接得到最终图像的融合特征表达直方图, 记为<b><i>p</i></b>:</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">p</mi><mo>=</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Η</mi><mo>, </mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false"> (</mo><mi>h</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>h</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>h</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>z</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">获取所有<i>m</i>张图像的融合特征集<b><i>P</i></b>={<b><i>p</i></b><sub>1</sub>, <b><i>p</i></b><sub>2</sub>, …, <b><i>p</i></b><sub><i>m</i></sub>}后, 通过直方图交叉核 (HIK) 的SVM对图像的特征表达直方图进行分类:</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><msub><mrow></mrow><mtext>Δ</mtext></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mtext>m</mtext></mstyle><mtext>i</mtext><mtext>n</mtext><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>k</mi><mo>, </mo></mrow></msub><mi>p</mi><msub><mrow></mrow><mrow><mi>j</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">式中<i>p</i><sub><i>i</i>, <i>k</i></sub>是<b><i>p</i></b><sub><i>i</i></sub>的第<i>k</i>个元素。</p>
                </div>
                <h3 id="100" name="100" class="anchor-tag">3 实验与分析</h3>
                <h4 class="anchor-tag" id="101" name="101"><b>3.1 实验设置及数据集</b></h4>
                <div class="p1">
                    <p id="102">实验均采用5折交叉验证方案, 并在载有1块NVIDA GeForce GTX 1060的显卡、Inter©core<sup>TM</sup> i7-6700K CPU@ 4.00 GHz、RAM为16.0 GB的工作站上进行。</p>
                </div>
                <div class="p1">
                    <p id="103">实验数据采用两个不同规模的遥感图像场景数据集 (图4) :1) UC Merced (UCM) 数据集, 选自美国USGS国家城市地图航空遥感影像, 共21类, 每类包含100幅尺寸为256 pixel×256 pixel的遥感场景图像, 空间分辨率为0.3 m;2) SIRI数据集, 选自google earth影像数据, 主要覆盖中国的城市及周边区域, 共12类, 每类包含200幅尺寸为200 pixel×200 pixel的遥感场景图像, 空间分辨率为2 m。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903002_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 遥感场景图像示例。" src="Detail/GetImg?filename=images/GXXB201903002_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 遥感场景图像示例。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903002_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Image examples of remote sensing scene.</p>
                                <p class="img_note"> (a) UCM数据集; (b) SIRI数据集</p>
                                <p class="img_note"> (a) UCM dataset; (b) SIRI dataset</p>

                </div>
                <div class="p1">
                    <p id="105">首先对VGG-19不同卷积层特征的表达效果进行对比分析, 并通过对比其他多种类型特征如方向梯度直方图特征 (HOG) 、尺度不变特征变换特征 (SIFT) 和局部二值模式特征 (LBP) 等来验证卷积层特征的有效性;然后通过与多种前沿方法的对比来验证所提GLDFB方法的有效性和优越性。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106"><b>3.2 局部特征表达能力分析</b></h4>
                <div class="p1">
                    <p id="107">VGG-19共有16层卷积层, 但底层的4层卷积层conv1_1～1_2、conv2_1～2_2过大的特征图导致极大的存储、计算和时间耗费, 研究意义受限。因此, 着重分析另外12层卷积层特征在遥感场景分类任务中的表达能力。根据特征图的抽象程度及大小, 可将其划分为中层 (conv3_1～3_4) 、中高层 (conv4_1～4_4) 和高层 (conv5_1～5_4) 三种类型的卷积层进行分析。</p>
                </div>
                <div class="p1">
                    <p id="108">表2和表3对比了三种类型的卷积层特征在不同<i>K</i>值下的平均精度与几种其他特征的分类精度。当<i>K</i>值较小时, 如<i>K</i>=100时, 中层卷积层特征表现最优;随着<i>K</i>值的增加, 中高层卷积层特征平均精度大幅升高, 超过中层卷积层特征;当UCM和SIRI数据集上的<i>K</i>值分别增加至3000和2000时, 中层卷积层特征的平均精度下降, 而中高层卷积层特征的平均精度仍在增高, 在两个数据集上均达到最高值;高层卷积层特征的平均精度是三类卷积层特征的平均精度中最低的, 但仍远高于表3中HOG、SIFT、LBP和CNN (6conv+2fc) 等特征的分类精度。其中, CNN (6conv+2fc) 特征是从一个由6个卷积层和2个全连接层组成的CNN中提取的全连接层特征。通过对比表2和表3可知, 卷积层特征在高分辨率遥感影像场景分类中更有效, 对场景的表达能力更好, 在三种类型的卷积层特征中, 中高层特征的平均精度最高。</p>
                </div>
                <div class="p1">
                    <p id="109">图5对比了各卷积层特征在不同<i>K</i>值下聚类过程中单次迭代的时间耗费。由于中层、中高层和高层卷积层的特征图大小递减, 而计算耗费与特征图大小呈正相关, 因此三种卷积层特征在单次迭代时间上呈递减趋势, 且<i>K</i>值越小, 卷积层类型层次越高, 时间耗费越少。同时, 由图5可见, 卷积层类型层次越高, <i>K</i>值对时间耗费的影响也越小。总体而言, 中高层和高层卷积层特征在所有<i>K</i>值下的时间耗费均优于中层卷积层特征。结合表2的平均精度和图5的时间耗费, 综合权衡三种类型卷积层特征的平均精度, 发现在UCM和SIRI数据集上, 中高层卷积层特征分别在<i>K</i>=3000和<i>K</i>=2000时的平均精度最高且时间耗费较少, 综合表现最优。图6展示了各卷积层特征在UCM和SIRI数据集中不同<i>K</i>值下的具体分类精度, 具有与表2类似的总体分布趋势。同时, 在UCM和SIRI数据集上, <i>K</i>=3000和<i>K</i>=2000时, 中高层卷积层特征中conv4_1层的特征分类精度最高, 因此选择conv4_1层作为局部特征提取器, 在UCM和SIRI数据集上分别设置<i>K</i>=3000和<i>K</i>=2000。</p>
                </div>
                <div class="area_img" id="110">
                    <p class="img_tit">表2 三种类型卷积层特征在不同<i>K</i>值下的平均分类精度比较 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Average classification accuracy comparison of three kinds of convolutional layer features under different <i>K</i> values</p>
                    <p class="img_note"></p>
                    <table id="110" border="1"><tr><td rowspan="2"><br />Layer type</td><td colspan="5"><br />UCM</td><td colspan="5">SIRI</td></tr><tr><td><i>K</i>=100</td><td><i>K</i>=500</td><td><i>K</i>=1000</td><td><i>K</i>=2000</td><td><i>K</i>=3000</td><td><i>K</i>=100</td><td><i>K</i>=500</td><td><i>K</i>=1000</td><td><i>K</i>=1500</td><td><i>K</i>=2000</td></tr><tr><td>Middle layer</td><td>90.14</td><td>94.24</td><td>94.60</td><td>95.89</td><td>95.42</td><td>91.22</td><td>93.49</td><td>93.91</td><td>94.58</td><td>94.32</td></tr><tr><td><br />Middle-high layer</td><td>89.76</td><td>95.18</td><td>95.42</td><td>95.95</td><td>96.49</td><td>89.48</td><td>93.96</td><td>94.51</td><td>94.91</td><td>95.16</td></tr><tr><td><br />High layer</td><td>88.87</td><td>94.46</td><td>94.94</td><td>95.42</td><td>94.88</td><td>87.80</td><td>92.12</td><td>92.88</td><td>93.65</td><td>93.44</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="111">
                    <p class="img_tit">表3 其他几种特征的分类精度 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Classification accuracies of several other features</p>
                    <p class="img_note"></p>
                    <table id="111" border="1"><tr><td><br />Dataset</td><td colspan="4">UCM</td><td colspan="4">SIRI</td></tr><tr><td>Feature</td><td>HOG</td><td>SIFT</td><td>LBP</td><td>CNN (6conv+2fc) </td><td>HOG</td><td>SIFT</td><td>LBP</td><td>CNN (6conv+2fc) </td></tr><tr><td>Accuracy /%</td><td>52.14</td><td>58.33</td><td>31.43</td><td>63.10</td><td>44.79</td><td>53.96</td><td>46.25</td><td>60.42</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903002_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 12层卷积层特征在不同K值下的聚类过程中单次迭代的时间耗费。" src="Detail/GetImg?filename=images/GXXB201903002_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 12层卷积层特征在不同<i>K</i>值下的聚类过程中单次迭代的时间耗费。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903002_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Time consumption for single iteration in k-means clustering process of 12 convolutional layer features under different <i>K</i> values. </p>
                                <p class="img_note"> (a) UCM数据集; (b) SIRI数据集</p>
                                <p class="img_note"> (a) UCM dataset; (b) SIRI dataset</p>

                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903002_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 12层卷积层特征在不同K值下的分类精度。" src="Detail/GetImg?filename=images/GXXB201903002_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 12层卷积层特征在不同<i>K</i>值下的分类精度。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903002_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Classification accuracies of 12 convolutional layer features under different <i>K</i> values. </p>
                                <p class="img_note"> (a) UCM数据集; (b) SIRI数据集</p>
                                <p class="img_note"> (a) UCM dataset; (b) SIRI dataset</p>

                </div>
                <h4 class="anchor-tag" id="114" name="114"><b>3.3 融合策略有效性分析</b></h4>
                <div class="p1">
                    <p id="115">GLDFB将conv4_1局部特征和FC6全局特征进行融合并分类, 最终结果如表4所示。对比单独使用conv4_1局部特征和FC6全局特征分类的结果 (表4中第1～2行) , GLDFB有效地融合了局部特征和全局特征, 在UCM和SIRI数据集上将分类精度分别提高至97.62%和96.67%。该融合方法对其他特征同样适用, 将GLDFB中conv4_1局部特征和FC6全局特征替换为SIFT+HOG和SIFT+FC6两种特征组合模式 (表4中第3～4行) , 其他部分保持不变。SIFT+HOG模式下的分类精度达到73.81%, 相比表3中HOG和SIFT特征单独分类的结果 (分别为52.14%和58.33%) 有非常大的提升, SIFT+FC6模式下的分类精度相较SIFT和FC6特征单独分类结果亦有一定的提升。由此证明了该方法中特征融合策略的有效性。</p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit">表4 多种特征的分类精度对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Classification accuracy comparison of many kinds of features</p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td rowspan="2"><br />No.</td><td rowspan="2">Feature</td><td colspan="2"><br />Accuracy /%</td></tr><tr><td><br />UCM</td><td>SIRI</td></tr><tr><td><br />1</td><td>FC6</td><td>94.60</td><td>93.54</td></tr><tr><td><br />2</td><td>conv4_1</td><td>96.90</td><td>95.63</td></tr><tr><td><br />3</td><td>SIFT+HOG</td><td>73.81</td><td>67.92</td></tr><tr><td><br />4</td><td>SIFT+FC6</td><td>95.00</td><td>95.00</td></tr><tr><td><br />5</td><td>GLDFB (conv4_1+FC6) </td><td>97.62</td><td>96.67</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="117" name="117"><b>3.4 GLDFB实验结果及分析</b></h4>
                <div class="p1">
                    <p id="118">结合conv4_1局部特征提取器和FC6全局特征提取器, GLDFB在UCM和SIRI数据集上的表现如下:</p>
                </div>
                <div class="p1">
                    <p id="119">1) UCM数据集:表5对比了几种前沿方法与本文所提GLDFB方法在UCM数据集上的分类精度。基于SIFT特征的BoVW方法的分类精度仅达76.81%;大多数深度学习类方法的分类精度可达90%以上 (表5中第6～8行) , 但使用层数较多的VGG-19和Resnet50模型框架训练方法的分类精度仅达83.48%和85.71%, 这是由于两者参数较多而数据集样本数量较少导致训练不完全。在同等训练数据量下, GLDFB借助预训练的深度卷积网络提取场景局部信息与全局信息的高层次特征并进行融合, 在降低对训练数据量需求的同时, 汲取了深度特征对图像语义高效表达的优点, 将分类精度提升至97.62%。</p>
                </div>
                <div class="p1">
                    <p id="120">图7的混淆矩阵显示19类场景的分类精度达95%及以上, 其中13类场景分类精度达100%, 包括目标简单特征明显的飞机、沙滩、停车场等场景类别, 及部分目标复杂或与其他场景极为相似的活动房区、公路等场景类别。分类精度未达100%的场景类别间有较高的相似度, 可划分为道路类和房屋类, 如图8所示, 道路类中场景的主要差别在于道路的数目、走向和高度, 房屋类中场景的主要差别则在于房屋密度和屋顶材料。由于其高层特征表达具有较高的相似度, GLDFB将图8中高架桥划分为公路, 将建筑和中等住宅区划分为密集住宅区, 可见GLDFB对这两大类场景的区分能力仍有一定的提升空间。尽管如此, 两大类场景中最低的楼房的分类精度也达到90%, 其他场景的分类精度均达95%～100%, 总体平均精度超过95%。总体而言, GLDFB对背景特征单一的简单场景和特征复杂、差别微小的复杂场景都有较好的表达能力, 可获得较高的分类精度, 且对简单场景的区分度略优于对复杂场景及相似场景的区分度。</p>
                </div>
                <div class="area_img" id="121">
                    <p class="img_tit">表5 UCM数据集上的分类精度比较 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Classification accuracy comparison on UCM dataset</p>
                    <p class="img_note"></p>
                    <table id="121" border="1"><tr><td><br />No.</td><td>Method</td><td>Accuracy /%</td></tr><tr><td><br />1</td><td>RF</td><td>44.77</td></tr><tr><td><br />2</td><td>SIFT+BoVW</td><td>76.81</td></tr><tr><td><br />3</td><td>SPCK<sup>[4]</sup></td><td>77.38</td></tr><tr><td><br />4</td><td>VGG-19 (training from scratch) </td><td>83.48</td></tr><tr><td><br />5</td><td>Resnet50 (training from scratch) </td><td>85.71</td></tr><tr><td><br />6</td><td>CaffeNet<sup>[11]</sup></td><td>93.42±1.00</td></tr><tr><td><br />7</td><td>DCT-CNN<sup>[7]</sup></td><td>95.76</td></tr><tr><td><br />8</td><td>GLDFB</td><td>97.62</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="123">2) SIRI数据集:表6对比了所提GLDFB方法与其他几种方法在SIRI数据集上的分类结果。通过随机森林分类仅可获得49.90%的精度;基于传统中低层特征方法的分类精度有所提高 (表6中第2～3行) , 但这类方法的分类精度难以超过80%。所提GLDFB方法可自动学习场景图像中局部信息与全局信息的高层特征并进行融合, 得到表达能力更强的深度特征, 在SIRI数据集上的分类精度提升至96.67%。而在同等数据量下直接使用VGG-19和Resnet50模型框架训练方法的分类结果仅达86.13%和89.26%。从图9的混淆矩阵可观察到GLDFB对所有类别场景的分类精度均高于90%, 其中农田、水域等场景类别的分类精度可达100%。特别地, 在存在二义性的商业区和住宅区的分类精度也达到100%, 可见GLDFB对这类组成单一的简单场景和差别微小的复杂场景都有一定的区分能力。同时, GLDFB方法对于裸地、河流等场景的分类精度降低至92.5%, 其中河流类中部分带有桥的场景图像被划分为高架桥, 另一些则被划分成同样带有水的港口, 可见GLDFB方法对这类组成成分复杂的场景的分类精度仍需进一步提升。除此之外, 大部分场景的分类精度在95%及以上, GLDFB总体能较好的表达不同复杂程度的场景。</p>
                </div>
                <div class="area_img" id="124">
                    <p class="img_tit">表6 SIRI数据集上的分类精度比较 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 6 Classification accuracy comparison on SIRI dataset</p>
                    <p class="img_note"></p>
                    <table id="124" border="1"><tr><td><br />No.</td><td>Method</td><td>Accuracy /%</td></tr><tr><td><br />1</td><td>RF</td><td>49.90</td></tr><tr><td><br />2</td><td>SIFT+BoVW</td><td>75.63</td></tr><tr><td><br />3</td><td>SPMK<sup>[3]</sup></td><td>77.69±1.01</td></tr><tr><td><br />4</td><td>VGG-19 (training from scratch) </td><td>86.13</td></tr><tr><td><br />5</td><td>MeanStd-SIFI+LDA-H<sup>[17]</sup></td><td>86.29</td></tr><tr><td><br />6</td><td>Resnet50 (training from scratch) </td><td>89.26</td></tr><tr><td><br />7</td><td>GLDFB</td><td>96.67</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="126">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903002_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 GLDFB在UCM数据集上的分类混淆矩阵" src="Detail/GetImg?filename=images/GXXB201903002_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 GLDFB在UCM数据集上的分类混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903002_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Classification confusion matrix of GLDFB on UCM dataset</p>

                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903002_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 两大类易混淆场景。" src="Detail/GetImg?filename=images/GXXB201903002_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 两大类易混淆场景。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903002_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Two kinds of misclassified scenes. </p>
                                <p class="img_note"> (a) 道路类; (b) 建筑类</p>
                                <p class="img_note"> (a) Road type; (b) building type</p>

                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903002_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 GLDFB在SIRI数据集上的分类混淆矩阵" src="Detail/GetImg?filename=images/GXXB201903002_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 GLDFB在SIRI数据集上的分类混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903002_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Classification confusion matrix of GLDFB on SIRI dataset</p>

                </div>
                <div class="p1">
                    <p id="129">上述实验证明了GLDFB的有效性和优越性, GLDFB同样适用于其他数据。不同复杂程度的数据集有不同的最优<i>K</i>值, 但通过在UCM和SIRI数据集上的实验发现, 在未取得最优<i>K</i>值时卷积层特征的表现虽有降低, 但仍远高于其他传统方法。因此, 直接设定<i>K</i>=2000, 测试GLDFB在其他数据集的分类效果。以USGS数据库中美国俄亥俄州蒙哥马利地区的影像[图10 (a) ]进行实验, 该影像尺寸为10 000 pixel×9 000 pixel, 空间分辨率为0.6 m, 包含居民地、农场、树林和停车场4类场景。从该影像中为每类场景采集50幅150 pixel×150 pixel的场景图像作为训练样本。最终GLDFB对整幅影像的预测结果如图10 (b) 所示, 可观察到预测类别基本与实际类别一致, 能正确地反映出该地区各类区域的分布情况, 可见GLDFB能较好地解析该高分辨率影像。</p>
                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903002_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 GLDFB结果。" src="Detail/GetImg?filename=images/GXXB201903002_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 GLDFB结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903002_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 GLDFB results.</p>
                                <p class="img_note"> (a) USGS大幅遥感影像; (b) 分类结果</p>
                                <p class="img_note"> (a) USGS large remote sensing image; (b) classification result</p>

                </div>
                <h4 class="anchor-tag" id="131" name="131"><b>3.5 迁移实验</b></h4>
                <div class="p1">
                    <p id="132">所提GLDFB方法可扩展到其他预训练CNN上。以UCM数据集为例, 在多种预训练CNN下进行简单实验, 其中局部特征提取器直接选用当前CNN的任一中间层卷积层, 全局特征提取器选用当前CNN的第一个全连接层, <i>K</i>值设为2000。表7列出了GLDFB应用到其他预训练CNN的分类结果, 所有融合特征的分类结果相较单独使用局部特征或全局特征的分类结果都有一定的提升, 证明了GLDFB在各种预训练CNN下都是适用的且表现较好。相较VGG-19模型, Resnet50和Resnet101模型的网络更深、结构更优化, 这两者应用在GLDFB的表现也略优于VGG-19, 更优于直接通过Resnet50训练模型的分类结果 (表5) 。</p>
                </div>
                <div class="area_img" id="133">
                    <p class="img_tit">表7 其他预训练CNN下GLDFB的分类结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 7 Classification results of GLDFB with other pre-training CNNs</p>
                    <p class="img_note"></p>
                    <table id="133" border="1"><tr><td rowspan="2"><br />Pre-training model</td><td rowspan="2">Local feature extraction layer</td><td colspan="3"><br />Accuracy /%</td></tr><tr><td><br />Local feature</td><td>Global feature</td><td>Fused feature</td></tr><tr><td><br />Alexnet<sup>[18]</sup></td><td>conv3</td><td>93.81</td><td>95.24</td><td>96.91</td></tr><tr><td><br />Caffenet<sup>[19]</sup></td><td>conv3</td><td>94.05</td><td>96.90</td><td>97.62</td></tr><tr><td><br />VGG-F<sup>[20]</sup></td><td>conv3</td><td>95.24</td><td>96.19</td><td>97.62</td></tr><tr><td><br />VGG-M<sup>[20]</sup></td><td>conv3</td><td>95.00</td><td>96.43</td><td>97.62</td></tr><tr><td><br />VGG-S<sup>[20]</sup></td><td>conv3</td><td>93.81</td><td>96.43</td><td>96.67</td></tr><tr><td><br />VGG-16<sup>[14]</sup></td><td>conv4_1</td><td>95.00</td><td>96.19</td><td>95.95</td></tr><tr><td><br />Resnet50<sup>[21]</sup></td><td>Res3a</td><td>95.71</td><td>96.90</td><td>97.86</td></tr><tr><td><br />Resnet101<sup>[21]</sup></td><td>Res3a</td><td>95.23</td><td>96.90</td><td>97.86</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="134" name="134" class="anchor-tag">4 结 论</h3>
                <div class="p1">
                    <p id="135">提出了一种同时兼顾局部信息和全局信息的遥感影像场景分类方法——融合全局和局部深度特征的视觉词袋模型, 该模型利用卷积神经网络提取的高层特征对遥感图像的语义信息进行表达, 并通过视觉词袋模型对高维度卷积层特征进行编码并融合全连接层特征, 形成的多层次融合特征同时兼顾了局部信息和全局信息, 能对高分辨率遥感影像场景进行高效表达, 提高了分类精度。</p>
                </div>
                <div class="p1">
                    <p id="136">GLDFB无需大量的标签数据进行复杂的模型训练, 只需一次正向计算即可获取多个层次的特征。相较传统方法, GLDFB提取的特征抽象程度及表达能力得到了极大地提升;相较其他针对高分辨率遥感场景数据的深度学习类方法, GLDFB无需对小样本数据集进行额外的数据增强, 避免了大样本数据集产生的过大计算复杂度和对硬件设施的高要求。多个高分辨率遥感影像场景的实验表明:该方法可提升特征表达能力和分类精度。未来将重点研究多种模型的特征结合, 以及融合特征的有效降维, 以获取区分度和表达能力更强的特征。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised feature learning for aerial scene classification">

                                <b>[1]</b> Cheriyadat A M. Unsupervised feature learning for aerial scene classification[J]. IEEE Transactions on Geoscience and Remote Sensing, 2014, 52 (1) : 439-451.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual categorization with bags of keypoints">

                                <b>[2]</b> Csurka G. Visual categorization with bags of keypoints[C]. European Conference on Computer Vision, 2004: 1-22.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond bags of features:Spatial pyramid matching for recognizing natural scene categories">

                                <b>[3]</b> Lazebnik S, Schmid C, Ponce J. Beyond bags of features: spatial pyramid matching for recognizing natural scene categories[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2006: 2169-2178.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid co-occurrence for image classification">

                                <b>[4]</b> Yang Y, Newsam S. Spatial pyramid co-occurrence for image classification[C]. International Conference on Computer Vision, 2011: 1465-1472.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201802058&amp;v=MDY0NjdlWmVWdUZ5SGxXN3JQTHlyUFpMRzRIOW5Nclk5QWJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Fang X, Wang G H, Yang H C, <i>et al</i>. High resolution remote sensing image classification combining with mean-shift segmentation and fully convolution neural network[J]. Laser &amp; Optoelectronics Progress, 2018, 55 (2) : 022802.  方旭, 王光辉, 杨化超, 等. 结合均值漂移分割与全卷积神经网络的高分辨遥感影像分类[J]. 激光与光电子学进展, 2018, 55 (2) : 022802.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604039&amp;v=MTY1NTRIbFc3clBJalhUYkxHNEg5Zk1xNDlHYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Liu D W, Han L, Han X Y. High spatial resolution remote sensing image classification based on deep learning[J]. Acta Optica Sinica, 2016, 36 (4) : 0428001.  刘大伟, 韩玲, 韩晓勇. 基于深度学习的高分辨率遥感影像分类研究[J]. 光学学报, 2016, 36 (4) : 0428001.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806037&amp;v=MTUyNThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIbFc3clBJalhUYkxHNEg5bk1xWTlHWTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Liu F, Lu L X, Huang G W, <i>et al</i>. Landform image classification based on discrete cosine transformation and deep network [J]. Acta Optica Sinica, 2018, 38 (6) : 0620001. 刘芳, 路丽霞, 黄光伟, 等. 基于离散余弦变换和深度网络的地貌图像分类[J]. 光学学报, 2018, 38 (6) : 0620001.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Land use classification in remote sensing images by convolutional neural networks">

                                <b>[8]</b> Castelluccio M, Poggi G, Sansone C, <i>et al</i>. Land use classification in remote sensing images by convolutional neural networks[J]. Acta Ecologica Sinica, 2015, 28 (2) : 627-635.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large patch convolutional neural networks for the scene classification of high spatial resolution imagery">

                                <b>[9]</b> Zhong Y F, Fei F, Zhang L P. Large patch convolutional neural networks for the scene classification of high spatial resolution imagery[J]. Journal of Applied Remote Sensing, 2016, 10 (2) : 025006.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201710056&amp;v=MDQ1NjBMeXJQWkxHNEg5Yk5yNDlBWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIbFc3clA=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Chen Y, Fan R S, Wang J X, <i>et al</i>. High resolution image classification method combining with minimum noise fraction rotation and convolution neural network[J]. Laser &amp; Optoelectronics Progress, 2017, 54 (10) : 102801.  陈洋, 范荣双, 王竞雪, 等. 结合最小噪声分离变换和卷积神经网络的高分辨影像分类方法[J]. 激光与光电子学进展, 2017, 54 (10) : 102801.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?">

                                <b>[11]</b> Penatti O A B, Nogueira K, dos Santos J A. Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?[C]. IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2015: 44-51.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery">

                                <b>[12]</b> Hu F, Xia G S, Hu J W, <i>et al</i>. Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery[J]. Remote Sensing, 2015, 7 (11) : 14680-14707.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Aggregating local deep features for image retrieval">

                                <b>[13]</b> Yandex A B, Lempitsky V. Aggregating local deep features for image retrieval[C]. IEEE International Conference on Computer Vision, 2015: 1269-1277.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition [EB/OL]. (2015-04-10) [2018-09-28].https://arxiv.org/abs/1409.1556
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-Resolution Image Inpainting Using Multi-scale Neural Patch Synthesis">

                                <b>[15]</b> Yang C, Lu X, Lin Z, <i>et al</i>. High-resolution image inpainting using multi-scale neural patch synthesis[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2017: 4076-4084.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-scale Orderless Pooling of Deep Convolutional Activation Features">

                                <b>[16]</b> Gong Y C, Wang L W, Guo R Q, <i>et al</i>. Multi-scale orderless pooling of deep convolutional activation features[C]. European Conference on Computer Vision, 2014:392-407.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dirichlet-derived multiple topic scene classification model for high spatial resolution remote sensing imagery">

                                <b>[17]</b> Zhao B, Zhong Y F, Xia G S, <i>et al</i>. Dirichlet-derived multiple topic scene classification model for high spatial resolution remote sensing imagery[J]. IEEE Transactions on Geoscience and Remote Sensing, 2016, 54 (4) : 2108-2123.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">

                                <b>[18]</b> Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks [C]. International Conference on Neural Information Processing Systems, 2012: 1097-1105.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffe:Convolutional architecture for fast feature embedding">

                                <b>[19]</b> Jia Y, Shelhamer E, Donahue J, <i>et al</i>. Caffe: Convolutional architecture for fast feature embedding[C]. The ACM International Conference on Multimedia, 2014: 675-678.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Return of the Devil in the Details:Delving Deep into Convolutional Nets">

                                <b>[20]</b> Chatfield K, Simonyan K, Vedaldi A, <i>et al</i>. Return of the devil in the details: delving deep into convolutional nets[EB/OL]. (2014-11-05) [2018-09-28]. https://arxiv.org/abs/1405.3531
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[21]</b> He K M, Zhang X Y, Ren S Q, <i>et al</i>. Deep residual learning for image recognition[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 770-778.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201903002" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201903002&amp;v=MzA1ODZPZVplVnVGeUhsVzdyTUlqWFRiTEc0SDlqTXJJOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

