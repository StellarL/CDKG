

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133833898721250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201908016%26RESULT%3d1%26SIGN%3dqKXPxOIy4EfZjhytS1VeqS1zj1Q%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201908016&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201908016&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201908016&amp;v=MjI0MTA1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlua1ViN0JJalhUYkxHNEg5ak1wNDlFWW9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#88" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="2 图像预处理算法原理 ">2 图像预处理算法原理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#94" data-title="&lt;b&gt;2.1 二维K-L变换&lt;/b&gt;"><b>2.1 二维K-L变换</b></a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;2.2 多尺度形态学变换&lt;/b&gt;"><b>2.2 多尺度形态学变换</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#109" data-title="3 网络结构 ">3 网络结构</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#110" data-title="&lt;b&gt;3.1 可变形卷积&lt;/b&gt;"><b>3.1 可变形卷积</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;3.2 DB-DC模型&lt;/b&gt;"><b>3.2 DB-DC模型</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;3.3 多尺度空洞卷积模型&lt;/b&gt;"><b>3.3 多尺度空洞卷积模型</b></a></li>
                                                <li><a href="#139" data-title="&lt;b&gt;3.4 带有Attention模型的反卷积网络&lt;/b&gt;"><b>3.4 带有Attention模型的反卷积网络</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;3.5 自适应尺度信息的血管分割网络&lt;/b&gt;"><b>3.5 自适应尺度信息的血管分割网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#154" data-title="4 实验结果分析与讨论 ">4 实验结果分析与讨论</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#155" data-title="&lt;b&gt;4.1 实验环境与数据集&lt;/b&gt;"><b>4.1 实验环境与数据集</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;4.2 实验数据预处理&lt;/b&gt;"><b>4.2 实验数据预处理</b></a></li>
                                                <li><a href="#165" data-title="&lt;b&gt;4.3 训练参数设置&lt;/b&gt;"><b>4.3 训练参数设置</b></a></li>
                                                <li><a href="#167" data-title="&lt;b&gt;4.4 不同算法血管分割结果主观分析&lt;/b&gt;"><b>4.4 不同算法血管分割结果主观分析</b></a></li>
                                                <li><a href="#174" data-title="&lt;b&gt;4.5 不同算法血管分割结果细节分析&lt;/b&gt;"><b>4.5 不同算法血管分割结果细节分析</b></a></li>
                                                <li><a href="#179" data-title="&lt;b&gt;4.6 视网膜分割评价标准&lt;/b&gt;"><b>4.6 视网膜分割评价标准</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#197" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#101" data-title="图1 经K-L变换后彩色坐标空间形成的点簇">图1 经K-L变换后彩色坐标空间形成的点簇</a></li>
                                                <li><a href="#115" data-title="图2 3&#215;3的正常卷积与可变形卷积示意图">图2 3×3的正常卷积与可变形卷积示意图</a></li>
                                                <li><a href="#123" data-title="图3 可变形卷积特征提取过程">图3 可变形卷积特征提取过程</a></li>
                                                <li><a href="#129" data-title="图4 密集可变形卷积模型内部结构">图4 密集可变形卷积模型内部结构</a></li>
                                                <li><a href="#138" data-title="图5 多尺度空洞卷积">图5 多尺度空洞卷积</a></li>
                                                <li><a href="#147" data-title="图6 AGs内部结构图">图6 AGs内部结构图</a></li>
                                                <li><a href="#148" data-title="图7 兼容性张量维度计算图">图7 兼容性张量维度计算图</a></li>
                                                <li><a href="#152" data-title="图8 自适应尺度信息的U型网络架构模型">图8 自适应尺度信息的U型网络架构模型</a></li>
                                                <li><a href="#162" data-title="图9 各阶段预处理图像。">图9 各阶段预处理图像。</a></li>
                                                <li><a href="#163" data-title="图10 血管局部块状信息图。">图10 血管局部块状信息图。</a></li>
                                                <li><a href="#172" data-title="图11 不同算法视网膜血管分割结果。">图11 不同算法视网膜血管分割结果。</a></li>
                                                <li><a href="#177" data-title="图12 深度学习分割算法性能比较。">图12 深度学习分割算法性能比较。</a></li>
                                                <li><a href="#186" data-title="表1 基于U型架构的不同网络结构性能比较">表1 基于U型架构的不同网络结构性能比较</a></li>
                                                <li><a href="#190" data-title="表2 DRIVE数据库视网膜血管分割结果">表2 DRIVE数据库视网膜血管分割结果</a></li>
                                                <li><a href="#191" data-title="表3 STARE数据库视网膜血管分割结果">表3 STARE数据库视网膜血管分割结果</a></li>
                                                <li><a href="#195" data-title="图13 本文算法的ROC曲线。">图13 本文算法的ROC曲线。</a></li>
                                                <li><a href="#196" data-title="图14 不同算法的&lt;i&gt;F&lt;/i&gt;&lt;sub&gt;measure&lt;/sub&gt;。">图14 不同算法的<i>F</i><sub>measure</sub>。</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" title=" Fraz M M, Remagnino P, Hoppe A, &lt;i&gt;et al&lt;/i&gt;.Blood vessel segmentation methodologies in retinal images:a survey[J].Computer Methods and Programs in Biomedicine, 2012, 108 (1) :407-433." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300239047&amp;v=MDY2MDJLN0h0RE9ySTlGWnVnR0RIZytvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lJVjhXYXhzPU5pZk9mYg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Fraz M M, Remagnino P, Hoppe A, &lt;i&gt;et al&lt;/i&gt;.Blood vessel segmentation methodologies in retinal images:a survey[J].Computer Methods and Programs in Biomedicine, 2012, 108 (1) :407-433.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" Zhu C Z, Zou B J, Zhao R C, &lt;i&gt;et al&lt;/i&gt;.Retinal vessel segmentation in colour fundus images using Extreme Learning Machine[J].Computerized Medical Imaging and Graphics, 2017, 55:68-77." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Retinal vessel segmentation in colour fundus images using extreme learning machine">
                                        <b>[2]</b>
                                         Zhu C Z, Zou B J, Zhao R C, &lt;i&gt;et al&lt;/i&gt;.Retinal vessel segmentation in colour fundus images using Extreme Learning Machine[J].Computerized Medical Imaging and Graphics, 2017, 55:68-77.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" title=" Liang L M, Liu B W, Yang H L, &lt;i&gt;et al&lt;/i&gt;.Supervised blood vessel extraction in retinal images based on multiple feature fusion[J].Chinese Journal of Computers, 2018, 41 (11) :2566-2580.梁礼明, 刘博文, 杨海龙, 等.基于多特征融合的有监督视网膜血管提取[J].计算机学报, 2018, 41 (11) :2566-2580." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201811010&amp;v=MjE0OTJybzlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlua1ViN0JMejdCZHJHNEg5bk4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Liang L M, Liu B W, Yang H L, &lt;i&gt;et al&lt;/i&gt;.Supervised blood vessel extraction in retinal images based on multiple feature fusion[J].Chinese Journal of Computers, 2018, 41 (11) :2566-2580.梁礼明, 刘博文, 杨海龙, 等.基于多特征融合的有监督视网膜血管提取[J].计算机学报, 2018, 41 (11) :2566-2580.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" title=" Memari N, Ramli A R, Bin Saripan M I, &lt;i&gt;et al&lt;/i&gt;.Supervised retinal vessel segmentation from color fundus images based on matched filtering and AdaBoost classifier[J].PLoS ONE, 2017, 12 (12) :e0188939." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised retinal vessel segmentation from color fundus images based on matched filtering and AdaBoost classifier">
                                        <b>[4]</b>
                                         Memari N, Ramli A R, Bin Saripan M I, &lt;i&gt;et al&lt;/i&gt;.Supervised retinal vessel segmentation from color fundus images based on matched filtering and AdaBoost classifier[J].PLoS ONE, 2017, 12 (12) :e0188939.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" title=" Rousson M, Paragios N.Prior knowledge, level set representations &amp;amp; visual grouping[J].International Journal of Computer Vision, 2008, 76 (3) :231-243." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002831189&amp;v=MzI0ODlNSDdSN3FlYnVkdEZTamxVcjdLSVZjPU5qN0Jhck80SHRIT3A0eEVaZU1HWTNrNXpCZGg0ajk5U1hxUnJ4b3hj&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Rousson M, Paragios N.Prior knowledge, level set representations &amp;amp; visual grouping[J].International Journal of Computer Vision, 2008, 76 (3) :231-243.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" title=" Liang L M, Huang C L, Shi F, &lt;i&gt;et al&lt;/i&gt;.Retinal vessel segmentation using level set combined with shape priori[J].Chinese Journal of Computers, 2018, 41 (7) :1678-1692.梁礼明, 黄朝林, 石霏, 等.融合形状先验的水平集眼底图像血管分割[J].计算机学报, 2018, 41 (7) :1678-1692." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201807013&amp;v=MDc0NTNJOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5rVWI3Qkx6N0Jkckc0SDluTXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Liang L M, Huang C L, Shi F, &lt;i&gt;et al&lt;/i&gt;.Retinal vessel segmentation using level set combined with shape priori[J].Chinese Journal of Computers, 2018, 41 (7) :1678-1692.梁礼明, 黄朝林, 石霏, 等.融合形状先验的水平集眼底图像血管分割[J].计算机学报, 2018, 41 (7) :1678-1692.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" Wu C Y, Yi B S, Zhang Y G, &lt;i&gt;et al&lt;/i&gt;.Retinal vessel image segmentation based on improved convolutional neural network[J].Acta Optica Sinica, 2018, 38 (11) :1111004.吴晨玥, 易本顺, 章云港, 等.基于改进卷积神经网络的视网膜血管图像分割[J].光学学报, 2018, 38 (11) :1111004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811016&amp;v=MDg5NTJybzlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlua1ViN0JJalhUYkxHNEg5bk4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Wu C Y, Yi B S, Zhang Y G, &lt;i&gt;et al&lt;/i&gt;.Retinal vessel image segmentation based on improved convolutional neural network[J].Acta Optica Sinica, 2018, 38 (11) :1111004.吴晨玥, 易本顺, 章云港, 等.基于改进卷积神经网络的视网膜血管图像分割[J].光学学报, 2018, 38 (11) :1111004.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" title=" Fu H Z, Xu Y W, Wong D W K, &lt;i&gt;et al&lt;/i&gt;.Retinal vessel segmentation via deep learning network and fully-connected conditional random fields[C]//2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI) , April 13-16, 2016, Prague, Czech Republic.New York:IEEE, 2016:698-701." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Retinal vessel segmentation via deep learning network and fully-connected conditional random fields">
                                        <b>[8]</b>
                                         Fu H Z, Xu Y W, Wong D W K, &lt;i&gt;et al&lt;/i&gt;.Retinal vessel segmentation via deep learning network and fully-connected conditional random fields[C]//2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI) , April 13-16, 2016, Prague, Czech Republic.New York:IEEE, 2016:698-701.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" Gao X R, Cai Y H, Qiu C Y, &lt;i&gt;et al&lt;/i&gt;.Retinal blood vessel segmentation based on the Gaussian matched filter and U-net[C]//2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI) , October 14-16, 2017, Shanghai.New York:IEEE, 2017:17582703." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Retinal blood vessel segmentation based on the Gaussian matched filter and U-net">
                                        <b>[9]</b>
                                         Gao X R, Cai Y H, Qiu C Y, &lt;i&gt;et al&lt;/i&gt;.Retinal blood vessel segmentation based on the Gaussian matched filter and U-net[C]//2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI) , October 14-16, 2017, Shanghai.New York:IEEE, 2017:17582703.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     Liang L M, Sheng X Q, Guo K, &lt;i&gt;et al&lt;/i&gt;.Improved U-Net retinal vessels segmentation [J/OL].Application Research of Computers, 2019, 37 (4) [2019-02-16].https://doi.org/10.19734/j.issn.1001-3695.2018.09.0775.梁礼明, 盛校棋, 郭凯, 等.基于改进的U-Net眼底视网膜血管分割[J/OL].计算机应用研究, 2019, 37 (4) [2019-02-16].https://doi.org/10.19734/j.issn.1001-3695.2018.09.0775.</a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" title=" Ronneberger O, Fischer P, Brox T.U-Net:convolutional networks for biomedical image segmentation[M]//Navab N, Hornegger J, Wells W, &lt;i&gt;et al&lt;/i&gt;.Medical image computing and computer-assisted intervention-MICCAI 2015.Lecture notes in computer science.Cham:Springer, 2015, 9351:234-241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-Net:convolutional networks for biomedical image segmentation">
                                        <b>[11]</b>
                                         Ronneberger O, Fischer P, Brox T.U-Net:convolutional networks for biomedical image segmentation[M]//Navab N, Hornegger J, Wells W, &lt;i&gt;et al&lt;/i&gt;.Medical image computing and computer-assisted intervention-MICCAI 2015.Lecture notes in computer science.Cham:Springer, 2015, 9351:234-241.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" title=" Morales S, Naranjo V, Angulo J, &lt;i&gt;et al&lt;/i&gt;.Automatic detection of optic disc based on PCA and mathematical morphology[J].IEEE Transactions on Medical Imaging, 2013, 32 (4) :786-796." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic detection of optic disc based on pca and mathematical morphology">
                                        <b>[12]</b>
                                         Morales S, Naranjo V, Angulo J, &lt;i&gt;et al&lt;/i&gt;.Automatic detection of optic disc based on PCA and mathematical morphology[J].IEEE Transactions on Medical Imaging, 2013, 32 (4) :786-796.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" title=" Wang H Y, Tong Q, Lian Z P, &lt;i&gt;et al&lt;/i&gt;.K-L transform optimization algorithm for measurement matrix[J].Computer Engineering and Applications, 2018, 54 (19) :186-190, 215.王海艳, 佟岐, 连志鹏, 等.K-L变换观测矩阵优化算法[J].计算机工程与应用, 2018, 54 (19) :186-190, 215." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201819029&amp;v=MzA0NTVxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmtVYjdCTHo3TWFiRzRIOW5OcG85SGJZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Wang H Y, Tong Q, Lian Z P, &lt;i&gt;et al&lt;/i&gt;.K-L transform optimization algorithm for measurement matrix[J].Computer Engineering and Applications, 2018, 54 (19) :186-190, 215.王海艳, 佟岐, 连志鹏, 等.K-L变换观测矩阵优化算法[J].计算机工程与应用, 2018, 54 (19) :186-190, 215.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" Liu Y L, Gui Z G.Contrast enhancement using extracted details based on multi-scale top-hat transformation[J].Computer Engineering and Design, 2014, 35 (4) :1332-1335, 1340.刘艳莉, 桂志国.多尺度top-hat变换提取细节的对比度增强算法[J].计算机工程与设计, 2014, 35 (4) :1332-1335, 1340." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201404041&amp;v=MzAwNzBYTXE0OUJaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5rVWI3Qk5pZllaTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Liu Y L, Gui Z G.Contrast enhancement using extracted details based on multi-scale top-hat transformation[J].Computer Engineering and Design, 2014, 35 (4) :1332-1335, 1340.刘艳莉, 桂志国.多尺度top-hat变换提取细节的对比度增强算法[J].计算机工程与设计, 2014, 35 (4) :1332-1335, 1340.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" Jin Q G, Meng Z P, Pham T D, &lt;i&gt;et al&lt;/i&gt;.DUNet:a deformable network for retinal vessel segmentation[J].Knowledge-Based Systems, 2019, 178:149-162." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DUNet:a deformable network for retinal vessel segmentation">
                                        <b>[15]</b>
                                         Jin Q G, Meng Z P, Pham T D, &lt;i&gt;et al&lt;/i&gt;.DUNet:a deformable network for retinal vessel segmentation[J].Knowledge-Based Systems, 2019, 178:149-162.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" title=" Dai J F, Qi H Z, Xiong Y W, &lt;i&gt;et al&lt;/i&gt;.Deformable convolutional networks[C]//2017 IEEE International Conference on Computer Vision (ICCV) , October 22-29, 2017, Venice.New York:IEEE, 2017:764-773." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deformable Convolutional Networks">
                                        <b>[16]</b>
                                         Dai J F, Qi H Z, Xiong Y W, &lt;i&gt;et al&lt;/i&gt;.Deformable convolutional networks[C]//2017 IEEE International Conference on Computer Vision (ICCV) , October 22-29, 2017, Venice.New York:IEEE, 2017:764-773.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" title=" Huang G, Liu Z, van der Maaten L, &lt;i&gt;et al&lt;/i&gt;.Densely connected convolutional networks[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI.New York:IEEE, 2017:2261-2269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DenselyConnected Convolutional Networks">
                                        <b>[17]</b>
                                         Huang G, Liu Z, van der Maaten L, &lt;i&gt;et al&lt;/i&gt;.Densely connected convolutional networks[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI.New York:IEEE, 2017:2261-2269.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" Ioffe S, Szegedy C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//32nd International Conference on International Conference on Machine Learning, July 6-11, 2015, Lille, France.Massachusetts:JMLR.org, 2015, 37:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[18]</b>
                                         Ioffe S, Szegedy C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//32nd International Conference on International Conference on Machine Learning, July 6-11, 2015, Lille, France.Massachusetts:JMLR.org, 2015, 37:448-456.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_19" title=" Li Y, Fan C X, Li Y, &lt;i&gt;et al&lt;/i&gt;.Improving deep neural network with multiple parametric exponential linear units[J].Neurocomputing, 2018, 301:11-24." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES936E09C3510F694C1FD3E07228A3B546&amp;v=MTg2NzU9TmlmT2ZicTdHS1RNcHZ4R1llb1Blbm93eTJVU25FdCtQWC9scmhBOUNMSG1RTDZaQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGhoeGJtOXdhRQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         Li Y, Fan C X, Li Y, &lt;i&gt;et al&lt;/i&gt;.Improving deep neural network with multiple parametric exponential linear units[J].Neurocomputing, 2018, 301:11-24.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_20" title=" Vo D M, Lee S W.Semantic image segmentation using fully convolutional neural networks with multi-scale images and multi-scale dilated convolutions[J].Multimedia Tools and Applications, 2018, 77 (14) :18689-18707." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic Image Segmentation Using Fully Convolutional Neural Networks with Multi-Scale Images and Multi-Scale Dilated Convolutions">
                                        <b>[20]</b>
                                         Vo D M, Lee S W.Semantic image segmentation using fully convolutional neural networks with multi-scale images and multi-scale dilated convolutions[J].Multimedia Tools and Applications, 2018, 77 (14) :18689-18707.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_21" title=" Wang P Q, Chen P F, Yuan Y, &lt;i&gt;et al&lt;/i&gt;.Understanding convolution for semantic segmentation[C]//2018 IEEE Winter Conference on Applications of Computer Vision (WACV) , March 12-15, 2018, Lake Tahoe, NV.New York:IEEE, 2018:1451-1460." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding convolution for semantic segmentation">
                                        <b>[21]</b>
                                         Wang P Q, Chen P F, Yuan Y, &lt;i&gt;et al&lt;/i&gt;.Understanding convolution for semantic segmentation[C]//2018 IEEE Winter Conference on Applications of Computer Vision (WACV) , March 12-15, 2018, Lake Tahoe, NV.New York:IEEE, 2018:1451-1460.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_22" title=" Zeiler M D, Fergus R.Visualizing and understanding convolutional networks[M]//Fleet D, Pajdla T, Schiele B, &lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer, 2014, 8689:818-833." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">
                                        <b>[22]</b>
                                         Zeiler M D, Fergus R.Visualizing and understanding convolutional networks[M]//Fleet D, Pajdla T, Schiele B, &lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer, 2014, 8689:818-833.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_23" title=" Noh H, Hong S, Han B.Learning deconvolution network for semantic segmentation[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1520-1528." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deconvolution network for semantic segmentation">
                                        <b>[23]</b>
                                         Noh H, Hong S, Han B.Learning deconvolution network for semantic segmentation[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1520-1528.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_24" title=" Oktay O, Schlemper J, Folgoc L L, &lt;i&gt;et al&lt;/i&gt;.Attention U-Net:learning where to look for the pancreas[J/OL]. (2018-08-20) [2019-02-16].https://arxiv.org/abs/1804.03999." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attention U-Net:learning where to look for the pancreas">
                                        <b>[24]</b>
                                         Oktay O, Schlemper J, Folgoc L L, &lt;i&gt;et al&lt;/i&gt;.Attention U-Net:learning where to look for the pancreas[J/OL]. (2018-08-20) [2019-02-16].https://arxiv.org/abs/1804.03999.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_25" title=" Staal J, Abramoff M D, Niemeijer M, &lt;i&gt;et al&lt;/i&gt;.Ridge-based vessel segmentation in color images of the retina[J].IEEE Transactions on Medical Imaging, 2004, 23 (4) :501-509." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ridge-based vessel segmentation in color images of the retina">
                                        <b>[25]</b>
                                         Staal J, Abramoff M D, Niemeijer M, &lt;i&gt;et al&lt;/i&gt;.Ridge-based vessel segmentation in color images of the retina[J].IEEE Transactions on Medical Imaging, 2004, 23 (4) :501-509.
                                    </a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_26" title=" Hoover A D, Kouznetsova V, Goldbaum M.Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response[J].IEEE Transactions on Medical Imaging, 2000, 19 (3) :203-210." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response">
                                        <b>[26]</b>
                                         Hoover A D, Kouznetsova V, Goldbaum M.Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response[J].IEEE Transactions on Medical Imaging, 2000, 19 (3) :203-210.
                                    </a>
                                </li>
                                <li id="62">


                                    <a id="bibliography_27" title=" Feng Z W, Yang J, Yao L X.Patch-based fully convolutional neural network with skip connections for retinal blood vessel segmentation[C]//2017 IEEE International Conference on Image Processing (ICIP) , September 17-20, 2017, Beijing.New York:IEEE, 2017:1742-1746." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Patch-based fully convolutional neural network with skip connections for retinal blood vessel segmentation">
                                        <b>[27]</b>
                                         Feng Z W, Yang J, Yao L X.Patch-based fully convolutional neural network with skip connections for retinal blood vessel segmentation[C]//2017 IEEE International Conference on Image Processing (ICIP) , September 17-20, 2017, Beijing.New York:IEEE, 2017:1742-1746.
                                    </a>
                                </li>
                                <li id="64">


                                    <a id="bibliography_28" title=" Orlando J I, Prokofyeva E, Blaschko M B.A discriminatively trained fully connected conditional random field model for blood vessel segmentation in fundus images[J].IEEE Transactions on Biomedical Engineering, 2017, 64 (1) :16-27." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A discriminatively trained fully connected conditional random field model for blood vessel segmentation in fundus images">
                                        <b>[28]</b>
                                         Orlando J I, Prokofyeva E, Blaschko M B.A discriminatively trained fully connected conditional random field model for blood vessel segmentation in fundus images[J].IEEE Transactions on Biomedical Engineering, 2017, 64 (1) :16-27.
                                    </a>
                                </li>
                                <li id="66">


                                    <a id="bibliography_29" title=" Son J, Park S J, Jung K H.Retinal vessel segmentation in fundoscopic images with generative adversarial networks[J/OL]. (2017-06-28) [2019-02-16].https://arxiv.org/abs/1706.09318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Retinal vessel segmentation in fundoscopic images with generative adversarial networks">
                                        <b>[29]</b>
                                         Son J, Park S J, Jung K H.Retinal vessel segmentation in fundoscopic images with generative adversarial networks[J/OL]. (2017-06-28) [2019-02-16].https://arxiv.org/abs/1706.09318.
                                    </a>
                                </li>
                                <li id="68">


                                    <a id="bibliography_30" title=" Peng S H, Zheng C X, Xu F, &lt;i&gt;et al&lt;/i&gt;.Blood vessels segmentation by using CDNet[C]//2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC) , June 27-29, 2018, Chongqing.New York:IEEE, 2018:305-310." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Blood vessels segmentation by using CDNet">
                                        <b>[30]</b>
                                         Peng S H, Zheng C X, Xu F, &lt;i&gt;et al&lt;/i&gt;.Blood vessels segmentation by using CDNet[C]//2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC) , June 27-29, 2018, Chongqing.New York:IEEE, 2018:305-310.
                                    </a>
                                </li>
                                <li id="70">


                                    <a id="bibliography_31" title=" Zheng T Y, Tang C, Lei Z K.Multi-scale retinal vessel segmentation based on fully convolutional neural network[J].Acta Optica Sinica, 2019, 39 (2) :0211002.郑婷月, 唐晨, 雷振坤.基于全卷积神经网络的多尺度视网膜血管分割[J].光学学报, 2019, 39 (2) :0211002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902014&amp;v=MjYxNzVFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlua1ViN0JJalhUYkxHNEg5ak1yWTk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[31]</b>
                                         Zheng T Y, Tang C, Lei Z K.Multi-scale retinal vessel segmentation based on fully convolutional neural network[J].Acta Optica Sinica, 2019, 39 (2) :0211002.郑婷月, 唐晨, 雷振坤.基于全卷积神经网络的多尺度视网膜血管分割[J].光学学报, 2019, 39 (2) :0211002.
                                    </a>
                                </li>
                                <li id="72">


                                    <a id="bibliography_32" title=" Wang S L, Yin Y L, Cao G B, &lt;i&gt;et al&lt;/i&gt;.Hierarchical retinal blood vessel segmentation based on feature and ensemble learning[J].Neurocomputing, 2015, 149:708-717." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical retinal blood vessel segmentation based on feature and ensemble learning">
                                        <b>[32]</b>
                                         Wang S L, Yin Y L, Cao G B, &lt;i&gt;et al&lt;/i&gt;.Hierarchical retinal blood vessel segmentation based on feature and ensemble learning[J].Neurocomputing, 2015, 149:708-717.
                                    </a>
                                </li>
                                <li id="74">


                                    <a id="bibliography_33" title=" Dasgupta A, Singh S.A fully convolutional neural network based structured prediction approach towards the retinal vessel segmentation[C]//2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017) , April 18-21, 2017, Melbourne, Australia.New York:IEEE, 2017:248-251." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A fully convolutional neural network based structured prediction approach towards the retinal vessel segmentation">
                                        <b>[33]</b>
                                         Dasgupta A, Singh S.A fully convolutional neural network based structured prediction approach towards the retinal vessel segmentation[C]//2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017) , April 18-21, 2017, Melbourne, Australia.New York:IEEE, 2017:248-251.
                                    </a>
                                </li>
                                <li id="76">


                                    <a id="bibliography_34" title=" Cai Y H, Gao X R, Qiu C Y, &lt;i&gt;et al&lt;/i&gt;.Retinal vessel segmentation method with efficient hybrid features fusion[J].Journal of Electronics &amp;amp; Information Technology, 2017, 39 (8) :1956-1963.蔡轶珩, 高旭蓉, 邱长炎, 等.一种混合特征高效融合的视网膜血管分割方法[J].电子与信息学报, 2017, 39 (8) :1956-1963." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201708025&amp;v=MzA5NzZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5rVWI3QklUZlNkckc0SDliTXA0OUg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[34]</b>
                                         Cai Y H, Gao X R, Qiu C Y, &lt;i&gt;et al&lt;/i&gt;.Retinal vessel segmentation method with efficient hybrid features fusion[J].Journal of Electronics &amp;amp; Information Technology, 2017, 39 (8) :1956-1963.蔡轶珩, 高旭蓉, 邱长炎, 等.一种混合特征高效融合的视网膜血管分割方法[J].电子与信息学报, 2017, 39 (8) :1956-1963.
                                    </a>
                                </li>
                                <li id="78">


                                    <a id="bibliography_35" title=" Strisciuglio N, Azzopardi G, Vento M, &lt;i&gt;et al&lt;/i&gt;.Supervised vessel delineation in retinal fundus images with the automatic selection of B-COSFIRE filters[J].Machine Vision and Applications, 2016, 27 (8) :1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised vessel delineation in retinal fundus images with the automatic selection of B-COSFIRE filters">
                                        <b>[35]</b>
                                         Strisciuglio N, Azzopardi G, Vento M, &lt;i&gt;et al&lt;/i&gt;.Supervised vessel delineation in retinal fundus images with the automatic selection of B-COSFIRE filters[J].Machine Vision and Applications, 2016, 27 (8) :1137-1149.
                                    </a>
                                </li>
                                <li id="80">


                                    <a id="bibliography_36" title=" Yan Z Q, Yang X, Cheng K T.Joint segment-level and pixel-wise losses for deep learning based retinal vessel segmentation[J].IEEE Transactions on Biomedical Engineering, 2018, 65 (9) :1912-1923." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint segment-level and pixel-wise losses for deep learning based retinal vessel segmentation">
                                        <b>[36]</b>
                                         Yan Z Q, Yang X, Cheng K T.Joint segment-level and pixel-wise losses for deep learning based retinal vessel segmentation[J].IEEE Transactions on Biomedical Engineering, 2018, 65 (9) :1912-1923.
                                    </a>
                                </li>
                                <li id="82">


                                    <a id="bibliography_37" title=" Zhang J, Dashtbozorg B, Bekkers E, &lt;i&gt;et al&lt;/i&gt;.Robust retinal vessel segmentation via locally adaptive derivative frames in orientation scores[J].IEEE Transactions on Medical Imaging, 2016, 35 (12) :2631-2644." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Robust Retinal Vessel Segmentat ion via Locally Adapt ive Derivative Frames in Orientation Scores,&amp;quot;">
                                        <b>[37]</b>
                                         Zhang J, Dashtbozorg B, Bekkers E, &lt;i&gt;et al&lt;/i&gt;.Robust retinal vessel segmentation via locally adaptive derivative frames in orientation scores[J].IEEE Transactions on Medical Imaging, 2016, 35 (12) :2631-2644.
                                    </a>
                                </li>
                                <li id="84">


                                    <a id="bibliography_38" title=" Ganin Y, Lempitsky V.&lt;i&gt;N&lt;/i&gt;&lt;sup&gt;4&lt;/sup&gt;-fields:neural network nearest neighbor fields for image transforms[M]//Cremers D, Reid I, Saito H, &lt;i&gt;et al&lt;/i&gt;.Computer vision-ACCV 2014.Lecture notes in computer science.Cham:Springer, 2015, 9004:536-551." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=N4-Fields:Neural Network Nearest Neighbor Fields for Image Transforms">
                                        <b>[38]</b>
                                         Ganin Y, Lempitsky V.&lt;i&gt;N&lt;/i&gt;&lt;sup&gt;4&lt;/sup&gt;-fields:neural network nearest neighbor fields for image transforms[M]//Cremers D, Reid I, Saito H, &lt;i&gt;et al&lt;/i&gt;.Computer vision-ACCV 2014.Lecture notes in computer science.Cham:Springer, 2015, 9004:536-551.
                                    </a>
                                </li>
                                <li id="86">


                                    <a id="bibliography_39" title=" Xie S N, Tu Z W.Holistically-nested edge detection[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1395-1403." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Holistically-nested edge detection">
                                        <b>[39]</b>
                                         Xie S N, Tu Z W.Holistically-nested edge detection[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1395-1403.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-05-08 11:51</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(08),126-140 DOI:10.3788/AOS201939.0810004            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>自适应尺度信息的U型视网膜血管分割算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%A2%81%E7%A4%BC%E6%98%8E&amp;code=07851079&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">梁礼明</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%9B%9B%E6%A0%A1%E6%A3%8B&amp;code=40968652&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">盛校棋</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%93%9D%E6%99%BA%E6%95%8F&amp;code=42051574&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蓝智敏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E5%9B%BD%E4%BA%AE&amp;code=07858537&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨国亮</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E6%96%B0%E5%BB%BA&amp;code=24346785&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈新建</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E8%A5%BF%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E7%94%B5%E6%B0%94%E5%B7%A5%E7%A8%8B%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=0217990&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江西理工大学电气工程与自动化学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%8B%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0240077&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏州大学电子信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对视网膜血管形态结构和尺度信息复杂多变的特点, 提出一种自适应血管形态结构和尺度信息的U型视网膜血管分割算法。首先采用二维K-L (Karhunen-Loeve) 变换 (即霍特林变换) 综合分析彩色图像三通道的频带信息, 从而得到视网膜灰度图像以及多尺度形态学滤波增强血管与背景的对比度信息。然后将预处理图像经U型分割模型对图像进行端对端训练, 并利用局部信息熵采样进行数据增强。该网络编码部分的密集可变形卷积结构根据上下特征层信息有效地捕捉图像中多种尺度信息和形状结构, 底部金字塔型的多尺度空洞卷积扩大局部感受野, 同时解码阶段带有Attention机制的反卷积网络将底层与高层特征映射有效结合, 解决权重分散和图像纹理损失的问题。最后通过SoftMax激活函数得到最终的分割结果。在DRIVE (Digital Retinal Images for Vessel Extraction) 与STARE (Structured Analysis of the Retina) 数据集上对该算法进行了仿真, 准确率分别达到97.48%与96.83%, 特异性分别达到98.83%与97.75%, 总体性能优于现有算法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E7%BD%91%E8%86%9C%E8%A1%80%E7%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视网膜血管;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BD%A2%E6%80%81%E5%AD%A6%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">形态学滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">可变形卷积;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空洞卷积;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *梁礼明 E-mail:lianglm67@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-04</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点基础研究发展计划 (2014CB748600);</span>
                                <span>国家自然科学基金 (81371629, 61401293, 61401294, 81401451, 81401472, 51365017);</span>
                                <span>江苏省自然科学基金 (BK20140052);</span>
                                <span>江西省自然科学基金 (20132BAB203020);</span>
                                <span>江西省教育厅科学技术研究重点项目 (GJJ170491);</span>
                    </p>
            </div>
                    <h1><b>U-Shaped Retinal Vessel Segmentation Algorithm Based on Adaptive Scale Information</b></h1>
                    <h2>
                    <span>Liang Liming</span>
                    <span>Sheng Xiaoqi</span>
                    <span>Lan Zhimin</span>
                    <span>Yang Guoliang</span>
                    <span>Chen Xinjian</span>
            </h2>
                    <h2>
                    <span>School of Electrical Engineering and Automation, Jiangxi University of Science and Technology</span>
                    <span>School of Electronic and Information Engineering, Soochow University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In view of the complex and changeable morphological structure and scale information of retinal vessels, an U-shaped retinal vessel segmentation algorithm based on the adaptive morphological structure and scale information is proposed. First, the gray image of retina is obtained by synthetically analyzing the three-channel frequency information of the image with two-dimensional K-L (Karhunen-Loeve) transform, and the contrast information between the vessel and the background is enhanced by multi-scale morphological filtering. Then the preprocessed image is trained end-to-end by using the U-shaped segmentation model, and the data is enhanced by local information entropy sampling. The dense deformable convolution structure of the network coding part captures the multi-scale information and shape structure of the image effectively according to the informations of the upper and lower feature layers, and the pyramid-shaped multi-scale dilated convolution at the bottom enlarges the local receptive field. At the same time, introducing deconvolution layer with attention mechanism in decoding phase, which effectively combines the bottom and top feature mappings, can solve the problems of weight dispersion and image texture loss. Finally, the final segmentation result is obtained by using the SoftMax activation function. This approach achieves average accuracies of 97.48% and 96.83% and specificities of 98.83% and 97.75% on the DRIVE (Digital Retinal Images for Vessel Extraction) and STARE (Structured Analysis of the Retina) datasets respectively, which is better than the existing algorithms.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=retinal%20vessels&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">retinal vessels;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=morphological%20filtering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">morphological filtering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deformable%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deformable convolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dilated%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dilated convolution;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-04</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="88" name="88" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="89">血管是视网膜最重要的组成部分之一, 其具有的多种形态结构, 如长度、宽度、迂曲和角度, 可用于各种心血管和眼科疾病的诊断、筛选、治疗和评估<citation id="199" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。在视网膜眼底图像计算机辅助诊断系统中, 血管网络的状况和外观可被视为一个重要方面, 现阶段基于无监督与有监督视网膜图像分割方法大部分依赖于人工制作或专家标记的金标准特征来表征血管和背景之间的差异性, 但是计算机图像分割技术的关键是使算法能自适应血管尺度、形态和几何变换, 达到智能化识别血管特征, 从而准确高效地分割出有效的目标轮廓结构的目的, 进而满足辅助临床诊断的需求。</p>
                </div>
                <div class="p1">
                    <p id="90">传统机器学习算法一般需通过主成分分析、聚类或者融合人工制作的特征作为数据的先验信息, 以此得到具有可分性血管特征, 并与分类器相结合获得最终的分割结果。文献<citation id="200" type="reference">[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</citation>利用滤波器、形态学算子和线性检测算子提取视网膜血管形态特征作为分类器的先验信息, 该算法主要利用人工设定好的特征提取器作为分类器的主要先验信息, 具有特征提取效率高以及模型训练速度快等特点, 但该类人工制作的特征阻止了对血管未知几何变换的泛化, 不能有效地根据具体血管的形态结构进行自适应建模。另一种算法是基于水平集算法<citation id="201" type="reference"><link href="18" rel="bibliography" /><link href="20" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>并将图像分割问题转换为能量最小化问题的可变模型, 通过在水平集算法中施加不同的正则项, 实现其对血管形态结构特征任意变化的适应性, 使得不同的能量项表征不同的图像强度、形状和纹理特征等信息, 但存在非凸极小化问题而达不到全局最优。</p>
                </div>
                <div class="p1">
                    <p id="91">基于深度学习的视网膜图像分割算法区别于传统机器学习算法对图像数据先验信息的依赖性, 具有较强的数据表征能力。文献<citation id="202" type="reference">[<a class="sup">7</a>]</citation>将残差网路结构与密集连接网络 (DenseNet) 相结合, 有效地将血管特征重复利用, 有助于模型根据金标准图像学到更加稳健的形态结构信息, 但过多地利用Dense Block结构会造成内存占用率高, 计算量较大等缺陷。文献<citation id="203" type="reference">[<a class="sup">8</a>]</citation>将血管分割问题转换为边界检测问题, 利用全连接卷积神经网络 (CNN) 鉴别特征并生成血管概率图, 接着通过全连接条件随机场 (CRF) 生成具有密集全局像素相关性的二值血管图像, 但该算法利用的CNN存在模型几何结构固化的特点, 卷积单元在图像固定位置进行特征信息的采样, 不能有效地根据血管形状信息进行转换, 而且CRF虽然能通过局部和远程依赖项来细化预测特征, 但无法捕捉到血管中与曲率相关的复杂形状属性, 进而造成微血管分割不全的现象。文献<citation id="204" type="reference">[<a class="sup">9</a>]</citation>融合U型CNN (U-net) 和多尺度滤波的血管算法, 将人工特征与金标准特征相结合, 利用U-net架构将编码层与解码层的输出进行连接, 较好地解决低级信息共享的问题, 从而使得分割出的微血管稳健性更强, 但该模型的几何变换建模能力依然来自人工制作特征和数据扩充, 并且解码部分的上采样层不能有效地恢复编码损失的细节信息。文献<citation id="205" type="reference">[<a class="sup">10</a>]</citation>将空洞卷积与DenseNet嵌入U型网络, 提升了网络特征复用能力和网络的整体感受野, 解决了微血管分割不足的问题, 但由于不具备自适应血管尺度信息的能力, 仍存在少许视盘周围血管和主血管末端的微血管分割断裂等问题。</p>
                </div>
                <div class="p1">
                    <p id="92">针对上述算法存在的特征模型固化、对眼底图像噪声稳健性不高和尺度与形态结构信息提取不健全问题, 本文在U型框架的基础上提出一种融合密集可变形卷积 (DB-DC) 和注意门 (AGs) 的视网膜血管分割算法。首先, 利用二维K-L (Karhunen-Loeve) 变换 (即霍特林变换) 提取眼底图像RGB三通道的最优特征值并加权结合成一幅新的灰度图, 从而消除通道之间数据的相关性, 并利用多尺度形态学滤波对目标和背景依不同比例增强;然后通过U型框架<citation id="206" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>的CNN提取血管特征, 将编码部分的卷积替换为可变形卷积, 进而有助于充分提取浅层网络部分基于目标形状结构的特征信息, 同时结合Dense Block结构充分利用层与层之间的特征信息;最后, 为了进一步增加感受野, 使网络能够提取更多的目标特征, 在U型网络底部采用金字塔型的空洞卷积, 并在解码部分引入Attention模型、反卷积和上池化操作, 其中Attention模型能根据金标准图像动态权衡目标图像与背景部分权重比例, 降低周围噪声的权重, 而反卷积和上池化层能有助于重构血管像素, 解决池化层信息损失的问题。</p>
                </div>
                <h3 id="93" name="93" class="anchor-tag">2 图像预处理算法原理</h3>
                <h4 class="anchor-tag" id="94" name="94"><b>2.1 二维K-L变换</b></h4>
                <div class="p1">
                    <p id="95">二维K-L变换的中心思想是通过分析图像的统计信息来减少由多个相关变量组成的数据维度, 同时尽可能地保留数据集中的主要空间信息<citation id="207" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 这样可以通过减少特征空间的维数来达到更低的计算量和显存, 使得算法稳健性更强。对于三通道图像, 经K-L变换后频带之间信息是不相关的, 故将原始颜色频带分量转换到主分量空间, 即创建3个新通道, 相当于将原始实线RGB坐标系的中心点移到点分布的中心位置, 图像颜色数据信息变换后的<i>R</i>′, <i>G</i>′, <i>B</i>′组成的虚线部分如图1所示。其数据变换矩阵可表示为</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mi mathvariant="bold-italic">α</mi><msubsup><mrow></mrow><mi>k</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">x</mi><mo>=</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>k</mi><msub><mrow></mrow><mtext>R</mtext></msub></mrow></msub><mi>x</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>k</mi><msub><mrow></mrow><mtext>G</mtext></msub></mrow></msub><mi>x</mi><msub><mrow></mrow><mtext>G</mtext></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>k</mi><msub><mrow></mrow><mtext>B</mtext></msub></mrow></msub><mi>x</mi><msub><mrow></mrow><mtext>B</mtext></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">式中:<i>k</i><sub>R</sub>、<i>k</i><sub>G</sub>和<i>k</i><sub>B</sub>分别为红色通道分量、绿色通道分量和蓝色通道分量;<i>α</i><sub><i>k</i></sub>=[<i>α</i><sub><i>k</i><sub>R</sub></sub>, <i>α</i><sub><i>k</i><sub>G</sub></sub>, <i>α</i><sub><i>k</i><sub>B</sub></sub>]<sup>T</sup>为三通道的特征向量矩阵, T为转置符号;<b><i>x</i></b>=[<i>x</i><sub>R</sub>, <i>x</i><sub>G</sub>, <i>x</i><sub>B</sub>]<sup>T</sup>为RGB三通道图像;<i>k</i>∈{1, 2, 3}为主分量序号。为了找到图像三通道的主分量<i>P</i>, 并得到特征向量矩阵, 需要将数据进行协方差对角化, 故定义协方差矩阵<i>Σ</i>为</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Σ</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>c</mi><msub><mrow></mrow><mrow><mtext>R</mtext><mtext>R</mtext></mrow></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>c</mi><msub><mrow></mrow><mrow><mtext>G</mtext><mtext>B</mtext></mrow></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>c</mi><msub><mrow></mrow><mrow><mtext>B</mtext><mtext>R</mtext></mrow></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">其中</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">[</mo></mstyle></mrow></mstyle><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>0</mn></mrow></msub><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>j</mi><mn>0</mn></mrow></msub><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908016_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 经K-L变换后彩色坐标空间形成的点簇" src="Detail/GetImg?filename=images/GXXB201908016_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 经K-L变换后彩色坐标空间形成的点簇  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908016_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Point clusters after K-L transformation in color coordinate space</p>

                </div>
                <div class="p1">
                    <p id="102">式中:<i>c</i><sub><i>ij</i></sub> (<i>i</i>, <i>j</i>=R, G, B) 为<i>Σ</i>中第<i>i</i>频带和第<i>j</i>频带的值;<i>x</i><sub><i>i</i></sub> (<i>m</i>, <i>n</i>) 和<i>x</i><sub><i>j</i></sub> (<i>m</i>, <i>n</i>) 分别为像素点 (<i>m</i>, <i>n</i>) 在第<i>i</i>频带和第<i>j</i>频带的值;<i>x</i><sub><i>i</i>0</sub>和<i>x</i><sub><i>j</i>0</sub>分别为第<i>i</i>频带和第<i>j</i>频带的均值;<i>N</i>为像素个数。K-L变换综合考虑原始图像RGB三通道频带信息, 并不是对频带信息进行简单的取舍, 因此变换后得到灰度图能更好地反映出目标的本质特征信息<citation id="208" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 其中第一个主分量<i>P</i><sub>1</sub>包含最优结构化的目标形态结构和特征信息, 故选取第一主分量视网膜图像<i>I</i><sub>1</sub>作为进一步处理对象。</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103"><b>2.2 多尺度形态学变换</b></h4>
                <div class="p1">
                    <p id="104">考虑到经K-L变换后部分第一主成分<i>I</i><sub>1</sub>的血管结构与背景对比度不明显, 于是采用多尺度形态学Top-Hot变换<citation id="209" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提升血管与背景对比度。利用图像边缘梯度信息控制因子<i>γ</i><sub><i>a</i></sub>控制目标与背景的像素差值, 表示为</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>=</mo><mi>Ι</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>a</mi><mo>=</mo><mn>0</mn></mrow><mi>Ν</mi></munderover><mi>γ</mi></mstyle><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false"> (</mo><mi>D</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>p</mtext><mi>a</mi></mrow></msub><mo>-</mo><mi>D</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mi>a</mi></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">式中:<i>λ</i>为图像边缘增强因子;<i>I</i><sub>d</sub>为输出图像;<i>D</i><sub>op<i>a</i></sub>与<i>D</i><sub>cl<i>a</i></sub>分别为图像中的亮与暗细节特征;下标<i>a</i>为图像中的像素值。<i>γ</i><sub><i>a</i></sub>定义为</p>
                </div>
                <div class="p1">
                    <p id="107" class="code-formula">
                        <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo>=</mo><mtext>S</mtext><mtext>i</mtext><mtext>g</mtext><mtext>m</mtext><mtext>o</mtext><mtext>i</mtext><mtext>d</mtext><mrow><mo>[</mo><mrow><mi>e</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mfrac><mrow><mi>e</mi><msub><mrow></mrow><mrow><mi>a</mi><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></msub><mo>+</mo><mi>e</mi><msub><mrow></mrow><mrow><mi>a</mi><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></msub></mrow><mn>2</mn></mfrac></mrow><mo>]</mo></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="108">式中:<i>e</i><sub><i>a</i></sub>为图像膨胀与腐蚀之差;<i>e</i><sub><i>a</i>max</sub>与<i>e</i><sub><i>a</i>min</sub>分别为<i>e</i><sub><i>a</i></sub>的最大值与最小值。采用Sigmoid非线性函数以更好地适应复杂结构的目标图像。<i>γ</i><sub><i>a</i></sub>的变化主要由目标与背景的梯度信息决定。该方法能有效根据眼底视网膜图像的形态学梯度信息提升目标和背景的对比度, 不会额外扩大噪声信息。</p>
                </div>
                <h3 id="109" name="109" class="anchor-tag">3 网络结构</h3>
                <h4 class="anchor-tag" id="110" name="110"><b>3.1 可变形卷积</b></h4>
                <div class="p1">
                    <p id="111">眼底视网膜图像分割的一个极大挑战是对具有各种形状和尺度的对象进行有效的建模<citation id="210" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。传统卷积模型因其自身固定几何结构, 缺乏处理几何变换的内在机制。为了解决这一局限性问题, 利用可变形卷积<citation id="211" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>充分提取底层的特征信息。可变形卷积通过可变形的接收场自适应地捕获血管的各种形状和尺度信息, 而且不需要额外的监督机制, 如图2所示。</p>
                </div>
                <div class="p1">
                    <p id="112">在可变形卷积中, 将偏移量添加到标准卷积通常使用的网格采样位置中, 并且偏移量的大小是从偏移场前附加卷积层生成的特征图中学习确定。因此, 带有偏移量的卷积能够适应视网膜血管中不同的尺度、形状和取向等。为了进一步详细说明可变形卷积, 此处给出带有一个扩张率为1的3×3卷积, 定义为</p>
                </div>
                <div class="p1">
                    <p id="113" class="code-formula">
                        <mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>R</mtext><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false"> (</mo><mo>-</mo><mn>1</mn><mo>, </mo><mn>1</mn><mo stretchy="false">) </mo><mo>, </mo><mo stretchy="false"> (</mo><mo>-</mo><mn>1</mn><mo>, </mo><mn>0</mn><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mo stretchy="false"> (</mo><mn>0</mn><mo>, </mo><mn>1</mn><mo stretchy="false">) </mo><mo>, </mo><mo stretchy="false"> (</mo><mn>1</mn><mo>, </mo><mn>1</mn><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="114">由此, 在局部位置下<i>t</i><sub>0</sub>的输出特征映射<i>y</i>可以定义为</p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908016_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 3&#215;3的正常卷积与可变形卷积示意图" src="Detail/GetImg?filename=images/GXXB201908016_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 3×3的正常卷积与可变形卷积示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908016_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 3×3 <i>normal convolution and deformable</i><i>convolution diagram</i></p>

                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>t</mi><msub><mrow></mrow><mi>b</mi></msub><mo>∈</mo><mtext>R</mtext></mrow></munder><mi>w</mi></mstyle><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">) </mo><mi>f</mi><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>t</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">式中:f (·) 表示输入特征图;w表示样本权重值;t<sub>b</sub>表示R的局部位置, <i>b</i>=1, 2, …, <i>N</i>, <i>N</i>=|R|。然而, 可变形卷积通过偏移量Δ<i>t</i><sub><i>b</i></sub>提升R的提取特征能力, 定义如下:</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>t</mi><msub><mrow></mrow><mi>b</mi></msub><mo>∈</mo><mtext>R</mtext></mrow></munder><mi>w</mi></mstyle><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">) </mo><mi>f</mi><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>t</mi><msub><mrow></mrow><mi>b</mi></msub><mo>+</mo><mtext>Δ</mtext><mi>t</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">) </mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119"> (8) 式和图2说明卷积利用与输入特征映射具有相同空间分辨率的偏移矢量, 使得原采样点向外扩展以聚焦于血管轮廓。在训练期间, 用于生成输出特征的卷积核和偏移量同时学习眼底血管的特征信息, 而偏移量Δ<i>t</i><sub><i>b</i></sub>由附加卷积层学习得到, 在卷积充分学习偏移量和样本权重信息的情况下, 该模型可以获得更加稳健的几何变化特征。为了有效地学习偏移量, 网络的梯度信息通过双线性差值反传到卷积层来确定后采样点的值。可变形卷积特征提取过程如图3所示。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120"><b>3.2 DB-DC模型</b></h4>
                <div class="p1">
                    <p id="121">为了更有效地提取血管的特征信息, 在编码部分融合可变形卷积与Dense Block模型<citation id="212" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 而且Dense Block结构构成的层间密集连接比传统网络具有更少的输出维度, 避免学习冗余的目标特征信息。此外, 密集连接的路径有助于保证可变形卷积层之间的最大信息流, 进而提高可变形卷积对目标尺度和形态特征的提取, 从而解决梯度消失的问题。设<i>l</i>层的输出为<i>θ</i><sub><i>l</i></sub>, 则该模型第<i>l</i>层的输出为</p>
                </div>
                <div class="area_img" id="123">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908016_12300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 可变形卷积特征提取过程" src="Detail/GetImg?filename=images/GXXB201908016_12300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 可变形卷积特征提取过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908016_12300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Deformable convolution feature extraction process</p>

                </div>
                <div class="p1">
                    <p id="125" class="code-formula">
                        <mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mi>l</mi></msub><mo>=</mo><mi>Η</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="126">式中:<i>θ</i><sub>0</sub>, <i>θ</i><sub>1</sub>, …, <i>θ</i><sub><i>l</i>-1</sub>表示0, 1, …, <i>l</i>-1层输出特征层的特征相合并;<i>H</i><sub><i>l</i></sub>[·]表示第<i>l</i>层非线性映射。<i>H</i><sub><i>l</i></sub>[·]会产生<i>κ</i>个特征映射, <i>κ</i>为增长率, 则<i>l</i>层会具有<i>κ</i><sub>0</sub>+<i>κ</i> (<i>l</i>-1) 个特征映射, 其中<i>κ</i><sub>0</sub>为输入层中的通道数目, 因此每一层可以访问其模块中所有前面的特征映射。考虑到数据集较少和训练复杂度的问题, 设置增长率<i>κ</i>=12<citation id="213" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。虽然每层的特征映射均由<i>κ</i>限制, 但下一个密集模块仍然具有大量的输入, 因此在每个3×3可变形卷积前和整个密集模块后引入1×1卷积层作为瓶颈层, 以减少输入特征映射的数量, 从而提高计算效率, 并在可变形卷积后面加入批量归一化<citation id="214" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>和ELU激活函数<citation id="215" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, 以加速网络收敛并缓解梯度爆炸。密集可变形卷积模型内部结构如图4所示。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127"><b>3.3 多尺度空洞卷积模型</b></h4>
                <div class="p1">
                    <p id="128">现有的大部分图像分割算法均利用空洞卷积结构<citation id="216" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>完全取缔池化层, 以达到增加网络感受野和减少图片因池化层造成的信息损失。虽然空洞卷积能在增加感受野的同时不增加网络参数复杂度, 但并没有降低网络总体参数和整合特征的能力。于是本文在U型网络架构中编码部分仍保留部分池化层以整合特征, 降低网络参数复杂度, 使特征具有更好的位移不变性;而在U型网络的底部去掉池化层, 使用金字塔型的多尺度空洞卷积, 从而进一步增加中间视网膜图像特征映射的感受野, 提取局部区域的多尺度特征, 以便生成更精确的预测, 其模型如图5所示。空洞卷积在传统矩形卷积核的每个像素之间插入值为0的像素值, 从而增加网络的扩张率<i>r</i>。设输入与滤波器分别为<i>E</i>[<i>s</i>]和<mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ω</mi><mo stretchy="false">[</mo><mi>β</mi><mo stretchy="false">]</mo></mrow></math></mathml>的空洞卷积的输出y′[s]为</p>
                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908016_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 密集可变形卷积模型内部结构" src="Detail/GetImg?filename=images/GXXB201908016_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 密集可变形卷积模型内部结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908016_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Internal structure of dense deformable convolution model</p>

                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>y</mi><mo>′</mo></msup><mo stretchy="false">[</mo><mi>s</mi><mo stretchy="false">]</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>β</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>E</mi></mstyle><mo stretchy="false">[</mo><mi>s</mi><mo>+</mo><mi>r</mi><mi>β</mi><mo stretchy="false">]</mo><mi>Ω</mi><mo stretchy="false">[</mo><mi>β</mi><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133">式中:L为<mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ω</mi><mo stretchy="false">[</mo><mi>β</mi><mo stretchy="false">]</mo></mrow></math></mathml>的长度;β为卷积核的大小, 经扩张后卷积核为β′=β+ (β-1) (r-1) 。例如:当卷积核为3×3时, 设r=4为一个9×9空洞卷积核。虽然空洞卷积能在不增加算法参数复杂度的情况下, 有效地增加网络的感受野信息, 但是空洞卷积存在“网格化”现象, 比如上述r=4的空洞卷积, 仅能有效计算25个像素的相关特征信息。同时, 当存在过大的r时, 易造成滤波器之间引入零的位置处丢失局部小区域的上下文信息, 而且本文在编码阶段加入了最大池化操作, 在一定程度上增加了样本特征的稀疏性;在较深层网络采用过大的扩张率可能引起解码结构在采样时, 因特征图过于稀疏而信息采样失败的现象, 并且采用多个单一扩张率的空洞卷积易导致空间信息不连续现象<citation id="217" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, 因此本文采用小扩张率多尺度的策略, 组成金字塔型的空洞卷积网络, 该扩张率设置公式<citation id="218" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>可表示为</p>
                </div>
                <div class="p1">
                    <p id="135" class="code-formula">
                        <mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><msub><mrow></mrow><mi>v</mi></msub><mo>=</mo><mi>max</mi><mo stretchy="false">[</mo><mi>Μ</mi><msub><mrow></mrow><mrow><mi>v</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>-</mo><mn>2</mn><mi>r</mi><msub><mrow></mrow><mi>v</mi></msub><mo>, </mo><mi>Μ</mi><msub><mrow></mrow><mrow><mi>v</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>-</mo><mn>2</mn><mo stretchy="false"> (</mo><mi>Μ</mi><msub><mrow></mrow><mrow><mi>v</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>-</mo><mi>r</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mi>r</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="136">式中:M<sub>v</sub>为第v层的最大扩张率;r<sub>v</sub>为第v层的扩张率。通过小扩张率提高对微小血管的定位能力, 较大的扩张率能捕捉更大的上下文血管特征信息, 将较深层网络的扩张率设为1, 以降低特征信息过稀疏的问题。</p>
                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908016_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 多尺度空洞卷积" src="Detail/GetImg?filename=images/GXXB201908016_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 多尺度空洞卷积  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908016_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Multiscale dilated convolution</p>

                </div>
                <h4 class="anchor-tag" id="139" name="139"><b>3.4 带有Attention模型的反卷积网络</b></h4>
                <div class="p1">
                    <p id="140">虽然编码结构的最大池化层能保留稳健性强的激活特征, 但是仍会造成部分空间信息在汇集期间丢失。为了解决这一问题, 本文在解码结构中利用上池化跟踪目标图像的原始位置, 上池化层使用一组转换变量记录每个池化区域内的最大值激活位置, 以获得近似的池化逆过程, 有效地重构血管的精细分辨率结构信息<citation id="219" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。同时, 考虑到经上池化层输出的信息是一个被放大且稀疏的特征图, 且解码结构的主要任务是恢复图像结构信息, 故在解码部分并未使用可变形卷积提取深层次图像的形态结构信息, 而是采用具有多个学习滤波器反卷积层<citation id="220" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>学习上池化层输出的稀疏特征图, 其分层结构用来捕获不同级别的特征图细节信息, 较低层的过滤器用来捕获目标的总体形状, 而特定类的特征信息则编码在高层的过滤器中, 如视网膜图像中的微血管和交叉血管处的细节, 使得特征图更密集, 进一步重构输入目标的形状, 保证最后分类器输出的稳健性。</p>
                </div>
                <div class="p1">
                    <p id="141">融合上池化和反卷积虽然有效地重构了图像中目标的细节特征信息, 但考虑到图像会存在遮挡和畸变等噪声信息, 因此在上述优化的基础上引入AGs<citation id="221" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>并将其集成到U型网络的解码部分。AGs在训练阶段可以在没有额外监督的情况下自动学习目标结构, 同时在测试阶段会动态地隐式生成软区域, 从而抑制与金标准不相关的特征激活, 提高网络分割图像主要目标轮廓的灵敏度和准确率。设AGs模型的门注意系数为0≤∂<sub><i>τ</i></sub>≤1, 通过该系数能够识别血管特征的显著性区域和修剪相应的特征, 仅保留与特征任务相关的特征信息, 降低数据的冗余度。AGs输出特征<mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>τ</mi><mo>, </mo><mi>d</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>定义为</p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>τ</mi><mo>, </mo><mi>d</mi></mrow><mi>l</mi></msubsup><mo>=</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><msubsup><mrow></mrow><mrow><mi>τ</mi><mo>, </mo><mi>d</mi></mrow><mi>l</mi></msubsup><mo>∂</mo><msubsup><mrow></mrow><mi>τ</mi><mi>l</mi></msubsup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144">式中:x′<sup>l</sup><sub>τ, d</sub>为输入特征图;l为网络层数;d为通道尺寸;τ为像素空间大小。∂<sub>τ</sub>可以通过<i>Attention</i>机制公式I<sub>att</sub>得到, 定义为</p>
                </div>
                <div class="p1">
                    <p id="145" class="code-formula">
                        <mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>Ι</mtext><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext></mrow><mi>l</mi></msubsup><mo>=</mo><mi mathvariant="bold-italic">η</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">[</mo><msup><mi>σ</mi><mo>′</mo></msup><msub><mrow></mrow><mtext>l</mtext></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mtext>Τ</mtext></msubsup><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><msubsup><mrow></mrow><mi>τ</mi><mi>l</mi></msubsup><mo>+</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">g</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>τ</mi></msub><mo>+</mo><msup><mi mathvariant="bold-italic">b</mi><mo>′</mo></msup><msub><mrow></mrow><mi mathvariant="bold-italic">g</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo><msup><mi>b</mi><mo>′</mo></msup><msub><mrow></mrow><mi>η</mi></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>∂</mo><msubsup><mrow></mrow><mi>τ</mi><mi>l</mi></msubsup><mo>=</mo><msup><mi>σ</mi><mo>′</mo></msup><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">[</mo><mtext>Ι</mtext><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext></mrow><mi>l</mi></msubsup><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><msubsup><mrow></mrow><mi>τ</mi><mi>l</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>τ</mi></msub><mo>;</mo><mi>ξ</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="146">式中:<i>η</i>∈<b><i>R</i></b><sup>1×<i>F</i><sub>int</sub></sup>为1×1卷积的权值参数向量;AGs的特征由一组包含线性变换的参数<i>ξ</i><sub>att</sub>得到, 该参数包括线性转换系数<b><i>W</i></b><sub><b><i>x</i></b>′</sub>, <b><i>W</i></b><sub><b><i>g</i></b></sub> (<b><i>W</i></b><sub><b><i>x</i></b>′</sub>∈<b><i>R</i></b><sup><i>F</i><sub>int</sub>×<i>F</i><sub><i>l</i></sub></sup>, <b><i>W</i></b><sub><b><i>g</i></b></sub>∈<b><i>R</i></b><sup><i>F</i><sub>int</sub>×<i>F</i><sub><b><i>g</i></b></sub></sup>分别为输入图像和选通信号权重参数矩阵) ;<b><i>b</i></b>′<sub><b><i>g</i></b></sub>∈<b><i>R</i></b><sup><i>F</i><sub>int</sub>×1</sup>和<i>b</i>′<sub><i>η</i></sub>∈<b><i>R</i></b><sup>1×1</sup>为偏置项;<i>F</i>为像素特征向量的长度;<i>F</i><sub><i>l</i></sub>为输入<b><i>x</i></b>′<sup><i>l</i></sup><sub><i>τ</i></sub>的特征向量长度;<i>F</i><sub><b><i>g</i></b></sub>为选通信号<b><i>g</i></b>的特征向量长度;<i>σ</i>′<sub>1</sub>为ReLU激活函数, <i>σ</i>′<sub>1</sub> (<i>x</i>′<sup><i>l</i></sup><sub><i>τ</i>, <i>d</i></sub>) =max (0, <i>x</i>′<sup><i>l</i></sup><sub><i>τ</i>, <i>d</i></sub>) 。在 (14) 式中, 为防止特征过于稀疏, 选择Sigmoid激活函数<i>σ</i><sub>2</sub> (<i>x</i>′<sub><i>τ</i></sub>, <i>c</i>) =1/1+exp (-<i>x</i>′<sub><i>τ</i>.<i>d</i></sub>) 进行非线性变换;<b><i>x</i></b>′<sub><i>τ</i></sub>∈<b><i>R</i></b><sup><i>F</i><sub><i>l</i></sub>×1</sup>和<b><i>g</i></b><sub><i>τ</i></sub>∈<b><i>R</i></b><sup><i>F</i><sub><b><i>g</i></b></sub>×1</sup>分别为输入特征图和选通信号, AGs通过对选通信号的分析得到相应的门相关系数, 使得AGs能够集中于图像主要目标信息的结构特征, 剔除其他噪声等异常特征信息的影响。AGs在解码结构中通过跳过上池化层直接级联到下一个反卷积层以融合互补目标的特征信息, 同时采用1×1卷积层进行线性变换, 将特征解耦并映射到低维空间进行选通操作, 而U型结构自身具有将高层信息与底层信息结合的优势, 有助于进一步降低眼底图像细节因恢复不足而造成的目标结构分割断裂或出现缺口现象。AGs模型的内部结构和Attention机制公式兼容性张量计算如图6和图7所示。</p>
                </div>
                <div class="area_img" id="147">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908016_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 AGs内部结构图" src="Detail/GetImg?filename=images/GXXB201908016_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 AGs内部结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908016_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Internal structure of AGs</p>

                </div>
                <div class="area_img" id="148">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908016_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 兼容性张量维度计算图" src="Detail/GetImg?filename=images/GXXB201908016_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 兼容性张量维度计算图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908016_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Computation of compatibility tensor dimension</p>

                </div>
                <h4 class="anchor-tag" id="149" name="149"><b>3.5 自适应尺度信息的血管分割网络</b></h4>
                <div class="p1">
                    <p id="150">由于视网膜血管存在血管形态结构复杂和尺度信息多变的特点, 所以算法需要具有一定的自适应能力, 以确定目标不同的尺度或感受野大小, 而且算法自身能在编码部分合理地重复利用上下文特征信息, 在解码部分充分恢复目标结构, 抑制伪影和遮挡等噪声的影响, 降低编码部分的信息损失。因此, 本文用U型网络架构以充分利用上下文信息, 增强网络对小数据的适应性。在编码部分利用更丰富的滤波器对输入图像进行低维编码, 即采用密集可变形卷积网络, 该网络模型不仅具有原始卷积网络有效学习金标准提供的特征信息的能力, 而且具有自适应目标形态结构和尺度信息的能力, 使得算法更稳健地提取目标细节的特征信息;中间底层位置利用金字塔型空洞卷积, 通过设置不同空洞尺度捕捉目标局部或者全局特征, 并增加一定的感受野信息;解码部分采用带有Attention模型的上池化层与反卷积模块, 合并低维特征, 并降低编码部分最大池化层造成的信息损失, 从而得到更为优异的分割结果。自适应尺度信息的U型网络架构模型如图8所示。</p>
                </div>
                <div class="p1">
                    <p id="151">如图8所示, 本文网络结构共分为3大主要部分:编码部分、金字塔型空洞卷积和解码部分, 卷积核的大小均设置为3×3。编码部分分为4组DB-DC模块, 设置编码阶段初始DB-DC模块的滤波窗口为48×48, 通道数为32, 每一个DB-DC模块后紧跟一个1×1卷积层以减少输入特征映射的数量, 并能有效融合多通道的信息, 使网络提取的特征更加丰富。金字塔型空洞卷积通道数均为512。在解码部分, 反卷积层中带有跳过连接的AGs模型, 该模型的门控信号能聚合来自多个成像比例的信息, 并且能有效降低权重分散。最后由SoftMax激活函数对血管与背景图像进行分类。</p>
                </div>
                <div class="area_img" id="152">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908016_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 自适应尺度信息的U型网络架构模型" src="Detail/GetImg?filename=images/GXXB201908016_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 自适应尺度信息的U型网络架构模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908016_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 U-shaped network architecture model of adaptive scale information</p>

                </div>
                <h3 id="154" name="154" class="anchor-tag">4 实验结果分析与讨论</h3>
                <h4 class="anchor-tag" id="155" name="155"><b>4.1 实验环境与数据集</b></h4>
                <div class="p1">
                    <p id="156">仿真平台为PyCharm, 使用Keras及其TensorFlow端口, 计算机配置为Intel<sup>©</sup>Core<sup>TM</sup>i7-6700H CPU, 16 GB内存, Nvidia GeForce GTX 2070 GPU, 采用64位Win10操作系统。选取DRIVE (Digital Retinal Images for Vessel Extraction) <citation id="222" type="reference"><link href="58" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>与STARE (Structured Analysis of the Retina) <citation id="223" type="reference"><link href="60" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>数据集中第一专家手动分割的眼底血管图像作为训练标签, 第二专家手工分割的眼底血管图像作为最终血管分割结果参考标准。DRIVE数据集包括图像像素尺寸为565 pixel×584 pixel的20幅训练集图片和20幅测试集图片。STARE数据集包括20幅图像像素尺寸为605 pixel×700 pixel彩色眼底图像, 其中一半包含病变的眼底图像。上述两个数据集每幅图像均有专家标记的金标准, 且存在病变图像、血管结构复杂多变和微血管信息对比度不明显等特点, 说明该类数据集对图像分割任务具有一定的挑战性。考虑到STARE数据集没有区分训练图像与测试图像, 本文选取前10张图像作为训练图像, 后10张图像作为测试图像, 以便后续数据的进一步扩充。</p>
                </div>
                <h4 class="anchor-tag" id="157" name="157"><b>4.2 实验数据预处理</b></h4>
                <div class="p1">
                    <p id="158">由于数据集在采集时会存在伪影和畸变的现象, 故本文首先使用K-L变换提取出视网膜图像最优灰度特征图<i>I</i><sub>1</sub>, 再利用多尺度Top-Hot变换提高血管与背景的整体对比度, 预处理图像如图9所示。同时DRIVE和STARE数据集存在训练图像偏少的现象, 故本文首先通过对两个数据集以90°和45°为旋转轴、通过上下对换和左右对换等方式将数据集扩充到原来的10倍, 即DRIVE数据集训练图像为200张, STARE数据集训练图像为100张。</p>
                </div>
                <div class="p1">
                    <p id="159">为了使本文算法能更好地适应小数据模型, 并使算法泛化性更强, 需降低网络存在过拟合的程度。于是, 本文采用48×48的滑动窗口, 同时裁剪金标准图像和训练图像局部区域, 并通过局部信息熵采样的方法抽取出预处理图像中的信息熵最高的块状部分, 确保局部块状中含有目标特征信息。由于高信息熵的块状具有更多上下文表示信息, 因此通过局部信息熵采样的方式提取块状目标信息, 有助于捕捉血管重要的轮廓结构, 从而使得网络能够提取视网膜图像中更稳健的特征信息<citation id="224" type="reference"><link href="62" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>。图像信息熵定义为</p>
                </div>
                <div class="p1">
                    <p id="160" class="code-formula">
                        <mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>0</mn></mrow><mrow><mn>2</mn><mn>5</mn><mn>5</mn></mrow></munderover><mi>q</mi></mstyle><msub><mrow></mrow><mi>h</mi></msub><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mi>q</mi><msub><mrow></mrow><mi>h</mi></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="161">式中:<i>h</i>为灰度级;<i>q</i><sub><i>h</i></sub>为在<i>h</i>下的概率。血管局部块状信息如图10所示。对于每张训练集图像, 总共抽取最高信息熵的5000个训练块状信息。同时, 测试集为完整的视网膜图像, 且不进行数据扩充处理。</p>
                </div>
                <div class="area_img" id="162">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908016_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 各阶段预处理图像。" src="Detail/GetImg?filename=images/GXXB201908016_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 各阶段预处理图像。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908016_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Preprocessing images in each stage.</p>
                                <p class="img_note">（a）原始图像；（b）绿色通道；（c）主分量I<sub>1</sub>；（d）主分量I<sub>2</sub>；（e）主分量I<sub>3</sub>；（f）形态学变换</p>
                                <p class="img_note">（a）Original image；（b）image of green channel；（c）principal component I<sub>1</sub>;（d）principal component I<sub>2</sub>；（e）principal component I<sub>3</sub>；（f）morphological transformation</p>

                </div>
                <div class="area_img" id="163">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908016_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 血管局部块状信息图。" src="Detail/GetImg?filename=images/GXXB201908016_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 血管局部块状信息图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908016_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Local vessel block informations. </p>
                                <p class="img_note"> (a) 金标准块状信息; (b) DRIVE数据集块状信息</p>
                                <p class="img_note"> (a) Gold standard block information; (b) DRIVE data block information</p>

                </div>
                <h4 class="anchor-tag" id="165" name="165"><b>4.3 训练参数设置</b></h4>
                <div class="p1">
                    <p id="166">通过算法随机初始化初始权重信息, 设置批量大小为15, 迭代100次。通过交叉验证最小化像素错误率, 采用Adam算法优化损失函数, 其初始学习率设为默认值0.001。为了降低过拟合和加速训练过程收敛, 在训练过程中使用动态的方法设置学习率, 若经4次迭代损失值仍不改变, 则学习率降低为原来的1/10;若经20次迭代损失值仍不改变, 则停止训练<citation id="225" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="167" name="167"><b>4.4 不同算法血管分割结果主观分析</b></h4>
                <div class="p1">
                    <p id="168">图11给出在DRIVE和STARE数据集上本文算法与文献<citation id="226" type="reference">[<a class="sup">9</a>]</citation>和文献<citation id="227" type="reference">[<a class="sup">28</a>,<a class="sup">29</a>]</citation>中视网膜血管分割效果图。其中, 第1行为DRIVE数据集健康视网膜图像, 第2行和第3行为DRIVE数据集病变视网膜图像;第4行和第5行分别为STARE数据集健康视网膜图像与病变视网膜图像。图11 (a) 为原始图像, 图11 (b) 为第二专家金标准图像, 图11 (c) 为本文算法图像, 图11 (d) 为结合匹配滤波与原始U-net的分割图像, 图11 (e) 为基于带有辨别器的全连接CRF分割的结果, 图11 (f) 为生成对抗网络分割的结果。</p>
                </div>
                <div class="p1">
                    <p id="169">从图11第1行DRIVE数据集健康视网膜图像中可以看出, 本文算法、文献<citation id="228" type="reference">[<a class="sup">9</a>]</citation>和文献<citation id="229" type="reference">[<a class="sup">29</a>]</citation>的算法分割出的微血管数目较多, 而文献<citation id="230" type="reference">[<a class="sup">28</a>]</citation>虽然相比文献<citation id="231" type="reference">[<a class="sup">29</a>]</citation>在视盘位置处血管稳健性较强, 但丢失了大量的微血管信息, 因此灵敏度较低。从第2行和第3行的病变图像中可以看出, 本文算法、文献<citation id="232" type="reference">[<a class="sup">9</a>]</citation>和文献<citation id="233" type="reference">[<a class="sup">29</a>]</citation>的算法均较好地抑制了病理信息的过分割, 但文献<citation id="234" type="reference">[<a class="sup">28</a>]</citation>不仅没有分割出应有的微血管信息, 反而将一定的病理信息作为血管分割, 造成假阳性过高。本文集成的U-net算法与文献<citation id="235" type="reference">[<a class="sup">9</a>]</citation>中传统U-net算法在健康视网膜上相比, 能够分割出更稳健的微血管信息, 同时本文算法基本避免了病变视网膜图像中病理信息的误分割, 而且病变处的血管基本不会出现断裂的情况, 从而说明本文在解码阶段引入反卷积网络与Attention机制的有效性。文献<citation id="236" type="reference">[<a class="sup">29</a>]</citation>基于生成对抗网络方法与本文算法相比较存在视盘处主血管分割不全, 微血管分割断裂的现象。</p>
                </div>
                <div class="p1">
                    <p id="170">由图11第4行的健康视网膜图像可以看出, 文献<citation id="237" type="reference">[<a class="sup">9</a>]</citation>, <citation id="238" type="reference">[<a class="sup">28</a>,<a class="sup">29</a>]</citation>虽然解决了视盘被误分割的现象, 但是视盘周围主血管出现断裂和血管易链结的现象。而本文算法基本解决了上述3种算法存在的缺陷, 并且结合第5行病变视网膜图像中可以进一步看出本文算法不仅在病变眼底图像中有优良的分割性能, 而且可在病变较严重的眼底图像中分割出更多微小血管且不易产生断裂, 说明本文引入密集可变形卷积和金字塔型空洞卷积能有效地捕捉图像中的复杂结构信息。</p>
                </div>
                <div class="p1">
                    <p id="171">综上, 本文算法相比以上算法具有较大的改进, 能以优异的性能减少病理信息等噪声被误分割的可能, 在血管交叉处血管不易断裂且能分割出更加详细的微血管信息, 而且微血管与宽血管有较好的连通性和完整性, 不会出现微血管分割过粗与部分血管分割存在缺陷的问题, 从而说明本文集成的U型网络架构对于视网膜血管分割具有稳健性。</p>
                </div>
                <div class="area_img" id="172">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908016_172.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 不同算法视网膜血管分割结果。" src="Detail/GetImg?filename=images/GXXB201908016_172.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 不同算法视网膜血管分割结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908016_172.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Retinal vascular segmentation results using different algorithms.</p>
                                <p class="img_note"> (a) 原始图像; (b) 金标准; (c) 本文算法; (d) 文献<citation id="282" type="reference"><link href="26" rel="bibliography" />[9]</citation>; (e) 文献<citation id="283" type="reference"><link href="64" rel="bibliography" />[28]</citation>; (f) 文献<citation id="284" type="reference"><link href="66" rel="bibliography" />[29]</citation></p>
                                <p class="img_note">（a）Origin image；（b）gold standard of image;（c）proposed algorithm；（d）results in Ref．<citation id="285" type="reference"><link href="26" rel="bibliography" />[9]</citation>；（e）results in Ref．<citation id="286" type="reference"><link href="64" rel="bibliography" />[28]</citation>；（f）results in Ref．<citation id="287" type="reference"><link href="66" rel="bibliography" />[29]</citation></p>

                </div>
                <h4 class="anchor-tag" id="174" name="174"><b>4.5 不同算法血管分割结果细节分析</b></h4>
                <div class="p1">
                    <p id="175">为了更清晰地展现出本文算法性能的优异性, 图12比较了在DRIVE和STARE数据集上本文算法与文献<citation id="245" type="reference">[<a class="sup">9</a>]</citation>和文献<citation id="246" type="reference">[<a class="sup">29</a>]</citation>中深度学习算法得到的血管细节信息。图12显示血管交叉处与末端微血管处的局部放大的图像, 包含多个血管相互连接和接近的细节信息。其中图12 (a) 、 (b) 分别为原始图像和原始图像局部细节信息, 图12 (c) ～ (f) 分别为金标准、本文算法、文献<citation id="247" type="reference">[<a class="sup">9</a>]</citation>和文献<citation id="248" type="reference">[<a class="sup">29</a>]</citation>血管局部细节图。</p>
                </div>
                <div class="p1">
                    <p id="176">由于视网膜眼底图像的血管树复杂多变且存在伪影和病理信息的干扰, 精准分割血管的难度较大。在血管连接的区域, 文献<citation id="249" type="reference">[<a class="sup">9</a>]</citation>中U-net算法和文献<citation id="250" type="reference">[<a class="sup">29</a>]</citation>中生成对抗网络算法受网络的限制, 存在交叉血管分割断裂或血管缺口的现象, 文献<citation id="251" type="reference">[<a class="sup">9</a>]</citation>融合匹配滤波器和U-net相比文献<citation id="252" type="reference">[<a class="sup">29</a>]</citation>中生成对抗网络能提取更加详细的血管信息, 显示其捕获各种视网膜血管形状的能力。本文在原始U型网络编码部分融合密集可变形卷积, 解码部分引入带有Attention机制的反卷积层, 在捕捉血管不同尺度信息的同时能够有效恢复和整合编码部分的特征信息, 因此能成功地分割血管交叉处的血管, 较好地解决了交叉血管易断裂的问题。在微血管区域, 文献<citation id="253" type="reference">[<a class="sup">9</a>]</citation>中算法在细节方面表现出其局限性, 然而本文算法能系统地捕捉血管的形态结构信息, 从而能分割出更多的微血管信息。由图12细节图可知, 金标准图像和文献<citation id="254" type="reference">[<a class="sup">29</a>]</citation>中算法分割的血管出现较明显的锯齿形状和血管存在缺口的现象, 然而本文算法不仅能与金标准图像分割出几乎相同数量的微血管, 而且微血管粗细比金标准更加合理且不存在血管出现锯齿或缺口的现象。</p>
                </div>
                <div class="area_img" id="177">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908016_177.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 深度学习分割算法性能比较。" src="Detail/GetImg?filename=images/GXXB201908016_177.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 深度学习分割算法性能比较。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908016_177.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Performance comparison of deep learning segmentation algorithms.</p>
                                <p class="img_note">（a）原始图像；（b）原始图像细节；（c）金标准细节；（d）本文算法细节；（e）文献<citation id="288" type="reference"><link href="26" rel="bibliography" />[9]</citation>中算法细节；（f）文献<citation id="289" type="reference"><link href="66" rel="bibliography" />[29]</citation>中算法细节</p>
                                <p class="img_note">（a）Origin images；（b）details of original images；（c）details of gold standard；（d）details of proposed algorithm；（e）details of algorithm in Ref．<citation id="290" type="reference"><link href="26" rel="bibliography" />[9]</citation>；（f）details of algorithm in Ref．<citation id="291" type="reference"><link href="66" rel="bibliography" />[29]</citation></p>

                </div>
                <div class="p1">
                    <p id="178">实验表明, 引入密集可变形卷积和Attention机制反卷积层的U型网络, 能够区分结构不同的血管, 其分割结果优于其他算法, 从而说明本文算法在处理血管复杂处和血管薄弱的结构时具有更理想的性能。</p>
                </div>
                <h4 class="anchor-tag" id="179" name="179"><b>4.6 视网膜分割评价标准</b></h4>
                <div class="p1">
                    <p id="180">为了进一步定量分析本文算法对视网膜血管提取的算法性能, 定义几个评价指标为</p>
                </div>
                <div class="p1">
                    <p id="181" class="code-formula">
                        <mathml id="181"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>σ</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>e</mtext><mtext>n</mtext><mtext>s</mtext><mtext>i</mtext><mtext>t</mtext><mtext>i</mtext><mtext>v</mtext><mtext>i</mtext><mtext>t</mtext><mtext>y</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Τ</mi><msub><mrow></mrow><mtext>Ρ</mtext></msub></mrow><mrow><mi>Τ</mi><msub><mrow></mrow><mtext>Ρ</mtext></msub><mo>+</mo><mi>F</mi><msub><mrow></mrow><mtext>Ν</mtext></msub></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>σ</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>p</mtext><mtext>e</mtext><mtext>c</mtext><mtext>i</mtext><mtext>f</mtext><mtext>i</mtext><mtext>c</mtext><mtext>i</mtext><mtext>t</mtext><mtext>y</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Τ</mi><msub><mrow></mrow><mtext>Ν</mtext></msub></mrow><mrow><mi>Τ</mi><msub><mrow></mrow><mtext>Ν</mtext></msub><mo>+</mo><mi>F</mi><msub><mrow></mrow><mtext>Ρ</mtext></msub></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>σ</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>Ρ</mtext><mtext>R</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>F</mi><msub><mrow></mrow><mtext>p</mtext></msub></mrow><mrow><mi>F</mi><msub><mrow></mrow><mtext>p</mtext></msub><mo>+</mo><mi>Τ</mi><msub><mrow></mrow><mtext>Ν</mtext></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="left"><mi>σ</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ρ</mtext><mtext>R</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Τ</mi><msub><mrow></mrow><mtext>p</mtext></msub></mrow><mrow><mi>Τ</mi><msub><mrow></mrow><mtext>p</mtext></msub><mo>+</mo><mi>F</mi><msub><mrow></mrow><mtext>Ν</mtext></msub></mrow></mfrac></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>σ</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>i</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Τ</mi><msub><mrow></mrow><mtext>p</mtext></msub></mrow><mrow><mi>Τ</mi><msub><mrow></mrow><mtext>p</mtext></msub><mo>+</mo><mi>F</mi><msub><mrow></mrow><mtext>p</mtext></msub></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>σ</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>c</mtext><mtext>c</mtext><mtext>u</mtext><mtext>r</mtext><mtext>a</mtext><mtext>c</mtext><mtext>y</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Τ</mi><msub><mrow></mrow><mtext>p</mtext></msub><mo>+</mo><mi>Τ</mi><msub><mrow></mrow><mtext>Ν</mtext></msub></mrow><mrow><mi>Τ</mi><msub><mrow></mrow><mtext>p</mtext></msub><mo>+</mo><mi>Τ</mi><msub><mrow></mrow><mtext>Ν</mtext></msub><mo>+</mo><mi>F</mi><msub><mrow></mrow><mtext>p</mtext></msub><mo>+</mo><mi>F</mi><msub><mrow></mrow><mtext>Ν</mtext></msub></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtable><mtr><mtd><mi>F</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>s</mtext><mtext>u</mtext><mtext>r</mtext><mtext>e</mtext></mrow></msub><mo>=</mo><mn>2</mn><mi>σ</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>i</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext></mrow></msub><mfrac><mrow><mi>σ</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>e</mtext><mtext>n</mtext><mtext>s</mtext><mtext>i</mtext><mtext>t</mtext><mtext>i</mtext><mtext>v</mtext><mtext>i</mtext><mtext>t</mtext><mtext>y</mtext></mrow></msub></mrow><mrow><mi>σ</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>e</mtext><mtext>n</mtext><mtext>s</mtext><mtext>i</mtext><mtext>t</mtext><mtext>i</mtext><mtext>v</mtext><mtext>i</mtext><mtext>t</mtext><mtext>y</mtext></mrow></msub><mo>+</mo><mi>σ</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>i</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext></mrow></msub></mrow></mfrac></mtd></mtr></mtable><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="182">式中:<i>T</i><sub>p</sub>, <i>T</i><sub>N</sub>, <i>F</i><sub>p</sub>, <i>F</i><sub>N</sub>分别表示真阳性、真阴性、假阳性、假阴性;<i>σ</i><sub>Sensitivity</sub>为敏感度, 又称真阳性率, 表示正确分类的血管像素占真实血管像素的百分比;<i>σ</i><sub>Specificity</sub>为特异性, 表示正确分类的非血管像素占真实非血管像素的百分比;<i>σ</i><sub>FPR</sub>为假阳性率, 表示将背景像素错分为血管的像素占真实背景像素的百分比;<i>σ</i><sub>TPR</sub>为真阳性率, 表示分割血管像素占真实血管像素的比例;<i>σ</i><sub>Precision</sub>为精确率, 表示正确分类的血管像素占总分类像素的百分率;<i>σ</i><sub>Accuracy</sub>为准确率, 表示正确分类血管和非血管像素占整个图像总像素的百分比;<i>F</i><sub>measure</sub>为敏感度与精确率之间的比率关系。通过真阳性率和假阳性率之间的面积求出曲线下面积 (AUC) 并进行绘制。这些指标中<i>σ</i><sub>FPR</sub>越小, 其余指标越大, 说明分割算法可靠性越强。</p>
                </div>
                <div class="p1">
                    <p id="183">接受者操作特性 (ROC) 曲线用<i>T</i><sub>P</sub>和<i>F</i><sub>P</sub>之间的图形表示。AUC等指标与ROC相关, AUC范围为0～1。AUC值越接近1, 表示模型的预测能力越好, 而AUC值越低, 表示预测模型的错误分类概率越高。</p>
                </div>
                <h4 class="anchor-tag" id="184" name="184">4.6.1 不同网络结构算法性能比较</h4>
                <div class="p1">
                    <p id="185">为了充分说明本文将密集可变形卷积、空洞卷积和带有Attention机制的反卷积层融入U型网络的有效性, 将基于U型架构基础的网络做一些调整, 并给出在DRIVE数据集上测试的准确率、灵敏度、特异性和AUC值, 如表1所示, 其中加粗的部分为该项的最优值。具体调整如下:1) 仅用初始U-net进行分割, 记为M1;2) 文献<citation id="259" type="reference">[<a class="sup">15</a>]</citation>中可变形卷积与U-net相融合的分割算法, 记为M2;3) 文献<citation id="260" type="reference">[<a class="sup">30</a>]</citation>中反卷积网络与U-net相融合的分割算法, 记为M3;4) 在本文算法基础上去掉可变形卷积的U型网络, 记为M4;5) 在本文算法基础上去掉可变形卷积、Attention机制和空洞卷积, 仅保留反卷积层和DenseNet的U型网络, 记为M5;6) 仅利用原始U-net结构和底层空洞卷积的网络, 记为M6;7) 本文算法, 记为TP。</p>
                </div>
                <div class="area_img" id="186">
                    <p class="img_tit">表1 基于U型架构的不同网络结构性能比较 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Performance comparison of different network structures based on U-shaped network architecture</p>
                    <p class="img_note"></p>
                    <table id="186" border="1"><tr><td><br />Method</td><td><i>σ</i><sub>Accuracy</sub> /<br />%</td><td><i>σ</i><sub>Sensitivity</sub> /<br />%</td><td><i>σ</i><sub>Specificity</sub> /<br />%</td><td>AUC /<br />%</td></tr><tr><td><br />M1</td><td>96.21</td><td>79.69</td><td>98.52</td><td>98.30</td></tr><tr><td><br />M2</td><td>96.50</td><td>83.21</td><td>98.63</td><td>98.56</td></tr><tr><td><br />M3</td><td>96.29</td><td>81.12</td><td>98.52</td><td>98.41</td></tr><tr><td><br />M4</td><td>96.85</td><td>82.74</td><td>98.71</td><td>98.63</td></tr><tr><td><br />M5</td><td>96.54</td><td>81.50</td><td>98.20</td><td>98.28</td></tr><tr><td><br />M6</td><td>96.24</td><td>80.67</td><td>98.54</td><td>98.36</td></tr><tr><td><br />TP</td><td><b>97.48</b></td><td><b>85.78</b></td><td><b>98.83</b></td><td><b>98.72</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="187">由表1可知, 传统U-net对小数据的医学图像具有较强的稳健性, 在网络结构没有任何改进前其准确率、特异性和AUC值已经分别达到96.21%、98.52%和98.30%, 但存在灵敏度较低的现象。于是, 文献<citation id="261" type="reference">[<a class="sup">15</a>]</citation>和文献<citation id="262" type="reference">[<a class="sup">30</a>]</citation>对传统U-net进行改进, 其中文献<citation id="263" type="reference">[<a class="sup">15</a>]</citation>利用可变形卷积的灵敏度高于文献<citation id="264" type="reference">[<a class="sup">30</a>]</citation>, 说明可变形卷积对血管尺度信息和形态结构具有较强的捕捉能力, 但二者的准确率仍在96%左右, 说明传统上采样层存在信息的丢失。然而, 本文所设计的Attention U-net的分割精度达到97%, 并且灵敏度也有大幅度提升, 从而说明加入Attention机制有助于解决网络权重分散的现象, 将网络主要目标集中于分割血管, 可降低病理信息等噪声影响, 引入反卷积层可有效地恢复图像的细节信息, 有助于保持目标结构特征的良好拓扑结构。本文将上述算法进行合理融合, 尽可能地发挥出各个模块的优势, 使得准确率、灵敏度和AUC值分别达到97.48%、85.78%和98.72%, 进一步说明本文算法的合理性和有效性。</p>
                </div>
                <h4 class="anchor-tag" id="188" name="188">4.6.2 不同视网膜分割算法客观分析</h4>
                <div class="p1">
                    <p id="189">为了更好地表现出本文算法的血管分割性能, 表2和表3给出不同血管分割算法在DRIVE与STARE数据集上的灵敏度、特异性、准确率和AUC值, 其中加粗的部分为该项的最优值。</p>
                </div>
                <div class="area_img" id="190">
                    <p class="img_tit">表2 DRIVE数据库视网膜血管分割结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Retinal vessel segmentation results in DRIVE dataset</p>
                    <p class="img_note"></p>
                    <table id="190" border="1"><tr><td>Number</td><td>Method</td><td><i>σ</i><sub>Sensitivity</sub> /<br />%</td><td><i>σ</i><sub>Specificity</sub> /<br />%</td><td><i>σ</i><sub>Accuracy</sub> /<br />%</td><td>AUC /<br />%</td></tr><tr><td><br />1</td><td>2<sup>nd</sup> human observer</td><td>77.96</td><td>97.17</td><td>94.64</td><td>94.66</td></tr><tr><td><br />2</td><td>Method in Ref. [2]</td><td>71.40</td><td>98.68</td><td>96.07</td><td>90.86</td></tr><tr><td><br />3</td><td>Method in Ref. [3]</td><td>83.54</td><td>95.91</td><td>94.82</td><td>—</td></tr><tr><td><br />4</td><td>Method in Ref. [6]</td><td>75.35</td><td>97.26</td><td>95.36</td><td>—</td></tr><tr><td><br />5</td><td>Method in Ref. [7]</td><td>80.36</td><td>97.78</td><td>95.56</td><td>98.00</td></tr><tr><td><br />6</td><td>Method in Ref. [9]</td><td>78.02</td><td>98.76</td><td>96.36</td><td>95.88</td></tr><tr><td><br />7</td><td>Method in Ref. [10]</td><td>81.50</td><td>98.20</td><td>96.74</td><td>98.08</td></tr><tr><td><br />8</td><td>Method in Ref. [28]</td><td>78.97</td><td>96.84</td><td>—</td><td>—</td></tr><tr><td><br />9</td><td>Method in Ref. [29]</td><td>81.15</td><td>97.24</td><td>95.20</td><td>98.03</td></tr><tr><td><br />10</td><td>Method in Ref. [31]</td><td>80.53</td><td>97.67</td><td>95.46</td><td>97.71</td></tr><tr><td><br />11</td><td>Method in Ref. [32]</td><td>81.73</td><td>97.33</td><td><b>97.67</b></td><td>94.75</td></tr><tr><td><br />12</td><td>Method in Ref. [33]</td><td>76.91</td><td>98.01</td><td>95.33</td><td>—</td></tr><tr><td><br />13</td><td>Method in Ref. [34]</td><td>84.73</td><td>95.92</td><td>95.12</td><td>—</td></tr><tr><td><br />14</td><td>Method in Ref. [35]</td><td>77.31</td><td>97.24</td><td>94.67</td><td>—</td></tr><tr><td><br />15</td><td>Method in Ref. [36]</td><td>72.92</td><td>98.15</td><td>94.94</td><td>95.99</td></tr><tr><td><br />16</td><td>TP</td><td><b>85.78</b></td><td><b>98.83</b></td><td>97.48</td><td><b>98.72</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="191">
                    <p class="img_tit">表3 STARE数据库视网膜血管分割结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Retinal vessel segmentation results in STARE dataset</p>
                    <p class="img_note"></p>
                    <table id="191" border="1"><tr><td>Number</td><td>Method</td><td><i>σ</i><sub>Sensitivity</sub> /<br />%</td><td><i>σ</i><sub>Specificity</sub> /<br />%</td><td><i>σ</i><sub>Accuracy</sub> /<br />%</td><td>AUC /<br />%</td></tr><tr><td><br />1</td><td>2<sup>nd</sup> human observer</td><td>89.55</td><td>93.84</td><td>93.47</td><td>96.86</td></tr><tr><td><br />2</td><td>Method in Ref. [3]</td><td><b>84.52</b></td><td>96.19</td><td>95.34</td><td>—</td></tr><tr><td><br />3</td><td>Method in Ref. [6]</td><td>79.09</td><td>96.30</td><td>95.03</td><td>—</td></tr><tr><td><br />4</td><td>Method in Ref. [28]</td><td>63.50</td><td>97.38</td><td>—</td><td>—</td></tr><tr><td><br />5</td><td>Method in Ref. [29]</td><td>76.86</td><td>—</td><td>96.62</td><td>98.03</td></tr><tr><td><br />6</td><td>Method in Ref. [31]</td><td>82.99</td><td><b>97.94</b></td><td>96.84</td><td><b>98.17</b></td></tr><tr><td><br />7</td><td>Method in Ref. [32]</td><td>81.04</td><td>97.91</td><td><b>98.13</b></td><td>97.51</td></tr><tr><td><br />8</td><td>Method in Ref. [36]</td><td>72.11</td><td>98.40</td><td>95.69</td><td>97.08</td></tr><tr><td><br />9</td><td>Method in Ref. [37]</td><td>77.91</td><td>97.58</td><td>95.54</td><td>97.48</td></tr><tr><td><br />10</td><td>TP</td><td>84.32</td><td>97.75</td><td>96.83</td><td>98.13</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="192">由表2可知, 现有血管分割算法在DRIVE数据集上的灵敏度、特异性和AUC值均低于本文算法, 从而说明本文算法可得到更多的微血管信息且能正确分割非血管与血管像素, 进而说明本文算法的假阳性率低。文献<citation id="265" type="reference">[<a class="sup">10</a>]</citation>为前期的深度学习血管分割算法, 该算法仅局限于特征的重复利用和增加网络的感受野, 并未使网络自适应视网膜血管形态结构等信息, 并且U型网络存在解码部分特征提取能力不强等缺陷, 于是本文综合上述缺陷改进了前期的核心算法, 仅在编码部分使用密集连接层, 并采用可变形卷积以适应复杂多变的血管结构信息, 在解码部分加入Attention机制, 通过选通信号选择解码部分的最优特征, 同时利用上池化层定位最大池化层的位置信息并结合反卷积层恢复图像信息, 从而提高解码结构恢复图像和提取特征的能力, 本文算法的灵敏度和准确率分别比文献<citation id="266" type="reference">[<a class="sup">10</a>]</citation>高4.28%和0.74%。文献<citation id="267" type="reference">[<a class="sup">32</a>]</citation>将深度学习与机器学习有机结合, 在血管分割精度方面仅高于本文算法0.19%, 而本文算法的灵敏度和总体AUC值分别比文献<citation id="268" type="reference">[<a class="sup">32</a>]</citation>高4.05和3.97, 从而说明本文总体算法性能优越。同时, 文献<citation id="269" type="reference">[<a class="sup">32</a>]</citation>存在时间复杂度较高和网络结构复杂的缺陷, 训练时长达8 d左右, 但本文算法在DRIVE数据集训练时长约为7 h左右, 每幅图片的测试时长为10 s, 进一步说明本文采用的深度网络模型基本能达到临床辅助医疗的水平。</p>
                </div>
                <div class="p1">
                    <p id="193">表3中文献<citation id="270" type="reference">[<a class="sup">3</a>]</citation>灵敏度最高, 略高出本文算法0.20%, 但本文算法的准确率比文献<citation id="271" type="reference">[<a class="sup">3</a>]</citation>高1.49%。本文算法在STARE数据集上的特异性、准确率和AUC值与文献<citation id="272" type="reference">[<a class="sup">31</a>]</citation>相比相差甚微, 具有较高的灵敏度, 能分割出更多的微血管信息。然而, 通过表2可知, 文献<citation id="273" type="reference">[<a class="sup">31</a>]</citation>在DRIVE数据集上的分割结果均远低于本文算法, 进而说明本文算法的总体稳健性较强。文献<citation id="274" type="reference">[<a class="sup">32</a>]</citation>在STARE数据集中的准确率最高, 高于本文算法1.30%, 但其算法灵敏度和AUC值分别比本文算法低3.28%和0.62%, 从而说明本文算法在保持较高准确率的情况下, 相比文献<citation id="275" type="reference">[<a class="sup">31</a>]</citation>能分割出更多的微血管信息。本文算法的ROC曲线如图13所示, 其在DRIVE数据集中AUC值为98.72%[图13 (a) ], 其在STARE数据集的AUC值为98.13%[图13 (b) ]。</p>
                </div>
                <div class="p1">
                    <p id="194">图14 (a) 展示了在DRIVE数据集上文献<citation id="276" type="reference">[<a class="sup">3</a>]</citation>、文献<citation id="277" type="reference">[<a class="sup">6</a>]</citation>、文献<citation id="278" type="reference">[<a class="sup">38</a>]</citation>和本文算法的<i>F</i><sub>measure</sub>性能评价指标, 其结果分别为0.7389, 0.7259, 0.7970, 0.8547。图14 (b) 展示了在STARE数据集上文献<citation id="279" type="reference">[<a class="sup">3</a>]</citation>、文献<citation id="280" type="reference">[<a class="sup">6</a>]</citation>、文献<citation id="281" type="reference">[<a class="sup">39</a>]</citation>和本文算法的<i>F</i><sub>measure</sub>性能评价指标, 其结果分别为0.7341, 0.7251, 0.7271, 0.8382。从图14 (a) 、 (b) 中可以看出, 本文算法较其他文献在DRIVE和STRAE数据集上的<i>F</i><sub>measure</sub>曲线均较平滑且<i>F</i><sub>measure</sub>值较高, 说明本文算法不仅对健康视网膜图像具有很强的稳健性, 而且对病变视网膜图像也具有较好的分割性能。</p>
                </div>
                <div class="area_img" id="195">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908016_195.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 本文算法的ROC曲线。" src="Detail/GetImg?filename=images/GXXB201908016_195.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 本文算法的ROC曲线。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908016_195.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 13 ROC curves of proposed algorithm.</p>
                                <p class="img_note"> (a) DRIVE数据集ROC曲线; (b) STARE数据集ROC曲线</p>
                                <p class="img_note"> (a) ROC curve of DRIVE dataset; (b) ROC curve of STARE dataset</p>

                </div>
                <div class="area_img" id="196">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201908016_196.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图14 不同算法的Fmeasure。" src="Detail/GetImg?filename=images/GXXB201908016_196.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图14 不同算法的<i>F</i><sub>measure</sub>。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201908016_196.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 14 <i>F</i><sub>measure</sub> of different algorithms.</p>
                                <p class="img_note"> (a) DRIVE数据库的F<sub>measure</sub>; (b) STARE数据库中F<sub>measure</sub>曲线</p>
                                <p class="img_note"> (a) F<sub>measure</sub> of DRIVE dataset; (b) F<sub>measure</sub> of STARE dataset</p>

                </div>
                <h3 id="197" name="197" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="198">现阶段的深度学习神经网络能利用自身的分层学习特征的优势, 自动且高精度地完成识别图像目标形态结构的能力。根据现阶段视网膜图像分割技术存在的难点, 提出融合密集可变形卷积、金字塔型空洞卷积和带有Attention机制的反卷积层U型CNN。首先, 在预处理阶段利用二维K-L变换综合分析RGB三通道的颜色信息, 并提取第一主分量作为主要处理对象, 并通过多尺度形态学滤波进行简单的图像增强;然后, 根据图像信息熵将预处理图像划分为具有明显目标信息的块状图像并送入本文设计的U型深度神经网络中, U形结构的网络能以端到端的方式提取图像的局部特征, 并且密集可变形卷积、金字塔型空洞卷积和带有Attention机制的解码结构能根据目标的形态结构有效地提取学习或恢复目标特征信息。本文算法较好地解决了现有深度学习算法对小数据存在易过拟合和稳健性不高的缺陷, 将本文算法应用在医学图像视网膜血管分割上, 解决了微血管分割不足和视盘周围主血管断裂的问题, 较好地捕捉了目标复杂的形态结构, 有效降低了周围噪声的影响。该方法在整个过程中无需人工干预, 也不需要额外的后处理过程, 整体的泛化性较强。考虑本文算法在视网膜血管分割中仍会产生极少数微血管分割断裂血管和病理信息图像微血管分割不全面的问题, 对深度神经网络进行训练前, 仍需将视网膜图像进行较多的预处理。因此, 今后主要工作重点将集中在提高算法对病理信息的稳健性和进一步优化预处理过程以提高算法的泛化能力。同时, 进一步优化网络结构、添加自动剪枝, 以使网络更加轻量化。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300239047&amp;v=MTEwMDdJSVY4V2F4cz1OaWZPZmJLN0h0RE9ySTlGWnVnR0RIZytvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViLw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Fraz M M, Remagnino P, Hoppe A, <i>et al</i>.Blood vessel segmentation methodologies in retinal images:a survey[J].Computer Methods and Programs in Biomedicine, 2012, 108 (1) :407-433.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Retinal vessel segmentation in colour fundus images using extreme learning machine">

                                <b>[2]</b> Zhu C Z, Zou B J, Zhao R C, <i>et al</i>.Retinal vessel segmentation in colour fundus images using Extreme Learning Machine[J].Computerized Medical Imaging and Graphics, 2017, 55:68-77.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201811010&amp;v=MDYwNjI3QmRyRzRIOW5Ocm85RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmtVYjdCTHo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Liang L M, Liu B W, Yang H L, <i>et al</i>.Supervised blood vessel extraction in retinal images based on multiple feature fusion[J].Chinese Journal of Computers, 2018, 41 (11) :2566-2580.梁礼明, 刘博文, 杨海龙, 等.基于多特征融合的有监督视网膜血管提取[J].计算机学报, 2018, 41 (11) :2566-2580.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised retinal vessel segmentation from color fundus images based on matched filtering and AdaBoost classifier">

                                <b>[4]</b> Memari N, Ramli A R, Bin Saripan M I, <i>et al</i>.Supervised retinal vessel segmentation from color fundus images based on matched filtering and AdaBoost classifier[J].PLoS ONE, 2017, 12 (12) :e0188939.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002831189&amp;v=MTc3NDJ4RVplTUdZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTamxVcjdLSVZjPU5qN0Jhck80SHRIT3A0&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Rousson M, Paragios N.Prior knowledge, level set representations &amp; visual grouping[J].International Journal of Computer Vision, 2008, 76 (3) :231-243.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201807013&amp;v=MTE4NjBGckNVUkxPZVplVnVGeW5rVWI3Qkx6N0Jkckc0SDluTXFJOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Liang L M, Huang C L, Shi F, <i>et al</i>.Retinal vessel segmentation using level set combined with shape priori[J].Chinese Journal of Computers, 2018, 41 (7) :1678-1692.梁礼明, 黄朝林, 石霏, 等.融合形状先验的水平集眼底图像血管分割[J].计算机学报, 2018, 41 (7) :1678-1692.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811016&amp;v=MzA0MDJDVVJMT2VaZVZ1Rnlua1ViN0JJalhUYkxHNEg5bk5ybzlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Wu C Y, Yi B S, Zhang Y G, <i>et al</i>.Retinal vessel image segmentation based on improved convolutional neural network[J].Acta Optica Sinica, 2018, 38 (11) :1111004.吴晨玥, 易本顺, 章云港, 等.基于改进卷积神经网络的视网膜血管图像分割[J].光学学报, 2018, 38 (11) :1111004.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Retinal vessel segmentation via deep learning network and fully-connected conditional random fields">

                                <b>[8]</b> Fu H Z, Xu Y W, Wong D W K, <i>et al</i>.Retinal vessel segmentation via deep learning network and fully-connected conditional random fields[C]//2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI) , April 13-16, 2016, Prague, Czech Republic.New York:IEEE, 2016:698-701.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Retinal blood vessel segmentation based on the Gaussian matched filter and U-net">

                                <b>[9]</b> Gao X R, Cai Y H, Qiu C Y, <i>et al</i>.Retinal blood vessel segmentation based on the Gaussian matched filter and U-net[C]//2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI) , October 14-16, 2017, Shanghai.New York:IEEE, 2017:17582703.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 Liang L M, Sheng X Q, Guo K, <i>et al</i>.Improved U-Net retinal vessels segmentation [J/OL].Application Research of Computers, 2019, 37 (4) [2019-02-16].https://doi.org/10.19734/j.issn.1001-3695.2018.09.0775.梁礼明, 盛校棋, 郭凯, 等.基于改进的U-Net眼底视网膜血管分割[J/OL].计算机应用研究, 2019, 37 (4) [2019-02-16].https://doi.org/10.19734/j.issn.1001-3695.2018.09.0775.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-Net:convolutional networks for biomedical image segmentation">

                                <b>[11]</b> Ronneberger O, Fischer P, Brox T.U-Net:convolutional networks for biomedical image segmentation[M]//Navab N, Hornegger J, Wells W, <i>et al</i>.Medical image computing and computer-assisted intervention-MICCAI 2015.Lecture notes in computer science.Cham:Springer, 2015, 9351:234-241.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic detection of optic disc based on pca and mathematical morphology">

                                <b>[12]</b> Morales S, Naranjo V, Angulo J, <i>et al</i>.Automatic detection of optic disc based on PCA and mathematical morphology[J].IEEE Transactions on Medical Imaging, 2013, 32 (4) :786-796.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201819029&amp;v=MTgyNzR6cXFCdEdGckNVUkxPZVplVnVGeW5rVWI3Qkx6N01hYkc0SDluTnBvOUhiWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Wang H Y, Tong Q, Lian Z P, <i>et al</i>.K-L transform optimization algorithm for measurement matrix[J].Computer Engineering and Applications, 2018, 54 (19) :186-190, 215.王海艳, 佟岐, 连志鹏, 等.K-L变换观测矩阵优化算法[J].计算机工程与应用, 2018, 54 (19) :186-190, 215.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201404041&amp;v=MzEzNTJPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmtVYjdCTmlmWVpMRzRIOVhNcTQ5QlpZUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Liu Y L, Gui Z G.Contrast enhancement using extracted details based on multi-scale top-hat transformation[J].Computer Engineering and Design, 2014, 35 (4) :1332-1335, 1340.刘艳莉, 桂志国.多尺度top-hat变换提取细节的对比度增强算法[J].计算机工程与设计, 2014, 35 (4) :1332-1335, 1340.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DUNet:a deformable network for retinal vessel segmentation">

                                <b>[15]</b> Jin Q G, Meng Z P, Pham T D, <i>et al</i>.DUNet:a deformable network for retinal vessel segmentation[J].Knowledge-Based Systems, 2019, 178:149-162.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deformable Convolutional Networks">

                                <b>[16]</b> Dai J F, Qi H Z, Xiong Y W, <i>et al</i>.Deformable convolutional networks[C]//2017 IEEE International Conference on Computer Vision (ICCV) , October 22-29, 2017, Venice.New York:IEEE, 2017:764-773.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DenselyConnected Convolutional Networks">

                                <b>[17]</b> Huang G, Liu Z, van der Maaten L, <i>et al</i>.Densely connected convolutional networks[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI.New York:IEEE, 2017:2261-2269.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[18]</b> Ioffe S, Szegedy C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//32nd International Conference on International Conference on Machine Learning, July 6-11, 2015, Lille, France.Massachusetts:JMLR.org, 2015, 37:448-456.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES936E09C3510F694C1FD3E07228A3B546&amp;v=MTkwMTVtYUJ1SFlmT0dRbGZDcGJRMzVkaGh4Ym05d2FFPU5pZk9mYnE3R0tUTXB2eEdZZW9QZW5vd3kyVVNuRXQrUFgvbHJoQTlDTEhtUUw2WkNPTnZGU2lXV3I3SklGcA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> Li Y, Fan C X, Li Y, <i>et al</i>.Improving deep neural network with multiple parametric exponential linear units[J].Neurocomputing, 2018, 301:11-24.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic Image Segmentation Using Fully Convolutional Neural Networks with Multi-Scale Images and Multi-Scale Dilated Convolutions">

                                <b>[20]</b> Vo D M, Lee S W.Semantic image segmentation using fully convolutional neural networks with multi-scale images and multi-scale dilated convolutions[J].Multimedia Tools and Applications, 2018, 77 (14) :18689-18707.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding convolution for semantic segmentation">

                                <b>[21]</b> Wang P Q, Chen P F, Yuan Y, <i>et al</i>.Understanding convolution for semantic segmentation[C]//2018 IEEE Winter Conference on Applications of Computer Vision (WACV) , March 12-15, 2018, Lake Tahoe, NV.New York:IEEE, 2018:1451-1460.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">

                                <b>[22]</b> Zeiler M D, Fergus R.Visualizing and understanding convolutional networks[M]//Fleet D, Pajdla T, Schiele B, <i>et al</i>.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer, 2014, 8689:818-833.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deconvolution network for semantic segmentation">

                                <b>[23]</b> Noh H, Hong S, Han B.Learning deconvolution network for semantic segmentation[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1520-1528.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attention U-Net:learning where to look for the pancreas">

                                <b>[24]</b> Oktay O, Schlemper J, Folgoc L L, <i>et al</i>.Attention U-Net:learning where to look for the pancreas[J/OL]. (2018-08-20) [2019-02-16].https://arxiv.org/abs/1804.03999.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ridge-based vessel segmentation in color images of the retina">

                                <b>[25]</b> Staal J, Abramoff M D, Niemeijer M, <i>et al</i>.Ridge-based vessel segmentation in color images of the retina[J].IEEE Transactions on Medical Imaging, 2004, 23 (4) :501-509.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response">

                                <b>[26]</b> Hoover A D, Kouznetsova V, Goldbaum M.Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response[J].IEEE Transactions on Medical Imaging, 2000, 19 (3) :203-210.
                            </a>
                        </p>
                        <p id="62">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Patch-based fully convolutional neural network with skip connections for retinal blood vessel segmentation">

                                <b>[27]</b> Feng Z W, Yang J, Yao L X.Patch-based fully convolutional neural network with skip connections for retinal blood vessel segmentation[C]//2017 IEEE International Conference on Image Processing (ICIP) , September 17-20, 2017, Beijing.New York:IEEE, 2017:1742-1746.
                            </a>
                        </p>
                        <p id="64">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A discriminatively trained fully connected conditional random field model for blood vessel segmentation in fundus images">

                                <b>[28]</b> Orlando J I, Prokofyeva E, Blaschko M B.A discriminatively trained fully connected conditional random field model for blood vessel segmentation in fundus images[J].IEEE Transactions on Biomedical Engineering, 2017, 64 (1) :16-27.
                            </a>
                        </p>
                        <p id="66">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Retinal vessel segmentation in fundoscopic images with generative adversarial networks">

                                <b>[29]</b> Son J, Park S J, Jung K H.Retinal vessel segmentation in fundoscopic images with generative adversarial networks[J/OL]. (2017-06-28) [2019-02-16].https://arxiv.org/abs/1706.09318.
                            </a>
                        </p>
                        <p id="68">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Blood vessels segmentation by using CDNet">

                                <b>[30]</b> Peng S H, Zheng C X, Xu F, <i>et al</i>.Blood vessels segmentation by using CDNet[C]//2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC) , June 27-29, 2018, Chongqing.New York:IEEE, 2018:305-310.
                            </a>
                        </p>
                        <p id="70">
                            <a id="bibliography_31" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902014&amp;v=MjI0ODdlWmVWdUZ5bmtVYjdCSWpYVGJMRzRIOWpNclk5RVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[31]</b> Zheng T Y, Tang C, Lei Z K.Multi-scale retinal vessel segmentation based on fully convolutional neural network[J].Acta Optica Sinica, 2019, 39 (2) :0211002.郑婷月, 唐晨, 雷振坤.基于全卷积神经网络的多尺度视网膜血管分割[J].光学学报, 2019, 39 (2) :0211002.
                            </a>
                        </p>
                        <p id="72">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical retinal blood vessel segmentation based on feature and ensemble learning">

                                <b>[32]</b> Wang S L, Yin Y L, Cao G B, <i>et al</i>.Hierarchical retinal blood vessel segmentation based on feature and ensemble learning[J].Neurocomputing, 2015, 149:708-717.
                            </a>
                        </p>
                        <p id="74">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A fully convolutional neural network based structured prediction approach towards the retinal vessel segmentation">

                                <b>[33]</b> Dasgupta A, Singh S.A fully convolutional neural network based structured prediction approach towards the retinal vessel segmentation[C]//2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017) , April 18-21, 2017, Melbourne, Australia.New York:IEEE, 2017:248-251.
                            </a>
                        </p>
                        <p id="76">
                            <a id="bibliography_34" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201708025&amp;v=MjAzNzdCdEdGckNVUkxPZVplVnVGeW5rVWI3QklUZlNkckc0SDliTXA0OUhZWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[34]</b> Cai Y H, Gao X R, Qiu C Y, <i>et al</i>.Retinal vessel segmentation method with efficient hybrid features fusion[J].Journal of Electronics &amp; Information Technology, 2017, 39 (8) :1956-1963.蔡轶珩, 高旭蓉, 邱长炎, 等.一种混合特征高效融合的视网膜血管分割方法[J].电子与信息学报, 2017, 39 (8) :1956-1963.
                            </a>
                        </p>
                        <p id="78">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised vessel delineation in retinal fundus images with the automatic selection of B-COSFIRE filters">

                                <b>[35]</b> Strisciuglio N, Azzopardi G, Vento M, <i>et al</i>.Supervised vessel delineation in retinal fundus images with the automatic selection of B-COSFIRE filters[J].Machine Vision and Applications, 2016, 27 (8) :1137-1149.
                            </a>
                        </p>
                        <p id="80">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint segment-level and pixel-wise losses for deep learning based retinal vessel segmentation">

                                <b>[36]</b> Yan Z Q, Yang X, Cheng K T.Joint segment-level and pixel-wise losses for deep learning based retinal vessel segmentation[J].IEEE Transactions on Biomedical Engineering, 2018, 65 (9) :1912-1923.
                            </a>
                        </p>
                        <p id="82">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Robust Retinal Vessel Segmentat ion via Locally Adapt ive Derivative Frames in Orientation Scores,&amp;quot;">

                                <b>[37]</b> Zhang J, Dashtbozorg B, Bekkers E, <i>et al</i>.Robust retinal vessel segmentation via locally adaptive derivative frames in orientation scores[J].IEEE Transactions on Medical Imaging, 2016, 35 (12) :2631-2644.
                            </a>
                        </p>
                        <p id="84">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=N4-Fields:Neural Network Nearest Neighbor Fields for Image Transforms">

                                <b>[38]</b> Ganin Y, Lempitsky V.<i>N</i><sup>4</sup>-fields:neural network nearest neighbor fields for image transforms[M]//Cremers D, Reid I, Saito H, <i>et al</i>.Computer vision-ACCV 2014.Lecture notes in computer science.Cham:Springer, 2015, 9004:536-551.
                            </a>
                        </p>
                        <p id="86">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Holistically-nested edge detection">

                                <b>[39]</b> Xie S N, Tu Z W.Holistically-nested edge detection[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1395-1403.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201908016" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201908016&amp;v=MjI0MTA1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlua1ViN0JJalhUYkxHNEg5ak1wNDlFWW9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

