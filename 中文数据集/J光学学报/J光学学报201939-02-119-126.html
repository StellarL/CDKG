

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135525807756250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201902014%26RESULT%3d1%26SIGN%3dQ3kdLVISuNZNlUBgkM3VuLC%252bPGs%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201902014&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201902014&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902014&amp;v=MDE1Mjg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVWI3S0lqWFRiTEc0SDlqTXJZOUVZSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="2 网络结构与算法原理 ">2 网络结构与算法原理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#69" data-title="&lt;b&gt;2.1&lt;/b&gt;&lt;b&gt;基于残差学习的特征提取&lt;/b&gt;"><b>2.1</b><b>基于残差学习的特征提取</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;2.2&lt;/b&gt;&lt;b&gt;多尺度融合的ASPP模块&lt;/b&gt;"><b>2.2</b><b>多尺度融合的ASPP模块</b></a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;2.3&lt;/b&gt;&lt;b&gt;多尺度视网膜血管分割网络&lt;/b&gt;"><b>2.3</b><b>多尺度视网膜血管分割网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#102" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#103" data-title="&lt;b&gt;3.1&lt;/b&gt;&lt;b&gt;实验环境与数据库&lt;/b&gt;"><b>3.1</b><b>实验环境与数据库</b></a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;3.2&lt;/b&gt;&lt;b&gt;数据预处理&lt;/b&gt;"><b>3.2</b><b>数据预处理</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;3.3&lt;/b&gt;&lt;b&gt;网络参数设置&lt;/b&gt;"><b>3.3</b><b>网络参数设置</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;3.4&lt;/b&gt;&lt;b&gt;评价指标&lt;/b&gt;"><b>3.4</b><b>评价指标</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;3.5&lt;/b&gt;&lt;b&gt;结果分析&lt;/b&gt;"><b>3.5</b><b>结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#129" data-title="4 结 论 ">4 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="图1 残差块结构示意图">图1 残差块结构示意图</a></li>
                                                <li><a href="#82" data-title="图2 多尺度ASPP模块示意图">图2 多尺度ASPP模块示意图</a></li>
                                                <li><a href="#88" data-title="图3 视网膜血管分割网络结构图">图3 视网膜血管分割网络结构图</a></li>
                                                <li><a href="#107" data-title="图4 图像预处理。 (a) 典型的DRIVE数据集眼底图像; (b) 预处理之后的眼底图像">图4 图像预处理。 (a) 典型的DRIVE数据集眼底图像; (b) 预处理之后的眼底图像</a></li>
                                                <li><a href="#116" data-title="图5 基于DRIVE数据集的测试分割结果。 (a) 原始眼 底图像; (b) 图像分割标准图; (c) 分割结果图Fig. 5 Segmentation test results based on DRIVE dataset. (a) Original fundus images; (b) segmentation standard images; (c) segmentation results of images">图5 基于DRIVE数据集的测试分割结果。 (a) 原始眼 底图像; (b) 图像分割标准图; (c......</a></li>
                                                <li><a href="#118" data-title="图6 基于STARE数据集的测试分割结果。 (a) 原始眼 底图像; (b) 图像分割标准图; (c) 分割结果图">图6 基于STARE数据集的测试分割结果。 (a) 原始眼 底图像; (b) 图像分割标准图; (c......</a></li>
                                                <li><a href="#120" data-title="图7 局部区域分割结果。 (a) (b) 原始眼底图像; (c) ～ (f) 局部眼底图像; (g) ～ (j) 分割标准图; (k) ～ (n) 分割结果图">图7 局部区域分割结果。 (a) (b) 原始眼底图像; (c) ～ (f) 局部眼底图像; (g)......</a></li>
                                                <li><a href="#122" data-title="表1 DRIVE和STARE数据集的平均性能评估结果">表1 DRIVE和STARE数据集的平均性能评估结果</a></li>
                                                <li><a href="#127" data-title="表2 DRIVE数据集上与其他方法的性能对比">表2 DRIVE数据集上与其他方法的性能对比</a></li>
                                                <li><a href="#128" data-title="表3 STARE数据集上与其他方法的性能对比">表3 STARE数据集上与其他方法的性能对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Fraz M M, Barman S A, Remagnino P, &lt;i&gt;et al&lt;/i&gt;. An approach to localize the retinal blood vessels using bit planes and centerline detection[J]. Computer Methods and Programs in Biomedicine, 2012, 108 (2) : 600-616." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300239023&amp;v=MjY2Nzk2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpKS0Y4V2F4Yz1OaWZPZmJLN0h0RE9ySTlGWnVnR0RINA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Fraz M M, Barman S A, Remagnino P, &lt;i&gt;et al&lt;/i&gt;. An approach to localize the retinal blood vessels using bit planes and centerline detection[J]. Computer Methods and Programs in Biomedicine, 2012, 108 (2) : 600-616.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Azzopardi G, Strisciuglio N, Vento M, &lt;i&gt;et al&lt;/i&gt;. Trainable COSFIRE filters for vessel delineation with application to retinal images[J]. Medical Image Analysis, 2015, 19 (1) : 46-57." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES0864FBC4F9BDCCF2B04B9059EA7545AE&amp;v=MDc4NjRRTXZxQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHRnekxtOXdhMD1OaWZPZmJPd0dOVzYzZnhCRXVKOWVBOUt1UlJoNmpzUFFYL25wV2RFZnJlUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Azzopardi G, Strisciuglio N, Vento M, &lt;i&gt;et al&lt;/i&gt;. Trainable COSFIRE filters for vessel delineation with application to retinal images[J]. Medical Image Analysis, 2015, 19 (1) : 46-57.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" You X G, Peng Q M, Yuan Y, &lt;i&gt;et al&lt;/i&gt;. Segmentation of retinal blood vessels using the radial projection and semi-supervised approach[J]. Pattern Recognition, 2011, 44 (10/11) : 2314-2324." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738123&amp;v=Mjk0MjFNbndaZVp0RmlubFVyekpLRjhXYXhjPU5pZk9mYks3SHRETnFZOUZZK2dIRFg0Nm9CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         You X G, Peng Q M, Yuan Y, &lt;i&gt;et al&lt;/i&gt;. Segmentation of retinal blood vessels using the radial projection and semi-supervised approach[J]. Pattern Recognition, 2011, 44 (10/11) : 2314-2324.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Zhang J, Li H Q, Nie Q, &lt;i&gt;et al&lt;/i&gt;. A retinal vessel boundary tracking method based on Bayesian theory and multi-scale line detection[J]. Computerized Medical Imaging and Graphics, 2014, 38 (6) : 517-525." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061700188508&amp;v=MDg0MDVycVFUTW53WmVadEZpbmxVcnpKS0Y4V2F4Yz1OaWZPZmJLOEh0Zk5xSTlGWmVNSENYd3hvQk1UNlQ0UFFIL2lyUmRHZQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Zhang J, Li H Q, Nie Q, &lt;i&gt;et al&lt;/i&gt;. A retinal vessel boundary tracking method based on Bayesian theory and multi-scale line detection[J]. Computerized Medical Imaging and Graphics, 2014, 38 (6) : 517-525.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" Emary E, Zawbaa H M, Hassanien A E, &lt;i&gt;et al&lt;/i&gt;. Retinal vessel segmentation based on possibilistic fuzzy c-means clustering optimised with cuckoo search[C]. IEEE International Joint Conference on Neural Networks, 2014: 1792-1796." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Retinal Vessel Segmentation based on Possibilistic Fuzzy c-means Clustering Optimised with Cuckoo Search">
                                        <b>[5]</b>
                                         Emary E, Zawbaa H M, Hassanien A E, &lt;i&gt;et al&lt;/i&gt;. Retinal vessel segmentation based on possibilistic fuzzy c-means clustering optimised with cuckoo search[C]. IEEE International Joint Conference on Neural Networks, 2014: 1792-1796.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Zhu Z L, Wang J F. Image segmentation based on adaptive fuzzy C-means and post processing correction[J]. Laser &amp;amp; Optoelectronics Progress, 2018, 55 (1) : 011004. 朱占龙, 王军芬. 基于自适应模糊C均值与后处理的图像分割算法[J]. 激光与光电子学进展, 2018, 55 (1) : 011004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201801026&amp;v=MDg4MjZHNEg5bk1ybzlIWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEa1ViN05MeXJQWkw=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Zhu Z L, Wang J F. Image segmentation based on adaptive fuzzy C-means and post processing correction[J]. Laser &amp;amp; Optoelectronics Progress, 2018, 55 (1) : 011004. 朱占龙, 王军芬. 基于自适应模糊C均值与后处理的图像分割算法[J]. 激光与光电子学进展, 2018, 55 (1) : 011004.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" Zhao Y T, Rada L, Chen K, &lt;i&gt;et al&lt;/i&gt;. Automated vessel segmentation using infinite perimeter active contour model with hybrid region information with application to retinal images[J]. IEEE Transactions on Medical Imaging, 2015, 34 (9) : 1797-1807." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automated vessel segmentation using infinite perimeter active contour model with hybrid region information with application to retinal images">
                                        <b>[7]</b>
                                         Zhao Y T, Rada L, Chen K, &lt;i&gt;et al&lt;/i&gt;. Automated vessel segmentation using infinite perimeter active contour model with hybrid region information with application to retinal images[J]. IEEE Transactions on Medical Imaging, 2015, 34 (9) : 1797-1807.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" Niemeijer M, van Ginneken B, Loog M, &lt;i&gt;et al&lt;/i&gt;. Comparative study of retinal vessel segmentation methods on a new publicly available database[J]. Proceedings of SPIE, 2004, 5370: 648-656." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Comparative study of retinal vessel segmentation methods on a new publicly available database">
                                        <b>[8]</b>
                                         Niemeijer M, van Ginneken B, Loog M, &lt;i&gt;et al&lt;/i&gt;. Comparative study of retinal vessel segmentation methods on a new publicly available database[J]. Proceedings of SPIE, 2004, 5370: 648-656.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" Fraz M M, Remagnino P, Hoppe A, &lt;i&gt;et al&lt;/i&gt;. An ensemble classification-based approach applied to retinal blood vessel segmentation[J]. IEEE Transactions on Biomedical Engineering, 2012, 59 (9) : 2538-2548." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An ensemble classification-based approach applied to retinal blood vessel segmentation">
                                        <b>[9]</b>
                                         Fraz M M, Remagnino P, Hoppe A, &lt;i&gt;et al&lt;/i&gt;. An ensemble classification-based approach applied to retinal blood vessel segmentation[J]. IEEE Transactions on Biomedical Engineering, 2012, 59 (9) : 2538-2548.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" Wang W X, Fu Y T, Dong F, &lt;i&gt;et al&lt;/i&gt;. Infrared ship target detection method based on deep convolution neural network[J]. Acta Optica Sinica, 2018, 38 (7) : 0712006. 王文秀, 傅雨田, 董峰, 等. 基于深度卷积神经网络的红外船只目标检测方法[J]. 光学学报, 2018, 38 (7) : 0712006." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201807020&amp;v=MDY4MTZxQnRHRnJDVVI3cWZadVp0RmlEa1ViN05JalhUYkxHNEg5bk1xSTlIWklRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Wang W X, Fu Y T, Dong F, &lt;i&gt;et al&lt;/i&gt;. Infrared ship target detection method based on deep convolution neural network[J]. Acta Optica Sinica, 2018, 38 (7) : 0712006. 王文秀, 傅雨田, 董峰, 等. 基于深度卷积神经网络的红外船只目标检测方法[J]. 光学学报, 2018, 38 (7) : 0712006.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" Liu F, Shen T S, Ma X X. Convolutional neural network based multi-band ship target recognition with feature fusion[J]. Acta Optica Sinica, 2017, 37 (10) : 1015002. 刘峰, 沈同圣, 马新星. 特征融合的卷积神经网络多波段舰船目标识别[J]. 光学学报, 2017, 37 (10) : 1015002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201710031&amp;v=MDc1MTZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVWI3TklqWFRiTEc0SDliTnI0OUc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Liu F, Shen T S, Ma X X. Convolutional neural network based multi-band ship target recognition with feature fusion[J]. Acta Optica Sinica, 2017, 37 (10) : 1015002. 刘峰, 沈同圣, 马新星. 特征融合的卷积神经网络多波段舰船目标识别[J]. 光学学报, 2017, 37 (10) : 1015002.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" Liskowski P, Krawiec K. Segmenting retinal blood vessels with deep neural networks[J]. IEEE Transactions on Medical Imaging, 2016, 35 (11) : 2369-2380." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Segmenting Retinal Blood Vessels With&amp;lt;?Pub_newline?&amp;gt;Deep Neural Networks">
                                        <b>[12]</b>
                                         Liskowski P, Krawiec K. Segmenting retinal blood vessels with deep neural networks[J]. IEEE Transactions on Medical Imaging, 2016, 35 (11) : 2369-2380.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" Wang S, Yin Y, Cao G, &lt;i&gt;et al&lt;/i&gt;. Hierarchical retinal blood vessel segmentation based on feature and ensemble learning[J]. Neurocomputing, 2015, 149 (B) : 708-717." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700312646&amp;v=Mjc3MjZadEZpbmxVcnpKS0Y4V2F4Yz1OaWZPZmJLOEg5RE1xSTlGWitvTkNuZy9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Wang S, Yin Y, Cao G, &lt;i&gt;et al&lt;/i&gt;. Hierarchical retinal blood vessel segmentation based on feature and ensemble learning[J]. Neurocomputing, 2015, 149 (B) : 708-717.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" Li Q L, Feng B W, Xie L P, &lt;i&gt;et al&lt;/i&gt;. A cross-modality learning approach for vessel segmentation in retinal images[J]. IEEE Transactions on Medical Imaging, 2016, 35 (1) : 109-118." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;A Cross-Modality Learning Approach for Vessel Segmentation in Ret inal Images,&amp;quot;">
                                        <b>[14]</b>
                                         Li Q L, Feng B W, Xie L P, &lt;i&gt;et al&lt;/i&gt;. A cross-modality learning approach for vessel segmentation in retinal images[J]. IEEE Transactions on Medical Imaging, 2016, 35 (1) : 109-118.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" Fu H, Xu Y, Lin S, &lt;i&gt;et al&lt;/i&gt;. DeepVessel: retinal vessel segmentation via deep learning and conditional random field[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention, 2016: 132-139." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepVessel:Retinal vessel segmentation via deep learning and conditional random Field">
                                        <b>[15]</b>
                                         Fu H, Xu Y, Lin S, &lt;i&gt;et al&lt;/i&gt;. DeepVessel: retinal vessel segmentation via deep learning and conditional random field[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention, 2016: 132-139.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" Ronneberger O, Fischer P, Brox T. U-Net: convolutional networks for biomedical image segmentation[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention, 2015: 234-241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">
                                        <b>[16]</b>
                                         Ronneberger O, Fischer P, Brox T. U-Net: convolutional networks for biomedical image segmentation[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention, 2015: 234-241.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_17" title=" Song J, Lee B. Development of automatic retinal vessel segmentation method in fundus images via convolutional neural networks[C]. 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, 2017: 681-684." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Development of automatic retinal vessel segmentation method in fundus images via convolutional neural networks">
                                        <b>[17]</b>
                                         Song J, Lee B. Development of automatic retinal vessel segmentation method in fundus images via convolutional neural networks[C]. 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, 2017: 681-684.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_18" title=" He K, Zhang X, Ren S, &lt;i&gt;et al&lt;/i&gt;. Identity mappings in deep residual networks[C]. European Conference on Computer Vision, 2016: 630-645." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identity Mappings in Deep Residual Networks">
                                        <b>[18]</b>
                                         He K, Zhang X, Ren S, &lt;i&gt;et al&lt;/i&gt;. Identity mappings in deep residual networks[C]. European Conference on Computer Vision, 2016: 630-645.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_19" title=" Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift[C]. International Conference on Machine Learning, 2015: 448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[19]</b>
                                         Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift[C]. International Conference on Machine Learning, 2015: 448-456.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_20" >
                                        <b>[20]</b>
                                     He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;. Deep residual learning for image recognition[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 770-778.</a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_21" >
                                        <b>[21]</b>
                                     Srivastava N, Hinton G, Krizhevsky A, &lt;i&gt;et al&lt;/i&gt;. Dropout: a simple way to prevent neural networks from overfitting[J]. Journal of Machine Learning Research, 2014, 15 (1) : 1929-1958.</a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_22" >
                                        <b>[22]</b>
                                     Chen L C, Papandreou G, Kokkinos I, &lt;i&gt;et al&lt;/i&gt;. DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (4) : 834-848.</a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_23" title=" Xie S N, Tu Z W. Holistically-nested edge detection[J]. International Journal of Computer Vision, 2017, 125 (1/2/3) : 3-18." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Holistically-Nested Edge Detection">
                                        <b>[23]</b>
                                         Xie S N, Tu Z W. Holistically-nested edge detection[J]. International Journal of Computer Vision, 2017, 125 (1/2/3) : 3-18.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_24" title=" Staal J, Abr&#224;moff M D, Niemeijer M, &lt;i&gt;et al&lt;/i&gt;. Ridge-based vessel segmentation in color images of the retina[J]. IEEE Transactions on Medical Imaging, 2004, 23 (4) : 501-509." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ridge-based vessel segmentation in color images of the retina">
                                        <b>[24]</b>
                                         Staal J, Abr&#224;moff M D, Niemeijer M, &lt;i&gt;et al&lt;/i&gt;. Ridge-based vessel segmentation in color images of the retina[J]. IEEE Transactions on Medical Imaging, 2004, 23 (4) : 501-509.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_25" title=" Hoover A D, Kouznetsova V, Goldbaum M. Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response[J]. IEEE Transactions on Medical Imaging, 2000, 19 (3) : 203-210." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response">
                                        <b>[25]</b>
                                         Hoover A D, Kouznetsova V, Goldbaum M. Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response[J]. IEEE Transactions on Medical Imaging, 2000, 19 (3) : 203-210.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_26" title=" Reza A M. Realization of the contrast limited adaptive histogram equalization (CLAHE) for real-time image enhancement[J]. Journal of VLSI Signal Processing: Systems for Signal, Image, and Video Technology, 2004, 38 (1) : 35-44." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002832026&amp;v=Mjg2NjY3S0lWcz1OajdCYXJPNEh0SE9wNHhIWk9rSlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rml2a1c3&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         Reza A M. Realization of the contrast limited adaptive histogram equalization (CLAHE) for real-time image enhancement[J]. Journal of VLSI Signal Processing: Systems for Signal, Image, and Video Technology, 2004, 38 (1) : 35-44.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-10-07 14:16</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(02),119-126 DOI:10.3788/AOS201939.0211002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于全卷积神经网络的多尺度视网膜血管分割</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%91%E5%A9%B7%E6%9C%88&amp;code=39058469&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郑婷月</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%94%90%E6%99%A8&amp;code=08893875&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">唐晨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%9B%B7%E6%8C%AF%E5%9D%A4&amp;code=06502658&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">雷振坤</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A9%E6%B4%A5%E5%A4%A7%E5%AD%A6%E7%94%B5%E6%B0%94%E8%87%AA%E5%8A%A8%E5%8C%96%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0246359&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">天津大学电气自动化与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A7%E8%BF%9E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E5%B7%A5%E4%B8%9A%E8%A3%85%E5%A4%87%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0222286&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">大连理工大学工业装备结构分析国家重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出了一种基于多尺度特征融合的全卷积神经网络的视网膜血管分割方法, 无需手工设计特征和后处理过程。利用跳跃连接构建编码器-解码器结构全卷积神经网络, 将高层语义信息和低层特征信息进行融合;利用残差块进一步学习细节和纹理特征;利用不同空洞率的空洞卷积构建多尺度空间金字塔池化结构, 进一步扩大感受野, 充分结合图像上下文信息;采用类别平衡损失函数解决正负样本不均衡问题。实验结果表明, 在DRIVE (Digital Retinal Images for Vessel Extraction) 和STARE (Structured Analysis of the Retina) 数据集上的准确率分别为95.46%和96.84%, 敏感性分别为80.53%和82.99%, 特异性分别为97.67%和97.94%, 受试者工作特征 (ROC) 曲线下的面积分别为97.71%和98.17%。所提方法相较于其他方法性能更优。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E7%BD%91%E8%86%9C%E8%A1%80%E7%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视网膜血管;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">监督学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *唐晨, E-mail:tangchen@tju.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-02</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (11772081);</span>
                    </p>
            </div>
                    <h1>Multi-Scale Retinal Vessel Segmentation Based on Fully Convolutional Neural Network</h1>
                    <h2>
                    <span>Zheng Tingyue</span>
                    <span>Tang Chen</span>
                    <span>Lei Zhenkun</span>
            </h2>
                    <h2>
                    <span>School of Electrical and Information Engineering, Tianjin University</span>
                    <span>State Key Laboratory of Structural Analysis for Industrial Equipment, Dalian University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A method for retinal vessel segmentation is proposed based on a fully convolutional neural network with multi-scale feature fusion, which does not need hand-crafted features or specific post-processing. The architecture of skip connection is utilized, which combines the high-level semantic information with the low-level features. Residual block has been introduced to help learn details and texture features. The multi-scale spatial pyramid pooling module is built by atrous convolutions with different atrous rates to further enlarge the receptive fields and fully combine the context information. The class-balanced loss function is applied to solve the problem of imbalanced distribution of samples. The experimental results show that in the two datasets of digital retinal images for vessel extraction (DRIVE) and structured analysis of the retina (STARE) , the accuracies are 95.46% and 96.84%, the sensitivities are 80.53% and 82.99%, the specificities are 97.67% and 97.94%, and the areas under receiver operating characteristic (ROC) curve are 97.71% and 98.17%, respectively. The proposed method is superior to the other existing methods.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=retinal%20vessels&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">retinal vessels;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fully%20convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fully convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=supervised%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">supervised learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-08-02</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="63" name="63" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="64">视网膜血管是人体中唯一能够非创伤观察到的完整血管结构, 其形态结构变化是诊断糖尿病、高血压、微动脉瘤和动脉硬化等疾病的重要参考。因此, 视网膜图像的分析检测对血管疾病的早期筛查和诊断至关重要。然而, 视网膜血管分割需要专业医生进行手工标注, 既耗时又费力, 且容易受主观因素的影响, 无法满足大规模眼底图像分析的需要。近年来, 有关视网膜血管分割算法的研究受到广泛关注, 面临的困难包括<citation id="131" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>:1) 不同区域中血管大小、形状和亮度有较大差异;2) 视网膜边界、视盘、中央凹等区域的变化会造成一定的干扰;3) 图像采集的光照不均匀和数据量缺乏。</p>
                </div>
                <div class="p1">
                    <p id="65">目前, 不少国内外专家学者已对视网膜血管分割算法进行研究, 研究方法主要分为非监督和监督学习两类。非监督学习方法利用特征之间的内在联系识别目标血管, 主要包括匹配滤波器法、血管追踪法、形态学处理法等。文献<citation id="132" type="reference">[<a class="sup">2</a>]</citation>通过组合移位滤波器选择性检测条形结构。血管追踪法<citation id="133" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>通过在血管中心线和边缘上设置初始种子点追踪血管。文献<citation id="134" type="reference">[<a class="sup">4</a>]</citation>通过提取血管交叉点的强度分布特征构建多尺度线检测器。聚类方法<citation id="138" type="reference"><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>根据特征空间的相对距离对像素进行分类。文献<citation id="135" type="reference">[<a class="sup">7</a>]</citation>利用活动轮廓模型学习不同区域的特征信息。然而, 无监督方法缺乏监督信息, 分割结果的精度较低。监督学习方法由特征提取和像素分类两个阶段组成。其中, 特征提取方法可分为手工设计特征的方法和基于学习的方法。文献<citation id="136" type="reference">[<a class="sup">8</a>]</citation>采用高斯匹配滤波器提取特征。文献<citation id="137" type="reference">[<a class="sup">9</a>]</citation>利用血管方向信息, 结合决策树和集成学习进行像素分类。然而, 手工设计特征的方法费时费力, 相比数据驱动的方式, 更容易受主观因素的影响。</p>
                </div>
                <div class="p1">
                    <p id="66">近年来, 卷积神经网络 (CNN) 算法成为研究热点, 其在图像识别和检测<citation id="143" type="reference"><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>等领域中都取得了显著成果。与传统机器学习方法不同, CNN算法通过权重共享和逐层训练, 在训练数据的驱动下提取图像由低层至高层的特征, 具有良好的尺度平移不变性和稳健性。文献<citation id="139" type="reference">[<a class="sup">12</a>]</citation>利用深度神经网络进行像素分类。文献<citation id="140" type="reference">[<a class="sup">13</a>]</citation>提出一种结合CNN和随机森林的视网膜血管分割方法, 并利用集成学习提升分割效果。文献<citation id="141" type="reference">[<a class="sup">14</a>]</citation>提出了一种跨模态学习方法进行血管分割, 文献<citation id="142" type="reference">[<a class="sup">15</a>]</citation>设计了一种带侧输出的CNN, 并利用条件随机场学习像素之间的相关性以及多层次的特征信息, 但仍然存在多输出导致的参数过多和过拟合问题。最近, 基于U-net的分割方法<citation id="144" type="reference"><link href="41" rel="bibliography" /><link href="43" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>在医学图像分割上取得了良好的效果, 但是未针对多尺度的图像分割进行优化。</p>
                </div>
                <div class="p1">
                    <p id="67">针对上述参数过多和细小血管的分割问题, 本文提出一种基于多尺度特征融合的全卷积神经网络的视网膜血管分割方法。首先, 采用残差学习和空洞空间金字塔池化 (ASPP) 模块, 学习来自不同感受野的多尺度血管特征。在减少学习参数的同时, 有效提高细小血管分割的准确性和泛化性。实验证明, 所提方法相较于其他血管分割方法具有更好的分割性能, 可以进一步扩展到其他图像分割领域中。</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag">2 网络结构与算法原理</h3>
                <h4 class="anchor-tag" id="69" name="69"><b>2.1</b><b>基于残差学习的特征提取</b></h4>
                <div class="p1">
                    <p id="70">经典CNN利用卷积层和池化层提取图像特征, 并通过逐层前向学习和梯度反向传播优化参数。残差学习的思想是利用多层卷积拟合一种残差映射<b><i>F</i></b> (<b><i>x</i></b><sub><i>l</i></sub>, <b><i>W</i></b><sub><i>l</i></sub>) , 相较于直接学习目标近似恒等映射更容易优化。假设输入变量为<b><i>x</i></b>, 目标输出的实际映射为<b><i>H</i></b> (<b><i>x</i></b><sub><i>l</i></sub>) , 则学习到的残差映射<citation id="145" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>可以定义为</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">F</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>l</mi></msub><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi mathvariant="bold-italic">Η</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>l</mi></msub><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">式中:<b><i>x</i></b><sub><i>l</i></sub>为<i>l</i>层的输入量;<b><i>W</i></b><sub><i>l</i></sub>为<i>l</i>层的权重矩阵。通过“短路连接”的方式, 将恒等映射加入到堆叠的卷积层输出中, 这种方式避免了网络加深导致的梯度消失等问题。因此, 在卷积层之间引入残差学习, 构建的残差块结构如图1所示。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902014_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 残差块结构示意图" src="Detail/GetImg?filename=images/GXXB201902014_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 残差块结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902014_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Structural diagram of residual block</p>

                </div>
                <div class="p1">
                    <p id="74">为了控制输出的特征层通道数, 在恒等映射中加入一层卷积核大小为1×1的卷积层, 具体可以定义为</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">F</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>l</mi></msub><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi mathvariant="bold-italic">Η</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">式中:<i>f</i> (<b><i>x</i></b><sub><i>l</i></sub>) 代表恒等映射上的卷积操作。每一个残差块的残差映射都由两层卷积核大小为3×3的卷积层堆叠而成。所有卷积层都配有修正线性单元 (ReLU) 激活函数, 并添加批规范化 (BN) 处理<citation id="146" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, 以加快网络收敛。所提方法按照BN-ReLU-Conv的顺序构建残差块结构, 可较快地进行模型训练, 同时提高分割性能<citation id="147" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。为了避免过拟合现象, 每个卷积层进行<i>L</i><sub>2</sub>正则化, 通过添加<i>L</i><sub>2</sub>范数正则化项, 可使反向传播中的权重衰减, 抗过拟合能力增强。同时, 残差块中的堆叠卷积层之间还引入了随机失活层<citation id="148" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation> (失活概率设为0.2) , 迭代中随机失活部分神经元, 且不依赖某些局部特征, 从而防止参数过拟合, 有效提高泛化性能和稳健性。编码器模块中的池化层会导致特征信息丢失, 本研究使用卷积层代替原始的池化层。在所提出的网络中, 下采样的卷积核大小为3×3, 步长为2。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>2.2</b><b>多尺度融合的ASPP模块</b></h4>
                <div class="p1">
                    <p id="78">CNN中卷积核尺寸越大, 对应的感受野也就越大, 但同时会导致学习参数增多。针对上述问题, Chen等<citation id="149" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>提出空洞卷积的方式, 在不增加额外的参数的同时扩大感受野, 并保留多尺度特征和细节信息。空洞卷积是一种带孔的卷积, 能够通过控制扩张率<i>r</i>改变输出特征图的分辨率。对于输出特征层上<i>y</i>的每个像素点<i>i</i>, 空洞卷积操作的过程如下:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>x</mi></mstyle><mo stretchy="false"> (</mo><mi>i</mi><mo>+</mo><mi>r</mi><mi>k</mi><mo stretchy="false">) </mo><mi>w</mi><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">式中:<i>w</i>代表卷积核;<i>k</i>代表卷积核尺寸;<i>x</i>代表卷积输入量。<i>r</i>决定输入信号的采样间隔, 代表在输入特征层上引入<i>r</i>-1个小孔, 通过调整<i>r</i>可以自适应地调节感受野大小, 卷积核大小为3×3的卷积层的扩张率等效为1。随着深度网络模型的层数增多, 图像的细节信息丢失过多。同时, 网络过于复杂会造成过拟合等问题。为了简化网络模型, 保留多尺度的血管特征, 提出利用空洞卷积构建一个ASPP模块, 如图2所示。</p>
                </div>
                <div class="p1">
                    <p id="81">该模块主要包括4个并行通道, 即一个卷积核大小为1×1的卷积层和三层空洞卷积层, 空洞卷积层的卷积核尺寸均为3×3, <i>r</i>分别为1, 2, 4, 4个通道卷积的特征层数均为128。接着, 将4个卷积层的输出特征层进行融合, 作为下一个残差块的输出。一方面, 由于4个通道的扩张率不同, 多尺度ASPP模块可以学习来自不同感受野的多尺度特征, 从而有效提取细小血管。另一方面, 并行的多通道结构可以避免网络层数过多, 减少过拟合现象。同时, 空洞卷积的操作大大减少了学习参数, 避免学习过多参数, 降低了模型成本。</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902014_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 多尺度ASPP模块示意图" src="Detail/GetImg?filename=images/GXXB201902014_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 多尺度ASPP模块示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902014_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Schematic of multi-scale ASPP module</p>

                </div>
                <h4 class="anchor-tag" id="84" name="84"><b>2.3</b><b>多尺度视网膜血管分割网络</b></h4>
                <div class="p1">
                    <p id="85">所提出的分割网络结构如图3所示, 主要由编码器模块、多尺度ASPP模块和解码器模块组成。</p>
                </div>
                <div class="p1">
                    <p id="86">编码器和解码器中分别有两个残差块结构, 其卷积特征层通道数设为64和128。ASPP模块输出到一个残差块, 其卷积特征层通道数设为256。使用跳跃连接结构, 将编码器部分对应的低层特征与高层语义信息进行融合, 通过反卷积操作进行上采样, 构成一个对称的分割网络。倒数第二层使用卷积核为1×1的卷积层, 将多通道特征图映射到通道数为<i>n</i>的特征图, <i>n</i>代表像素类别的个数。本研究中, <i>n</i>=2代表视网膜的前景和背景两类。最后, 将网络生成的高分辨率特征图送入Softmax分类器<i>C</i><sub>Softmax</sub>, 并输出血管分割概率图, 表征每个像素是血管还是背景的概率值。第<i>m</i>个最终输出<i>o</i><sub><i>m</i></sub>的后验概率可表示为</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>o</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>C</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>o</mtext><mtext>f</mtext><mtext>t</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>a</mi><msubsup><mrow></mrow><mi>m</mi><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mi>a</mi><msubsup><mrow></mrow><mi>m</mi><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mi>j</mi></msub><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi>a</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup><mo stretchy="false">) </mo></mrow></mfrac><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902014_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 视网膜血管分割网络结构图" src="Detail/GetImg?filename=images/GXXB201902014_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 视网膜血管分割网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902014_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Structural diagram of network for retinal vessel segmentation</p>

                </div>
                <div class="p1">
                    <p id="89">式中:<i>a</i><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup></mrow></math></mathml>代表分类器第m个激活输出; a<mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup></mrow></math></mathml>代表分类器第j个激活输出。</p>
                </div>
                <div class="p1">
                    <p id="92">因为血管和非血管正负样本的分布不均衡, 引入权重项β, 自动调整网络损失函数。这里借鉴图像边缘提取方法<citation id="150" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>, 引入类别均衡的交叉熵损失函数。假设<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>X</mi><mo>=</mo><mrow><mo>{</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>, </mo><mrow><mo>|</mo><mi>X</mi><mo>|</mo></mrow></mrow><mo>}</mo></mrow></mrow></math></mathml>代表原始输入图像, x<sub>i</sub>代表像素i的输入量, <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi>X</mi><mo>|</mo></mrow></mrow></math></mathml>代表输入图像的像素个数;<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Y</mi><mo>=</mo><mrow><mo>{</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>, </mo><mrow><mo>|</mo><mi>X</mi><mo>|</mo></mrow></mrow><mo>}</mo></mrow></mrow></math></mathml>代表对应的二元标签值, y<sub>i</sub>代表像素i的输出标签值, 即<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo></mrow><mrow><mo>{</mo><mrow><mn>0</mn><mo>, </mo><mn>1</mn></mrow><mo>}</mo></mrow></mrow></math></mathml>;W代表网络中的权重参数。因此, 网络的损失函数可定义为</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false"> (</mo><mi>W</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mi>β</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mi>X</mi><mo>|</mo></mrow></mrow></munderover><mi>y</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>lg</mi><mrow></mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo stretchy="false">|</mo><mi>X</mi><mo>;</mo><mi>W</mi><mo stretchy="false">) </mo><mo>-</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>β</mi><mo stretchy="false">) </mo><mo>×</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mi>X</mi><mo>|</mo></mrow></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mi>lg</mi><mrow></mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>0</mn><mo stretchy="false">|</mo><mi>X</mi><mo>;</mo><mi>W</mi><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">式中:<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><mo>=</mo><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mo>-</mo></msub></mrow><mo>|</mo></mrow><mo>/</mo><mrow><mo>|</mo><mi>X</mi><mo>|</mo></mrow><mo>, </mo><mn>1</mn><mo>-</mo><mi>β</mi><mo>=</mo><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mo>+</mo></msub></mrow><mo>|</mo></mrow><mo>/</mo><mrow><mo>|</mo><mi>X</mi><mo>|</mo></mrow></mrow></math></mathml>, 其中<mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mo>+</mo></msub></mrow><mo>|</mo></mrow></mrow></math></mathml>和<mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mo>-</mo></msub></mrow><mo>|</mo></mrow></mrow></math></mathml>分别代表输入图像中的血管像素数和非血管像素数。</p>
                </div>
                <h3 id="102" name="102" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="103" name="103"><b>3.1</b><b>实验环境与数据库</b></h4>
                <div class="p1">
                    <p id="104">实验模型是基于Tensorflow的深度学习框架, 实验硬件配置为美国Intel E5-2685 CPU (Central Processing Unit) 和美国NVIDIA Titan XP GPU (Graphics Processing Unit) , 软件环境为Ubuntu 16.04操作系统。实验所使用的眼底图像数据来自于DRIVE<citation id="151" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation> (Digital Retinal Images for Vessel Extraction) 和STARE<citation id="152" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation> (Structured Analysis of the Retina) 两个公开眼底数据集。其中, DRIVE数据集来自400名糖尿病患者的眼底图像, 包含20幅训练和20幅测试图像, 均由日本Canon CR5 3CCD相机拍摄, 图像分辨率为584 pixel×565 pixel。测试集中有两个专家的分割标准图, 第一个专家的分割图作为分割标准, 第二个专家的分割图用于结果对比。STARE数据集包含20幅彩色眼底图像, 由日本Topcon TRV-50眼底相机拍摄, 图像分辨率为605 pixel×700 pixel。由于STARE数据集没有区分训练集和测试集, 采用留一法进行交叉验证, 即每次选用1幅图像作为测试集, 选用剩余的19幅图像作为训练集, 重复上述操作, 最后对训练出的20个模型结果求平均。</p>
                </div>
                <h4 class="anchor-tag" id="105" name="105"><b>3.2</b><b>数据预处理</b></h4>
                <div class="p1">
                    <p id="106">为了减小背景干扰、光照不均匀等因素的影响, 进一步提高血管分割效果, 需要进行适当的图像预处理。图4表示原始视网膜和预处理之后的视网膜。首先, 将输入眼底图像转化为灰度图像, 再通过对比度受限的自适应直方图均衡化<citation id="153" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>和伽马亮度调整, 提高图像亮度和对比度。图像输入尺寸过大会导致模型复杂, 需要对输入的眼底图像进行分块处理, 通过滑窗方式裁剪, 在每幅眼底图像上随机裁剪出4800个尺寸为32×32的重叠图像块。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902014_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 图像预处理。 (a) 典型的DRIVE数据集眼底图像; (b) 预处理之后的眼底图像" src="Detail/GetImg?filename=images/GXXB201902014_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 图像预处理。 (a) 典型的DRIVE数据集眼底图像; (b) 预处理之后的眼底图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902014_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Image preprocessing. (a) Typical fundus image of DRIVE dataset; (b) fundus image after preprocessing</p>

                </div>
                <h4 class="anchor-tag" id="108" name="108"><b>3.3</b><b>网络参数设置</b></h4>
                <div class="p1">
                    <p id="109">网络权重采用正态分布随机初始化, 训练过程使用随机梯度下降法优化, 动量因子设为0.3, 学习率设为0.01。模型训练迭代次数为150, 训练块的大小设为32×32。两个数据集中, 均选择训练集中10%的图像块作为验证集。该实验配置下, 整个模型训练过程大约需要4 h。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110"><b>3.4</b><b>评价指标</b></h4>
                <div class="p1">
                    <p id="111">视网膜血管分割相当于像素分类问题, 即判断每个像素属于血管还是非血管。实验中, 为了定量地对血管分割效果进行评估, 使用4种评价指标, 包括准确率<i>R</i><sub>Acc</sub>、敏感性<i>R</i><sub>Se</sub>、特异性<i>R</i><sub>Sp</sub>和受试者工作特征 (ROC) 曲线下方的面积 (AUC) <i>R</i><sub>AUC</sub>。前3种评价指标可分别定义为</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>c</mtext><mtext>c</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ρ</mtext></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ν</mtext></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ρ</mtext></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ν</mtext></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>Ρ</mtext></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>Ν</mtext></mrow></msub></mrow></mfrac><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>R</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>e</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ρ</mtext></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ρ</mtext></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>Ν</mtext></mrow></msub></mrow></mfrac><mo>‚</mo><mi>R</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>p</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ν</mtext></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ν</mtext></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>Ρ</mtext></mrow></msub></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">式中:<i>N</i><sub>TP</sub>为真阳性, 即分割正确的血管像素个数;<i>N</i><sub>TN</sub>为真阴性, 即分割正确的非血管像素个数;<i>N</i><sub>FP</sub>为假阳性, 即分割错误的血管像素个数;<i>N</i><sub>FN</sub>为假阴性, 即分割错误的非血管像素个数。敏感性用来衡量正确检测出血管像素的能力;特异性用来衡量识别非血管像素的能力;准确率表示正确分割的像素在全部像素中所占的比例。ROC曲线是以假阳性率为横坐标、真阳性率为纵坐标。ROC曲线下方的面积越接近1, 说明算法的分割效果越好。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114"><b>3.5</b><b>结果分析</b></h4>
                <div class="p1">
                    <p id="115">实验不仅通过分割效果图进行定性分析比对, 还利用4种评价指标定量评估分割性能。图5和图6分别表示DRIVE和STARE数据集上的视觉分割效果。由上述结果图可以看出, 分割结果与专家标注的结果基本一致, 在一些细小的血管分支上分割效果更好, 证明所提出的网络有较好的分割效果。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902014_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 基于DRIVE数据集的测试分割结果。 (a) 原始眼 底图像; (b) 图像分割标准图; (c) 分割结果图Fig. 5 Segmentation test results based on DRIVE dataset. (a) Original fundus images; (b) segmentation standard images; (c) segmentation results of images" src="Detail/GetImg?filename=images/GXXB201902014_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 基于DRIVE数据集的测试分割结果。 (a) 原始眼 底图像; (b) 图像分割标准图; (c) 分割结果图Fig. 5 Segmentation test results based on DRIVE dataset. (a) Original fundus images; (b) segmentation standard images; (c) segmentation results of images  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902014_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="117">图7中展示了一些局部血管区域对比图, 包括交叉点上的粗血管和低对比度下的细小血管。可以看出, 所提方法不受低对比度和血管形状多变等因素的影响, 仍然表现出良好的分割性能, 算法具有较好的稳健性。同时, 视觉分割效果也体现出所提算法在多尺度血管中的分割优势, 与专家标准图相比, 无论在粗血管还是细小血管上都具有更好的分割效果。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902014_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 基于STARE数据集的测试分割结果。 (a) 原始眼 底图像; (b) 图像分割标准图; (c) 分割结果图" src="Detail/GetImg?filename=images/GXXB201902014_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 基于STARE数据集的测试分割结果。 (a) 原始眼 底图像; (b) 图像分割标准图; (c) 分割结果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902014_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Segmentation test results based on STARE dataset. (a) Original fundus images; (b) segmentation standard images; (c) segmentation results of images</p>

                </div>
                <div class="p1">
                    <p id="119">表1列出了定量评估指标的平均结果, 并与第二个专家分割标准图的结果进行对比。可以看出, DRIVE数据集上所提算法的量化指标均优于专家标准图, 其中敏感性差异最明显。STARE数据集中, 准确率、特异性都高于专家标准图的结果。由于STARE数据集的图像背景干扰更多, 敏感性稍低于手工标注结果。</p>
                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902014_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 局部区域分割结果。 (a) (b) 原始眼底图像; (c) ～ (f) 局部眼底图像; (g) ～ (j) 分割标准图; (k) ～ (n) 分割结果图" src="Detail/GetImg?filename=images/GXXB201902014_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 局部区域分割结果。 (a) (b) 原始眼底图像; (c) ～ (f) 局部眼底图像; (g) ～ (j) 分割标准图; (k) ～ (n) 分割结果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902014_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Segmentation results in local areas. (a) (b) Original fundus images; (c) - (f) local fundus images; (g) - (j) segmentation standard images; (k) - (n) segmentation results of images</p>

                </div>
                <div class="area_img" id="122">
                    <p class="img_tit">表1 DRIVE和STARE数据集的平均性能评估结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Average performance evaluation results based on DRIVE and STARE datasets</p>
                    <p class="img_note"></p>
                    <table id="122" border="1"><tr><td><br />Dataset</td><td>Method</td><td><i>R</i><sub>Se</sub> /%</td><td><i>R</i><sub>Sp</sub> /%</td><td><i>R</i><sub>Acc</sub> /%</td><td><i>R</i><sub>AUC</sub> /%</td></tr><tr><td><br /><br />DRIVE</td><td>2nd human observer</td><td>77.60</td><td>97.24</td><td>94.72</td><td>—</td></tr><tr><td><br /></td><td>Proposed method</td><td>80.53</td><td>97.67</td><td>95.46</td><td>97.71</td></tr><tr><td><br /><br />STARE</td><td>2nd human observer</td><td>89.52</td><td>93.84</td><td>93.49</td><td>—</td></tr><tr><td><br /></td><td>Proposed method</td><td>82.99</td><td>97.94</td><td>96.84</td><td>98.17</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="123">实验量化指标具体统计如下:</p>
                </div>
                <div class="p1">
                    <p id="124">1) DRIVE数据集中, 最好的情况下准确率可达96.22%, 敏感性为92.15%, 特异性为98.64%, AUC为98.91%;平均结果中, 准确率可达95.46%, 敏感性为80.53%, 特异性为97.67%, AUC为97.71%。</p>
                </div>
                <div class="p1">
                    <p id="125">2) STARE数据集中, 对训练模型进行交叉验证。最好的情况中, 准确率为97.86%, 特异性为99.25%, 敏感性为95.48%, AUC为99.51%;平均结果中, 准确率可达96.84%, 敏感性为82.99%, 特异性为96.84%, AUC为98.17%。</p>
                </div>
                <div class="p1">
                    <p id="126">表2和表3对比了DRIVE和STARE数据集上所提方法与其他文献方法的分割性能。可以看出所提方法在准确性、敏感性和AUC上都优于非监督方法, 证明监督学习能有效提高分割效果。所提方法的AUC结果仅在STARE数据集上略低于文献<citation id="154" type="reference">[<a class="sup">14</a>]</citation>的0.62%。在特异性上, 所提方法在DRIVE数据集上分别比文献<citation id="155" type="reference">[<a class="sup">5</a>]</citation>和文献<citation id="156" type="reference">[<a class="sup">14</a>]</citation>低0.73%和0.49%, 在STARE数据集上分别低0.76%和0.5%;而敏感性在DRIVE数据集上比文献<citation id="157" type="reference">[<a class="sup">5</a>]</citation>和文献<citation id="158" type="reference">[<a class="sup">14</a>]</citation>高出17.73%和4.84%, 在STARE数据集上分别高出24.39%和5.73%。所提方法在敏感性上表现优异, 在STARE数据集上结果最优, 在DRIVE数据集上的结果高于大部分方法, 仅略低于文献<citation id="159" type="reference">[<a class="sup">13</a>]</citation>的1.2%。因此, 可以证明所提方法提高了细小血管上的分割效果, 减少血管像素漏判的情况。与其他深度学习的方法相比, 所提方法各项指标均较高。虽然所提方法的部分性能略低于文献<citation id="160" type="reference">[<a class="sup">13</a>]</citation>, 但其构建了一个端到端的网络, 整合了特征提取和像素分类过程, 减少了训练步骤和参数。综合来看, 所提算法的性能更优, 尤其在细小血管上的分割效果更好。</p>
                </div>
                <div class="area_img" id="127">
                    <p class="img_tit">表2 DRIVE数据集上与其他方法的性能对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Performance comparison of the proposed and other methods based on DRIVE dataset</p>
                    <p class="img_note"></p>
                    <table id="127" border="1"><tr><td><br />Type</td><td>Method</td><td>Year</td><td><i>R</i><sub>Se</sub> /%</td><td><i>R</i><sub>Sp</sub> /%</td><td><i>R</i><sub>Acc</sub> /%</td><td><i>R</i><sub>AUC</sub> /%</td></tr><tr><td><br /></td><td>Ref.[3]</td><td>2011</td><td>74.10</td><td>97.51</td><td>94.34</td><td>—</td></tr><tr><td><br />Unsupervised</td><td>Ref. [5]</td><td>2014</td><td>62.80</td><td>98.40</td><td>93.80</td><td>—</td></tr><tr><td><br /> methods</td><td>Ref. [2]</td><td>2015</td><td>76.55</td><td>97.04</td><td>94.42</td><td>96.14</td></tr><tr><td><br /></td><td>Ref. [7]</td><td>2015</td><td>74.20</td><td>98.20</td><td>95.40</td><td>86.20</td></tr><tr><td><br /></td><td>Ref. [9]</td><td>2012</td><td>74.06</td><td>98.07</td><td>94.80</td><td>97.47</td></tr><tr><td><br /></td><td>Ref. [13]</td><td>2015</td><td>81.73</td><td>97.33</td><td>97.67</td><td>94.75</td></tr><tr><td><br />Supervised</td><td>Ref. [12]</td><td>2016</td><td>77.63</td><td>97.68</td><td>94.95</td><td>97.20</td></tr><tr><td><br /> methods</td><td>Ref. [14]</td><td>2016</td><td>75.69</td><td>98.16</td><td>95.27</td><td>97.38</td></tr><tr><td><br /></td><td>Ref. [15]</td><td>2016</td><td>76.03</td><td>—</td><td>95.23</td><td>—</td></tr><tr><td><br /></td><td>Ref. [17]</td><td>2017</td><td>75.01</td><td>97.95</td><td>94.99</td><td>—</td></tr><tr><td><br /></td><td>Proposed method</td><td>2018</td><td>80.53</td><td>97.67</td><td>95.46</td><td>97.71</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="128">
                    <p class="img_tit">表3 STARE数据集上与其他方法的性能对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Performance comparison of the proposed and other methods based on STARE dataset</p>
                    <p class="img_note"></p>
                    <table id="128" border="1"><tr><td><br />Type</td><td>Methods</td><td>Year</td><td><i>R</i><sub>Se</sub> /%</td><td><i>R</i><sub>Sp</sub> /%</td><td><i>R</i><sub>Acc</sub> /%</td><td><i>R</i><sub>AUC</sub> /%</td></tr><tr><td><br /></td><td>Ref. [3]</td><td>2011</td><td>72.60</td><td>97.56</td><td>94.97</td><td>—</td></tr><tr><td><br />Unsupervised</td><td>Ref. [5]</td><td>2014</td><td>58.60</td><td>98.70</td><td>94.48</td><td>—</td></tr><tr><td><br /> methods</td><td>Ref. [2]</td><td>2015</td><td>77.16</td><td>97.01</td><td>95.63</td><td>94.97</td></tr><tr><td><br /></td><td>Ref. [7]</td><td>2015</td><td>78.00</td><td>97.80</td><td>95.60</td><td>87.40</td></tr><tr><td><br /></td><td>Ref. [9]</td><td>2012</td><td>75.48</td><td>97.63</td><td>95.34</td><td>97.68</td></tr><tr><td><br /></td><td>Ref. [13]</td><td>2015</td><td>81.04</td><td>97.91</td><td>98.13</td><td>97.51</td></tr><tr><td><br />Supervised</td><td>Ref. [12]</td><td>2016</td><td>78.67</td><td>97.54</td><td>95.66</td><td>97.85</td></tr><tr><td><br /> methods</td><td>Ref. [14]</td><td>2016</td><td>77.26</td><td>98.44</td><td>96.28</td><td>98.79</td></tr><tr><td><br /></td><td>Ref. [15]</td><td>2016</td><td>74.12</td><td>—</td><td>95.85</td><td>—</td></tr><tr><td><br /></td><td>Proposed method</td><td>2018</td><td>82.99</td><td>97.94</td><td>96.84</td><td>98.17</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="129" name="129" class="anchor-tag">4 结 论</h3>
                <div class="p1">
                    <p id="130">提出了基于全卷积神经网络的多尺度视网膜血管分割方法, 使用跳跃连接的编码器-解码器结构, 将高层语义信息和低层特征进行融合。通过使用残差块结构, 充分学习视网膜血管的边缘和纹理信息。同时, 构建多尺度ASPP模块, 提取多尺度血管特征, 提高对细小血管的分割能力。最后, 使用类别平衡损失函数, 有效解决数据集中正负样本不均衡的问题。实验证明, 所提算法灵敏度高, 多尺度分割能力强, 在视网膜血管分割上优于现有方法。同时, 所提算法利用较少的学习参数, 大大降低模型复杂度。下一步将针对医学数据量少的问题, 通过迁移学习进一步提升网络性能, 提高模型在更多数据集上的泛化能力。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300239023&amp;v=MjkwODd0RmlubFVyekpLRjhXYXhjPU5pZk9mYks3SHRET3JJOUZadWdHREg0Nm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Fraz M M, Barman S A, Remagnino P, <i>et al</i>. An approach to localize the retinal blood vessels using bit planes and centerline detection[J]. Computer Methods and Programs in Biomedicine, 2012, 108 (2) : 600-616.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES0864FBC4F9BDCCF2B04B9059EA7545AE&amp;v=Mjc2ODF0dGd6TG05d2EwPU5pZk9mYk93R05XNjNmeEJFdUo5ZUE5S3VSUmg2anNQUVgvbnBXZEVmcmVRUU12cUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Azzopardi G, Strisciuglio N, Vento M, <i>et al</i>. Trainable COSFIRE filters for vessel delineation with application to retinal images[J]. Medical Image Analysis, 2015, 19 (1) : 46-57.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738123&amp;v=MTk5NjRlcnFRVE1ud1plWnRGaW5sVXJ6SktGOFdheGM9TmlmT2ZiSzdIdEROcVk5RlkrZ0hEWDQ2b0JNVDZUNFBRSC9pclJkRw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> You X G, Peng Q M, Yuan Y, <i>et al</i>. Segmentation of retinal blood vessels using the radial projection and semi-supervised approach[J]. Pattern Recognition, 2011, 44 (10/11) : 2314-2324.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061700188508&amp;v=MDEzNDBaZVp0RmlubFVyekpLRjhXYXhjPU5pZk9mYks4SHRmTnFJOUZaZU1IQ1h3eG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Zhang J, Li H Q, Nie Q, <i>et al</i>. A retinal vessel boundary tracking method based on Bayesian theory and multi-scale line detection[J]. Computerized Medical Imaging and Graphics, 2014, 38 (6) : 517-525.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Retinal Vessel Segmentation based on Possibilistic Fuzzy c-means Clustering Optimised with Cuckoo Search">

                                <b>[5]</b> Emary E, Zawbaa H M, Hassanien A E, <i>et al</i>. Retinal vessel segmentation based on possibilistic fuzzy c-means clustering optimised with cuckoo search[C]. IEEE International Joint Conference on Neural Networks, 2014: 1792-1796.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201801026&amp;v=MDU2ODZxZlp1WnRGaURrVWI3Tkx5clBaTEc0SDluTXJvOUhZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Zhu Z L, Wang J F. Image segmentation based on adaptive fuzzy C-means and post processing correction[J]. Laser &amp; Optoelectronics Progress, 2018, 55 (1) : 011004. 朱占龙, 王军芬. 基于自适应模糊C均值与后处理的图像分割算法[J]. 激光与光电子学进展, 2018, 55 (1) : 011004.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automated vessel segmentation using infinite perimeter active contour model with hybrid region information with application to retinal images">

                                <b>[7]</b> Zhao Y T, Rada L, Chen K, <i>et al</i>. Automated vessel segmentation using infinite perimeter active contour model with hybrid region information with application to retinal images[J]. IEEE Transactions on Medical Imaging, 2015, 34 (9) : 1797-1807.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Comparative study of retinal vessel segmentation methods on a new publicly available database">

                                <b>[8]</b> Niemeijer M, van Ginneken B, Loog M, <i>et al</i>. Comparative study of retinal vessel segmentation methods on a new publicly available database[J]. Proceedings of SPIE, 2004, 5370: 648-656.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An ensemble classification-based approach applied to retinal blood vessel segmentation">

                                <b>[9]</b> Fraz M M, Remagnino P, Hoppe A, <i>et al</i>. An ensemble classification-based approach applied to retinal blood vessel segmentation[J]. IEEE Transactions on Biomedical Engineering, 2012, 59 (9) : 2538-2548.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201807020&amp;v=MDYwMjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEa1ViN05JalhUYkxHNEg5bk1xSTlIWklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Wang W X, Fu Y T, Dong F, <i>et al</i>. Infrared ship target detection method based on deep convolution neural network[J]. Acta Optica Sinica, 2018, 38 (7) : 0712006. 王文秀, 傅雨田, 董峰, 等. 基于深度卷积神经网络的红外船只目标检测方法[J]. 光学学报, 2018, 38 (7) : 0712006.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201710031&amp;v=MTE2NjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVWI3TklqWFRiTEc0SDliTnI0OUdaWVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Liu F, Shen T S, Ma X X. Convolutional neural network based multi-band ship target recognition with feature fusion[J]. Acta Optica Sinica, 2017, 37 (10) : 1015002. 刘峰, 沈同圣, 马新星. 特征融合的卷积神经网络多波段舰船目标识别[J]. 光学学报, 2017, 37 (10) : 1015002.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Segmenting Retinal Blood Vessels With&amp;lt;?Pub_newline?&amp;gt;Deep Neural Networks">

                                <b>[12]</b> Liskowski P, Krawiec K. Segmenting retinal blood vessels with deep neural networks[J]. IEEE Transactions on Medical Imaging, 2016, 35 (11) : 2369-2380.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700312646&amp;v=MTQ3MjFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyekpLRjhXYXhjPU5pZk9mYks4SDlETXFJOUZaK29OQ25nL29CTVQ2VDRQUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Wang S, Yin Y, Cao G, <i>et al</i>. Hierarchical retinal blood vessel segmentation based on feature and ensemble learning[J]. Neurocomputing, 2015, 149 (B) : 708-717.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;A Cross-Modality Learning Approach for Vessel Segmentation in Ret inal Images,&amp;quot;">

                                <b>[14]</b> Li Q L, Feng B W, Xie L P, <i>et al</i>. A cross-modality learning approach for vessel segmentation in retinal images[J]. IEEE Transactions on Medical Imaging, 2016, 35 (1) : 109-118.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepVessel:Retinal vessel segmentation via deep learning and conditional random Field">

                                <b>[15]</b> Fu H, Xu Y, Lin S, <i>et al</i>. DeepVessel: retinal vessel segmentation via deep learning and conditional random field[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention, 2016: 132-139.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">

                                <b>[16]</b> Ronneberger O, Fischer P, Brox T. U-Net: convolutional networks for biomedical image segmentation[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention, 2015: 234-241.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Development of automatic retinal vessel segmentation method in fundus images via convolutional neural networks">

                                <b>[17]</b> Song J, Lee B. Development of automatic retinal vessel segmentation method in fundus images via convolutional neural networks[C]. 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, 2017: 681-684.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identity Mappings in Deep Residual Networks">

                                <b>[18]</b> He K, Zhang X, Ren S, <i>et al</i>. Identity mappings in deep residual networks[C]. European Conference on Computer Vision, 2016: 630-645.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[19]</b> Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift[C]. International Conference on Machine Learning, 2015: 448-456.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_20" >
                                    <b>[20]</b>
                                 He K M, Zhang X Y, Ren S Q, <i>et al</i>. Deep residual learning for image recognition[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 770-778.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_21" >
                                    <b>[21]</b>
                                 Srivastava N, Hinton G, Krizhevsky A, <i>et al</i>. Dropout: a simple way to prevent neural networks from overfitting[J]. Journal of Machine Learning Research, 2014, 15 (1) : 1929-1958.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_22" >
                                    <b>[22]</b>
                                 Chen L C, Papandreou G, Kokkinos I, <i>et al</i>. DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (4) : 834-848.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Holistically-Nested Edge Detection">

                                <b>[23]</b> Xie S N, Tu Z W. Holistically-nested edge detection[J]. International Journal of Computer Vision, 2017, 125 (1/2/3) : 3-18.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ridge-based vessel segmentation in color images of the retina">

                                <b>[24]</b> Staal J, Abràmoff M D, Niemeijer M, <i>et al</i>. Ridge-based vessel segmentation in color images of the retina[J]. IEEE Transactions on Medical Imaging, 2004, 23 (4) : 501-509.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response">

                                <b>[25]</b> Hoover A D, Kouznetsova V, Goldbaum M. Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response[J]. IEEE Transactions on Medical Imaging, 2000, 19 (3) : 203-210.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002832026&amp;v=MDYwNjRaK1p1Rml2a1c3N0tJVnM9Tmo3QmFyTzRIdEhPcDR4SFpPa0pZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3Fk&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> Reza A M. Realization of the contrast limited adaptive histogram equalization (CLAHE) for real-time image enhancement[J]. Journal of VLSI Signal Processing: Systems for Signal, Image, and Video Technology, 2004, 38 (1) : 35-44.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201902014" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902014&amp;v=MDE1Mjg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVWI3S0lqWFRiTEc0SDlqTXJZOUVZSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="6" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

