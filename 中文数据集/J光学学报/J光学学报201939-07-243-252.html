

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133863738252500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201907027%26RESULT%3d1%26SIGN%3d7m6COJPcmaiiCjfpoNwz3Hz7pLY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201907027&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201907027&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201907027&amp;v=MDk3MzVIWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnluaFVMdkxJalhUYkxHNEg5ak1xSTk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#62" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#69" data-title="2 相关工作 ">2 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="&lt;b&gt;2.1 SRCNN&lt;/b&gt;"><b>2.1 SRCNN</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;2.2 VDSR&lt;/b&gt;"><b>2.2 VDSR</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#78" data-title="3 本文算法 ">3 本文算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#81" data-title="&lt;b&gt;3.1 低层特征提取网络&lt;/b&gt;"><b>3.1 低层特征提取网络</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;3.2 跳跃级联块网络&lt;/b&gt;"><b>3.2 跳跃级联块网络</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;3.3 重建网络&lt;/b&gt;"><b>3.3 重建网络</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;3.4 损失函数和评价标准&lt;/b&gt;"><b>3.4 损失函数和评价标准</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#104" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#105" data-title="&lt;b&gt;4.1 实验环境&lt;/b&gt;"><b>4.1 实验环境</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;4.2 数据集及优化方法&lt;/b&gt;"><b>4.2 数据集及优化方法</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;4.3 网络参数的设置选择&lt;/b&gt;"><b>4.3 网络参数的设置选择</b></a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;4.4 实验结果分析&lt;/b&gt;"><b>4.4 实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#140" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="图1 SRCNN网络结构图">图1 SRCNN网络结构图</a></li>
                                                <li><a href="#76" data-title="图2 VDSR网络结构图">图2 VDSR网络结构图</a></li>
                                                <li><a href="#80" data-title="图3 本文算法网络结构图">图3 本文算法网络结构图</a></li>
                                                <li><a href="#93" data-title="图4 跳跃级联块">图4 跳跃级联块</a></li>
                                                <li><a href="#107" data-title="图5 图像butterfly放大4倍后的可视化结果。">图5 图像butterfly放大4倍后的可视化结果。</a></li>
                                                <li><a href="#118" data-title="图6 Set5测试集下不同层数PSNR均值随迭代次数增长的曲线">图6 Set5测试集下不同层数PSNR均值随迭代次数增长的曲线</a></li>
                                                <li><a href="#121" data-title="图7 Set5测试集下不同网络结构的迭代次数与PSNR均值的关系曲线">图7 Set5测试集下不同网络结构的迭代次数与PSNR均值的关系曲线</a></li>
                                                <li><a href="#124" data-title="图8 Set5测试集下不同激活函数PSNR均值随迭代次数的变化曲线">图8 Set5测试集下不同激活函数PSNR均值随迭代次数的变化曲线</a></li>
                                                <li><a href="#127" data-title="图9 Set5测试集下不同滤波器数目条件下的PSNR均值随迭代次数的变化曲线">图9 Set5测试集下不同滤波器数目条件下的PSNR均值随迭代次数的变化曲线</a></li>
                                                <li><a href="#132" data-title="图10 Set5测试集下不同方法的运行时间与PSNR均值的关系曲线">图10 Set5测试集下不同方法的运行时间与PSNR均值的关系曲线</a></li>
                                                <li><a href="#135" data-title="图11 Butterfly在不同算法下的效果对比图">图11 Butterfly在不同算法下的效果对比图</a></li>
                                                <li><a href="#136" data-title="图12 PPT在不同算法下的效果对比图">图12 PPT在不同算法下的效果对比图</a></li>
                                                <li><a href="#137" data-title="图13 Man在不同算法的对比图">图13 Man在不同算法的对比图</a></li>
                                                <li><a href="#138" data-title="表1 在测试集Set5, Set14, BSD100下比较不同方法的PSNR均值">表1 在测试集Set5, Set14, BSD100下比较不同方法的PSNR均值</a></li>
                                                <li><a href="#139" data-title="表2 在测试集Set5, Set14, BSD100下比较不同方法的SSIM均值">表2 在测试集Set5, Set14, BSD100下比较不同方法的SSIM均值</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="14">


                                    <a id="bibliography_1" title=" Zhang H, Zhang L, Shen H.A blind super-resolution reconstruction method considering image registration errors[J].International Journal of Fuzzy Systems, 2015, 17 (2) :353-364." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A blind super-resolution reconstruction method considering image registration errors">
                                        <b>[1]</b>
                                         Zhang H, Zhang L, Shen H.A blind super-resolution reconstruction method considering image registration errors[J].International Journal of Fuzzy Systems, 2015, 17 (2) :353-364.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_2" title=" Chen H G, He X H, Teng Q Z, &lt;i&gt;et al&lt;/i&gt;.Single image super resolution using local smoothness and nonlocal self-similarity priors[J].Signal Processing:Image Communication, 2016, 43:68-81." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES56BE354FB2B92978E52A6C8F7643372A&amp;v=MTMyNzY3ejBNVGd6cTJoVXpmYkdYUXJqdUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRoaHhieTh4S3M9TmlmT2ZiYStiS1RQcW9zekZ1bDlCWDR3eUI1bQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Chen H G, He X H, Teng Q Z, &lt;i&gt;et al&lt;/i&gt;.Single image super resolution using local smoothness and nonlocal self-similarity priors[J].Signal Processing:Image Communication, 2016, 43:68-81.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_3" title=" Li S M, Lei G Q, Fan R.Depth map super-resolution reconstruction based on convolutional neural networks[J].Acta Optica Sinica, 2017, 37 (12) :1210002.李素梅, 雷国庆, 范如.基于卷积神经网络的深度图超分辨率重建[J].光学学报, 2017, 37 (12) :1210002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201712015&amp;v=MTM0NzA1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnluaFVMdkxJalhUYkxHNEg5Yk5yWTlFWVlRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Li S M, Lei G Q, Fan R.Depth map super-resolution reconstruction based on convolutional neural networks[J].Acta Optica Sinica, 2017, 37 (12) :1210002.李素梅, 雷国庆, 范如.基于卷积神经网络的深度图超分辨率重建[J].光学学报, 2017, 37 (12) :1210002.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_4" title=" B&#228;tz M, Eichenseer A, Seiler J, &lt;i&gt;et al&lt;/i&gt;.Hybrid super-resolution combining example-based single-image and interpolation-based multi-image reconstruction approaches[C]//2015 IEEE International Conference on Image Processing (ICIP) , September 27-30, 2015, Quebec City, QC, Canada.New York:IEEE, 2015:58-62." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hybrid super-resolution combining example-based single-image and interpolationbased multi-image reconstruction approaches">
                                        <b>[4]</b>
                                         B&#228;tz M, Eichenseer A, Seiler J, &lt;i&gt;et al&lt;/i&gt;.Hybrid super-resolution combining example-based single-image and interpolation-based multi-image reconstruction approaches[C]//2015 IEEE International Conference on Image Processing (ICIP) , September 27-30, 2015, Quebec City, QC, Canada.New York:IEEE, 2015:58-62.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_5" title=" Kim K I, Kwon Y.Single-image super-resolution using sparse regression and natural image prior[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (6) :1127-1133." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-image super-resolution using sparse regression and natural image prior">
                                        <b>[5]</b>
                                         Kim K I, Kwon Y.Single-image super-resolution using sparse regression and natural image prior[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (6) :1127-1133.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_6" title=" Yang J, Wang Z, Lin Z, &lt;i&gt;et al&lt;/i&gt;.Couple dictionary training for image super-resolution[J].IEEE Transactions on Image Processing, 2012, 21 (8) :3467-3478." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coupled Dictionary Training for Image Super-Resolution">
                                        <b>[6]</b>
                                         Yang J, Wang Z, Lin Z, &lt;i&gt;et al&lt;/i&gt;.Couple dictionary training for image super-resolution[J].IEEE Transactions on Image Processing, 2012, 21 (8) :3467-3478.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_7" title=" Xiao J S, Liu E Y, Zhu L, &lt;i&gt;et al&lt;/i&gt;.Improved image super-resolution algorithm based on convolutional neural network[J].Acta Optica Sinica, 2017, 37 (3) :0318011.肖进胜, 刘恩雨, 朱力, 等.改进的基于卷积神经网络的图像超分辨率算法[J].光学学报, 2017, 37 (3) :0318011." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201703012&amp;v=MDQ1NDZPZVplVnVGeW5oVUx2TElqWFRiTEc0SDliTXJJOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Xiao J S, Liu E Y, Zhu L, &lt;i&gt;et al&lt;/i&gt;.Improved image super-resolution algorithm based on convolutional neural network[J].Acta Optica Sinica, 2017, 37 (3) :0318011.肖进胜, 刘恩雨, 朱力, 等.改进的基于卷积神经网络的图像超分辨率算法[J].光学学报, 2017, 37 (3) :0318011.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_8" title=" Irani M, Peleg S.Improving resolution by image registration[J].CVGIP:Graphical Models and Image Processing, 1991, 53 (3) :231-239." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving resolution by image registration">
                                        <b>[8]</b>
                                         Irani M, Peleg S.Improving resolution by image registration[J].CVGIP:Graphical Models and Image Processing, 1991, 53 (3) :231-239.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_9" title=" Stark H, Oskoui P.High-resolution image recovery from image-plane arrays, using convex projections[J].Journal of the Optical Society of America A, 1989, 6 (11) :1715-1726." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-resolution image recovery from image-plane arrays, using convex projections">
                                        <b>[9]</b>
                                         Stark H, Oskoui P.High-resolution image recovery from image-plane arrays, using convex projections[J].Journal of the Optical Society of America A, 1989, 6 (11) :1715-1726.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_10" title=" Chang H, Yeung D Y, Xiong Y M.Super-resolution through neighbor embedding[C]//Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 27-July 2, 2004, Washington D C, USA.New York:IEEE, 2004:1315043." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Super-resolution through neighbor embedding">
                                        <b>[10]</b>
                                         Chang H, Yeung D Y, Xiong Y M.Super-resolution through neighbor embedding[C]//Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 27-July 2, 2004, Washington D C, USA.New York:IEEE, 2004:1315043.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_11" title=" Yang J C, Wright J, Huang T S, &lt;i&gt;et al&lt;/i&gt;.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing, 2010, 19 (11) :2861-2873." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">
                                        <b>[11]</b>
                                         Yang J C, Wright J, Huang T S, &lt;i&gt;et al&lt;/i&gt;.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing, 2010, 19 (11) :2861-2873.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_12" title=" Dong C, Loy C C, He K M, &lt;i&gt;et al&lt;/i&gt;.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (2) :295-307." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using deep convolutional networks">
                                        <b>[12]</b>
                                         Dong C, Loy C C, He K M, &lt;i&gt;et al&lt;/i&gt;.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (2) :295-307.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_13" title=" Dong C, Chen C L, Tang X.Accelerating the super-resolution convolutional neural network[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Cham:Springer, 2016, 9906:391-407." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accelerating the super-resolution convolutional neural network">
                                        <b>[13]</b>
                                         Dong C, Chen C L, Tang X.Accelerating the super-resolution convolutional neural network[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Cham:Springer, 2016, 9906:391-407.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_14" title=" Shi W Z, Caballero J, Husz&#225;r F, &lt;i&gt;et al&lt;/i&gt;.Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1874-1883." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network">
                                        <b>[14]</b>
                                         Shi W Z, Caballero J, Husz&#225;r F, &lt;i&gt;et al&lt;/i&gt;.Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1874-1883.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_15" title=" Kim J, Lee J K, Lee K M.Accurate image super-resolution using very deep convolutional networks[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1646-1654." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate image super-resolution using very deep convolutional networks">
                                        <b>[15]</b>
                                         Kim J, Lee J K, Lee K M.Accurate image super-resolution using very deep convolutional networks[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1646-1654.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_16" title=" He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.Deep residual learning for image recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[16]</b>
                                         He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.Deep residual learning for image recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_17" title=" Huang G, Sun Y, Liu Z, &lt;i&gt;et al&lt;/i&gt;.Deep networks with stochastic depth[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Cham:Springer, 2016, 9908:646-661." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep networks with stochastic depth">
                                        <b>[17]</b>
                                         Huang G, Sun Y, Liu Z, &lt;i&gt;et al&lt;/i&gt;.Deep networks with stochastic depth[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science.Cham:Springer, 2016, 9908:646-661.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_18" title=" Huang G, Liu Z, Maaten L V D, &lt;i&gt;et al&lt;/i&gt;.Densely connected convolutional networks[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:2261-2269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Densely connected convolutional networks">
                                        <b>[18]</b>
                                         Huang G, Liu Z, Maaten L V D, &lt;i&gt;et al&lt;/i&gt;.Densely connected convolutional networks[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:2261-2269.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_19" title=" Qu Y Y, Lin L, Shen F M, &lt;i&gt;et al&lt;/i&gt;.Joint hierarchical category structure learning and large-scale image classification[J].IEEE Transactions on Image Processing, 2017, 26 (9) :4331-4346." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint hierarchical category structure learning and large-scale image classification">
                                        <b>[19]</b>
                                         Qu Y Y, Lin L, Shen F M, &lt;i&gt;et al&lt;/i&gt;.Joint hierarchical category structure learning and large-scale image classification[J].IEEE Transactions on Image Processing, 2017, 26 (9) :4331-4346.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_20" title=" Ye Y X, Shan J, Bruzzone L, &lt;i&gt;et al&lt;/i&gt;.Robust registration of multimodal remote sensing images based on structural similarity[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (5) :2941-2958." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust Registration of Multimodal Remote Sensing Images Based on Structural Similarity">
                                        <b>[20]</b>
                                         Ye Y X, Shan J, Bruzzone L, &lt;i&gt;et al&lt;/i&gt;.Robust registration of multimodal remote sensing images based on structural similarity[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (5) :2941-2958.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_21" title=" Jia Y, Shelhamer E, Donahue J, &lt;i&gt;et al&lt;/i&gt;.Caffe:convolutional architecture for fast feature embedding [EB/OL]. (2014-06-20) [2019-03-09].https://arxiv.org/abs/1408.5093." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffe:convolutional architecture for fast feature embedding">
                                        <b>[21]</b>
                                         Jia Y, Shelhamer E, Donahue J, &lt;i&gt;et al&lt;/i&gt;.Caffe:convolutional architecture for fast feature embedding [EB/OL]. (2014-06-20) [2019-03-09].https://arxiv.org/abs/1408.5093.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_22" title=" Timofte R, de Smet V, van Gool L.A+:adjusted anchored neighborhood regression for fast super-resolution[M]//Cremers D, Reid I, Saito H, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science, Cham:Springer, 2014, 9006:111-126." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A+:Adjusted anchored neighborhood regression for fast superresolution">
                                        <b>[22]</b>
                                         Timofte R, de Smet V, van Gool L.A+:adjusted anchored neighborhood regression for fast super-resolution[M]//Cremers D, Reid I, Saito H, &lt;i&gt;et al&lt;/i&gt;.Lecture notes in computer science, Cham:Springer, 2014, 9006:111-126.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_23" title=" Martin D, Fowlkes C, Tal D, &lt;i&gt;et al&lt;/i&gt;.A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics[C]//Proceedings Eighth IEEE International Conference on Computer Vision, July 7-14, 2001, Vancouver, BC, Canada.New York:IEEE, 2001:416-423." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics">
                                        <b>[23]</b>
                                         Martin D, Fowlkes C, Tal D, &lt;i&gt;et al&lt;/i&gt;.A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics[C]//Proceedings Eighth IEEE International Conference on Computer Vision, July 7-14, 2001, Vancouver, BC, Canada.New York:IEEE, 2001:416-423.
                                    </a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_24" title=" Du B, Wang Z M, Zhang L F, &lt;i&gt;et al&lt;/i&gt;.Robust and discriminative labeling for multi-label active learning based on maximum correntropy criterion[J].IEEE Transactions on Image Processing, 2017, 26 (4) :1694-1707." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust and Discriminative Labeling for Multi-Label Active Learning Based on Maximum Correntropy Criterion">
                                        <b>[24]</b>
                                         Du B, Wang Z M, Zhang L F, &lt;i&gt;et al&lt;/i&gt;.Robust and discriminative labeling for multi-label active learning based on maximum correntropy criterion[J].IEEE Transactions on Image Processing, 2017, 26 (4) :1694-1707.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-02 14:28</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(07),243-252 DOI:10.3788/AOS201939.0715003            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度跳跃级联的图像超分辨率重建</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A2%81%E6%98%86%E9%B9%8F&amp;code=40593719&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">袁昆鹏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B8%AD%E5%BF%97%E7%BA%A2&amp;code=05981488&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">席志红</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%93%88%E5%B0%94%E6%BB%A8%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E4%B8%8E%E9%80%9A%E4%BF%A1%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0119964&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">哈尔滨工程大学信息与通信工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对模型VDSR (very deep super resolution) 收敛速度慢, 训练前需要对原始图像进行预处理, 以及网络中存在的冗余性等问题, 提出了一种基于深度跳跃级联的单幅图像超分辨率重建 (DCSR) 算法。DCSR算法省去了图像预处理, 直接在低分辨率图像上提取浅层特征, 并使用亚像素卷积对图像进行放大;通过使用跳跃级联块可以充分利用每个卷积层提取到图像特征, 实现特征重用, 减少网络的冗余性。网络的跳跃级联块可以直接从输出到每一层建立短连接, 加快网络的收敛速度, 缓解梯度消失问题。实验结果表明, 在几种公开数据集上, 所提算法的峰值信噪比、结构相似度值均高于现有的几种算法, 充分证明了所提算法的出色性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">超分辨率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B7%B3%E8%B7%83%E7%BA%A7%E8%81%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">跳跃级联;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">梯度消失;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E5%A4%8D%E7%94%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征复用;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%9A%E5%83%8F%E7%B4%A0%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">亚像素卷积;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%86%97%E4%BD%99%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冗余性;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *袁昆鹏, E-mail:xizhihong@hrbeu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-08</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (60875025);</span>
                    </p>
            </div>
                    <h1><b>Image Super Resolution Based on Depth Jumping Cascade</b></h1>
                    <h2>
                    <span>Yuan Kunpeng</span>
                    <span>Xi Zhihong</span>
            </h2>
                    <h2>
                    <span>College of Information and Communication Engineering, Harbin Engineering University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The very deep super resolution model has disadvantages: the convergence speed is low, the original image must be preprocessed before training, and the network redundancy must be reduced. This study proposes a single-image super resolution reconstruction method based on depth jumping cascade (DCSR) . First, DCSR eliminates pre-processing, extracts the shallow features directly on the low-resolution image, and finally uses sub-pixel convolution to magnify the image. Second, each convolutional layer is fully utilized to extract the image features using the jump cascading block, thereby realizing feature reuse and network redundancy reduction. The jump cascading block of the network establishes a short connection directly from the output to each layer, speeding up the network convergence speed and alleviating the gradient disappearance problem. The experimental results show that on several public datasets, the peak-signal-to-noise ratio and the structural similarity of the algorithm are higher than those of existing algorithms, which fully demonstrates an excellent algorithm performance.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=super%20resolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">super resolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=jumping%20cascade&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">jumping cascade;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=gradient%20disappear&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">gradient disappear;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20reuse&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature reuse;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sub-pixel%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sub-pixel convolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=redundancy&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">redundancy;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-08</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="62" name="62" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="63">图像的超分辨率重建技术就是应用软件通过一幅或多幅低分辨率 (LR) 图像重建出一幅高分辨率 (HR) 图像<citation id="152" type="reference"><link href="14" rel="bibliography" /><link href="16" rel="bibliography" /><link href="18" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>, 本文主要研究单幅图像超分辨率重建 (SISR) 。图像的超分辨率重建技术已经广泛应用在医学影像、军事侦查和视频监控等领域。目前, 超分辨率重建技术可分为三类, 分别是基于插值<citation id="142" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>的方法、基于重建<citation id="143" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的方法和基于学习<citation id="153" type="reference"><link href="24" rel="bibliography" /><link href="26" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>的方法。基于插值的方法有双线性插值、双立方插值和最近邻插值等, 这类方法虽然速度快, 但是重建出的图像边缘会出现锯齿, 导致图像不清晰。基于重建的方法有迭代反向投影法<citation id="144" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和凸集投影法等<citation id="145" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 这类方法随着放大倍数的增加, 不能很好地重建一些高频细节。基于学习的方法主要有邻域嵌入法<citation id="146" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、稀疏表示法<citation id="147" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和卷积神经网络 (CNN) 等, 这类方法可构建低分辨率图像与高分辨率图像的映射关系, 从而实现图像的重建。近几年, 随着人工智能的迅速发展, 基于CNN的图像超分辨率重建技术以其速度快、端对端学习效果好的特点, 正在逐步成为主流方法。最早将CNN用于图像超分辨率重建的是超分辨率卷积神经网络 (SRCNN) <citation id="148" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 该网络模型很简单, 仅使用三层卷积来模拟非线性映射, 从而实现图像的重建。Dong等<citation id="154" type="reference"><link href="36" rel="bibliography" /><link href="38" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>对SRCNN进行了改进并提出了快速超分辨率卷积神经网络 (FSRCNN) , FSRCNN使用更小的卷积核和更多的卷积层, 并通过反卷积层实现图像的放大。由于FSRCNN不需要将原始图像经过双三次差值放大到目标尺寸大小, 且使用了更小的卷积核, 所以在训练速度上有较大的提升。Shi等<citation id="149" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了亚像素卷积神经网络 (ESPCN) , ESPCN同样不需要将原始图像经过双三次差值来进行图像放大, 直接在低分辨率图像上提取特征, 使用了tanh激活函数代替了ReLU, 最后通过亚像素卷积层将像素进行重新排列以实现图像的重建。之后, Kim等<citation id="150" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出了VDSR (very deep super resolution) 模型, 加深了网络深度, 使用残差网络结构 (ResNet<citation id="151" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>) , 加速了收敛过程, 同时避免了网络加深而引起的梯度消失问题, 提升了图像重建的质量。</p>
                </div>
                <div class="p1">
                    <p id="64">上述几种方法仍存在一些缺点, 对于SRCNN<citation id="155" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 其收敛速度慢, 卷积层数少, 无法充分获取更多的图像重建信息;对于VDSR<citation id="156" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 虽然其网络层数加深到20层, 但需要对低分辨率图像进行双三次差值, 因此增加了计算量, 且插值会导致一些图像的细节信息丢失。文献<citation id="157" type="reference">[<a class="sup">17</a>]</citation>表明, 在ResNet<citation id="158" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>网络结构中, 训练时随机丢弃一些卷积层并不影响ResNet<citation id="159" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>的测试性能, 说明神经网络中每一个卷积层学习到的图像特征很少, 导致网络中有大量的冗余信息。为了解决网络的冗余, Huang等<citation id="160" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出了DenseNet, DenseNet打破了通过加深网络层数和加宽网络结构来提高性能的惯性思维, 从特征图角度出发, 正常<i>N</i>层网络只有<i>N</i>层连接, 而DenseNet的每一层与之前所有层都相连, 所以<i>N</i>层网络有<i>N</i> (<i>N</i>+1) /2层连接, 这种网络的优点是可以缓解梯度消失的问题, 加强特征的传播, 有效地减少网络冗余。基于这一结论可知, 先前的这几种方法都忽视了充分利用各卷积层提取到的图像特征, 采用递进的层级网络, 每一个卷积层只能获取到紧邻的上一个卷积层输出的图像特征, 仅采用来自LR空间的最后一个卷积层的特征映射进行图像的放大。VDSR<citation id="161" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>采用一个残差网络, 使用这种跳跃的短连接使得重建的卷积层不仅可以获得紧邻卷积层输出的图像特征, 还可以获得插值后低分辨率的图像特征。本文利用跳跃级联块 (JCD) , 后面卷积层充分利用了前面卷积层的学习特征, 实现了特征重用, 减少了网络冗余, 从而更好地重建图像。</p>
                </div>
                <div class="p1">
                    <p id="65">本文提出了基于深度跳跃级联的图像超分辨率重建方法 (DCSR) , 该算法的优点包括:</p>
                </div>
                <div class="p1">
                    <p id="66">1) 不需要对低分辨率图像进行双三次插值放大, 在重建网络中使用高效亚像素卷积进行图像的放大;</p>
                </div>
                <div class="p1">
                    <p id="67">2) 使用跳跃级联块, 实现特征重用, 减少网络冗余, 加速参数更新的速度, 提高网络的收敛速度;</p>
                </div>
                <div class="p1">
                    <p id="68">3) 通过建立的短连接, 缓解了由网络层数过深引起的梯度消失问题。</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag">2 相关工作</h3>
                <h4 class="anchor-tag" id="70" name="70"><b>2.1 SRCNN</b></h4>
                <div class="p1">
                    <p id="71">SRCNN<citation id="162" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>算法需要使用双三次插值将低分辨率图像放大到所需的图像大小, 然后通过特征提取、非线性映射和重建, 最终输出高分辨率图像, 其网络结构如图1所示。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907027_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 SRCNN网络结构图" src="Detail/GetImg?filename=images/GXXB201907027_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 SRCNN网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907027_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Diagram of SRCNN network structure</p>

                </div>
                <div class="p1">
                    <p id="73">在输入到网络之前, 需要利用双三次差值将低分辨率图像放大到目标图像尺寸, 首先经过第一层卷积 (9×9) 对图像块进行特征提取, 然后经过第二层卷积 (1×1) 将低分辨率的特征映射为高分辨率的特征, 再经过最后一层卷积 (5×5) 对得到的高分辨率特征进行最终的重建。</p>
                </div>
                <h4 class="anchor-tag" id="74" name="74"><b>2.2 VDSR</b></h4>
                <div class="p1">
                    <p id="75">VDSR<citation id="163" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>借鉴了ResNet<citation id="164" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>的思想, 使用残差学习, 防止了梯度消失, 并加快了网络的收敛速度, 其网络结构如图2所示。</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907027_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 VDSR网络结构图" src="Detail/GetImg?filename=images/GXXB201907027_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 VDSR网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907027_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Diagram of VDSR network structure</p>

                </div>
                <div class="p1">
                    <p id="77">网络层数加深到20层后, 增大了感受野, 由SRCNN<citation id="165" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>的13×13变为41×41, 每层卷积使用了更小的卷积核3×3, 为了充分利用图像边缘信息, 在每次卷积前进行边界补零, 使图像的大小保持不变。</p>
                </div>
                <h3 id="78" name="78" class="anchor-tag">3 本文算法</h3>
                <div class="p1">
                    <p id="79">本文算法的网络结构如图3所示, 分为低层特征提取网路 (LFENet) 、跳跃级联块网络 (DCNet) 和重建网络 (ReconNet) 三个部分。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907027_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 本文算法网络结构图" src="Detail/GetImg?filename=images/GXXB201907027_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 本文算法网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907027_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Network structural diagram of proposed algorithm</p>

                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>3.1 低层特征提取网络</b></h4>
                <div class="p1">
                    <p id="82">首先借鉴牛津大学VGG小组提出的基于卷积神经网络的视觉识别算法中所采用的VGG-16<citation id="166" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>模型, 使用3×3卷积核进行低层特征提取。使用线性整流函数PReLU作为激活函数。为了充分利用图像的边缘信息, 在每次卷积前进行补零操作, 卷积层可表示为</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Φ</mi><mo stretchy="false"> (</mo><mi>W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>×</mo><mi>Ι</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>R</mtext></mrow></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">式中:<i>Y</i>为低层特征提取网络的输出;<i>I</i><sub>LR</sub>为输入的低分辨率图像;<i>W</i><sub>1</sub>表示一组滤波器, 其大小可表示为<i>n</i><sub>1</sub>×<i>k</i><sub>1</sub>×<i>k</i><sub>1</sub>×<i>c</i><sub>1</sub>, 其中<i>n</i><sub>1</sub>代表输出滤波器的数量, 这里取32, <i>k</i><sub>1</sub>代表卷积核大小, 这里<i>k</i><sub>1</sub>取3, <i>c</i><sub>1</sub>代表图像的通道数, 图像正常有三个通道 (<i>Y</i>, <i>C</i><sub>b</sub>, <i>C</i><sub>r</sub>) , 由于人眼对图像的亮度信息更敏感, 所以只需要对<i>Y</i>通道进行处理, <i>c</i><sub>1</sub>取值为1, 步长为1, 边界零填充也为1;<i>b</i><sub>1</sub>为偏移项;<i>Φ</i> (<i>X</i>) 表示激活函数, 这里采用带有参数的PReLu作为激活函数, <i>Φ</i> (<i>X</i>) 可表示为</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Φ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mi>max</mi><mo stretchy="false"> (</mo><mn>0</mn><mo>, </mo><mi>X</mi><mo stretchy="false">) </mo><mo>+</mo><mi>α</mi><msub><mrow></mrow><mi>i</mi></msub><mi>min</mi><mo stretchy="false"> (</mo><mn>0</mn><mo>, </mo><mi>X</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">其中</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Δ</mtext><mi>α</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>μ</mi><mtext>Δ</mtext><mi>α</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>η</mi><mfrac><mrow><mo>∂</mo><mi>ε</mi></mrow><mrow><mo>∂</mo><mi>α</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">式中:<i>X</i>为自变量;<i>α</i><sub><i>i</i></sub>为可学习的参数, 初始化为0.25;Δ<i>α</i><sub><i>i</i></sub>为上一次更新的参数值, 网络反向传播时, 采用带动量的更新方式更新参数<i>α</i><sub><i>i</i></sub>;<i>μ</i>和<i>η</i>分别为动量大小和学习率;<i>ε</i>为损失函数。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89"><b>3.2 跳跃级联块网络</b></h4>
                <div class="p1">
                    <p id="90">在现有的CNN中, 随着网络层数的加深, 数据在传播过程中会出现梯度消失的问题, 导致网络不能继续训练。对于DenseNet<citation id="167" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>, 靠近输入的层与靠近输出的层之间的连接越短, 卷积神经网络可以设计得越深, 精度更高, 则可以更加有效地进行训练, 这种短连接可以减轻梯度消失的问题。依照该结论, 本文构造了跳跃级联块, 由图3可以看出, 在反向传播时, 每一层的梯度都来自于后面的所有层, 因此靠近输入层的梯度不会随着网络深度的增加而越来越小, 从而使梯度更好地在网络中传输, 降低了梯度消失的风险。虽然VDSR<citation id="168" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>通过建立一个跳跃的短连接结构在一定程度上促进了数据在卷积层间的流通, 但还是未充分利用各层之间的图像特征。每一个跳跃级联块的连接方式如图4所示, 每一层的输入都来自所有前面层的输出, 从而实现特征的重复利用, 减少网络冗余。与VDSR<citation id="169" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>不同的是, 数据聚合方式不是对特征映射直接求和, 而是采用连接的方式, 将每个跳跃级联块采用相同的结构进行连接, 进一步加强网络中信息的流通。这样能保证层与层之间信息最大程度的利用, 加深了网络深度。为了防止过拟合, 引入了dropout, 在训练的时候, 按照一定的概率对每个神经元进行随机采样, 相当于给每个神经元加上了一个开关, 因此只有一部分神经元在工作, 减少了神经元之间的联系, 并形成一个子网络, 这样训练时参数就会大大减少, 一个复杂的网络就变得简单。具体可表示为</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>w</mi><msub><mrow></mrow><mrow><mi>d</mi><mo>, </mo><mi>c</mi></mrow></msub><mo stretchy="false">|</mo><msub><mrow></mrow><mi>p</mi></msub><mo>=</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>d</mi><mo>, </mo><mi>c</mi></mrow></msub><mo>×</mo><mi>ρ</mi><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>F</mi><msub><mrow></mrow><mrow><mi>d</mi><mo>, </mo><mi>c</mi></mrow></msub><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>d</mi><mo>, </mo><mi>c</mi></mrow></msub><mo stretchy="false">|</mo><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">[</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>d</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>, </mo><mi>F</mi><msub><mrow></mrow><mrow><mi>d</mi><mo>, </mo><mn>1</mn></mrow></msub><mo>, </mo><mspace width="0.25em" /><mspace width="0.25em" /><mo>⋯</mo><mspace width="0.25em" /><mspace width="0.25em" /><mo>, </mo><mi>F</mi><msub><mrow></mrow><mrow><mi>d</mi><mo>, </mo><mi>c</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">]</mo><mo>+</mo><mi>b</mi><msub><mrow></mrow><mrow><mi>d</mi><mo>, </mo><mi>c</mi></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">式中:<i>w</i><sub><i>d</i>, <i>c</i></sub>|<sub><i>p</i></sub>表示权重是服从概率为<i>ρ</i>的伯努利分布, 设置<i>ρ</i>等于0.2;<i>w</i><sub><i>d</i>, <i>c</i></sub>和<i>F</i><sub><i>d</i>, <i>c</i></sub>分别表示第<i>d</i>个JCD的第<i>c</i>层的一组滤波器, 可表示为<i>n</i><sub>2</sub>×<i>k</i><sub>2</sub>×<i>k</i><sub>2</sub>×<i>c</i><sub>2</sub>;<i>n</i><sub>2</sub>代表滤波器的数量, 这里取32;<i>k</i><sub>2</sub>代表卷积核大小, 这里取3;<i>c</i><sub>2</sub>取值为1, 步长为1, 边界零填充为1;<i>b</i><sub><i>d</i>, <i>c</i></sub>表示偏移项;<i>σ</i>表示激活函数PReLU;<i>F</i><sub><i>d</i>, <i>c</i></sub>表示第<i>d</i>个JCD的第<i>c</i>层的输出; [<i>F</i><sub><i>d</i>-1</sub>, <i>F</i><sub><i>d</i>, 1</sub>,  …  , <i>F</i><sub><i>d</i>, <i>c</i>-1</sub>]表示由第<i>d</i>-1个JCD和第<i>d</i>个JCD的1, 2, …, <i>c</i>-1层输出的特征映射的连接。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907027_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 跳跃级联块" src="Detail/GetImg?filename=images/GXXB201907027_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 跳跃级联块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907027_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Jumping cascade block</p>

                </div>
                <h4 class="anchor-tag" id="94" name="94"><b>3.3 重建网络</b></h4>
                <div class="p1">
                    <p id="95">重建网络包括两个部分, 首先使用一个1×1的卷积层对跳跃密集连接块网络进行一个特征映射的融合, 然后将融合后的特征映射经过亚像素卷积层得到最终重建的高分辨率图像。经过1×1卷积层后特征融合的输出<i>F</i><sub>FF</sub>和最终重建的高分辨率图像<i>F</i><sub>HR</sub>可分别表示为</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>F</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>F</mtext></mrow></msub><mo>=</mo><mi>Η</mi><mo stretchy="false"> (</mo><mo stretchy="false">[</mo><mi>F</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>F</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mspace width="0.25em" /><mspace width="0.25em" /><mo>⋯</mo><mspace width="0.25em" /><mspace width="0.25em" /><mo>, </mo><mi>F</mi><msub><mrow></mrow><mi>D</mi></msub><mo stretchy="false">]</mo><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>F</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>R</mtext></mrow></msub><mo>=</mo><mi>A</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>R</mtext></mrow></msub><mo>×</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>F</mtext></mrow></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>R</mtext></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">式中:<i>D</i>表示SDCD块个数;[<i>F</i><sub>1</sub>, <i>F</i><sub>2</sub>,  …  , <i>F</i><sub><i>D</i></sub>]表示<i>D</i>个SDCD块的输出特征映射的连接;<i>H</i>表示1×1的卷积函数;<i>A</i> (·) 表示排列函数;<i>w</i><sub>HR</sub>表示一组大小为<i>n</i><sub>3</sub>×<i>k</i><sub>3</sub>×<i>k</i><sub>3</sub>×<i>c</i><sub>3</sub>的滤波器, <i>n</i><sub>3</sub>=<i>h</i><sup>2</sup>, <i>h</i>表示放大倍数, <i>k</i><sub>3</sub>代表卷积核大小, 这里取3, 步长为1, 边界零填充为1;<i>b</i><sub>HR</sub>表示偏移项。</p>
                </div>
                <div class="p1">
                    <p id="98">图像经过亚像素卷积层时进行卷积和排列两个过程:1) 经过卷积层, 输出的特征图尺寸大小与输入图像一样, 但是通道数变为<i>h</i><sup>2</sup>, 然后再将特征图像的<i>h</i><sup>2</sup>个像素按照特定的顺序排列成一个大小为<i>h</i>×<i>h</i> 的区域, 该区域对应高分辨率图像一个大小为<i>h</i>×<i>h</i>的子块, 依照这种方式, 就可以将一个大小为<i>h</i><sup>2</sup>×<i>H</i>×<i>W</i>的低分辨率特征图重新排列成一个大小为1×<i>hH</i>×<i>hW</i>的高分辨率图像, 前面的卷积是在低分辨率图像上进行的, 并且排列不需要卷积, 因此效率很高。</p>
                </div>
                <div class="p1">
                    <p id="99">图5 (a) ～ (c) 分别是本文选取图像butterfly放大4倍时, 经过低层特征提取网络、第一个跳跃级联网络和重建网络后的可视化结果, 随着网络层数的加深, 提取到的特征变复杂。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100"><b>3.4 损失函数和评价标准</b></h4>
                <div class="p1">
                    <p id="101">使用均方误差 (MSE) 作为损失函数<i>f</i><sub>MSE</sub>, MSE用来比较原始图像与重建高清图像之间的绝对误差, MSE的值越小, 说明重建出的图像与原始图像越接近。<i>f</i><sub>MSE</sub>可表示为</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>S</mtext><mtext>E</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi>F</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>R</mtext></mrow></msub><mo>-</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">式中:<i>n</i>表示训练集的数目;<i>X</i><sub><i>i</i></sub>表示原始清晰图像。使用最普遍的图像评价指标, 即峰值信噪比 (PSNR) 和结构相似度 (SSIM) <citation id="170" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, 对重建后的图像进行评价。PSNR的值越大, 表明重建后的图像失真越小, 效果越好。SSIM的值越接近于1, 表明重建后的图像与原始图像越接近, 重建图像质量越高。</p>
                </div>
                <h3 id="104" name="104" class="anchor-tag">4 实验结果与分析</h3>
                <h4 class="anchor-tag" id="105" name="105"><b>4.1 实验环境</b></h4>
                <div class="p1">
                    <p id="106">本实验基于ubuntu 16.04LTS操作系统, 训练时使用深度学习框架caffe<citation id="171" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, 并搭建了cuda, cudnn用于实验的加速, 测试使用Matlab R2014a, 电脑硬件配置CPU I7 8700, 显卡是NVDIA Geforce 1050TI。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907027_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 图像butterfly放大4倍后的可视化结果。" src="Detail/GetImg?filename=images/GXXB201907027_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 图像butterfly放大4倍后的可视化结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907027_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Visualization results of picture butterfly after 4-time magnification. </p>
                                <p class="img_note"> (a) 低层特征; (b) 跳跃级联特征; (c) 重建特征</p>
                                <p class="img_note"> (a) Low-level feature; (b) hopping cascade feature; (c) reconstructing feature</p>

                </div>
                <h4 class="anchor-tag" id="109" name="109"><b>4.2 数据集及优化方法</b></h4>
                <div class="p1">
                    <p id="110">本实验采用的数据集是公开的Timofte dataset<citation id="172" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>和Berkeley Segmentation Datasets<citation id="173" type="reference"><link href="58" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>, 将Timofte dataset中91张图片作为训练集, Set14和Set5作为测试集, 再将Berkeley Segmentation Datasets中的BSD100作为测试集。随着网络层数的加深, 需要对训练集进行数据扩增, 先将原始的91张训练集进行缩放, 缩放到原来尺寸的50%, 60%, 70%, 80%, 90%, 再进行旋转, 顺时针旋转90°, 180°, 270°, 扩增后的数据是原来的24倍;接着对图像进行预处理, 先将训练集的图像进行<i>f</i>倍的下采样, 得到对应的低分辨率的图像, 再以步长14对图像进行裁剪, 将其裁剪成25×25的子图像, 将裁剪好的子图像作为网络的输入。</p>
                </div>
                <div class="p1">
                    <p id="111">采用随机梯度下降法 (SGD) 结合动量作为优化方法, 随机梯度下降法每次只选择一个样本来更新网络的参数, 学习速度很快, 但是其波动范围较大, 会使收敛速度下降, 所以加入了动量, 可以有效地解决振荡, 并加速收敛速度, 该过程可表示为</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>W</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>W</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mi>V</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>V</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>Y</mi><mi>V</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>η</mi><mo>∇</mo><msub><mrow></mrow><mtext>W</mtext></msub><mi>J</mi><mo stretchy="false"> (</mo><mi>W</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo><mspace width="0.25em" /></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">式中:<i>t</i>为迭代次数;<i>W</i>表示权重;<i>V</i>表示更新值;<i>Y</i>表示动量, 设置为0.9;<i>J</i> (<i>W</i><sub><i>t</i></sub>) 表示代价函数;∇<sub>W</sub><i>J</i> (<i>W</i><sub><i>t</i></sub>) 表示代价函数的梯度;<i>η</i>表示学习率, 初始的学习率设置为0.0001, 每迭代100 000次, 学习率降为原来的1/10。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114"><b>4.3 网络参数的设置选择</b></h4>
                <div class="p1">
                    <p id="115">为了提升超分辨率重建后的效果, 得到最优的网络参数设置, 通过以下5个方面来展开实验, 并画出测试集峰值信噪比的均值随迭代次数增加的曲线。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116">4.3.1 层数设置</h4>
                <div class="p1">
                    <p id="117">有研究表明, 网络深度是影响网络性能的重要因素之一<citation id="174" type="reference"><link href="60" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>。适当增加网络层数可以提取更多的图像特征, 便于后期的重建, 为此进行了三组实验, 实验以<i>f</i>=4为例, 采用Set5作为测试集, 如图6所示, 分别是跳跃密集连接块里面有4 (JCD4) , 6 (JCD6) , 8 (JCD8) 个卷积层, 网络对应深度为21, 27, 35层, 初始阶段35层网络的PSNR值是最高的, 随着迭代次数的增加, 27层网络与35层网络的差距越来越小, 只保持了微小的优势, 考虑到运算的复杂度和时间成本, 最终选取27层的网络模型。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907027_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 Set5测试集下不同层数PSNR均值随迭代次数增长的曲线" src="Detail/GetImg?filename=images/GXXB201907027_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 Set5测试集下不同层数PSNR均值随迭代次数增长的曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907027_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Relationship between average PSNR of different layers with number of iterations under Set5 test set</p>

                </div>
                <h4 class="anchor-tag" id="119" name="119">4.3.2 收敛速度</h4>
                <div class="p1">
                    <p id="120">网络参数的更新速度越快, 越有利于网络收敛。进行了6组实验, 实验以<i>f</i>=4为例, 采用Set5作为测试集, 如图7所示, 分别采用了15, 21, 27层带跳跃密集连接块的网络和15, 21, 27层不带跳跃密集连接块的网络, 由 (9) 、 (10) 式可得, 权重的更新与损失函数的梯度有关, 梯度越大, 权重更新的速度越快, 越有利于网络的收敛。采用跳跃密集连接块的网络, 在反向传播时, 每一层的梯度都会接收到后面所有层的梯度, 远大于同一层没有跳跃密集连接块的网络梯度, 进而加速了参数更新的速度, 提高了网络的收敛速度。与不同层数的网络模型和不带跳跃密集连接块的网络模型相比, 本文模型的收敛速度更快, 效果更好。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907027_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 Set5测试集下不同网络结构的迭代次数与PSNR均值的关系曲线" src="Detail/GetImg?filename=images/GXXB201907027_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 Set5测试集下不同网络结构的迭代次数与PSNR均值的关系曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907027_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Relationship between iteration times and average PSNR of different network structures under Set5 test set</p>

                </div>
                <h4 class="anchor-tag" id="122" name="122">4.3.3 激活函数</h4>
                <div class="p1">
                    <p id="123">使用激活函数可以增加网络的非线性, 提高网络模型的表达能力。进行两组实验, 实验以<i>f</i>=4为例, 采用Set5作为测试集, 分别使用ReLU函数与PReLU函数进行比较, ReLU函数的表达式<i>f</i> (<i>x</i>) =max (0, <i>x</i>) , 从这个表达式可以看出, 负半轴的斜率为零, 在训练过程中, 一旦有数据进入负半区, 就会发生神经元死亡, 而PReLU函数<i>Φ</i> (<i>x</i>) =max (0, <i>x</i>) +<i>α</i><sub><i>i</i></sub>min (0, <i>x</i>) , Δ<i>α</i><sub><i>i</i>+1</sub>=<i>μ</i>Δ<i>α</i><sub><i>i</i></sub>+<i>η</i>∂<i>ε</i>/∂<i>α</i><sub><i>i</i></sub>, PReLU函数可以自适应地学习更新参数<i>α</i><sub><i>i</i></sub>, 有效避免了神经元死亡。从图8中可以看出, PReLU函数比ReLU函数的效果更好, 收敛速度更快, 所以选择PReLU函数作为本文的激活函数。</p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907027_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 Set5测试集下不同激活函数PSNR均值随迭代次数的变化曲线" src="Detail/GetImg?filename=images/GXXB201907027_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 Set5测试集下不同激活函数PSNR均值随迭代次数的变化曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907027_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Relationship between average PSNR of different activation functions with number of iterations under Set5 test set</p>

                </div>
                <h4 class="anchor-tag" id="125" name="125">4.3.4 滤波器数目</h4>
                <div class="p1">
                    <p id="126">滤波器数目的不同也会影响网络模型的效果。实验以<i>f</i>=4为例, 采用Set5作为测试集, 如图9所示, 分别测试滤波器数目为8和16时, 可以发现随着滤波器数目的增大, 网络模型的效果明显变好了。本文选择滤波器数目为32的网络模型。</p>
                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907027_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 Set5测试集下不同滤波器数目条件下的PSNR均值随迭代次数的变化曲线" src="Detail/GetImg?filename=images/GXXB201907027_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 Set5测试集下不同滤波器数目条件下的PSNR均值随迭代次数的变化曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907027_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Relationship between average PSNR of different filter numbers with number of iterations under Set5 test set</p>

                </div>
                <h4 class="anchor-tag" id="128" name="128">4.3.5 运行时间</h4>
                <div class="p1">
                    <p id="129">为了比较不同模型的运行时间, 在相同的环境下进行了5组实验, 实验以<i>f</i>=4为例, 采用Set5作为测试集, 如图10所示, 分别测试了 DCSR、VDSR<citation id="175" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、ESPCN<citation id="176" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、FSRCNN<citation id="177" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和SRCNN<citation id="178" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 可以发现模型DCSR的PSNR值最高, 为31.87 dB, 说明在重建图像效果方面, 本文的模型是最好的, 这是因为本文的网络层数最多 (27层) , 运行时间为0.756 s, 比大部分模型都快, 仅比ESPCN慢了0.1 s。</p>
                </div>
                <h4 class="anchor-tag" id="130" name="130"><b>4.4 实验结果分析</b></h4>
                <div class="p1">
                    <p id="131">为了测试本文算法 (DCSR) 的性能, 将其分别与Bicubic、SRCNN<citation id="179" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、FSRCNN<citation id="180" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、ESPCN<citation id="181" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>和VDSR<citation id="182" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>进行了比较, 分别测试了图像放大2, 3, 4倍的效果, 测试集采用了图像超分辨率重建领域常用的数据集, 包括Set5、Set14和BSD100, 从中选取了边缘细节丰富的3张图像进行测试, 并比较了图像放大后的细节。受数据集和迭代次数的影响, FSRCNN<citation id="183" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、VDSR<citation id="184" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>与本文算法的结果略有不同。从主观的视觉效果来看, Bicubic的整体效果最模糊, 看不清楚图像的细节信息。SRCNN<citation id="185" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>和ESPCN<citation id="186" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>的整体效果比较好, 但是图像边缘还是比较模糊, FSRCNN<citation id="187" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和VDSR<citation id="188" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>的效果相对较好, 细节恢复也相对完整, DCSR能恢复出更多的图像细节, 边缘也更加锐利, 辨识度较高。特别是放大倍数为4时, 相比于其他算法, DCSR可以轻松发现视觉上的改进, 图11中DCSR重建出的蝴蝶翅膀的边缘信息更加锐利, 对比度更高, 图12中DCSR重建出的How字母局部的高频信息更加丰富, 边缘更加自然。图13中DCSR重建出的衣服细纹细节信息更清晰, 主观效果更好。表1和表2为测试集Set5、 Set14和BSD100下不同方法的PSNR和SSIM均值, 可以看出, 本文模型的PSNR值和SSIM值均是最高的, 从恢复出的图像效果来看, 本文算法恢复出的图像最接近原始图像。</p>
                </div>
                <div class="area_img" id="132">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907027_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 Set5测试集下不同方法的运行时间与PSNR均值的关系曲线" src="Detail/GetImg?filename=images/GXXB201907027_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 Set5测试集下不同方法的运行时间与PSNR均值的关系曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907027_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Relationship between run time and average PSNR of different methods under Set5 test set</p>

                </div>
                <div class="area_img" id="135">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907027_13500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 Butterfly在不同算法下的效果对比图" src="Detail/GetImg?filename=images/GXXB201907027_13500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 Butterfly在不同算法下的效果对比图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907027_13500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Comparison of butterfly images obtained by different algorithms</p>

                </div>
                <div class="area_img" id="136">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907027_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 PPT在不同算法下的效果对比图" src="Detail/GetImg?filename=images/GXXB201907027_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 PPT在不同算法下的效果对比图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907027_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Comparison of PPT obtained by different algorithms</p>

                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907027_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 Man在不同算法的对比图" src="Detail/GetImg?filename=images/GXXB201907027_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 Man在不同算法的对比图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907027_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 13 Comparison of man images obtained by different algorithms</p>

                </div>
                <div class="area_img" id="138">
                    <p class="img_tit">表1 在测试集Set5, Set14, BSD100下比较不同方法的PSNR均值 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Average PSNR of test sets Set5, Set14, and BSD100 under different algorithms</p>
                    <p class="img_note"></p>
                    <table id="138" border="1"><tr><td>Dataset</td><td>Scale</td><td>Bicubic</td><td>SRCNN<sup>[12]</sup></td><td>ESPCN<sup>[14]</sup></td><td>FSRCNN<sup>[13]</sup></td><td>VDSR<sup>[15]</sup></td><td>DCSR</td></tr><tr><td><br /></td><td>2</td><td>33.68</td><td>36.19</td><td>36.38</td><td>36.45</td><td>37.34</td><td>37.70</td></tr><tr><td><br />Set5</td><td>3</td><td>30.45</td><td>32.46</td><td>32.71</td><td>32.59</td><td>33.47</td><td>34.13</td></tr><tr><td><br /></td><td>4</td><td>28.46</td><td>30.15</td><td>30.29</td><td>30.42</td><td>30.78</td><td>31.87</td></tr><tr><td><br /></td><td>2</td><td>30.21</td><td>32.1</td><td>32.2</td><td>32.21</td><td>32.82</td><td>33.26</td></tr><tr><td><br />Set14</td><td>3</td><td>27.51</td><td>28.99</td><td>29.12</td><td>29.12</td><td>29.51</td><td>29.97</td></tr><tr><td><br /></td><td>4</td><td>25.98</td><td>27.23</td><td>27.17</td><td>27.43</td><td>27.62</td><td>28.27</td></tr><tr><td><br /></td><td>2</td><td>29.43</td><td>30.88</td><td>30.93</td><td>31.24</td><td>31.51</td><td>31.81</td></tr><tr><td><br />BSD100</td><td>3</td><td>27.08</td><td>28.06</td><td>28.16</td><td>28.25</td><td>28.43</td><td>28.79</td></tr><tr><td><br /></td><td>4</td><td>25.84</td><td>26.63</td><td>26.59</td><td>26.85</td><td>26.87</td><td>27.28</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="139">
                    <p class="img_tit">表2 在测试集Set5, Set14, BSD100下比较不同方法的SSIM均值 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Average SSIM of test sets Set5, Set14, and BSD100 under different algorithms</p>
                    <p class="img_note"></p>
                    <table id="139" border="1"><tr><td>Dataset</td><td>Scale</td><td>Bicubic</td><td>SRCNN<sup>[12]</sup></td><td>ESPCN<sup>[14]</sup></td><td>FSRCNN<sup>[13]</sup></td><td>VDSR<sup>[15]</sup></td><td>DCSR</td></tr><tr><td><br /></td><td>2</td><td>0.9306</td><td>0.9551</td><td>0.9568</td><td>0.9567</td><td>0.9580</td><td>0.9636</td></tr><tr><td><br />Set5</td><td>3</td><td>0.8686</td><td>0.9110</td><td>0.9150</td><td>0.9122</td><td>0.9188</td><td>0.9321</td></tr><tr><td><br /></td><td>4</td><td>0.8102</td><td>0.8621</td><td>0.8629</td><td>0.8659</td><td>0.8750</td><td>0.8979</td></tr><tr><td><br /></td><td>2</td><td>0.8693</td><td>0.9576</td><td>0.9597</td><td>0.9634</td><td>0.9104</td><td>0.9644</td></tr><tr><td><br />Set14</td><td>3</td><td>0.7744</td><td>0.8836</td><td>0.8873</td><td>0.8923</td><td>0.8271</td><td>0.8981</td></tr><tr><td><br /></td><td>4</td><td>0.7023</td><td>0.8207</td><td>0.8227</td><td>0.8273</td><td>0.7592</td><td>0.8446</td></tr><tr><td><br /></td><td>2</td><td>0.8440</td><td>0.8801</td><td>0.8831</td><td>0.8867</td><td>0.8924</td><td>0.8964</td></tr><tr><td><br />BSD100</td><td>3</td><td>0.7401</td><td>0.7755</td><td>0.7811</td><td>0.7803</td><td>0.7928</td><td>0.7987</td></tr><tr><td><br /></td><td>4</td><td>0.6697</td><td>0.692</td><td>0.6943</td><td>0.7009</td><td>0.7186</td><td>0.7203</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="140" name="140" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="141">针对现有模型收敛速度慢, 训练前需要对图片进行预处理, 以及未充分利用各卷积层提取到的图像特征等问题, 提出了一种基于跳跃密集连接的单幅图像超分辨率重建的方法, 该算法充分利用各卷积层学习到的图像特征, 加快了网络的收敛速度, 并减少了网络的冗余性, 且该算法使用亚像素卷积对图像进行放大。本文算法在评价指标和视觉效果上均优于现有算法, 下一步将继续优化网络模型, 在保证测试精度的同时, 降低运行时间。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="14">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A blind super-resolution reconstruction method considering image registration errors">

                                <b>[1]</b> Zhang H, Zhang L, Shen H.A blind super-resolution reconstruction method considering image registration errors[J].International Journal of Fuzzy Systems, 2015, 17 (2) :353-364.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES56BE354FB2B92978E52A6C8F7643372A&amp;v=MDAzOTF3eUI1bTd6ME1UZ3pxMmhVemZiR1hRcmp1Q09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGhoeGJ5OHhLcz1OaWZPZmJhK2JLVFBxb3N6RnVsOUJYNA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Chen H G, He X H, Teng Q Z, <i>et al</i>.Single image super resolution using local smoothness and nonlocal self-similarity priors[J].Signal Processing:Image Communication, 2016, 43:68-81.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201712015&amp;v=Mjc3MzZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5oVUx2TElqWFRiTEc0SDliTnJZOUU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Li S M, Lei G Q, Fan R.Depth map super-resolution reconstruction based on convolutional neural networks[J].Acta Optica Sinica, 2017, 37 (12) :1210002.李素梅, 雷国庆, 范如.基于卷积神经网络的深度图超分辨率重建[J].光学学报, 2017, 37 (12) :1210002.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hybrid super-resolution combining example-based single-image and interpolationbased multi-image reconstruction approaches">

                                <b>[4]</b> Bätz M, Eichenseer A, Seiler J, <i>et al</i>.Hybrid super-resolution combining example-based single-image and interpolation-based multi-image reconstruction approaches[C]//2015 IEEE International Conference on Image Processing (ICIP) , September 27-30, 2015, Quebec City, QC, Canada.New York:IEEE, 2015:58-62.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-image super-resolution using sparse regression and natural image prior">

                                <b>[5]</b> Kim K I, Kwon Y.Single-image super-resolution using sparse regression and natural image prior[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (6) :1127-1133.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coupled Dictionary Training for Image Super-Resolution">

                                <b>[6]</b> Yang J, Wang Z, Lin Z, <i>et al</i>.Couple dictionary training for image super-resolution[J].IEEE Transactions on Image Processing, 2012, 21 (8) :3467-3478.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201703012&amp;v=MjAwODZPZVplVnVGeW5oVUx2TElqWFRiTEc0SDliTXJJOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Xiao J S, Liu E Y, Zhu L, <i>et al</i>.Improved image super-resolution algorithm based on convolutional neural network[J].Acta Optica Sinica, 2017, 37 (3) :0318011.肖进胜, 刘恩雨, 朱力, 等.改进的基于卷积神经网络的图像超分辨率算法[J].光学学报, 2017, 37 (3) :0318011.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving resolution by image registration">

                                <b>[8]</b> Irani M, Peleg S.Improving resolution by image registration[J].CVGIP:Graphical Models and Image Processing, 1991, 53 (3) :231-239.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-resolution image recovery from image-plane arrays, using convex projections">

                                <b>[9]</b> Stark H, Oskoui P.High-resolution image recovery from image-plane arrays, using convex projections[J].Journal of the Optical Society of America A, 1989, 6 (11) :1715-1726.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Super-resolution through neighbor embedding">

                                <b>[10]</b> Chang H, Yeung D Y, Xiong Y M.Super-resolution through neighbor embedding[C]//Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 27-July 2, 2004, Washington D C, USA.New York:IEEE, 2004:1315043.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">

                                <b>[11]</b> Yang J C, Wright J, Huang T S, <i>et al</i>.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing, 2010, 19 (11) :2861-2873.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using deep convolutional networks">

                                <b>[12]</b> Dong C, Loy C C, He K M, <i>et al</i>.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (2) :295-307.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accelerating the super-resolution convolutional neural network">

                                <b>[13]</b> Dong C, Chen C L, Tang X.Accelerating the super-resolution convolutional neural network[M]//Leibe B, Matas J, Sebe N, <i>et al</i>.Lecture notes in computer science.Cham:Springer, 2016, 9906:391-407.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network">

                                <b>[14]</b> Shi W Z, Caballero J, Huszár F, <i>et al</i>.Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1874-1883.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate image super-resolution using very deep convolutional networks">

                                <b>[15]</b> Kim J, Lee J K, Lee K M.Accurate image super-resolution using very deep convolutional networks[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:1646-1654.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[16]</b> He K M, Zhang X Y, Ren S Q, <i>et al</i>.Deep residual learning for image recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep networks with stochastic depth">

                                <b>[17]</b> Huang G, Sun Y, Liu Z, <i>et al</i>.Deep networks with stochastic depth[M]//Leibe B, Matas J, Sebe N, <i>et al</i>.Lecture notes in computer science.Cham:Springer, 2016, 9908:646-661.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Densely connected convolutional networks">

                                <b>[18]</b> Huang G, Liu Z, Maaten L V D, <i>et al</i>.Densely connected convolutional networks[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:2261-2269.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint hierarchical category structure learning and large-scale image classification">

                                <b>[19]</b> Qu Y Y, Lin L, Shen F M, <i>et al</i>.Joint hierarchical category structure learning and large-scale image classification[J].IEEE Transactions on Image Processing, 2017, 26 (9) :4331-4346.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust Registration of Multimodal Remote Sensing Images Based on Structural Similarity">

                                <b>[20]</b> Ye Y X, Shan J, Bruzzone L, <i>et al</i>.Robust registration of multimodal remote sensing images based on structural similarity[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (5) :2941-2958.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffe:convolutional architecture for fast feature embedding">

                                <b>[21]</b> Jia Y, Shelhamer E, Donahue J, <i>et al</i>.Caffe:convolutional architecture for fast feature embedding [EB/OL]. (2014-06-20) [2019-03-09].https://arxiv.org/abs/1408.5093.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A+:Adjusted anchored neighborhood regression for fast superresolution">

                                <b>[22]</b> Timofte R, de Smet V, van Gool L.A+:adjusted anchored neighborhood regression for fast super-resolution[M]//Cremers D, Reid I, Saito H, <i>et al</i>.Lecture notes in computer science, Cham:Springer, 2014, 9006:111-126.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics">

                                <b>[23]</b> Martin D, Fowlkes C, Tal D, <i>et al</i>.A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics[C]//Proceedings Eighth IEEE International Conference on Computer Vision, July 7-14, 2001, Vancouver, BC, Canada.New York:IEEE, 2001:416-423.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust and Discriminative Labeling for Multi-Label Active Learning Based on Maximum Correntropy Criterion">

                                <b>[24]</b> Du B, Wang Z M, Zhang L F, <i>et al</i>.Robust and discriminative labeling for multi-label active learning based on maximum correntropy criterion[J].IEEE Transactions on Image Processing, 2017, 26 (4) :1694-1707.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201907027" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201907027&amp;v=MDk3MzVIWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnluaFVMdkxJalhUYkxHNEg5ak1xSTk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

