

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133066723252500%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dGXXB201911029%26RESULT%3d1%26SIGN%3d11%252fuZFihz4YCnmDKp5jrxWviAZQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201911029&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201911029&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201911029&amp;v=Mjk5OTBWdkZ5dmdWcjNJSWpYVGJMRzRIOWpOcm85SGJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#62" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#69" data-title="2 算法概述 ">2 算法概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#72" data-title="3 复杂情况下自适应特征更新目标跟踪算法 ">3 复杂情况下自适应特征更新目标跟踪算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="&lt;b&gt;3.1 目标特征表征&lt;/b&gt;"><b>3.1 目标特征表征</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;3.2 基于投票的融合特征选择&lt;/b&gt;"><b>3.2 基于投票的融合特征选择</b></a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;3.3 复杂情况下更新机制&lt;/b&gt;"><b>3.3 复杂情况下更新机制</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;3.4 算法总体流程&lt;/b&gt;"><b>3.4 算法总体流程</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#122" data-title="4 实验与分析 ">4 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#123" data-title="&lt;b&gt;4.1 实验环境&lt;/b&gt;"><b>4.1 实验环境</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;4.2 评价指标&lt;/b&gt;"><b>4.2 评价指标</b></a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;4.3 定性分析&lt;/b&gt;"><b>4.3 定性分析</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;4.4 定量分析&lt;/b&gt;"><b>4.4 定量分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#167" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#71" data-title="图1 算法框架图">图1 算法框架图</a></li>
                                                <li><a href="#77" data-title="图2 目标特征可视化。">图2 目标特征可视化。</a></li>
                                                <li><a href="#78" data-title="图3 特征融合示意图">图3 特征融合示意图</a></li>
                                                <li><a href="#119" data-title="图4 时序模型更新机制">图4 时序模型更新机制</a></li>
                                                <li><a href="#125" data-title="图5 算法流程">图5 算法流程</a></li>
                                                <li><a href="#137" data-title="图6 10种跟踪算法在部分视频序列上的定性结果显示">图6 10种跟踪算法在部分视频序列上的定性结果显示</a></li>
                                                <li><a href="#145" data-title="表1 特征器频次统计">表1 特征器频次统计</a></li>
                                                <li><a href="#147" data-title="图7 不同特征器跟踪结果">图7 不同特征器跟踪结果</a></li>
                                                <li><a href="#151" data-title="图8 在OTB-2013和OTB-2015数据库下,算法跟踪精度和跟踪成功率。">图8 在OTB-2013和OTB-2015数据库下,算法跟踪精度和跟踪成功率。</a></li>
                                                <li><a href="#156" data-title="表2 算法在OTB-2013和OTB2015上精度、成功率值和速率">表2 算法在OTB-2013和OTB2015上精度、成功率值和速率</a></li>
                                                <li><a href="#162" data-title="图9 在OTB-2013下11种不同属性视频序列跟踪精度。">图9 在OTB-2013下11种不同属性视频序列跟踪精度。</a></li>
                                                <li><a href="#163" data-title="图10 在OTB-2013下11种不同属性视频序列跟踪成功率。">图10 在OTB-2013下11种不同属性视频序列跟踪成功率。</a></li>
                                                <li><a href="#164" data-title="图11 TB-2015下11种不同属性视频序列跟踪精度。">图11 TB-2015下11种不同属性视频序列跟踪精度。</a></li>
                                                <li><a href="#165" data-title="图12 OTB-2015下11种不同属性视频序列跟踪成功率。">图12 OTB-2015下11种不同属性视频序列跟踪成功率。</a></li>
                                                <li><a href="#166" data-title="图13 长时跟踪情况下算法跟踪精度和成功率。">图13 长时跟踪情况下算法跟踪精度和成功率。</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" title=" Wang Q,Zhang L,Bertinetto L,&lt;i&gt;et al&lt;/i&gt;.Fast online object tracking and segmentation:a unifying approach[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 16-20,2019,Long Beach,CA,USA.New York:IEEE,2019:1328-1338." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast online object tracking and segmentation:a unifying approach">
                                        <b>[1]</b>
                                         Wang Q,Zhang L,Bertinetto L,&lt;i&gt;et al&lt;/i&gt;.Fast online object tracking and segmentation:a unifying approach[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 16-20,2019,Long Beach,CA,USA.New York:IEEE,2019:1328-1338.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" Lu H C,Li P X,Wang D.Visual object tracking:a survey[J].Pattern Recognition and Artificial Intelligence,2018,31(1):61-76.卢湖川,李佩霞,王栋.目标跟踪算法综述[J].模式识别与人工智能,2018,31(1):61-76." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201801008&amp;v=MjAzODFyQ1VSTE9lWmVWdkZ5dmdWcjNMS0Q3WWJMRzRIOW5Ncm85RmJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Lu H C,Li P X,Wang D.Visual object tracking:a survey[J].Pattern Recognition and Artificial Intelligence,2018,31(1):61-76.卢湖川,李佩霞,王栋.目标跟踪算法综述[J].模式识别与人工智能,2018,31(1):61-76.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" title=" Li J L,Yin K,Chu C X,&lt;i&gt;et al&lt;/i&gt;.Review of video target tracking technology[J].Journal of Yanshan University,2019,43(3):251-262.李均利,尹宽,储诚曦,等.视频目标跟踪技术综述[J].燕山大学学报,2019,43(3):251-262." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DBZX201903009&amp;v=MzEyNjZHNEg5ak1ySTlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl2Z1ZyM0xJUy9SZHI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Li J L,Yin K,Chu C X,&lt;i&gt;et al&lt;/i&gt;.Review of video target tracking technology[J].Journal of Yanshan University,2019,43(3):251-262.李均利,尹宽,储诚曦,等.视频目标跟踪技术综述[J].燕山大学学报,2019,43(3):251-262.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" title=" Bolme D S,Beveridge J R,Draper B A,&lt;i&gt;et al&lt;/i&gt;.Visual object tracking using adaptive correlation filters[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,June 13-18,2010,San Francisco,CA,USA.New York:IEEE,2010:2544-2550." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">
                                        <b>[4]</b>
                                         Bolme D S,Beveridge J R,Draper B A,&lt;i&gt;et al&lt;/i&gt;.Visual object tracking using adaptive correlation filters[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,June 13-18,2010,San Francisco,CA,USA.New York:IEEE,2010:2544-2550.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" title=" Henriques J F,Caseiro R,Martins P,&lt;i&gt;et al&lt;/i&gt;.Exploiting the circulant structure of tracking-by-detection with kernels[M]//Fitzgibbon A,Lazebnik S,Perona P,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2012.Lecture notes in computer science.Berlin,Heidelberg:Springer,2012,7575:702-715." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">
                                        <b>[5]</b>
                                         Henriques J F,Caseiro R,Martins P,&lt;i&gt;et al&lt;/i&gt;.Exploiting the circulant structure of tracking-by-detection with kernels[M]//Fitzgibbon A,Lazebnik S,Perona P,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2012.Lecture notes in computer science.Berlin,Heidelberg:Springer,2012,7575:702-715.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" title=" Zhang K H,Zhang L,Liu Q S,&lt;i&gt;et al&lt;/i&gt;.Fast visual tracking via dense spatio-temporal context learning[M]//Fleet D,Pajdla T,Schiele B,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8693:127-141." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast visual tracking via dense spatio-temporal context learning">
                                        <b>[6]</b>
                                         Zhang K H,Zhang L,Liu Q S,&lt;i&gt;et al&lt;/i&gt;.Fast visual tracking via dense spatio-temporal context learning[M]//Fleet D,Pajdla T,Schiele B,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8693:127-141.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" Danelljan M,H&#228;ger G,Khan F S,&lt;i&gt;et al&lt;/i&gt;.Accurate scale estimation for robust visual tracking[C]//Proceedings of the British Machine Vision Conference 2014,September 1-5,2014,University of Nottingham,UK.UK:BMVA Press,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate scale estimation for robust visual tracking">
                                        <b>[7]</b>
                                         Danelljan M,H&#228;ger G,Khan F S,&lt;i&gt;et al&lt;/i&gt;.Accurate scale estimation for robust visual tracking[C]//Proceedings of the British Machine Vision Conference 2014,September 1-5,2014,University of Nottingham,UK.UK:BMVA Press,2014.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" title=" Wang N Y,Yeung D Y.Learning a deep compact image representation for visual tracking[C]//NIPS′13 Proceedings of the 26th International Conference on Neural Information Processing Systems,December 5-10,2013,Lake Tahoe,Nevada.New York:ACM,2013,1:809-817." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a Deep Compact Image Representation for Visual Tracking">
                                        <b>[8]</b>
                                         Wang N Y,Yeung D Y.Learning a deep compact image representation for visual tracking[C]//NIPS′13 Proceedings of the 26th International Conference on Neural Information Processing Systems,December 5-10,2013,Lake Tahoe,Nevada.New York:ACM,2013,1:809-817.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" Danelljan M,Robinson A,Khan F S,&lt;i&gt;et al&lt;/i&gt;.Beyond correlation filters:learning continuous convolution operators for visual tracking[M]//Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9909:472-488." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond Correlation Filters:Learning Continuous Convolution Operators for Visual Tracking">
                                        <b>[9]</b>
                                         Danelljan M,Robinson A,Khan F S,&lt;i&gt;et al&lt;/i&gt;.Beyond correlation filters:learning continuous convolution operators for visual tracking[M]//Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9909:472-488.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" title=" Wang N,Zhou W G,Tian Q,&lt;i&gt;et al&lt;/i&gt;.Multi-cue correlation filters for robust visual tracking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:4844-4853." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-cue correlation filters for robust visual tracking">
                                        <b>[10]</b>
                                         Wang N,Zhou W G,Tian Q,&lt;i&gt;et al&lt;/i&gt;.Multi-cue correlation filters for robust visual tracking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:4844-4853.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" title=" Shen Q,Yan X L,Liu L F,&lt;i&gt;et al&lt;/i&gt;.Multi-scale correlation filtering tracker based on adaptive feature selection[J].Acta Optica Sinica,2017,37(5):0515001.沈秋,严小乐,刘霖枫,等.基于自适应特征选择的多尺度相关滤波跟踪[J].光学学报,2017,37(5):0515001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201705021&amp;v=MjMxNzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXZnVnIzTElqWFRiTEc0SDliTXFvOUhaWVE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Shen Q,Yan X L,Liu L F,&lt;i&gt;et al&lt;/i&gt;.Multi-scale correlation filtering tracker based on adaptive feature selection[J].Acta Optica Sinica,2017,37(5):0515001.沈秋,严小乐,刘霖枫,等.基于自适应特征选择的多尺度相关滤波跟踪[J].光学学报,2017,37(5):0515001.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" title=" Ge B Y,Zuo X Z,Hu Y J.Long-term object tracking based on feature fusion[J].Acta Optica Sinica,2018,38(11):1115002.葛宝义,左宪章,胡永江.基于特征融合的长时目标跟踪算法[J].光学学报,2018,38(11):1115002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811025&amp;v=MTIzNzNvOUhZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXZnVnIzTElqWFRiTEc0SDluTnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Ge B Y,Zuo X Z,Hu Y J.Long-term object tracking based on feature fusion[J].Acta Optica Sinica,2018,38(11):1115002.葛宝义,左宪章,胡永江.基于特征融合的长时目标跟踪算法[J].光学学报,2018,38(11):1115002.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" title=" Dalal N,Triggs B.Histograms of oriented gradients for human detection[C]∥2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR′05),June 20-25,2005,San Diego,CA,USA.New York:IEEE,2005:8588935." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">
                                        <b>[13]</b>
                                         Dalal N,Triggs B.Histograms of oriented gradients for human detection[C]∥2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR′05),June 20-25,2005,San Diego,CA,USA.New York:IEEE,2005:8588935.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" Danelljan M,Khan F S,Felsberg M,&lt;i&gt;et al&lt;/i&gt;.Adaptive color attributes for real-time visual tracking[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:1090-1097." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive color attributes for real-time visual tracking">
                                        <b>[14]</b>
                                         Danelljan M,Khan F S,Felsberg M,&lt;i&gt;et al&lt;/i&gt;.Adaptive color attributes for real-time visual tracking[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:1090-1097.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" Simonyan K,Zissermanl A.Very deep convolutional networks for large-scale image recognition[J/OL].(2015-04-10)[2019-05-30].https://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[15]</b>
                                         Simonyan K,Zissermanl A.Very deep convolutional networks for large-scale image recognition[J/OL].(2015-04-10)[2019-05-30].https://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" title=" Bhat G,Johnander J,Danelljan M,&lt;i&gt;et al&lt;/i&gt;.Unveiling the power of deep tracking[M]//Ferrari V,Hebert M,Sminchisescu C,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2018.Lecture notes in computer science.Cham:Springer,2018,11206:493-509." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unveiling the power of deep tracking">
                                        <b>[16]</b>
                                         Bhat G,Johnander J,Danelljan M,&lt;i&gt;et al&lt;/i&gt;.Unveiling the power of deep tracking[M]//Ferrari V,Hebert M,Sminchisescu C,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2018.Lecture notes in computer science.Cham:Springer,2018,11206:493-509.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" title=" Wu Y,Lim J,Yang M H.Online object tracking:a benchmark[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:2411-2418." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online object tracking:A benchmark">
                                        <b>[17]</b>
                                         Wu Y,Lim J,Yang M H.Online object tracking:a benchmark[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:2411-2418.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" Wu Y,Lim J,Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(9):1834-1848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">
                                        <b>[18]</b>
                                         Wu Y,Lim J,Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(9):1834-1848.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_19" title=" Danelljan M,Bhat G,Khan F S,&lt;i&gt;et al&lt;/i&gt;.ECO:efficient convolution operators for tracking[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:6931-6939." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ECO:Efficient Convolution Operators for Tracking">
                                        <b>[19]</b>
                                         Danelljan M,Bhat G,Khan F S,&lt;i&gt;et al&lt;/i&gt;.ECO:efficient convolution operators for tracking[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:6931-6939.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_20" title=" Ma C,Huang J B,Yang X K,&lt;i&gt;et al&lt;/i&gt;.Hierarchical convolutional features for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:3074-3082." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical Convolutional Features for Visual Tracking">
                                        <b>[20]</b>
                                         Ma C,Huang J B,Yang X K,&lt;i&gt;et al&lt;/i&gt;.Hierarchical convolutional features for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:3074-3082.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_21" >
                                        <b>[21]</b>
                                     Danelljan M,Hager G,Khan F S,&lt;i&gt;et al&lt;/i&gt;.Learning spatially regularized correlation filters for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:4310-4318.</a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_22" title=" Bertinetto L,Valmadre J,Golodetz S,&lt;i&gt;et al&lt;/i&gt;.Staple:complementary learners for real-time tracking[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1401-1409." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Staple:complementary learners for real-time tracking">
                                        <b>[22]</b>
                                         Bertinetto L,Valmadre J,Golodetz S,&lt;i&gt;et al&lt;/i&gt;.Staple:complementary learners for real-time tracking[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1401-1409.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_23" title=" Yun S,Choi J,Yoo Y,&lt;i&gt;et al&lt;/i&gt;.Action-decision networks for visual tracking with deep reinforcement learning[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:1349-1358." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning">
                                        <b>[23]</b>
                                         Yun S,Choi J,Yoo Y,&lt;i&gt;et al&lt;/i&gt;.Action-decision networks for visual tracking with deep reinforcement learning[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:1349-1358.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_24" title=" Henriques J F,Caseiro R,Martins P,&lt;i&gt;et al&lt;/i&gt;.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(3):583-596." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">
                                        <b>[24]</b>
                                         Henriques J F,Caseiro R,Martins P,&lt;i&gt;et al&lt;/i&gt;.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(3):583-596.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_25" title=" Li F,Tian C,Zuo W M,&lt;i&gt;et al&lt;/i&gt;.Learning spatial-temporal regularized correlation filters for visual tracking[C]∥2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:4904-4913." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatial-temporal regularized correlation filters for visual tracking">
                                        <b>[25]</b>
                                         Li F,Tian C,Zuo W M,&lt;i&gt;et al&lt;/i&gt;.Learning spatial-temporal regularized correlation filters for visual tracking[C]∥2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:4904-4913.
                                    </a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_26" title=" Ma C,Yang X K,Zhang C Y,&lt;i&gt;et al&lt;/i&gt;.Long-term correlation tracking[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:5388-5396." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long-term correlation tracking">
                                        <b>[26]</b>
                                         Ma C,Yang X K,Zhang C Y,&lt;i&gt;et al&lt;/i&gt;.Long-term correlation tracking[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:5388-5396.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-07-23 13:23</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(11),235-250 DOI:10.3788/AOS201939.1115002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>复杂情况下自适应特征更新目标跟踪算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B0%B9%E5%AE%BD&amp;code=42074464&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">尹宽</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%9D%87%E5%88%A9&amp;code=28262498&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李均利</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E4%B8%BD&amp;code=08832062&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李丽</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%82%A8%E8%AF%9A%E6%9B%A6&amp;code=28262497&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">储诚曦</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%9B%E5%B7%9D%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0105226&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">四川师范大学计算机科学学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%AE%81%E6%B3%A2%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0160135&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">宁波大学信息科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为提高复杂情况下目标跟踪的稳健性,提出一种自适应特征更新的目标跟踪算法。对目标提取分级深度特征和手工设计特征,通过不同线性组合方式进行多特征融合,构建多个融合特征器;对不同融合特征器进行可信度判定,选择可信度最高的融合特征作为当前帧的跟踪特征,构建位置相关滤波器,预测出当前帧的目标位置;对跟踪结果进行可靠性检测,可靠性低于阈值则启动融合特征器更新机制,加入时序信息和语义信息进行重跟踪,降低了模型的误差累积。在OTB-2013和OTB-2015数据库上进行测试,结果表明,与近年来比较流行的9种算法相比,提出的算法在快速运动、背景杂波、运动模糊、形变等复杂情况下具有较高的成功率和较好的稳健性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标跟踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E7%BA%A7%E6%B7%B1%E5%BA%A6%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分级深度特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%85%B3%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相关滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%B6%E5%BA%8F%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">时序信息;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李均利,E-mail:li.junli@vip.163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-31</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61403266,61403196);</span>
                    </p>
            </div>
                    <h1><b>Adaptive Feature Update Object-Tracking Algorithm in Complex Situations</b></h1>
                    <h2>
                    <span>Yin Kuan</span>
                    <span>Li Junli</span>
                    <span>Li Li</span>
                    <span>Chu Chengxi</span>
            </h2>
                    <h2>
                    <span>College of Computer Science, Sichuan Normal University</span>
                    <span>Faculty of Electrical Engineering and Computer Science, Ningbo University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To improve the robustness of object tracking in complex situations, a new algorithm based on adaptive feature updating is proposed. First, hierarchical deep and hand-crafted features are simultaneously extracted from the object, and multiple fusion feature experts are constructed through multi-feature fusion by using different linear combination methods. Second, the credibility score of each expert is computed and the highest score is selected as the tracking feature of the current frame. A position correlation filter is then constructed to predict the frame′s target position. Finally, the reliability of the tracking result is detected. When this reliability is found to be lower than a certain threshold, the fusion feature updating mechanism is initiated, and the temporal and semantic informations are added to the re-track, which reduces the error accumulation of the model. The proposed algorithm is tested on OTB-2013 and OTB-2015 datasets, and the obtained results are compared with those of 9 recently developed popular algorithms. Our proposed algorithm demonstrates a higher success rate and better robustness in complex situations, such as fast motion, background clutter, motion blur, and deformation, than existing algorithms.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=object%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">object tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=hierarchical%20deep%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">hierarchical deep feature;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=correlation%20filter&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">correlation filter;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=temporal%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">temporal information;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-05-31</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="62" name="62" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="63">目标跟踪是计算机视觉领域近年来的研究热点,同时也是各种视频应用中的一个基本任务<citation id="200" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,广泛应用于智能视频监控、智能交通系统、智能视觉导航、现代化军事、人机交互等领域<citation id="201" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。现实场景和目标自身存在的复杂性和不确定性<citation id="202" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>,例如场景的光照变化、角度变化、遮挡情况和目标运动过程中出现的姿态变化、尺度变化,给目标跟踪带来了一系列挑战,使得目标跟踪的效果受到一定的影响。因此,如何实现复杂情况下稳健的目标跟踪成为一个值得关注的研究课题。</p>
                </div>
                <div class="p1">
                    <p id="64">根据不同的目标建模方式,目标跟踪算法分为基于生成式模型方法和基于判别式模型方法。基于生成式模型方法的思想是对目标进行建模或特征提取,在后续视频序列中进行目标模型与候选目标的相似度计算,相似度最高的候选目标即视为当前帧的跟踪目标。这种方法仅对目标进行建模,没有充分利用背景信息,在跟踪过程中存在一定的局限性,具体表现在当目标外观快速变化或存在遮挡时跟踪效果欠佳。基于判别式模型方法则同时考虑目标信息和背景信息,将跟踪视为一个二分类问题,构建一个稳健的分类器,将目标与背景有效地区分出来,从而实现目标的跟踪。相对于基于生成式模型方法,基于判别式模型方法具有更加稳健的目标跟踪效果,所以这种方法逐渐成为了目标跟踪的主流方法。</p>
                </div>
                <div class="p1">
                    <p id="65">基于相关滤波的跟踪算法是一种判别式模型方法,2010年Bolme等<citation id="203" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出一种最小输出平方误差滤波器(MOSSE)算法,将相关滤波的思想首次引入目标跟踪领域,把信号域的相关性计算应用在跟踪器中,把跟踪的问题转换为求解两个图像块相似度的问题,相似度最高的响应位置即为跟踪目标的当前位置,提高了滤波器的准确度,同时也将时域计算转换为频域计算,提高了计算的速度,使得算法的速度达到了669 frame/s。此后,基于相关滤波的跟踪成为目标跟踪的主流方向。针对相关滤波中样本数量不足的问题,Henriques等<citation id="204" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>在跟踪器中引入循环矩阵思想扩充跟踪样本,提出核函数循环结构跟踪(CSK)算法,提升了跟踪性能;Zhang等<citation id="205" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>基于跟踪过程中的时空上下文,提出了时空上下文跟踪(STC)算法,对跟踪目标上下文区域的时空关系用贝叶斯框架进行建模,得到目标和邻近区域低级特征统计相关性,通过置信图评估得到目标在新一帧中的位置,提高了跟踪的稳健性;Danelljan等<citation id="206" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>通过在跟踪算法中加入一个位置滤波器和尺度滤波器,不仅实现了目标的跟踪,还实现了对目标精准的尺度估计。在跟踪算法中,特征的选取在某些程度上对算法的优劣起着关键作用,在2014年之前,大部分算法均采用手工特征,如:颜色空间(CN)、梯度直方图(HOG)等。Wang等<citation id="207" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出的深度学习跟踪(DLT)算法是第一个将深度学习思想引入目标跟踪领域的算法,此后深度特征也开始在跟踪领域崭露头角。Danelljan等<citation id="208" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>利用深度特征进行跟踪,取得了很好的稳健性和准确性;Wang等<citation id="209" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出一种线性组合的多特征融合策略,有效提高了跟踪的效果。</p>
                </div>
                <div class="p1">
                    <p id="66">相关滤波虽然以较高的速度和精度在目标跟踪领域得到了广泛应用,但对于在运动过程中的目标易产生快速变形或者目标运动过快等情况时易导致跟踪漂移<citation id="210" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。单一特征跟踪时,不能对复杂情况下的目标进行稳健的描述,导致跟踪失败;用多特征融合策略时,不明确的特征融合方式不能较好地实现跟踪效果,也会产生较大的计算开销<citation id="211" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="67">针对以上问题,结合多特征融合相关滤波思想,以多线索相关滤波跟踪(MCCT)<citation id="212" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>算法作为基本框架,提出一种复杂情况下的自适应特征更新跟踪方法。针对手工设计特征和深度特征对目标不同侧重表征的特点,将手工设计特征和深度特征进行融合,运用不同的特征组合方式进行线性加权运算,得到不同的融合特征,对融合特征的可信度进行判定,最终选取最可靠特征作为跟踪特征。针对复杂情况,如遮挡、消失后重现、目标形变等,提出一个新的融合特征更新机制,利用目标在连续时间内的相关性,将当前帧特征与时序稳健特征进行线性加权计算,得出更新后的融合特征。</p>
                </div>
                <div class="p1">
                    <p id="68">本文的贡献主要在于采用稳健的分级深度特征选取以及融合方式,考虑不同层级深度特征的不同特点,在不同跟踪情况下提取特定的分级深度特征,并采取不同的融合方式,同时增加手工设计特征作为补充,提高定位精度;计算PSR评分和融合特征可靠性评分,将两者综合考虑以判别跟踪效果优劣,以此找到跟踪效果欠佳的视频帧,防止持续累积误差造成后续帧的性能下降;考虑时间序列上跟踪目标的稳定性,加入时序特征更新融合机制,稳健应对复杂跟踪情况。不同于直接的特征融合,本文算法充分且稳定地利用了多信息的互补性,并考虑时序上的特征稳定性,通过多特征计算得到滤波响应,用融合响应预测目标的位置。在OTB-2013和OTB-2015标准数据库上进行大量实验,实验结果验证了本算法切实可行,在复杂情况下跟踪精度有较好的提升。</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag">2 算法概述</h3>
                <div class="p1">
                    <p id="70">提出的算法主要分为5个部分:1)将给定视频序列第一帧中标定的目标候选区域位置图像块输入去掉全连接层的预训练深度特征网络VGG-19中,提取对应图像的conv3-4、conv4-4、conv5-4的深度卷积特征图,同时提取对应图像的HOG特征;2)利用线性组合方式,对提取的对应图像的conv4-4、conv5-4、HOG三种特征进行加权融合,得到7种不同融合方式的融合特征,以相关性作为融合特征质量评价指标,选出最佳融合特征;3)在进行第<i>t</i>帧跟踪时,利用<i>t</i>-1帧跟踪结果作为位置中心,确定第<i>t</i>帧跟踪的搜索框范围;4)利用最佳的融合特征与搜索范围内提取的融合特征进行相关计算,根据快速傅里叶变换进行滤波器训练和响应图计算,得到响应值最大的点,即当前帧目标的中心点,再利用尺度滤波器计算预测得到最佳跟踪框;5)计算跟踪结果的质量评价指标,判别跟踪是否存在遮挡、快速形变等复杂情况,如遇复杂情况,利用时序稳定性模型和目标语义信息对模型进行更新。算法的整体框图如图1所示。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911029_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 算法框架图" src="Detail/GetImg?filename=images/GXXB201911029_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 算法框架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911029_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Framework of proposed algorithm</p>

                </div>
                <h3 id="72" name="72" class="anchor-tag">3 复杂情况下自适应特征更新目标跟踪算法</h3>
                <h4 class="anchor-tag" id="73" name="73"><b>3.1 目标特征表征</b></h4>
                <div class="p1">
                    <p id="74">不同的特征能够从不同角度反映目标的特点,而不同的特征又各有优劣。经典的跟踪算法大都采用手工设计特征,如HOG<citation id="213" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、Color Names<citation id="214" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>等,自深度学习研究得到发展以来,通过深度神经网络提取出来的深度特征在跟踪中也得到了广泛应用。跟踪中常用VGG(Visual Geometry Group)<citation id="215" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>作为特征提取网络,深度特征和手工设计特征在跟踪中各有不同侧重点<citation id="216" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>:深度特征含有高层语义信息,但是分辨率低,在跟踪时更注重跟踪的稳健性;而手工设计特征是低层高分辨率的特征,更加强调跟踪的精度,但是在目标外观变化较大时会出现对手工设计特征的跟踪失败。在跟踪中,对当前帧的目标特征进行更新以进行下一帧的跟踪,不管是当前帧结果的稳健性还是精度,都会对特征更新产生一定影响。根据误差传播理论,每一帧的不可靠的跟踪产生的累计误差不断地迭代,均会对之后的跟踪效果产生一定的影响,所以需要在稳健性和精度之间取得一定的平衡,基于利用手工设计特征对深度特征进行补充的特征融合方法,提高了跟踪的效果。</p>
                </div>
                <div class="p1">
                    <p id="75">在特征选择方面,HOG特征能够很好地描述局部信息,以及具有几何、光学不变性,所以本文算法的手工设计特征选择HOG特征。通过计算和统计图像归一化的局部区域梯度方向直方图来构成HOG特征。随着层数的加深,深度特征的特征分辨率会越来越低,但包含的语义信息则更加丰富。本文算法中的深度特征采用的是去掉全连接层后的VGG-Net-19网络的conv3-4、conv4-4层和conv5-4层提取出来的三种分级深度特征,特征可视化图像如图2所示,图2(a)为原始图像,图2(b)为HOG特征可视化图像,图2(c)为conv3-4特征可视化图,图2(d)为conv4-4特征可视化图,图2(e)为conv5-4特征可视化图。</p>
                </div>
                <div class="p1">
                    <p id="76">在特征融合方面,正常情况下进行跟踪时,将HOG特征与conv4-4层和conv5-4层的特征以线性组合的方式进行多特征融合<citation id="217" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,通过<i>C</i><mathml id="169"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>3</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>+<i>C</i><sup>(2)</sup><sub>3</sub>+<i>C</i><mathml id="170"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>3</mn><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>=7的组合方式得到7个融合特征器,<i>C</i><mathml id="171"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>表示从n个对象中不重复地选出m个对象的所有组合方式。对7个融合特征器进行可靠性评价,选择最可靠的特征进行跟踪,特征融合形式如图3所示。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911029_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 目标特征可视化。" src="Detail/GetImg?filename=images/GXXB201911029_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 目标特征可视化。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911029_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Target-feature visualization. </p>
                                <p class="img_note">(a)原始图像;(b) HOG特征;(c) conv3-4特征;(d) conv4-4特征;(e) conv5-4特征</p>
                                <p class="img_note">(a) Original image; (b) HOG feature; (c) conv3-4 feature; 
(d) conv4-4 feature; (e) conv5-4 feature</p>

                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911029_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 特征融合示意图" src="Detail/GetImg?filename=images/GXXB201911029_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 特征融合示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911029_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Schematic of feature fusion</p>

                </div>
                <h4 class="anchor-tag" id="79" name="79"><b>3.2 基于投票的融合特征选择</b></h4>
                <div class="p1">
                    <p id="80">在跟踪过程中,7个融合特征器分别单独地对目标进行特征的提取和融合,构成相互独立的特征器以进行后续跟踪。在出现跟踪效果欠佳时,构建一个更新融合特征器进行重跟踪,所以,在正常跟踪过程中,需要从7个相互独立的融合特征器中选出1个融合特征器进行稳健的跟踪。采用投票的方法选择最优融合特征器进行目标跟踪,从而实现稳健的跟踪效果。</p>
                </div>
                <div class="p1">
                    <p id="81">鉴于7个融合特征器都是对同一跟踪目标进行的特征提取和融合,所以这7个特征融合器之间应具有高度一致性。对7个融合特征器两两进行一致性评分计算,将其中与其余6个特征器一致性最高的融合特征器作为当前帧跟踪中采用的特征器。融合特征器间一致性的计算表达式为</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>A</mi><mo stretchy="false">(</mo><mi>B</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mstyle displaystyle="true"><mo>∩</mo><mi>B</mi></mstyle><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><mrow><mi>A</mi><mo stretchy="false">(</mo><mi>B</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mstyle displaystyle="true"><mo>∪</mo><mi>B</mi></mstyle><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mfrac><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">式中:∩为求交集运算;∪为求并集运算;<i>A</i>为求得的面积;<i>E</i><sub><i>i</i></sub>为第<i>i</i>个融合特征器;<i>B</i><mathml id="172"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>为在跟踪过程的第t帧中,第i个融合特征器跟踪的结果,即跟踪目标当前结果的预测目标边框;O<mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>为在跟踪过程第t帧中,第i个融合特征器和第j个融合特征器跟踪结果的交并比。(1)式是对不同融合特征器跟踪结果进行交并比(<i>IOU</i>)计算。</p>
                </div>
                <div class="p1">
                    <p id="84">为减小O<mathml id="174"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>值的波动幅度,对O<mathml id="175"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>采用一种非线性高斯函数计算,将所有交并比的值规范在一个较小的范围内,即</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>Ο</mi><mo>′</mo></msup><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mi>exp</mi><mo stretchy="false">[</mo><mo>-</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>Ο</mi><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">]</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">式中:O′<mathml id="176"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>为对O<mathml id="177"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>进行规范化后的值。第i个融合特征器的平均一致性计算公式为</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>Κ</mi></mfrac><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></msubsup><msup><mi>Ο</mi><mo>′</mo></msup></mstyle><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">式中:M<mathml id="178"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>为在第t帧跟踪时第i个融合特征器与其他融合特征器之间的一致性评分的平均值;K为融合特征器的数量。在跟踪中,通常来说融合特征器之间的一致性评分在短时间内应具有时间稳定性。因此在一定的短周期<i>Δ</i>t内融合特征器一致性计算的值的波动程度也反映了E<sub>i</sub>与其他融合特征器之间的稳定性,可通过计算其标准差V<mathml id="179"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>进行表征,即</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>Κ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mo stretchy="false">[</mo><msup><mi>Ο</mi><mo>′</mo></msup><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mover accent="true"><mi>Ο</mi><mo stretchy="true">¯</mo></mover><mo>´</mo><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mtext>Δ</mtext><mi>t</mi><mo>+</mo><mn>1</mn><mo>∶</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">]</mo></mrow></msqrt><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">式中:<mathml id="180"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mover accent="true"><mi>Ο</mi><mo>¯</mo></mover><mo>′</mo></msup><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mtext>Δ</mtext><mi>t</mi><mo>+</mo><mn>1</mn><mo>∶</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mfrac><mn>1</mn><mrow><mtext>Δ</mtext><mi>t</mi></mrow></mfrac><mstyle displaystyle="true"><msub><mo>∑</mo><mi>τ</mi></msub><msup><mi>Ο</mi><mo>′</mo></msup></mstyle><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>E</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>τ</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>;τ为时间索引,且τ∈[t-<i>Δ</i>t+1,t]。</p>
                </div>
                <div class="p1">
                    <p id="91">为了避免融合特征器性能波动,算法进一步考虑时间稳定性并在计算中引入一个递增的权重序列W={ρ<sub>0</sub>,ρ<sub>1</sub>,…,ρ<sub><i>Δ</i></sub><sub>t-1</sub>},对于时序上越近的一致性评分给予更大的权重。此时,平均的加权均值M′<mathml id="181"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>和标准差V′<mathml id="182"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>计算公式为</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><msup><mi>Μ</mi><mo>′</mo></msup><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><msub><mo>∑</mo><mi>τ</mi></msub><mi>W</mi></mstyle><msub><mrow></mrow><mi>τ</mi></msub><mi>Μ</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><msup><mi>V</mi><mo>′</mo></msup><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><msub><mo>∑</mo><mi>τ</mi></msub><mi>W</mi></mstyle><msub><mrow></mrow><mi>τ</mi></msub><mi>V</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>τ</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">式中:W<sub>τ</sub>为权重序列W中的第τ-t+<i>Δ</i>t个权值;N为归一化因子,N=∑<sub>τ</sub>W<sub>τ</sub>。融合特征器最终的一致性评分R<mathml id="183"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>p</mtext><mtext>a</mtext><mtext>i</mtext><mtext>r</mtext></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>(E<sub>i</sub>)计算公式为</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msubsup><mrow></mrow><mrow><mtext>p</mtext><mtext>a</mtext><mtext>i</mtext><mtext>r</mtext></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mi>Μ</mi><mo>′</mo></msup><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mrow><msup><mi>V</mi><mo>′</mo></msup><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>ξ</mi></mrow></mfrac><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">式中:ξ为防止分母为0的一个极小的约束因子。R<mathml id="184"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>p</mtext><mtext>a</mtext><mtext>i</mtext><mtext>r</mtext></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>(E<sub>i</sub>)越大表示当前融合特征器与其余融合特征器之间的一致性越高,即在跟踪时具有更高的稳定性。</p>
                </div>
                <div class="p1">
                    <p id="96">此外,每个融合特征器在进行独立跟踪时,其轨迹具有连续性和平滑性,每个融合特征器的轨迹平滑度在一定程度上表明了其跟踪结果的可靠性。融合特征器的可靠性S<mathml id="185"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>可以表示为</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mi>exp</mi><mrow><mo>[</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>σ</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mfrac><mo stretchy="false">(</mo><mi>D</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>]</mo></mrow><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">式中:D<mathml id="186"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>为第t帧跟踪结果c(B<mathml id="187"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>)与第t-1帧跟踪结果c(B<mathml id="188"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>)之间的欧氏距离;σ<mathml id="189"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>为第i个融合特征器跟踪结果框的长度W(B<mathml id="190"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>)和宽度H(B<mathml id="191"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>)的平均值。D<mathml id="192"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>、σ<mathml id="193"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>的计算方法分别为</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>D</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mo stretchy="false">∥</mo><mi>c</mi><mo stretchy="false">(</mo><mi>B</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo><mo>-</mo><mi>c</mi><mo stretchy="false">(</mo><mi>B</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo><mo stretchy="false">∥</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>σ</mi><msub><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">[</mo><mi>W</mi><mo stretchy="false">(</mo><mi>B</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo><mo>+</mo><mi>Η</mi><mo stretchy="false">(</mo><mi>B</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">最后采用同特征器间一致性计算的相同方式,并考虑时间稳定性,对跟踪器的最终可靠性进行相同处理,R<mathml id="194"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>s</mtext><mtext>e</mtext><mtext>l</mtext><mtext>f</mtext></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>(E<sub>i</sub>)的计算方式为</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msubsup><mrow></mrow><mrow><mtext>s</mtext><mtext>e</mtext><mtext>l</mtext><mtext>f</mtext></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><msub><mo>∑</mo><mi>τ</mi></msub><mi>W</mi></mstyle><msub><mrow></mrow><mi>τ</mi></msub><mi>S</mi><msubsup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">(</mo><mi>τ</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">对当前帧进行跟踪时最终选取的融合特征器的投票分数是将融合特征器间的一致性和融合特征器的可靠性进行综合考虑,通过线性加权的方式计算最终投票分数,计算公式为</p>
                </div>
                <div class="p1">
                    <p id="103" class="code-formula">
                        <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>μ</mi><mi>R</mi><msubsup><mrow></mrow><mrow><mtext>p</mtext><mtext>a</mtext><mtext>i</mtext><mtext>r</mtext></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>μ</mi><mo stretchy="false">)</mo><mi>R</mi><msubsup><mrow></mrow><mrow><mtext>s</mtext><mtext>e</mtext><mtext>l</mtext><mtext>f</mtext></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="104">式中:μ为权重。在跟踪过程中,最终计算出的R<sup>(t)</sup>(E<sub>i</sub>)值最大的融合特征器是当前帧中最稳健的融合特征器,用来对当前帧进行跟踪,将其跟踪结果作为最佳跟踪结果。</p>
                </div>
                <h4 class="anchor-tag" id="105" name="105"><b>3.3 复杂情况下更新机制</b></h4>
                <div class="p1">
                    <p id="106">在跟踪过程中,将当前帧的跟踪结果作为训练数据对目标特征模型进行更新,即以当前帧标注的目标物体作为样本在下一帧跟踪过程中对跟踪目标进行特征模型更新,以进行后续跟踪。在连续的跟踪过程中,每一帧跟踪产生的漂移误差在模型更新中不断累积,这会对后续帧的跟踪效果产生一定的影响。尤其是在目标出现遮挡、短暂消失等复杂情况时,需要选取合适的特征模型进行后续跟踪以保证跟踪效果。</p>
                </div>
                <div class="p1">
                    <p id="107">在大多数DCF跟踪算法中,通常会采用峰值旁瓣比率(PSR)来衡量目标样本的可靠性<citation id="218" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,PSR(<i>P</i>)的计算方法为</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mi>R</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>m</mi></mrow><mi>σ</mi></mfrac><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">式中:<i>R</i><sub>max</sub>为融合特征器评分的最大值;<i>m</i>和<i>σ</i>分别为响应值的均值和标准差。不同特征的平均PSR(<i>P</i><mathml id="195"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>)计算式为</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mfrac><mn>1</mn><mn>3</mn></mfrac><mo stretchy="false">(</mo><mi>Ρ</mi><msubsup><mrow></mrow><mtext>Η</mtext><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>Ρ</mi><msubsup><mrow></mrow><mtext>Μ</mtext><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>Ρ</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">式中:P<mathml id="196"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mtext>Η</mtext><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>、<i>P</i><mathml id="197"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mtext>Μ</mtext><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>、<i>P</i><mathml id="198"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mtext>L</mtext><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>分别表示高级、中级和低级特征响应图的PSR。PSR的值能够反映当前样本的可靠性,值越大说明样本越可靠。</p>
                </div>
                <div class="p1">
                    <p id="112">在跟踪过程中出现遮挡或者剧烈形变时,融合特征器的投票评分<i>R</i><sup>(</sup><sup><i>t</i></sup><sup>)</sup>(<i>E</i><sub><i>i</i></sub>)会出现明显的下降,所以采用PSR和<i>R</i><sup>(</sup><sup><i>t</i></sup><sup>)</sup>(<i>E</i><sub><i>i</i></sub>)均值结合的方式来判定跟踪样本的稳定性以及当前跟踪效果的优劣,即</p>
                </div>
                <div class="p1">
                    <p id="113" class="code-formula">
                        <mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><msubsup><mrow></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>Κ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>R</mi></mstyle><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>Ρ</mi><msubsup><mrow></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⋅</mo><mi>R</mi><msubsup><mrow></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="114">式中:<i>S</i><sub><i>t</i></sub>表示当前帧跟踪结果评分。在跟踪过程中,若当前帧<i>S</i><sub><i>t</i></sub>急剧减小,可以判定当前帧的跟踪质量不佳,可能是出现了遮挡或者目标形变等问题。针对这种情况,构建了一个引入时间稳定性的目标外观模型更新模块。</p>
                </div>
                <div class="p1">
                    <p id="115">考虑到跟踪目标在连续时间序列上会出现一系列的外观变化以及受环境因素影响产生的变化,目标的改变是一个渐变的过程,在时序上存在连续性。而通常跟踪的做法是将当前帧的跟踪结果作为后续跟踪的训练样本进行更新,在出现遮挡、形变、背景复杂等跟踪质量不佳的情况时,误差较大的当前帧跟踪结果作为后续跟踪过程样本进行模型更新会对后续的跟踪产生影响,持续的低质量跟踪过程中不断累积的误差最终可能导致目标跟踪失败。为此,提出考虑时间稳定性的短时记忆特征更新策略,构建一个稳健短时记忆时序特征提取模块,在跟踪过程中,将最终评分较高的帧的特征作为保留时序特征,在跟踪质量不佳时,选取Δ<i>t</i>时序内的最佳特征作为最终时序特征。提出一个跟踪质量判别机制,当跟踪质量不佳时,对目标表征进行自适应更新:1)利用最邻近稳健短时记忆时序特征以及当前帧的最佳融合特征进行特征提取更新,对于目标形变、光照变化、消失后重现等复杂情况,只有依靠稳健时序特征以及当前帧提取特征信息才能有效地寻找到目标;2)考虑到目标变化的不确定性,融入不包含全连接层的VGG19网络conv3-4层的包含较高分辨率和较丰富语义信息的特征模型,以提高目标跟踪匹配成功率,在背景复杂、目标旋转等复杂情况时,融合更多的深层高分辨率语义信息能够提升目标跟踪效果。自适应更新计算公式表示为</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mrow><mtext>u</mtext><mtext>p</mtext><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>e</mtext></mrow></msub><mo>=</mo><mi>W</mi><msub><mrow></mrow><mn>1</mn></msub><mi>R</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>o</mtext><mtext>b</mtext><mtext>u</mtext><mtext>s</mtext><mtext>t</mtext></mrow></msub><mo>+</mo><mi>W</mi><msub><mrow></mrow><mn>2</mn></msub><mi>R</mi><msubsup><mrow></mrow><mrow><mtext>t</mtext><mtext>i</mtext><mtext>m</mtext><mtext>e</mtext></mrow><mrow><mo stretchy="false">(</mo><mtext>Δ</mtext><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>W</mi><msub><mrow></mrow><mn>3</mn></msub><mi>F</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>-</mo><mn>4</mn></mrow></msub><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">式中:<i>R</i><sub>robust</sub>为当前跟踪器提取的稳健融合特征;<i>R</i><mathml id="199"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>t</mtext><mtext>i</mtext><mtext>m</mtext><mtext>e</mtext></mrow><mrow><mo stretchy="false">(</mo><mtext>Δ</mtext><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>为时序最邻近稳健特征;F<sub><i>conv</i></sub><sub>3-4</sub>为包含全连接层的<i>VGG</i>-<i>Net</i>-19网络<i>conv</i>3-4层深度特征;W<sub>1</sub>、W<sub>2</sub>、W<sub>3</sub>为权重参数。模型更新机制如图4所示,图中R表示跟踪质量评分阈值。</p>
                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911029_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 时序模型更新机制" src="Detail/GetImg?filename=images/GXXB201911029_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 时序模型更新机制  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911029_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Update mechanism of temporal model</p>

                </div>
                <h4 class="anchor-tag" id="120" name="120"><b>3.4 算法总体流程</b></h4>
                <div class="p1">
                    <p id="121">结合以上对本文算法关键点的描述,算法总体流程如图5所示。图中,<i>P</i><sub><i>t</i></sub>表示第<i>t</i>帧时目标的位置。<i>x</i><sub><i>t</i></sub>,<i>y</i><sub><i>t</i></sub>,<i>w</i><sub><i>t</i></sub>,<i>h</i><sub><i>t</i></sub>分别代表第<i>t</i>帧时,目标位置的<i>x</i>坐标、<i>y</i>坐标、目标框的宽度、目标框的高度。</p>
                </div>
                <h3 id="122" name="122" class="anchor-tag">4 实验与分析</h3>
                <h4 class="anchor-tag" id="123" name="123"><b>4.1 实验环境</b></h4>
                <div class="p1">
                    <p id="124">实验平台采用MATLAB2016a和C++在Matconvnet上的深度学习库混合编程,硬件环境为:CPU:Intel(R) Core(TM) i7-8700,3.20 GHz;16 GB内存;GPU:NVIDIA GeForce 1080Ti。</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911029_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 算法流程" src="Detail/GetImg?filename=images/GXXB201911029_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911029_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Flow chart of algorithm</p>

                </div>
                <h4 class="anchor-tag" id="127" name="127"><b>4.2 评价指标</b></h4>
                <div class="p1">
                    <p id="128">在OTB-2013<citation id="219" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>和OTB-2015<citation id="220" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>数据库上进行大量测试,利用一次性通过评价(OPE)标准来分析算法性能,测试数据库涵盖11个属性:背景杂波、快速运动、平面内旋转、平面外旋转、运动模糊、尺度变化、光照变化、低分辨率、遮挡、超出视线范围、形变。每段序列可能有多个属性,背景属性相对复杂,以准确率、成功率作为评价算法性能的指标。为了更好地评价算法性能,将本文算法(ours)同时与近年来比较流行的MCCT<citation id="221" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、ECO<citation id="222" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、CF2<citation id="223" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、SRDCF<citation id="224" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、Staple<citation id="225" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、ADNet<citation id="226" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、KCF<citation id="227" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、STRCF<citation id="228" type="reference"><link href="58" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、LCT<citation id="229" type="reference"><link href="60" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>算法进行对比。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129"><b>4.3 定性分析</b></h4>
                <div class="p1">
                    <p id="130">图6是10种跟踪算法在几个视频序列上的部分跟踪结果,视频从上至下、从左至右分别是Soccer(第1,132,277,380帧)、CarScale(第1,211,214,240帧)、Bolt2(第1,49,75,99帧)、MotorRolling(第1,67,91,126帧)、Skiing(第1,30,49,72帧)、Ironman(第1,26,35,108帧),通过对10个算法在不同视频序列中的跟踪结果进行对比分析,可以发现本文算法的跟踪质量和稳健性比较理想。</p>
                </div>
                <h4 class="anchor-tag" id="131" name="131">4.3.1 遮挡、背景复杂</h4>
                <div class="p1">
                    <p id="132">以“Soccer”视频序列为例,目标在运动过程中多次受到奖杯和周围人群的遮挡,并且目标与背景环境极其相似,多数算法都出现了漂移现象甚至跟踪失败,这是由跟踪器在出现遮挡、背景复杂时学习到背景信息导致的。而本文算法在复杂情况时,判别目标不全依赖当前学习到的特征,而是融入时序稳健特征,可以更好地对目标进行表征。如图6第1行所示,只有本文算法和MCCT算法能够较好地跟踪到目标。</p>
                </div>
                <h4 class="anchor-tag" id="133" name="133">4.3.2 尺度变化</h4>
                <div class="p1">
                    <p id="134">以“CarScale”视频序列为例,目标车辆离镜头由远及近,使得目标在视频序列中尺度发生较大变化,如图6第2行所示,虽然大多数算法都能跟上目标,但只有本文算法能够更好地进行目标的定位以及尺度估计。</p>
                </div>
                <h4 class="anchor-tag" id="135" name="135">4.3.3 快速运动、形变</h4>
                <div class="p1">
                    <p id="136">以“Bolt2”视频序列为例,视频是短跑比赛场景,目标在视频序列内快速运动,自身也在持续发生形变,如图6第3行所示。其他算法在跟踪过程中多次出现跟踪漂移和跟踪失败现象,本文算法充分考虑时序信息,对目标的变化实时更新,只有本文算法能够稳定地对目标进行跟踪。</p>
                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911029_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 10种跟踪算法在部分视频序列上的定性结果显示" src="Detail/GetImg?filename=images/GXXB201911029_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 10种跟踪算法在部分视频序列上的定性结果显示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911029_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Qualitative results of 10 tracking algorithms for some video sequences</p>

                </div>
                <h4 class="anchor-tag" id="138" name="138">4.3.4 目标平面内/外旋转</h4>
                <div class="p1">
                    <p id="139">以“MotorRolling”视频序列为例,目标摩托车在运动过程中多次出现各种旋转,本文算法采用多特征融合方式,对目标具有更强的表征能力。如图6第4行所示,除本文算法和ADNet算法能够对目标进行稳健跟踪外,其他算法都出现了跟踪失败现象,但本文算法在目标尺度估计上更加精确。</p>
                </div>
                <h4 class="anchor-tag" id="140" name="140">4.3.5 低分辨率</h4>
                <div class="p1">
                    <p id="141">以“Skiing”视频序列为例,该视频序列分辨率较低,本文算法充分考虑特征语义信息,增强了目标的表征能力。如图6第5行所示,在视频第21帧时,除本文算法和CF2算法外,其余算法都出现了跟踪失败现象。</p>
                </div>
                <h4 class="anchor-tag" id="142" name="142">4.3.6 光照变化</h4>
                <div class="p1">
                    <p id="143">以“Ironman”视频序列为例,在跟踪过程中,视频序列多次出现强烈的光照变化,本文算法加入实时更新机制,在目标出现较大变化时能够及时对其进行特征更新。如图6第6行所示,除本文算法和CF2、MCCT算法外,其余算法都不能成功跟踪到目标。</p>
                </div>
                <div class="p1">
                    <p id="144">为验证本文融合特征器的有效性,以“Soccer”视频序列为例,对每一帧中融合特征器的选取情况进行统计分析,统计结果如表1所示。</p>
                </div>
                <div class="area_img" id="145">
                    <p class="img_tit">表1 特征器频次统计 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Frequency statistics of feature experts</p>
                    <p class="img_note"></p>
                    <table id="145" border="1"><tr><td><br />Expert</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td></tr><tr><td><br />Frequency</td><td>18</td><td>54</td><td>18</td><td>80</td><td>77</td><td>85</td><td>25</td><td>34</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="146">表中Expert 1～7对应表示7个不同融合特征器,Expert 8表示自适应更新特征,Frequency表示在整个跟踪过程中,选取当前融合特征器结果作为最终跟踪结果的视频帧数量。在同一帧跟踪中,不同融合特征器的跟踪结果可视化如图7所示。图中由左至右、由上至下分别为“Soccer”视频序列中第48帧、第60帧、第67帧和第191帧,不同颜色的虚线矩形框表示不同的特征器结果,最终采用的跟踪结果由对应特征器颜色的实线框表示。由图7可以看出在第48帧中,选取了Expert 5的跟踪结果作为最终结果;第60帧中选取了自适应更新特征的跟踪结果作为最终结果;第67帧中选取了Expert 2的跟踪结果作为最终结果;第191帧中选取了Expert 7的跟踪结果作为最终结果。</p>
                </div>
                <div class="area_img" id="147">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911029_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同特征器跟踪结果" src="Detail/GetImg?filename=images/GXXB201911029_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同特征器跟踪结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911029_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Tracking results of different feature experts</p>

                </div>
                <h4 class="anchor-tag" id="149" name="149"><b>4.4 定量分析</b></h4>
                <div class="p1">
                    <p id="150">为进一步全面地评价本文算法,采用跟踪精度和跟踪成功率对算法进行定量分析,图8是本文算法在OTB-2013和OTB-2015测试数据库上的精度曲线和成功率曲线。</p>
                </div>
                <div class="area_img" id="151">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911029_151.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 在OTB-2013和OTB-2015数据库下,算法跟踪精度和跟踪成功率。" src="Detail/GetImg?filename=images/GXXB201911029_151.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 在OTB-2013和OTB-2015数据库下,算法跟踪精度和跟踪成功率。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911029_151.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Tracking accuracy and success rate of algorithm on OTB-2013 and OTB-2015 databases. </p>
                                <p class="img_note">(a) OTB-2013跟踪精度;(b) OTB-2013
跟踪成功率;(c) OTB-2015跟踪精度;(d) OTB-2015跟踪成功率</p>
                                <p class="img_note">(a) Tracking accuracy on OTB-2013 database; (b) tracking success rate on OTB-2013 database; (c) tracking accuracy on OTB-2015 database; (d) tracking success rate on OTB-2015 database</p>

                </div>
                <div class="p1">
                    <p id="153">图8(a)和(b)分别为8种跟踪算法在OTB-2013数据库上的跟踪精度曲线图和跟踪成功率曲线图,可以看出本文算法不管是跟踪精度还是跟踪成功率都取得了最优效果;图8(c)和图8(d)分别为8种跟踪算法在OTB-2015数据库上的跟踪精度曲线图和跟踪成功率曲线图,可以看出本文算法的跟踪精度和跟踪成功率仍然取得了最优效果。</p>
                </div>
                <div class="p1">
                    <p id="154">为更直观地评估本文算法的效果,对算法在OTB-2013及OTB-2015测试数据库上的跟踪精度和跟踪成功率以及跟踪速率的数据进行分析,如表2所示。由数据分析可知,在OTB-2013测试数据库下,本文算法的跟踪精度最高,达到了90.2%,相比排名第二的CF2算法精度提高了1.1%,相比于STRCF算法精度提高了1.3%;在成功率方面,本文算法的跟踪成功率也最高,达到了87.6%,相比于排名第二的MCCT算法成功率提升了1.3%,相比于CF2算法成功率提升了6.7%。在OTB-2015测试数据库下,本文算法的跟踪精度最高,达到了87.1%,相比于排名第二的STRDCF算法精度提高了0.7%,相比于MCCT算法精度提高了1.1%,相比于CF2算法精度提高了2.6%;在成功率方面,本文算法的跟踪成功率也最高,达到了82.9%,相比于STRDCF成功率提高了2.9%,相比于MCCT算法成功率提升了1.1%,相比于CF2算法成功率提升了7.8%。在跟踪速率方面,更高的跟踪精度与跟踪成功率是以计算量的增加为代价的,给跟踪速率带来一定的影响。本文算法跟踪速率为3.9 frame/s,高于CF2算法的1.5 frame/s,仍然满足实时性要求。</p>
                </div>
                <div class="p1">
                    <p id="155">为了更进一步地分析本文算法在不同跟踪条件下的跟踪性能,图9和图10分别给出了算法在OTB-2013测试数据库下11种不同属性条件下的跟踪精度和成功率。</p>
                </div>
                <div class="area_img" id="156">
                    <p class="img_tit">表2 算法在OTB-2013和OTB2015上精度、成功率值和速率 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Tracking accuracy, success rate, and speed of algorithm on OTB-2013 and OTB2015 databases</p>
                    <p class="img_note"></p>
                    <table id="156" border="1"><tr><td>Database</td><td>Parameter</td><td>Ours</td><td>STRCF</td><td>MCCT</td><td>CF2</td><td>ECO</td><td>SRDCF</td><td>ADNet</td><td>Staple</td><td>KCF</td><td>LCT</td></tr><tr><td><br />OTB-2013</td><td>Precision</td><td>0.902</td><td>0.889</td><td>0.883</td><td>0.891</td><td>0.855</td><td>0.838</td><td>0.798</td><td>0.782</td><td>0.740</td><td>0.848</td></tr><tr><td><br /></td><td>Success rate</td><td>0.876</td><td>0.845</td><td>0.863</td><td>0.809</td><td>0.806</td><td>0.789</td><td>0.721</td><td>0.738</td><td>0.623</td><td>0.738</td></tr><tr><td><br />OTB-2015</td><td>Precision</td><td>0.871</td><td>0.864</td><td>0.860</td><td>0.845</td><td>0.836</td><td>0.788</td><td>0.772</td><td>0.784</td><td>0.696</td><td>0.762</td></tr><tr><td><br /></td><td>Success rate</td><td>0.829</td><td>0.800</td><td>0.818</td><td>0.751</td><td>0.772</td><td>0.730</td><td>0.700</td><td>0.699</td><td>0.526</td><td>0.629</td></tr><tr><td colspan="2">Average FPS</td><td>3.9</td><td>28.0</td><td>4.2</td><td>1.5</td><td>54.8</td><td>8.0</td><td>12.3</td><td>97.6</td><td>349.3</td><td>29.4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="158">在OTB-2013测试数据库下,测试11种不同属性视频序列的结果显示,本文算法的跟踪精度始终处于最优或次优水平,跟踪精度除了在low resolution、background clutter、deformation中处于次优,在其他8个属性中均得到最优成绩。本文算法的跟踪成功率在11种不同属性视频序列上的测试结果始终保持最优。</p>
                </div>
                <div class="p1">
                    <p id="159">图11、图12分别给出了算法在OTB-2015测试数据库下11种不同属性条件下的跟踪精度和成功率。在OTB-2015测试数据库下,测试11种不同属性视频序列的结果显示,本文算法的跟踪精度始终处于最优或次优水平,跟踪精度除了在low resolution、deformation、scale variation中处于次优,在其他8个属性中均得到最优成绩。本文算法的跟踪成功率在11种不同属性视频序列上的测试结果始终保持最优。</p>
                </div>
                <div class="p1">
                    <p id="160">对于长时跟踪情况,本文算法不管是跟踪成功率还是跟踪精度在8种对比算法中仍然处于最优。在加入自适应特征更新模块后,复杂情况下的低质量跟踪效果得到提升,减小了当前帧的跟踪误差。由于跟踪时下一帧的跟踪是以当前帧跟踪结果作为初始跟踪状态,所以本文算法在减少当前帧跟踪误差的同时,减小了模型的累积误差,能够提高后续帧跟踪的精度和成功率。本文算法的跟踪精度达到了0.901,跟踪成功率达到了0.866。图13为本文算法与加入长时跟踪效果较好的算法LCT<citation id="230" type="reference"><link href="60" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>作为对比的算法对OTB数据库中较长视频序列中的跟踪精度及跟踪成功率。</p>
                </div>
                <div class="p1">
                    <p id="161">OTB-2013、OTB-2015测试数据库上的跟踪结果显示,本文算法在11种不同属性视频序列上的成功率和精度均有较好的提升,平均跟踪速率为3.9 frame/s,与近年来比较流行的深度学习跟踪算法MCCT、CF2等相当。同时,本文算法对于长时跟踪情况同样有较好的提升,能够实现复杂情况下的稳健的跟踪。</p>
                </div>
                <div class="area_img" id="162">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911029_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 在OTB-2013下11种不同属性视频序列跟踪精度。" src="Detail/GetImg?filename=images/GXXB201911029_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 在OTB-2013下11种不同属性视频序列跟踪精度。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911029_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Tracking precision of 11 different attribute video sequences on OTB-2013 database. </p>
                                <p class="img_note">(a)背景杂波;(b)形变;(c)快速运动;(d)平面内旋转;
(e)光照变换;(f)低分辨率;(g)运动模糊;(h)遮挡;(i)平面外旋转;(j)超出视线范围;(k)尺度变化</p>
                                <p class="img_note">(a) Background clutter; (b) deformation; (c) fast motion; (d) in-plane rotation; (e) illumination variation; (f) low resolution; (g) motion blur; (h) occlusion; (i) out-of-plane rotation; (j) out of view; (k) scale variation</p>

                </div>
                <div class="area_img" id="163">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911029_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 在OTB-2013下11种不同属性视频序列跟踪成功率。" src="Detail/GetImg?filename=images/GXXB201911029_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 在OTB-2013下11种不同属性视频序列跟踪成功率。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911029_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Tracking success rates of 11 different attribute video sequences on OTB-2013 database. </p>
                                <p class="img_note">(a)背景杂波;(b)形变;(c)快速运动;(d)平面内旋转;
(e)光照变换;(f)低分辨率;(g)运动模糊;(h)遮挡;(i)平面外旋转;(j)超出视线范围;(k)尺度变化</p>
                                <p class="img_note">(a) Background clutter; (b) deformation; (c) fast motion; (d) in-plane rotation; (e) illumination variation; (f) low resolution; (g) motion blur; (h) occlusion; (i) out-of-plane rotation; (j) out of view; (k) scale variation</p>

                </div>
                <div class="area_img" id="164">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911029_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 TB-2015下11种不同属性视频序列跟踪精度。" src="Detail/GetImg?filename=images/GXXB201911029_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 TB-2015下11种不同属性视频序列跟踪精度。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911029_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Tracking precision of 11 different attribute video sequences on OTB-2015 database. </p>
                                <p class="img_note">(a)背景杂波;(b)形变;(c)快速运动;(d)平面内旋转;(e)光照变换;
(f)低分辨率;(g)运动模糊;(h)遮挡;(i)平面外旋转;(j)超出视线范围;(k)尺度变化</p>
                                <p class="img_note">(a) Background clutter; (b) deformation; (c) fast motion; (d) in-plane rotation; (e) illumination variation; (f) low resolution; (g) motion blur; (h) occlusion; (i) out-of-plane rotation; (j) out of view; (k) scale variation</p>

                </div>
                <div class="area_img" id="165">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911029_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 OTB-2015下11种不同属性视频序列跟踪成功率。" src="Detail/GetImg?filename=images/GXXB201911029_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 OTB-2015下11种不同属性视频序列跟踪成功率。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911029_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Tracking success rates of 11 different attribute video sequences on OTB-2015 database.</p>
                                <p class="img_note">(a)背景杂波;(b)形变;(c)快速运动;(d)平面内旋转;
(e)光照变换;(f)低分辨率;(g)运动模糊;(h)遮挡;(i)平面外旋转;(j)超出视线范围;(k)尺度变化</p>
                                <p class="img_note"> (a) Background clutter; (b) deformation; (c) fast motion; (d) in-plane rotation; (e) illumination variation; (f) low resolution; (g) motion blur; (h) occlusion; (i) out-of-plane rotation; (j) out of view; (k) scale variation</p>

                </div>
                <div class="area_img" id="166">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911029_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 长时跟踪情况下算法跟踪精度和成功率。" src="Detail/GetImg?filename=images/GXXB201911029_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 长时跟踪情况下算法跟踪精度和成功率。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911029_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 13 Tracking precision and success rate of algorithm under long-term tracking. </p>
                                <p class="img_note">(a)跟踪精度;(b)跟踪成功率</p>
                                <p class="img_note">(a) Tracking precision;
(b) tracking success rate</p>

                </div>
                <h3 id="167" name="167" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="168">提出一种复杂情况下自适应特征更新目标跟踪算法,其主要思想是充分利用不同层级深度特征的丰富语义信息以及手工设计特征定位精度较高的优势,采取线性组合的多特征融合手段得到多个融合特征器,通过置信度评估选择最优融合特征;在目标跟踪阶段,通过可靠性投票计算判别当前跟踪效果,在跟踪效果不佳时采取自适应特征更新方法,从而获得稳健的跟踪效果。上述实验表明,与近年来主流算法相比,本文算法在各种复杂的情况下,有效地提高了目标跟踪的精确性和稳健性,能够较好地适应不同的跟踪环境,获得较高质量的跟踪效果。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast online object tracking and segmentation:a unifying approach">

                                <b>[1]</b> Wang Q,Zhang L,Bertinetto L,<i>et al</i>.Fast online object tracking and segmentation:a unifying approach[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 16-20,2019,Long Beach,CA,USA.New York:IEEE,2019:1328-1338.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201801008&amp;v=MTc2ODhaZVZ2Rnl2Z1ZyM0xLRDdZYkxHNEg5bk1ybzlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Lu H C,Li P X,Wang D.Visual object tracking:a survey[J].Pattern Recognition and Artificial Intelligence,2018,31(1):61-76.卢湖川,李佩霞,王栋.目标跟踪算法综述[J].模式识别与人工智能,2018,31(1):61-76.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DBZX201903009&amp;v=MDQ4Mjg2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXZnVnIzTElTL1Jkckc0SDlqTXJJOUZiWVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Li J L,Yin K,Chu C X,<i>et al</i>.Review of video target tracking technology[J].Journal of Yanshan University,2019,43(3):251-262.李均利,尹宽,储诚曦,等.视频目标跟踪技术综述[J].燕山大学学报,2019,43(3):251-262.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">

                                <b>[4]</b> Bolme D S,Beveridge J R,Draper B A,<i>et al</i>.Visual object tracking using adaptive correlation filters[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,June 13-18,2010,San Francisco,CA,USA.New York:IEEE,2010:2544-2550.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">

                                <b>[5]</b> Henriques J F,Caseiro R,Martins P,<i>et al</i>.Exploiting the circulant structure of tracking-by-detection with kernels[M]//Fitzgibbon A,Lazebnik S,Perona P,<i>et al</i>.Computer vision-ECCV 2012.Lecture notes in computer science.Berlin,Heidelberg:Springer,2012,7575:702-715.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast visual tracking via dense spatio-temporal context learning">

                                <b>[6]</b> Zhang K H,Zhang L,Liu Q S,<i>et al</i>.Fast visual tracking via dense spatio-temporal context learning[M]//Fleet D,Pajdla T,Schiele B,<i>et al</i>.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8693:127-141.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate scale estimation for robust visual tracking">

                                <b>[7]</b> Danelljan M,Häger G,Khan F S,<i>et al</i>.Accurate scale estimation for robust visual tracking[C]//Proceedings of the British Machine Vision Conference 2014,September 1-5,2014,University of Nottingham,UK.UK:BMVA Press,2014.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a Deep Compact Image Representation for Visual Tracking">

                                <b>[8]</b> Wang N Y,Yeung D Y.Learning a deep compact image representation for visual tracking[C]//NIPS′13 Proceedings of the 26th International Conference on Neural Information Processing Systems,December 5-10,2013,Lake Tahoe,Nevada.New York:ACM,2013,1:809-817.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond Correlation Filters:Learning Continuous Convolution Operators for Visual Tracking">

                                <b>[9]</b> Danelljan M,Robinson A,Khan F S,<i>et al</i>.Beyond correlation filters:learning continuous convolution operators for visual tracking[M]//Leibe B,Matas J,Sebe N,<i>et al</i>.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9909:472-488.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-cue correlation filters for robust visual tracking">

                                <b>[10]</b> Wang N,Zhou W G,Tian Q,<i>et al</i>.Multi-cue correlation filters for robust visual tracking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:4844-4853.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201705021&amp;v=MjMwNjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXZnVnIzTElqWFRiTEc0SDliTXFvOUhaWVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Shen Q,Yan X L,Liu L F,<i>et al</i>.Multi-scale correlation filtering tracker based on adaptive feature selection[J].Acta Optica Sinica,2017,37(5):0515001.沈秋,严小乐,刘霖枫,等.基于自适应特征选择的多尺度相关滤波跟踪[J].光学学报,2017,37(5):0515001.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811025&amp;v=MTM1MDZHNEg5bk5ybzlIWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl2Z1ZyM0xJalhUYkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Ge B Y,Zuo X Z,Hu Y J.Long-term object tracking based on feature fusion[J].Acta Optica Sinica,2018,38(11):1115002.葛宝义,左宪章,胡永江.基于特征融合的长时目标跟踪算法[J].光学学报,2018,38(11):1115002.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">

                                <b>[13]</b> Dalal N,Triggs B.Histograms of oriented gradients for human detection[C]∥2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR′05),June 20-25,2005,San Diego,CA,USA.New York:IEEE,2005:8588935.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive color attributes for real-time visual tracking">

                                <b>[14]</b> Danelljan M,Khan F S,Felsberg M,<i>et al</i>.Adaptive color attributes for real-time visual tracking[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:1090-1097.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[15]</b> Simonyan K,Zissermanl A.Very deep convolutional networks for large-scale image recognition[J/OL].(2015-04-10)[2019-05-30].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unveiling the power of deep tracking">

                                <b>[16]</b> Bhat G,Johnander J,Danelljan M,<i>et al</i>.Unveiling the power of deep tracking[M]//Ferrari V,Hebert M,Sminchisescu C,<i>et al</i>.Computer vision-ECCV 2018.Lecture notes in computer science.Cham:Springer,2018,11206:493-509.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online object tracking:A benchmark">

                                <b>[17]</b> Wu Y,Lim J,Yang M H.Online object tracking:a benchmark[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition,June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:2411-2418.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">

                                <b>[18]</b> Wu Y,Lim J,Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(9):1834-1848.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ECO:Efficient Convolution Operators for Tracking">

                                <b>[19]</b> Danelljan M,Bhat G,Khan F S,<i>et al</i>.ECO:efficient convolution operators for tracking[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:6931-6939.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical Convolutional Features for Visual Tracking">

                                <b>[20]</b> Ma C,Huang J B,Yang X K,<i>et al</i>.Hierarchical convolutional features for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:3074-3082.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_21" >
                                    <b>[21]</b>
                                 Danelljan M,Hager G,Khan F S,<i>et al</i>.Learning spatially regularized correlation filters for visual tracking[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:4310-4318.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Staple:complementary learners for real-time tracking">

                                <b>[22]</b> Bertinetto L,Valmadre J,Golodetz S,<i>et al</i>.Staple:complementary learners for real-time tracking[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1401-1409.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning">

                                <b>[23]</b> Yun S,Choi J,Yoo Y,<i>et al</i>.Action-decision networks for visual tracking with deep reinforcement learning[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI,USA.New York:IEEE,2017:1349-1358.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">

                                <b>[24]</b> Henriques J F,Caseiro R,Martins P,<i>et al</i>.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(3):583-596.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatial-temporal regularized correlation filters for visual tracking">

                                <b>[25]</b> Li F,Tian C,Zuo W M,<i>et al</i>.Learning spatial-temporal regularized correlation filters for visual tracking[C]∥2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT,USA.New York:IEEE,2018:4904-4913.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long-term correlation tracking">

                                <b>[26]</b> Ma C,Yang X K,Zhang C Y,<i>et al</i>.Long-term correlation tracking[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:5388-5396.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201911029" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201911029&amp;v=Mjk5OTBWdkZ5dmdWcjNJSWpYVGJMRzRIOWpOcm85SGJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

