

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133091800440000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dGXXB201910026%26RESULT%3d1%26SIGN%3dweBPMY9SoJXJW0TSmtwRW8DKIWQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201910026&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201910026&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201910026&amp;v=MjAwNjdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2RnlybVZML0FJalhUYkxHNEg5ak5yNDlIWW9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#54" data-title="2 基于卷积自编码器和残差块的图像融合方法 ">2 基于卷积自编码器和残差块的图像融合方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="&lt;b&gt;2.1 残差块&lt;/b&gt;"><b>2.1 残差块</b></a></li>
                                                <li><a href="#59" data-title="&lt;b&gt;2.2 总体思路框架&lt;/b&gt;"><b>2.2 总体思路框架</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;2.3 改进的基于L1-norm的相似度融合策略&lt;/b&gt;"><b>2.3 改进的基于L1-norm的相似度融合策略</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;2.4 损失函数设计&lt;/b&gt;"><b>2.4 损失函数设计</b></a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;2.5 训练框架&lt;/b&gt;"><b>2.5 训练框架</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#124" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#125" data-title="&lt;b&gt;3.1 实验设置&lt;/b&gt;"><b>3.1 实验设置</b></a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;3.2 主观视觉评价&lt;/b&gt;"><b>3.2 主观视觉评价</b></a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;3.3 客观评价&lt;/b&gt;"><b>3.3 客观评价</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#147" data-title="4 结  论 ">4 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="图1 残差块的结构">图1 残差块的结构</a></li>
                                                <li><a href="#62" data-title="图2 所提出的网络架构">图2 所提出的网络架构</a></li>
                                                <li><a href="#79" data-title="图3 融合策略图">图3 融合策略图</a></li>
                                                <li><a href="#120" data-title="图4 训练过程中不同损失函数的下降曲线图">图4 训练过程中不同损失函数的下降曲线图</a></li>
                                                <li><a href="#122" data-title="图5 训练流程框架">图5 训练流程框架</a></li>
                                                <li><a href="#138" data-title="图6 第一组图像融合对比实验。">图6 第一组图像融合对比实验。</a></li>
                                                <li><a href="#139" data-title="图7 第二组图像融合对比实验。">图7 第二组图像融合对比实验。</a></li>
                                                <li><a href="#140" data-title="图8 第三组图像融合对比实验。">图8 第三组图像融合对比实验。</a></li>
                                                <li><a href="#143" data-title="表1 第一组对比实验的客观评价结果">表1 第一组对比实验的客观评价结果</a></li>
                                                <li><a href="#144" data-title="表2 第二组对比实验的客观评价结果">表2 第二组对比实验的客观评价结果</a></li>
                                                <li><a href="#145" data-title="表3 第三组对比实验的客观评价结果">表3 第三组对比实验的客观评价结果</a></li>
                                                <li><a href="#146" data-title="图9 部分图像融合对比实验结果。">图9 部分图像融合对比实验结果。</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="206">


                                    <a id="bibliography_1" title=" Jiang Z T,Wu H,Zhou X L.Infrared and visible image fusion algorithm based on improved guided filtering and dual-channel spiking cortical model[J].Acta Optica Sinica,2018,38(2):0210002.江泽涛,吴辉,周哓玲.基于改进引导滤波和双通道脉冲发放皮层模型的红外与可见光图像融合算法[J].光学学报,2018,38(2):0210002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201802015&amp;v=MjIxNzZxQnRHRnJDVVJMT2VaZVZ2RnlybVZML0FJalhUYkxHNEg5bk1yWTlFWVlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Jiang Z T,Wu H,Zhou X L.Infrared and visible image fusion algorithm based on improved guided filtering and dual-channel spiking cortical model[J].Acta Optica Sinica,2018,38(2):0210002.江泽涛,吴辉,周哓玲.基于改进引导滤波和双通道脉冲发放皮层模型的红外与可见光图像融合算法[J].光学学报,2018,38(2):0210002.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_2" title=" Ma J Y,Ma Y,Li C.Infrared and visible image fusion methods and applications:a survey[J].Information Fusion,2019,45:153-178." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF1AA21F05383F7D583B754F0E37B5D1D&amp;v=MTI2OTlPZmNXNWI2RE9ydmxGWWVnSER3byt1eE1iNlUxNlRYdVVyR2MyZnNDUk1idnJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkbGh4cnU0d0tBPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Ma J Y,Ma Y,Li C.Infrared and visible image fusion methods and applications:a survey[J].Information Fusion,2019,45:153-178.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_3" title=" Liu X H,Chen Z B.Fusion of infrared and visible images based on multi-scale directional guided filter and convolutional sparse representation[J].Acta Optica Sinica,2017,37(11):1110004.刘先红,陈志斌.基于多尺度方向引导滤波和卷积稀疏表示的红外与可见光图像融合[J].光学学报,2017,37(11):1110004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201711013&amp;v=MDAxMzJybzlFWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2RnlybVZML0FJalhUYkxHNEg5Yk4=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Liu X H,Chen Z B.Fusion of infrared and visible images based on multi-scale directional guided filter and convolutional sparse representation[J].Acta Optica Sinica,2017,37(11):1110004.刘先红,陈志斌.基于多尺度方向引导滤波和卷积稀疏表示的红外与可见光图像融合[J].光学学报,2017,37(11):1110004.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_4" title=" Lin S Z,Han Z.Images fusion based on deep stack convolutional neural network[J].Chinese Journal of Computers,2017,40(11):2506-2518.蔺素珍,韩泽.基于深度堆叠卷积神经网络的图像融合[J].计算机学报,2017,40(11):2506-2518." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201711006&amp;v=MDA4NzZxQnRHRnJDVVJMT2VaZVZ2RnlybVZML0FMejdCZHJHNEg5Yk5ybzlGWW9RS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Lin S Z,Han Z.Images fusion based on deep stack convolutional neural network[J].Chinese Journal of Computers,2017,40(11):2506-2518.蔺素珍,韩泽.基于深度堆叠卷积神经网络的图像融合[J].计算机学报,2017,40(11):2506-2518.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_5" title=" Li H,Wu X J,Kittler J.Infrared and visible image fusion using a deep learning framework[C]//2018 24th International Conference on Pattern Recognition (ICPR),August 20-24,2018,Beijing,China.New York:IEEE,2018:2705-2710." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Infrared and visible image fusion using a deep learning framework">
                                        <b>[5]</b>
                                         Li H,Wu X J,Kittler J.Infrared and visible image fusion using a deep learning framework[C]//2018 24th International Conference on Pattern Recognition (ICPR),August 20-24,2018,Beijing,China.New York:IEEE,2018:2705-2710.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_6" title=" Liu Y,Chen X,Peng H,et al.Multi-focus image fusion with a deep convolutional neural network[J].Information Fusion,2017,36:191-207." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES427C64D5871F65335C5537AD8A37BD74&amp;v=MTU5MzNPd09lbm84ekJVV21UcDRTM2lUMkJwRWVyWG1NYjJiQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxoeHJ1NHdLQT1OaWZPZmJlNkdhTEtxL3RBYg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Liu Y,Chen X,Peng H,et al.Multi-focus image fusion with a deep convolutional neural network[J].Information Fusion,2017,36:191-207.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_7" title=" Prabhakar K R,Sai Srikar V,Babu R V.DeepFuse:a deep unsupervised approach for exposure fusion with extreme exposure image pairs[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice,Italy.New York:IEEE,2017:4724-4732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepFuse:A Deep Unsupervised Approach for Exposure Fusion with Extreme Exposure Image Pairs">
                                        <b>[7]</b>
                                         Prabhakar K R,Sai Srikar V,Babu R V.DeepFuse:a deep unsupervised approach for exposure fusion with extreme exposure image pairs[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice,Italy.New York:IEEE,2017:4724-4732.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                     He K M,Zhang X Y,Ren S Q,et al.Deep residual learning for image recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:770-778.</a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_9" title=" Lu Y S,Li Y X,Liu B,et al.Hyperspectral data haze monitoring based on deep residual network[J].Acta Optica Sinica,2017,37(11):1128001.陆永帅,李元祥,刘波,等.基于深度残差网络的高光谱遥感数据霾监测[J].光学学报,2017,37(11):1128001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201711037&amp;v=MTk0MjBGckNVUkxPZVplVnZGeXJtVkwvQUlqWFRiTEc0SDliTnJvOUdZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Lu Y S,Li Y X,Liu B,et al.Hyperspectral data haze monitoring based on deep residual network[J].Acta Optica Sinica,2017,37(11):1128001.陆永帅,李元祥,刘波,等.基于深度残差网络的高光谱遥感数据霾监测[J].光学学报,2017,37(11):1128001.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_10" title=" Liu Y,Chen X,Ward R K,et al.Image fusion with convolutional sparse representation[J].IEEE Signal Processing Letters,2016,23(12):1882-1886." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image fusion with convolutional sparse representation">
                                        <b>[10]</b>
                                         Liu Y,Chen X,Ward R K,et al.Image fusion with convolutional sparse representation[J].IEEE Signal Processing Letters,2016,23(12):1882-1886.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_11" title=" Zhao H,Gallo O,Frosio I,et al.Loss functions for image restoration with neural networks[J].IEEE Transactions on Computational Imaging,2017,3(1):47-57." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Loss functions for image restoration with neural networks">
                                        <b>[11]</b>
                                         Zhao H,Gallo O,Frosio I,et al.Loss functions for image restoration with neural networks[J].IEEE Transactions on Computational Imaging,2017,3(1):47-57.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_12" title=" Dong C,Loy C C,He K M,et al.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(2):295-307." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using deep convolutional networks">
                                        <b>[12]</b>
                                         Dong C,Loy C C,He K M,et al.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(2):295-307.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_13" title=" Huang R,Zhang S,Li T Y,et al.Beyond face rotation:global and local perception GAN for photorealistic and identity preserving frontal view synthesis[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice,Italy.New York:IEEE,2017:2458-2467." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond face rotation:Global and local perception gan for photorealistic and identity preserving frontal view synthesis">
                                        <b>[13]</b>
                                         Huang R,Zhang S,Li T Y,et al.Beyond face rotation:global and local perception GAN for photorealistic and identity preserving frontal view synthesis[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice,Italy.New York:IEEE,2017:2458-2467.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_14" title=" Tao L,Zhu C,Xiang G Q,et al.LLCNN:a convolutional neural network for low-light image enhancement[C]∥2017 IEEE Visual Communications and Image Processing (VCIP),December 10-13,2017,St.Petersburg,FL,USA.New York:IEEE,2017:17614346." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LLCNN a convolutional neural network for low-light image enhancement">
                                        <b>[14]</b>
                                         Tao L,Zhu C,Xiang G Q,et al.LLCNN:a convolutional neural network for low-light image enhancement[C]∥2017 IEEE Visual Communications and Image Processing (VCIP),December 10-13,2017,St.Petersburg,FL,USA.New York:IEEE,2017:17614346.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_15" title=" Liu Y,Chen X,Cheng J,et al.Infrared and visible image fusion with convolutional neural networks[J].International Journal of Wavelets,Multiresolution and Information Processing,2018,16(3):1850018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Infrared and visible image fusion with convolutional neural networks">
                                        <b>[15]</b>
                                         Liu Y,Chen X,Cheng J,et al.Infrared and visible image fusion with convolutional neural networks[J].International Journal of Wavelets,Multiresolution and Information Processing,2018,16(3):1850018.
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_16" title=" Ma J L,Zhou Z Q,Wang B,et al.Infrared and visible image fusion based on visual saliency map and weighted least square optimization[J].Infrared Physics &amp;amp; Technology,2017,82:8-17." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF5DBE0F083A76FEA02629927F5B5D7E3&amp;v=MDM3NzJOaWZPZmNXOWFxTzVyL2xGYk9oK0MzcFB1bWNUNkRsL1FYYmdxMlF3QzdmZ1FzK2NDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkbGh4cnU0d0tBPQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         Ma J L,Zhou Z Q,Wang B,et al.Infrared and visible image fusion based on visual saliency map and weighted least square optimization[J].Infrared Physics &amp;amp; Technology,2017,82:8-17.
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_17" title=" Zhang Y,Zhang L J,Bai X Z,et al.Infrared and visual image fusion through infrared feature extraction and visual information preservation[J].Infrared Physics &amp;amp; Technology,2017,83:227-237." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES9430CCB0989C35D95CF68177E5B5C3D8&amp;v=MjA3NTVXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRsaHhydTR3S0E9TmlmT2ZicThIZEcvM1AxRmJlTUdmMzg4dXg4V21VbDdRSDdscTJjd0M3Zm5SczZYQ09OdkZTaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         Zhang Y,Zhang L J,Bai X Z,et al.Infrared and visual image fusion through infrared feature extraction and visual information preservation[J].Infrared Physics &amp;amp; Technology,2017,83:227-237.
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_18" title=" Chen M S.Image fusion of visual and infrared image based on NSCT and compressed sensing[J].Journal of Image and Graphics,2016,21(1):39-44.陈木生.结合NSCT和压缩感知的红外与可见光图像融合[J].中国图象图形学报,2016,21(1):39-44." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201601005&amp;v=MTYwMjVMRzRIOWZNcm85RllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5cm1WTC9BUHlyZmI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Chen M S.Image fusion of visual and infrared image based on NSCT and compressed sensing[J].Journal of Image and Graphics,2016,21(1):39-44.陈木生.结合NSCT和压缩感知的红外与可见光图像融合[J].中国图象图形学报,2016,21(1):39-44.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-06-12 14:27</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(10),218-226 DOI:10.3788/AOS201939.1015001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于卷积自编码器和残差块的红外与可见光图像融合方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="javascript:;">江泽涛</a>
                                <a href="javascript:;">何玉婷</a>
                </h2>
                    <h2>

                    <span>桂林电子科技大学广西图像图形与处理智能处理重点实验室</span>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了在红外与可见光图像融合中充分利用中间层提取的信息,防止信息过度丢失,提出一种新的基于卷积自编码器和残差块的图像融合方法。该方法采用由编码器、融合层和解码器三部分组成的网络结构。将残差网络引入编码器中,将红外与可见光图像分别送入编码器后,通过卷积层和残差块来获取图像的特征图;将得到的特征图采用改进的基于L1-norm的相似度融合策略进行融合,并将其整合为一个包含源图像显著特征的特征图;重新设计损失函数,利用解码器对融合后的图像进行重构。实验结果表明,与其他融合方法相比,该方法有效地提取并保留了源图像的深层信息,融合结果在主观和客观评价中都有着一定的优势。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">可见光图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">红外图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AE%8B%E5%B7%AE%E5%9D%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">残差块;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积自编码器;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *何玉婷,E-mail:839191881@qq.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-23</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61572147,61876049,61762066);</span>
                                <span>广西科技计划(AC16380108);</span>
                                <span>广西图像图形与智能处理重点实验项目(GIIP201701,GIIP201801,GIIP201802,GIIP201803);</span>
                                <span>广西研究生教育创新计划(2019YCXS043);</span>
                    </p>
            </div>
                    <h1><b>Infrared and Visible Image Fusion Method Based on Convolutional Auto-Encoder and Residual Block</b></h1>
                    <h2>
                    <span>Jiang Zetao</span>
                    <span>He Yuting</span>
            </h2>
                    <h2>
                    <span>Guangxi Key Laboratory of Image and Graphic Intelligent Processing, Guilin University of Electronic Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to make full use of the information extracted from the middle layer and prevent information from losing excessively, a new image fusion method based on a convolutional auto-encoder and a residual block is proposed, which is composed of an encoder, a fusion layer, and a decoder. First, the residual network is introduced into the encoder, the infrared and visible images are fed into the encoder, and the convolution layer and residual block are used to obtain the feature map of the image. Then, the obtained feature map is fused by using an improved fusion strategy based on L1-norm similarity, which is integrated into a feature map containing the salient features of the source image. Finally, the loss function is redesigned and the decoder is used to reconstruct the fused image. The experimental results show that compared with other fusion methods, the method effectively extracts and preserves the deep information of the source image, which makes the fusion result have certain advantages in both subjective and objective evaluation.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visible%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visible image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=infrared%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">infrared image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=residual%20block&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">residual block;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20auto-encoder&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional auto-encoder;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-04-23</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="50">红外与可见光图像的融合是图像处理领域中的重要应用之一,它尝试从源图像中提取出特征信息,然后通过适当的方法将这些特征信息融合到单个图像中<citation id="242" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,使得融合图像兼具红外与可见光图像的显著特征,从而可以提供更多的目标与背景信息,因此将红外与可见光图像进行融合的方法在目标识别、视频监控、军事应用等领域有着广阔的应用前景。</p>
                </div>
                <div class="p1">
                    <p id="51">图像融合的关键问题是如何从源图像中提取出特征,并将其结合起来生成融合后的图像<citation id="243" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。许多信号处理方法都可用于图像融合中,以提取图像的特征,如多尺度分解、稀疏表示等。基于多尺度分解的方法,首先采用图像分解的方法来提取图像特征信息,然后采用适当的融合策略来获得最终的融合图像<citation id="244" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。基于稀疏表示的方法通过构造冗余字典对图像进行稀疏编码并提取源图像特征,再根据一定的规则融合稀疏表示系数,然后通过字典和融合的稀疏表示系数重构融合图像。这些融合方法虽然能够取得良好的融合效果,但在提取特征时,仍会丢失有用的信息,不能较好地保留源图像的信息。</p>
                </div>
                <div class="p1">
                    <p id="52">深度学习现已广泛应用于图像融合任务中,比经典方法实现了更好的融合性能和时间效率。基于深度学习的算法<citation id="245" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>使用大量的图像对网络进行预训练,使得训练后的网络可以获得图像的显著特征。其中,卷积神经网络(CNN)可用于提取图像特征和重构融合后的图像。在基于CNN的融合方法中<citation id="246" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,若只使用最后一层的结果作为图像的特征信息,将会丢失中间层获得的许多深层信息,从而影响最终融合结果。文献<citation id="247" type="reference">[<a class="sup">6</a>]</citation>提出了一种基于CNN的多聚焦图像融合方法,输入图像是不同模糊位置的图像块,用于训练网络并获得决策图,然后使用决策图和源图像获得融合后的图像,但是该方法仅适用于多聚焦图像融合。文献<citation id="248" type="reference">[<a class="sup">7</a>]</citation>提出一种基于CNN的多曝光图像融合方法,采用Siamese网络结构,其权重是相互关联的。首先将两个输入图像送入编码器中,得到两个特征映射序列,用加法策略进行融合,融合后的特征图由解码器的三层卷积层进行重构。该方法虽然取得了较好的性能,但仍存在两个主要缺点:1)网络架构过于简单,无法提取出深层特征;2)只使用了编码网络中最后一层的结果,中间层会丢失许多特征信息,网络越深,特征信息丢失的现象越严重。</p>
                </div>
                <div class="p1">
                    <p id="53">基于上述问题,本文提出一种新的基于卷积自编码器和残差块的网络结构,用于红外图像与可见光图像融合。利用编码器来提取图像特征,利用解码器重构融合后的图像。编码器由卷积层和残差块构成,可以保存更多来自中间层的有用信息,并且易于训练。利用改进的基于L1-norm的相似度融合策略和包含4个卷积层的解码器对融合后的图像进行重构。将本文方法与其他融合方法的进行对比,验证本文算法在保留中间层信息方面具有一定的优势。</p>
                </div>
                <h3 id="54" name="54" class="anchor-tag">2 基于卷积自编码器和残差块的图像融合方法</h3>
                <h4 class="anchor-tag" id="55" name="55"><b>2.1 残差块</b></h4>
                <div class="p1">
                    <p id="56">在传统的CNN中,随着网络深度增加,网络的退化问题也随之暴露,即中间层提取的信息并没有得到充分的利用。为了解决退化问题,He等<citation id="249" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出深度残差学习框架,通过快捷连接和残差表示,直接将输入信息绕道传至输出,从而保护信息的完整性。该网络更容易优化,并通过增加深度提高了准确率。残差块的结构如图1所示。</p>
                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910026_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 残差块的结构" src="Detail/GetImg?filename=images/GXXB201910026_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 残差块的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910026_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Architecture of residual block</p>

                </div>
                <div class="p1">
                    <p id="58">残差块是在传统的线性网络结构基础上通过跳跃连接来实现的。一般地,通过直接求和操作可实现特征图的跳跃连接<citation id="250" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>,本文改用拼接操作。经过卷积操作后,网络会保留一些特征信息,但也会丢失一些信息,而丢失的这些信息可能是有用的信息,因此通过加入原始输入,使网络拥有更多的选择,从而使得丢失的信息得到了保留。为此,在编码器中加入残差块,使得网络可以保存更多来自中间层的特征信息,并且易于训练。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59"><b>2.2 总体思路框架</b></h4>
                <div class="p1">
                    <p id="60">将红外与可见光图像分别定义为<i><b>X</b></i><sub>IR</sub>和<i><b>X</b></i><sub>VIS</sub>。该网络结构由三部分组成:编码器、融合层和解码器。编码器包含三个部分——C1、ResidualBlock、C2,用于提取图像特征。其中,ResidualBlock部分包含两个卷积层。编码器中每个卷积操作的滤波器大小和步长分别是3×3和1,输入图像的大小不受限制。残差块可以尽可能地保留更多的特征,确保深层特征用于融合层。融合层的输出作为解码器的输入。解码器包含4个卷积层。所提出的网络框架如图2所示。</p>
                </div>
                <div class="p1">
                    <p id="61">将<i><b>X</b></i><sub>IR</sub>和<i><b>X</b></i><sub>VIS</sub>分别送入编码器中。假设有<i>m</i>个卷积核,每个卷积核由卷积矩阵<i><b>a</b></i><sup>(</sup><sup><i>m</i></sup><sup>)</sup>和偏置<i><b>b</b></i><sup>(</sup><sup><i>m</i></sup><sup>)</sup>参数组成,激活函数为<i>δ</i>,将输入图像<i><b>X</b></i><sub><i>k</i></sub>进行卷积后生成<i>m</i>个特征图<i><b>A</b></i>,则有</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910026_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 所提出的网络架构" src="Detail/GetImg?filename=images/GXXB201910026_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 所提出的网络架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910026_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Network architecture of proposed method</p>

                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mi>δ</mi><mo stretchy="false">[</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>k</mi></msub><mo>*</mo><mi mathvariant="bold-italic">a</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">式中:<i>m</i>表示上一层的输出通道数目,<i>m</i>∈{1,2,…,<i>M</i>},<i>M</i>=64代表特征图的数量;<i><b>A</b></i><sup>(</sup><sup><i>m</i></sup><sup>)</sup><sub><i>k</i></sub>表示由编码器得到的特征图,<i>k</i>∈{1,2},1、2分别表示红外图像与可见光图像;*表示卷积。</p>
                </div>
                <div class="p1">
                    <p id="66">将生成的特征图送进ResidualBlock中, 经过第一层线性变化并激活后的输出<i>F</i>为</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo>=</mo><mi>W</mi><msub><mrow></mrow><mn>2</mn></msub><mi>δ</mi><mo stretchy="false">[</mo><mi>W</mi><msub><mrow></mrow><mn>1</mn></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">]</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">式中:<i>W</i><sub>1</sub>为第一层的权重参数;<i>W</i><sub>2</sub>为第二层的权重参数。</p>
                </div>
                <div class="p1">
                    <p id="69">通过一个快捷连接和第二个非线性函数ReLU,获得输出<i>y</i>,表达式为</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo>=</mo><mi>F</mi><mo stretchy="false">{</mo><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mo stretchy="false">{</mo><mi>W</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">}</mo><mo stretchy="false">}</mo><mo>+</mo><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">式中:<i>W</i><sub><i>t</i></sub>为权重参数,<i>t</i>∈{1,2}</p>
                </div>
                <div class="p1">
                    <p id="72">将由编码器得到的红外图像特征图<i><b>A</b></i><sup>(</sup><sup><i>m</i></sup><sup>)</sup><sub>1</sub>和可见光图像特征图<i><b>A</b></i><sup>(</sup><sup><i>m</i></sup><sup>)</sup><sub>2</sub>送入融合层,通过改进的基于L1-norm的相似度融合策略进行融合,得到融合特征图<i><b>f</b></i><sup>(</sup><sup><i>m</i></sup><sup>)</sup>。</p>
                </div>
                <div class="p1">
                    <p id="73">最后将融合特征图<i><b>f</b></i><sup>(</sup><sup><i>m</i></sup><sup>)</sup>送入解码器中,该部分由4个卷积层组成。将每张融合特征图<i><b>f</b></i>与其对应的卷积核的转置<mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow></math></mathml>进行卷积操作并对结果进行求和,再加上偏置<i><b>c</b></i>,即得到特征重构之后的结果:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub><mo>=</mo><mi>δ</mi><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><mo>∑</mo><mi mathvariant="bold-italic">f</mi></mstyle><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo>*</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>¯</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi mathvariant="bold-italic">c</mi></mrow><mo>]</mo></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">要更新权值,需要确定损失函数,该操作是在训练框架部分完成的,即将输入的图像与最终利用特征重构出来的结果进行比较,通过反向传播算法进行优化,就可以得到一个完整的训练框架。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76"><b>2.3 改进的基于L1-norm的相似度融合策略</b></h4>
                <div class="p1">
                    <p id="77">在测试阶段,将红外图像与可见光图像分别送入编码器。当编码器和解码器的权值固定时,使用融合策略来融合编码器得到的特征。参照文献<citation id="251" type="reference">[<a class="sup">10</a>]</citation>,提出改进的基于L1-norm的相似度融合策略来融合从编码器得到的特征图。融合策略如图3所示。</p>
                </div>
                <div class="p1">
                    <p id="78">首先,由编码器得到的特征图<i><b>A</b></i><sup>(</sup><sup><i>m</i></sup><sup>)</sup><sub><i>k</i></sub>的L1-norm获得初始活动水平图<i>C</i><sub><i>k</i></sub>,通过基于窗口的平均算子来计算得到最终的活动水平图<mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>C</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>;然后,对活动水平图<mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>C</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>进行<i>softmax</i>函数计算,得到初始权重图w<sub>k</sub>,计算初始活动水平图C<sub>1</sub>和C<sub>2</sub>之间的相似度S;最终,根据相似度S选择融合规则,得到融合特征图<i><b>f</b></i><sup>(</sup><sup><i>m</i></sup><sup>)</sup>。详细步骤如下。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910026_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 融合策略图" src="Detail/GetImg?filename=images/GXXB201910026_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 融合策略图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910026_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Diagram of fusion strategy</p>

                </div>
                <div class="p1">
                    <p id="81">1) 采用<i><b>A</b></i><sup>1:</sup><sup><i>M</i></sup><sub><i>k</i></sub>(<i>i</i>,<i>j</i>)的L1-norm作为源图像的活动水平测量,从而获得初始活动水平图<i>C</i><sub><i>k</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mn>1</mn><mo>:</mo><mi>Μ</mi></mrow></msubsup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">式中:<i><b>A</b></i><sup>1:</sup><sup><i>M</i></sup><sub><i>k</i></sub>(<i>i</i>,<i>j</i>)是一个<i>M</i>维的向量,表示特征图中位置(<i>i</i>,<i>j</i>)处的<i><b>A</b></i><sup>(</sup><sup><i>m</i></sup><sup>)</sup><sub><i>k</i></sub>;(<i>i</i>,<i>j</i>)代表位置坐标。</p>
                </div>
                <div class="p1">
                    <p id="84">2) 使用基于窗口的平均算子来计算最终的活动水平图<mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>C</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>,以使融合方法对错误配准具有稳健性。<mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>C</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>可表示为</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>C</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>a</mi><mo>=</mo><mo>-</mo><mi>r</mi></mrow><mi>r</mi></munderover><mspace width="0.25em" /></mstyle><mspace width="0.25em" /><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>b</mi><mo>=</mo><mo>-</mo><mi>r</mi></mrow><mi>r</mi></munderover><mi>C</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>+</mo><mi>a</mi><mo>,</mo><mi>j</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><mrow><mrow><mo stretchy="false">(</mo><mn>2</mn><mi>r</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">式中:a为窗口横坐标取值;b为窗口纵坐标取值;r代表窗口大小。r决定块大小:当r值较大时,能考虑到图像的整体信息,但忽略了边缘细节信息,因此使用较大的r有着更强的稳健性,但会丢失一些细节,从而导致融合效果的细节信息比较模糊;当r值过小时,如选取r=0,由于未考虑到图像的整体信息,忽略了周围特征信息的相关性,因此稳健性较差。在红外与可见光图像的融合中,由于小尺度细节的存在,采用较小的r效果更好,经过验证,本研究选取r=1。</p>
                </div>
                <div class="p1">
                    <p id="87">3) 根据得到的活动水平图<mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>C</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>,通过soft-max函数计算得到初始权重图<i>w</i><sub><i>k</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mover accent="true"><mi>C</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mover accent="true"><mi>C</mi><mo>¯</mo></mover></mstyle><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">式中:<i>n</i>为<i>k</i>的变量取值,<i>k</i>的定义与(1)式相同。</p>
                </div>
                <div class="p1">
                    <p id="90">4) 通过初始活动水平图<i>C</i><sub><i>k</i></sub>来计算初始活动水平图<i>C</i><sub>1</sub>和<i>C</i><sub>2</sub>之间的相似度<i>S</i>:</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mn>2</mn><msqrt><mrow><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msqrt></mrow><mrow><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>+</mo><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">5) 根据得到的相似度<i>S</i>来选择融合规则,从而得到融合之后的特征图<i><b>f</b></i><sup>(</sup><sup><i>m</i></sup><sup>)</sup>。</p>
                </div>
                <div class="p1">
                    <p id="93">当<i>S</i>(<i>i</i>,<i>j</i>)≥<i>T</i>(<i>T</i>为阈值)时,有</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">f</mi><msup><mrow></mrow><mi>m</mi></msup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>+</mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">式中:<mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mover accent="true"><mi>C</mi><mo>¯</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mrow><mover accent="true"><mi>C</mi><mo>¯</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>+</mo><mover accent="true"><mi>C</mi><mo>¯</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></math></mathml>;<mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mover accent="true"><mi>C</mi><mo>¯</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mrow><mover accent="true"><mi>C</mi><mo>¯</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>+</mo><mover accent="true"><mi>C</mi><mo>¯</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="96">当S(x,y)&lt;T时,有</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">f</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>,</mo></mtd><mtd><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>≥</mo><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>,</mo></mtd><mtd><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>&lt;</mo><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">经过验证,T=0.8时融合效果最好。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>2.4 损失函数设计</b></h4>
                <div class="p1">
                    <p id="100">为了更精确地重构输入图像,选择最小化损失函数<i>L</i>来训练编码器和解码器。损失函数可用来估计网络的输入与输出之间的差异<citation id="252" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。常用的损失函数是均方误差,但其不能很好地学习到图像的目标细节信息,会导致输出图像产生模糊。在此基础上,引入结构损失和梯度损失,以更好地保留纹理信息和边缘信息。综合考虑图像的结构信息、边缘信息,以及内容信息之间的差异,最后的损失函数<i>L</i>为</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mi>L</mi><msub><mrow></mrow><mtext>p</mtext></msub><mo>+</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>r</mtext><mtext>a</mtext><mtext>d</mtext></mrow></msub><mo>+</mo><mi>λ</mi><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>s</mtext><mtext>i</mtext><mtext>m</mtext></mrow></msub><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">式中:<i>L</i><sub>p</sub>为均方误差损失;<i>L</i><sub>grad</sub>为梯度损失;<i>L</i><sub>ssim</sub>为结构相似性损失;<i>λ</i>为比例系数。</p>
                </div>
                <div class="p1">
                    <p id="103">因为均方误差损失和梯度损失与结构损失有3个数量级的差异,所以在训练阶段将<i>λ</i>分别设置为1,10,100,1000,发现<i>λ</i>=10时效果最好。</p>
                </div>
                <div class="p1">
                    <p id="104">均方误差损失<i>L</i><sub>p</sub>通过计算网络训练过程中输出图像与输入图像之间的差值,使得图像内容保持一致性<citation id="253" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,表达式为</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mtext>p</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><mo>∑</mo><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">Ο</mi><mo>-</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">式中:<i><b>O</b></i>,<i><b>I</b></i>分别代表输出和输入图像;<i>N</i>为训练样本的个数。</p>
                </div>
                <div class="p1">
                    <p id="107">梯度损失<i>L</i><sub>grad</sub>是引入Sobel算子来计算输出图像和输入图像的梯度,通过二者之间的差值来优化图像的边缘<citation id="254" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>,表达式为</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>r</mtext><mtext>a</mtext><mtext>d</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><mo>∑</mo><mo stretchy="false">|</mo></mstyle><mi mathvariant="bold-italic">G</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ο</mi></msub><mo>-</mo><mi mathvariant="bold-italic">G</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ι</mi></msub><mo stretchy="false">|</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">式中:<i>G</i><sub><i><b>O</b></i></sub>、<i>G</i><sub><i><b>I</b></i></sub>分别为输出图像和输入图像的梯度,</p>
                </div>
                <div class="area_img" id="110">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201910026_11000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="112">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201910026_11200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="113"><i>G</i><sub><i><b>O</b></i></sub>、<i>G</i><sub><i><b>I</b></i></sub>分别用来检测水平和垂直方向的边缘。</p>
                </div>
                <div class="p1">
                    <p id="114">结构损失是通过结构相似性损失<i>L</i><sub>ssim</sub>来评估的,它表示两幅图像之间的相似度。</p>
                </div>
                <div class="p1">
                    <p id="115" class="code-formula">
                        <mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>s</mtext><mtext>i</mtext><mtext>m</mtext></mrow></msub><mo>=</mo><mn>1</mn><mo>-</mo><mi>f</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>S</mtext><mtext>Ι</mtext><mtext>Μ</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Ο</mi><mo>,</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>S</mtext><mtext>Ι</mtext><mtext>Μ</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Ο</mi><mo>,</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mn>2</mn><mi>μ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ο</mi></msub><mi>μ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ι</mi></msub><mo>+</mo><mi>D</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mi>μ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Ο</mi><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Ι</mi><mn>2</mn></msubsup><mo>+</mo><mi>D</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mn>2</mn><mi>σ</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">Ο</mi><mi mathvariant="bold-italic">Ι</mi></mrow></msub><mo>+</mo><mi>D</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mrow><mi>σ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Ο</mi><mn>2</mn></msubsup><mo>+</mo><mi>σ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Ι</mi><mn>2</mn></msubsup><mo>+</mo><mi>D</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="116">式中:<i>f</i><sub>SSIM</sub>(·)<citation id="255" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>表示两幅图像之间的相似度;<i>μ</i><sub><i><b>O</b></i></sub>、<i>μ</i><sub><i><b>I</b></i></sub>分别是输出图像和输入图像的平均值;<i>σ</i><sup>2</sup><sub><i><b>O</b></i></sub>、σ<sup>2</sup><sub><i><b>I</b></i></sub>分别是输出图像和输入图像的方差;σ<sub><i><b>OI</b></i></sub>是输出图像和输入图像的协方差;<i>D</i><sub>1</sub>、<i>D</i><sub>2</sub>是用来维持稳定的常数。</p>
                </div>
                <div class="p1">
                    <p id="117">为了证明该损失函数的有效性,将该损失函数与传统的损失函数均方误差进行对比,给出训练过程中不同损失函数的下降曲线,如图4所示。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118"><b>2.5 训练框架</b></h4>
                <div class="p1">
                    <p id="119">在训练过程,仅考虑编码部分和解码部分,尝试通过训练编码器和解码器来重构输入图像。该训练策略的明显优势在于,可以为特定的任务设计合适的融合层,因此还具有进一步发展融合层的空间。该网络的详细训练框架如图5所示。</p>
                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910026_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 训练过程中不同损失函数的下降曲线图" src="Detail/GetImg?filename=images/GXXB201910026_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 训练过程中不同损失函数的下降曲线图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910026_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Decline curves of different loss functions 
in training process</p>

                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910026_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 训练流程框架" src="Detail/GetImg?filename=images/GXXB201910026_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 训练流程框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910026_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Framework of training process</p>

                </div>
                <div class="p1">
                    <p id="123">C1、C2是编码器中的卷积层,包含3×3的滤波器。DC1、DC2是残差块中的卷积层。解码器由C3、C4、C5和C6组成,用于重建输入图像。为了更精确地重建输入图像,选取最小化损失函数<i>L</i>来训练编码器和解码器。</p>
                </div>
                <h3 id="124" name="124" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="125" name="125"><b>3.1 实验设置</b></h4>
                <div class="p1">
                    <p id="126">在训练阶段,使用MS-COCO数据集中的可见光图像来训练编码器和解码器的权重。随机选取4×10<sup>4</sup>张图像作为输入图像,并将其转换成256 pixel×256 pixel的灰度图像。在每一次迭代中用450张图像来验证网络的重构能力。</p>
                </div>
                <div class="p1">
                    <p id="127">使用tensorflow搭建实验环境,并使用GTX1080Ti GPU进行训练。在网络训练过程中,优化器为Adam,学习速率为0.0001,激活函数是ReLu,Batch Size为8,Epoch为20。</p>
                </div>
                <div class="p1">
                    <p id="128">为了验证本文方法的性能,从20对测试图像中选取3组红外与可见光图像,与文献<citation id="256" type="reference">[<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</citation>的方法进行对比。</p>
                </div>
                <div class="p1">
                    <p id="129">从主观视觉效果和客观评价标准两方面验证各方法的图像融合效果。</p>
                </div>
                <h4 class="anchor-tag" id="130" name="130"><b>3.2 主观视觉评价</b></h4>
                <div class="p1">
                    <p id="131">4组图像的实验结果如图6～8所示。图6～8中的(a)均为红外图像;(b)均为可见光图像;(c)均为基于CNN融合方法的融合结果;(d)均为基于视觉显著性图和加权最小二乘优化的融合方法的融合结果;(e)均为基于红外特征提取与视觉信息保存的融合方法的融合结果;(f)均为NSCT(Nonsubsampled Contourlet变换)和压缩感知的融合方法的融合结果;(g)均为本文方法的融合结果。为了更直观地比较图像之间的差异,对图中的融合区域作标注。</p>
                </div>
                <div class="p1">
                    <p id="132">对比图6中屏幕下的桌椅、地灯、行人以及电杆标志可以明显看出,图6(c)～(f)中这些地方比较模糊,存在噪声,有些甚至出现失真现象;而本文算法[图6(g)]具有较好的视觉效果,可以清晰地识别出屏幕下的桌椅、电杆标志等。</p>
                </div>
                <div class="p1">
                    <p id="133">从图7可以看出,图7(c)中地面出现黑色阴影区域,未能保留出可见光图像的背景信息;图7(d)～(f)地面偏暗,两边草丛模糊;而本文算法[图7(g)]亮度均匀,能清晰地看出地面和两边的草丛,较好地保留了背景信息。</p>
                </div>
                <div class="p1">
                    <p id="134">从图8可以看出,图8(c)～(f)中车前方玻璃出现黑色区域,房子周围的树木模糊,部分区域出现失真现象;本文算法[图8(g)]可以清晰地看到玻璃和树木的纹理信息,充分融合了可见光图像的背景信息,相比其他方法具有明显优势。</p>
                </div>
                <div class="p1">
                    <p id="135">受篇幅限制,为了进一步验证本文方法的优越性,图9仅展示部分融合结果。</p>
                </div>
                <h4 class="anchor-tag" id="136" name="136"><b>3.3 客观评价</b></h4>
                <div class="p1">
                    <p id="137">为了进一步对上述不同融合方法所得的实验结果进行客观评价,选取信息熵(IE)、标准差(SD)、平均梯度(AG)和互信息(MI)这4个指标进行客观评价,结果如表1～3所示。</p>
                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910026_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 第一组图像融合对比实验。" src="Detail/GetImg?filename=images/GXXB201910026_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 第一组图像融合对比实验。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910026_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 The first group of image fusion contrast experiments. </p>
                                <p class="img_note">(a)红外图像;(b)可见光图像;(c)文献<citation id="257" type="reference"><link href="234" rel="bibliography" />[15]</citation>方法;(d)文献<citation id="258" type="reference"><link href="236" rel="bibliography" />[16]</citation>方法;
(e)文献<citation id="259" type="reference"><link href="238" rel="bibliography" />[17]</citation>方法;(f)文献<citation id="260" type="reference"><link href="240" rel="bibliography" />[18]</citation>方法;(g)本文方法</p>
                                <p class="img_note">(a) Infrared image; (b) visible image; (c) method in 
Ref. [15]; (d) method in Ref. <citation id="261" type="reference"><link href="236" rel="bibliography" />[16]</citation>; (e) method in Ref. <citation id="262" type="reference"><link href="238" rel="bibliography" />[17]</citation>; (f) method in Ref. <citation id="263" type="reference"><link href="240" rel="bibliography" />[18]</citation>; (g) proposed method</p>

                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910026_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 第二组图像融合对比实验。" src="Detail/GetImg?filename=images/GXXB201910026_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 第二组图像融合对比实验。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910026_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 The second group of image fusion contrast experiments. </p>
                                <p class="img_note">(a)红外图像;(b)可见光图像;(c)文献<citation id="264" type="reference"><link href="234" rel="bibliography" />[15]</citation>方法;(d)文献<citation id="265" type="reference"><link href="236" rel="bibliography" />[16]</citation>方法;
(e)文献<citation id="266" type="reference"><link href="238" rel="bibliography" />[17]</citation>方法;(f)文献<citation id="267" type="reference"><link href="240" rel="bibliography" />[18]</citation>方法;(g)本文方法</p>
                                <p class="img_note">(a) Infrared image; (b) visible image; (c) method in 
Ref. [15]; (d) method in Ref. <citation id="268" type="reference"><link href="236" rel="bibliography" />[16]</citation>; (e) method in Ref. <citation id="269" type="reference"><link href="238" rel="bibliography" />[17]</citation>; (f) method in Ref. <citation id="270" type="reference"><link href="240" rel="bibliography" />[18]</citation>; (g) proposed method</p>

                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910026_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 第三组图像融合对比实验。" src="Detail/GetImg?filename=images/GXXB201910026_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 第三组图像融合对比实验。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910026_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 The third group of image fusion contrast experiments. </p>
                                <p class="img_note">(a)红外图像;(b)可见光图像;(c)文献<citation id="271" type="reference"><link href="234" rel="bibliography" />[15]</citation>方法;(d)文献<citation id="272" type="reference"><link href="236" rel="bibliography" />[16]</citation>方法;
(e)文献<citation id="273" type="reference"><link href="238" rel="bibliography" />[17]</citation>方法;(f)文献<citation id="274" type="reference"><link href="240" rel="bibliography" />[18]</citation>方法;(g)本文方法</p>
                                <p class="img_note">(a) Infrared image; (b) visible image; (c) method in 
Ref. [15]; (d) method in Ref. <citation id="275" type="reference"><link href="236" rel="bibliography" />[16]</citation>; (e) method in Ref. <citation id="276" type="reference"><link href="238" rel="bibliography" />[17]</citation>; (f) method in Ref. <citation id="277" type="reference"><link href="240" rel="bibliography" />[18]</citation>; (g) proposed method</p>

                </div>
                <div class="p1">
                    <p id="142">从表1～3可以看出,本文方法在这4项评价指标上明显优于其他融合算法。虽然本文方法在表1的IE指标小于文献<citation id="278" type="reference">[<a class="sup">18</a>]</citation>中的方法,在表3中的IE指标小于文献<citation id="279" type="reference">[<a class="sup">15</a>]</citation>的方法,但在其他指标上均有明显优势。</p>
                </div>
                <div class="area_img" id="143">
                    <p class="img_tit">表1 第一组对比实验的客观评价结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Objective evaluation indicators of the first group of comparative experimental results</p>
                    <p class="img_note"></p>
                    <table id="143" border="1"><tr><td>Fusion method</td><td>IE</td><td>SD</td><td>AG</td><td>MI</td></tr><tr><td><br />Method in Ref. [15]</td><td>6.7072</td><td>37.049</td><td>3.5297</td><td>22.3850</td></tr><tr><td><br />Method in Ref. [16]</td><td>6.1381</td><td>32.436</td><td>3.3973</td><td>30.7770</td></tr><tr><td><br />Method in Ref. [17]</td><td>6.6984</td><td>33.590</td><td>3.3490</td><td>22.7620</td></tr><tr><td><br />Method in Ref. [18]</td><td><b>6.7510</b></td><td>35.192</td><td>3.4359</td><td>23.1432</td></tr><tr><td><br />Proposed method</td><td>6.6411</td><td><b>38.270</b></td><td><b>4.1455</b></td><td><b>31.8710</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="144">
                    <p class="img_tit">表2 第二组对比实验的客观评价结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Objective evaluation indicators of the second group of comparative experimental results</p>
                    <p class="img_note"></p>
                    <table id="144" border="1"><tr><td>Fusion method</td><td>IE</td><td>SD</td><td>AG</td><td>MI</td></tr><tr><td><br />Method in Ref. [15]</td><td>7.1000</td><td>53.673</td><td>3.8040</td><td>15.6260</td></tr><tr><td><br />Method in Ref. [16]</td><td>6.9540</td><td>49.513</td><td>4.2530</td><td>16.5480</td></tr><tr><td><br />Method in Ref. [17]</td><td>6.9710</td><td>52.781</td><td>3.7172</td><td>19.3090</td></tr><tr><td><br />Method in Ref. [18]</td><td>7.1947</td><td>51.664</td><td>4.4496</td><td>13.0329</td></tr><tr><td><br />Proposed method</td><td><b>7.4754</b></td><td><b>58.703</b></td><td><b>4.4842</b></td><td><b>24.5260</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="145">
                    <p class="img_tit">表3 第三组对比实验的客观评价结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Objective evaluation indicators of the third group of comparative experimental results</p>
                    <p class="img_note"></p>
                    <table id="145" border="1"><tr><td>Fusion method</td><td>IE</td><td>SD</td><td>AG</td><td>MI</td></tr><tr><td><br />Method in Ref. [15]</td><td><b>7.4114</b></td><td>39.817</td><td>3.0124</td><td>17.6580</td></tr><tr><td><br />Method in Ref. [16]</td><td>7.1459</td><td>35.916</td><td>3.6467</td><td>17.6800</td></tr><tr><td><br />Method in Ref. [17]</td><td>7.0764</td><td>34.879</td><td>3.1716</td><td>16.9430</td></tr><tr><td><br />Method in Ref. [18]</td><td>7.1703</td><td>37.727</td><td>3.3523</td><td>21.7977</td></tr><tr><td><br />Proposed method</td><td>7.3484</td><td><b>41.758</b></td><td><b>3.8451</b></td><td><b>27.3920</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="146">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201910026_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 部分图像融合对比实验结果。" src="Detail/GetImg?filename=images/GXXB201910026_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 部分图像融合对比实验结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201910026_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Partial results of image fusion contrast experiments. </p>
                                <p class="img_note">(a1)～(a6)红外图像;(b1)～(b6)可见光图像;(c1)～(c6)文献<citation id="280" type="reference"><link href="234" rel="bibliography" />[15]</citation>方法;
(d1)～(d6)文献<citation id="281" type="reference"><link href="236" rel="bibliography" />[16]</citation>方法;(e1)～(e6)文献<citation id="282" type="reference"><link href="238" rel="bibliography" />[17]</citation>方法;(f1)～(f6)文献<citation id="283" type="reference"><link href="240" rel="bibliography" />[18]</citation>方法;(g1)～(g6)本文方法</p>
                                <p class="img_note">(a1)-(a6) Infrared images; (b1)-(b6) visible images; (c1)-(c6) method in Ref. [15]; (d1)-(d6) method in Ref. <citation id="284" type="reference"><link href="236" rel="bibliography" />[16]</citation>; (e1)-(e6) method in Ref. <citation id="285" type="reference"><link href="238" rel="bibliography" />[17]</citation>; (f1)-(f6) method in Ref. <citation id="286" type="reference"><link href="240" rel="bibliography" />[18]</citation>; (g1)-(g6) proposed method</p>

                </div>
                <h3 id="147" name="147" class="anchor-tag">4 结  论</h3>
                <div class="p1">
                    <p id="148">提出了一种基于卷积自编码器和残差块的红外与可见光图像融合方法。将源图像用作编码器的输入,通过卷积层和残差块来获取图像的特征图;将得到的特征图采用改进的基于L1-norm的相似度融合策略进行融合,将其整合为一个包含源图像显著特征的特征图;利用解码器对融合后的图像进行重构。实验结果表明,与其他融合方法相比,本文方法能够有效地保留图像的中间层信息,融合后的图像更加清晰自然,是一种有效的融合方法。接下来,将在保证效率的情况下进一步优化网络模型,使得网络的性能更好。同时,将继续研究融合层策略,使其能够更好地应用于其他融合任务,如多聚焦图像、遥感图像等。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="206">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201802015&amp;v=MDYxNjNVUkxPZVplVnZGeXJtVkwvQUlqWFRiTEc0SDluTXJZOUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Jiang Z T,Wu H,Zhou X L.Infrared and visible image fusion algorithm based on improved guided filtering and dual-channel spiking cortical model[J].Acta Optica Sinica,2018,38(2):0210002.江泽涛,吴辉,周哓玲.基于改进引导滤波和双通道脉冲发放皮层模型的红外与可见光图像融合算法[J].光学学报,2018,38(2):0210002.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF1AA21F05383F7D583B754F0E37B5D1D&amp;v=MDk4NDJBPU5pZk9mY1c1YjZET3J2bEZZZWdIRHdvK3V4TWI2VTE2VFh1VXJHYzJmc0NSTWJ2ckNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRsaHhydTR3Sw==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Ma J Y,Ma Y,Li C.Infrared and visible image fusion methods and applications:a survey[J].Information Fusion,2019,45:153-178.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201711013&amp;v=MjY2NTQ5RVo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5cm1WTC9BSWpYVGJMRzRIOWJOcm8=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Liu X H,Chen Z B.Fusion of infrared and visible images based on multi-scale directional guided filter and convolutional sparse representation[J].Acta Optica Sinica,2017,37(11):1110004.刘先红,陈志斌.基于多尺度方向引导滤波和卷积稀疏表示的红外与可见光图像融合[J].光学学报,2017,37(11):1110004.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201711006&amp;v=MjU5MDhaZVZ2RnlybVZML0FMejdCZHJHNEg5Yk5ybzlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Lin S Z,Han Z.Images fusion based on deep stack convolutional neural network[J].Chinese Journal of Computers,2017,40(11):2506-2518.蔺素珍,韩泽.基于深度堆叠卷积神经网络的图像融合[J].计算机学报,2017,40(11):2506-2518.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Infrared and visible image fusion using a deep learning framework">

                                <b>[5]</b> Li H,Wu X J,Kittler J.Infrared and visible image fusion using a deep learning framework[C]//2018 24th International Conference on Pattern Recognition (ICPR),August 20-24,2018,Beijing,China.New York:IEEE,2018:2705-2710.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES427C64D5871F65335C5537AD8A37BD74&amp;v=MjAwNTJ3S0E9TmlmT2ZiZTZHYUxLcS90QWJPd09lbm84ekJVV21UcDRTM2lUMkJwRWVyWG1NYjJiQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxoeHJ1NA==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Liu Y,Chen X,Peng H,et al.Multi-focus image fusion with a deep convolutional neural network[J].Information Fusion,2017,36:191-207.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepFuse:A Deep Unsupervised Approach for Exposure Fusion with Extreme Exposure Image Pairs">

                                <b>[7]</b> Prabhakar K R,Sai Srikar V,Babu R V.DeepFuse:a deep unsupervised approach for exposure fusion with extreme exposure image pairs[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice,Italy.New York:IEEE,2017:4724-4732.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                 He K M,Zhang X Y,Ren S Q,et al.Deep residual learning for image recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:770-778.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201711037&amp;v=MTU4MDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2RnlybVZML0FJalhUYkxHNEg5Yk5ybzlHWTRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Lu Y S,Li Y X,Liu B,et al.Hyperspectral data haze monitoring based on deep residual network[J].Acta Optica Sinica,2017,37(11):1128001.陆永帅,李元祥,刘波,等.基于深度残差网络的高光谱遥感数据霾监测[J].光学学报,2017,37(11):1128001.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image fusion with convolutional sparse representation">

                                <b>[10]</b> Liu Y,Chen X,Ward R K,et al.Image fusion with convolutional sparse representation[J].IEEE Signal Processing Letters,2016,23(12):1882-1886.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Loss functions for image restoration with neural networks">

                                <b>[11]</b> Zhao H,Gallo O,Frosio I,et al.Loss functions for image restoration with neural networks[J].IEEE Transactions on Computational Imaging,2017,3(1):47-57.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using deep convolutional networks">

                                <b>[12]</b> Dong C,Loy C C,He K M,et al.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(2):295-307.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond face rotation:Global and local perception gan for photorealistic and identity preserving frontal view synthesis">

                                <b>[13]</b> Huang R,Zhang S,Li T Y,et al.Beyond face rotation:global and local perception GAN for photorealistic and identity preserving frontal view synthesis[C]//2017 IEEE International Conference on Computer Vision (ICCV),October 22-29,2017,Venice,Italy.New York:IEEE,2017:2458-2467.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LLCNN a convolutional neural network for low-light image enhancement">

                                <b>[14]</b> Tao L,Zhu C,Xiang G Q,et al.LLCNN:a convolutional neural network for low-light image enhancement[C]∥2017 IEEE Visual Communications and Image Processing (VCIP),December 10-13,2017,St.Petersburg,FL,USA.New York:IEEE,2017:17614346.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Infrared and visible image fusion with convolutional neural networks">

                                <b>[15]</b> Liu Y,Chen X,Cheng J,et al.Infrared and visible image fusion with convolutional neural networks[J].International Journal of Wavelets,Multiresolution and Information Processing,2018,16(3):1850018.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF5DBE0F083A76FEA02629927F5B5D7E3&amp;v=MTkyOTBwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxoeHJ1NHdLQT1OaWZPZmNXOWFxTzVyL2xGYk9oK0MzcFB1bWNUNkRsL1FYYmdxMlF3QzdmZ1FzK2NDT052RlNpV1dyN0pJRg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> Ma J L,Zhou Z Q,Wang B,et al.Infrared and visible image fusion based on visual saliency map and weighted least square optimization[J].Infrared Physics &amp; Technology,2017,82:8-17.
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES9430CCB0989C35D95CF68177E5B5C3D8&amp;v=MTQ5NjNKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxoeHJ1NHdLQT1OaWZPZmJxOEhkRy8zUDFGYmVNR2YzODh1eDhXbVVsN1FIN2xxMmN3QzdmblJzNlhDT052RlNpV1dyNw==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> Zhang Y,Zhang L J,Bai X Z,et al.Infrared and visual image fusion through infrared feature extraction and visual information preservation[J].Infrared Physics &amp; Technology,2017,83:227-237.
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201601005&amp;v=MjUzNDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2RnlybVZML0FQeXJmYkxHNEg5Zk1ybzlGWVlRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Chen M S.Image fusion of visual and infrared image based on NSCT and compressed sensing[J].Journal of Image and Graphics,2016,21(1):39-44.陈木生.结合NSCT和压缩感知的红外与可见光图像融合[J].中国图象图形学报,2016,21(1):39-44.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201910026" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201910026&amp;v=MjAwNjdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2RnlybVZML0FJalhUYkxHNEg5ak5yNDlIWW9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0NYeHlWeFdJdzdmQitCc1pzVVVuYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

