

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133883000283750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201906014%26RESULT%3d1%26SIGN%3dleJUDv4jkofdY%252f34vQ0u5PlNT7Q%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201906014&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201906014&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201906014&amp;v=MjkwMDg2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeWptVnJ2QklqWFRiTEc0SDlqTXFZOUVZSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#50" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="2 高速铁路场景碎片化区域的生成与组合 ">2 高速铁路场景碎片化区域的生成与组合</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="&lt;b&gt;2.1 基于自适应多尺度边界权值算法的铁路场景碎片化处理&lt;/b&gt;"><b>2.1 基于自适应多尺度边界权值算法的铁路场景碎片化处理</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;2.2 基于碎片化区域面积及边界强度的快速组合&lt;/b&gt;"><b>2.2 基于碎片化区域面积及边界强度的快速组合</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="3 高速铁路场景局部区域的识别 ">3 高速铁路场景局部区域的识别</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="&lt;b&gt;3.1 简化卷积神经网络结构&lt;/b&gt;"><b>3.1 简化卷积神经网络结构</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;3.2 优化卷积神经网络参数&lt;/b&gt;"><b>3.2 优化卷积神经网络参数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#109" data-title="4 分析与讨论 ">4 分析与讨论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#118" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="图1 铁路场景及轨道区域">图1 铁路场景及轨道区域</a></li>
                                                <li><a href="#63" data-title="图2 铁路场景边缘特征图">图2 铁路场景边缘特征图</a></li>
                                                <li><a href="#68" data-title="图3 霍夫变换后的直线特征分布图">图3 霍夫变换后的直线特征分布图</a></li>
                                                <li><a href="#69" data-title="图4 自适应调整角度的高斯卷积核">图4 自适应调整角度的高斯卷积核</a></li>
                                                <li><a href="#91" data-title="图5 碎片化区域合并过程图">图5 碎片化区域合并过程图</a></li>
                                                <li><a href="#95" data-title="图6 卷积神经网络结构示意图">图6 卷积神经网络结构示意图</a></li>
                                                <li><a href="#97" data-title="表1 不同卷积神经网络结构对比实验结果">表1 不同卷积神经网络结构对比实验结果</a></li>
                                                <li><a href="#101" data-title="图7 利用自编码网络预训练卷积核">图7 利用自编码网络预训练卷积核</a></li>
                                                <li><a href="#108" data-title="表2 不同卷积神经网络结构优化后的对比实验结果">表2 不同卷积神经网络结构优化后的对比实验结果</a></li>
                                                <li><a href="#111" data-title="图8 高速铁路周界侵限检测系统组成结构示意图">图8 高速铁路周界侵限检测系统组成结构示意图</a></li>
                                                <li><a href="#113" data-title="图9 不同算法识别轨道区域的结果对比图">图9 不同算法识别轨道区域的结果对比图</a></li>
                                                <li><a href="#115" data-title="表3 不同算法实验结果对比">表3 不同算法实验结果对比</a></li>
                                                <li><a href="#117" data-title="表4 不同算法的漏报警区域与误报警区域">表4 不同算法的漏报警区域与误报警区域</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" title=" He F L, Guo Y C, Gao C.Improved PCNN method for human target infrared image segmentation under complex environments[J].Acta Optica Sinica, 2017, 37 (2) :0215003.贺付亮, 郭永彩, 高潮.复杂环境下用于人体目标红外图像分割的改进PCNN方法[J].光学学报, 2017, 37 (2) :0215003." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201702022&amp;v=MDgxOTA1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlqbVZydkJJalhUYkxHNEg5Yk1yWTlIWm9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         He F L, Guo Y C, Gao C.Improved PCNN method for human target infrared image segmentation under complex environments[J].Acta Optica Sinica, 2017, 37 (2) :0215003.贺付亮, 郭永彩, 高潮.复杂环境下用于人体目标红外图像分割的改进PCNN方法[J].光学学报, 2017, 37 (2) :0215003.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" Wu C Y, Yi B S, Zhang Y G, &lt;i&gt;et al&lt;/i&gt;.Retinal vessel image segmentation based on improved convolutional neural network[J].Acta Optica Sinica, 2018, 38 (11) :1111004.吴晨玥, 易本顺, 章云港, 等.基于改进卷积神经网络的视网膜血管图像分割[J].光学学报, 2018, 38 (11) :1111004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811016&amp;v=MTgyNDRiTEc0SDluTnJvOUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeWptVnJ2QklqWFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Wu C Y, Yi B S, Zhang Y G, &lt;i&gt;et al&lt;/i&gt;.Retinal vessel image segmentation based on improved convolutional neural network[J].Acta Optica Sinica, 2018, 38 (11) :1111004.吴晨玥, 易本顺, 章云港, 等.基于改进卷积神经网络的视网膜血管图像分割[J].光学学报, 2018, 38 (11) :1111004.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" title=" Guo B Q, Yang L X, Shi H M, &lt;i&gt;et al&lt;/i&gt;.High-speed railway clearance intrusion detection algorithm with fast background subtraction[J].Chinese Journal of Scientific Instrument, 2016, 37 (6) :1371-1378.郭保青, 杨柳旭, 史红梅, 等.基于快速背景差分的高速铁路异物侵入检测算法[J].仪器仪表学报, 2016, 37 (6) :1371-1378." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201606022&amp;v=MDQ1MjdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlqbVZydkJQRHpUYkxHNEg5Zk1xWTlIWm9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Guo B Q, Yang L X, Shi H M, &lt;i&gt;et al&lt;/i&gt;.High-speed railway clearance intrusion detection algorithm with fast background subtraction[J].Chinese Journal of Scientific Instrument, 2016, 37 (6) :1371-1378.郭保青, 杨柳旭, 史红梅, 等.基于快速背景差分的高速铁路异物侵入检测算法[J].仪器仪表学报, 2016, 37 (6) :1371-1378.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" title=" Wang Y, Yu Z J, Zhu L Q, &lt;i&gt;et al&lt;/i&gt;.High-speed railway clearance surveillance system based on convolutional neural networks[J].Proceedings of SPIE, 2016, 10033:100335S." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-speed railway clearance surveillance system based on convolutional neural networks">
                                        <b>[4]</b>
                                         Wang Y, Yu Z J, Zhu L Q, &lt;i&gt;et al&lt;/i&gt;.High-speed railway clearance surveillance system based on convolutional neural networks[J].Proceedings of SPIE, 2016, 10033:100335S.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" title=" Achanta R, Shaji A, Smith K, &lt;i&gt;et al&lt;/i&gt;.SLIC superpixels compared to state-of-the-art superpixel methods[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (11) :2274-2282." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SLIC Superpixels Compared to State-of-the-Art Superpixel Methods">
                                        <b>[5]</b>
                                         Achanta R, Shaji A, Smith K, &lt;i&gt;et al&lt;/i&gt;.SLIC superpixels compared to state-of-the-art superpixel methods[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (11) :2274-2282.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" title=" Chen H Y, Qie L Z, Yang D D, &lt;i&gt;et al&lt;/i&gt;.Visual background extraction algorithm based on superpixel information feedback[J].Acta Optica Sinica, 2017, 37 (7) :0715001.陈海永, 郄丽忠, 杨德东, 等.基于超像素信息反馈的视觉背景提取算法[J].光学学报, 2017, 37 (7) :0715001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201707023&amp;v=MDg5OTh2QklqWFRiTEc0SDliTXFJOUhaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeWptVnI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Chen H Y, Qie L Z, Yang D D, &lt;i&gt;et al&lt;/i&gt;.Visual background extraction algorithm based on superpixel information feedback[J].Acta Optica Sinica, 2017, 37 (7) :0715001.陈海永, 郄丽忠, 杨德东, 等.基于超像素信息反馈的视觉背景提取算法[J].光学学报, 2017, 37 (7) :0715001.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" Liu Y C, Chen Y P, Zhang S S, &lt;i&gt;et al&lt;/i&gt;.Traffic sign recognition based on pyramid histogram fusion descriptor and HIK-SVM[J].Journal of Transportation Systems Engineering and Information Technology, 2017, 17 (1) :220-226.刘亚辰, 陈跃鹏, 张赛硕, 等.融合式空间塔式算子和HIK-SVM的交通标志识别研究[J].交通运输系统工程与信息, 2017, 17 (1) :220-226." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YSXT201701033&amp;v=MTg1MDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5am1WcnZCUEQ3VGVyRzRIOWJNcm85R1o0UUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Liu Y C, Chen Y P, Zhang S S, &lt;i&gt;et al&lt;/i&gt;.Traffic sign recognition based on pyramid histogram fusion descriptor and HIK-SVM[J].Journal of Transportation Systems Engineering and Information Technology, 2017, 17 (1) :220-226.刘亚辰, 陈跃鹏, 张赛硕, 等.融合式空间塔式算子和HIK-SVM的交通标志识别研究[J].交通运输系统工程与信息, 2017, 17 (1) :220-226.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" title=" Fang Z P, Duan J M, Zheng B G.Traffic signs recognition and tracking based on feature color and SNCC algorithm[J].Journal of Transportation Systems Engineering and Information Technology, 2014, 14 (1) :47-52.房泽平, 段建民, 郑榜贵.基于特征颜色和SNCC的交通标志识别与跟踪[J].交通运输系统工程与信息, 2014, 14 (1) :47-52." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YSXT201401009&amp;v=MDIzNTFNcm85RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5am1WcnZCUEQ3VGVyRzRIOVg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Fang Z P, Duan J M, Zheng B G.Traffic signs recognition and tracking based on feature color and SNCC algorithm[J].Journal of Transportation Systems Engineering and Information Technology, 2014, 14 (1) :47-52.房泽平, 段建民, 郑榜贵.基于特征颜色和SNCC的交通标志识别与跟踪[J].交通运输系统工程与信息, 2014, 14 (1) :47-52.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" Liu K P, Ying Z L, Zhai Y K, &lt;i&gt;et al&lt;/i&gt;.SAR image target recognition based on unsupervised K-means feature and data augmentation[J].Journal of Signal Processing, 2017, 33 (3) :452-458.刘凯品, 应自炉, 翟懿奎, 等.基于无监督K均值特征和数据增强的SAR图像目标识别方法[J].信号处理, 2017, 33 (3) :452-458." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXCN201703029&amp;v=MTYwNjNVUkxPZVplVnVGeWptVnJ2QlBUWElZTEc0SDliTXJJOUhiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Liu K P, Ying Z L, Zhai Y K, &lt;i&gt;et al&lt;/i&gt;.SAR image target recognition based on unsupervised K-means feature and data augmentation[J].Journal of Signal Processing, 2017, 33 (3) :452-458.刘凯品, 应自炉, 翟懿奎, 等.基于无监督K均值特征和数据增强的SAR图像目标识别方法[J].信号处理, 2017, 33 (3) :452-458.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" title=" Zhang X D, Fan J L, Xu J, &lt;i&gt;et al&lt;/i&gt;.Image super-resolution algorithm via K-means clustering and support vector data description[J].Journal of Image and Graphics, 2016, 21 (2) :135-144.张小丹, 范九伦, 徐健, 等.K均值聚类和支持向量数据描述的图像超分辨率算法[J].中国图象图形学报, 2016, 21 (2) :135-144." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201602002&amp;v=MjI5ODF5cmZiTEc0SDlmTXJZOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeWptVnJ2QlA=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Zhang X D, Fan J L, Xu J, &lt;i&gt;et al&lt;/i&gt;.Image super-resolution algorithm via K-means clustering and support vector data description[J].Journal of Image and Graphics, 2016, 21 (2) :135-144.张小丹, 范九伦, 徐健, 等.K均值聚类和支持向量数据描述的图像超分辨率算法[J].中国图象图形学报, 2016, 21 (2) :135-144.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" title=" Ma G Q, Tian Y C, Li X L.Application of K-means clustering algorithm in colour image segmentation of grouper in seawater background[J].Computer Applications and Software, 2016, 33 (5) :192-195.马国强, 田云臣, 李晓岚.K-均值聚类算法在海水背景石斑鱼彩色图像分割中的应用[J].计算机应用与软件, 2016, 33 (5) :192-195." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201605049&amp;v=MTAzMDhaZVZ1RnlqbVZydkJMelRaWkxHNEg5Zk1xbzlCYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Ma G Q, Tian Y C, Li X L.Application of K-means clustering algorithm in colour image segmentation of grouper in seawater background[J].Computer Applications and Software, 2016, 33 (5) :192-195.马国强, 田云臣, 李晓岚.K-均值聚类算法在海水背景石斑鱼彩色图像分割中的应用[J].计算机应用与软件, 2016, 33 (5) :192-195.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" title=" Arbel&#225;ez P, Pont-Tuset J, Barron J T, Marques F, &lt;i&gt;et al&lt;/i&gt;.Multiscale combinatorial grouping[C]∥2004 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:328-335." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiscale Combinatorial Grouping">
                                        <b>[12]</b>
                                         Arbel&#225;ez P, Pont-Tuset J, Barron J T, Marques F, &lt;i&gt;et al&lt;/i&gt;.Multiscale combinatorial grouping[C]∥2004 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:328-335.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" title=" Arbel&#225;ez P, Maire M, Fowlkes C, &lt;i&gt;et al&lt;/i&gt;.Contour detection and hierarchical image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) :898-916." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Contour Detection and Hierarchical Image Segmentation">
                                        <b>[13]</b>
                                         Arbel&#225;ez P, Maire M, Fowlkes C, &lt;i&gt;et al&lt;/i&gt;.Contour detection and hierarchical image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) :898-916.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" Arbel&#225;ez P.Boundary extraction in natural images using ultrametric contour maps[C]//2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW′06) , June 17-22, 2006, New York, USA.New York:IEEE, 2006:182." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Boundary extraction in natural images using ultrametric contour maps">
                                        <b>[14]</b>
                                         Arbel&#225;ez P.Boundary extraction in natural images using ultrametric contour maps[C]//2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW′06) , June 17-22, 2006, New York, USA.New York:IEEE, 2006:182.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" Farabet C, Couprie C, Najman L, &lt;i&gt;et al&lt;/i&gt;.Learning hierarchical features for scene labeling[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (8) :1915-1929." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning hierarchical features for scene labeling">
                                        <b>[15]</b>
                                         Farabet C, Couprie C, Najman L, &lt;i&gt;et al&lt;/i&gt;.Learning hierarchical features for scene labeling[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (8) :1915-1929.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" title=" Couprie C, Farabet C, Najman L, &lt;i&gt;et al&lt;/i&gt;.Indoor semantic segmentation using depth information[EB/OL]. (2013-03-14) [2019-01-25].https://arxiv.org/abs/1301.3572." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Indoor semantic segmentation using depth information">
                                        <b>[16]</b>
                                         Couprie C, Farabet C, Najman L, &lt;i&gt;et al&lt;/i&gt;.Indoor semantic segmentation using depth information[EB/OL]. (2013-03-14) [2019-01-25].https://arxiv.org/abs/1301.3572.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" title=" Gupta S, Girshick R, Arbel&#225;ez P, &lt;i&gt;et al&lt;/i&gt;.Learning rich features from RGB-D images for object detection and segmentation[M]//Fleet D, Pajdla T, Schiele B, &lt;i&gt;et al&lt;/i&gt;.Computer Vision-ECCV 2014.Cham:Springer, 2014, 8695:345-360." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning rich features from RGB-D images for object detection and segmentation">
                                        <b>[17]</b>
                                         Gupta S, Girshick R, Arbel&#225;ez P, &lt;i&gt;et al&lt;/i&gt;.Learning rich features from RGB-D images for object detection and segmentation[M]//Fleet D, Pajdla T, Schiele B, &lt;i&gt;et al&lt;/i&gt;.Computer Vision-ECCV 2014.Cham:Springer, 2014, 8695:345-360.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" Petrelli A, Pau D, di Stefano L.Analysis of compact features for RGB-D visual search[M]∥Murino V, Puppo E.Image Analysis and Processing-ICIAP 2015.Cham:Springer, 2015, 9280:14-24." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Analysis of compact features for RGB-D visual search">
                                        <b>[18]</b>
                                         Petrelli A, Pau D, di Stefano L.Analysis of compact features for RGB-D visual search[M]∥Murino V, Puppo E.Image Analysis and Processing-ICIAP 2015.Cham:Springer, 2015, 9280:14-24.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_19" title=" Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[19]</b>
                                         Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_20" title=" Zheng T Y, Tang C, Lei Z K.Multi-scale retinal vessel segmentation based on fully convolutional neural network[J].Acta Optica Sinica, 2019, 39 (2) :0211002.郑婷月, 唐晨, 雷振坤.基于全卷积神经网络的多尺度视网膜血管分割[J].光学学报, 2019, 39 (2) :0211002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902014&amp;v=MDk3MzBqTXJZOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeWptVnJ2QklqWFRiTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         Zheng T Y, Tang C, Lei Z K.Multi-scale retinal vessel segmentation based on fully convolutional neural network[J].Acta Optica Sinica, 2019, 39 (2) :0211002.郑婷月, 唐晨, 雷振坤.基于全卷积神经网络的多尺度视网膜血管分割[J].光学学报, 2019, 39 (2) :0211002.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-16 17:46</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(06),119-126 DOI:10.3788/AOS201939.0610004            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>高速铁路场景的分割与识别算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="javascript:;">王洋</a>
                                <a href="javascript:;">朱力强</a>
                                <a href="javascript:;">余祖俊</a>
                                <a href="javascript:;">郭保青</a>
                </h2>
                    <h2>

                    <span>北京交通大学机械与电子控制工程学院</span>
                    <span>北京交通大学载运工具先进制造与测控技术教育部重点实验室</span>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为实现高速铁路周界侵限检测系统自动识别轨道区域的功能, 提出了一种自适应的图像分割与识别算法。计算了每个场景的直线特征极大值以调节自适应参数, 提出了新的基于边界点权重及区域面积的聚类组合规则, 将碎片化区域快速组合成局部区域;简化了卷积神经网络, 通过对卷积核进行预训练并在损失函数中增加稀疏项来提高特征图的差异性。在不使用显卡的前提下, 对比实验结果表明所提算法的像素准确率最高 (95.9%) , 计算时间最短 (2.5 s) , 网络参数约为0.18×10<sup>6</sup>个, 在分割精准度、识别准确率、计算时间、人工操作复杂度和系统硬件成本等之间找到了有效平衡点, 提高了铁路周界侵限检测系统的自动化程度和工作效率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%BA%E6%99%AF%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">场景分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%BA%E6%99%AF%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">场景识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度边缘检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    朱力强 E-mail:lqzhu@bjtu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-22</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划高速铁路系统安全保障课题 (2016YFB1200401);</span>
                    </p>
            </div>
                    <h1><b>Segmentation and Recognition Algorithm for High-Speed Railway Scene</b></h1>
                    <h2>
                    <span>Wang Yang</span>
                    <span>Zhu Liqiang</span>
                    <span>Yu Zujun</span>
                    <span>Guo Baoqing</span>
            </h2>
                    <h2>
                    <span>School of Mechanical, Electronic and Control Engineering, Beijing Jiaotong University</span>
                    <span>Key Laboratory of Vehicle Advanced Manufacturing, Measuring and Control Technology, Beijing Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To recognize a monitored area automatically for a high-speed railway perimeter-intrusion detecting system, an adaptive image segmentation and recognition algorithm is proposed. The maximum linear feature of each scene is calculated to regulate the adaptive parameters. Moreover, a new combination rule based on the weight of the boundary point and the area size is proposed to rapidly combine the fragmented regions into local areas. A simplified convolutional neural network is designed, the convolutional kernels are pre-trained, and a sparse element is added into the loss function to enhance the diversity of the feature maps. Experimental comparison results indicate that without the graphics processing unit, the pixel accuracy of the proposed algorithm is highest (95.9%) , the calculation time is the least (2.5 s) , and the number of network parameters is about 0.18×10<sup>6</sup>. The proposed algorithm considers an effective balance among the segmentation precision, recognition accuracy, calculation time, manual workload, and hardware cost of the system. Therefore, the automation and efficiency of the railway perimeter intrusion detection system are enhanced.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale%20edge%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale edge detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20networks&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural networks;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-22</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="50" name="50" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="51">基于视频检测技术的铁路周界侵限检测系统能够有效检测侵入轨道区域周界以内的异物, 是实现铁路安全运行的重要保障。轨道区域的位置及边界是系统判断异物是否侵限的重要依据, 其划分过程却是通过人工操作在监控场景中事先标注的, 而随着铁路沿线监控相机数量的激增, 单纯依靠人工标注变得费时费力。除此之外, 铁路沿线还存在大量的变焦相机, 它们会因不同业务需求而临时改变拍摄角度及焦距, 变动后的监控场景需要及时地进行重新标注, 这同样需要大量的人工操作。因此, 如果检测系统能够利用相应的场景分割与识别算法<citation id="120" type="reference"><link href="10" rel="bibliography" /><link href="12" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>, 自动划分需要监控的轨道区域, 将大幅提高检测系统的自动化程度和工作效率。在实际工程应用中, 检测系统要求相应算法不仅要有良好的分割精准度和识别准确率, 而且要具备快速处理临时变动场景的能力, 同时系统对算法本身的计算量也有所限制, 以便将算法移植进不同的数据处理硬件平台<citation id="121" type="reference"><link href="14" rel="bibliography" /><link href="16" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="52">传统方法如超级像素点算法<citation id="124" type="reference"><link href="18" rel="bibliography" /><link href="20" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>是将图像从RGB颜色空间转换到CIE-Lab颜色空间, 组成一个5维向量, 通过计算向量距离, 将图像分割成多个碎片化区域。刘亚辰等<citation id="122" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>通过增加图像的灰度、颜色和边缘梯度等融合式空间塔式直方图特征信息, 并使用支持向量机 (SVM) 对特征进行分类。房泽平等<citation id="123" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>在YCbCr颜色空间对图像进行颜色阈值分割, 通过相关区域与人工设计模板的匹配来定位和识别现场的交通标志。上述算法说明将图像从RGB颜色空间转换到其他颜色空间可以增加更多维度的特征信息。为了将碎片化区域组合为代表场景组成元素的局部区域, 刘凯品等<citation id="125" type="reference"><link href="26" rel="bibliography" /><link href="28" rel="bibliography" /><link href="30" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>将K-Mean (多维度) 聚类思想应用到实际工程中, 根据不同的场景设定不同的颜色聚类规则, 进而对每个碎片化区域中的连通域进行分析, 最后对各区域进行组合与聚类。Arbeláez等<citation id="126" type="reference"><link href="32" rel="bibliography" /><link href="34" rel="bibliography" /><link href="36" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>提出多尺度组合聚合 (MCG) 算法, 通过训练随机森林回归器优化碎片化区域的组合方式, 最终得到前景物体的完整轮廓。上述算法的聚类过程通常涉及大量的迭代运算, 使得局部区域的精准度与计算时间成反比。</p>
                </div>
                <div class="p1">
                    <p id="53">新兴的卷积神经网络 (CNN) 作为深度学习的重要分支, 在图像识别、语义分割和目标检测等领域有着广泛的应用。Farabet等<citation id="127" type="reference"><link href="38" rel="bibliography" /><link href="40" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>在卷积神经网络结构的基础上分别并行引入超级像素点、随机效应场和图像语义分割等算法。Gupta等<citation id="128" type="reference"><link href="42" rel="bibliography" /><link href="44" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>提出先对RGB-D图像的深度信息进行水平差异、地面高度和重力倾角的编码, 最后串行连接CNN分类网络。全连接卷积神经网络 (FCN) 算法<citation id="129" type="reference"><link href="46" rel="bibliography" /><link href="48" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>则是通过深度卷积神经网络自身同时完成特征提取、组合及分类, 并实现像素级的图像分割与识别。上述算法说明卷积神经网络可以并行、串行和独立完成相关图像的分割与识别工作, 其功能及效果取决于网络结构的复杂程度, 而大量的网络参数导致相应计算只能依赖于GPU显卡, 不利于将算法移植进分布在铁路沿线的不同配置的数据处理平台。</p>
                </div>
                <div class="p1">
                    <p id="54">对MCG算法和FCN算法进行比较。MCG算法的计算结果与真值的平均交互重合率 (MIU) 为80%, 但计算时间为7 s (图像分辨率为90 pixel×150 pixel, RGB三色) , 导致其只适用于固定监控场景的初始划分或后期处理, 而不适用于变动场景的快速划分。而FCN算法 (运算结果因FCN具体结构而异) 的区域分割结果与真值的交互重合率 (IU) 在70%～80%之间, 像素准确率约为90%, 网络参数约为57×10<sup>6</sup>～134×10<sup>6</sup>个, 只能依赖GPU显卡并行计算。本文结合传统图像分割算法与卷积神经网络算法的优点, 提出了先分割得到精确的局部区域边界, 再识别局部区域类别的计算方案, 并针对上述方法的缺点逐一做出改进。</p>
                </div>
                <div class="p1">
                    <p id="55">针对图像分割依赖大量迭代运算的问题, 通过充分利用铁路场景直线特征强烈、组成区域类别固定等特点, 提出自适应的多尺度边界点权值计算方法, 大幅减少传统算法中的计算项, 加快碎片化区域的生成;然后通过阈值筛选强弱边界点, 减少碎片化区域数量, 并进一步通过区域面积及相邻区域边界强度将碎片化区域快速组合成局部区域。</p>
                </div>
                <div class="p1">
                    <p id="56">针对卷积神经网络计算量大的问题, 提出只将卷积神经网络应用于局部区域的识别, 通过简化卷积神经网络及减少网络参数来加快识别速度并摆脱对GPU显卡的依赖;进一步通过对卷积核进行预训练并增加损失函数的稀疏项来提高特征图的差异性, 以弥补因简化网络而损失的准确率。</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag">2 高速铁路场景碎片化区域的生成与组合</h3>
                <h4 class="anchor-tag" id="58" name="58"><b>2.1 基于自适应多尺度边界权值算法的铁路场景碎片化处理</b></h4>
                <div class="p1">
                    <p id="59">如图1所示, 铁路场景主要包括轨道区域、天空、接触网、绿化带和附属建筑等多个区域, 其中包含钢轨、枕木、路基或高铁轨道板的轨道区域 (标记区域) 通常被划定为需要监控的限界区域。</p>
                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906014_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 铁路场景及轨道区域" src="Detail/GetImg?filename=images/GXXB201906014_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 铁路场景及轨道区域  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906014_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Railway scene and track area</p>

                </div>
                <div class="p1">
                    <p id="61">多尺度边界点权重计算是在不同缩放比例<i>S</i>下, 利用不同倾斜角<i>θ</i>的高斯卷积核来对不同通道<i>C</i>内的特征图局部区域进行卷积运算, 从而得到像素点自身图像特征分布与临近点之间的相似度分布, 然后通过加权求和得到每个点成为边界点的可能性, 最终将图像分割成碎片化区域。传统方法是在0-π之间均匀选择8～16组<i>θ</i>角度的卷积核, 而在实际应用中发现, 铁路场景包含的各个区域多是以消隐点为中心呈放射状分布的, 特别是轨道区域多数与接触网立柱、支架、建筑等相邻, 它们之间的边界带有强烈的直线特征, 因此如果针对每个具体场景自动调整高斯卷积核的倾斜角<i>θ</i>, 则可加强相关区域直线边界点的权重, 这样就可以用少量自适应的卷积核代替传统算法中大量的均匀取值的卷积核, 从而大幅减少加权项的数量, 加快计算速度。为此提取原始图像边缘特征 (图2) , 根据空间转换关系<mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ρ</mi><mo>=</mo><mi>x</mi><mrow><mi>cos</mi></mrow><mspace width="0.25em" /><msup><mi>θ</mi><mo>′</mo></msup><mo>+</mo><mi>y</mi><mrow><mi>sin</mi></mrow><mspace width="0.25em" /><msup><mi>θ</mi><mo>′</mo></msup><mo>, </mo><mo>-</mo><mfrac><mi>π</mi><mn>2</mn></mfrac><mo>&lt;</mo><msup><mi>θ</mi><mo>′</mo></msup><mo>&lt;</mo><mfrac><mi>π</mi><mn>2</mn></mfrac></mrow></math></mathml>, 进行霍夫 (Hough) 变换, 其中<i>x</i>, <i>y</i>为像素点在笛卡尔坐标系下的坐标, <i>θ</i>′为极坐标系下的极角, <i>ρ</i>为极径。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906014_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 铁路场景边缘特征图" src="Detail/GetImg?filename=images/GXXB201906014_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 铁路场景边缘特征图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906014_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Edge feature map of railway scene</p>

                </div>
                <div class="p1">
                    <p id="64">由于直角坐标系中的共线点对应着霍夫变换坐标内有共同交点的一组曲线, 所以霍夫变换后的极大值<i>H</i> (<i>θ</i>′, <i>ρ</i>) (图3矩形框所示) 意味着有最多的点共线, 根据表达式</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mrow><mi>tan</mi><mspace width="0.25em" /><msup><mi>θ</mi><mo>′</mo></msup></mrow></mfrac><mi>x</mi><mo>+</mo><mfrac><mi>ρ</mi><mrow><mi>sin</mi><mspace width="0.25em" /><msup><mi>θ</mi><mo>′</mo></msup></mrow></mfrac><mo>=</mo><mi>k</mi><mi>x</mi><mo>+</mo><mi>b</mi><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">反向计算极值点所代表的直线方程, 即可得到直角坐标系中直线的斜率<i>k</i>和截距<i>b</i>, 对应到0-π之间的倾斜角<mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><mo>=</mo><mfrac><mi>π</mi><mn>2</mn></mfrac><mo>-</mo><msup><mi>θ</mi><mo>′</mo></msup></mrow></math></mathml>。图4是倾斜角分别为22°, 38°, 90°, 178°的4个自适应的卷积核, 其中 (X, Y) 为卷积核像素坐标。</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906014_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 霍夫变换后的直线特征分布图" src="Detail/GetImg?filename=images/GXXB201906014_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 霍夫变换后的直线特征分布图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906014_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Distribution of linear character after Hough transformation</p>

                </div>
                <div class="area_img" id="69">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906014_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 自适应调整角度的高斯卷积核" src="Detail/GetImg?filename=images/GXXB201906014_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 自适应调整角度的高斯卷积核  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906014_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Gaussian convolution kernels rotated by adaptive angles</p>
                                <p class="img_note"> (a) θ=22°; (b) θ=38°; (c) θ=90°; (d) θ=178°</p>
                                <p class="img_note"> (a) θ=22°; (b) θ=38°; (c) θ=90°; (d) θ=178°</p>

                </div>
                <div class="p1">
                    <p id="70">在确定卷积核之后, 先将原始图像从RGB色彩空间转换至CIE-Lab色彩空间, 并获取包含亮度、色度<i>A</i>、色度<i>B</i>和纹理在内的四通道特征图;然后将图像中每一个像素<i>P</i> (<i>x</i>, <i>y</i>) , 在缩放比例<i>S</i>下通道<i>C</i>中及倾斜角<i>θ</i>上的高斯卷积结果记作<i>G</i> (<i>x</i>, <i>y</i>, <i>θ</i>, <i>C</i>, <i>S</i>) ;最后通过加权求和 (权重为<i>α</i><sub><i>C</i>, <i>S</i></sub>) 得到像素点<i>P</i> (<i>x</i>, <i>y</i>) 的色彩纹理特征分布<i>f</i><sub><i>PC</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mi>Ρ</mi><mi>C</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo>, </mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>S</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>C</mi></munder><mi>α</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>C</mi><mo>, </mo><mi>S</mi></mrow></msub><mi>G</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo>, </mo><mi>θ</mi><mo>, </mo><mi>C</mi><mo>, </mo><mi>S</mi><mo stretchy="false">) </mo><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">定义像素点<i>i</i>与<i>j</i>之间的连线<i>l</i>上所有点的<i>PC</i>最大值为两点之间的相似度<i>S</i><sub>imilarity</sub> (<i>i</i>, <i>j</i>) :</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>S</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>m</mtext><mtext>i</mtext><mtext>l</mtext><mtext>a</mtext><mtext>r</mtext><mtext>i</mtext><mtext>t</mtext><mtext>y</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mi>exp</mi><mo stretchy="false">{</mo><mo>-</mo><mi>max</mi><mo stretchy="false">[</mo><mi>f</mi><msub><mrow></mrow><mrow><mi>Ρ</mi><mi>C</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>∈</mo><mi>l</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">利用 (3) 式可以得到局部区域内任意点到中心点的相似度矩阵<b><i>M</i></b>。计算矩阵<b><i>M</i></b>的特征值<i>λ</i>和特征向量<i>ν</i>, 提取特征向量前<i>t</i>维作为区域中心像素与周围像素的相似度值, 进而组成全图的<i>t</i>层相似度特征图;在不同缩放比例<i>S</i>下, 通过不同倾斜角<i>θ</i>的高斯卷积及加权求和 (权重为<i>β</i><sub><i>t</i>, <i>S</i></sub>) 得到每个像素<i>P</i> (<i>x</i>, <i>y</i>) 的相似度分布<i>f</i><sub><i>PS</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mi>Ρ</mi><mi>S</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo>, </mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>S</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>t</mi></munder><mi>β</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mi>S</mi></mrow></msub><mi>G</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo>, </mo><mi>θ</mi><mo>, </mo><mi>t</mi><mo>, </mo><mi>S</mi><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">最终求和得到<i>B</i> (<i>x</i>, <i>y</i>) :</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>θ</mi></munder><mi>f</mi></mstyle><msub><mrow></mrow><mrow><mi>Ρ</mi><mi>C</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo>, </mo><mi>θ</mi><mo stretchy="false">) </mo><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>θ</mi></munder><mi>f</mi></mstyle><msub><mrow></mrow><mrow><mi>Ρ</mi><mi>S</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo>, </mo><mi>θ</mi><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78"><i>B</i> (<i>x</i>, <i>y</i>) 即为该像素点成为边界点的可能性。带有不同强弱权重的边界点将会把原始图像分割成碎片化区域。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79"><b>2.2 基于碎片化区域面积及边界强度的快速组合</b></h4>
                <div class="p1">
                    <p id="80">通过多尺度边界点权重计算得到的强弱边界点如图5 (a) 所示, 像素点的权值越高、越明亮, 表示该点越有可能成为边界点。为加快碎片化区域的合并, 提出通过自动选取权重阈值来减弱边界以减少碎片化区域数量, 进一步将小碎片合并进边界权重最弱的邻域, 通过循环操作逐渐将碎片化区域合并成局部区域, 主要过程分为以下7个步骤:</p>
                </div>
                <div class="p1">
                    <p id="81">1) 对得到的边界点权重<i>B</i> (<i>x</i><sub><i>m</i></sub>, <i>y</i><sub><i>m</i></sub>) 进行sigmoid运算, 得到边界点<i>B</i> (<i>m</i>) 为</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>B</mi><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo><mo>=</mo><mtext>s</mtext><mtext>i</mtext><mtext>g</mtext><mtext>m</mtext><mtext>o</mtext><mtext>i</mtext><mtext>d</mtext><mo stretchy="false">[</mo><mi>B</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>m</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>=</mo></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo stretchy="false">[</mo><mo>-</mo><mi>B</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>m</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow></mfrac><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">边界点<i>B</i> (<i>m</i>) 的取值为0～1之间, 序号<i>m</i>=1, 2, …, <i>M</i>, <i>M</i>为边界点总数, 其中 (<i>x</i><sub><i>m</i></sub>, <i>y</i><sub><i>m</i></sub>) 为像素点<i>m</i>的坐标, 如图5 (a) 所示;</p>
                </div>
                <div class="p1">
                    <p id="84">2) 边界点权值分布统计图如图5 (b) 所示, 边界点权值共分为10个等级, 筛选阈值<i>B</i>, 选取最小权重等级, 删掉边界强度<i>B</i> (<i>i</i>) ≤<i>B</i>的边界点;</p>
                </div>
                <div class="p1">
                    <p id="85">3) 通过膨胀腐蚀处理连接断点, 如图5 (c) 所示, 得到的碎片化区域如图5 (d) 所示;</p>
                </div>
                <div class="p1">
                    <p id="86">4) 统计碎片化区域面积分布, 如图5 (e) 所示, 区域编号<i>n</i>=1, 2, 3, …, <i>N</i>, <i>N</i>为总区域数, 选择像素面积最小的碎片化区域<i>n</i><sub>min</sub>, 沿该区域边界寻找边界权值最小的邻域<i>n</i>′, 将两者合并。</p>
                </div>
                <div class="p1">
                    <p id="87">5) 重复步骤4) , 以减小<i>N</i>值, 直至最小碎片化区域面积大于最小碎片面积<i>S</i><sub>min</sub>, 如图5 (f) 所示, 区域编号1与2, 3, 4合并;</p>
                </div>
                <div class="p1">
                    <p id="88">6) 若<i>N</i>&gt;<i>T</i>, <i>T</i>为最终保留区域个数, 则选择<i>B</i>为边界权值次小等级, 并返回步骤2) 重新开始;</p>
                </div>
                <div class="p1">
                    <p id="89">7) 将分割后的局部区域调整为像素尺寸64 pixel×64 pixel, 如图5 (g) ～ (o) 所示, 将其送入卷积神经网络, 得到分类标签。</p>
                </div>
                <div class="p1">
                    <p id="90">通过多次对比实验并结合铁路场景包含区域的自身特点 (放射性、类别少且面积大) , 将铁路场景图像设定为90 pixel×150 pixel, 保留区域个数<i>T</i>设定为10, 最小碎片面积<i>S</i><sub>min</sub>设定为图像总面积的10%, 从而防止结果过于碎片化。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906014_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 碎片化区域合并过程图" src="Detail/GetImg?filename=images/GXXB201906014_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 碎片化区域合并过程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906014_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Procedures of combining fragmented regions</p>
                                <p class="img_note"> (a) 强弱边界图; (b) 边界权重分布图; (c) 筛选掉弱边界点; (d) 碎片化区域; (e) 碎片化区域面积分布; (f) 合并后得到的局部区域; (g) - (o) 分割后的局部区域</p>
                                <p class="img_note"> (a) Strong and weak boundaries; (b) distribution of boundary weight; (c) boundaries after deletion of weak points; (d) fragmented regions; (e) distribution of fragmented region area; (f) local areas after combination; (g) - (o) local areas after segmentation</p>

                </div>
                <h3 id="92" name="92" class="anchor-tag">3 高速铁路场景局部区域的识别</h3>
                <h4 class="anchor-tag" id="93" name="93"><b>3.1 简化卷积神经网络结构</b></h4>
                <div class="p1">
                    <p id="94">传统深度卷积神经网络算法虽然可以同时实现像素级的图像分割与识别, 但是在实际工程应用中除了使用GPU显卡进行并行计算外, 还需要大量的人工操作来细化样本区域及边界的像素级标签, 因此通过设计一个简化的小型卷积神经网络, 可简化人工标注工作并节省成本。卷积神经网络结构主要是由输入层、两层卷积层 (C1和C2) 、两层均值采样池化层 (S1和S2) 、一个全连接层 (FC) 和一个分类层 (softmax) 组成, 如图6所示。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906014_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 卷积神经网络结构示意图" src="Detail/GetImg?filename=images/GXXB201906014_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 卷积神经网络结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906014_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Schematic of convolutional neural network structure</p>

                </div>
                <div class="p1">
                    <p id="96">不同卷积神经网络结构的对比实验结果如表1所示, 在简化网络结构、减少计算参数和节省计算时间的前提下, 调整卷积核数量 (表1中第1行, C1层分别选用30个和100个卷积核) 和卷积核尺寸的方法对简化卷积神经网络识别准确率的提升有限。对比结果表明:简化卷积神经网络对识别准确率的提升有限, 远不如结构复杂的、使用GPU显卡参与运算的深度卷积神经网络。</p>
                </div>
                <div class="area_img" id="97">
                    <p class="img_tit">表1 不同卷积神经网络结构对比实验结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Comparison of experimental results of different CNN network structures</p>
                    <p class="img_note"></p>
                    <table id="97" border="1"><tr><td rowspan="2"><br />Kernel size</td><td colspan="2"><br />Kernel quantity</td><td rowspan="2">Accuracy /%</td></tr><tr><td><br />C1</td><td>C2</td></tr><tr><td rowspan="2"><br />3×3</td><td><br />30</td><td>10</td><td>72.5</td></tr><tr><td><br />100</td><td>10</td><td>75.0</td></tr><tr><td>5×5</td><td>100</td><td>10</td><td>76.0</td></tr><tr><td><br />8×8</td><td>100</td><td>10</td><td>76.5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="98">为了弥补因简化网络而损失的准确率, 需要对网络参数进行优化, 因此提出对卷积核进行预处理, 使其能够更好地提取图像底层特征, 同时增加损失函数中的稀疏项, 通过抑制特征图的平均输出并提高个别特征图的输出值来增大特征图的差异性。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>3.2 优化卷积神经网络参数</b></h4>
                <div class="p1">
                    <p id="100">在铁路场景中采集彩色小图像补丁 (像素大小为8 pixel×8 pixel, RGB三色) , 采集数据构成列向量并作为自编码网络AutoEncoder[图7 (a) ]的输入, 隐含层为100个隐含神经元, 令输出等于输入, 经无监督训练后的W1作为卷积层C1的卷积核, 结果如图7 (b) 所示。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906014_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 利用自编码网络预训练卷积核" src="Detail/GetImg?filename=images/GXXB201906014_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 利用自编码网络预训练卷积核  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906014_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Pre-train convolutional kernels using autoencoder network</p>
                                <p class="img_note"> (a) 自编码网络结构; (b) 预训练后的卷积核</p>
                                <p class="img_note"> (a) Structure of autoencoder networks; (b) pre-trained convolution kernels</p>

                </div>
                <div class="p1">
                    <p id="102">原有的代价函数可表示为</p>
                </div>
                <div class="p1">
                    <p id="103" class="code-formula">
                        <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>J</mi><mo>=</mo><mrow><mo>{</mo><mrow><mfrac><mn>1</mn><mi>Ρ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>Ρ</mi></munderover><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></mstyle><mrow><mo stretchy="false">[</mo><mi>h</mi><mo stretchy="false"> (</mo><mi>e</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>l</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">]</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>}</mo></mrow><mo>+</mo></mtd></mtr><mtr><mtd><mi>τ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mrow><mn>1</mn><mn>0</mn></mrow></munderover><mrow><mrow><mo>[</mo><mrow><mi>χ</mi><mi>lg</mi><mfrac><mi>χ</mi><mrow><mi>η</mi><msub><mrow></mrow><mi>f</mi></msub></mrow></mfrac><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>χ</mi><mo stretchy="false">) </mo><mi>lg</mi><mfrac><mrow><mn>1</mn><mo>-</mo><mi>χ</mi></mrow><mrow><mn>1</mn><mo>-</mo><mi>η</mi><msub><mrow></mrow><mi>f</mi></msub></mrow></mfrac></mrow><mo>]</mo></mrow></mrow></mstyle><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="104">其中</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>η</mi><msub><mrow></mrow><mi>f</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>Ρ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>Ρ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>u</mi><mo>=</mo><mn>1</mn></mrow><mrow><mn>2</mn><mn>8</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mn>2</mn><mn>8</mn></mrow></munderover><mi>Ο</mi></mstyle></mrow></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>f</mi><mo>, </mo><mi>e</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">式中:<i>e</i><sub><i>p</i></sub>为第<i>p</i>个输入样本;<i>l</i><sub><i>p</i></sub>为对应标签;<i>P</i>为样本总量;<i>h</i> (<i>e</i><sub><i>p</i></sub>) 为输出;<i>τ</i>为控制稀疏性惩罚因子的权值;<i>χ</i>为稀疏参数, 取接近0的较小数值, 如0.05;<i>η</i><sub><i>f</i></sub>为第二卷积层生成的第<i>f</i>张特征图<i>O</i> (<i>u</i>, <i>v</i>) (第二卷积层共有10张特征图) 在<i>P</i>个样本中的平均激活度;<i>u</i>和<i>v</i>为特征图像素坐标 (特征图尺寸为28 pixel×28 pixel) 。根据 (7) 式和 (8) 式, 在原有代价函数<i>J</i>的基础上增加稀疏项。每次迭代训练中, 通过稀疏项来抑制多数特征图输出和提高个别特征图输出, 从而增加卷积核的差异性, 提高特征图的可分性, 最终提高分类准确率。</p>
                </div>
                <div class="p1">
                    <p id="107">不同卷积神经网络结构优化后的对比实验结果如表2所示。通过对比表1和表2可以发现:在经过卷积核预处理和特征图稀疏化以后, 拥有不同卷积核尺寸及数量的卷积神经网络的识别准确率都得到了提高。</p>
                </div>
                <div class="area_img" id="108">
                    <p class="img_tit">表2 不同卷积神经网络结构优化后的对比实验结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Comparison of experimental results of different convolutional neural network structures after optimization</p>
                    <p class="img_note"></p>
                    <table id="108" border="1"><tr><td rowspan="2"><br />Kernel size</td><td colspan="2"><br />Kernel quantity</td><td rowspan="2">Accuracy /%</td></tr><tr><td><br />C1</td><td>C2</td></tr><tr><td rowspan="2"><br />3×3</td><td><br />30</td><td>10</td><td>92.5</td></tr><tr><td><br />100</td><td>10</td><td>96.0</td></tr><tr><td>5×5</td><td>100</td><td>10</td><td>98.5</td></tr><tr><td><br />8×8</td><td>100</td><td>10</td><td>99.5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="109" name="109" class="anchor-tag">4 分析与讨论</h3>
                <div class="p1">
                    <p id="110">为了验证算法的可行性, 本文在沪宁城际高速铁路建立了高速铁路周界侵限检测系统实验平台, 如图8所示。服务器放置在沪宁城际南京站联合机房内, 通过光纤网络通信获取线路视频。服务器之间通过千兆以太网交换机进行通信。报警服务器与车载终端之间通过4G网络通信。</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906014_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 高速铁路周界侵限检测系统组成结构示意图" src="Detail/GetImg?filename=images/GXXB201906014_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 高速铁路周界侵限检测系统组成结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906014_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Structural schematic of high-speed railway intrusion detecting system</p>

                </div>
                <div class="p1">
                    <p id="112">本文算法通过提取直行线路上行方向、下行方向、弯道和公跨铁桥梁等典型铁路场景并进行计算, 将其与MCG和FCN算法作对比, 实验平台采用统一配置, Intel i5处理器, 8 G内存, 不使用GPU显卡, 实验结果如图9所示, 其中标记区域为轨道区域。</p>
                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906014_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 不同算法识别轨道区域的结果对比图" src="Detail/GetImg?filename=images/GXXB201906014_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 不同算法识别轨道区域的结果对比图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906014_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Comparison diagrams of results of different algorithms for track area recognition</p>
                                <p class="img_note"> (a) 铁路场景; (b) 人工标记区域; (c) MCG算法结果; (d) FCN算法结果; (e) 所提算法结果</p>
                                <p class="img_note"> (a) Railway scenes; (b) manually labeled regions; (c) results of MCG algorithm; (d) results of FCN algorithm; (e) results of proposed algorithm</p>

                </div>
                <div class="p1">
                    <p id="114">不同算法实验结果分析如表3所示, 用Intersection over Union (IU) 表示实验结果与真值的交集与并集比例, 用Pixel Accuracy (PA) 表示属于真值中轨道区域的像素点被实验结果标记为轨道区域的比例;用Extra Pixel (EP) 表示真值中不属于轨道区域的像素点被实验结果标记为轨道区域的比例。</p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit">表3 不同算法实验结果对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Comparison of experimental results of different algorithms</p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td>Algorithm</td><td>Mean IU /%</td><td>Mean PA /%</td><td>Mean EP /%</td><td>Time /s</td><td>Net parameter quantity /10<sup>6</sup></td></tr><tr><td><br />MCG</td><td>72.05</td><td>79.94</td><td>10.63</td><td>7</td><td>—</td></tr><tr><td><br />FCN</td><td>89.83</td><td>91.26</td><td>16.20</td><td>41</td><td>134</td></tr><tr><td><br />Proposed algorithm</td><td>81.94</td><td>95.90</td><td>18.17</td><td>2.5</td><td>0.18</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="116">平均PA值越高意味着越少的轨道区域被漏掉 (表4第2行所示) , 带来越低的漏报率;平均EP值越高意味着越多的无关区域被监控 (表4第3行所示) , 带来越高的误报率。对高速铁路周界侵限检测系统来说, 误报警情况尚可通过2次处理来排除, 但漏报警情况却无法弥补, 将带来巨大的安全隐患, 因此在漏报与误报之间选择最高的PA值更具有实际意义。实验结果表明, 所提算法得到的像素级准确率PA值为最高的95.90% (与FCN算法相比增加了4.64%的准确区域, 仅增加了1.97%的错误区域, MCG算法的PA值太低) , 计算时间最短 (2.5 s) , 网络参数仅为0.18×10<sup>6</sup>个, 摆脱了对GPU显卡的依赖, 降低了系统成本, 便于移植进不同配置的数据处理平台。当高速铁路周界侵限检测系统增加监控相机或改变相机监控区域时, 所提算法能使系统快速准确地自动划分所要监控的轨道区域。</p>
                </div>
                <div class="area_img" id="117">
                    <p class="img_tit">表4 不同算法的漏报警区域与误报警区域 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Missing alarm area and false alarm area of different algorithms</p>
                    <p class="img_note"></p>
                    <table id="117" border="1"><tr><td><br />Algorithm</td><td>MCG</td><td>FCN</td><td>Proposed <br />algorithm</td></tr><tr><td><br />Result</td><td><img src="images\GXXB201906014_130.jpg" /></td><td><img src="images\GXXB201906014_131.jpg" /></td><td><img src="images\GXXB201906014_132.jpg" /></td></tr><tr><td><br />Missing area<br />Missing alarm</td><td><img src="images\GXXB201906014_133.jpg" /></td><td><img src="images\GXXB201906014_134.jpg" /></td><td><img src="images\GXXB201906014_135.jpg" /></td></tr><tr><td><br />Extra area<br />False alarm</td><td><img src="images\GXXB201906014_136.jpg" /></td><td><img src="images\GXXB201906014_137.jpg" /></td><td><img src="images\GXXB201906014_138.jpg" /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="118" name="118" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="119">所提出的高速铁路场景分割与识别算法结合了传统图像分割算法与卷积神经网络算法的优点, 充分利用铁路场景直线特征强烈、组成区域类别固定等特点, 对图像分割与识别算法在实际工程应用中遇到的一系列问题做出改进, 在边界精准度、识别准确率、计算时间和人工操作复杂度等多对矛盾中寻求有效的平衡点, 从而使铁路周界侵限检测系统能够自动、快速、准确地划定监控场景中轨道区域的周界范围, 提高了系统的工作效率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201702022&amp;v=MjI4NTN5am1WcnZCSWpYVGJMRzRIOWJNclk5SFpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> He F L, Guo Y C, Gao C.Improved PCNN method for human target infrared image segmentation under complex environments[J].Acta Optica Sinica, 2017, 37 (2) :0215003.贺付亮, 郭永彩, 高潮.复杂环境下用于人体目标红外图像分割的改进PCNN方法[J].光学学报, 2017, 37 (2) :0215003.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811016&amp;v=MTY3NjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeWptVnJ2QklqWFRiTEc0SDluTnJvOUVZb1FLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Wu C Y, Yi B S, Zhang Y G, <i>et al</i>.Retinal vessel image segmentation based on improved convolutional neural network[J].Acta Optica Sinica, 2018, 38 (11) :1111004.吴晨玥, 易本顺, 章云港, 等.基于改进卷积神经网络的视网膜血管图像分割[J].光学学报, 2018, 38 (11) :1111004.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201606022&amp;v=MTE0MDc0SDlmTXFZOUhab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeWptVnJ2QlBEelRiTEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Guo B Q, Yang L X, Shi H M, <i>et al</i>.High-speed railway clearance intrusion detection algorithm with fast background subtraction[J].Chinese Journal of Scientific Instrument, 2016, 37 (6) :1371-1378.郭保青, 杨柳旭, 史红梅, 等.基于快速背景差分的高速铁路异物侵入检测算法[J].仪器仪表学报, 2016, 37 (6) :1371-1378.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-speed railway clearance surveillance system based on convolutional neural networks">

                                <b>[4]</b> Wang Y, Yu Z J, Zhu L Q, <i>et al</i>.High-speed railway clearance surveillance system based on convolutional neural networks[J].Proceedings of SPIE, 2016, 10033:100335S.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SLIC Superpixels Compared to State-of-the-Art Superpixel Methods">

                                <b>[5]</b> Achanta R, Shaji A, Smith K, <i>et al</i>.SLIC superpixels compared to state-of-the-art superpixel methods[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (11) :2274-2282.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201707023&amp;v=MTcxODg2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeWptVnJ2QklqWFRiTEc0SDliTXFJOUhaNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Chen H Y, Qie L Z, Yang D D, <i>et al</i>.Visual background extraction algorithm based on superpixel information feedback[J].Acta Optica Sinica, 2017, 37 (7) :0715001.陈海永, 郄丽忠, 杨德东, 等.基于超像素信息反馈的视觉背景提取算法[J].光学学报, 2017, 37 (7) :0715001.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YSXT201701033&amp;v=Mjg0NDBQRDdUZXJHNEg5Yk1ybzlHWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlqbVZydkI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Liu Y C, Chen Y P, Zhang S S, <i>et al</i>.Traffic sign recognition based on pyramid histogram fusion descriptor and HIK-SVM[J].Journal of Transportation Systems Engineering and Information Technology, 2017, 17 (1) :220-226.刘亚辰, 陈跃鹏, 张赛硕, 等.融合式空间塔式算子和HIK-SVM的交通标志识别研究[J].交通运输系统工程与信息, 2017, 17 (1) :220-226.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YSXT201401009&amp;v=MzAzNTJGeWptVnJ2QlBEN1Rlckc0SDlYTXJvOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Fang Z P, Duan J M, Zheng B G.Traffic signs recognition and tracking based on feature color and SNCC algorithm[J].Journal of Transportation Systems Engineering and Information Technology, 2014, 14 (1) :47-52.房泽平, 段建民, 郑榜贵.基于特征颜色和SNCC的交通标志识别与跟踪[J].交通运输系统工程与信息, 2014, 14 (1) :47-52.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXCN201703029&amp;v=MjgzNzlCUFRYSVlMRzRIOWJNckk5SGJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5am1WcnY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Liu K P, Ying Z L, Zhai Y K, <i>et al</i>.SAR image target recognition based on unsupervised K-means feature and data augmentation[J].Journal of Signal Processing, 2017, 33 (3) :452-458.刘凯品, 应自炉, 翟懿奎, 等.基于无监督K均值特征和数据增强的SAR图像目标识别方法[J].信号处理, 2017, 33 (3) :452-458.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201602002&amp;v=MDIwMDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlqbVZydkJQeXJmYkxHNEg5Zk1yWTlGWm9RS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Zhang X D, Fan J L, Xu J, <i>et al</i>.Image super-resolution algorithm via K-means clustering and support vector data description[J].Journal of Image and Graphics, 2016, 21 (2) :135-144.张小丹, 范九伦, 徐健, 等.K均值聚类和支持向量数据描述的图像超分辨率算法[J].中国图象图形学报, 2016, 21 (2) :135-144.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201605049&amp;v=MjgwNDg2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeWptVnJ2Qkx6VFpaTEc0SDlmTXFvOUJiWVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Ma G Q, Tian Y C, Li X L.Application of K-means clustering algorithm in colour image segmentation of grouper in seawater background[J].Computer Applications and Software, 2016, 33 (5) :192-195.马国强, 田云臣, 李晓岚.K-均值聚类算法在海水背景石斑鱼彩色图像分割中的应用[J].计算机应用与软件, 2016, 33 (5) :192-195.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiscale Combinatorial Grouping">

                                <b>[12]</b> Arbeláez P, Pont-Tuset J, Barron J T, Marques F, <i>et al</i>.Multiscale combinatorial grouping[C]∥2004 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:328-335.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Contour Detection and Hierarchical Image Segmentation">

                                <b>[13]</b> Arbeláez P, Maire M, Fowlkes C, <i>et al</i>.Contour detection and hierarchical image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) :898-916.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Boundary extraction in natural images using ultrametric contour maps">

                                <b>[14]</b> Arbeláez P.Boundary extraction in natural images using ultrametric contour maps[C]//2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW′06) , June 17-22, 2006, New York, USA.New York:IEEE, 2006:182.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning hierarchical features for scene labeling">

                                <b>[15]</b> Farabet C, Couprie C, Najman L, <i>et al</i>.Learning hierarchical features for scene labeling[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (8) :1915-1929.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Indoor semantic segmentation using depth information">

                                <b>[16]</b> Couprie C, Farabet C, Najman L, <i>et al</i>.Indoor semantic segmentation using depth information[EB/OL]. (2013-03-14) [2019-01-25].https://arxiv.org/abs/1301.3572.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning rich features from RGB-D images for object detection and segmentation">

                                <b>[17]</b> Gupta S, Girshick R, Arbeláez P, <i>et al</i>.Learning rich features from RGB-D images for object detection and segmentation[M]//Fleet D, Pajdla T, Schiele B, <i>et al</i>.Computer Vision-ECCV 2014.Cham:Springer, 2014, 8695:345-360.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Analysis of compact features for RGB-D visual search">

                                <b>[18]</b> Petrelli A, Pau D, di Stefano L.Analysis of compact features for RGB-D visual search[M]∥Murino V, Puppo E.Image Analysis and Processing-ICIAP 2015.Cham:Springer, 2015, 9280:14-24.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[19]</b> Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902014&amp;v=MTgzODY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5am1WcnZCSWpYVGJMRzRIOWpNclk5RVlJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> Zheng T Y, Tang C, Lei Z K.Multi-scale retinal vessel segmentation based on fully convolutional neural network[J].Acta Optica Sinica, 2019, 39 (2) :0211002.郑婷月, 唐晨, 雷振坤.基于全卷积神经网络的多尺度视网膜血管分割[J].光学学报, 2019, 39 (2) :0211002.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201906014" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201906014&amp;v=MjkwMDg2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeWptVnJ2QklqWFRiTEc0SDlqTXFZOUVZSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNhZzVpTFRHQzdURWdCM25xWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

