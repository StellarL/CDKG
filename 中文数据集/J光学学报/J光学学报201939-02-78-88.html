

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135524973068750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201902010%26RESULT%3d1%26SIGN%3dZ%252f1HzIxbcyDRftcR2JFtB0hzw7c%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201902010&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201902010&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902010&amp;v=MjY3NDFyQ1VSN3FmWnVadEZpRGtVN3JKSWpYVGJMRzRIOWpNclk5RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#81" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#88" data-title="2 基本原理 ">2 基本原理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#89" data-title="&lt;b&gt;2.1&lt;/b&gt;&lt;b&gt;区域建议网络&lt;/b&gt;"><b>2.1</b><b>区域建议网络</b></a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;2.2&lt;/b&gt;&lt;b&gt;多视角模型&lt;/b&gt;"><b>2.2</b><b>多视角模型</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;2.3&lt;/b&gt;&lt;b&gt;联合学习网络模型&lt;/b&gt;"><b>2.3</b><b>联合学习网络模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#157" data-title="3 实验部分 ">3 实验部分</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#159" data-title="&lt;b&gt;3.1&lt;/b&gt;&lt;b&gt;性能评价指标&lt;/b&gt;"><b>3.1</b><b>性能评价指标</b></a></li>
                                                <li><a href="#168" data-title="&lt;b&gt;3.2&lt;/b&gt;&lt;b&gt;数据集建立&lt;/b&gt;"><b>3.2</b><b>数据集建立</b></a></li>
                                                <li><a href="#170" data-title="&lt;b&gt;3.3&lt;/b&gt;&lt;b&gt;训练网络模型&lt;/b&gt;"><b>3.3</b><b>训练网络模型</b></a></li>
                                                <li><a href="#175" data-title="&lt;b&gt;3.4&lt;/b&gt;&lt;b&gt;MVNN的切片分析&lt;/b&gt;"><b>3.4</b><b>MVNN的切片分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#191" data-title="4 分析与讨论 ">4 分析与讨论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#200" data-title="5 结 论 ">5 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#91" data-title="图1 区域建议网络模型结构图">图1 区域建议网络模型结构图</a></li>
                                                <li><a href="#98" data-title="图2 多视角样本。 (a) 正面; (b) 侧面; (c) 背面">图2 多视角样本。 (a) 正面; (b) 侧面; (c) 背面</a></li>
                                                <li><a href="#106" data-title="图3 MVNN模型框架">图3 MVNN模型框架</a></li>
                                                <li><a href="#107" data-title="图4 MVNN主体检测网络模型框架">图4 MVNN主体检测网络模型框架</a></li>
                                                <li><a href="#108" data-title="图5 输入层三通道示意图。 (a) 样本1; (b) 样本2">图5 输入层三通道示意图。 (a) 样本1; (b) 样本2</a></li>
                                                <li><a href="#115" data-title="图6 可变形层的部件分数计算模型">图6 可变形层的部件分数计算模型</a></li>
                                                <li><a href="#127" data-title="表1 所提算法中部件滤波器参数">表1 所提算法中部件滤波器参数</a></li>
                                                <li><a href="#173" data-title="表2 室内人员检测数据集">表2 室内人员检测数据集</a></li>
                                                <li><a href="#174" data-title="表3 第二阶段网络模型参数设置">表3 第二阶段网络模型参数设置</a></li>
                                                <li><a href="#179" data-title="图7 输入数据的区域建议结果比较。 (a) HOG特征结合Adaboost方法; (b) 所提区域建议算法">图7 输入数据的区域建议结果比较。 (a) HOG特征结合Adaboost方法; (b) 所提区域建......</a></li>
                                                <li><a href="#185" data-title="图8 多视角模型测试结果。 (a) 不同视角下测试样本的检测结果; (b) 单视角模型和多视角模型的检测效果对比">图8 多视角模型测试结果。 (a) 不同视角下测试样本的检测结果; (b) 单视角模型和多视角模型的......</a></li>
                                                <li><a href="#193" data-title="图9 DPM测试结果">图9 DPM测试结果</a></li>
                                                <li><a href="#194" data-title="图10 所提算法在IHDD上的测试结果。 (a) &lt;i&gt;R&lt;/i&gt;&lt;sub&gt;FPPI&lt;/sub&gt;-&lt;i&gt;R&lt;/i&gt;&lt;sub&gt;MR&lt;/sub&gt;曲线; (b) P-R曲线">图10 所提算法在IHDD上的测试结果。 (a) <i>R</i><sub>FPPI</sub>-<i>R</i><sub>MR</sub>曲线; (b) P-R曲线</a></li>
                                                <li><a href="#196" data-title="表4 不同算法在数据集上的定量比较">表4 不同算法在数据集上的定量比较</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Zou J H, Zhao Q C, Yang W, &lt;i&gt;et al&lt;/i&gt;. Occupancy detection in the office by analyzing surveillance videos and its application to building energy conservation[J]. Energy and Buildings, 2017, 152: 385-398." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Occupancy detection in the office by analyzing surveillance videos and its application to building energy conservation">
                                        <b>[1]</b>
                                         Zou J H, Zhao Q C, Yang W, &lt;i&gt;et al&lt;/i&gt;. Occupancy detection in the office by analyzing surveillance videos and its application to building energy conservation[J]. Energy and Buildings, 2017, 152: 385-398.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Benenson R, Omran M, Hosang J, &lt;i&gt;et al&lt;/i&gt;. Ten years of pedestrian detection, what have we learned?[C]. European Conference on Computer Vision, 2015: 613-627." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ten years of pedestrian detection, what have we learned?">
                                        <b>[2]</b>
                                         Benenson R, Omran M, Hosang J, &lt;i&gt;et al&lt;/i&gt;. Ten years of pedestrian detection, what have we learned?[C]. European Conference on Computer Vision, 2015: 613-627.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" Dalal N, Triggs B. Histograms of oriented gradients for human detection[C]. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005: 886-893." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">
                                        <b>[3]</b>
                                         Dalal N, Triggs B. Histograms of oriented gradients for human detection[C]. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005: 886-893.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Wang X Y, Han T X, Yan S C. An HOG-LBP human detector with partial occlusion handling[C]. IEEE 12th International Conference on Computer Vision, 2009: 32-39." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An HOG-LBP Human Detector with Partial Occlusion Handling">
                                        <b>[4]</b>
                                         Wang X Y, Han T X, Yan S C. An HOG-LBP human detector with partial occlusion handling[C]. IEEE 12th International Conference on Computer Vision, 2009: 32-39.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" Zhang J G, Huang K Q, Yu Y N, &lt;i&gt;et al&lt;/i&gt;. Boosted local structured HOG-LBP for object localization[C]. IEEE International Conference on Computer Vision and Pattern Recognition, 2011: 1393-1400." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Boosted local structured HOG-LBP for object localization">
                                        <b>[5]</b>
                                         Zhang J G, Huang K Q, Yu Y N, &lt;i&gt;et al&lt;/i&gt;. Boosted local structured HOG-LBP for object localization[C]. IEEE International Conference on Computer Vision and Pattern Recognition, 2011: 1393-1400.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Lowe D G. Distinctiveimage features from scale-invariant keypoints[J]. International Journal of Computer Vision, 2004, 60 (2) : 91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=Mjc3Nzg3QmFyTzRIdEhPcDR4RmJlc09ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZpdmtXNzdJSlY4PU5q&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Lowe D G. Distinctiveimage features from scale-invariant keypoints[J]. International Journal of Computer Vision, 2004, 60 (2) : 91-110.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" >
                                        <b>[7]</b>
                                     Lienhart R, Maydt J. An extended set of Haar-like features for rapid object detection[C]. International Conference on Image Processing, 2002: 900-903</a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                     Dollar P, Tu Z W, Perona P, &lt;i&gt;et al&lt;/i&gt;. Integral channel features[J]. Proceedings of the British Machine Vision Conference, 2009: 91.</a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     Dollar P, Appel R, Belongie S, &lt;i&gt;et al&lt;/i&gt;. Fast feature pyramids for object detection[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014, 36 (8) : 1532-1545.</a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" Zhang S S, Bauckhage C, Cremers A B. Informed haar-like features improve pedestrian detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 947-954." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Informed Haar-like features improve pedestrian detection">
                                        <b>[10]</b>
                                         Zhang S S, Bauckhage C, Cremers A B. Informed haar-like features improve pedestrian detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 947-954.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" Nam W, Doll&#225;r P, Han J H. Local decorrelation for improved detection[J]. Advances in Neural Information Processing Systems, 2014, 1: 424-432." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Local decorrelation for improved detection">
                                        <b>[11]</b>
                                         Nam W, Doll&#225;r P, Han J H. Local decorrelation for improved detection[J]. Advances in Neural Information Processing Systems, 2014, 1: 424-432.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" Zhang S S, Benenson R, Schiele B. Filtered channel features for pedestrian detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2015: 1751-1760." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Filtered channel features for pedestrian detection">
                                        <b>[12]</b>
                                         Zhang S S, Benenson R, Schiele B. Filtered channel features for pedestrian detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2015: 1751-1760.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" Sermanet P, Kavukcuoglu K, Chintala S, &lt;i&gt;et al&lt;/i&gt;. Pedestrian detection with unsupervised multi-stage feature learning[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2013: 3626-3633." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pedestrian detection with unsupervised multi-stage feature learning">
                                        <b>[13]</b>
                                         Sermanet P, Kavukcuoglu K, Chintala S, &lt;i&gt;et al&lt;/i&gt;. Pedestrian detection with unsupervised multi-stage feature learning[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2013: 3626-3633.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" Hosang J, Omran M, Benenson R, &lt;i&gt;et al&lt;/i&gt;. Taking a deeper look at pedestrians[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2015: 4073-4082." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Taking a Deeper Look at Pedestrians">
                                        <b>[14]</b>
                                         Hosang J, Omran M, Benenson R, &lt;i&gt;et al&lt;/i&gt;. Taking a deeper look at pedestrians[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2015: 4073-4082.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" Cai Z W, Saberian M, Vasconcelos N. Learning complexity-aware cascades for deep pedestrian detection[C]. IEEE International Conference on Computer Vision, 2015: 3361-3369." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Complexity-Aware Cascades for Deep Pedestrian Detection">
                                        <b>[15]</b>
                                         Cai Z W, Saberian M, Vasconcelos N. Learning complexity-aware cascades for deep pedestrian detection[C]. IEEE International Conference on Computer Vision, 2015: 3361-3369.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" Maji S, Berg A C, Malik J. Classification using intersection kernel support vector machines is efficient[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2008: 4587630." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Classification Using Intersection KernelSupport Vector Machines is efficient">
                                        <b>[16]</b>
                                         Maji S, Berg A C, Malik J. Classification using intersection kernel support vector machines is efficient[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2008: 4587630.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_17" >
                                        <b>[17]</b>
                                     Felzenszwalb P F, Girshick R B, McAllester D, &lt;i&gt;et al&lt;/i&gt;. Object detection with discriminatively trained part-based models[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (9) : 1627-1645.</a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_18" title=" Ouyang W L, Wang X G. Joint deep learning for pedestrian detection[C]. IEEE International Conference on Computer Vision, 2013: 2056-2063." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint deep learning for pedestrian detection">
                                        <b>[18]</b>
                                         Ouyang W L, Wang X G. Joint deep learning for pedestrian detection[C]. IEEE International Conference on Computer Vision, 2013: 2056-2063.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_19" title=" Girshick R, Donahue J, Darrell T, &lt;i&gt;et al&lt;/i&gt;. Rich feature hierarchies for accurate object detection and semantic segmentation[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic">
                                        <b>[19]</b>
                                         Girshick R, Donahue J, Darrell T, &lt;i&gt;et al&lt;/i&gt;. Rich feature hierarchies for accurate object detection and semantic segmentation[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 580-587.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_20" title=" Girshick R. Fast R-CNN[C]. IEEE International Conference on Computer Vision, 2015: 1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[20]</b>
                                         Girshick R. Fast R-CNN[C]. IEEE International Conference on Computer Vision, 2015: 1440-1448.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_21" >
                                        <b>[21]</b>
                                     Ren S Q, He K M, Girshick R, &lt;i&gt;et al&lt;/i&gt;. Faster R-CNN: Towards real-time object detection with region proposal networks[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) : 1137-1149.</a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_22" title=" Feng X Y, Mei W, Hu D S. Aerialtarget detection based on improved faster R-CNN [J]. Acta Optica Sinica, 2018, 38 (6) : 0615004. 冯小雨, 梅卫, 胡大帅. 基于改进Faster R-CNN的空中目标检测[J]. 光学学报, 2018, 38 (6) : 0615004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806034&amp;v=MTkxMzc3ckpJalhUYkxHNEg5bk1xWTlHWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEa1U=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         Feng X Y, Mei W, Hu D S. Aerialtarget detection based on improved faster R-CNN [J]. Acta Optica Sinica, 2018, 38 (6) : 0615004. 冯小雨, 梅卫, 胡大帅. 基于改进Faster R-CNN的空中目标检测[J]. 光学学报, 2018, 38 (6) : 0615004.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_23" title=" Redmon J, Divvala S, Girshick R, &lt;i&gt;et al&lt;/i&gt;. You only look once: unified, real-time object detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 779-788." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You Only Look Once:Unified,Real-Time Object Detection">
                                        <b>[23]</b>
                                         Redmon J, Divvala S, Girshick R, &lt;i&gt;et al&lt;/i&gt;. You only look once: unified, real-time object detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 779-788.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_24" >
                                        <b>[24]</b>
                                     Liu W, Anguelov D, Erhan D, &lt;i&gt;et al&lt;/i&gt;. SSD: single shot multibox detector[C]. European Conference on Computer Vision, 2016: 21-37.</a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_25" title=" Liu H, Peng L, Wen J W. Multi-scale aware pedestrian detection algorithm based on improved full convolutional network[J]. Laser &amp;amp; Optoelectronics Progress, 2018, 55 (9) : 091504. 刘辉, 彭力, 闻继伟. 基于改进全卷积网络的多尺度感知行人检测算法[J]. 激光与光电子学进展, 2018, 55 (9) : 091504." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201809039&amp;v=MjAzNDk5bk1wbzlHYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEa1U3ckpMeXJQWkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         Liu H, Peng L, Wen J W. Multi-scale aware pedestrian detection algorithm based on improved full convolutional network[J]. Laser &amp;amp; Optoelectronics Progress, 2018, 55 (9) : 091504. 刘辉, 彭力, 闻继伟. 基于改进全卷积网络的多尺度感知行人检测算法[J]. 激光与光电子学进展, 2018, 55 (9) : 091504.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_26" title=" Enzweiler M, Eigenstetter A, Schiele B, &lt;i&gt;et al&lt;/i&gt;. Multi-cue pedestrian classification with partial occlusion handling[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2010: 990-997." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-cue pedestrian classification with partial occlusion handling">
                                        <b>[26]</b>
                                         Enzweiler M, Eigenstetter A, Schiele B, &lt;i&gt;et al&lt;/i&gt;. Multi-cue pedestrian classification with partial occlusion handling[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2010: 990-997.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_27" title=" Ouyang W L, Wang X G. A discriminative deep model for pedestrian detection with occlusion handling[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2012: 3258-3265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A discriminative deep model for pedestrian detection with occlusion handling">
                                        <b>[27]</b>
                                         Ouyang W L, Wang X G. A discriminative deep model for pedestrian detection with occlusion handling[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2012: 3258-3265.
                                    </a>
                                </li>
                                <li id="65">


                                    <a id="bibliography_28" title=" Uijlings J R R, van de Sande K E A, Gevers T, &lt;i&gt;et al&lt;/i&gt;. Selective search for object recognition[J]. International Journal of Computer Vision, 2013, 104 (2) : 154-171." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13080200013634&amp;v=MDE3NDRUTW53WmVadEZpbmxVcnpKS0Y4VWJ4TT1OajdCYXJLN0h0bk1yWTlGWk9vTUNuODlvQk1UNlQ0UFFIL2lyUmRHZXJxUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                         Uijlings J R R, van de Sande K E A, Gevers T, &lt;i&gt;et al&lt;/i&gt;. Selective search for object recognition[J]. International Journal of Computer Vision, 2013, 104 (2) : 154-171.
                                    </a>
                                </li>
                                <li id="67">


                                    <a id="bibliography_29" title=" Lawrence Zitnick C, Doll&#225;r P. Edge boxes: locating object proposals from edges[C]. European Conference on Computer Vision, 2014: 391-405." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locating object proposals from edges">
                                        <b>[29]</b>
                                         Lawrence Zitnick C, Doll&#225;r P. Edge boxes: locating object proposals from edges[C]. European Conference on Computer Vision, 2014: 391-405.
                                    </a>
                                </li>
                                <li id="69">


                                    <a id="bibliography_30" title=" Ouyang W L, Wang X G. Single-pedestrian detection aided by multi-pedestrian detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2013: 3198-3205." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-pedestrian detection aided by multi-pedestrian detection">
                                        <b>[30]</b>
                                         Ouyang W L, Wang X G. Single-pedestrian detection aided by multi-pedestrian detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2013: 3198-3205.
                                    </a>
                                </li>
                                <li id="71">


                                    <a id="bibliography_31" title=" Luo P, Tian Y L, Wang X G, &lt;i&gt;et al&lt;/i&gt;. Switchable deep network for pedestrian detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 899-906." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Switchable deep network for pedestrian detection">
                                        <b>[31]</b>
                                         Luo P, Tian Y L, Wang X G, &lt;i&gt;et al&lt;/i&gt;. Switchable deep network for pedestrian detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 899-906.
                                    </a>
                                </li>
                                <li id="73">


                                    <a id="bibliography_32" title=" Zhang S S, Benenson R, Omran M, &lt;i&gt;et al&lt;/i&gt;. Towards reaching human performance in pedestrian detection[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (4) : 973-986." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards reaching human performance in pedestrian detection">
                                        <b>[32]</b>
                                         Zhang S S, Benenson R, Omran M, &lt;i&gt;et al&lt;/i&gt;. Towards reaching human performance in pedestrian detection[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (4) : 973-986.
                                    </a>
                                </li>
                                <li id="75">


                                    <a id="bibliography_33" title=" Hosang J, Benenson R, Dollar P, &lt;i&gt;et al&lt;/i&gt;. What makes for effective detection proposals?[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (4) : 814-830." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=What makes for effective detection proposals?">
                                        <b>[33]</b>
                                         Hosang J, Benenson R, Dollar P, &lt;i&gt;et al&lt;/i&gt;. What makes for effective detection proposals?[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (4) : 814-830.
                                    </a>
                                </li>
                                <li id="77">


                                    <a id="bibliography_34" title=" Dollar P, Wojek C, Schiele B, &lt;i&gt;et al&lt;/i&gt;. Pedestrian detection: A benchmark[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2009: 304-311." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pedestrian detection:A benchmark">
                                        <b>[34]</b>
                                         Dollar P, Wojek C, Schiele B, &lt;i&gt;et al&lt;/i&gt;. Pedestrian detection: A benchmark[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2009: 304-311.
                                    </a>
                                </li>
                                <li id="79">


                                    <a id="bibliography_35" title=" Everingham M, van Gool L, Williams C K I, &lt;i&gt;et al&lt;/i&gt;. The pascal visual object classes (VOC) challenge[J]. International Journal of Computer Vision, 2010, 88 (2) : 303-338." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MjU3MTRadUZpdmtXNzdJSlY4PU5qN0Jhck80SHRIUHFZZEhZK0lMWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZFor&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[35]</b>
                                         Everingham M, van Gool L, Williams C K I, &lt;i&gt;et al&lt;/i&gt;. The pascal visual object classes (VOC) challenge[J]. International Journal of Computer Vision, 2010, 88 (2) : 303-338.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-09-25 07:25</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(02),78-88 DOI:10.3788/AOS201939.0210002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于联合学习的多视角室内人员检测网络</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E9%9C%9E&amp;code=08900898&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王霞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%B8%BA&amp;code=09966565&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张为</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A9%E6%B4%A5%E5%A4%A7%E5%AD%A6%E7%94%B5%E6%B0%94%E8%87%AA%E5%8A%A8%E5%8C%96%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0246359&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">天津大学电气自动化与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A9%E6%B4%A5%E5%A4%A7%E5%AD%A6%E5%BE%AE%E7%94%B5%E5%AD%90%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">天津大学微电子学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>建立了室内人员检测数据集 (IHDD) , 提出了基于联合学习的多视角室内人员检测网络模型 (MVNN) 。该模型由输入数据层、特征提取层、可变形处理层、可见性估计层、分类判别层等组成, 并加入区域建议模型和多视角模型以提升算法的检测性能。在自建的IHDD数据集上的实验结果表明, 与现有其他检测算法相比, MVNN算法的检测率更高;在人体目标呈现多视角、多姿态及存在遮挡等困难情况下仍有不错的检测效果, 具有一定的理论研究价值和实际应用价值。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%A4%E5%86%85%E4%BA%BA%E5%91%98%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">室内人员检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E8%A7%86%E8%A7%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多视角;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%94%E5%90%88%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">联合学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E9%A2%91%E7%9B%91%E6%8E%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视频监控;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *王霞, E-mail:1257833489@qq.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-06</p>

                    <p>

                            <b>基金：</b>
                                                        <span>公安部技术研究计划竞争性遴选项目 (2016JSYJD04-O3);</span>
                                <span>火灾调查视频图像分析关键技术研究 (2017JSYJC35);</span>
                    </p>
            </div>
                    <h1>Multi-View Indoor Human Detection Neural Network Based on Joint Learning</h1>
                    <h2>
                    <span>Wang Xia</span>
                    <span>Zhang Wei</span>
            </h2>
                    <h2>
                    <span>School of Electrical Automation and Information Engineering, Tianjin University</span>
                    <span>School of Microelectronics, Tianjin University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>An indoor human detection dataset (IHDD) is established, and a novel multi-view indoor human detection neural network (MVNN) based on joint learning is proposed. The model consists of input data layer, feature extraction layer, deformation layer, visibility reasoning layer and classification layer, and the proposed MVNN algorithm can improve the detection performance when combined with the region proposal model and the multi-view model. Experimental results on the self-built IHDD show that compared with other existing detection algorithms, the proposed MVNN algorithm has a higher detection rate. It can still obtain good detection results even in the case of difficult situations such as various views, changing poses and occlusion for human targets, which indicates certain theoretical research value and practical value.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=indoor%20human%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">indoor human detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-view&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-view;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=joint%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">joint learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=video%20surveillance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">video surveillance;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-08-06</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="81" name="81" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="82">随着生活水平的不断提高, 人们的安防意识也不断增强, 视频监控设备也被广泛应用到生产和生活中, 伴随计算机视觉技术的不断发展, 智能视频监控应运而生。目标检测, 尤其人体目标检测是智能视频监控领域中一项必不可少的研究课题, 在自动驾驶、智能机器人、行为分析等方面应用广泛<citation id="203" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 尤其在一些特殊场所, 如值班室、消防控制室、军事哨所等, 实时检测室内是否有人在岗对于保障人们的生命财产安全意义重大。</p>
                </div>
                <div class="p1">
                    <p id="83">国内外有大量人体目标检测, 尤其是行人检测的相关研究<citation id="204" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。一般而言, 人体目标检测框架主要包括特征提取和分类判别两个部分。特征提取部分可分为两大类。第一类是人工设计的传统特征, 用来描述颜色、边缘、纹理、深度及梯度信息等, 有时会添加运动信息、上下文信息等。最早的人工设计的传统特征以Dalal等<citation id="205" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>设计的方向梯度直方图 (HOG) 特征为代表, 描述目标的边缘信息, 可以适应目标的微小形变, 成为后续研究的基础。之后有人将其与描述纹理信息的局部二值模式 (LBP) 特征<citation id="219" type="reference"><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>、尺度不变特征变换 (SIFT) 特征<citation id="206" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、描述输入图像矩形特征的Haar-like特征<citation id="207" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>等相结合。之后人工设计的传统特征则主要以Dollar等<citation id="208" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出的ICF (Integral Channel Features) 为代表, 提出三种通道, 即LUV颜色通道 (L表示亮度, U和V是色度) 、归一化梯度幅值通道和梯度直方图通道。后续一些方法也是基于这种思想, 比如ACF (Aggregate Channel Features) <citation id="209" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、Informed Haar特征<citation id="210" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、LDCF (Locally Decorrelated Channel Features) 特征<citation id="211" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、Checkerboards特征<citation id="212" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等。这类提取手工设计特征的方法主要依据行人外形, 并不能很好地区分具有相似外形特征的背景目标。第二类是提取神经网络特征的方法<citation id="220" type="reference"><link href="35" rel="bibliography" /><link href="37" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>。此外, 还有将两种特征结合的方法, 如Cai等<citation id="213" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>将传统特征与深度网络特征相结合以提升检测性能。分类判别方法首先从HOG特征+支持向量机 (SVM) 框架<citation id="214" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>中的SVM开始, 采用线性与非线性核函数 (如线性SVM、直方图交叉核SVM<citation id="215" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、latent SVM<citation id="216" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>等) 进行分类。后来出现了一些利用统计概率思想的分类方法, 如基于提升思想的Adaboost (Adaptive Boosting) 方法等, 这些方法对环境中的噪声等非目标因素较为敏感。随着相关技术的不断发展, 出现了一些基于神经网络模型进行分类判别的方法, 如ChnFtrs<citation id="217" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、JointDeep<citation id="218" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation> 等方法将多个模块进行集成, 以实现最终的分类判别。</p>
                </div>
                <div class="p1">
                    <p id="84">随着深度学习的不断发展, 研究人员开始转向利用神经网络实现目标检测方法的研究, 这种方法大致可以分为两大类:一类是以R-CNN (Regions with CNN features) <citation id="221" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>为代表的两阶段网络模型, 如Fast R-CNN<citation id="222" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、Faster R-CNN<citation id="226" type="reference"><link href="51" rel="bibliography" /><link href="53" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>等;另一类是单阶段模型, 以YOLO (You Only Look Once) 系列<citation id="223" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、SSD (Single Shot MultiBox Detector) <citation id="224" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>等为典型代表。这些网络模型最初都是针对二分类或多分类问题提出的, 将这些网络模型用于人的检测效果并不理想, 而且, 其中大部分方法都是用于室外开阔环境中处于直立状态的行人检测<citation id="225" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>, 图像采集范围广、距离远, 人体目标比较完整, 且基本处于站姿或走动状态, 呈现正面或背面视角。室内人体目标检测的情形则与之不同:采集图像的范围小, 距离近, 目标存在一定倾角, 大部分时候仅部分人体躯干可见, 基本上呈现的都是坐姿, 少量为站姿, 且存在正面、背面、侧面等多种视角, 多种姿态变化以及不同程度的变形与遮挡, 受到复杂背景等因素的干扰等。为此, 研究人员提出了一些方法来解决这些问题。</p>
                </div>
                <div class="p1">
                    <p id="85">针对多变外形、多姿态的问题, 主要以可变形部件模型 (DPM) 方法<citation id="227" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>为代表, 在整体模型的基础上集成了部件模型并考虑其变形因素, 在一定程度上提升了检测性能;还有一些方法对部件的旋转、大小变化、形状等进行建模以应对更复杂的变形问题<citation id="228" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>, 但基本上都是利用传统特征。针对遮挡问题, 一些方法对遮挡进行推理, 以部件图像块的检测分数为基础估计部件可见性<citation id="230" type="reference"><link href="17" rel="bibliography" /><link href="63" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">27</a>]</sup></citation>;还有一些方法利用其他手段, 如进行图像分割处理或提取深度信息等辅助判断<citation id="229" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>, 有时也加入运动信息、上下文信息、帧间信息等, 但都未考虑与模型其他部分之间的联系。</p>
                </div>
                <div class="p1">
                    <p id="86">除了检测性能之外, 检测速度也是一个重要考虑因素, 主要体现在图像区域预选取阶段。最早的方法是对图像进行密集式的滑动窗口遍历搜索<citation id="231" type="reference"><link href="65" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>, 然后提取特征进行检测, 计算复杂、耗时长, 对分类器性能要求高。后来, 区域建议的方法陆续出现, 将目标转向重点关注区域, 大大降低了计算成本, 提升了分类、检测等后续阶段的效率。一般可以将其分为两类:一类基于区域分组的思想, 以Selective Search<citation id="232" type="reference"><link href="65" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>为代表;一类以窗口得分为基准, 以Edge Boxes<citation id="233" type="reference"><link href="67" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>为代表, 后者的速度更快。之后, 很多网络模型开始采用这种区域建议的思想, 如DBN-Mut模型使用DPM进行区域建议<citation id="234" type="reference"><link href="69" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>;JointDeep<citation id="235" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、SDN (Switchable Deep Network) <citation id="236" type="reference"><link href="71" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>采用HOG+CSS (Color Self-Similarity) 特征结合线性SVM的方法进行区域建议;还有不少网络模型使用ICF特征检测算子进行区域建议<citation id="237" type="reference"><link href="73" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>;Faster R-CNN<citation id="238" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>则基于一个网络模型实现区域建议。研究表明, 除了提升算法效率之外, 区域建议对检测性能也有一定的提升作用<citation id="239" type="reference"><link href="75" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="87">基于以上研究基础, 针对监控条件下的室内人体目标检测, 本文提出了一种基于联合学习思想的多视角室内人员检测网络模型 (MVNN) 。该模型包括两大部分, 即运用区域建议和多视角模型的预处理部分和多模块联合学习的检测网络部分。其中, 区域建议部分可对输入图像进行预处理, 排除大量的非目标区域;多视角模型有效提升了对多视角人体目标的召回率, 提升网络的整体检测性能;多模块联合学习的检测网络可实现特征提取、可变形处理, 并可对可见性进行估计, 得到最终的分类结果。实验结果表明, 所提出的算法模型比其他算法模型具有更高的准确率, 尤其在处理变形与遮挡情况和多视角人体检测方面的性能更优。</p>
                </div>
                <h3 id="88" name="88" class="anchor-tag">2 基本原理</h3>
                <h4 class="anchor-tag" id="89" name="89"><b>2.1</b><b>区域建议网络</b></h4>
                <div class="p1">
                    <p id="90">分析采集的视频片段, 主要对人体上半身进行标注。借鉴Faster R-CNN<citation id="240" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>模型中的区域建议网络部分, 网络结构 (基于VGG模型) 示意如图1所示。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902010_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 区域建议网络模型结构图" src="Detail/GetImg?filename=images/GXXB201902010_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 区域建议网络模型结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902010_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Architecture of region proposal network</p>

                </div>
                <div class="p1">
                    <p id="92">图1中, input data表示输入图像数据;Conv表示卷积层;Relu为修正线性单元, 是人工神经网络中常用的一种激活函数, 可在一定程度上避免神经网络中容易出现的梯度消失等问题;pool为进行pooling运算的池化层, 常常接在卷积层之后, 可以有效地减少运算参数, 提升神经网络的整体运算性能;Conv1_1与Conv1_2表示第一层级中的两个参数相同的卷积层, 同理类推, 共有5种卷积层, 最终得到类别检测分数和目标的空间位置信息;cls_score表示类别检测分数结果;bbox_predict表示目标边界框坐标信息的预测结果。</p>
                </div>
                <div class="p1">
                    <p id="93">该网络通过提取图像CNN特征预提取有较大概率存在目标对象的图像块。实验结果表明, 与原算法<citation id="241" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>中预处理阶段的HOG+CSS特征结合SVM的算法模型相比, 使用该区域建议网络进行预提取具有更高的召回率。通常采用非最大值抑制 (NMS) 方法对区域建议结果进行进一步处理。依据IoU (Intersection Over Union) 指标, 可用交并比<i>R</i><sub>IoU</sub>描述预测框BB<sub>1</sub>与真实框BB<sub>2</sub>之间的关系, 即两者相交面积与集合面积的比值, 计算公式为</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>o</mtext><mtext>U</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>A</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mo>_</mo><mn>1</mn></mrow></msub></mrow><mrow><mi>A</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mo>_</mo><mn>2</mn></mrow></msub></mrow></mfrac><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">式中:<i>A</i><sub>area_1</sub>表示两者的相交面积;<i>A</i><sub>area_2</sub>表示两者的集合面积。将交并比<i>R</i><sub>IoU</sub>大于0.5的建议框预测为可能含有目标的预测框。设定阈值, 从最高分预测框开始对所有的预测框进行NMS处理。最后按照分数从高到低的顺序保存这些区域建议块的坐标信息、可信度分数、类别标签及对应的图像块。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96"><b>2.2</b><b>多视角模型</b></h4>
                <div class="p1">
                    <p id="97">设计了三视角 (包括正面、背面、侧面三种视角) 模型。由于监控摄像头的安装位置固定, 目标会存在一定倾角, 同时呈现出多种姿态, 尤其是侧面视角, 且每种视角又存在不同程度的变形, 多视角模型比单视角模型更适合这种应用环境, 部分样本如图2所示。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902010_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 多视角样本。 (a) 正面; (b) 侧面; (c) 背面" src="Detail/GetImg?filename=images/GXXB201902010_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 多视角样本。 (a) 正面; (b) 侧面; (c) 背面  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902010_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Multi-view samples. (a) Frontal view; (b) profile view; (c) back view</p>

                </div>
                <div class="p1">
                    <p id="99">这些多视角样本分为三类, 包括正面、侧面、背面三种视角的人体目标。图2 (a) 为正面视角样例, 这种情况较少, 因为摄像头的安装位置常在与桌椅成对角线的屋顶角落里, 一般较难从正面拍摄到人脸;图2 (b) 为侧面视角样例, 这种情形最多;图2 (c) 为背面视角样例, 整个身体完全处于背面状态的情形并不多。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100"><b>2.3</b><b>联合学习网络模型</b></h4>
                <div class="p1">
                    <p id="101">联合学习网络主要包括特征提取、可变形处理、可见性估计、分类判定模块, 网络模型框架如图3所示。该网络模型的设计借鉴JointDeep模型<citation id="242" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation> (灰色部分为原有模块, 蓝色部分为重新设计的模块) , 主要区别包括:预处理阶段的区域建议模型和多视角模型, MVNN数据输入层以及可变形处理层。</p>
                </div>
                <div class="p1">
                    <p id="102">区域建议网络和多视角模型部分主要是对大量的原始图像数据做预处理, 提取最可能包含人体目标的候选框, 减少后面网络层的计算量, 且保证了较高的召回率。MVNN数据输入层的作用是将区域建议图像块做进一步处理, 经过特征提取, 添加适合本应用的可变形处理层, 以应对遮挡及多视角、多姿态的情况, 提升检测率和召回率。</p>
                </div>
                <div class="p1">
                    <p id="103">在该基础模型上进行调整, 使之适合本应用环境, 主体网络结构及参数设置如图4所示。输入层样本大小为84×84, 后面部件映射层采用15种部件滤波器与前面特征映射层进行卷积, 得到对应的部件检测分数, 再经过可见性估计得到预测分数, 根据预测分数判定是否为正类。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">2.3.1 图像特征提取模块</h4>
                <div class="p1">
                    <p id="105">将区域建议图像块作为输入, 提取其颜色通道特征和边缘信息, 最终以三通道图像的形式实现对图像信息的多层次表达。具体来说, 第一通道是原图的Y通道;第二通道是由四个图像块拼接而成, 包括U通道、V通道、Y通道和全零图像块;第三通道是计算前面的U、V、Y三通道图像块边缘信息并选择其最大值。三个通道都归一化到零均值-单位方差的分布, 然后综合三通道信息描述图像, 图5给出两个样例。</p>
                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902010_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 MVNN模型框架" src="Detail/GetImg?filename=images/GXXB201902010_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 MVNN模型框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902010_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Architecture of MVNN</p>

                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902010_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 MVNN主体检测网络模型框架" src="Detail/GetImg?filename=images/GXXB201902010_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 MVNN主体检测网络模型框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902010_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Principal detection neural network model of MVNN</p>

                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902010_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 输入层三通道示意图。 (a) 样本1; (b) 样本2" src="Detail/GetImg?filename=images/GXXB201902010_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 输入层三通道示意图。 (a) 样本1; (b) 样本2  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902010_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Three channels of input layer. (a) Sample 1; (b) sample 2</p>

                </div>
                <div class="p1">
                    <p id="109">图5中, 对应每个样本的第一列是原图, 第一通道信息并未在图中显示, 第二、三列分别对应第二、三通道。此时, 将其输入网络中的卷积层, 进一步提取特征信息, 并进行可变形处理、可见性估计及分类等。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110">2.3.2 目标可变形处理模块</h4>
                <div class="p1">
                    <p id="111">室内环境中, 由于监控摄像头安装位置固定, 目标人员在监控视野中存在一定倾角, 而且人的状态不定, 呈现出多种视角, 如背面、正面和侧面视角等, 有时也会出现一些复杂情形, 如人突然起身走动、弯腰捡物品、伸展双臂等, 不同于通常的坐姿状态, 这些情形给检测造成困难。为此, 设计一种与之适应的DPM, 可以在一定程度上提高检测率。</p>
                </div>
                <div class="p1">
                    <p id="112">部件模型源于DPM<citation id="243" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>的思想, 主要是将原图像目标看成两个层次, 即整体层次和分部件层次, 综合两层次得到最终结果, 包括检测分数及位置信息。</p>
                </div>
                <div class="p1">
                    <p id="113">如图6所示, 向可变形层输入<i>p</i>个部件映射图, 对应输出<i>p</i>个部件分数<i>s</i>={<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>p</i></sub>}, <i>p</i>=15。第<i>p</i>个部件的综合检测分数为</p>
                </div>
                <div class="p1">
                    <p id="114" class="code-formula">
                        <mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi><msub><mrow></mrow><mi>p</mi></msub><mo>=</mo><mi>Μ</mi><msub><mrow></mrow><mi>p</mi></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>c</mi></mstyle><msub><mrow></mrow><mrow><mi>n</mi><mo>, </mo><mi>p</mi></mrow></msub><mi>D</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>, </mo><mi>p</mi></mrow></msub><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902010_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 可变形层的部件分数计算模型" src="Detail/GetImg?filename=images/GXXB201902010_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 可变形层的部件分数计算模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902010_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Calculation model of part score for deformation layer</p>

                </div>
                <div class="p1">
                    <p id="116">式中:<i>M</i><sub><i>p</i></sub>为检测分数;<i>D</i><sub><i>n</i>, <i>p</i></sub>为第<i>p</i>个部件对应的第<i>n</i>个变形映射分数;<i>c</i><sub><i>n</i>, <i>p</i></sub>为第<i>p</i>个部件的权重系数, <i>c</i><sub><i>n</i>, <i>p</i></sub>和<i>D</i><sub><i>n</i>, <i>p</i></sub>是需要学习的关键参数;<i>N</i>为变形映射的个数, <i>n</i>从1到<i>N</i>取值。第<i>p</i>个部件的最终检测分数为<i>s</i><sub><i>p</i></sub>, 由预测到的所有部件位置[用部件中心坐标 (<i>x</i>, <i>y</i>) 表示]上的部件综合检测分数<i>B</i><sub><i>p</i></sub>决定, 再对<i>B</i><sub><i>p</i></sub>进行全局最大池化计算, 得到的最大值即为最终部件检测分数<i>s</i><sub><i>p</i></sub>, 取得最大检测分数<i>s</i><sub><i>p</i></sub>的位置 (<i>x</i>, <i>y</i>) <sub><i>p</i></sub>即为部件预测位置, 计算公式分别为</p>
                </div>
                <div class="p1">
                    <p id="117" class="code-formula">
                        <mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>s</mi><msub><mrow></mrow><mi>p</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></munder><mspace width="0.25em" /><mi>b</mi><msubsup><mrow></mrow><mi>p</mi><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mi>p</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></munder><mspace width="0.25em" /><mi>b</mi><msubsup><mrow></mrow><mi>p</mi><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="118">式中:<i>b</i><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>p</mi><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>为B<sub>p</sub>中对应坐标 (x, y) 处的部件检测分数, 其计算公式为</p>
                </div>
                <div class="p1">
                    <p id="120" class="code-formula">
                        <mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>b</mi><msubsup><mrow></mrow><mi>p</mi><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mi>m</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msup><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mrow><mo> (</mo><mrow><mi>x</mi><mo>-</mo><mi>a</mi><msub><mrow></mrow><mi>x</mi></msub><mo>+</mo><mfrac><mrow><mi>c</mi><msub><mrow></mrow><mn>3</mn></msub></mrow><mrow><mn>2</mn><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mrow><mo> (</mo><mrow><mi>y</mi><mo>-</mo><mi>a</mi><msub><mrow></mrow><mi>y</mi></msub><mo>+</mo><mfrac><mrow><mi>c</mi><msub><mrow></mrow><mn>4</mn></msub></mrow><mrow><mn>2</mn><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="121">式中:m<sup> (x, y) </sup>是M<sub>p</sub>中 (x, y) 位置的元素, 代表中心坐标位于 (x, y) 位置的部件初始检测分数; (a<sub>x</sub>, a<sub>y</sub>) 是第p个部件的原定目标位置, 通过c<sub>3</sub>/2c<sub>1</sub>和c<sub>4</sub>/2c<sub>2</sub>对该位置进行调整;c<sub>1</sub>和c<sub>2</sub>决定变形损失。结合 (2) 式和 (5) 式, 可得</p>
                </div>
                <div class="p1">
                    <p id="122" class="code-formula">
                        <mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi><msub><mrow></mrow><mi>p</mi></msub><mo>=</mo><mi>Μ</mi><msub><mrow></mrow><mi>p</mi></msub><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mi>D</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>‚</mo><mi>p</mi></mrow></msub><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mi>D</mi><msub><mrow></mrow><mrow><mn>2</mn><mo>, </mo><mi>p</mi></mrow></msub><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>3</mn></msub><mi>D</mi><msub><mrow></mrow><mrow><mn>3</mn><mo>, </mo><mi>p</mi></mrow></msub><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>4</mn></msub><mi>D</mi><msub><mrow></mrow><mrow><mn>4</mn><mo>, </mo><mi>p</mi></mrow></msub><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>5</mn></msub><mo>⋅</mo><mn>1</mn><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="123">式中:d<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>是D<sub>n, p</sub> (n=1～4) 中坐标为 (x, y) 位置处的元素;c<sub>1</sub>～c<sub>4</sub>均是需要学习的参数, c<sub>5</sub>是一个定值。一些变量的具体计算式为</p>
                </div>
                <div class="p1">
                    <p id="125" class="code-formula">
                        <mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>b</mi><msubsup><mrow></mrow><mi>p</mi><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mi>m</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msup><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mi>d</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mi>d</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>3</mn></msub><mi>d</mi><msubsup><mrow></mrow><mn>3</mn><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>4</mn></msub><mi>d</mi><msubsup><mrow></mrow><mn>4</mn><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>5</mn></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>d</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mi>x</mi><mo>-</mo><mi>a</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mspace width="0.25em" /><mi>d</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mi>y</mi><mo>-</mo><mi>a</mi><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mspace width="0.25em" /><mi>d</mi><msubsup><mrow></mrow><mn>3</mn><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mi>x</mi><mo>-</mo><mi>a</mi><msub><mrow></mrow><mi>x</mi></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>d</mi><msubsup><mrow></mrow><mn>4</mn><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mi>y</mi><mo>-</mo><mi>a</mi><msub><mrow></mrow><mi>y</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>c</mi><msub><mrow></mrow><mn>5</mn></msub><mo>=</mo><mi>c</mi><msubsup><mrow></mrow><mn>3</mn><mn>2</mn></msubsup><mo>/</mo><mo stretchy="false"> (</mo><mn>4</mn><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>+</mo><mi>c</mi><msubsup><mrow></mrow><mn>4</mn><mn>2</mn></msubsup><mo>/</mo><mo stretchy="false"> (</mo><mn>4</mn><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="126">结合文献<citation id="244" type="reference">[<a class="sup">18</a>]</citation>中的部件模型, 共设计了15种部件滤波器, 参数设置如表1所示。</p>
                </div>
                <div class="area_img" id="127">
                    <p class="img_tit">表1 所提算法中部件滤波器参数 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Table</i> 1 <i>Parameters of part filters in proposed algorithm</i></p>
                    <p class="img_note"></p>
                    <table id="127" border="1"><tr><td><i>Parameter</i></td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td></tr><tr><td><br /><i>Starting line</i></td><td>1</td><td>1</td><td>4</td><td>4</td><td>1</td><td>1</td><td>1</td><td>4</td><td>1</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>1</td><td>1</td></tr><tr><td><br /><i>Ending line</i></td><td>3</td><td>3</td><td>9</td><td>9</td><td>3</td><td>9</td><td>9</td><td>9</td><td>3</td><td>9</td><td>9</td><td>9</td><td>9</td><td>9</td><td>9</td></tr><tr><td><br /><i>Starting column</i></td><td>1</td><td>3</td><td>1</td><td>3</td><td>1</td><td>1</td><td>4</td><td>1</td><td>1</td><td>1</td><td>1</td><td>4</td><td>1</td><td>1</td><td>1</td></tr><tr><td><br /><i>Ending column</i></td><td>3</td><td>5</td><td>3</td><td>5</td><td>5</td><td>2</td><td>5</td><td>5</td><td>5</td><td>5</td><td>2</td><td>5</td><td>5</td><td>5</td><td>5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="128">使用4个变量来定义部件滤波器的对应位置, 即起始行、结束行、起始列和结束列。每列代表一种部件滤波器, 假设目标整体尺寸大小是9×5, 因此行的范围是1～9 , 列的范围是1～5。这15种部件滤波器分为3个层次, 第1～4, 5～8, 9～15种部件分别为第1, 2, 3层次。高层次部件是由低层次的对应部件组成, 如第5个部件由部件1和部件2组成, 滤波器参数设置和研究目标相关。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129">2.3.3 可见性估计及分类模块</h4>
                <div class="p1">
                    <p id="130">图像中会存在遮挡情况, 如桌椅、植物、设备的遮挡, 目标间的遮挡等, 借鉴文献<citation id="245" type="reference">[<a class="sup">18</a>]</citation>, 在DPM基础上估计可见性, 可改善由遮挡导致的信息丢失问题, 提高对遮挡目标的检测率。计算该图像区域块的检测分数, 估计可见性, 给出最终的分类结果, 基于可变形层输出的部件分数<i>s</i>={<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>p</i></sub>}进行可见性估计, 计算公式为</p>
                </div>
                <div class="p1">
                    <p id="131" class="code-formula">
                        <mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>h</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mi>c</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo>+</mo><mi>g</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mi>s</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="132">式中:<i>h</i><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>表示第l层级的第j个部件的可见性, 上波浪线表示预测;s<mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>表示其检测分数;g<mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>和c<mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>分别为s<mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>的权重和偏置项;σ (t) 代表<i>sigmoid</i>函数, 即σ (t) =[1+<i>exp</i> (-t) ]<sup>-1</sup>;h<sup>l</sup>为处于第l层级的共P<sub>l</sub>个部件的可见性, <b><i>h</i></b><sup><i>l</i></sup>=[<i>h</i><mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>l</mi></msubsup></mrow></math></mathml><i>h</i><mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>l</mi></msubsup></mrow></math></mathml>…h<mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>l</mi></msub></mrow><mi>l</mi></msubsup></mrow></math></mathml>]<sup><i>T</i></sup>。输入前面步骤得到的部件检测分数s, 计算部件的可见性并判断分类。对于相邻层级的部件, 其可见性估计之间存在一定的转化关系:</p>
                </div>
                <div class="p1">
                    <p id="141" class="code-formula">
                        <mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>h</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mi>l</mi><mtext>Τ</mtext></mrow></msup><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mrow><mo>*</mo><mo>, </mo><mi>j</mi></mrow><mi>l</mi></msubsup><mo>+</mo><mi>c</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>+</mo><mi>g</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mi>s</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mi>l</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="142">式中: <b><i>W</i></b><sup><i>l</i></sup>表示第<i>l</i>层的可见性估计<i>h</i><sup><i>l</i></sup>与<i>h</i><sup><i>l</i>+</sup><sup>1</sup>之间的转换矩阵;<b><i>w</i></b><sup><i>l</i></sup><sub>*, <i>j</i></sub>是<b><i>W</i></b><sup><i>l</i></sup>中的第<i>j</i>个元素, 表示<mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>h</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>与<mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>h</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>之间的转化矩阵。 模型预测的类别标签<mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>y</mi><mo>˜</mo></mover></math></mathml>可表示为</p>
                </div>
                <div class="p1">
                    <p id="146" class="code-formula">
                        <mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>y</mi><mo>˜</mo></mover><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mover accent="true"><mi>h</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mn>3</mn><mtext>Τ</mtext></mrow></msup><mi>w</mi><msup><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msup><mo>+</mo><mi>b</mi><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="147">式中:<i>w</i><sup>cls</sup>是对应<mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>h</mi><mo>˜</mo></mover><msup><mrow></mrow><mn>3</mn></msup></mrow></math></mathml>的线性分类器, 上标<i>cls</i>表示分类。同一层级的部件可见性相关, g<mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>, c<mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>, W<sup>l</sup>, w<sup><i>cls</i></sup>和b为需要学习的参数。</p>
                </div>
                <div class="p1">
                    <p id="151">所提模型中采用了二分类中常用的交叉熵对数损失函数, 计算方法为</p>
                </div>
                <div class="p1">
                    <p id="152" class="code-formula">
                        <mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mi>y</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>n</mtext><mtext>d</mtext></mrow></msub><mi>lg</mi><mspace width="0.25em" /><mover accent="true"><mi>y</mi><mo>˜</mo></mover><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>n</mtext><mtext>d</mtext></mrow></msub><mo stretchy="false">) </mo><mi>lg</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mover accent="true"><mi>y</mi><mo>˜</mo></mover><mo stretchy="false">) </mo><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="153">式中:y<sub><i>gnd</i></sub>为部件的真实类别标签; L为类别预测的交叉熵对数损失函数值。</p>
                </div>
                <div class="p1">
                    <p id="154">根据随机梯度下降 (<i>SGD</i>) 准则进行网络训练, 输入为前面得到的部件检测分数, 将<i>SGD</i>训练过程中的参数计算分解为</p>
                </div>
                <div class="p1">
                    <p id="155" class="code-formula">
                        <mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi>s</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi>h</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></mfrac><mfrac><mrow><mo>∂</mo><mi>h</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow><mrow><mo>∂</mo><mi>s</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi>h</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></mfrac><mi>h</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>h</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup><mo stretchy="false">) </mo><mi>g</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi>h</mi><msubsup><mrow></mrow><mi>i</mi><mn>3</mn></msubsup></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mover accent="true"><mi>y</mi><mo>˜</mo></mover></mrow></mfrac><mover accent="true"><mi>y</mi><mo>˜</mo></mover><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mover accent="true"><mi>y</mi><mo>˜</mo></mover><mo stretchy="false">) </mo><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msubsup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi>h</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></mfrac><mo>=</mo><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mo>*</mo></mrow><mn>2</mn></msubsup><mrow><mo>[</mo><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi>h</mi><msup><mrow></mrow><mn>3</mn></msup></mrow></mfrac><mo>⊙</mo><mi>h</mi><msup><mrow></mrow><mn>3</mn></msup><mo>⊙</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>h</mi><msup><mrow></mrow><mn>3</mn></msup><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi>h</mi><msubsup><mrow></mrow><mi>i</mi><mn>1</mn></msubsup></mrow></mfrac><mo>=</mo><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mo>*</mo></mrow><mn>1</mn></msubsup><mrow><mo>[</mo><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi>h</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo>⊙</mo><mi>h</mi><msup><mrow></mrow><mn>2</mn></msup><mo>⊙</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>h</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="156">式中:⊙代表 (U⊙V) <sub>i, j</sub>=U<sub>i, j</sub>V<sub>i, j</sub>的运算;<b><i>w</i></b><sup><i>l</i></sup><sub><i>i</i>, *</sub>为<b><i>W</i></b><sup><i>l</i></sup>的第<i>i</i>行元素;<b><i>w</i></b><sup>cls</sup><sub><i>i</i></sub>是<b><i>w</i></b><sup>cls</sup>的第<i>i</i>个元素。</p>
                </div>
                <h3 id="157" name="157" class="anchor-tag">3 实验部分</h3>
                <div class="p1">
                    <p id="158">实验平台是基于64位的Ubuntu14.04操作系统和NVIDIA TITAN Xp GPU, 采用的软件有Matlab2014b、Python2.7, 采用的深度学习框架为tensorflow及相关深度学习库。</p>
                </div>
                <h4 class="anchor-tag" id="159" name="159"><b>3.1</b><b>性能评价指标</b></h4>
                <div class="p1">
                    <p id="160">二分类问题中, 通常采用查准率 (<i>R</i><sub>Precision</sub>) 、查全率 (<i>R</i><sub>Recall</sub>) 来客观评价算法性能, 就人体目标检测算法而言, 将样例真实类别与算法的预测类别相比较。<i>R</i><sub>Precision</sub>、<i>R</i><sub>Recall</sub>计算式为</p>
                </div>
                <div class="p1">
                    <p id="161" class="code-formula">
                        <mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><msub><mrow></mrow><mrow><mtext>R</mtext><mtext>e</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ρ</mtext></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ρ</mtext></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>Ν</mtext></mrow></msub></mrow></mfrac><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>R</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>i</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ρ</mtext></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ρ</mtext></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>Ρ</mtext></mrow></msub></mrow></mfrac><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="162">式中:<i>N</i><sub>TP</sub>、<i>N</i><sub>FP</sub>、<i>N</i><sub>FN</sub>和<i>N</i><sub>TN</sub>分别为将人体样本正确预测为正样本数、非人样本错误预测为正样本数、人体样本错误预测为负样本数、非人样本正确预测为负样本数。<i>R</i><sub>Recall</sub>反映分类器对正例的覆盖能力, <i>R</i><sub>Precision</sub>反映分类器预测正例的准确程度。以查全率<i>R</i><sub>Recall</sub>和查准率<i>R</i><sub>Precision</sub>分别作为横纵坐标, 绘制查准率-查全率曲线 (P-R曲线) , 该曲线可以反映分类器对正例的识别准确程度和对正例的覆盖能力之间的权衡。<i>R</i><sub>Precision</sub>和 <i>R</i><sub>Recall</sub>往往此消彼长, <i>R</i><sub>AP</sub>为P-R曲线与<i>x</i>轴围成的曲线下图形面积。该图形面积越大, 分类器的效果越好。对于连续的P-R曲线, <i>R</i><sub>AP</sub>计算方式为</p>
                </div>
                <div class="p1">
                    <p id="163" class="code-formula">
                        <mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><mrow><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mi>p</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi>r</mi><mo stretchy="false">) </mo><mtext>d</mtext><mi>r</mi><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="164">式中:<i>p</i>代表<i>R</i><sub>Precision</sub>; <i>R</i><sub>Precision</sub>是关于<i>R</i><sub>Recall</sub>分段常数的函数。</p>
                </div>
                <div class="p1">
                    <p id="165">人体目标检测领域, 根据Dollar等<citation id="246" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>提出的工具箱, 通常用FPPI-MR (False Positives Per Image-Miss Rate) 衡量人体目标检测算法的检测性能, 把测试图片中包含人体目标的窗口切割出来, 然后从不包含人体目标的测试集采样非人样本, 将窗口作为测试集来评估算法的性能。相关分析表明<citation id="247" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>, 相比FPPW (False Positives Per Window) , FPPI指标更合理。具体而言, 给定一定数目的包含或不包含人体目标的样本图像, <i>N</i><sub>Pos</sub>和<i>N</i><sub>Neg</sub>分别为正、负样本数, <i>R</i><sub>FPPI</sub>表示分类器的误检率, <i>R</i><sub>MR</sub> (Miss Rate) 与召回率<i>R</i><sub>Recall</sub>的和为1, 表示分类器的漏检率, <i>R</i><sub>MR</sub>值越低, 算法性能越好, 计算公式分别为</p>
                </div>
                <div class="p1">
                    <p id="166" class="code-formula">
                        <mathml id="166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>Ρ</mtext><mtext>Ρ</mtext><mtext>Ι</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>Ρ</mtext></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Ν</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>o</mtext><mtext>s</mtext></mrow></msub></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mo>‚</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>R</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>R</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>Ν</mtext></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>Ν</mtext></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>Ρ</mtext></mrow></msub></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="167">为了验证所提算法模型对室内人员目标的检测性能, 将其与其他相关算法模型进行了对比, 选择<i>R</i><sub>FPPI</sub>-<i>R</i><sub>MR</sub>和<i>R</i><sub>AP</sub>作为衡量多个算法性能的评价指标。</p>
                </div>
                <h4 class="anchor-tag" id="168" name="168"><b>3.2</b><b>数据集建立</b></h4>
                <div class="p1">
                    <p id="169">人体目标检测研究领域中, INRIA、Caltech、PASCAL VOC等数据集在相关研究中使用较多, 其中, PASCAL VOC的图像标注方式及质量都较合理, 常被用在一些比赛及课题研究中<citation id="248" type="reference"><link href="79" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>。针对室内人员检测任务, 目前尚没有公开相关专用数据集, 因此, 按照PASCAL VOC数据集的格式标准建立了一个监控环境下的室内人员检测数据集 (IHDD) , 数据采集自某企业值班室和工作室, 对应的每种场所都在不同角落安装了摄像头, 人的上半身基本可完整呈现在监控视野中。对一个月内的监控视频进行截取并采集图像, 涉及白天和夜晚时段、不同性别和年龄段的人员以及正面、背面、侧面多种视角, 大部分人员是坐姿及其变形状态, 也有少量直立或倾斜站立的姿态。共采集了14665个图像样本, 其中包含8799个训练样本, 2933个验证样本, 以及2933个测试样本, 一共标注了17854个人体目标, 数据集构成如表2所示。同一种场所中的多个视频是由不同位置的摄像头采集得到的。</p>
                </div>
                <h4 class="anchor-tag" id="170" name="170"><b>3.3</b><b>训练网络模型</b></h4>
                <div class="p1">
                    <p id="171">所提网络模型包括两个部分。第一部分是区域建议网络, 其训练方法和文献<citation id="249" type="reference">[<a class="sup">21</a>]</citation>一致, 使用了VGG-16在ImageNet上的预训练模型。训练参数为:动量为0.9, 权重衰减为0.0005, 两阶段学习率分别为0.001和0.0001。第二部分是联合学习的多视角检测网络, 网络训练参数设置如表3所示。</p>
                </div>
                <div class="p1">
                    <p id="172">根据学习率和模型误差间的变化关系, 设定两阶段学习率分别为0.025和0.0125, 并将epoch设定为250。模型选择了交叉熵损失函数, 使用SGD的方式进行训练。</p>
                </div>
                <div class="area_img" id="173">
                    <p class="img_tit">表2 室内人员检测数据集 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Indoor human detection dataset</p>
                    <p class="img_note"></p>
                    <table id="173" border="1"><tr><td><br />Test environment</td><td>Total frames</td><td>Annotated humans<br />No.</td></tr><tr><td><br />Office day 1</td><td>3912</td><td>5163</td></tr><tr><td><br />Office day 2</td><td>3157</td><td>4785</td></tr><tr><td><br />Office day 3</td><td>246</td><td>394</td></tr><tr><td><br />Duty room day 1</td><td>108</td><td>178</td></tr><tr><td><br />Duty room day 2</td><td>813</td><td>893</td></tr><tr><td><br />Duty room night 1</td><td>6063</td><td>6072</td></tr><tr><td><br />Duty room night 2</td><td>369</td><td>369</td></tr><tr><td><br />Training set</td><td>8799</td><td>10479</td></tr><tr><td><br />Validation set</td><td>2933</td><td>3701</td></tr><tr><td><br />Test set</td><td>2933</td><td>3674</td></tr><tr><td><br />Total number of samples</td><td>14665</td><td>17854</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="174">
                    <p class="img_tit">表3 第二阶段网络模型参数设置 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Parameter setting of second-stage network model</p>
                    <p class="img_note"></p>
                    <table id="174" border="1"><tr><td><br />Parameter</td><td>Epoch 1-150</td><td>Epoch 151-250</td></tr><tr><td><br />Learning rate</td><td>0.025</td><td>0.0125</td></tr><tr><td><br /> Momentum</td><td>0.9</td><td>0.9</td></tr><tr><td><br />Batch size</td><td>80</td><td>80</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="175" name="175"><b>3.4</b><b>MVNN的切片分析</b></h4>
                <div class="p1">
                    <p id="176">为了验证所提算法模型的有效性, 下面对几个主要部分的作用单独进行研究。</p>
                </div>
                <h4 class="anchor-tag" id="177" name="177">3.4.1 区域建议网络</h4>
                <div class="p1">
                    <p id="178">所提模型构建了区域建议网络, 为说明此部分作用, 与文献<citation id="250" type="reference">[<a class="sup">18</a>]</citation>中HOG+CSS+SVM的预处理算法模型进行了对比。由于对应源码不容易获得, 此处采用具有类似检测思想的HOG特征结合Adaboost方法的模型替代该方法, 测试结果如图7所示。</p>
                </div>
                <div class="area_img" id="179">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902010_179.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 输入数据的区域建议结果比较。 (a) HOG特征结合Adaboost方法; (b) 所提区域建议算法" src="Detail/GetImg?filename=images/GXXB201902010_179.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 输入数据的区域建议结果比较。 (a) HOG特征结合Adaboost方法; (b) 所提区域建议算法  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902010_179.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Comparison results of region proposal for input data. (a) HOG+Adaboost algorithm; (b) Proposed region proposal algorithm</p>

                </div>
                <div class="p1">
                    <p id="180">图7中共4个测试样例, 图7 (a) 对应HOG特征结合Adaboost方法, 图7 (b) 对应所提的构建区域建议网络的方法。分析前两个样例发现, 所提算法在处理人体存在较大变形时仍有较好的检测性能, 而文献<citation id="251" type="reference">[<a class="sup">18</a>]</citation>中的方法易出现误检, 如对第一个样例中的行李包和第二个样例中的盆景出现了误检;文献<citation id="252" type="reference">[<a class="sup">18</a>]</citation>的方法对存在一定遮挡与变形的情形 (如第三个测试样例) 出现了漏检, 而所提算法在该情况下仍有较好的检测效果;对于最后一个样例, 两人体目标区域存在一定重叠与遮挡, 文献<citation id="253" type="reference">[<a class="sup">18</a>]</citation>的方法只检测到一个目标, 而所提算法在检测到两个人体目标的前提下, 结合多视角模型, 可以正确获得两个目标的检测结果。</p>
                </div>
                <div class="p1">
                    <p id="181">出现上述现象的原因主要是算法特征类型不同, 文献<citation id="254" type="reference">[<a class="sup">18</a>]</citation>利用的是传统HOG特征, 仅提取边缘信息, 对目标的表达能力有限, 当出现其他特征类似的目标时容易误检, 而所提算法利用深度网络特征, 对目标的特征表达更充分, 不易出现误检, 也更易检测出同类的新测试样例。文献<citation id="255" type="reference">[<a class="sup">18</a>]</citation>算法的特征表达能力有限, 当应对变形较大或存在遮挡的新样例时易出现漏检。</p>
                </div>
                <div class="p1">
                    <p id="182">分析以上测试结果发现, 所提的区域建议模型具有更高的召回率和更低的误检率, 尤其在处理遮挡和多视角情形的时候检测效果更好。</p>
                </div>
                <h4 class="anchor-tag" id="183" name="183">3.4.2 多视角模型</h4>
                <div class="p1">
                    <p id="184">视频监控下的人体目标往往不能显示出完整外形, 且大部分都存在一定倾角与形变, 并呈现多种视角。为此, 设计了多视角模型, 部分检测结果如图8所示。</p>
                </div>
                <div class="area_img" id="185">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902010_185.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 多视角模型测试结果。 (a) 不同视角下测试样本的检测结果; (b) 单视角模型和多视角模型的检测效果对比" src="Detail/GetImg?filename=images/GXXB201902010_185.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 多视角模型测试结果。 (a) 不同视角下测试样本的检测结果; (b) 单视角模型和多视角模型的检测效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902010_185.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Testing result of multi-view model. (a) Testing result of multiple views; (b) Comparison results of single-view model and multi-view model</p>

                </div>
                <div class="p1">
                    <p id="186">图8 (a) 所示为不同视角下测试样本的检测结果。第1列对应正面视角;第2列是侧面视角;第3列是背面视角, 除了坐姿也有站立和行走中的情形;第4列也是背面视角, 不过存在一定变形;第5列是包含两目标且可能为相同视角和不同视角的情形, 分别对应了两侧面视角和正面+背面视角。实验结果表明, 多视角模型对多视角目标的检测效果较好。</p>
                </div>
                <div class="p1">
                    <p id="187">图8 (b) 是单视角模型和多视角模型的检测效果对比, 每一列代表一个测试样本, 两行分别对应单视角模型和所提多视角模型。对于前两个样本, 多视角模型可将两个目标都检测出来, 而单视角模型仅检出其中一个目标, 漏检了存在遮挡的目标。第3列为两个人体目标之间存在遮挡且处于不同视角的情形, 在这种情况下, 单视角模型仅检测出一个目标, 而多视角模型根据不同视角类型正确检测出两个目标。分析可能原因:1) 该结果与对检测结果进行非最大值抑制处理有关, 单视角模型会抑制属于同类且相交面积较大的检测结果;2) 单视角模型只检测出其中基本未被遮挡的人体目标。第4列是两目标几乎无遮挡但距离较近的情形, 在这种情况下, 单视角模型产生误检, 多视角模型检测正确, 可能原因在于多视角模型学习每种视角的特征, 而单视角模型综合学习各种视角的目标特征, 模型特征识别能力弱, 遇到相似图像易产生误检。</p>
                </div>
                <div class="p1">
                    <p id="188">总的来说, 针对监控环境下的室内人员检测, 多视角模型比单视角模型具有更高的检测率和召回率, 不易产生误检。</p>
                </div>
                <h4 class="anchor-tag" id="189" name="189">3.4.3 可变形处理和可见性估计</h4>
                <div class="p1">
                    <p id="190">使用可变形处理可在一定程度上应对目标形变, 并通过估计可见性应对遮挡, 文献<citation id="256" type="reference">[<a class="sup">18</a>]</citation>中已说明了其对提高模型性能的重要作用。部分测试如图9所示, 实验结果表明, 对存在变形或部分遮挡的目标, 所提算法仍有不错的检测性能, 检测率和召回率均有提升。</p>
                </div>
                <h3 id="191" name="191" class="anchor-tag">4 分析与讨论</h3>
                <div class="p1">
                    <p id="192">所提算法的最终定量统计结果如图10所示, 图10 (a) 中为<i>R</i><sub>FPPI</sub>-<i>R</i><sub>MR</sub>指标下的测试结果, 可得<i>R</i><sub>MR</sub>=14.66%, 图10 (b) 中P-R曲线下面积表示的平均准确率<i>R</i><sub>AP</sub>=87.34%。</p>
                </div>
                <div class="area_img" id="193">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902010_193.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 DPM测试结果" src="Detail/GetImg?filename=images/GXXB201902010_193.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 DPM测试结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902010_193.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Testing result of DPM</p>

                </div>
                <div class="area_img" id="194">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201902010_194.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 所提算法在IHDD上的测试结果。 (a) RFPPI-RMR曲线; (b) P-R曲线" src="Detail/GetImg?filename=images/GXXB201902010_194.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 所提算法在IHDD上的测试结果。 (a) <i>R</i><sub>FPPI</sub>-<i>R</i><sub>MR</sub>曲线; (b) P-R曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201902010_194.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Testing result of proposed algorithm on IHDD. (a) <i>R</i><sub>FPPI</sub>-<i>R</i><sub>MR</sub> curve; (b) P-R curve</p>

                </div>
                <div class="p1">
                    <p id="195">为了进一步验证所提算法的有效性, 将其与现有人体目标检测经典算法模型 (包括文献<citation id="257" type="reference">[<a class="sup">3</a>]</citation>、文献<citation id="258" type="reference">[<a class="sup">21</a>]</citation>和文献<citation id="259" type="reference">[<a class="sup">18</a>]</citation>中的算法模型) 进行对比, 相关测试结果统计如表4所示。</p>
                </div>
                <div class="area_img" id="196">
                    <p class="img_tit">表4 不同算法在数据集上的定量比较 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Quantitative comparison of different algorithms on dataset</p>
                    <p class="img_note"></p>
                    <table id="196" border="1"><tr><td><br />Algorithm</td><td><i>R</i><sub>MR</sub></td><td><i>R</i><sub>AP</sub> /%</td></tr><tr><td><br />measurements</td><td> (<i>R</i><sub>FPPI</sub>-<i>R</i><sub>MR</sub>) /%</td><td></td></tr><tr><td><br />Ref. [3]</td><td>37.38</td><td>57.32</td></tr><tr><td><br />Ref. [21]</td><td>17.29</td><td>84.82</td></tr><tr><td><br />Ref. [18]</td><td>28.84</td><td>75.52</td></tr><tr><td><br />Proposed</td><td>14.66</td><td>87.34</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="197">表4中, 文献<citation id="260" type="reference">[<a class="sup">3</a>]</citation>代表传统特征检测方法, 即HOG特征结合SVM的方法, 此处使用提升算法Adaboost替代SVM, 文献<citation id="261" type="reference">[<a class="sup">21</a>]</citation>是代表基于精度较高的神经网络检测模型faster R-CNN的方法, 文献<citation id="262" type="reference">[<a class="sup">18</a>]</citation>为基于基准网络模型JointDeep的方法。</p>
                </div>
                <div class="p1">
                    <p id="198">可以看出, 对于监控环境下的室内人体目标检测, 所提算法的召回率与检测率更高。分析原因如下:1) 文献<citation id="263" type="reference">[<a class="sup">3</a>]</citation>的方法是在遍历图像的基础上提取人工设计的特征, 在一定程度上表达了目标的边缘等信息, 但是对于其他方面的信息 (如颜色通道信息、纹理信息等) 表达还不够充分, 而且其他物体的边缘信息也可能和人体目标很相似, 容易造成误检, 通过网络学习提取的CNN特征则具有更丰富的表达能力, 可以提取出目标的更多代表性信息, 更能做出可靠性判断。2) 文献<citation id="264" type="reference">[<a class="sup">21</a>]</citation>的方法是采用两阶段的网络模型, 通过第一阶段的区域预提取保证了较高的召回率, 第二阶段在此基础上进行更精确地判断, 但其针对的是通常情况下的目标分类与检测问题, 没有考虑监控环境下室内人体目标的多视角、多形态、存在变形及遮挡的情形, 因而检测性能并不理想。3) 文献<citation id="265" type="reference">[<a class="sup">18</a>]</citation>基于基准网络模型, 采用联合学习的方法处理目标的变形与遮挡问题, 在第一阶段中采用传统目标检测方法进行区域预提取。这种方法速度快, 但准确率和召回率并不高。类似地, 该模型针对的是处于开阔环境中处于水平视角的行人, 并不适合所提算法模型的研究环境, 尤其是监控环境下室内人体目标存在一定倾角、呈现多视角、多形态、信息不完整的复杂情形。</p>
                </div>
                <div class="p1">
                    <p id="199">综上所述, 针对监控环境下室内人体目标检测任务, 所提算法模型能够合理处理不同光照 (如白天和黑夜) 、多种形态、多视角、存在一定遮挡的情形, 检测效果更好。</p>
                </div>
                <h3 id="200" name="200" class="anchor-tag">5 结 论</h3>
                <div class="p1">
                    <p id="201">针对现有室内人员检测算法在处理多视角、存在多种变形和遮挡等方面的不足, 结合监控条件下室内人员检测任务的特点, 搭建了一个联合学习的MVNN, 集成了区域建议、多视角模型、特征提取、可变形处理、可见性估计和判别分类等部分。在自建的IHDD上进行训练和测试, 结果表明, 相比于传统的人员检测算法和利用经典网络模型检测的方法, 所提算法通过使用区域建议网络模型显著提高了目标召回率, 而多视角模型和DPM的设计则使得算法在处理多视角、存在一定变形与遮挡的情形时仍有较高的检测率和较低的误检率, 可以为后续相关研究提供参考。</p>
                </div>
                <div class="p1">
                    <p id="202">由于实际监控条件和室内环境的不确定性, 所提算法在一些复杂环境下的人员检测性能还有提升空间, 综合考虑经济条件和应用需求, 以后可在更多相关场所中架设摄像头, 增加样本的数量并提高样本质量, 不断提升所提算法在多视角、存在变形与遮挡等复杂环境下的检测性能。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Occupancy detection in the office by analyzing surveillance videos and its application to building energy conservation">

                                <b>[1]</b> Zou J H, Zhao Q C, Yang W, <i>et al</i>. Occupancy detection in the office by analyzing surveillance videos and its application to building energy conservation[J]. Energy and Buildings, 2017, 152: 385-398.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ten years of pedestrian detection, what have we learned?">

                                <b>[2]</b> Benenson R, Omran M, Hosang J, <i>et al</i>. Ten years of pedestrian detection, what have we learned?[C]. European Conference on Computer Vision, 2015: 613-627.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">

                                <b>[3]</b> Dalal N, Triggs B. Histograms of oriented gradients for human detection[C]. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005: 886-893.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An HOG-LBP Human Detector with Partial Occlusion Handling">

                                <b>[4]</b> Wang X Y, Han T X, Yan S C. An HOG-LBP human detector with partial occlusion handling[C]. IEEE 12th International Conference on Computer Vision, 2009: 32-39.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Boosted local structured HOG-LBP for object localization">

                                <b>[5]</b> Zhang J G, Huang K Q, Yu Y N, <i>et al</i>. Boosted local structured HOG-LBP for object localization[C]. IEEE International Conference on Computer Vision and Pattern Recognition, 2011: 1393-1400.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MDEwNjhSN3FkWitadUZpdmtXNzdJSlY4PU5qN0Jhck80SHRIT3A0eEZiZXNPWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Lowe D G. Distinctiveimage features from scale-invariant keypoints[J]. International Journal of Computer Vision, 2004, 60 (2) : 91-110.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" >
                                    <b>[7]</b>
                                 Lienhart R, Maydt J. An extended set of Haar-like features for rapid object detection[C]. International Conference on Image Processing, 2002: 900-903
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                 Dollar P, Tu Z W, Perona P, <i>et al</i>. Integral channel features[J]. Proceedings of the British Machine Vision Conference, 2009: 91.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 Dollar P, Appel R, Belongie S, <i>et al</i>. Fast feature pyramids for object detection[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014, 36 (8) : 1532-1545.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Informed Haar-like features improve pedestrian detection">

                                <b>[10]</b> Zhang S S, Bauckhage C, Cremers A B. Informed haar-like features improve pedestrian detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 947-954.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Local decorrelation for improved detection">

                                <b>[11]</b> Nam W, Dollár P, Han J H. Local decorrelation for improved detection[J]. Advances in Neural Information Processing Systems, 2014, 1: 424-432.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Filtered channel features for pedestrian detection">

                                <b>[12]</b> Zhang S S, Benenson R, Schiele B. Filtered channel features for pedestrian detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2015: 1751-1760.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pedestrian detection with unsupervised multi-stage feature learning">

                                <b>[13]</b> Sermanet P, Kavukcuoglu K, Chintala S, <i>et al</i>. Pedestrian detection with unsupervised multi-stage feature learning[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2013: 3626-3633.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Taking a Deeper Look at Pedestrians">

                                <b>[14]</b> Hosang J, Omran M, Benenson R, <i>et al</i>. Taking a deeper look at pedestrians[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2015: 4073-4082.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Complexity-Aware Cascades for Deep Pedestrian Detection">

                                <b>[15]</b> Cai Z W, Saberian M, Vasconcelos N. Learning complexity-aware cascades for deep pedestrian detection[C]. IEEE International Conference on Computer Vision, 2015: 3361-3369.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Classification Using Intersection KernelSupport Vector Machines is efficient">

                                <b>[16]</b> Maji S, Berg A C, Malik J. Classification using intersection kernel support vector machines is efficient[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2008: 4587630.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_17" >
                                    <b>[17]</b>
                                 Felzenszwalb P F, Girshick R B, McAllester D, <i>et al</i>. Object detection with discriminatively trained part-based models[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (9) : 1627-1645.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint deep learning for pedestrian detection">

                                <b>[18]</b> Ouyang W L, Wang X G. Joint deep learning for pedestrian detection[C]. IEEE International Conference on Computer Vision, 2013: 2056-2063.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic">

                                <b>[19]</b> Girshick R, Donahue J, Darrell T, <i>et al</i>. Rich feature hierarchies for accurate object detection and semantic segmentation[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 580-587.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[20]</b> Girshick R. Fast R-CNN[C]. IEEE International Conference on Computer Vision, 2015: 1440-1448.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_21" >
                                    <b>[21]</b>
                                 Ren S Q, He K M, Girshick R, <i>et al</i>. Faster R-CNN: Towards real-time object detection with region proposal networks[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) : 1137-1149.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806034&amp;v=Mjg4NzNZOUdZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURrVTdySklqWFRiTEc0SDluTXE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> Feng X Y, Mei W, Hu D S. Aerialtarget detection based on improved faster R-CNN [J]. Acta Optica Sinica, 2018, 38 (6) : 0615004. 冯小雨, 梅卫, 胡大帅. 基于改进Faster R-CNN的空中目标检测[J]. 光学学报, 2018, 38 (6) : 0615004.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You Only Look Once:Unified,Real-Time Object Detection">

                                <b>[23]</b> Redmon J, Divvala S, Girshick R, <i>et al</i>. You only look once: unified, real-time object detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 779-788.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_24" >
                                    <b>[24]</b>
                                 Liu W, Anguelov D, Erhan D, <i>et al</i>. SSD: single shot multibox detector[C]. European Conference on Computer Vision, 2016: 21-37.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201809039&amp;v=MDk4NThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEa1U3ckpMeXJQWkxHNEg5bk1wbzlHYlk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> Liu H, Peng L, Wen J W. Multi-scale aware pedestrian detection algorithm based on improved full convolutional network[J]. Laser &amp; Optoelectronics Progress, 2018, 55 (9) : 091504. 刘辉, 彭力, 闻继伟. 基于改进全卷积网络的多尺度感知行人检测算法[J]. 激光与光电子学进展, 2018, 55 (9) : 091504.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-cue pedestrian classification with partial occlusion handling">

                                <b>[26]</b> Enzweiler M, Eigenstetter A, Schiele B, <i>et al</i>. Multi-cue pedestrian classification with partial occlusion handling[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2010: 990-997.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A discriminative deep model for pedestrian detection with occlusion handling">

                                <b>[27]</b> Ouyang W L, Wang X G. A discriminative deep model for pedestrian detection with occlusion handling[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2012: 3258-3265.
                            </a>
                        </p>
                        <p id="65">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13080200013634&amp;v=MTM3ODZadEZpbmxVcnpKS0Y4VWJ4TT1OajdCYXJLN0h0bk1yWTlGWk9vTUNuODlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b> Uijlings J R R, van de Sande K E A, Gevers T, <i>et al</i>. Selective search for object recognition[J]. International Journal of Computer Vision, 2013, 104 (2) : 154-171.
                            </a>
                        </p>
                        <p id="67">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locating object proposals from edges">

                                <b>[29]</b> Lawrence Zitnick C, Dollár P. Edge boxes: locating object proposals from edges[C]. European Conference on Computer Vision, 2014: 391-405.
                            </a>
                        </p>
                        <p id="69">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-pedestrian detection aided by multi-pedestrian detection">

                                <b>[30]</b> Ouyang W L, Wang X G. Single-pedestrian detection aided by multi-pedestrian detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2013: 3198-3205.
                            </a>
                        </p>
                        <p id="71">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Switchable deep network for pedestrian detection">

                                <b>[31]</b> Luo P, Tian Y L, Wang X G, <i>et al</i>. Switchable deep network for pedestrian detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 899-906.
                            </a>
                        </p>
                        <p id="73">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards reaching human performance in pedestrian detection">

                                <b>[32]</b> Zhang S S, Benenson R, Omran M, <i>et al</i>. Towards reaching human performance in pedestrian detection[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (4) : 973-986.
                            </a>
                        </p>
                        <p id="75">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=What makes for effective detection proposals?">

                                <b>[33]</b> Hosang J, Benenson R, Dollar P, <i>et al</i>. What makes for effective detection proposals?[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (4) : 814-830.
                            </a>
                        </p>
                        <p id="77">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pedestrian detection:A benchmark">

                                <b>[34]</b> Dollar P, Wojek C, Schiele B, <i>et al</i>. Pedestrian detection: A benchmark[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2009: 304-311.
                            </a>
                        </p>
                        <p id="79">
                            <a id="bibliography_35" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MTk2NDNJSlY4PU5qN0Jhck80SHRIUHFZZEhZK0lMWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXZrVzc3&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[35]</b> Everingham M, van Gool L, Williams C K I, <i>et al</i>. The pascal visual object classes (VOC) challenge[J]. International Journal of Computer Vision, 2010, 88 (2) : 303-338.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201902010" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201902010&amp;v=MjY3NDFyQ1VSN3FmWnVadEZpRGtVN3JKSWpYVGJMRzRIOWpNclk5RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dFFaNjNMSWtaUGZ1aW9HelVZVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

