

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133153001690000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201909033%26RESULT%3d1%26SIGN%3dcliLIqZa5AEF8iz9eXGAHLsdaAg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201909033&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201909033&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201909033&amp;v=MjQ4OTJGeXprV3IvQUlqWFRiTEc0SDlqTXBvOUdaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#103" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#106" data-title="2 行人再识别 ">2 行人再识别</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#107" data-title="&lt;b&gt;2.1 行人再识别研究现状&lt;/b&gt;"><b>2.1 行人再识别研究现状</b></a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;2.2 行人再识别的深度学习应用&lt;/b&gt;"><b>2.2 行人再识别的深度学习应用</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#116" data-title="3 本文算法架构 ">3 本文算法架构</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#120" data-title="&lt;b&gt;3.1 行人多级深度特征表示网络&lt;/b&gt;"><b>3.1 行人多级深度特征表示网络</b></a></li>
                                                <li><a href="#137" data-title="&lt;b&gt;3.2 有序加权距离测度融合算法&lt;/b&gt;"><b>3.2 有序加权距离测度融合算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#164" data-title="4 实验介绍 ">4 实验介绍</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#165" data-title="&lt;b&gt;4.1 实验数据集介绍&lt;/b&gt;"><b>4.1 实验数据集介绍</b></a></li>
                                                <li><a href="#169" data-title="&lt;b&gt;4.2 实验设计及评价准则&lt;/b&gt;"><b>4.2 实验设计及评价准则</b></a></li>
                                                <li><a href="#174" data-title="&lt;b&gt;4.3 与基于静止图像的相关方法的对比实验&lt;/b&gt;"><b>4.3 与基于静止图像的相关方法的对比实验</b></a></li>
                                                <li><a href="#179" data-title="&lt;b&gt;4.4 与基于视频的相关方法对比实验&lt;/b&gt;"><b>4.4 与基于视频的相关方法对比实验</b></a></li>
                                                <li><a href="#186" data-title="&lt;b&gt;4.5 与基于深度学习的相关方法对比实验&lt;/b&gt;"><b>4.5 与基于深度学习的相关方法对比实验</b></a></li>
                                                <li><a href="#191" data-title="&lt;b&gt;4.6 关键因素的验证实验&lt;/b&gt;"><b>4.6 关键因素的验证实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#199" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#111" data-title="图1 视频行人再识别中的挑战与困难">图1 视频行人再识别中的挑战与困难</a></li>
                                                <li><a href="#118" data-title="图2 联合多级深度特征表示和有序加权距离融合的视频行人再识别算法架构">图2 联合多级深度特征表示和有序加权距离融合的视频行人再识别算法架构</a></li>
                                                <li><a href="#123" data-title="表1 全局与局部的外观深度特征网络结构参数">表1 全局与局部的外观深度特征网络结构参数</a></li>
                                                <li><a href="#129" data-title="图3 RNN结构图">图3 RNN结构图</a></li>
                                                <li><a href="#157" data-title="图4 本文所采用的距离测度算法计算行人之间距离的流程">图4 本文所采用的距离测度算法计算行人之间距离的流程</a></li>
                                                <li><a href="#160" data-title="图5 有序加权距离融合算法图">图5 有序加权距离融合算法图</a></li>
                                                <li><a href="#171" data-title="表2 行人再识别常用公共数据集">表2 行人再识别常用公共数据集</a></li>
                                                <li><a href="#177" data-title="表3 与基于静止图像的相关方法在i-LIDS数据集比较结果">表3 与基于静止图像的相关方法在i-LIDS数据集比较结果</a></li>
                                                <li><a href="#178" data-title="表4 与基于静止图像的相关方法在PRID-2011数据集比较结果">表4 与基于静止图像的相关方法在PRID-2011数据集比较结果</a></li>
                                                <li><a href="#181" data-title="图6 与基于静止图像行人再识别方法在i-LIDS数据集和PRID-2011数据集相比较的CMC。">图6 与基于静止图像行人再识别方法在i-LIDS数据集和PRID-2011数据集相比较的CMC。</a></li>
                                                <li><a href="#182" data-title="表5 与基于视频行人再识别的相关方法在i-LIDS数据集比较结果">表5 与基于视频行人再识别的相关方法在i-LIDS数据集比较结果</a></li>
                                                <li><a href="#183" data-title="表6 与基于视频行人再识别的相关方法在PRID-2011数据集比较结果">表6 与基于视频行人再识别的相关方法在PRID-2011数据集比较结果</a></li>
                                                <li><a href="#184" data-title="图7 与基于视频行人再识别方法在i-LIDS数据集和PRID-2011数据集相比较的CMC。">图7 与基于视频行人再识别方法在i-LIDS数据集和PRID-2011数据集相比较的CMC。</a></li>
                                                <li><a href="#188" data-title="表7 与基于深度学习行人再识别的相关方法在i-LIDS数据集比较结果">表7 与基于深度学习行人再识别的相关方法在i-LIDS数据集比较结果</a></li>
                                                <li><a href="#189" data-title="表8 与基于深度学习行人再识别的相关方法在PRID-2011数据集比较结果">表8 与基于深度学习行人再识别的相关方法在PRID-2011数据集比较结果</a></li>
                                                <li><a href="#193" data-title="图8 与基于深度学习行人再识别方法在i-LIDS数据集和PRID-2011数据集相比较的CMC。">图8 与基于深度学习行人再识别方法在i-LIDS数据集和PRID-2011数据集相比较的CMC。</a></li>
                                                <li><a href="#195" data-title="表9 关键因素在i-LIDS数据集的比较结果">表9 关键因素在i-LIDS数据集的比较结果</a></li>
                                                <li><a href="#196" data-title="表10 关键因素在PRID-2011数据集的比较结果">表10 关键因素在PRID-2011数据集的比较结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="13">


                                    <a id="bibliography_1" title=" Bauml M,Tapaswi M,Schumann A,&lt;i&gt;et al&lt;/i&gt;.Contextual constraints for person retrieval in camera networks[C]//2012 IEEE Ninth International Conference on Advanced Video and Signal-Based Surveillance,September 18-21,2012,Beijing,China.New York:IEEE,2012:221-227." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Contextual constraints for person retrieval in camera networks">
                                        <b>[1]</b>
                                         Bauml M,Tapaswi M,Schumann A,&lt;i&gt;et al&lt;/i&gt;.Contextual constraints for person retrieval in camera networks[C]//2012 IEEE Ninth International Conference on Advanced Video and Signal-Based Surveillance,September 18-21,2012,Beijing,China.New York:IEEE,2012:221-227.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_2" title=" McLaughlin N,del Rincon J M,Miller P.Recurrent convolutional network for video-based person re-identification[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1325-1334." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recurrent Convolutional Network for Video-Based Person Re-identification">
                                        <b>[2]</b>
                                         McLaughlin N,del Rincon J M,Miller P.Recurrent convolutional network for video-based person re-identification[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1325-1334.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_3" title=" Zhang W,Yu X D,He X.Learning bidirectional temporal cues for video-based person re-identification[J].IEEE Transactions on Circuits and Systems for Video Technology,2018,28(10):2768-2776." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning bidirectional temporal cues for video-based person re-identification">
                                        <b>[3]</b>
                                         Zhang W,Yu X D,He X.Learning bidirectional temporal cues for video-based person re-identification[J].IEEE Transactions on Circuits and Systems for Video Technology,2018,28(10):2768-2776.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_4" title=" Tao D P,Guo Y N,Yu B S,&lt;i&gt;et al&lt;/i&gt;.Deep multi-view feature learning for person re-identification[J].IEEE Transactions on Circuits and Systems for Video Technology,2018,28(10):2657-2666." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep multi-view feature learning for person re-identification">
                                        <b>[4]</b>
                                         Tao D P,Guo Y N,Yu B S,&lt;i&gt;et al&lt;/i&gt;.Deep multi-view feature learning for person re-identification[J].IEEE Transactions on Circuits and Systems for Video Technology,2018,28(10):2657-2666.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_5" title=" Chu M D,Wu S,Gu Y F,&lt;i&gt;et al&lt;/i&gt;.Rich features and precise localization with region proposal network for object detection[M]//Zhou J,Wang Y H,Sun Z A,&lt;i&gt;et al&lt;/i&gt;.Biometric recognition.Lecture notes in computer science.Cham:Springer,2017,10568:605-614." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich features and precise localization with region proposal network for object detection">
                                        <b>[5]</b>
                                         Chu M D,Wu S,Gu Y F,&lt;i&gt;et al&lt;/i&gt;.Rich features and precise localization with region proposal network for object detection[M]//Zhou J,Wang Y H,Sun Z A,&lt;i&gt;et al&lt;/i&gt;.Biometric recognition.Lecture notes in computer science.Cham:Springer,2017,10568:605-614.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_6" title=" Mirmahboub B,Mekhalfi M L,Murino V.Distance penalization and fusion for person re-identification[C]∥2017 IEEE Winter Conference on Applications of Computer Vision (WACV),March 24-31,2017,Santa Rosa,CA,USA.New York:IEEE,2017:1306-1314." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distance penalization and fusion for person re-identification">
                                        <b>[6]</b>
                                         Mirmahboub B,Mekhalfi M L,Murino V.Distance penalization and fusion for person re-identification[C]∥2017 IEEE Winter Conference on Applications of Computer Vision (WACV),March 24-31,2017,Santa Rosa,CA,USA.New York:IEEE,2017:1306-1314.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_7" title=" Xu Y,Lu Y W.Adaptive weighted fusion:a novel fusion approach for image classification[J].Neurocomputing,2015,168:566-574." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES540EB119935514AB03D8BFDEC639AC31&amp;v=MTE4OTVUNlV0MU9nbVcyV0V6ZXJ2bE5ybWVDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkbGh3TG0yd0tBPU5pZk9mYmE4SHFTK3JvNU1iZWdLQ1gwOXZtUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Xu Y,Lu Y W.Adaptive weighted fusion:a novel fusion approach for image classification[J].Neurocomputing,2015,168:566-574.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_8" title=" Kviatkovsky I,Adam A,Rivlin E.Color invariants for person reidentification[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(7):1622-1634." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Color Invariants for Person Reidentification">
                                        <b>[8]</b>
                                         Kviatkovsky I,Adam A,Rivlin E.Color invariants for person reidentification[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(7):1622-1634.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_9" title=" Wu Z Y,Li Y,Radke R J.Viewpoint invariant human re-identification in camera networks using pose priors and subject-discriminative features[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(5):1095-1108." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Viewpoint invariant human re-identification in camera networks using pose priors and subject-discriminative features">
                                        <b>[9]</b>
                                         Wu Z Y,Li Y,Radke R J.Viewpoint invariant human re-identification in camera networks using pose priors and subject-discriminative features[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(5):1095-1108.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_10" title=" Wu S X,Chen Y C,Li X,&lt;i&gt;et al&lt;/i&gt;.An enhanced deep feature representation for person re-identification[C]∥2016 IEEE Winter Conference on Applications of Computer Vision (WACV),March 7-10,2016,Lake Placid,NY,USA.New York:IEEE,2016:7477681." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An enhanced deep feature representation for person re-identification">
                                        <b>[10]</b>
                                         Wu S X,Chen Y C,Li X,&lt;i&gt;et al&lt;/i&gt;.An enhanced deep feature representation for person re-identification[C]∥2016 IEEE Winter Conference on Applications of Computer Vision (WACV),March 7-10,2016,Lake Placid,NY,USA.New York:IEEE,2016:7477681.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_11" title=" Zhu X B,Che J.Person re-identification algorithm based on feature fusion and subspace learning[J].Laser &amp;amp; Optoelectronics Progress,2019,56(2):021503.朱小波,车进.基于特征融合与子空间学习的行人重识别算法[J].激光与光电子学进展,2019,56(2):021503." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201902019&amp;v=MTU2NDNVUkxPZVplVnZGeXprV3IvQUx5clBaTEc0SDlqTXJZOUViWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Zhu X B,Che J.Person re-identification algorithm based on feature fusion and subspace learning[J].Laser &amp;amp; Optoelectronics Progress,2019,56(2):021503.朱小波,车进.基于特征融合与子空间学习的行人重识别算法[J].激光与光电子学进展,2019,56(2):021503.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_12" title=" Li Z,Chang S Y,Liang F,&lt;i&gt;et al&lt;/i&gt;.Learning locally-adaptive decision functions for person verification[C]∥2013 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3610-3617." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning locally-adaptive decision functions for person verification">
                                        <b>[12]</b>
                                         Li Z,Chang S Y,Liang F,&lt;i&gt;et al&lt;/i&gt;.Learning locally-adaptive decision functions for person verification[C]∥2013 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3610-3617.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_13" title=" Liao S C,Hu Y,Zhu X Y,&lt;i&gt;et al&lt;/i&gt;.Person re-identification by local maximal occurrence representation and metric learning[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:2197-2206." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification by local maximal occurrence representation and metric learning">
                                        <b>[13]</b>
                                         Liao S C,Hu Y,Zhu X Y,&lt;i&gt;et al&lt;/i&gt;.Person re-identification by local maximal occurrence representation and metric learning[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:2197-2206.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_14" title=" Xiong F,Gou M R,Camps O,&lt;i&gt;et al&lt;/i&gt;.Person re-identification using kernel-based metric learning methods[M]//Fleet D,Pajdla T,Schiele B,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8695:1-16." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification using kernel-based metric learning methods">
                                        <b>[14]</b>
                                         Xiong F,Gou M R,Camps O,&lt;i&gt;et al&lt;/i&gt;.Person re-identification using kernel-based metric learning methods[M]//Fleet D,Pajdla T,Schiele B,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8695:1-16.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_15" title=" Li W,Wang X G.Locally aligned feature transforms across views[C]∥2013 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3594-3601." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locally Aligned Feature Transforms across Views">
                                        <b>[15]</b>
                                         Li W,Wang X G.Locally aligned feature transforms across views[C]∥2013 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3594-3601.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_16" title=" Ojala T,Pietikainen M,Maenpaa T.Multiresolution gray-scale and rotation invariant texture classification with local binary patterns[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2002,24(7):971-987." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns">
                                        <b>[16]</b>
                                         Ojala T,Pietikainen M,Maenpaa T.Multiresolution gray-scale and rotation invariant texture classification with local binary patterns[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2002,24(7):971-987.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_17" title=" Farenzena M,Bazzani L,Perina A,&lt;i&gt;et al&lt;/i&gt;.Person re-identification by symmetry-driven accumulation of local features[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,June 13-18,2010,San Francisco,CA,USA.New York:IEEE,2010:2360-2367." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification by symmetry-driven accumulation of local features">
                                        <b>[17]</b>
                                         Farenzena M,Bazzani L,Perina A,&lt;i&gt;et al&lt;/i&gt;.Person re-identification by symmetry-driven accumulation of local features[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,June 13-18,2010,San Francisco,CA,USA.New York:IEEE,2010:2360-2367.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_18" title=" Matsukawa T,Suzuki E.Person re-identification using CNN features learned from combination of attributes[C]//2016 23rd International Conference on Pattern Recognition (ICPR),December 4-8,2016,Cancun.New York:IEEE,2016:2428-2433." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person Re-Identification Using CNN Features Learned from Combination of Attributes">
                                        <b>[18]</b>
                                         Matsukawa T,Suzuki E.Person re-identification using CNN features learned from combination of attributes[C]//2016 23rd International Conference on Pattern Recognition (ICPR),December 4-8,2016,Cancun.New York:IEEE,2016:2428-2433.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_19" title=" Chen B,Zha Y F,Li Y Q,&lt;i&gt;et al&lt;/i&gt;.Person re-identification based on convolutional neural network discriminative feature learning[J].Acta Optica Sinica,2018,38(7):0720001.陈兵,查宇飞,李运强,等.基于卷积神经网络判别特征学习的行人重识别[J].光学学报,2018,38(7):0720001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201807032&amp;v=MDYyNDJYVGJMRzRIOW5NcUk5R1pvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5emtXci9BSWo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         Chen B,Zha Y F,Li Y Q,&lt;i&gt;et al&lt;/i&gt;.Person re-identification based on convolutional neural network discriminative feature learning[J].Acta Optica Sinica,2018,38(7):0720001.陈兵,查宇飞,李运强,等.基于卷积神经网络判别特征学习的行人重识别[J].光学学报,2018,38(7):0720001.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_20" title=" Xie L X,Tian Q,Zhang B.Simple techniques make sense:feature pooling and normalization for image classification[J].IEEE Transactions on Circuits and Systems for Video Technology,2016,26(7):1251-1264." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simple Techniques Make Sense:Feature Pooling and Normalization for Image Classification">
                                        <b>[20]</b>
                                         Xie L X,Tian Q,Zhang B.Simple techniques make sense:feature pooling and normalization for image classification[J].IEEE Transactions on Circuits and Systems for Video Technology,2016,26(7):1251-1264.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_21" title=" Kostinger M,Hirzer M,Wohlhart P,&lt;i&gt;et al&lt;/i&gt;.Large scale metric learning from equivalence constraints[C]∥2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 16-21,2012,Providence,RI.New York:IEEE,2012:2288-2295." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large scale metric learning from equivalence constraints">
                                        <b>[21]</b>
                                         Kostinger M,Hirzer M,Wohlhart P,&lt;i&gt;et al&lt;/i&gt;.Large scale metric learning from equivalence constraints[C]∥2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 16-21,2012,Providence,RI.New York:IEEE,2012:2288-2295.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_22" title=" Weinberger K Q,Saul L K.Distance metric learning for large margin nearest neighbor classification[J].The Journal of Machine Learning Research,2009,10:207-244." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distance metric learning for large margin nearest neighbor classification">
                                        <b>[22]</b>
                                         Weinberger K Q,Saul L K.Distance metric learning for large margin nearest neighbor classification[J].The Journal of Machine Learning Research,2009,10:207-244.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_23" title=" Khamis S,Kuo C H,Singh V K,&lt;i&gt;et al&lt;/i&gt;.Joint learning for attribute-consistent person re-identification[M]//Agapito L,Bronstein M,Rother C.Computer vision-ECCV 2014 workshops.Lecture notes in computer science.Cham:Springer,2015,8927:134-146." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint learning for attribute-consistent person re-identification">
                                        <b>[23]</b>
                                         Khamis S,Kuo C H,Singh V K,&lt;i&gt;et al&lt;/i&gt;.Joint learning for attribute-consistent person re-identification[M]//Agapito L,Bronstein M,Rother C.Computer vision-ECCV 2014 workshops.Lecture notes in computer science.Cham:Springer,2015,8927:134-146.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_24" title=" Ye M,Li J W,Ma A J,&lt;i&gt;et al&lt;/i&gt;.Dynamic graph co-matching for unsupervised video-based person re-identification[J].IEEE Transactions on Image Processing,2019,28(6):2976-2990." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dynamic graph co-matching for unsupervised video-based person re-identification">
                                        <b>[24]</b>
                                         Ye M,Li J W,Ma A J,&lt;i&gt;et al&lt;/i&gt;.Dynamic graph co-matching for unsupervised video-based person re-identification[J].IEEE Transactions on Image Processing,2019,28(6):2976-2990.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_25" title=" L&#252; J,Chen W H,Li Q,&lt;i&gt;et al&lt;/i&gt;.Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT.New York:IEEE,2018:7948-7956." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Patterns">
                                        <b>[25]</b>
                                         L&#252; J,Chen W H,Li Q,&lt;i&gt;et al&lt;/i&gt;.Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT.New York:IEEE,2018:7948-7956.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_26" title=" Cheng D S,Cristani M,Stoppa M,&lt;i&gt;et al&lt;/i&gt;.Custom pictorial structures for re-identification[C]//The British Machine Vision Conference 2011,August 29-September 2,2011,Dundee.[S.l.]:BMVC Press,2011:68." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Custom pictorial structures for re-identification">
                                        <b>[26]</b>
                                         Cheng D S,Cristani M,Stoppa M,&lt;i&gt;et al&lt;/i&gt;.Custom pictorial structures for re-identification[C]//The British Machine Vision Conference 2011,August 29-September 2,2011,Dundee.[S.l.]:BMVC Press,2011:68.
                                    </a>
                                </li>
                                <li id="65">


                                    <a id="bibliography_27" title=" Yi D,Lei Z,Liao S C,&lt;i&gt;et al&lt;/i&gt;.Deep metric learning for person re-identification[C]//2014 22nd International Conference on Pattern Recognition,August 24-28,2014,Stockholm,Sweden.New York:IEEE,2014:34-39." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep metric learning for person re-identification">
                                        <b>[27]</b>
                                         Yi D,Lei Z,Liao S C,&lt;i&gt;et al&lt;/i&gt;.Deep metric learning for person re-identification[C]//2014 22nd International Conference on Pattern Recognition,August 24-28,2014,Stockholm,Sweden.New York:IEEE,2014:34-39.
                                    </a>
                                </li>
                                <li id="67">


                                    <a id="bibliography_28" title=" Li W,Zhao R,Xiao T,&lt;i&gt;et al&lt;/i&gt;.DeepReID:deep filter pairing neural network for person re-identification[C]∥2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:152-159." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deepreid Deep filter pairing neural network for person re-identification">
                                        <b>[28]</b>
                                         Li W,Zhao R,Xiao T,&lt;i&gt;et al&lt;/i&gt;.DeepReID:deep filter pairing neural network for person re-identification[C]∥2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:152-159.
                                    </a>
                                </li>
                                <li id="69">


                                    <a id="bibliography_29" title=" Yan Y C,Ni B B,Song Z C,&lt;i&gt;et al&lt;/i&gt;.Person re-identification via recurrent feature aggregation[M]//Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9910:701-716." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person Re-identification via Recurrent Feature Aggregation">
                                        <b>[29]</b>
                                         Yan Y C,Ni B B,Song Z C,&lt;i&gt;et al&lt;/i&gt;.Person re-identification via recurrent feature aggregation[M]//Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9910:701-716.
                                    </a>
                                </li>
                                <li id="71">


                                    <a id="bibliography_30" title=" Wu L,Shen C H,van den Hengel A.Deep recurrent convolutional networks for video-based person re-identification:an end-to-end approach [J/OL].(2016-06-12)[2018-12-30].https://arxiv.org/abs/1606.01609." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep recurrent convolutional networks for video-based person re-identification:an end-to-end approach">
                                        <b>[30]</b>
                                         Wu L,Shen C H,van den Hengel A.Deep recurrent convolutional networks for video-based person re-identification:an end-to-end approach [J/OL].(2016-06-12)[2018-12-30].https://arxiv.org/abs/1606.01609.
                                    </a>
                                </li>
                                <li id="73">


                                    <a id="bibliography_31" title=" Dai J,Zhang P P,Wang D,&lt;i&gt;et al&lt;/i&gt;.Video person re-identification by temporal residual learning[J].IEEE Transactions on Image Processing,2019,28(3):1366-1377." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Video person re-identification by temporal residual learning">
                                        <b>[31]</b>
                                         Dai J,Zhang P P,Wang D,&lt;i&gt;et al&lt;/i&gt;.Video person re-identification by temporal residual learning[J].IEEE Transactions on Image Processing,2019,28(3):1366-1377.
                                    </a>
                                </li>
                                <li id="75">


                                    <a id="bibliography_32" title=" You J J,Wu A C,Li X,&lt;i&gt;et al&lt;/i&gt;.Top-push video-based person re-identification[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1345-1353." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Top-push video-based person re-identification">
                                        <b>[32]</b>
                                         You J J,Wu A C,Li X,&lt;i&gt;et al&lt;/i&gt;.Top-push video-based person re-identification[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1345-1353.
                                    </a>
                                </li>
                                <li id="77">


                                    <a id="bibliography_33" title=" Yager R R.On ordered weighted averaging aggregation operators in multicriteria decisionmaking[J].IEEE Transactions on Systems,Man,and Cybernetics,1988,18(1):183-190." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On ordered weighted averaging aggregation operators in multicriteria decisionmaking">
                                        <b>[33]</b>
                                         Yager R R.On ordered weighted averaging aggregation operators in multicriteria decisionmaking[J].IEEE Transactions on Systems,Man,and Cybernetics,1988,18(1):183-190.
                                    </a>
                                </li>
                                <li id="79">


                                    <a id="bibliography_34" title=" Wang T Q,Gong S G,Zhu X T,&lt;i&gt;et al&lt;/i&gt;.Person re-identification by video ranking[M]//Fleet D,Pajdla T,Schiele B,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8692:688-703." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification by video ranking">
                                        <b>[34]</b>
                                         Wang T Q,Gong S G,Zhu X T,&lt;i&gt;et al&lt;/i&gt;.Person re-identification by video ranking[M]//Fleet D,Pajdla T,Schiele B,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8692:688-703.
                                    </a>
                                </li>
                                <li id="81">


                                    <a id="bibliography_35" title=" Hirzer M,Beleznai C,Roth P M,&lt;i&gt;et al&lt;/i&gt;.Person re-identification by descriptive and discriminative classification[M]//Heyden A,Kahl F.Image analysis.Lecture notes in computer science.Berlin,Heidelberg:Springer,2011,6688:91-102." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification by descriptive and discriminative classification">
                                        <b>[35]</b>
                                         Hirzer M,Beleznai C,Roth P M,&lt;i&gt;et al&lt;/i&gt;.Person re-identification by descriptive and discriminative classification[M]//Heyden A,Kahl F.Image analysis.Lecture notes in computer science.Berlin,Heidelberg:Springer,2011,6688:91-102.
                                    </a>
                                </li>
                                <li id="83">


                                    <a id="bibliography_36" title=" Jia Y Q,Shelhamer E,Donahue J,&lt;i&gt;et al&lt;/i&gt;.Caffe:convolutional architecture for fast feature embedding [J/OL].(2014-06-20)[2019-01-01].https://arxiv.org/abs/1408.5093." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffe:convolutional architecture for fast feature embedding">
                                        <b>[36]</b>
                                         Jia Y Q,Shelhamer E,Donahue J,&lt;i&gt;et al&lt;/i&gt;.Caffe:convolutional architecture for fast feature embedding [J/OL].(2014-06-20)[2019-01-01].https://arxiv.org/abs/1408.5093.
                                    </a>
                                </li>
                                <li id="85">


                                    <a id="bibliography_37" title=" Kodirov E,Xiang T,Fu Z Y,&lt;i&gt;et al&lt;/i&gt;.Person re-identification by unsupervised &lt;i&gt;l&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt; graph learning[M]//Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9905:178-195." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person Re-Identification by Unsupervised L1 Graph Learning">
                                        <b>[37]</b>
                                         Kodirov E,Xiang T,Fu Z Y,&lt;i&gt;et al&lt;/i&gt;.Person re-identification by unsupervised &lt;i&gt;l&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt; graph learning[M]//Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9905:178-195.
                                    </a>
                                </li>
                                <li id="87">


                                    <a id="bibliography_38" title=" Karanam S,Li Y,Radke R J.Person re-identification with discriminatively trained viewpoint invariant dictionaries[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:4516-4524." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification with discriminatively trained viewpoint invariant dictionaries">
                                        <b>[38]</b>
                                         Karanam S,Li Y,Radke R J.Person re-identification with discriminatively trained viewpoint invariant dictionaries[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:4516-4524.
                                    </a>
                                </li>
                                <li id="89">


                                    <a id="bibliography_39" title=" Karanam S,Li Y,Radke R J.Sparse re-id:block sparsity for person re-identification[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:33-40." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparse re-id:Block sparsity for person re-identification">
                                        <b>[39]</b>
                                         Karanam S,Li Y,Radke R J.Sparse re-id:block sparsity for person re-identification[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:33-40.
                                    </a>
                                </li>
                                <li id="91">


                                    <a id="bibliography_40" title=" Zhao R,Ouyang W L,Wang X G.Unsupervised salience learning for person re-identification[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3586-3593." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised salience learning for person re-identification">
                                        <b>[40]</b>
                                         Zhao R,Ouyang W L,Wang X G.Unsupervised salience learning for person re-identification[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3586-3593.
                                    </a>
                                </li>
                                <li id="93">


                                    <a id="bibliography_41" title=" Cho Y J,Yoon K J.Improving person re-identification via pose-aware multi-shot matching[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1354-1362." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving Person Re-identification via Pose-Aware Multi-shot Matching">
                                        <b>[41]</b>
                                         Cho Y J,Yoon K J.Improving person re-identification via pose-aware multi-shot matching[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1354-1362.
                                    </a>
                                </li>
                                <li id="95">


                                    <a id="bibliography_42" title=" Khan F M,Bremond F.Unsupervised data association for metric learning in the context of multi-shot person re-identification[C]//2016 13th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),August 23-26,2016,Colorado Springs,CO,USA.New York:IEEE,2016:256-262." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised data association for metric learning in the context of multi-shot person re-identification">
                                        <b>[42]</b>
                                         Khan F M,Bremond F.Unsupervised data association for metric learning in the context of multi-shot person re-identification[C]//2016 13th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),August 23-26,2016,Colorado Springs,CO,USA.New York:IEEE,2016:256-262.
                                    </a>
                                </li>
                                <li id="97">


                                    <a id="bibliography_43" title=" Lisanti G,Masi I,Bagdanov A D,&lt;i&gt;et al&lt;/i&gt;.Person re-identification by iterative re-weighted sparse ranking[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(8):1629-1642." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification by iterative re-weighted sparse ranking">
                                        <b>[43]</b>
                                         Lisanti G,Masi I,Bagdanov A D,&lt;i&gt;et al&lt;/i&gt;.Person re-identification by iterative re-weighted sparse ranking[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(8):1629-1642.
                                    </a>
                                </li>
                                <li id="99">


                                    <a id="bibliography_44" title=" Zheng L,Bie Z,Sun Y F,&lt;i&gt;et al&lt;/i&gt;.MARS:a video benchmark for large-scale person re-identification[M]∥Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9910:868-884." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MARS: A Video Benchmark for Large-Scale Person Re-Identification">
                                        <b>[44]</b>
                                         Zheng L,Bie Z,Sun Y F,&lt;i&gt;et al&lt;/i&gt;.MARS:a video benchmark for large-scale person re-identification[M]∥Leibe B,Matas J,Sebe N,&lt;i&gt;et al&lt;/i&gt;.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9910:868-884.
                                    </a>
                                </li>
                                <li id="101">


                                    <a id="bibliography_45" title=" Zhou Z,Huang Y,Wang W,&lt;i&gt;et al&lt;/i&gt;.See the forest for the trees:joint spatial and temporal recurrent neural networks for video-based person re-identification[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI.New York:IEEE,2017:6776-6785." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=See the Forest for the Trees:Joint Spatial and Temporal Recurrent Neural Networks for Video-based Person Re-identification">
                                        <b>[45]</b>
                                         Zhou Z,Huang Y,Wang W,&lt;i&gt;et al&lt;/i&gt;.See the forest for the trees:joint spatial and temporal recurrent neural networks for video-based person re-identification[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI.New York:IEEE,2017:6776-6785.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-06-11 17:47</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(09),279-293 DOI:10.3788/AOS201939.0915006            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>联合多级深度特征表示和有序加权距离融合的视频行人再识别方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E9%94%90&amp;code=11071925&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙锐</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E5%90%AF%E6%81%92&amp;code=37751891&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄启恒</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%86%E4%BC%9F%E6%98%8E&amp;code=42914572&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陆伟明</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E9%9A%BD&amp;code=05977156&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高隽</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%90%88%E8%82%A5%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0083575&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">合肥工业大学计算机与信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B7%A5%E4%B8%9A%E5%AE%89%E5%85%A8%E4%B8%8E%E5%BA%94%E6%80%A5%E6%8A%80%E6%9C%AF%E5%AE%89%E5%BE%BD%E7%9C%81%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">工业安全与应急技术安徽省重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对目前视频行人再识别中存在视角、光线变化,背景干扰与遮挡,行人外观与行为相似,以及相同行人在不同模态特征下距离的差异性而导致的匹配不正确问题,提出一种联合多级深度特征表示和有序加权距离融合的视频行人再识别方法。在行人特征表示阶段,提出了行人多级深度特征表示网络,该网络不仅能学习视频序列中行人的时空特征,还能获取行人的全局外观特征和局部外观特征。在有序加权距离融合阶段,将行人的特征表示输入到距离测度学习中,分别计算行人在三类特征下的独立距离,并将距离排序后,根据距离的排名优化距离权值,最后融合三类距离得到最终距离,从而准确匹配行人。通过在公共数据集中的实验表明,所提方法不仅能够提高视频行人再识别的识别率,还具有丰富和完整的行人特征表示能力。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E9%A2%91%E8%A1%8C%E4%BA%BA%E5%86%8D%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视频行人再识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E7%BA%A7%E6%B7%B1%E5%BA%A6%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多级深度特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B7%9D%E7%A6%BB%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">距离融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">循环神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%89%E5%BA%8F%E5%8A%A0%E6%9D%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">有序加权;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    黄启恒,E-mail:jchqh123@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-14</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金面上项目(61471154);</span>
                                <span>安徽省科技攻关强警项目(1704d0802181);</span>
                                <span>中央高校基本科研业务费专项资金资助项目(JZ2018YYPY0287);</span>
                    </p>
            </div>
                    <h1><b>Video-Based Person Re-Identification via Combined Multi-Level Deep Feature Representation and Ordered Weighted Distance Fusion</b></h1>
                    <h2>
                    <span>Sun Rui</span>
                    <span>Huang Qiheng</span>
                    <span>Lu Weiming</span>
                    <span>Gao Jun</span>
            </h2>
                    <h2>
                    <span>School of Computer and Information, Hefei University of Technology</span>
                    <span>Anhui Provincial Key Laboratory of Industry Safety and Emergency Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Video-based person re-identification problems are caused by perspective changes, lighting variations, background clutter, occlusion, appearance similarity, motion similarity, and mismatch resulting from the distance difference of same person with different modal features. This study proposes a video-based person re-identification method that combines multi-level deep feature representation and ordered weighted distance fusion. During the stage of person feature representation, the multi-level deep feature representation network proposed herein not only learns the space-time features of the persons in video sequences but also acquires the persons′ global and local appearance features. In the stage of the ordered weighted distance fusion, the feature representations of persons are firstly input into distance metric learning, and the independent distances of persons under three types of features are calculated. The fusion algorithm then sorts the distances to optimize distance weights according to distance ranking. Finally, to accurately match a person, the algorithm fuses the three types of distances to obtain the final distance. Experimental results compared with the results of related methods in public datasets show that the proposed method not only improves the recognition rate of video-based person re-identification but also possesses abundant and integral ability for person feature representation.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=video-based%20person%20re-identification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">video-based person re-identification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-level%20deep%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-level deep feature;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=distance%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">distance fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=recurrent%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">recurrent neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ordered%20weighted&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ordered weighted;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-14</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="103" name="103" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="104">行人再识别研究的是在非重叠视域的不同摄像机下识别匹配同一行人的问题,部分文献中也称其为行人检索<citation id="201" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。近些年来,行人再识别逐渐成为计算机视觉领域的研究热点。目前,行人再识别的研究主要集中于利用单帧静止图像获取行人外观特征,解决多摄像机视角下的单视行人匹配的问题,但单帧静止图像中可能存在的背景干扰、遮挡或外观相似及变化的问题易使行人特征表示受限。因此,本研究利用视频序列中隐含的时间运动关联信息和多样的外观信息来改善行人特征表示的能力。</p>
                </div>
                <div class="p1">
                    <p id="105">虽然视频行人再识别相比于静止图像行人再识别在特征表示方面具有特有的优势,但是视频行人再识别也面临着许多挑战。首先,不同行人如果具有相似外观,便会出现行人不同但特征表示相似的问题。其次,虽然借助视频序列中行人运动关联的时空特征可改善性能,但也可能面临不同行人之间运动姿态相似的困难。最后,由于不同模态的外观特征与时空特征的简单级联可能会降低某些区分性特征的重要性,因此相同行人在不同模态特征下的距离有明显差异,特征的简单融合会使行人再识别准确性产生误差。因此,在本研究的算法中,如何构建行人完整丰富的特征表示能力和如何处理不同模态特征之间的相互影响成为算法的关键。一方面,针对视频行人再识别中不同行人外观相似和运动行为相似的问题,已有研究者提出采用卷积神经网络(CNN)和循环神经网络(RNN)相结合的方法来获取时空特征<citation id="204" type="reference"><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。这类算法主要是将CNN获取的空间外观特征向量作为RNN的输入,从而获得行人的时空特征表示。虽然这类研究工作有效地提高了视频行人再识别的识别率,但是其算法级联式的特征网络未充分获取行人的外观特征信息。当不同行人之间衣物和行为相似时,准确匹配相同行人仍然是一项难题。另一方面,现有的视频行人再识别研究工作采用的策略是将各类特征简单级联,从而忽略不同模态的特征融合所产生信息丢失的影响<citation id="202" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。同时,相同行人在不同特征下距离的差异性也会导致匹配不准确。因此,为了解决上述困难,本文采用多级深度特征表示的思想<citation id="203" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>学习行人丰富的全局、局部外观特征和稳定的时空特征。同时,受到文献<citation id="205" type="reference">[<a class="sup">6</a>,<a class="sup">7</a>]</citation>的启发,本文设计一种有序加权的距离测度融合算法,对行人的全局外观特征、局部外观特征和时空特征分别进行独立距离测度学习,再计算行人之间在三类特征下的独立距离。最后根据三类距离值的排序,优化距离融合时的权值,从而达到最佳的距离学习效果,并有效减缓相同行人在不同特征下由距离差异导致的匹配不准确问题。将本文算法应用于i-LIDS和PRID_2011两个视频数据集中,多次实验的结果表明,本文联合多级深度特征表示和有序加权距离融合的视频行人再识别算法提高了行人识别的正确率。</p>
                </div>
                <h3 id="106" name="106" class="anchor-tag">2 行人再识别</h3>
                <h4 class="anchor-tag" id="107" name="107"><b>2.1 行人再识别研究现状</b></h4>
                <div class="p1">
                    <p id="108">目前,多数行人再识别算法所采用的策略可分为两部分:提取区分性特征信息<citation id="206" type="reference"><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>和学习稳定的距离测度<citation id="207" type="reference"><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><link href="37" rel="bibliography" /><link href="39" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="109">在行人再识别领域的研究中,获取行人的特征信息主要是提取行人图像或视频序列中的外观特征,利用该特征信息匹配行人,如图 1所示。本研究认为对于行人再识别中可利用的行人特征信息可从以下两个角度分类。从特征结构的角度,可分为低层特征和中高层特征,低层特征包括颜色<citation id="208" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、纹理<citation id="209" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、轮廓<citation id="210" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>等,而在近几年获取图像的中高层特征信息成为特征提取的热点,中高层特征主要指语义特征<citation id="211" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。从特征信息的角度,可分为可视特征和隐含特征,可视特征在行人图像中指的是行人的外观特征信息和部分生物行为特征信息,而隐含特征包括视频序列中的时空特征、行人图像中的语义特征和部分隐含的生物特征等。同时,行人再识别中特征提取的方式也可以分类为手动设计和自动学习。手动设计主要用于提取行人的颜色、纹理等多种特征,而自动学习提取特征主要是应用深度学习技术自动获得行人的外观特征<citation id="212" type="reference"><link href="49" rel="bibliography" /><link href="51" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="110">目前,可以将行人再识别的距离测度学习部分分为学习特征距离矩阵和映射特征子空间的两类测度学习方法。测度学习算法被广泛应用于人脸识别、行人再识别、场景分类和其他的视觉领域中。测度学习的基本思想是从特征空间到具有某些优点的距离空间中寻找映射函数,例如来自同一个人的特征向量之间的距离比来自不同行人的特征向量之间的距离更接近。典型的测度学习方法主要包括马氏测度学习算法(KISSME)<citation id="213" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、边缘最近邻算法(LMNN)<citation id="214" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、局部自适应决定函数算法(LADF)<citation id="215" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>和语义属性相同匹配算法<citation id="216" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。此外,上述行人再识别算法主要是有监督方法,且已有研究人员集中对无监督方式进行研究。例如,Ye等<citation id="217" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>结合最优全局匹配和摄像机之间关系的优势,采用图匹配无监督技术进行准确的标签估计。Lü等<citation id="218" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>提出了一种无监督的增量学习算法(TFusion),其通过迁移学习来辅助学习目标域中行人的时空模式之间的距离映射。</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909033_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 视频行人再识别中的挑战与困难" src="Detail/GetImg?filename=images/GXXB201909033_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 视频行人再识别中的挑战与困难  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909033_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Challenges and difficulties in video-based person re-identification</p>

                </div>
                <h4 class="anchor-tag" id="113" name="113"><b>2.2 行人再识别的深度学习应用</b></h4>
                <div class="p1">
                    <p id="114">随着深度学习在计算机视觉和模式识别领域的广泛应用,已经有部分文献采用深度学习网络方法处理行人再识别中的挑战和问题。Cheng等<citation id="219" type="reference"><link href="63" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>提出了一种改进的深度学习网络,其采用成对的图像作为其输入,并输出表示两个输入图像是否为同一个人的相似度得分。Yi等<citation id="220" type="reference"><link href="65" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>构建了一个Siamese神经网络来学习成对行人图像相似性,并且还使用身体部位来训练其CNN模型。Li等<citation id="221" type="reference"><link href="67" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>提出了一种利用新的滤波器配对神经网络(FPNN)的方法,它们使用图像块匹配层来匹配视图中局部图像块的滤波器响应以及其他卷积,联合处理未对准、光照和几何变换、遮挡与背景模糊等问题,并采用最大池化层来处理身体部位的位移。</p>
                </div>
                <div class="p1">
                    <p id="115">近些年,采用深度学习网络学习行人丰富的和稳定的特征的研究已经取得突破性进展,越来越多的研究者提出不同的特征表示网络。例如,Yan等<citation id="222" type="reference"><link href="69" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>采用长短期记忆网络(LSTM)学习并聚合视频序列的时空特征。而Matsukawa等<citation id="223" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>是将预训练改进的CNN模型用于提取行人的语义特征,进而应用于行人匹配中。Wu等<citation id="224" type="reference"><link href="71" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>将CNN与门循环单元(GRU)网络结合来获取行人的时空特征表示。Dai等<citation id="225" type="reference"><link href="73" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>设计了一个能同时提取连续图像帧中全局特征与局部特征的时间残差学习模块,提高了视频行人再识别的识别率。而本研究针对级联式网络的特征融合问题,提出用行人多级特征表示网络来学习视频中行人的全局及局部外观特征信息和时空特征信息。</p>
                </div>
                <h3 id="116" name="116" class="anchor-tag">3 本文算法架构</h3>
                <div class="p1">
                    <p id="117">所提出的多级深度特征表示网络与有序加权距离测度融合相联合的视频行人再识别方法如图2所示。该算法能学习视频序列中行人的全局外观特征、局部外观特征与时空特征,同时能有序加权融合三类特征下的独立距离测度。在行人多级深度特征表示网络中,视频序列经过由卷积层与全连接层组成的全局外观特征流和由卷积层、Slice层、Concat层与全连接层组成的局部外观特征流之后,分别学习行人的全局外观特征向量<i><b>f</b></i>′<sub>g</sub>(<i><b>s</b></i>)和局部外观特征向量<i><b>f</b></i>′<sub>l</sub>(<i><b>s</b></i>),<i><b>s</b></i>为输入的图像数据。同时,在时空特征流中,部分CNN获得的特征向量<i><b>f</b></i>′<sub>R</sub>(<i><b>s</b></i>)经RNN学习后,可得到行人的时空特征向量<i><b>O</b></i>′(<i><b>s</b></i>)。最后,采用平均时间池化方法将表示该行人的全局外观特征信息、局部外观特征信息与时空特征信息分别聚合为单个特征向量<i><b>f</b></i><sub>g</sub>(<i><b>s</b></i>)、<i><b>f</b></i><sub>l</sub>(<i><b>s</b></i>)和<i><b>O</b></i>(<i><b>s</b></i>),并以此联合描述表示该行人。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909033_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 联合多级深度特征表示和有序加权距离融合的视频行人再识别算法架构" src="Detail/GetImg?filename=images/GXXB201909033_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 联合多级深度特征表示和有序加权距离融合的视频行人再识别算法架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909033_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Architecture of video-based person re-identification algorithm combining multi-level depth feature representation and 
ordered weighted distance fusion</p>

                </div>
                <div class="p1">
                    <p id="119">在有序加权距离融合算法中,将所得到的全局外观特征<i><b>f</b></i><sub>g</sub>(<i><b>s</b></i>)、局部外观特征<i><b>f</b></i><sub>l</sub>(<i><b>s</b></i>)和时空特征<i><b>O</b></i>(<i><b>s</b></i>)分别输入最优距离差异的距离测度学习算法中,并计算行人在三类特征下独立的距离<i><b>d</b></i><sub>g</sub>(<i><b>f</b></i>)、<i><b>d</b></i><sub>l</sub>(<i><b>f</b></i>)和<i><b>d</b></i><sub>o</sub>(<i><b>O</b></i>)。根据每一行人对中三类距离值的排序来优化距离融合时的权值,距离融合后得到最终距离<i><b>d</b></i>并完成行人再识别。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120"><b>3.1 行人多级深度特征表示网络</b></h4>
                <h4 class="anchor-tag" id="121" name="121">3.1.1 全局与局部的外观特征表示-CNN</h4>
                <div class="p1">
                    <p id="122">如图2中的行人多级深度特征表示网络,通过CNN和时间池化层,获得视频序列中行人丰富的全局和局部外观特征信息。一般情况下,CNN由卷积层、池化层、非线性激活函数和全连接层组成,本研究改进的CNN中增加了随机失活(Dropout)层和归一化(Normalization)层。如表1所示,全局外观特征流与局部外观特征流的网络结构主要由5层卷积层、3层池化层、Slice层、Concat层以及全连接层组成。在全局外观特征流中,在第一层池化层和第二层池化层之后都同时增加了归一化层,并且在全连接层Fc6后增加随机失活层。增加这两类网络层的目的是抑制和均衡视频序列中行人某些突出特征的表示,减少过拟合,从而获得稳定的外观特征信息。在局部外观特征流,本研究利用Slice层将最后一级卷积操作输出的行人特征图均分成上下两部分,即均分行人为上半身与下半身。再者,利用全连接层分别将分割后的两个特征图形成局部特征映射。最后,采用Concat层合并行人上半身和下半身的局部特征,从而形成更好的行人上半身与下半身的局部外观信息。此外,该网络中池化层所采用的是最大池化(Max-pooling),非线性激活函数所采用的是修正线性单元(ReLU)激活函数。</p>
                </div>
                <div class="area_img" id="123">
                    <p class="img_tit">表1 全局与局部的外观深度特征网络结构参数 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Structure parameters of global and local appearance depth feature network</p>
                    <p class="img_note"></p>
                    <table id="123" border="1"><tr><td colspan="3"><br />Network layer/Parameter</td></tr><tr><td colspan="3"><br />Conv1/Filters(96,11×11),Stride 4</td></tr><tr><td colspan="3"><br />Max pooling1/Filters(3×3),Stride 2</td></tr><tr><td colspan="3"><br />Conv2/Filters(256,5×5),Stride 1,pad 2</td></tr><tr><td colspan="3"><br />Max pooling2/Filters(3×3),Stride 2</td></tr><tr><td colspan="3"><br />Conv3/Filters(384,3×3),Stride 1,pad 1</td></tr><tr><td colspan="3"><br />Conv4/Filters(384,3×3),Stride 1,pad 1</td></tr><tr><td colspan="3"><br />Conv5/Filters(256,3×3),Stride 1,pad 1</td></tr><tr><td colspan="3"><br />Max pooling5/Filters(3×3),Stride 2</td></tr><tr><td rowspan="2"><br />Fc6 layer</td><td colspan="2"><br />Slice layer</td></tr><tr><td><br />Fc6_Lh layer</td><td>Fc6_Ll layer</td></tr><tr><td><br />Fc7 layer</td><td colspan="2">Concat layer</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="124">给定长度为<i>L</i>的视频序列<i><b>s</b></i>={<i><b>s</b></i><sup>(1)</sup>,<i><b>s</b></i><sup>(2)</sup>,…,<i><b>s</b></i><sup>(</sup><sup><i>L</i></sup><sup>)</sup>}输入该CNN后,将会输出特征向量。该过程表示为</p>
                </div>
                <div class="p1">
                    <p id="125" class="code-formula">
                        <mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi mathvariant="bold-italic">f</mi><mo>′</mo></msup><msub><mrow></mrow><mi>Κ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">s</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">s</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>,</mo><mspace width="0.25em" /><mn>1</mn><mo>≤</mo><mi>t</mi><mo>≤</mo><mi>L</mi><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="126">式中:该特征向量<i><b>f</b></i>′<i>K</i>(<i><b>s</b></i>)是经任意卷积层后输出的特征向量图,<i>K</i>∈{g,l,o,R}为每一类特征向量所对应的下标表示,在(1)式中<i>K</i>的取值为g或l,而<i><b>f</b></i>′<sub>g</sub>(<i><b>s</b></i>)、<i><b>f</b></i>′<sub>l</sub>(<i><b>s</b></i>)即为本研究所需的行人全局外观特征表示和局部外观特征表示,并将其作为平均时间池化层的输入;<i><b>s</b></i><sup>(</sup><sup><i>t</i></sup><sup>)</sup>表示视频序列中<i>t</i>时刻的原始输入图像,conv()表达式所表示的是经任意层的卷积,即经池化,即经全连接层、或归一化层后非线性运算后的结果。此外,将经部分CNN(2层卷积层)输出的特征向量<i><b>f</b></i>′<sub>R</sub>(<i><b>s</b></i>)作为RNN的输入。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127">3.1.2 时空特征表示-RNN</h4>
                <div class="p1">
                    <p id="128">由于视频序列中包含某个时间段内的行人图像,所以采用RNN提取时空特征能充分利用视频中行人的运动信息,从而改善视频行人再识别的性能。因RNN具有递归处理和记忆历史信息的特点,故通常将其用于处理时间序列和空间序列中关联性较强的信息。在视频行人再识别中,视频序列的运动信息和时态信息的关联性是普遍存在的。采用RNN将视频中行人的关联信息转化表示为结构化的循环依赖关系,即是本研究使用RNN获取行人时空的特征信息。如图3所示,本研究的RNN是"环形"递归连接,图中的横向"环形"连接具有存储记忆功能,将当前时刻的信息作为下一时刻的部分输入信息。该RNN的输入层是将3.1.1节中的部分CNN预处理后输出的特征向量作为输入。将第二层卷积层预处理后的输出向量作为时空特征流输入的主要原因是两层卷积层所处理的行人特征图具有高维的空间信息映射。如果采用更深层的卷积特征图将会丢失更多的全局空间特征信息,此时再学习时空特征将会产生信息的偏差。</p>
                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909033_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 RNN结构图" src="Detail/GetImg?filename=images/GXXB201909033_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 RNN结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909033_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Structure of RNN</p>

                </div>
                <div class="p1">
                    <p id="130">具体地看,输入特征向量<i><b>f</b></i>′<sub>R</sub>(<i><b>s</b></i>)经RNN后得到输出向量<i><b>O</b></i>′(<i><b>s</b></i>),计算公式为</p>
                </div>
                <div class="p1">
                    <p id="131" class="code-formula">
                        <mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><msup><mi mathvariant="bold-italic">Ο</mi><mo>′</mo></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">s</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold-italic">W</mi><msup><mi mathvariant="bold-italic">f</mi><mo>′</mo></msup><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">s</mi><mo stretchy="false">)</mo><mo>+</mo><mi mathvariant="bold-italic">Η</mi><mi mathvariant="bold-italic">r</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">s</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">r</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">s</mi><mo stretchy="false">)</mo><mo>=</mo><mi>tanh</mi><mo stretchy="false">[</mo><msup><mi mathvariant="bold-italic">Ο</mi><mo>′</mo></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">s</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="132">式中:<i><b>s</b></i>表示的是视频序列<i><b>s</b></i><sup>(</sup><sup><i>L</i></sup><sup>)</sup>中<i>t</i>时刻的行人图像<i><b>s</b></i><sup>(</sup><sup><i>t</i></sup><sup>)</sup>;输入向量<i><b>f</b></i>′<sub>R</sub>(<i><b>s</b></i>)包含<i>t</i>时刻递归神经网络之前的图像信息;系数矩阵<i><b>W</b></i>和<i><b>H</b></i>是将<i><b>f</b></i>′<sub>R</sub>(<i><b>s</b></i>)映射到低维特征向量空间;<i><b>r</b></i>(<i><b>s</b></i>)是tanh[]非线性函数的输出;输出向量<i><b>O</b></i>′(<i><b>s</b></i>)表示的是<i>t</i>时刻RNN的特征信息,即是行人的时空特征表示。</p>
                </div>
                <h4 class="anchor-tag" id="133" name="133">3.1.3 聚合序列多级特征-时间池化层</h4>
                <div class="p1">
                    <p id="134">虽然本研究的多级深度特征表示网络可以分别提取行人的外观特征向量和时空特征向量,但是所获得的行人信息仍然存在某些局限性,该缺陷主要体现在两方面。第一,如果同一行人完整的视频序列经网络后输出的两类序列特征信息不经过任何处理,便会抑制外观特征信息和时空特征信息的作用。因为每一视频序列中具有分辨性的信息可能出现在任何一帧图像中,而并不能完全确定行人的区分性信息一定出现在视频序列的末尾。第二,因每一视频序列的时间尺度不同,故标准递归神经网络提取不同时间尺度下视频序列的时空特征是不明确的。针对上述两方面的缺陷,本研究在CNN和RNN的输出向量后增加时间池化层解决此问题。该时间池化层不仅能聚合所有时刻的信息和避免当前时刻与之前时刻的信息偏差,还能获取视频序列中的长尺度特征信息,RNN的短尺度时间特征和时间池化层的中尺度时间特征,从而建立完整的多时间尺度信息。</p>
                </div>
                <div class="p1">
                    <p id="135">假设长度为<i>L</i>的行人视频序列经CNN和RNN后得到的行人的全局外观特征、局部外观特征和时空特征分别为<i><b>f</b></i>′<sub>g</sub>(<i><b>s</b></i>)={<i><b>f</b></i>′<sub>g</sub>(<i><b>s</b></i><sup>(1)</sup>),<i><b>f</b></i>′<sub>g</sub>(<i><b>s</b></i><sup>(2)</sup>),…,<i><b>f</b></i>′<sub>g</sub>(<i><b>s</b></i><sup>(</sup><sup><i>L</i></sup><sup>)</sup>)}、<i><b>f</b></i>′<sub>l</sub>(<i><b>s</b></i>)={<i><b>f</b></i>′<sub>l</sub>(<i><b>s</b></i><sup>(1)</sup>),<i><b>f</b></i>′<sub>l</sub>(<i><b>s</b></i><sup>(2)</sup>),…,<i><b>f</b></i>′<sub>l</sub>(<i><b>s</b></i><sup>(</sup><sup><i>L</i></sup><sup>)</sup>)}和<i><b>O</b></i>′(<i><b>s</b></i>)={<i><b>O</b></i>′(<i><b>s</b></i><sup>(1)</sup>),<i><b>O</b></i>′(<i><b>s</b></i><sup>(2)</sup>),…,<i><b>O</b></i>′(<i><b>s</b></i><sup>(</sup><sup><i>L</i></sup><sup>)</sup>)},再将三类特征向量分别输入到平均时间池化层中,该层主要把表示序列的特征向量输入并将其分别聚合转化为单个全局外观特征向量、局部外观特征向量和时空特征向量,并用此输出的特征信息表示该视频序列中的行人。该过程如(4)式、(5)式和(6)式所示:</p>
                </div>
                <div class="p1">
                    <p id="136" class="code-formula">
                        <mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mtext>g</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">s</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><msup><mi mathvariant="bold-italic">f</mi><mo>′</mo></msup></mstyle><msub><mrow></mrow><mtext>g</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">s</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mtext>l</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">s</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><msup><mi mathvariant="bold-italic">f</mi><mo>′</mo></msup></mstyle><msub><mrow></mrow><mtext>l</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">s</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Ο</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">s</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><msup><mi mathvariant="bold-italic">Ο</mi><mo>′</mo></msup></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">s</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="137" name="137"><b>3.2 有序加权距离测度融合算法</b></h4>
                <h4 class="anchor-tag" id="138" name="138">3.2.1 距离测度学习</h4>
                <div class="p1">
                    <p id="139">基于上述获得的行人的全局外观特征表示、局部外观特征表示和时空特征表示,采用文献<citation id="226" type="reference">[<a class="sup">32</a>]</citation>中的距离测度学习算法,分别计算出行人之间三类特征下的独立距离。下面将具体简述该距离测度学习算法模型建立与优化,如图4所示。</p>
                </div>
                <div class="p1">
                    <p id="140">视频行人再识别中的理想结果是对于目标行人的视频序列,在视频序列集中匹配的最优排序总是正确的。这意味着每一目标行人与视频序列集中相同行人之间的距离总是比任一目标行人与视频序列集中不相同行人之间的距离小,即是最小类间距离总是大于最大类内距离。因此,该距离测度学习模型能增大类间距离,减小类内距离。</p>
                </div>
                <div class="p1">
                    <p id="141">参数表示:将行人样本的全局外观特征、局部外观特征和时空特征分别表示为集合<i><b>v</b></i><sub>g</sub><sub><i>m</i></sub>={<i><b>f</b></i><sub>g</sub><sub><i>m</i></sub>(<i><b>s</b></i>),<i>x</i><sub><i>m</i></sub>}、集合<i><b>v</b></i><sub>l</sub><sub><i>m</i></sub>={<i><b>f</b></i><sub>l</sub><sub><i>m</i></sub>(<i><b>s</b></i>),<i>x</i><sub><i>m</i></sub>}和集合<i><b>v</b></i><sub>o</sub><sub><i>m</i></sub>={<i><b>O</b></i><sub><i>m</i></sub>(<i><b>s</b></i>),<i>x</i><sub><i>m</i></sub>},其中<i><b>f</b></i><sub>g</sub><sub><i>m</i></sub>(<i><b>s</b></i>)、<i><b>f</b></i><sub>l</sub><sub><i>m</i></sub>(<i><b>s</b></i>)和<i><b>O</b></i><sub><i>m</i></sub>(<i><b>s</b></i>)分别表示视频序列中行人<i>x</i><sub><i>m</i></sub>的全局外观特征、局部外观特征和时空特征,<i>x</i><sub><i>m</i></sub>为行人的标签,<i>m</i>表示第<i>m</i>个行人。同时,行人样本集也可表示为{<i><b>v</b></i><sub><i>K</i></sub><sub>1</sub>,<i><b>v</b></i><sub><i>K</i></sub><sub>2</sub>,…,<i><b>v</b></i><sub><i>Km</i></sub>}<sup>T</sup><sub><i>m</i></sub><sub>=1</sub>。再者,样本集中任意一对行人之间的距离表达式为</p>
                </div>
                <div class="p1">
                    <p id="142" class="code-formula">
                        <mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>w</mi><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>d</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>⋄</mo></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mi>m</mi></mrow></msub><mo>,</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mi>p</mi></mrow></msub><mo stretchy="false">)</mo><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>m</mi></msub><mo>≠</mo><mi>x</mi><msub><mrow></mrow><mi>p</mi></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>d</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>∇</mo></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mi>m</mi></mrow></msub><mo>,</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mi>w</mi></mrow></msub><mo stretchy="false">)</mo><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>m</mi></msub><mo>=</mo><mi>x</mi><msub><mrow></mrow><mi>w</mi></msub></mtd></mtr></mtable></mrow></mrow><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="143">式中:<i>d</i><sub><i>K</i></sub><sub>◇</sub>表示类间距离,<i>d</i><sub><i>K</i></sub><sub>∇</sub>表示类内距离;<i>w</i>,<i>p</i>分别表示第<i>w</i>个行人和第<i>p</i>个行人。距离的定义与计算采用的是马氏距离,即</p>
                </div>
                <div class="p1">
                    <p id="144" class="code-formula">
                        <mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mi>m</mi></mrow></msub><mo>,</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mi>w</mi></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mi>m</mi></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mi>w</mi></mrow></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Μ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mi>m</mi></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mi>w</mi></mrow></msub><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="145">式中<i><b>M</b></i>为半正定矩阵。最后,将行人之间的所有距离表示为</p>
                </div>
                <div class="p1">
                    <p id="146" class="code-formula">
                        <mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>D</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>⋄</mo></mrow></msub><mo>=</mo><mrow><mo stretchy="false">{</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>⋄</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>⋄</mo><mn>2</mn></mrow></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>⋄</mo><mi>a</mi></mrow></msub><mo stretchy="false">}</mo></mrow><msub><mrow></mrow><mrow><mi>a</mi><mo>=</mo><mi>z</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>D</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>∇</mo></mrow></msub><mo>=</mo><mrow><mo stretchy="false">{</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>∇</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>∇</mo><mn>2</mn></mrow></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>∇</mo><mi>b</mi></mrow></msub><mo stretchy="false">}</mo></mrow><msub><mrow></mrow><mrow><mi>b</mi><mo>=</mo><mi>y</mi></mrow></msub></mtd></mtr></mtable></mrow></mrow><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="147">式中:<i>D</i><sub><i>K</i></sub><sub>◇</sub>表示类间距离集合,<i>D</i><sub><i>K</i></sub><sub>∇</sub>表示类内距离集合,<i>a</i>=<i>z</i>和<i>b</i>=<i>y</i>分别表示类间距离集合与类内距离集合中的数量。</p>
                </div>
                <div class="p1">
                    <p id="148">模型建立及优化:首先,为实现相对距离比较,在类间距离集合中寻找最小距离表示为min <i>D</i><sub><i>K</i></sub><sub>◇</sub>,并期望最小类间距离总是大于每一类内距离,该思想反映为</p>
                </div>
                <div class="p1">
                    <p id="149" class="code-formula">
                        <mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>min</mi><mspace width="0.25em" /><mi>D</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>⋄</mo></mrow></msub><mo>&gt;</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>∇</mo></mrow></msub><mo>+</mo><mi>ε</mi><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="150">式中<i>ε</i>是自由参数,设定<i>ε</i>=1。同时,既要保证最大化最小类间距离与类内距离的差值,又要减小类内距离min <i>D</i><sub><i>K</i></sub><sub>∇</sub>,并在此基础上融入相对距离比较的思想,故将目标函数定义为</p>
                </div>
                <div class="p1">
                    <p id="151" class="code-formula">
                        <mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>F</mi><mo stretchy="false">(</mo><mi>D</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>β</mi><mi>min</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mi>y</mi></munder><mi>D</mi></mstyle><msub><mrow></mrow><mrow><mi>Κ</mi><mo>∇</mo></mrow></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>β</mi><mo stretchy="false">)</mo><mo>⋅</mo></mtd></mtr><mtr><mtd><mi>max</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>y</mi><mo>,</mo><mi>z</mi></mrow></munder><mo stretchy="false">{</mo></mstyle><mi>D</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>∇</mo></mrow></msub><mo>+</mo><mi>ε</mi><mo>-</mo><mi>min</mi><mspace width="0.25em" /><mi>D</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>⋄</mo></mrow></msub><mo>,</mo><mn>0</mn><mo stretchy="false">}</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="152">式中,<i>β</i>为权值平衡参数,用于均衡该式中前后两项,其范围为<i>β</i>∈[0,1]。最后,采用随机梯度下降法优化此目标函数。优化过程中的梯度计算公式为</p>
                </div>
                <div class="p1">
                    <p id="153" class="code-formula">
                        <mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>G</mi><msub><mrow></mrow><mi>q</mi></msub><mo>=</mo><mi>β</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mi>y</mi></munder><mi>D</mi></mstyle><msub><mrow></mrow><mrow><mi>Κ</mi><mo>∇</mo></mrow></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>β</mi><mo stretchy="false">)</mo><mo>⋅</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>y</mi><mo>,</mo><mi>z</mi></mrow></munder><mo stretchy="false">{</mo></mstyle><mi>D</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>∇</mo></mrow></msub><mo>-</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>⋄</mo></mrow></msub><mo stretchy="false">}</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="154">式中<i>q</i>表示第<i>q</i>次梯度运算。</p>
                </div>
                <h4 class="anchor-tag" id="155" name="155">3.2.2 有序加权的距离融合</h4>
                <div class="p1">
                    <p id="156">为了解决相同行人在不同特征下距离的差异性而导致匹配错误的问题,设计了一种有序加权的距离融合算法,利用不同的距离权值减缓和消除不同特征在相同行人下的差异性,使得再识别行人时的匹配更加准确。此距离融合的策略即是由于相同行人的不同特征具有不同的置信值,最高置信值下的特征对应的是最小距离值,所以该特征应获得最高权值的奖励。受到文献<citation id="227" type="reference">[<a class="sup">6</a>,<a class="sup">33</a>]</citation>启发,本研究依据该融合策略设计了有序加权的距离融合算法。如图5所示,根据所得到的行人之间在三类特征下的独立距离<i><b>d</b></i><sub>g</sub>(<i><b>f</b></i>)、<i><b>d</b></i><sub>l</sub>(<i><b>f</b></i>)和<i><b>d</b></i><sub>o</sub>(<i><b>O</b></i>),将每一行人对的三类距离排序,再根据距离的排名优化计算距离的权值。最后将三类距离加权融合成最终距离<i><b>d</b></i>。</p>
                </div>
                <div class="area_img" id="157">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909033_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 本文所采用的距离测度算法计算行人之间距离的流程" src="Detail/GetImg?filename=images/GXXB201909033_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 本文所采用的距离测度算法计算行人之间距离的流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909033_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Flowchart of distance metric learning algorithm used for computing distance between persons</p>

                </div>
                <div class="p1">
                    <p id="159">具体地说,将每一行人三类特征下的独立距离<i><b>d</b></i><sub>g</sub>(<i><b>f</b></i>)、<i><b>d</b></i><sub>l</sub>(<i><b>f</b></i>)和<i><b>d</b></i><sub>o</sub>(<i><b>O</b></i>)作为有序加权距离融合算法的输入,将输入的距离由小到大排序表示为集合{[<i><b>d</b></i><sub>g</sub>(<i><b>f</b></i>),<i>u</i><sub>g</sub>],[<i><b>d</b></i><sub>l</sub>(<i><b>f</b></i>),<i>u</i><sub>l</sub>],[<i><b>d</b></i><sub>o</sub>(<i><b>O</b></i>),<i>u</i><sub>o</sub>]}<sub><i>xm</i></sub><sub>,</sub><sub><i>xn</i></sub>,1≤<i>u</i><sub><i>K</i></sub>≤3,<i>u</i><sub><i>K</i></sub>∈<b>N</b><sub>+</sub>,其中<i>K</i>∈{g,l,o,R},而<i>u</i><sub><i>K</i></sub>表示所对应距离的排名,即最小距离对应<i>u</i><sub><i>K</i></sub>=1。最后,根据排名加权融合距离,此过程表示为</p>
                </div>
                <div class="area_img" id="160">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909033_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 有序加权距离融合算法图" src="Detail/GetImg?filename=images/GXXB201909033_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 有序加权距离融合算法图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909033_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Algorithm of ordered weighted distance fusion</p>

                </div>
                <div class="p1">
                    <p id="162" class="code-formula">
                        <mathml id="162"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">d</mi><mo>=</mo><mi>α</mi><msub><mrow></mrow><mtext>g</mtext></msub><mi mathvariant="bold-italic">d</mi><msub><mrow></mrow><mtext>g</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">f</mi><mo stretchy="false">)</mo><mo>+</mo><mi>α</mi><msub><mrow></mrow><mtext>l</mtext></msub><mi mathvariant="bold-italic">d</mi><msub><mrow></mrow><mtext>l</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">f</mi><mo stretchy="false">)</mo><mo>+</mo><mi>α</mi><msub><mrow></mrow><mtext>o</mtext></msub><mi mathvariant="bold-italic">d</mi><msub><mrow></mrow><mtext>o</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Ο</mi><mo stretchy="false">)</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>α</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∏</mo><mi>Κ</mi></munder><mi>u</mi></mstyle><msub><mrow></mrow><mi>Κ</mi></msub><mo>-</mo><mi>u</mi><msub><mrow></mrow><mi>Κ</mi></msub></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>Κ</mi></munder><mi>u</mi></mstyle><msub><mrow></mrow><mi>Κ</mi></msub></mrow></mfrac><mo>,</mo><mi>Κ</mi><mo>∈</mo><mo stretchy="false">{</mo><mtext>g</mtext><mo>,</mo><mtext>l</mtext><mo>,</mo><mtext>o</mtext><mo>,</mo><mtext>R</mtext><mo stretchy="false">}</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="163">式中:<i>α</i><sub><i>K</i></sub>表示的是每一类特征的特征权值;<i><b>d</b></i>表示行人之间融合的最终距离。</p>
                </div>
                <h3 id="164" name="164" class="anchor-tag">4 实验介绍</h3>
                <h4 class="anchor-tag" id="165" name="165"><b>4.1 实验数据集介绍</b></h4>
                <div class="p1">
                    <p id="166">行人再识别研究中常用的实验数据集如表2所示。下面将主要介绍本研究实验所采用的i-LIDS<citation id="228" type="reference"><link href="79" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>数据集和PRID-2011<citation id="229" type="reference"><link href="81" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>数据集。</p>
                </div>
                <div class="p1">
                    <p id="167">i-LIDS数据集包括某机场119位行人的476张图片(每位行人平均4张图片),是常用的视频(多帧)图像序列的行人再识别实验数据集。i-LIDS数据集中的视频数据由两个摄像机视角1和2组成,其中包括300位行人,600段视频序列,每位行人视频序列的长度是在23 frame到192 frame图像之间,该数据集不仅存在行人视角、姿态和光照方面的变化,还存在较严重的部分遮挡现象。</p>
                </div>
                <div class="p1">
                    <p id="168">PRID数据集包含两个数据库,分别是PRID-2011和PRID-450S。其中,PRID-2011数据集由两个摄像机视角A和B组成,其中A视角下包括385位行人,B视角下包括749位行人,每个行人视频序列的长度在5 frame到675 frame图像之间。特别注意的是, PRID-2011数据集中,A、B视角下存在200位行人是相同的。这些视频序列存在视角、光照和背景的差异。</p>
                </div>
                <h4 class="anchor-tag" id="169" name="169"><b>4.2 实验设计及评价准则</b></h4>
                <div class="p1">
                    <p id="170">实验分别将PRID-2011和i-LIDS两个数据集随机均分为两部分:训练数据集和测试数据集。在i-LIDS数据集中,训练集和测试集中的行人分别为150, 在PRID-2011数据集中,训练集和测试集中的行人分别为89。实验中设定视频序列的长度<i>L</i>为8。当数据集中视频序列长度小于8时,则将弃用该视频序列行人。实验平台采用的是搭载显卡GTX 1070,内存16G的计算机。实验采用的框架和软件是深度学习框架Caffe<citation id="230" type="reference"><link href="83" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>和Matlab软件。在行人多级深度特征表示网络训练阶段,选择三个Softmax函数作为损失函数联合预训练行人多级深度特征表示网络,并获得特征模型。训练网络时参数设定如下:基础学习率为0.0001,学习率下降策略为“均值下降(step)策略”,批大小为50,最大迭代为10000次,网络训练中所采用的优化方法为随机梯度下降法(SGD),而所采用的预训练模型是在Imagnet数据集训练好的CaffeNet模型。在距离测度学习阶段,利用获得的特征向量分别训练优化独立的距离测度矩阵。在算法的测试(行人再识别)阶段,实验选取一个摄像机视角下的视频序列作为视频集,从另一个摄像机视角中挑选某个行人视频作为目标行人视频。并将视频序列输入到已完成训练的模型中提取行人的全局外观特征、局部外观特征和时空特征,再通过距离测度学习分别计算行人之间在三类特征下的独立距离。最后利用有序加权距离融合策略计算最终距离,并通过匹配判断是否为相同行人。实验重复10次,并记录平均结果,从而获得可靠的实验结果。</p>
                </div>
                <div class="area_img" id="171">
                    <p class="img_tit">表2 行人再识别常用公共数据集 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Public datasets for person re-identification</p>
                    <p class="img_note"></p>
                    <table id="171" border="1"><tr><td><br />Experimental dataset</td><td>Characteristic</td><td>Object</td></tr><tr><td><br />PRID-2011</td><td>Viewpoint, lighting, and background changes</td><td>Outdoor person</td></tr><tr><td><br />i-LIDS</td><td>Viewpoint and lighting changes, occlusions</td><td>Airport person</td></tr><tr><td><br />VIPeR</td><td>Viewpoint, lighting, and background changes</td><td>School person</td></tr><tr><td><br />CAVIAR4REID</td><td>Lighting changes and occlusions</td><td>Shopping person</td></tr><tr><td><br />GRID</td><td>Viewpoint and lighting changes, low resolution</td><td>Subway person</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="173">实验评价行人再识别算法优劣的准则是累计匹配特征曲线(CMC)和排序-<i>n</i>(Rank-<i>n</i>)识别率。CMC是描述和计算行人再识别算法性能表现的评价依据, Rank-<i>n</i>识别率(单位为%)是指在Rank-<i>n</i>的列表中,目标视频序列与视频序列集中匹配的准确率。</p>
                </div>
                <h4 class="anchor-tag" id="174" name="174"><b>4.3 与基于静止图像的相关方法的对比实验</b></h4>
                <div class="p1">
                    <p id="175">将所提算法与基于静止图像的相关行人再识别方法在i-LIDS和PRID-2011两个数据集中进行比较,结果如表3、表4所示,所选择的相关传统经典的基于静止图像行人再识别的方法包括以下4种:GRDL算法<citation id="231" type="reference"><link href="85" rel="bibliography" /><sup>[<a class="sup">37</a>]</sup></citation>、DVDL算法<citation id="232" type="reference"><link href="87" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>、SRID算法<citation id="233" type="reference"><link href="89" rel="bibliography" /><sup>[<a class="sup">39</a>]</sup></citation>、Salience算法<citation id="234" type="reference"><link href="91" rel="bibliography" /><sup>[<a class="sup">40</a>]</sup></citation>,表中的加粗数据表示为最高的Rank-<i>n</i>识别率。在i-LIDS数据集中,综合来看,所提算法的Rank-<i>n</i>识别率都高于其余4种算法的识别率;具体来看,所提算法的Rank-1识别率比GRDL算法高出30%,比DVDL算法高出29.8%。在PRID-2011数据集中,所提算法的Rank-1识别率与DVDL算法高出31.2%,比GRDL算法高出30.2%,而与其余算法相比,所提算法的识别率都优于其余两类算法。分析认为所提算法相比传统算法识别率较高的原因是描述行人的多级深度特征相比传统手工特征具有较强的辨识性,从而算法中行人的表征能力得到增强。图6给出所提算法与其余4类算法的CMC比较结果如图中所示,黑色虚线即表示所提算法在i-LIDS和PRID-2011实验数据集中的CMC。</p>
                </div>
                <div class="p1">
                    <p id="176">参考实验表3、表4中的数据和图6中的CMC,与基于静止图像的相关方法对比,发现所提算法体现基于视频的行人再识别方法中视频的特殊优势,在一定程度上增强了视频行人再识别中行人的多级特征表示能力,丰富的行人多级特征信息能提高行人再识别的准确率。</p>
                </div>
                <div class="area_img" id="177">
                    <p class="img_tit">表3 与基于静止图像的相关方法在i-LIDS数据集比较结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Results of comparison with still-image-based person re-identification methods on i-LIDS dataset</p>
                    <p class="img_note"></p>
                    <table id="177" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="4"><br />Matching rate of i-LIDS dataset <br />(image-based methods) /%</td></tr><tr><td><br />Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>Rank-20</td></tr><tr><td><br />Proposed</td><td><b>55.7</b></td><td><b>84.7</b></td><td><b>95.3</b></td><td><b>96.7</b></td></tr><tr><td><br />GRDL</td><td>25.7</td><td>49.9</td><td>63.2</td><td>77.6</td></tr><tr><td><br />DVDL</td><td>25.9</td><td>48.2</td><td>57.3</td><td>68.9</td></tr><tr><td><br />SRID</td><td>24.9</td><td>44.5</td><td>55.6</td><td>66.2</td></tr><tr><td><br />Salience</td><td>10.2</td><td>24.8</td><td>35.5</td><td>52.9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="178">
                    <p class="img_tit">表4 与基于静止图像的相关方法在PRID-2011数据集比较结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Results of comparison with still-image-based person re-identification methods on PRID 2011 dataset</p>
                    <p class="img_note"></p>
                    <table id="178" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="4"><br />Matching rate of PRID-2011 dataset <br />(image-based methods) /%</td></tr><tr><td><br />Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>Rank-20</td></tr><tr><td><br />Proposed</td><td><b>71.8</b></td><td><b>90.4</b></td><td><b>94.1</b></td><td><b>97.5</b></td></tr><tr><td><br />GRDL</td><td>41.6</td><td>76.4</td><td>84.9</td><td>89.9</td></tr><tr><td><br />DVDL</td><td>40.6</td><td>69.7</td><td>77.8</td><td>85.6</td></tr><tr><td><br />SRID</td><td>35.1</td><td>59.4</td><td>69.8</td><td>79.7</td></tr><tr><td><br />Salience</td><td>25.8</td><td>43.6</td><td>52.6</td><td>62.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="179" name="179"><b>4.4 与基于视频的相关方法对比实验</b></h4>
                <div class="p1">
                    <p id="180">为了评价所提算法与相关的基于视频行人再识别方法的有效性,实验选取6种相关的视频行人再识别的方法,分别如下:TDL<citation id="235" type="reference"><link href="75" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>、DGM+IDE<citation id="236" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>(IDE表示距离测度算法)、VR<citation id="237" type="reference"><link href="79" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>、PAMM<citation id="238" type="reference"><link href="93" rel="bibliography" /><sup>[<a class="sup">41</a>]</sup></citation>、UNKISS<citation id="239" type="reference"><link href="95" rel="bibliography" /><sup>[<a class="sup">42</a>]</sup></citation>、ISR<citation id="240" type="reference"><link href="97" rel="bibliography" /><sup>[<a class="sup">43</a>]</sup></citation>。如表5、表6所示,表中加粗数据表示最高的Rank-<i>n</i>识别率。从表中具体来看,在i-LIDS数据集中,所提算法的Rank-1识别率优于大部分算法,仅低于TDL算法0.6%。在PRID-2011数据集中,所提算法的Rank-1识别率都高于相对比的所有算法,并高于TDL算法15.1%。图7给出所提算法与6种对比算法在i-LIDS和PRID-2011两个数据集上的CMC,虚线表示所提算法。分析认为所提算法相比于6种视频行人再识别传统算法呈现较好结果的原因是丰富的外观特征表示给算法的特征表示模型带来了稳定性与全面性。表5、表6和图7的结果验证了所提算法表现性能在i-LIDS和PRID-2011两个数据集中具有一定的有效性。</p>
                </div>
                <div class="area_img" id="181">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909033_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 与基于静止图像行人再识别方法在i-LIDS数据集和PRID-2011数据集相比较的CMC。" src="Detail/GetImg?filename=images/GXXB201909033_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 与基于静止图像行人再识别方法在i-LIDS数据集和PRID-2011数据集相比较的CMC。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909033_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 CMCs of comparison with still-image-based person re-identification methods on i-LIDS and PRID-2011datasets.</p>
                                <p class="img_note">(a)i-LIDS数据集结果；（b)PRID-2011数据集结果</p>
                                <p class="img_note">(a)i-LIDS dataset;(b)PRID-2011dataset</p>

                </div>
                <div class="area_img" id="182">
                    <p class="img_tit">表5 与基于视频行人再识别的相关方法在i-LIDS数据集比较结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Results of comparison with video-based person re-identification methods on i-LIDS dataset</p>
                    <p class="img_note"></p>
                    <table id="182" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="4"><br />Matching rate of i-LIDS dataset <br />(video-based methods) /%</td></tr><tr><td><br />Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>Rank-20</td></tr><tr><td><br />Proposed</td><td>55.7</td><td>84.7</td><td>95.3</td><td>96.7</td></tr><tr><td><br />TDL</td><td><b>56.3</b></td><td><b>87.6</b></td><td><b>95.6</b></td><td><b>98.3</b></td></tr><tr><td><br />DGM+IDE</td><td>37.2</td><td>62.6</td><td>73.4</td><td>80.8</td></tr><tr><td><br />VR</td><td>23.3</td><td>42.4</td><td>55.3</td><td>68.4</td></tr><tr><td><br />PAMM</td><td>30.3</td><td>56.3</td><td>70.3</td><td>82.7</td></tr><tr><td><br />UNKISS</td><td>35.9</td><td>63.3</td><td>74.9</td><td>83.4</td></tr><tr><td><br />ISR</td><td>11.6</td><td>22.1</td><td>27.4</td><td>36.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="183">
                    <p class="img_tit">表6 与基于视频行人再识别的相关方法在PRID-2011数据集比较结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 6 Results of comparison with video-based person re-identification methods on PRID 2011 dataset</p>
                    <p class="img_note"></p>
                    <table id="183" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="4"><br />Matching rate of PRID-2011 dataset <br />(video-based methods) /%</td></tr><tr><td><br />Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>Rank-20</td></tr><tr><td><br />Proposed</td><td><b>71.8</b></td><td><b>90.4</b></td><td><b>94.1</b></td><td><b>97.5</b></td></tr><tr><td><br />TDL</td><td>56.7</td><td>80.0</td><td>87.6</td><td>93.6</td></tr><tr><td><br />DGM+IDE</td><td>61.6</td><td>89.0</td><td>94.8</td><td>98.2</td></tr><tr><td><br />VR</td><td>28.9</td><td>55.3</td><td>65.5</td><td>82.8</td></tr><tr><td><br />PAMM</td><td>45.0</td><td>72.0</td><td>85.0</td><td>92.5</td></tr><tr><td><br />UNKISS</td><td>58.1</td><td>81.9</td><td>89.6</td><td>96.0</td></tr><tr><td><br />ISR</td><td>17.6</td><td>35.8</td><td>43.0</td><td>54.4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="184">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909033_184.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 与基于视频行人再识别方法在i-LIDS数据集和PRID-2011数据集相比较的CMC。" src="Detail/GetImg?filename=images/GXXB201909033_184.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 与基于视频行人再识别方法在i-LIDS数据集和PRID-2011数据集相比较的CMC。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909033_184.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 CMCs of comparison with video-based person re-identification methods on i-LIDS and PRID-2011datasets.</p>
                                <p class="img_note">(a)i-LIDS数据集；（b)PRID-2011数据集</p>
                                <p class="img_note">(a)i-LIDS dataset;(b)PRID-2011dataset</p>

                </div>
                <h4 class="anchor-tag" id="186" name="186"><b>4.5 与基于深度学习的相关方法对比实验</b></h4>
                <div class="p1">
                    <p id="187">此节实验是将所提算法与相关的基于深度学习行人再识别方法进行比较,目的是评价所提出联合多级深度特征表示和有序加权距离融合的视频行人再识别方法的性能表现。如表7、表8所示,此次实验选取的6种基于深度学习行人再识别的方法如下:RFA<citation id="241" type="reference"><link href="69" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>、Deep-RCN<citation id="242" type="reference"><link href="71" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>、CNN-KISS<citation id="243" type="reference"><link href="99" rel="bibliography" /><sup>[<a class="sup">44</a>]</sup></citation>、BRNN<citation id="244" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、SRM-TAM<citation id="245" type="reference"><link href="101" rel="bibliography" /><sup>[<a class="sup">45</a>]</sup></citation>、TRL<citation id="246" type="reference"><link href="73" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>,表中加粗字体表示最高的识别率。从表7、表8可以看出,在i-LIDS数据集中,所提算法的Rank-1识别率领先BRNN算法仅0.4%,与SRM-TAM算法相比仅大于0.5%,与TRL算法相比小于2.0%,而与其余三种算法相比,所提算法的优势显著。在PRID-2011数据集中,所提算法的Rank-<i>n</i>识别率与BRNN算法、SRM-TAM算法、TRL算法相比具有一定的差距,但都优于其余三类算法。分析认为所提算法的识别率与BRNN算法、SRM-TAM算法、TRL算法在PRID-2011数据集中相比存在差距的可能原因是BRNN算法、SRM-TAM算法与TRL算法在应对视角变化(PRID-2011数据集基本特点)时具有优异的表现性能,而所提算法集中于应对背景干扰与遮挡(i-LIDS数据集基本特点)的问题。同时,这三类算法是端到端的学习模型,在训练优化时相比本研究的阶段式算法确有优势。图8给出所提算法与6种基于深度学习行人再识别方法的CMC结果,其中黑色曲线表示的仍然是所提算法的CMC。综合来看此次对比实验的结果,本研究中的行人多级深度特征表示网络发挥了学习提取特征的优势,具有一定的有效性和稳健性。</p>
                </div>
                <div class="area_img" id="188">
                    <p class="img_tit">表7 与基于深度学习行人再识别的相关方法在i-LIDS数据集比较结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 7 Results of comparison with deep-learning-based person re-identification methods on i-LIDS dataset</p>
                    <p class="img_note"></p>
                    <table id="188" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="4"><br />Matching rate of i-LIDS dataset <br />(deep learning methods) /%</td></tr><tr><td><br />Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>Rank-20</td></tr><tr><td><br />Proposed</td><td>55.7</td><td>84.7</td><td><b>95.3</b></td><td>96.7</td></tr><tr><td><br />RFA</td><td>49.3</td><td>76.8</td><td>85.3</td><td>90.0</td></tr><tr><td><br />Deep-RCN</td><td>42.6</td><td>70.2</td><td>86.4</td><td>92.3</td></tr><tr><td><br />CNN-KISS</td><td>48.8</td><td>75.6</td><td>-</td><td>92.6</td></tr><tr><td><br />BRNN</td><td>55.3</td><td>85.0</td><td>91.7</td><td>95.1</td></tr><tr><td><br />SRM-TAM</td><td>55.2</td><td><b>86.5</b></td><td>-</td><td><b>97.0</b></td></tr><tr><td><br />TRL</td><td><b>57.7</b></td><td>81.7</td><td>-</td><td>94.1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="189">
                    <p class="img_tit">表8 与基于深度学习行人再识别的相关方法在PRID-2011数据集比较结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 8 Results of comparison with deep-learning-based person re-identification methods on PRID 2011 dataset</p>
                    <p class="img_note"></p>
                    <table id="189" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="4"><br />Matching rate of PRID-2011 dataset <br />(deep learning methods) /%</td></tr><tr><td><br />Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>Rank-20</td></tr><tr><td><br />Proposed</td><td>71.8</td><td>90.4</td><td>94.1</td><td>97.5</td></tr><tr><td><br />RFA</td><td>58.2</td><td>85.8</td><td>93.4</td><td>97.9</td></tr><tr><td><br />Deep-RCN</td><td>49.8</td><td>77.4</td><td>90.7</td><td>94.6</td></tr><tr><td><br />CNN-KISS</td><td>69.9</td><td>90.6</td><td>-</td><td>98.2</td></tr><tr><td><br />BRNN</td><td>72.8</td><td>92.0</td><td><b>95.1</b></td><td>97.6</td></tr><tr><td><br />SRM-TAM</td><td>79.4</td><td>94.4</td><td>-</td><td>99.3</td></tr><tr><td><br />TRL</td><td><b>87.8</b></td><td><b>97.4</b></td><td><b></b></td><td><b>99.3</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="191" name="191"><b>4.6 关键因素的验证实验</b></h4>
                <div class="p1">
                    <p id="192">距离融合的影响:本次实验是为了验证有序加权的距离融合算法能缓解相同行人在不同特征下距离差异性导致的错误匹配问题,并能有效提高视频行人再识别结果。因此,将所提算法分别与独立的局部外观特征(LAF-1)、时空特征(ST-1)和全局外观特征(GAF-1)在i-LIDS和PRID-2011两个数据集中进行实验比较,如表9、表10中第1～4行所示。由以上两个数据集的实验数据可以看出,所提算法的Rank-<i>n</i>识别率显著高于对独立特征分别进行距离学习后的识别率,这表明采用有序加权的距离融合算法可以改善视频行人再识别的识别率。此外,局部外观特征(LAF-1)即将局部外观特征向量应用于距离测度学习算法后直接匹配行人,计算Rank-<i>n</i>识别率,全局外观特征(GAF-1)和时空特征(ST-1)同理。</p>
                </div>
                <div class="area_img" id="193">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909033_193.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 与基于深度学习行人再识别方法在i-LIDS数据集和PRID-2011数据集相比较的CMC。" src="Detail/GetImg?filename=images/GXXB201909033_193.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 与基于深度学习行人再识别方法在i-LIDS数据集和PRID-2011数据集相比较的CMC。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909033_193.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 CMCs of comparison with deep-learning-based person re-identification methods on i-LIDS and PRID-2011dataset.</p>
                                <p class="img_note">(a)i-LIDS数据集；（b)PRID-2011数据集</p>
                                <p class="img_note">(a)i-LIDS dataset;(b)PRID-2011dataset</p>

                </div>
                <div class="area_img" id="195">
                    <p class="img_tit">表9 关键因素在i-LIDS数据集的比较结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 9 Results of comparison with key factors on i-LIDS dataset</p>
                    <p class="img_note"></p>
                    <table id="195" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="4"><br />Matching rate of i-LIDS dataset /%</td></tr><tr><td><br />Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>Rank-20</td></tr><tr><td><br />Proposed</td><td>55.7</td><td>84.7</td><td>95.3</td><td>96.7</td></tr><tr><td><br />LAF-1</td><td>5.3</td><td>14.0</td><td>21.3</td><td>32.7</td></tr><tr><td><br />ST-1</td><td>38.7</td><td>71.3</td><td>82.3</td><td>88.0</td></tr><tr><td><br />GAF-1</td><td>46.0</td><td>74.0</td><td>79.3</td><td>89.3</td></tr><tr><td><br />LAF+ST</td><td>24.1</td><td>52.0</td><td>70.6</td><td>83.4</td></tr><tr><td><br />GAF+ST</td><td>31.5</td><td>61.4</td><td>71.3</td><td>84.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="196">
                    <p class="img_tit">表10 关键因素在PRID-2011数据集的比较结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 10 Results of comparison with key factors on PRID 2011 dataset</p>
                    <p class="img_note"></p>
                    <table id="196" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="4"><br />Matching rate of PRID-2011 dataset /%</td></tr><tr><td><br />Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>Rank-20</td></tr><tr><td><br />Proposed</td><td>71.8</td><td>90.4</td><td>94.1</td><td>97.5</td></tr><tr><td><br />LAF-1</td><td>12.4</td><td>30.3</td><td>41.6</td><td>61.8</td></tr><tr><td><br />SF-1</td><td>33.7</td><td>65.2</td><td>75.3</td><td>87.6</td></tr><tr><td><br />GAF-1</td><td>40.4</td><td>64.0</td><td>79.8</td><td>91.0</td></tr><tr><td><br />LAF+ST</td><td>28.1</td><td>53.9</td><td>68.5</td><td>80.9</td></tr><tr><td><br />GAF+ST</td><td>42.7</td><td>67.4</td><td>81.8</td><td>89.9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="197">多级特征表示的影响:此次实验是为了验证多级特征表示网络中所学习的行人多级特征信息能有效提高视频行人再识别的正确率。因此,将所提算法分别与局部外观特征及时空特征组合(LAF+ST)、全局外观特征及时空特征组合(GAF+ST)在i-LIDS和PRID-2011两个数据集中进行验证比较,如表9、表10中第1、第5、第6行。由在两个数据集的实验数据看出,所提算法的Rank-<i>n</i>识别率显著高于LAF+ST特征组合和GAF+ST特征组合的结果,验证了本研究的多级特征表示网络具有丰富的特征表示能力。其中,GAF+ST特征组合(即不分割的行人外观特征)的识别率低于所提算法(即分割后的行人外观特征)的识别率,由此能够验证局部外观特征有益于构建完整的行人特征表示。</p>
                </div>
                <div class="p1">
                    <p id="198">参数<i>β</i>的影响:本研究的距离测度学习算法中权值参数<i>β</i>的改变将会影响实验结果,在PRID-2011数据集中实验时,发现参数设定为<i>β</i>=0.15使算法模型在实验中获得较好的结果。而在i-LIDS数据集中实验时,将参数设定为<i>β</i>=0.13能获得较高的识别率。在两个数据中,当参数设定为<i>β</i>=1时,类内距离的变化最小,都将会导致过拟合现象的出现。</p>
                </div>
                <h3 id="199" name="199" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="200">提出联合多级深度特征表示和有序加权距离融合的视频行人再识别方法学习视频中行人的全局外观特征、局部外观特征和时空特征,再计算行人之间在三类特征下的独立距离并有序加权融合为最终距离,从而能准确再识别行人。其中,所提算法中提出的行人多级深度特征表示网络和距离融合算法能有效解决视频序列中行人外观、行为相似和相同行人在不同特征下距离的差异性而导致匹配不正确的问题。同时,实验结果也表明所提算法在与前沿的基于深度学习行人再识别方法比较时,依然具有一定的识别差距,分析认为这是由于本研究的行人多级深度特征表示所提取的某些背景干扰和外观相似的行人特征不具有区分性,并且本研究所采用数据集的样本数量有限而引起的。因此,进一步优化改进算法和增加训练样本数据的数量将是本课题组今后工作的重点。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="13">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Contextual constraints for person retrieval in camera networks">

                                <b>[1]</b> Bauml M,Tapaswi M,Schumann A,<i>et al</i>.Contextual constraints for person retrieval in camera networks[C]//2012 IEEE Ninth International Conference on Advanced Video and Signal-Based Surveillance,September 18-21,2012,Beijing,China.New York:IEEE,2012:221-227.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recurrent Convolutional Network for Video-Based Person Re-identification">

                                <b>[2]</b> McLaughlin N,del Rincon J M,Miller P.Recurrent convolutional network for video-based person re-identification[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1325-1334.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning bidirectional temporal cues for video-based person re-identification">

                                <b>[3]</b> Zhang W,Yu X D,He X.Learning bidirectional temporal cues for video-based person re-identification[J].IEEE Transactions on Circuits and Systems for Video Technology,2018,28(10):2768-2776.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep multi-view feature learning for person re-identification">

                                <b>[4]</b> Tao D P,Guo Y N,Yu B S,<i>et al</i>.Deep multi-view feature learning for person re-identification[J].IEEE Transactions on Circuits and Systems for Video Technology,2018,28(10):2657-2666.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich features and precise localization with region proposal network for object detection">

                                <b>[5]</b> Chu M D,Wu S,Gu Y F,<i>et al</i>.Rich features and precise localization with region proposal network for object detection[M]//Zhou J,Wang Y H,Sun Z A,<i>et al</i>.Biometric recognition.Lecture notes in computer science.Cham:Springer,2017,10568:605-614.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distance penalization and fusion for person re-identification">

                                <b>[6]</b> Mirmahboub B,Mekhalfi M L,Murino V.Distance penalization and fusion for person re-identification[C]∥2017 IEEE Winter Conference on Applications of Computer Vision (WACV),March 24-31,2017,Santa Rosa,CA,USA.New York:IEEE,2017:1306-1314.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES540EB119935514AB03D8BFDEC639AC31&amp;v=MjkyODhPZ21XMldFemVydmxOcm1lQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxod0xtMndLQT1OaWZPZmJhOEhxUytybzVNYmVnS0NYMDl2bVFUNlV0MQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Xu Y,Lu Y W.Adaptive weighted fusion:a novel fusion approach for image classification[J].Neurocomputing,2015,168:566-574.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Color Invariants for Person Reidentification">

                                <b>[8]</b> Kviatkovsky I,Adam A,Rivlin E.Color invariants for person reidentification[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(7):1622-1634.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Viewpoint invariant human re-identification in camera networks using pose priors and subject-discriminative features">

                                <b>[9]</b> Wu Z Y,Li Y,Radke R J.Viewpoint invariant human re-identification in camera networks using pose priors and subject-discriminative features[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(5):1095-1108.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An enhanced deep feature representation for person re-identification">

                                <b>[10]</b> Wu S X,Chen Y C,Li X,<i>et al</i>.An enhanced deep feature representation for person re-identification[C]∥2016 IEEE Winter Conference on Applications of Computer Vision (WACV),March 7-10,2016,Lake Placid,NY,USA.New York:IEEE,2016:7477681.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201902019&amp;v=MjM2Njg2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXprV3IvQUx5clBaTEc0SDlqTXJZOUViWVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Zhu X B,Che J.Person re-identification algorithm based on feature fusion and subspace learning[J].Laser &amp; Optoelectronics Progress,2019,56(2):021503.朱小波,车进.基于特征融合与子空间学习的行人重识别算法[J].激光与光电子学进展,2019,56(2):021503.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning locally-adaptive decision functions for person verification">

                                <b>[12]</b> Li Z,Chang S Y,Liang F,<i>et al</i>.Learning locally-adaptive decision functions for person verification[C]∥2013 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3610-3617.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification by local maximal occurrence representation and metric learning">

                                <b>[13]</b> Liao S C,Hu Y,Zhu X Y,<i>et al</i>.Person re-identification by local maximal occurrence representation and metric learning[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:2197-2206.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification using kernel-based metric learning methods">

                                <b>[14]</b> Xiong F,Gou M R,Camps O,<i>et al</i>.Person re-identification using kernel-based metric learning methods[M]//Fleet D,Pajdla T,Schiele B,<i>et al</i>.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8695:1-16.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locally Aligned Feature Transforms across Views">

                                <b>[15]</b> Li W,Wang X G.Locally aligned feature transforms across views[C]∥2013 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3594-3601.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns">

                                <b>[16]</b> Ojala T,Pietikainen M,Maenpaa T.Multiresolution gray-scale and rotation invariant texture classification with local binary patterns[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2002,24(7):971-987.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification by symmetry-driven accumulation of local features">

                                <b>[17]</b> Farenzena M,Bazzani L,Perina A,<i>et al</i>.Person re-identification by symmetry-driven accumulation of local features[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,June 13-18,2010,San Francisco,CA,USA.New York:IEEE,2010:2360-2367.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person Re-Identification Using CNN Features Learned from Combination of Attributes">

                                <b>[18]</b> Matsukawa T,Suzuki E.Person re-identification using CNN features learned from combination of attributes[C]//2016 23rd International Conference on Pattern Recognition (ICPR),December 4-8,2016,Cancun.New York:IEEE,2016:2428-2433.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201807032&amp;v=MTcxMjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl6a1dyL0FJalhUYkxHNEg5bk1xSTlHWm9RS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> Chen B,Zha Y F,Li Y Q,<i>et al</i>.Person re-identification based on convolutional neural network discriminative feature learning[J].Acta Optica Sinica,2018,38(7):0720001.陈兵,查宇飞,李运强,等.基于卷积神经网络判别特征学习的行人重识别[J].光学学报,2018,38(7):0720001.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simple Techniques Make Sense:Feature Pooling and Normalization for Image Classification">

                                <b>[20]</b> Xie L X,Tian Q,Zhang B.Simple techniques make sense:feature pooling and normalization for image classification[J].IEEE Transactions on Circuits and Systems for Video Technology,2016,26(7):1251-1264.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large scale metric learning from equivalence constraints">

                                <b>[21]</b> Kostinger M,Hirzer M,Wohlhart P,<i>et al</i>.Large scale metric learning from equivalence constraints[C]∥2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 16-21,2012,Providence,RI.New York:IEEE,2012:2288-2295.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distance metric learning for large margin nearest neighbor classification">

                                <b>[22]</b> Weinberger K Q,Saul L K.Distance metric learning for large margin nearest neighbor classification[J].The Journal of Machine Learning Research,2009,10:207-244.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint learning for attribute-consistent person re-identification">

                                <b>[23]</b> Khamis S,Kuo C H,Singh V K,<i>et al</i>.Joint learning for attribute-consistent person re-identification[M]//Agapito L,Bronstein M,Rother C.Computer vision-ECCV 2014 workshops.Lecture notes in computer science.Cham:Springer,2015,8927:134-146.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dynamic graph co-matching for unsupervised video-based person re-identification">

                                <b>[24]</b> Ye M,Li J W,Ma A J,<i>et al</i>.Dynamic graph co-matching for unsupervised video-based person re-identification[J].IEEE Transactions on Image Processing,2019,28(6):2976-2990.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Patterns">

                                <b>[25]</b> Lü J,Chen W H,Li Q,<i>et al</i>.Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,June 18-23,2018,Salt Lake City,UT.New York:IEEE,2018:7948-7956.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Custom pictorial structures for re-identification">

                                <b>[26]</b> Cheng D S,Cristani M,Stoppa M,<i>et al</i>.Custom pictorial structures for re-identification[C]//The British Machine Vision Conference 2011,August 29-September 2,2011,Dundee.[S.l.]:BMVC Press,2011:68.
                            </a>
                        </p>
                        <p id="65">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep metric learning for person re-identification">

                                <b>[27]</b> Yi D,Lei Z,Liao S C,<i>et al</i>.Deep metric learning for person re-identification[C]//2014 22nd International Conference on Pattern Recognition,August 24-28,2014,Stockholm,Sweden.New York:IEEE,2014:34-39.
                            </a>
                        </p>
                        <p id="67">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deepreid Deep filter pairing neural network for person re-identification">

                                <b>[28]</b> Li W,Zhao R,Xiao T,<i>et al</i>.DeepReID:deep filter pairing neural network for person re-identification[C]∥2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2014,Columbus,OH,USA.New York:IEEE,2014:152-159.
                            </a>
                        </p>
                        <p id="69">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person Re-identification via Recurrent Feature Aggregation">

                                <b>[29]</b> Yan Y C,Ni B B,Song Z C,<i>et al</i>.Person re-identification via recurrent feature aggregation[M]//Leibe B,Matas J,Sebe N,<i>et al</i>.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9910:701-716.
                            </a>
                        </p>
                        <p id="71">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep recurrent convolutional networks for video-based person re-identification:an end-to-end approach">

                                <b>[30]</b> Wu L,Shen C H,van den Hengel A.Deep recurrent convolutional networks for video-based person re-identification:an end-to-end approach [J/OL].(2016-06-12)[2018-12-30].https://arxiv.org/abs/1606.01609.
                            </a>
                        </p>
                        <p id="73">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Video person re-identification by temporal residual learning">

                                <b>[31]</b> Dai J,Zhang P P,Wang D,<i>et al</i>.Video person re-identification by temporal residual learning[J].IEEE Transactions on Image Processing,2019,28(3):1366-1377.
                            </a>
                        </p>
                        <p id="75">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Top-push video-based person re-identification">

                                <b>[32]</b> You J J,Wu A C,Li X,<i>et al</i>.Top-push video-based person re-identification[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1345-1353.
                            </a>
                        </p>
                        <p id="77">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On ordered weighted averaging aggregation operators in multicriteria decisionmaking">

                                <b>[33]</b> Yager R R.On ordered weighted averaging aggregation operators in multicriteria decisionmaking[J].IEEE Transactions on Systems,Man,and Cybernetics,1988,18(1):183-190.
                            </a>
                        </p>
                        <p id="79">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification by video ranking">

                                <b>[34]</b> Wang T Q,Gong S G,Zhu X T,<i>et al</i>.Person re-identification by video ranking[M]//Fleet D,Pajdla T,Schiele B,<i>et al</i>.Computer vision-ECCV 2014.Lecture notes in computer science.Cham:Springer,2014,8692:688-703.
                            </a>
                        </p>
                        <p id="81">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification by descriptive and discriminative classification">

                                <b>[35]</b> Hirzer M,Beleznai C,Roth P M,<i>et al</i>.Person re-identification by descriptive and discriminative classification[M]//Heyden A,Kahl F.Image analysis.Lecture notes in computer science.Berlin,Heidelberg:Springer,2011,6688:91-102.
                            </a>
                        </p>
                        <p id="83">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffe:convolutional architecture for fast feature embedding">

                                <b>[36]</b> Jia Y Q,Shelhamer E,Donahue J,<i>et al</i>.Caffe:convolutional architecture for fast feature embedding [J/OL].(2014-06-20)[2019-01-01].https://arxiv.org/abs/1408.5093.
                            </a>
                        </p>
                        <p id="85">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person Re-Identification by Unsupervised L1 Graph Learning">

                                <b>[37]</b> Kodirov E,Xiang T,Fu Z Y,<i>et al</i>.Person re-identification by unsupervised <i>l</i><sub>1</sub> graph learning[M]//Leibe B,Matas J,Sebe N,<i>et al</i>.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9905:178-195.
                            </a>
                        </p>
                        <p id="87">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification with discriminatively trained viewpoint invariant dictionaries">

                                <b>[38]</b> Karanam S,Li Y,Radke R J.Person re-identification with discriminatively trained viewpoint invariant dictionaries[C]//2015 IEEE International Conference on Computer Vision (ICCV),December 7-13,2015,Santiago,Chile.New York:IEEE,2015:4516-4524.
                            </a>
                        </p>
                        <p id="89">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparse re-id:Block sparsity for person re-identification">

                                <b>[39]</b> Karanam S,Li Y,Radke R J.Sparse re-id:block sparsity for person re-identification[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),June 7-12,2015,Boston,MA,USA.New York:IEEE,2015:33-40.
                            </a>
                        </p>
                        <p id="91">
                            <a id="bibliography_40" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised salience learning for person re-identification">

                                <b>[40]</b> Zhao R,Ouyang W L,Wang X G.Unsupervised salience learning for person re-identification[C]//2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 23-28,2013,Portland,OR,USA.New York:IEEE,2013:3586-3593.
                            </a>
                        </p>
                        <p id="93">
                            <a id="bibliography_41" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving Person Re-identification via Pose-Aware Multi-shot Matching">

                                <b>[41]</b> Cho Y J,Yoon K J.Improving person re-identification via pose-aware multi-shot matching[C]∥2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),June 27-30,2016,Las Vegas,NV,USA.New York:IEEE,2016:1354-1362.
                            </a>
                        </p>
                        <p id="95">
                            <a id="bibliography_42" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised data association for metric learning in the context of multi-shot person re-identification">

                                <b>[42]</b> Khan F M,Bremond F.Unsupervised data association for metric learning in the context of multi-shot person re-identification[C]//2016 13th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),August 23-26,2016,Colorado Springs,CO,USA.New York:IEEE,2016:256-262.
                            </a>
                        </p>
                        <p id="97">
                            <a id="bibliography_43" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification by iterative re-weighted sparse ranking">

                                <b>[43]</b> Lisanti G,Masi I,Bagdanov A D,<i>et al</i>.Person re-identification by iterative re-weighted sparse ranking[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(8):1629-1642.
                            </a>
                        </p>
                        <p id="99">
                            <a id="bibliography_44" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MARS: A Video Benchmark for Large-Scale Person Re-Identification">

                                <b>[44]</b> Zheng L,Bie Z,Sun Y F,<i>et al</i>.MARS:a video benchmark for large-scale person re-identification[M]∥Leibe B,Matas J,Sebe N,<i>et al</i>.Computer vision-ECCV 2016.Lecture notes in computer science.Cham:Springer,2016,9910:868-884.
                            </a>
                        </p>
                        <p id="101">
                            <a id="bibliography_45" target="_blank" href="http://scholar.cnki.net/result.aspx?q=See the Forest for the Trees:Joint Spatial and Temporal Recurrent Neural Networks for Video-based Person Re-identification">

                                <b>[45]</b> Zhou Z,Huang Y,Wang W,<i>et al</i>.See the forest for the trees:joint spatial and temporal recurrent neural networks for video-based person re-identification[C]∥2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),July 21-26,2017,Honolulu,HI.New York:IEEE,2017:6776-6785.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201909033" />
        <input id="dpi" type="hidden" value="200" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201909033&amp;v=MjQ4OTJGeXprV3IvQUlqWFRiTEc0SDlqTXBvOUdaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

