

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133923623877500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201906032%26RESULT%3d1%26SIGN%3dAquONu6K323qKgE7jqHMLW5TuiM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201906032&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201906032&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201906032&amp;v=MDAxOThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnl2blVMckpJalhUYkxHNEg5ak1xWTlHWm8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#50" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#53" data-title="2 行人重识别技术 ">2 行人重识别技术</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="&lt;b&gt;2.1 姿态敏感嵌入网络&lt;/b&gt;"><b>2.1 姿态敏感嵌入网络</b></a></li>
                                                <li><a href="#60" data-title="&lt;b&gt;2.2 PSE网络模型训练&lt;/b&gt;"><b>2.2 PSE网络模型训练</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#67" data-title="3 基于视角信息嵌入的行人重识别模型 ">3 基于视角信息嵌入的行人重识别模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#69" data-title="&lt;b&gt;3.1 基于视角信息嵌入的模型结构&lt;/b&gt;"><b>3.1 基于视角信息嵌入的模型结构</b></a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;3.2 算法实现步骤&lt;/b&gt;"><b>3.2 算法实现步骤</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#87" data-title="4 仿真实验与结果分析 ">4 仿真实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#88" data-title="&lt;b&gt;4.1 实验环境和参数设置&lt;/b&gt;"><b>4.1 实验环境和参数设置</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;4.2 数据集描述&lt;/b&gt;"><b>4.2 数据集描述</b></a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;4.3 仿真实验与结果分析&lt;/b&gt;"><b>4.3 仿真实验与结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#125" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="图1 PSE网络模型结构">图1 PSE网络模型结构</a></li>
                                                <li><a href="#74" data-title="图2 基于视角信息嵌入模型结构">图2 基于视角信息嵌入模型结构</a></li>
                                                <li><a href="#75" data-title="图3 深度可分离卷积结构">图3 深度可分离卷积结构</a></li>
                                                <li><a href="#79" data-title="图4 改进的深度可分离卷积结构">图4 改进的深度可分离卷积结构</a></li>
                                                <li><a href="#80" data-title="图5 深度可分离模块结构">图5 深度可分离模块结构</a></li>
                                                <li><a href="#101" data-title="表1 视角预测模块有效性验证实验结果">表1 视角预测模块有效性验证实验结果</a></li>
                                                <li><a href="#106" data-title="表2 改进深度可分离卷积的有效性验证实验结果">表2 改进深度可分离卷积的有效性验证实验结果</a></li>
                                                <li><a href="#107" data-title="表3 中层特征方法的有效性验证实验结果">表3 中层特征方法的有效性验证实验结果</a></li>
                                                <li><a href="#110" data-title="表4 改进模型有效性验证实验结果">表4 改进模型有效性验证实验结果</a></li>
                                                <li><a href="#119" data-title="表5 算法运行速度对比实验结果">表5 算法运行速度对比实验结果</a></li>
                                                <li><a href="#124" data-title="表6 算法结果对比">表6 算法结果对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="12">


                                    <a id="bibliography_1" title=" Guo P Y, Su A, Zhang H L, &lt;i&gt;et al&lt;/i&gt;.Online mixture of random Na&#239;ve Bayes tracker combined texture with shape feature[J].Acta Optica Sinica, 2015, 35 (3) :0315002.郭鹏宇, 苏昂, 张红良, 等.结合纹理和形状特征的在线混合随机朴素贝叶斯视觉跟踪器[J].光学学报, 2015, 35 (3) :0315002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201503025&amp;v=MzA4Njg2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeXZuVUxySUlqWFRiTEc0SDlUTXJJOUhZWVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Guo P Y, Su A, Zhang H L, &lt;i&gt;et al&lt;/i&gt;.Online mixture of random Na&#239;ve Bayes tracker combined texture with shape feature[J].Acta Optica Sinica, 2015, 35 (3) :0315002.郭鹏宇, 苏昂, 张红良, 等.结合纹理和形状特征的在线混合随机朴素贝叶斯视觉跟踪器[J].光学学报, 2015, 35 (3) :0315002.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_2" title=" Sun X W, Xu Q S, Cai Y, &lt;i&gt;et al&lt;/i&gt;.Sea sky line detection based on edge phase encoding in complicated background[J].Acta Optica Sinica, 2017, 37 (11) :1110002.孙熊伟, 徐青山, 蔡熠, 等.基于边缘相位编码的复杂背景下海天线检测[J].光学学报, 2017, 37 (11) :1110002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201711011&amp;v=MDg4OTFOcm85RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5dm5VTHJJSWpYVGJMRzRIOWI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Sun X W, Xu Q S, Cai Y, &lt;i&gt;et al&lt;/i&gt;.Sea sky line detection based on edge phase encoding in complicated background[J].Acta Optica Sinica, 2017, 37 (11) :1110002.孙熊伟, 徐青山, 蔡熠, 等.基于边缘相位编码的复杂背景下海天线检测[J].光学学报, 2017, 37 (11) :1110002.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_3" title=" Gray D, Tao H.Viewpoint invariant pedestrian recognition with an ensemble of localized features[M]//Forsyth D, Torr P, Zisserman A.Lecture Notes in Computer Science.Berlin, Heidelberg:Springer, 2008, 5302:262-275." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Viewpoint invariant pedestrian recognition with an ensemble of localized features">
                                        <b>[3]</b>
                                         Gray D, Tao H.Viewpoint invariant pedestrian recognition with an ensemble of localized features[M]//Forsyth D, Torr P, Zisserman A.Lecture Notes in Computer Science.Berlin, Heidelberg:Springer, 2008, 5302:262-275.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_4" title=" Prosser B J, Zheng W S, Gong S, &lt;i&gt;et al&lt;/i&gt;.Person re-identification by support vector ranking[C]//Proceedings of the British Machine Vision Conference.August 31-September 3, 2010, Aberystwyth, UK.Durham, England, UK:BMVA Press, 2010:21." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification by support vector ranking">
                                        <b>[4]</b>
                                         Prosser B J, Zheng W S, Gong S, &lt;i&gt;et al&lt;/i&gt;.Person re-identification by support vector ranking[C]//Proceedings of the British Machine Vision Conference.August 31-September 3, 2010, Aberystwyth, UK.Durham, England, UK:BMVA Press, 2010:21.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_5" title=" Chen Y, Fan R S, Wang J X, &lt;i&gt;et al&lt;/i&gt;.Cloud detection of ZY-3 satellite remote sensing images based on deep learning[J].Acta Optica Sinica, 2018, 38 (1) :0128005.陈洋, 范荣双, 王竞雪, 等.基于深度学习的资源三号卫星遥感影像云检测方法[J].光学学报, 2018, 38 (1) :0128005." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201801045&amp;v=MjQ5MDBJalhUYkxHNEg5bk1ybzlCWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnl2blVMckk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Chen Y, Fan R S, Wang J X, &lt;i&gt;et al&lt;/i&gt;.Cloud detection of ZY-3 satellite remote sensing images based on deep learning[J].Acta Optica Sinica, 2018, 38 (1) :0128005.陈洋, 范荣双, 王竞雪, 等.基于深度学习的资源三号卫星遥感影像云检测方法[J].光学学报, 2018, 38 (1) :0128005.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_6" title=" Li W, Zhao R, Xiao T, &lt;i&gt;et al&lt;/i&gt;.DeepReID:deep filter pairing neural network for person re-identification[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28 , 2014, Columbus, OH, USA.New York:IEEE, 2014:152-159." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepReID:Deep filter pairing neural network for person re-identification">
                                        <b>[6]</b>
                                         Li W, Zhao R, Xiao T, &lt;i&gt;et al&lt;/i&gt;.DeepReID:deep filter pairing neural network for person re-identification[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28 , 2014, Columbus, OH, USA.New York:IEEE, 2014:152-159.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_7" title=" Geng M Y, Wang Y M, Xiang T, &lt;i&gt;et al&lt;/i&gt;.Deep transfer learning for person reidentification[EB/OL]. (2016-12-22) [2018-12-20].https://arxiv.org/abs/1611.05244." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep transfer learning for person re-identification">
                                        <b>[7]</b>
                                         Geng M Y, Wang Y M, Xiang T, &lt;i&gt;et al&lt;/i&gt;.Deep transfer learning for person reidentification[EB/OL]. (2016-12-22) [2018-12-20].https://arxiv.org/abs/1611.05244.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_8" title=" Zheng L, Huang Y J, Lu H C, &lt;i&gt;et al&lt;/i&gt;.Pose invariant embedding for deep person re-identification[EB/OL]. (2017-01-26) [2018-02-20].https://arxiv.org/abs/1701.07732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pose invariant embedding for deep person re-identification">
                                        <b>[8]</b>
                                         Zheng L, Huang Y J, Lu H C, &lt;i&gt;et al&lt;/i&gt;.Pose invariant embedding for deep person re-identification[EB/OL]. (2017-01-26) [2018-02-20].https://arxiv.org/abs/1701.07732.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_9" title=" Zhang X, Luo H, Fan X, &lt;i&gt;et al&lt;/i&gt;.AlignedReID:surpassing human-level performance in person re-identification[EB/OL]. (2018-01-31) [2018-02-20].https://arxiv.org/abs/1711.08184." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=AlignedReID:surpassing human-level performance in person re-identification">
                                        <b>[9]</b>
                                         Zhang X, Luo H, Fan X, &lt;i&gt;et al&lt;/i&gt;.AlignedReID:surpassing human-level performance in person re-identification[EB/OL]. (2018-01-31) [2018-02-20].https://arxiv.org/abs/1711.08184.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_10" title=" Saquib Sarfraz M, Schumann A, Eberle A, &lt;i&gt;et al&lt;/i&gt;.A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:420-429." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking">
                                        <b>[10]</b>
                                         Saquib Sarfraz M, Schumann A, Eberle A, &lt;i&gt;et al&lt;/i&gt;.A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:420-429.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_11" title=" Chen Y B, Zhu X T, Gong S G.Person re-identification by deep learning multi-scale representations[C]//2017 IEEE International Conference on Computer Vision Workshops (ICCVW) , October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:2590-2600." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person Re-identification by Deep Learning Multi-scale Representations">
                                        <b>[11]</b>
                                         Chen Y B, Zhu X T, Gong S G.Person re-identification by deep learning multi-scale representations[C]//2017 IEEE International Conference on Computer Vision Workshops (ICCVW) , October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:2590-2600.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_12" title=" Liu Y, Yan J J, Ouyang W L.Quality aware network for set to set recognition[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4694-4703." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Quality Aware Network for Set to Set Recognition">
                                        <b>[12]</b>
                                         Liu Y, Yan J J, Ouyang W L.Quality aware network for set to set recognition[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4694-4703.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_13" title=" Cao K D, Rong Y, Li C, &lt;i&gt;et al&lt;/i&gt;.Pose-robust face recognition via deep residual equivariant mapping[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:5187-5196." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pose-robust face recognition via deep residual equivariant mapping">
                                        <b>[13]</b>
                                         Cao K D, Rong Y, Li C, &lt;i&gt;et al&lt;/i&gt;.Pose-robust face recognition via deep residual equivariant mapping[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:5187-5196.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_14" title=" Howard A G, Zhu M L, Chen B, &lt;i&gt;et al&lt;/i&gt;.MobileNets:efficient convolutional neural networks for mobile vision applications[EB/OL]. (2017-04-17) [2018-12-21].https://arxiv.org/abs/1704.04861." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mobilenets:efficient convolutional neural networks for mobile vision applications">
                                        <b>[14]</b>
                                         Howard A G, Zhu M L, Chen B, &lt;i&gt;et al&lt;/i&gt;.MobileNets:efficient convolutional neural networks for mobile vision applications[EB/OL]. (2017-04-17) [2018-12-21].https://arxiv.org/abs/1704.04861.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_15" title=" Hu J, Shen L, Sun G.Squeeze-and-excitation networks[EB/OL]. (2017-10-25) [2018-12-21].https://arxiv.org/abs/1709.01507." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Squeeze and excitation networks">
                                        <b>[15]</b>
                                         Hu J, Shen L, Sun G.Squeeze-and-excitation networks[EB/OL]. (2017-10-25) [2018-12-21].https://arxiv.org/abs/1709.01507.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_16" title=" Yu Q, Chang X, Song Y Z, &lt;i&gt;et al&lt;/i&gt;.The devil is in the middle:exploiting mid-level representations for cross-domain instance matching[EB/OL]. (2018-04-04) [2018-12-21].https://arxiv.org/abs/1711.08106." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The devil is in the middle:exploiting mid-level representations for cross-domain instance matching">
                                        <b>[16]</b>
                                         Yu Q, Chang X, Song Y Z, &lt;i&gt;et al&lt;/i&gt;.The devil is in the middle:exploiting mid-level representations for cross-domain instance matching[EB/OL]. (2018-04-04) [2018-12-21].https://arxiv.org/abs/1711.08106.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_17" title=" Zheng L, Shen L Y, Tian L, &lt;i&gt;et al&lt;/i&gt;.Scalable person re-identification:a benchmark[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1116-1124." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalable person re-identification A benchmark">
                                        <b>[17]</b>
                                         Zheng L, Shen L Y, Tian L, &lt;i&gt;et al&lt;/i&gt;.Scalable person re-identification:a benchmark[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1116-1124.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_18" title=" Ristani E, Solera F, Zou R, &lt;i&gt;et al&lt;/i&gt;.Performance measures and a data set for multi-target, multi-camera tracking[M]//Hua G, J&#233;gou H.Lecture Notes in Computer Science.Cham:Springer, 2016, 9914:17-35." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Performance Measures and a Data Set for Multi-target,Multi-camera Tracking">
                                        <b>[18]</b>
                                         Ristani E, Solera F, Zou R, &lt;i&gt;et al&lt;/i&gt;.Performance measures and a data set for multi-target, multi-camera tracking[M]//Hua G, J&#233;gou H.Lecture Notes in Computer Science.Cham:Springer, 2016, 9914:17-35.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_19" title=" Zheng L, Bie Z, Sun Y F, &lt;i&gt;et al&lt;/i&gt;.MARS:a video benchmark for large-scale person re-identification[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Computer Vision-ECCV 2016.Cham:Springer, 2016, 9910:868-884." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MARS: A Video Benchmark for Large-Scale Person Re-Identification">
                                        <b>[19]</b>
                                         Zheng L, Bie Z, Sun Y F, &lt;i&gt;et al&lt;/i&gt;.MARS:a video benchmark for large-scale person re-identification[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Computer Vision-ECCV 2016.Cham:Springer, 2016, 9910:868-884.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-03-19 09:09</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(06),262-271 DOI:10.3788/AOS201939.0615007            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于视角信息嵌入的行人重识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%AF%95%E6%99%93%E5%90%9B&amp;code=06982855&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">毕晓君</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%AA%E7%81%8F&amp;code=42150737&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">汪灏</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%93%88%E5%B0%94%E6%BB%A8%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E4%B8%8E%E9%80%9A%E4%BF%A1%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0119964&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">哈尔滨工程大学信息与通信工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出一种基于视角信息嵌入的行人重识别模型。结合行人图像视角朝向特点, 对PSE (pose-sensitive embedding) 网络结构进行了优化。首先将PSE特征向量融合部分由特征的融合改成更符合不同视角特征空间性质的三个视角单元特征向量的拼接;其次视角单元从骨架网络更浅层的blocks-3进行分离, 增加三个视角单元特征空间的差异性;最后利用改进的深度可分离卷积, 设计了一个深度可分离模块, 对视角单元进一步进行提取特征, 防止模型参数过大的同时提高网络非线性能力, 从而提高网络的泛化能力。利用Market1501、Duke-MTMC-reID和MARS数据集对所提的算法进行有效性验证实验, 结果表明所提的改进方法取得了更好的识别效果。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%89%E8%AE%A1%E7%AE%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">光计算;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">行人重识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%92%E4%BF%A1%E6%81%AF%E5%B5%8C%E5%85%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视角信息嵌入;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度残差卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度可分离卷积;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    汪灏 E-mail:jdzwanghao@hrbeu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-11</p>

            </div>
                    <h1><b>Person Re-Identification Based on View Information Embedding</b></h1>
                    <h2>
                    <span>Bi Xiaojun</span>
                    <span>Wang Hao</span>
            </h2>
                    <h2>
                    <span>College of Information and Communication Engineering, Harbin Engineering University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In this study, we propose a person re-identification model based on view information embedding. In particular, a pose-sensitive embedding (PSE) network structure is optimized based on the perspective towards characteristics of pedestrian images. First, the fusion part of the PSE feature vector is changed from feature fusion into the concatenation of the feature vectors of three view units, which is considerably reasonable for utilizing different view feature spaces. Second, the view units are separated from the shallow blocks-3 of the skeleton network, which improves the difference of the view feature space. Finally, we design a depthwise separable module based on the improved depth separable convolution to extract features of perspective units, preventing the model parameters from being considerably large and improving the network nonlinearity. The results of the validation experiments conducted using the Market1501, Duke-MTMC-reID and MARS datasets demonstrate that the proposed method can achieve a better recognition accuracy when compared with several advanced algorithms.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=optics%20in%20computing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">optics in computing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=person%20re-identification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">person re-identification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=perspective%20information%20embedding&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">perspective information embedding;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20residual%20convolution%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep residual convolution neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=depthwise%20separable%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">depthwise separable convolution;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-11</p>
                            </div>


        <!--brief start-->
                        <h3 id="50" name="50" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="51">行人重识别是用来匹配来自不相交区域摄像头捕捉到的行人图像技术。随着“天网行动”在我国的大力开展, 作为智能安防领域主要技术之一的行人重识别技术成为研究热点。受不同摄像头的安放位置、内部硬件差异以及所处区域环境不同等因素的影响, 所拍摄的行人图像存在光照、形变、遮挡、姿态差异和视角差异等问题。目前主流的行人重识别的方法是基于手工特征表示的方法, 寻找一个能够充分描述行人图像的描述模型来表示每个特定行人的特征 (如纹理特征<citation id="128" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、边缘特征<citation id="129" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、颜色直方图<citation id="130" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和联合特征<citation id="131" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>) , 从而判断不同摄像头中的行人是否为同一个人。尽管采用手工特征表示方法能够考虑到行人的多方面特征, 但对于行人重识别这样一个行人图像受多方面因素影响的情况来说, 这些手工特征表示方法的能力仍十分有限。</p>
                </div>
                <div class="p1">
                    <p id="52">近年来, 深度学习理论<citation id="132" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的提出为信号处理尤其是图像处理带来了全新的进展。特别地, 在图像特征提取方面, 深度学习方法取得了远超越基于手工特征表示方法的卓越效果。Li等<citation id="133" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>将深度学习方法引入行人重识别中, 通过在网络中添加一个补丁匹配层, 将两个图像的卷积响应与不同的水平条纹相乘得到最终的特征向量, 进而进行匹配, 该方法在一定程度上解决了行人匹配错位的问题。Geng等<citation id="134" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>通过改变深度学习训练策略, 将分类任务损失函数与验证任务损失函数一同引入行人重识别中, 让两种模式互相学习, 提高了行人重识别模型的泛化能力。Zheng等<citation id="135" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>将行人图片局部对齐的思想引入模型设计中, 利用人体姿态估计模型估计出人体关键部件区域, 再分别对每个行人的关键区域进行一对一匹配, 最终得到匹配结果。Zhang等<citation id="136" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>基于同样局部对齐的思想, 通过在局部匹配过程中引入动态对齐算法, 动态计算局部最短距离, 从而实现匹配, 该方法在有效地减少模型复杂程度的同时保证了匹配准确度。上述基于深度学习的方法虽然取得了优于手工特征表示方法的效果, 但忽视了行人的侧面、正面和背面这些视角变化所带来的深层信息, 而这些视角信息能更全面地描述行人, 对于行人重识别有着重要的影响。2018年, Saquib等<citation id="137" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出一种PSE (pose-sensitive embedding) 网络, 将行人姿态朝向的信息引入网络的设计中, 取得了较好的识别效果。但是PSE网络没有做到有效地利用姿态朝向的角度信息, 导致模型识别能力受限。为了提升行人重识别模型性能进而提高识别率, 本文从行人视角信息出发, 在PSE网络上进行改进, 利用视角朝向信息提出基于视角信息嵌入的行人重识别模型, 并取得了更好的行人图像识别效果。</p>
                </div>
                <h3 id="53" name="53" class="anchor-tag">2 行人重识别技术</h3>
                <div class="p1">
                    <p id="54">对于一张预查询行人图片, 如何在候选库中找到与其匹配的行人图片是行人重识别需要解决的问题。目前主流的基于手工特征表示方法的步骤如下:1) 设计一个具有代表性的特征提取器;2) 使用该特征提取器分别对预查询行人图片以及候选集中所有的图片进行特征向量提取, 并依次利用候选集中的图片特征向量与预查询行人图片的特征向量, 计算其向量空间距离;3) 将候选集中的图片按特征向量间距离从小到大顺序排列, 排在第一位的行人最有可能与预查询行人为同一个人。</p>
                </div>
                <div class="p1">
                    <p id="55">上述过程中, 关键的步骤在于设计一个具有代表性的特征提取器。目前行人重识别中的特征提取器主要分为两类, 分别是基于手工特征表示的方法和基于深度学习的方法。基于手工特征表示的方法受限于表达能力, 在行人重识别的特征提取过程中, 很难取得良好的识别效果。而基于深度学习的方法以其优秀的学习机制, 能够通过学习得到合适的特征表示, 并取得比基于手工特征表达方法更好的效果。目前, 在行人重识别中, 基于深度学习方法的特征表示有PSE网络<citation id="138" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、DPFL (deep pyramidal feature learning) <citation id="139" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和QAN (quality aware network) <citation id="140" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等。本研究算法就是基于PSE模型进行改进的。</p>
                </div>
                <h4 class="anchor-tag" id="56" name="56"><b>2.1 姿态敏感嵌入网络</b></h4>
                <div class="p1">
                    <p id="57">最近, Saquib等<citation id="141" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>在计算机视觉顶级会议CVPR2018上提出了一种PSE网络。该网络具有良好的泛化能力, 在行人重识别的多个数据集上取得了最优的结果。PSE网络的主要思想是将行人姿态与朝向信息引入网络, 在骨架网络侧面引入姿态朝向预测模块来辅助骨架网络, 最终取得了良好的泛化结果。PSE网络模型结构如图1所示。</p>
                </div>
                <div class="p1">
                    <p id="58">PSE网络模型由骨架、视角预测以及视角单元三个部分组成。整个网络是在ResNet50的基础上进行设计, 图中blocks-1、blocks-2、blocks-3与blocks-4对应ResNet50中的模块。</p>
                </div>
                <div class="p1">
                    <p id="59">PSE网络的输入是行人图片及其14通道的人体姿态热图。 layer-0层是将输入转化为适应ResNet50网络输入的形状。骨架部分的各个模块就是ResNet50中的对应模块。整个骨架部分用于特征预提取。将视角预测分支从骨架网络的blocks-1中的降维步骤后分离出来, 用于预测行人图像的朝向, 并输出各个朝向对应的概率值。将ResNet50中的blocks-4的结构复制三份, 分别作为代表正面、侧面和背面的视角单元, 并分别与视角预测分支的三路softmax输出进行加权, 三路加权结果从FC (fully connected layer) 输出并进行特征融合, 最终得到一个嵌入向量, 用于最终匹配。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60"><b>2.2 PSE网络模型训练</b></h4>
                <div class="p1">
                    <p id="61">PSE网络模型的训练整体分为预训练和训练两个部分。在所有训练之前需将行人重识别数据集分成训练集与测试集, 训练集由一定ID (identity) 数量的行人构成, 测试集由ID不属于训练集的行人构成。</p>
                </div>
                <div class="p1">
                    <p id="62">预训练分为三步。首先对骨架网络部分进行参数初始化, 采用在ImageNet数据集上预训练的方法进行参数初始化。然后固定骨架网络与视角单元部分, 使用RAP (richly annotated pedestrian) 数据集对视角预测部分进行预训练。最后固定视角预测部分与骨架部分, 使用行人重识别训练集对视角部分进行预训练。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906032_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 PSE网络模型结构" src="Detail/GetImg?filename=images/GXXB201906032_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 PSE网络模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906032_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Structure of PSE network model</p>

                </div>
                <div class="p1">
                    <p id="65">完成预训练后, 进行正式训练。首先根据训练集的ID数量, 使用softmax分类器设置成相同ID数量的类别, 分类器接在PSE网络的全连接层之后, 用于分类。一般地, 使用卷积神经网络框架对行人图片进行特征表示时, 需要使用softmax分类函数, 由于PSE网络模型的本质是一种卷积神经网络框架, 这里采用同样的操作。接着使用接有softmax分类器的PSE网络, 在分好的训练集上对行人图片进行分类训练, 并采用随机梯度下降算法。最后将训练好的模型参数保存在PSE网络模型中, 取下softmax分类器, 得到最终的PSE网络。</p>
                </div>
                <div class="p1">
                    <p id="66">虽然采用PSE网络模型取得了目前最优的效果, 但PSE网络并没有有效地利用视角朝向信息, 它对三路视角单元的特征采用的是特征融合的方式, 对于三路视角特征的利用不够充分有效, 造成模型的识别能力受限。在此通过实验发现, 模型结构上还有更合理的设计方案, 下面就这个方案进行说明。</p>
                </div>
                <h3 id="67" name="67" class="anchor-tag">3 基于视角信息嵌入的行人重识别模型</h3>
                <div class="p1">
                    <p id="68">在此对PSE网络进行了三个方面的改进:1) 将最终特征向量的组合方式从特征融合改成特征拼接;2) 采用骨架网络更浅层的blocks-3作为视角单元分离位置;3) 利用改进的深度可分离卷积, 设计一个深度可分离模块, 代替blocks-4对视角单元进一步进行特征提取。改进后的模型变成三个部分, 分别是骨架网络部分、视角预测部分以及视角分离部分。</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69"><b>3.1 基于视角信息嵌入的模型结构</b></h4>
                <div class="p1">
                    <p id="70">所提出的基于视角信息嵌入的模型结构如图2所示。</p>
                </div>
                <div class="p1">
                    <p id="71">首先, 将视角预测部分输出的三个朝向预测概率与三个视角单元的输出进行加权后, 可使网络在进行梯度更新时, 针对输入行人图片的朝向着重更新对应朝向的视角单元, 如当输入行人图片是正面朝向时, 视角预测部分正面概率较大, 该大概率会让梯度流向被大权值加权的那一路, 即正面朝向的视角单元, 因此不同视角单元都能特定学习对应行人朝向的特征, 这使得三路分开的视角单元具备分别以正面、侧面和背面方式描述特征的能力。实际上, 这种特征表达方式的不同反映在特征层面就是表示特征的特征空间不同。Cao等<citation id="142" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>在人脸识别任务中提出一个假设, 认为人脸的侧面与正面分别对应不同的特征空间, 并通过实验进行了验证。与人脸类似, 不同朝向的行人图像对应的特征空间也是不同的。而在PSE网络模型的视角部分, 网络对三个复制的分别代表正面、侧面和背面的特征图谱直接进行了加权融合, 而三个特征图谱所在的特征空间不同, 原PSE模型不太适合对来自不同特征空间的特征图谱进行线性加权融合。因此, 将特征融合改为特征拼接, 即将最终的三路768维特征向量首尾连接成2304维向量, 并将其作为最终的特征向量, 这就避免了在不同特征空间进行线性加权融合。</p>
                </div>
                <div class="p1">
                    <p id="72">其次, 为了更好地利用不同空间的信息, 与PSE网络在blocks-4分出三份视角单元不同的是, 所提出的方法在骨架网络更浅层的blocks-3层进行分离, 其中blocks-3_f表示ResNet50中的conv4_1, blocks-3_b表示ResNet50中的conv4的剩余部分, 这样的操作可以减少视角单元共享的学习参数, 增大分离的三个姿态单元的差异性, 从而能够更好地利用不同空间的信息。</p>
                </div>
                <div class="p1">
                    <p id="73">最后, 在PSE骨架的更浅层分离视角单元, 把Blocks-3原有的一路增加到三路, 这大大增加了模型参数量。为了防止网络过大而产生过拟合, 并保证网络的非线性, 以充分利用特征信息, 在此设计了一种深度可分离模块。深度可分离模块采用深度可分离卷积<citation id="143" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。深度可分离卷积由depthwise操作与pointwise操作构成, 其结构如图3所示。depthwise操作是使用尺寸为3×3、数量为输入通道数的卷积核在对应输入通道上进行卷积, pointwise操作是采用尺寸为1×1、数量为输出通道数的卷积核进行一般卷积操作。其中, <i>m</i>与<i>n</i>为标量, 表示通道数。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906032_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于视角信息嵌入模型结构" src="Detail/GetImg?filename=images/GXXB201906032_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基于视角信息嵌入模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906032_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Model based perspective information embedding</p>

                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906032_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 深度可分离卷积结构" src="Detail/GetImg?filename=images/GXXB201906032_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 深度可分离卷积结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906032_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Depthwise separable convolution</p>

                </div>
                <div class="p1">
                    <p id="76">所采用的深度可分离卷积是在原有结构的基础上进行改进。深度网络在提取特征时, 往往会提取大量特征, 而有些特征并不是识别所需要的特征。一个具备特征选择机制的网络, 不仅能够降低网络的冗余, 还能更加精准地进行匹配。受文献<citation id="144" type="reference">[<a class="sup">15</a>]</citation>的启发, 本研究在depthwise操作与pointwise操作之间引入挤压和激励块。挤压和激励块能对输入的各个通道特征进行加权, 通过网络学习, 对有利特征进行高加权, 对不利特征进行低加权, 从而达到特征选择的目的。改进的深度可分离卷积结构如图4所示。深度可分离模块结构如图5所示。</p>
                </div>
                <div class="p1">
                    <p id="77">深度可分离模块采用两个连续的深度可分离卷积堆叠而成, 其中每个深度可分离卷积后分别接一个BN (batch normalization) 层与ReLU单元。与原始卷积相比, 这种结构增加了网络非线性。将堆叠中间层的输出特征图谱与模块输出一同作为输出。文献<citation id="145" type="reference">[<a class="sup">16</a>]</citation>证明, 深度神经网络一般取最后一层输出作为特征匹配向量, 忽视了处在网络稍浅层的中层特征。而在进行匹配时, 不仅深层特征重要, 图像的中层特征也很重要, 中层特征在识别过程中同样能够提升识别性能。</p>
                </div>
                <div class="p1">
                    <p id="78">改进的深度可分离卷积既保证充分利用了每个通道的特征图谱信息, 又减少了模型的参数, 防止了模型过拟合情况的发生。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906032_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 改进的深度可分离卷积结构" src="Detail/GetImg?filename=images/GXXB201906032_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 改进的深度可分离卷积结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906032_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Improved depthwise separable convolution</p>

                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906032_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 深度可分离模块结构" src="Detail/GetImg?filename=images/GXXB201906032_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 深度可分离模块结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906032_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Structure of depthwise separable module</p>

                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>3.2 算法实现步骤</b></h4>
                <div class="p1">
                    <p id="82">为了便于理解, 在此给出所提算法的具体实现步骤。</p>
                </div>
                <div class="p1">
                    <p id="83">1) 将所提出的模型分为三个部分, 分别为ResNet50骨架网络、行人预测分支以及视角分离。模型损失函数采用交叉熵函数。</p>
                </div>
                <div class="p1">
                    <p id="84">2) 采用RAP (richly annotated pedestrian) 数据集对行人预测分支进行预训练。其中, 使用在ImageNet上的预训练参数作为骨架网络的参数进行初始化。训练过程中, 骨架网络部分的参数固定不训练。</p>
                </div>
                <div class="p1">
                    <p id="85">3) 用行人重识别数据集预训练视角分离部分。这里对整个网络进行训练, 但是需先固定行人预测分支与骨架网络的参数。进行反向传播时, 梯度只更新视角分离部分, 其目的是为视角分离部分的参数提供一个较为理想的初始化状态。</p>
                </div>
                <div class="p1">
                    <p id="86">4) 分别设定各个部分的学习率, 其中骨架网络、视角预测部分和视角分离部分的识别率分别为0.01, 0.001, 0.01, 用行人重识别数据集对整个网络进行训练。另外, 所提出的算法不需要引入PSE网络中的精细粒度的关节点信息, 仅使用粗粒度的行人朝向信息就能取得较原PSE网络更好的效果。</p>
                </div>
                <h3 id="87" name="87" class="anchor-tag">4 仿真实验与结果分析</h3>
                <h4 class="anchor-tag" id="88" name="88"><b>4.1 实验环境和参数设置</b></h4>
                <div class="p1">
                    <p id="89">实验中采用的硬件环境配置为Intel© Core<sup>TM</sup>i7 8700K 酷睿六核 3.7 GHz、32 G内存、64位处理器, 并采用GeForce GTX 1080 Ti GAMING X TRIO 11GB GDDR5X进行运算加速, 操作系统为64位Ubuntu 16.04, 采用基于Python3.6的深度学习框架Pytorch0.4.0完成程序编程。基础学习率为0.01, 视角单元部分的学习率为0.001, 迭代世代数为130, 批量大小为32。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90"><b>4.2 数据集描述</b></h4>
                <div class="p1">
                    <p id="91">为了验证所提出的改进模型在行人重识别任务中的优势, 在此选用目前行人重识别主流的公开数据集Market1501<citation id="146" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、Duke-MTMC-reID<citation id="147" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>和MARS<citation id="148" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>进行实验, 其中数据集的划分标准与PSE网络模型中所采用的标准<citation id="149" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>一致。</p>
                </div>
                <div class="p1">
                    <p id="92">Market1501数据集采集自清华大学校园, 由来自6台摄像机视频上的行人检测器生成的32668个边界框组成, 包括1501个不同ID行人。751人用于训练, 750人用于测试。训练集包含12936个图像, 候选集设置19732个图像, 查询集包含3368个图像。</p>
                </div>
                <div class="p1">
                    <p id="93">Duke-MTMC-reID数据集是根据8台摄像机的数据创建的。包含1404个ID行人、16522个训练图像、2228个查询图像和17661个图库图像。由于采用8台摄像机捕获了数量较多的图像, Duke-MTMC-reID数据集成为迄今为止最具挑战性的行人重识别数据集之一。</p>
                </div>
                <div class="p1">
                    <p id="94">MARS数据集是根据6台摄像机的数据创建的, 由20478个视频片段构成, 图片总数达到1191993, 包含1261个重复出现的行人, 625人用于训练, 636人用于测试。其中训练集由8298个视频片段构成, 测试集由12180个视频片段构成, 将图片训练集、测试集分别分割为509914、681089张图像。测试集中, 查询集由1980个视频片段构成, 候选集由9330个视频片段构成。在此, 训练集中所有视频片段的图片均参与训练, 对测试集中的查询集与候选集的每个行人视频片段均采用均匀抽样方法进行测试, 抽样数目为15。对该15张图片的特征向量进行加权, 得到该行人的特征向量。</p>
                </div>
                <div class="p1">
                    <p id="95">为防止模型出现过拟合现象, 通过对行人图片进行随机水平翻转与随机裁剪实现数据集增强。其中, 随机裁剪方法是把输入图片先调整到288 pixel×144 pixel大小, 再使用大小为256 pixel×128 pixel的框对调整后的图片进行随机裁剪。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96"><b>4.3 仿真实验与结果分析</b></h4>
                <div class="p1">
                    <p id="97">实验从模型的有效性与先进性两个方面进行验证, 并与目前最好的几种深度学习方法和主流机器学习方法进行对比验证。两个实验将测试集分成查询集与候选集, 本实验采用单查询方式验证结果, 单查询方式的查询集由每个ID行人在每个摄像头下的一张图片构成, 剩下的图片作为候选集。实验评价指标采用行人重识别通用评价指标——累积匹配特征 (CMC) 的排名1 (rank-1) 识别率和平均精度 (mAP) 。</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98">4.3.1 算法有效性验证实验</h4>
                <div class="p1">
                    <p id="99">为了验证所提模型的有效性, 实验选用Market1501、Duke-MTMC-reID和MARS数据集。</p>
                </div>
                <div class="p1">
                    <p id="100">首先, 所提出的模型将PSE网络中的特征融合改为特征拼接, 此时视角预测模块的用途发生改变。因此, 为了验证视角预测模块对于改进后的PSE网络仍然有效, 在此进行了视角预测模块有效性验证实验。实验结果如表1所示。</p>
                </div>
                <div class="area_img" id="101">
                                            <p class="img_tit">
                                                表1 视角预测模块有效性验证实验结果
                                                    <br />
                                                Table 1 Results of perspective predictor module verification experiment
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906032_10100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/GXXB201906032_10100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906032_10100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 视角预测模块有效性验证实验结果" src="Detail/GetImg?filename=images/GXXB201906032_10100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="102">表1中, Except perspective表示去掉视角预测模块的模型, All表示完整的改进模型。由表1可知, 在添加了视角预测模块后, 模型的性能有了大幅度提升, 这验证了视角预测模块的有效性。</p>
                </div>
                <div class="p1">
                    <p id="103">为了验证改进深度可分离卷积在所提模型中的有效性, 在此进行了改进深度可分离卷积有效性验证实验。实验结果如表2所示。</p>
                </div>
                <div class="p1">
                    <p id="104">表2中, Except SE-Block表示去除改进深度可分离卷积中的SE-Block。由表2可知, 在深度可分离卷积中添加了SE-Block之后, 模型的性能有了一定的提升, 这验证了改进深度可分离卷积的有效性。</p>
                </div>
                <div class="p1">
                    <p id="105">为了验证在深度可分离模块中使用中层特征方法的有效性, 进行了中层特征方法有效性验证实验, 实验结果如表3所示。</p>
                </div>
                <div class="area_img" id="106">
                                            <p class="img_tit">
                                                表2 改进深度可分离卷积的有效性验证实验结果
                                                    <br />
                                                Table 2 Results of improved depthwise separable convolution verification experiment
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906032_10600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/GXXB201906032_10600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906032_10600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 改进深度可分离卷积的有效性验证实验结果" src="Detail/GetImg?filename=images/GXXB201906032_10600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="107">
                                            <p class="img_tit">
                                                表3 中层特征方法的有效性验证实验结果
                                                    <br />
                                                Table 3 Verification experiment results of mid-level feature method
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906032_10700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/GXXB201906032_10700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906032_10700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 中层特征方法的有效性验证实验结果" src="Detail/GetImg?filename=images/GXXB201906032_10700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="108">表3中, Except Mid-level-feature表示深度可分离模块中不采用中层特征。由表3可知, 在添加了Mid-level-feature之后, 模型的性能有了一定的提升, 这验证了在深度可分离模块中采用中层特征方法的有效性。</p>
                </div>
                <div class="p1">
                    <p id="109">最后, 通过与PSE网络进行比较来验证所提模型的有效性。改进模型有效性验证实验结果如表4所示。</p>
                </div>
                <div class="area_img" id="110">
                                            <p class="img_tit">
                                                表4 改进模型有效性验证实验结果
                                                    <br />
                                                Table 4 Results of improved model verification experiment
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906032_11000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/GXXB201906032_11000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906032_11000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表4 改进模型有效性验证实验结果" src="Detail/GetImg?filename=images/GXXB201906032_11000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="111">在表4中, Improved method (IM) 被分成了三列。第一列的IM1表示是否采用拼接特征进行改进。第二列的IM2表示视角单元是否在更浅层的blocks_3进行分离。第三列的IM3表示是否采用深度可分离模块进行改进。在IM这三列中, “Y”表示采用该改进方式, “-”表示不采用该改进方式。①～⑧表示模型序号, 其中①表示PSE网络, ②～⑧表示所提的改进模型。</p>
                </div>
                <div class="p1">
                    <p id="112">由表4可知, 模型⑧在三个行人重识别数据集上都取得了优于PSE网络的效果, 验证了所提模型的有效性。下面对实验结果进行分析。</p>
                </div>
                <div class="p1">
                    <p id="113">对比⑥和⑧模型可知, 采用浅层分离改进的模型在三个数据集上的识别率比不采用时的识别率高。说明当模型同时采用拼接特征改进与深度可分离模块改进时, 浅层分离改进方法有效。</p>
                </div>
                <div class="p1">
                    <p id="114">对比③和④模型可知, 采用深度可分离模块改进的模型在三个数据集上的识别率比不采用时的识别率高。说明当模型采用浅层可分离改进时, 深度可分离模块的改进方法有效。同时对比⑦和⑧模型可知, 采用深度可分离模块改进的模型在三个数据集上的识别率比不采用时的高。说明当模型同时采用拼接特征与浅层可分离进行改进时, 深度可分离模块改进方法有效。</p>
                </div>
                <div class="p1">
                    <p id="115">对比④和⑧模型可知, 采用拼接特征改进的模型在三个数据集上的识别率比不采用时的识别率高。说明当模型同时采用浅层可分离改进与深度可分离模块进行改进时, 拼接特征改进的模型有效。</p>
                </div>
                <div class="p1">
                    <p id="116">综上所述, 当模型同时采用拼接特征、浅层分离以及深度可分离模块改进方法时, 所提算法对模型的改进有效, 并能够提升模型性能。这验证了所提算法的有效性。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117">4.3.2 算法运行速度分析实验</h4>
                <div class="p1">
                    <p id="118">对所提算法与PSE网络的运行速度进行对比。任取一张查询集中的行人图片, 对比所提网络与PSE网络完成查询的时间。查询集选用Market1501的查询集, 候选集选用Market1501的候选集。算法运行速度对比实验结果如表5所示。</p>
                </div>
                <div class="area_img" id="119">
                    <p class="img_tit">表5 算法运行速度对比实验结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Results of algorithm running speed comparison experiment</p>
                    <p class="img_note"></p>
                    <table id="119" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="2"><br />Time /s</td></tr><tr><td><br />Total match</td><td>Per match (19720) </td></tr><tr><td><br />PSE</td><td>141.57</td><td>0.0072</td></tr><tr><td><br />Proposed</td><td>288.96</td><td>0.0147</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="120">表5中, Total match表示查询图片与候选集所有图片进行匹配的时间, Per match表示查询图片与一张候选图片进行匹配的时间。由表5可知, 所提算法的运行速度比PSE网络的慢, 但是相差不大。由于Market1501的候选集图片高达19720张, 一次查询需要匹配19720次。而实际场景中, 通常行人是逐渐出现在监视区域, 候选图片是逐渐增加的, 实际场景中更加看重单对匹配的时间 (即模型只需计算新增加的匹配而无需重复计算原有候选库中的匹配) , 对于所提网络, 单对匹配时间仅有0.0147 s, 虽然较PSE网络略慢, 也完全满足现实需要。</p>
                </div>
                <h4 class="anchor-tag" id="121" name="121">4.3.3 算法先进性验证实验</h4>
                <div class="p1">
                    <p id="122">对目前在三个数据集上最主流的算法DPFL、QMA和PSE进行对比, 结果如表6所示。</p>
                </div>
                <div class="p1">
                    <p id="123">在表6中, “-”表示对应方法未在数据集上实验。由表6可知, 在三个数据集上所提算法的识别效果较原PSE网络均有较好的提升。尤其在Market1501数据集上, 所提方法的rank1超过了目前最好的DPFL算法, 同时在MARS数据集上的rank1与mAP最高。</p>
                </div>
                <div class="area_img" id="124">
                                            <p class="img_tit">
                                                表6 算法结果对比
                                                    <br />
                                                Table 6 Comparison of algorithm results
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906032_12400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/GXXB201906032_12400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906032_12400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表6 算法结果对比" src="Detail/GetImg?filename=images/GXXB201906032_12400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h3 id="125" name="125" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="126">针对目前最好的PSE算法进行行人重识别时视角朝向信息利用方面的不足, 进行了模型结构的改进。首先改进了特征向量的融合方式, 采用特征拼接的方式使得不同视角特征空间的差异能够得到更好的利用。改进了视角单元分离位置, 通过在更浅层分离视角单元, 增大视角单元间的差异性, 更好地构建了不同视角的特征空间。通过引入深度可分离模块, 降低了模型的复杂度, 防止了模型的过拟合现象。通过这两方面的改进可以得到判别性更强的网络特征, 进一步提高了重识别精度。利用Market1501和Duke-MTMC-reid以及MARS数据集这三种主流的数据库对所提算法进行了验证, 实验结果表明该算法具有很好的判别性。</p>
                </div>
                <div class="p1">
                    <p id="127">实验结果表明, 与PSE模型相比, 所提的改进算法取得了更好的识别效果, 同时也提高了模型的泛化能力。对于行人重识别这种存在大量朝向影响的问题, 所提算法具有非常广阔的应用前景。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="12">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201503025&amp;v=MjI5NDI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeXZuVUxySUlqWFRiTEc0SDlUTXJJOUhZWVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Guo P Y, Su A, Zhang H L, <i>et al</i>.Online mixture of random Naïve Bayes tracker combined texture with shape feature[J].Acta Optica Sinica, 2015, 35 (3) :0315002.郭鹏宇, 苏昂, 张红良, 等.结合纹理和形状特征的在线混合随机朴素贝叶斯视觉跟踪器[J].光学学报, 2015, 35 (3) :0315002.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201711011&amp;v=MjE0NzF1Rnl2blVMcklJalhUYkxHNEg5Yk5ybzlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Sun X W, Xu Q S, Cai Y, <i>et al</i>.Sea sky line detection based on edge phase encoding in complicated background[J].Acta Optica Sinica, 2017, 37 (11) :1110002.孙熊伟, 徐青山, 蔡熠, 等.基于边缘相位编码的复杂背景下海天线检测[J].光学学报, 2017, 37 (11) :1110002.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Viewpoint invariant pedestrian recognition with an ensemble of localized features">

                                <b>[3]</b> Gray D, Tao H.Viewpoint invariant pedestrian recognition with an ensemble of localized features[M]//Forsyth D, Torr P, Zisserman A.Lecture Notes in Computer Science.Berlin, Heidelberg:Springer, 2008, 5302:262-275.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification by support vector ranking">

                                <b>[4]</b> Prosser B J, Zheng W S, Gong S, <i>et al</i>.Person re-identification by support vector ranking[C]//Proceedings of the British Machine Vision Conference.August 31-September 3, 2010, Aberystwyth, UK.Durham, England, UK:BMVA Press, 2010:21.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201801045&amp;v=MzIxNjFqWFRiTEc0SDluTXJvOUJZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeXZuVUxySUk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Chen Y, Fan R S, Wang J X, <i>et al</i>.Cloud detection of ZY-3 satellite remote sensing images based on deep learning[J].Acta Optica Sinica, 2018, 38 (1) :0128005.陈洋, 范荣双, 王竞雪, 等.基于深度学习的资源三号卫星遥感影像云检测方法[J].光学学报, 2018, 38 (1) :0128005.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepReID:Deep filter pairing neural network for person re-identification">

                                <b>[6]</b> Li W, Zhao R, Xiao T, <i>et al</i>.DeepReID:deep filter pairing neural network for person re-identification[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28 , 2014, Columbus, OH, USA.New York:IEEE, 2014:152-159.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep transfer learning for person re-identification">

                                <b>[7]</b> Geng M Y, Wang Y M, Xiang T, <i>et al</i>.Deep transfer learning for person reidentification[EB/OL]. (2016-12-22) [2018-12-20].https://arxiv.org/abs/1611.05244.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pose invariant embedding for deep person re-identification">

                                <b>[8]</b> Zheng L, Huang Y J, Lu H C, <i>et al</i>.Pose invariant embedding for deep person re-identification[EB/OL]. (2017-01-26) [2018-02-20].https://arxiv.org/abs/1701.07732.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=AlignedReID:surpassing human-level performance in person re-identification">

                                <b>[9]</b> Zhang X, Luo H, Fan X, <i>et al</i>.AlignedReID:surpassing human-level performance in person re-identification[EB/OL]. (2018-01-31) [2018-02-20].https://arxiv.org/abs/1711.08184.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking">

                                <b>[10]</b> Saquib Sarfraz M, Schumann A, Eberle A, <i>et al</i>.A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:420-429.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person Re-identification by Deep Learning Multi-scale Representations">

                                <b>[11]</b> Chen Y B, Zhu X T, Gong S G.Person re-identification by deep learning multi-scale representations[C]//2017 IEEE International Conference on Computer Vision Workshops (ICCVW) , October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:2590-2600.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Quality Aware Network for Set to Set Recognition">

                                <b>[12]</b> Liu Y, Yan J J, Ouyang W L.Quality aware network for set to set recognition[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4694-4703.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pose-robust face recognition via deep residual equivariant mapping">

                                <b>[13]</b> Cao K D, Rong Y, Li C, <i>et al</i>.Pose-robust face recognition via deep residual equivariant mapping[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:5187-5196.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mobilenets:efficient convolutional neural networks for mobile vision applications">

                                <b>[14]</b> Howard A G, Zhu M L, Chen B, <i>et al</i>.MobileNets:efficient convolutional neural networks for mobile vision applications[EB/OL]. (2017-04-17) [2018-12-21].https://arxiv.org/abs/1704.04861.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Squeeze and excitation networks">

                                <b>[15]</b> Hu J, Shen L, Sun G.Squeeze-and-excitation networks[EB/OL]. (2017-10-25) [2018-12-21].https://arxiv.org/abs/1709.01507.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The devil is in the middle:exploiting mid-level representations for cross-domain instance matching">

                                <b>[16]</b> Yu Q, Chang X, Song Y Z, <i>et al</i>.The devil is in the middle:exploiting mid-level representations for cross-domain instance matching[EB/OL]. (2018-04-04) [2018-12-21].https://arxiv.org/abs/1711.08106.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalable person re-identification A benchmark">

                                <b>[17]</b> Zheng L, Shen L Y, Tian L, <i>et al</i>.Scalable person re-identification:a benchmark[C]//2015 IEEE International Conference on Computer Vision (ICCV) , December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:1116-1124.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Performance Measures and a Data Set for Multi-target,Multi-camera Tracking">

                                <b>[18]</b> Ristani E, Solera F, Zou R, <i>et al</i>.Performance measures and a data set for multi-target, multi-camera tracking[M]//Hua G, Jégou H.Lecture Notes in Computer Science.Cham:Springer, 2016, 9914:17-35.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MARS: A Video Benchmark for Large-Scale Person Re-Identification">

                                <b>[19]</b> Zheng L, Bie Z, Sun Y F, <i>et al</i>.MARS:a video benchmark for large-scale person re-identification[M]//Leibe B, Matas J, Sebe N, <i>et al</i>.Computer Vision-ECCV 2016.Cham:Springer, 2016, 9910:868-884.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201906032" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201906032&amp;v=MDAxOThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnl2blVMckpJalhUYkxHNEg5ak1xWTlHWm8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

