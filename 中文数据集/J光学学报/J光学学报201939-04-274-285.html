

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134122169815000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201904033%26RESULT%3d1%26SIGN%3dJgru1e11uQ1QkKgojDKZfKplv38%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201904033&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201904033&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201904033&amp;v=MjEyNDZHNEg5ak1xNDlHWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIa1ViM01JalhUYkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#66" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#70" data-title="2 跟踪算法 ">2 跟踪算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="&lt;b&gt;2.1 基于相关滤波和多特征降维的目标定位&lt;/b&gt;"><b>2.1 基于相关滤波和多特征降维的目标定位</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;2.2 高置信度更新策略及尺度估计&lt;/b&gt;"><b>2.2 高置信度更新策略及尺度估计</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#128" data-title="3 结果与分析 ">3 结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#133" data-title="&lt;b&gt;3.1 参数&lt;/b&gt;&lt;i&gt;α&lt;/i&gt;&lt;sub&gt;&lt;b&gt;1&lt;/b&gt;&lt;/sub&gt;&lt;b&gt;和&lt;/b&gt;&lt;i&gt;α&lt;/i&gt;&lt;sub&gt;&lt;b&gt;2&lt;/b&gt;&lt;/sub&gt;&lt;b&gt;的设定&lt;/b&gt;"><b>3.1 参数</b><i>α</i><sub><b>1</b></sub><b>和</b><i>α</i><sub><b>2</b></sub><b>的设定</b></a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;3.2 定量比较&lt;/b&gt;"><b>3.2 定量比较</b></a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;3.3 定性比较&lt;/b&gt;"><b>3.3 定性比较</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#157" data-title="4 结 论 ">4 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#108" data-title="图1 Jogging-2序列的 (a) 跟踪结果及 (b) 所提算法对应的响应图。 (a) 跟踪结果; (b) 响应图">图1 Jogging-2序列的 (a) 跟踪结果及 (b) 所提算法对应的响应图。 (a) 跟踪结果......</a></li>
                                                <li><a href="#113" data-title="图2 Jogging-2序列跟踪过程中高置信度更新策略的具体体现">图2 Jogging-2序列跟踪过程中高置信度更新策略的具体体现</a></li>
                                                <li><a href="#130" data-title="图3 所提算法的流程图">图3 所提算法的流程图</a></li>
                                                <li><a href="#137" data-title="图4 所提算法在不同参数下的跟踪结果。 (a) OPE模式下的精度图; (b) OPE模式下的成功图">图4 所提算法在不同参数下的跟踪结果。 (a) OPE模式下的精度图; (b) OPE模式下的成功图</a></li>
                                                <li><a href="#145" data-title="图5 各算法在100组视频序列上的跟踪结果。 (a) OPE模式下的精度图; (b) OPE模式下的成功图; (c) SRE模式下的精度图; (d) SRE模式下的成功图; (e) TRE模式下的精度图; (f) TRE模式下的成功图">图5 各算法在100组视频序列上的跟踪结果。 (a) OPE模式下的精度图; (b) OPE模式下的......</a></li>
                                                <li><a href="#146" data-title="表1 OPE模式下各算法对不同属性的视频序列集合进行测试后得到的精度图数据">表1 OPE模式下各算法对不同属性的视频序列集合进行测试后得到的精度图数据</a></li>
                                                <li><a href="#147" data-title="表2 OPE模式下各算法对不同属性的视频序列集合进行测试后得到的成功图数据">表2 OPE模式下各算法对不同属性的视频序列集合进行测试后得到的成功图数据</a></li>
                                                <li><a href="#148" data-title="表3 OPE模式下所提算法在不同阶段的性能评估">表3 OPE模式下所提算法在不同阶段的性能评估</a></li>
                                                <li><a href="#153" data-title="表4 各算法的平均跟踪速度">表4 各算法的平均跟踪速度</a></li>
                                                <li><a href="#154" data-title="表5 8组视频序列的属性及相关信息">表5 8组视频序列的属性及相关信息</a></li>
                                                <li><a href="#155" data-title="图6 8组视频序列上的部分跟踪结果。 (a) Tiger1; (b) DragonBaby; (c) Bird2; (d) Board; (e) Panda; (f) Jogging-1; (g) Girl2; (h) Human6">图6 8组视频序列上的部分跟踪结果。 (a) Tiger1; (b) DragonBaby; (c)......</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" title=" Lu H C, Li P X, Wang D.Visual object tracking:a survey[J].Pattern Recognition and Artificial Intelligence, 2018, 31 (1) :61-76.卢湖川, 李佩霞, 王栋.目标跟踪算法综述[J].模式识别与人工智能, 2018, 31 (1) :61-76." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201801008&amp;v=MzI2MDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIa1ViM01LRDdZYkxHNEg5bk1ybzlGYklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Lu H C, Li P X, Wang D.Visual object tracking:a survey[J].Pattern Recognition and Artificial Intelligence, 2018, 31 (1) :61-76.卢湖川, 李佩霞, 王栋.目标跟踪算法综述[J].模式识别与人工智能, 2018, 31 (1) :61-76.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" Kristan M, Leonardis A, Matas J, &lt;i&gt;et al&lt;/i&gt;.The visual object tracking VOT2017 challenge results[C]//Proceedings of IEEE International Conference on Computer Vision Workshop, October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:1949-1972." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Visual Object Tracking VOT2017 Challenge Results">
                                        <b>[2]</b>
                                         Kristan M, Leonardis A, Matas J, &lt;i&gt;et al&lt;/i&gt;.The visual object tracking VOT2017 challenge results[C]//Proceedings of IEEE International Conference on Computer Vision Workshop, October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:1949-1972.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" title=" Bolme D S, Beveridge J R, Draper B A, &lt;i&gt;et al&lt;/i&gt;.Visual object tracking using adaptive correlation filters[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:2544-2550." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">
                                        <b>[3]</b>
                                         Bolme D S, Beveridge J R, Draper B A, &lt;i&gt;et al&lt;/i&gt;.Visual object tracking using adaptive correlation filters[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:2544-2550.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" title=" Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;.Exploiting the circulant structure of tracking-by-detection with kernels[C]//Proceedings of the 12th European Conference on Computer Vision, October 7-13, 2012, Florence, Italy.Heidelberg:Springer-Verlag, 2012:702-715." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">
                                        <b>[4]</b>
                                         Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;.Exploiting the circulant structure of tracking-by-detection with kernels[C]//Proceedings of the 12th European Conference on Computer Vision, October 7-13, 2012, Florence, Italy.Heidelberg:Springer-Verlag, 2012:702-715.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" >
                                        <b>[5]</b>
                                     Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596.</a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" title=" Danelljan M, Khan F S, Felsberg M, &lt;i&gt;et al&lt;/i&gt;.Adaptive color attributes for real-time visual tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:1090-1097." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive color attributes for real-time visual tracking">
                                        <b>[6]</b>
                                         Danelljan M, Khan F S, Felsberg M, &lt;i&gt;et al&lt;/i&gt;.Adaptive color attributes for real-time visual tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:1090-1097.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" Shen Q, Yan X L, Liu L F, &lt;i&gt;et al&lt;/i&gt;.Multi-scale correlation filtering tracker based on adaptive feature selection[J].Acta Optica Sinica, 2017, 37 (5) :0515001.沈秋, 严小乐, 刘霖枫, 等.基于自适应特征选择的多尺度相关滤波跟踪[J].光学学报, 2017, 37 (5) :0515001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201705021&amp;v=MTczNDJYVGJMRzRIOWJNcW85SFpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtVYjNNSWo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Shen Q, Yan X L, Liu L F, &lt;i&gt;et al&lt;/i&gt;.Multi-scale correlation filtering tracker based on adaptive feature selection[J].Acta Optica Sinica, 2017, 37 (5) :0515001.沈秋, 严小乐, 刘霖枫, 等.基于自适应特征选择的多尺度相关滤波跟踪[J].光学学报, 2017, 37 (5) :0515001.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                     Li Y, Zhu J K.A scale adaptive kernel correlation filter tracker with feature integration[C]//Proceedings of the 13th European Conference on Computer Vision, September 6-7, 12, 2014, Zurich, Switzerland.Switzerland:Springer, 2014:254-265.</a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Accurate scale estimation for robust visual tracking[C]//Proceedings of British Machine Vision Conference, September 1-5, 2014, Nottingham, UK.Durham:BMVA Press, 2014:1-11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate scale estimation for robust visual tracking">
                                        <b>[9]</b>
                                         Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Accurate scale estimation for robust visual tracking[C]//Proceedings of British Machine Vision Conference, September 1-5, 2014, Nottingham, UK.Durham:BMVA Press, 2014:1-11.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     Danelljan M, Hager G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Discriminative scale space tracking[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (8) :1561-1575.</a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" title=" Liao X F, Hou Z Q, Yu W S, &lt;i&gt;et al&lt;/i&gt;.A scale adapted tracking algorithm based on kernelized correlation[J].Acta Optica Sinica, 2018, 38 (7) :0715002.廖秀峰, 侯志强, 余旺盛, 等.基于核相关的尺度自适应视觉跟踪[J].光学学报, 2018, 38 (7) :0715002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201807026&amp;v=MjE4NTdvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtVYjNNSWpYVGJMRzRIOW5NcUk5SFk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Liao X F, Hou Z Q, Yu W S, &lt;i&gt;et al&lt;/i&gt;.A scale adapted tracking algorithm based on kernelized correlation[J].Acta Optica Sinica, 2018, 38 (7) :0715002.廖秀峰, 侯志强, 余旺盛, 等.基于核相关的尺度自适应视觉跟踪[J].光学学报, 2018, 38 (7) :0715002.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                     Zhang T Z, Xu C S, Yang M H.Multi-task correlation particle filter for robust object tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4819-4827.</a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     Wang M M, Liu Y, Huang Z Y.Large margin object tracking with circulant feature maps[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4800-4808.</a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" Ma X H.Updating method of improved gradient threshold in object tracking[J].Laser &amp;amp; Optoelectronics Progress, 2018, 55 (6) :061502.马晓虹.目标跟踪中增强梯度阈值的更新方法[J].激光与光电子学进展, 2018, 55 (6) :061502." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201806041&amp;v=MDQ3MjRSTE9lWmVWdUZ5SGtVYjNNTHlyUFpMRzRIOW5NcVk5QlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Ma X H.Updating method of improved gradient threshold in object tracking[J].Laser &amp;amp; Optoelectronics Progress, 2018, 55 (6) :061502.马晓虹.目标跟踪中增强梯度阈值的更新方法[J].激光与光电子学进展, 2018, 55 (6) :061502.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" Ma C, Yang X K, Zhang C Y, &lt;i&gt;et al&lt;/i&gt;.Long-term correlation tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:5388-5396." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long-term correlation tracking">
                                        <b>[15]</b>
                                         Ma C, Yang X K, Zhang C Y, &lt;i&gt;et al&lt;/i&gt;.Long-term correlation tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:5388-5396.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" title=" Ma C, Huang J B, Yang X K, &lt;i&gt;et al&lt;/i&gt;.Hierarchical convolutional features for visual tracking[C]//Proceedings of IEEE International Conference on Computer Vision, December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:3074-3082." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical convolutional features for visual tracking">
                                        <b>[16]</b>
                                         Ma C, Huang J B, Yang X K, &lt;i&gt;et al&lt;/i&gt;.Hierarchical convolutional features for visual tracking[C]//Proceedings of IEEE International Conference on Computer Vision, December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:3074-3082.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" title=" Danelljan M, Robinson A, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Beyond correlation filters:learning continuous convolution operators for visual tracking[C]//Proceedings of the 14th European Conference on Computer Vision, October 11-14, 2016, Amsterdam, The Netherlands.Switzerland:Springer, 2016:472-488." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond correlation filters:Learning continuous convolution operators for visual tracking">
                                        <b>[17]</b>
                                         Danelljan M, Robinson A, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Beyond correlation filters:learning continuous convolution operators for visual tracking[C]//Proceedings of the 14th European Conference on Computer Vision, October 11-14, 2016, Amsterdam, The Netherlands.Switzerland:Springer, 2016:472-488.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" Wang X, Hou Z Q, Yu W S, &lt;i&gt;et al&lt;/i&gt;.Target scale adaptive robust tracking based on fusion of multilayer convolutional features[J].Acta Optica Sinica, 2017, 37 (11) :1115005.王鑫, 侯志强, 余旺盛, 等.基于多层卷积特征融合的目标尺度自适应稳健跟踪[J].光学学报, 2017, 37 (11) :1115005." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201711027&amp;v=MDY2NDRSTE9lWmVWdUZ5SGtVYjNNSWpYVGJMRzRIOWJOcm85SFk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Wang X, Hou Z Q, Yu W S, &lt;i&gt;et al&lt;/i&gt;.Target scale adaptive robust tracking based on fusion of multilayer convolutional features[J].Acta Optica Sinica, 2017, 37 (11) :1115005.王鑫, 侯志强, 余旺盛, 等.基于多层卷积特征融合的目标尺度自适应稳健跟踪[J].光学学报, 2017, 37 (11) :1115005.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_19" >
                                        <b>[19]</b>
                                     Danelljan M, Bhat G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.ECO:efficient convolution operators for tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:6931-6939.</a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_20" title=" Xiong C Z, Che M Q, Wang R L, &lt;i&gt;et al&lt;/i&gt;.Robust and real-time visual tracking via dual model adaptive switching[J].Acta Optica Sinica, 2018, 38 (10) :1015002.熊昌镇, 车满强, 王润玲, 等.稳健的双模型自适应切换实时跟踪算法[J].光学学报, 2018, 38 (10) :1015002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201810031&amp;v=MTg3NDg2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhrVWIzTUlqWFRiTEc0SDluTnI0OUdaWVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         Xiong C Z, Che M Q, Wang R L, &lt;i&gt;et al&lt;/i&gt;.Robust and real-time visual tracking via dual model adaptive switching[J].Acta Optica Sinica, 2018, 38 (10) :1015002.熊昌镇, 车满强, 王润玲, 等.稳健的双模型自适应切换实时跟踪算法[J].光学学报, 2018, 38 (10) :1015002.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_21" title=" Nam H, Han B.Learning multi-domain convolutional neural networks for visual tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:4293-4302." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning multi-domain convolutional neural networks for visual tracking">
                                        <b>[21]</b>
                                         Nam H, Han B.Learning multi-domain convolutional neural networks for visual tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:4293-4302.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_22" >
                                        <b>[22]</b>
                                     Bertinetto L, Valmadre J, Henriques J F, &lt;i&gt;et al&lt;/i&gt;.Fully-convolutional siamese networks for object tracking[C]//Proceedings of the 14th European Conference on Computer Vision, October 11-14, 2016, Amsterdam, The Netherlands.Switzerland:Springer, 2016:850-865.</a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_23" title=" Gao J Y, Zhang T Z, Yang X S, &lt;i&gt;et al&lt;/i&gt;.Deep relative tracking[J].IEEE Transactions on Image Processing, 2017, 26 (4) :1845-1858." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep relative tracking">
                                        <b>[23]</b>
                                         Gao J Y, Zhang T Z, Yang X S, &lt;i&gt;et al&lt;/i&gt;.Deep relative tracking[J].IEEE Transactions on Image Processing, 2017, 26 (4) :1845-1858.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_24" title=" Sun C, Wang D, Lu H C, &lt;i&gt;et al&lt;/i&gt;.Learning spatial-aware regressions for visual tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:8962-8970." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatial-aware regressions for visual tracking">
                                        <b>[24]</b>
                                         Sun C, Wang D, Lu H C, &lt;i&gt;et al&lt;/i&gt;.Learning spatial-aware regressions for visual tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:8962-8970.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_25" >
                                        <b>[25]</b>
                                     Wu Y, Lim J, Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1834-1848.</a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_26" title=" Galoogahi H K, Sim T, Lucey S.Correlation filters with limited boundaries[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:4630-4638." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Correlation Filters with Limited Boundaries">
                                        <b>[26]</b>
                                         Galoogahi H K, Sim T, Lucey S.Correlation filters with limited boundaries[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:4630-4638.
                                    </a>
                                </li>
                                <li id="62">


                                    <a id="bibliography_27" title=" Gundogdu E, Alatan A A.Spatial windowing for correlation filter based visual tracking[C]//Proceedings of IEEE International Conference on Image Processing, September 25-28, 2016, Phoenix, AZ, USA.New York:IEEE, 2016:1684-1688." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial windowing for correlation filter based visual tracking">
                                        <b>[27]</b>
                                         Gundogdu E, Alatan A A.Spatial windowing for correlation filter based visual tracking[C]//Proceedings of IEEE International Conference on Image Processing, September 25-28, 2016, Phoenix, AZ, USA.New York:IEEE, 2016:1684-1688.
                                    </a>
                                </li>
                                <li id="64">


                                    <a id="bibliography_28" title=" Guan H, Xue X Y, An Z Y.Advances on application of deep learning for video object tracking[J].Acta Automatica Sinica, 2016, 42 (6) :834-847.管皓, 薛向阳, 安志勇.深度学习在视频目标跟踪中的应用进展与展望[J].自动化学报, 2016, 42 (6) :834-847." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606004&amp;v=MDkyNzVxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtVYjNNS0NMZlliRzRIOWZNcVk5RllJUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                         Guan H, Xue X Y, An Z Y.Advances on application of deep learning for video object tracking[J].Acta Automatica Sinica, 2016, 42 (6) :834-847.管皓, 薛向阳, 安志勇.深度学习在视频目标跟踪中的应用进展与展望[J].自动化学报, 2016, 42 (6) :834-847.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-17 10:57</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(04),274-285 DOI:10.3788/AOS201939.0415003            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于高置信度更新策略的高速相关滤波跟踪算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9E%97%E5%BD%AC&amp;code=35853574&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">林彬</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%98%A0&amp;code=10203059&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李映</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8C%97%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2%E9%99%95%E8%A5%BF%E7%9C%81%E8%AF%AD%E9%9F%B3%E4%B8%8E%E5%9B%BE%E5%83%8F%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0085569&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西北工业大学计算机学院陕西省语音与图像信息处理重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%A1%82%E6%9E%97%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E7%90%86%E5%AD%A6%E9%99%A2&amp;code=1549785&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">桂林理工大学理学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了满足在线目标跟踪算法的实时性需求并提高算法的稳健性, 提出一种基于高置信度更新策略的相关滤波跟踪算法。在目标区域提取、融合多特征, 以构建稳健的外观表达, 并利用投影矩阵对特征进行降维, 以提高算法的运行效率;通过相关滤波器寻找最大响应值, 从而快速定位目标;利用最大响应值和平均峰值相关能量指标, 设计了一种高置信度更新策略。结果表明:所提算法在大规模公开数据集上取得了较高的跟踪精度和成功率, 平均跟踪速度达到122.3 frame/s。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标跟踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%85%B3%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相关滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%BA%E5%BA%A6%E4%BC%B0%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">尺度估计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E5%9E%8B%E6%9B%B4%E6%96%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模型更新;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *李映, E-mail:lybyp@nwpu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-17</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划 (2016YFB0502502);</span>
                                <span>国家自然科学基金 (61871460, 11502057, 11661028, 61703117, 71762009);</span>
                                <span>广西科技计划 (2015GXNSFBA139005, 2017GXNSFBA198113);</span>
                                <span>空间微波技术重点实验室基金 (6142411040404);</span>
                                <span>广西高校中青年教师基础能力提升项目 (2017KY0260);</span>
                    </p>
            </div>
                    <h1><b>High-Speed Correlation Filter Tracking Algorithm Based on High-Confidence Updating Strategy</b></h1>
                    <h2>
                    <span>Lin Bin</span>
                    <span>Li Ying</span>
            </h2>
                    <h2>
                    <span>Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, Northwestern Polytechnical University</span>
                    <span>School of Science, Guilin University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To satisfy the real-time requirements of the online object tracking algorithm and improve the robustness of the algorithm, we propose a correlation filter-based tracking algorithm with high-confidence updating strategy. Multi-features are extracted and integrated in the target region to construct robust appearance representation, and the projection matrix for dimension reduction of features is used to improve the operational efficiency of the algorithm. The correlation filter is used to localize the target at a high speed via the maximum response value. Two indicators of maximum response value and average peak-to-correlation energy are utilized to design a high-confidence updating strategy. The results show that the proposed algorithm achieves high tracking precision and success rate on large-scale public datasets while running at 122.3 frame/s on average.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=object%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">object tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=correlation%20filter&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">correlation filter;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=scale%20estimation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">scale estimation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=model%20updating&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">model updating;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-17</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="66" name="66" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="67">在线目标跟踪是计算机视觉研究中的重要课题, 其应用场景涉及自动驾驶、视频监控、无人机侦察、人机交互等众多领域。通常, 在视频的第1帧中指定待跟踪的目标后, 目标跟踪任务需要在后续帧中实时标记出目标的最新位置。这对于人眼视觉来说是相对简单的, 但是对于机器视觉而言, 由于目标在运动过程中可能会出现形变、旋转、尺度变化、被遮挡等干扰情况, 且存在训练数据缺乏 (只有第1帧数据) 和计算难以满足实时性需求等难题, 因此实现对目标的快速、稳定跟踪仍然是一项极具挑战性的任务。</p>
                </div>
                <div class="p1">
                    <p id="68">近年来, 目标跟踪算法的研究从传统的光流法、均值漂移、稀疏表示、粒子滤波等跟踪算法逐渐转向基于相关滤波和深度学习的方法<citation id="159" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。另外, 从国际顶级的视觉目标跟踪 (VOT) 挑战赛2017年的竞赛结果中可以发现, 排名前列的算法均为相关滤波或深度学习算法<citation id="160" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。但是上述各种干扰因素使得目前尚未有能够解决所有问题的通用方法。基于相关滤波的跟踪算法凭借其良好的跟踪性能和极高的计算效率受到了越来越多的关注。Bolme等<citation id="161" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>率先将判别式相关滤波 (DCF) 方法应用于目标跟踪领域, 通过提取目标的灰度特征, 训练一个最小均方误差滤波器 (MOSSE) 作用于搜索区域, 将最大响应值对应的坐标作为目标中心, 跟踪速度可达到数百帧每秒。Henriques等<citation id="176" type="reference"><link href="16" rel="bibliography" /><link href="18" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>利用循环密集采样的方式来增加训练样本的数量, 提出了核循环结构跟踪器 (CSK) <citation id="162" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>, 随后又提出了核化相关滤波器 (KCF) <citation id="163" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>, 将单通道的灰度特征扩展至多通道的方向梯度直方图 (HOG) 特征, 进一步提高了跟踪效果。Danelljan等<citation id="164" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出了基于颜色空间 (CN) 特征的算法, 使相关滤波跟踪算法能够更好地适应彩色图像序列。Shen等<citation id="165" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>分别提取形状、纹理、颜色特征, 通过相关滤波响应图来评估不同特征对目标的判别能力, 从而实现自适应地选择最优特征进行跟踪。为了解决尺度变化的问题, 自适应尺度变化的相关滤波跟踪器 (SAMF) <citation id="166" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和判别式尺度空间跟踪器 (DSST) <citation id="167" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>在相关滤波跟踪中引入了不同的尺度估计策略。SAMF设定了7个不同的尺度因子, 使用平移滤波器对多尺度图像区域进行检测, 从而产生了7个响应图, 并选取最大响应值对应的尺度因子作为新的目标尺度。DSST采用一个单独的尺度滤波器来解决尺度问题, 尺度滤波器使用了更加精细的33个尺度因子, 在平移滤波器完成目标定位后, 再通过尺度滤波器进行尺度估计。文献<citation id="168" type="reference">[<a class="sup">10</a>]</citation>和文献<citation id="169" type="reference">[<a class="sup">11</a>]</citation>在DSST的基础上进行了改进, 其中:文献<citation id="170" type="reference">[<a class="sup">10</a>]</citation>使用主成分分析 (PCA) 法进行特征降维, 通过提取更高效的特征来提升算法的执行速度;文献<citation id="171" type="reference">[<a class="sup">11</a>]</citation>使用核相关的尺度滤波器, 并加入了颜色信息, 使算法能够更好地适应目标的尺度变化。Zhang等<citation id="172" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>通过引入粒子滤波来解决相关滤波的尺度问题, 并实现了多特征相关滤波器的联合优化。Wang等<citation id="173" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>将相关滤波算法和支持向量机 (SVM) 相结合, 并提出利用平均峰值相关能量 (APCE) 指标为依据来自适应地更新滤波模型, 能够在一定程度上有效缓解模型的漂移。在此基础上, 马晓虹<citation id="174" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>将APCE阈值与APCE梯度阈值相结合, 以此来判断跟踪结果的可靠性, 并决定模型是否更新, 从而提升了算法对目标快速运动及背景干扰等问题的处理能力。Ma等<citation id="175" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出了一种长时相关跟踪 (LCT) 算法, 在跟踪失败时, 通过检测机制对目标进行重定位, 使跟踪器恢复到正确的跟踪状态, 从而实现长时跟踪。随着深度学习技术的不断发展, 一些相关滤波跟踪器<citation id="177" type="reference"><link href="40" rel="bibliography" /><link href="42" rel="bibliography" /><link href="44" rel="bibliography" /><link href="46" rel="bibliography" /><link href="48" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>,<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>开始使用卷积神经网络 (CNN) 进行特征提取, 借助深度特征来提高跟踪效果。另外, 目前的深度学习技术不仅局限于特征提取部分, 而且一些算法<citation id="178" type="reference"><link href="50" rel="bibliography" /><link href="52" rel="bibliography" /><link href="54" rel="bibliography" /><link href="56" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>,<a class="sup">23</a>,<a class="sup">24</a>]</sup></citation>还结合特征提取与分类器, 使用CNN替代相关滤波的整个过程, 从而建立端到端输出的跟踪框架, 在公开数据集的评测中效果显著。深度学习技术虽然有助于算法精度的提升, 但会极大地影响算法的执行速度, 难以满足在线跟踪场景下的实时性需求。</p>
                </div>
                <div class="p1">
                    <p id="69">基于以上已有的研究, 本文针对在线跟踪场景设计了一种高速相关滤波跟踪算法, 主要研究工作如下:1) 在特征选择方面, 融合灰度、HOG和CN特征 (考虑到实时性需求, 没有引入深度特征) 进行精准的目标定位, 并采用特征降维的方式对算法提速 (同样采用降维后的特征, 单独训练尺度滤波器, 从而进行尺度估计) ;2) 不同于传统的相关滤波跟踪算法 (如KCF<citation id="179" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、DSST<citation id="180" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等) , 在每一帧都更新滤波器模型, 利用响应图的最高响应值和APCE这两个指标, 设计了一种高置信度更新策略, 即只有同时满足指标条件时才进行尺度估计和模型更新, 从而避免了低置信度情况下不可靠的尺度估计以及可能会带来冗余的平移、尺度滤波器模型更新操作, 从而进一步提高了算法的效率;3) 在包含100组视频序列的通用数据集OTB (object tracking benchmark) 2015<citation id="181" type="reference"><link href="58" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>上进行了详细的实验验证, 并与近年的同类算法进行比较, 考察所提算法的跟踪性能和实时跟踪速度。</p>
                </div>
                <h3 id="70" name="70" class="anchor-tag">2 跟踪算法</h3>
                <div class="p1">
                    <p id="71">所提跟踪算法可归纳为两个阶段进行:首先对目标区域进行特征提取和特征降维, 利用平移滤波器计算输出相关滤波响应图, 从而确定目标的位置中心;然后根据所提出的高置信度更新策略, 判定是否满足高置信度条件, 如果满足, 则利用尺度滤波器对目标进行尺度估计, 更新目标的尺度因子, 并对平移滤波器和尺度滤波器分别进行模型更新, 否则不进行任何更新。</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72"><b>2.1 基于相关滤波和多特征降维的目标定位</b></h4>
                <div class="p1">
                    <p id="73">基于判别式相关滤波方法, 所提跟踪算法通过训练一个二维平移滤波器来实现对目标的定位。特征的选择对于相关滤波跟踪器的性能具有至关重要的影响。灰度特征对目标的表征能力有限, 单纯地使用灰度特征只能适用于简单场景。HOG特征计算的是图像局部区域的梯度方向, 共31维, 能够表征图像的边缘和形状信息, 且对目标的旋转和尺度变化相对不敏感。CN特征将RGB (red, green, blue) 颜色空间转换为CN空间, 反映的是10维主题颜色信息, 对彩色图像序列中目标的建模具有显著作用。相比于浅层特征, 深度特征含有丰富的语义信息, 具有极强的表征能力和抗干扰性。但是, 深度特征的高维度也会增大算法的计算量, 使相关滤波跟踪算法丧失其最突出的速度优势。目前, 绝大多数基于深度特征的跟踪算法均无法满足在线跟踪场景下跟踪速度大于20 frame/s的实时性需求。因此, 所提算法选择融合灰度特征、HOG特征和CN特征 (对灰度图像序列不使用CN特征) 进行目标定位。</p>
                </div>
                <div class="p1">
                    <p id="74">首先, 对初始目标尺寸以一定比例扩大 (扩大倍数记为<i>p</i><sub>adding</sub>) 后的目标样本区域进行特征提取。将不同的特征串联后, 得到一个<i>d</i>维 (灰度、HOG和CN特征共42维, 即实验中<i>d</i>=42) 的特征向量<b><i>u</i></b><sub>1</sub>。受文献<citation id="182" type="reference">[<a class="sup">10</a>]</citation>的启发, 使用PCA算法对其进行特征降维。通过计算<mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>∼</mo></mover><mo>×</mo><mi>d</mi></mrow></math></mathml>型的投影矩阵<b><i>P</i></b>, 进一步得到一个<mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>∼</mo></mover></mrow></math></mathml>维的低维特征向量<b><i>f</i></b><sub>1</sub>, 即</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mi mathvariant="bold-italic">Ρ</mi><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mn>1</mn></msub><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">实验中<i>p</i><sub>adding</sub>=2, 即提取的目标样本区域尺寸为目标尺寸的3倍, 降维后的<mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>∼</mo></mover></mrow></math></mathml>取28 (对灰度图像序列取<mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>∼</mo></mover><mo>=</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>。投影矩阵<b><i>P</i></b>可以通过最小化<b><i>u</i></b><sub>1</sub>的重构误差<i>ε</i>进行求解, 即</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ε</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Ρ</mi><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">式中:<b><i>u</i></b><sub>1</sub> (<i>k</i>) 为特征向量<b><i>u</i></b><sub>1</sub>中的第<i>k</i>个元素, <i>k</i>的取值范围覆盖<b><i>u</i></b><sub>1</sub>中的所有元素。最小化<i>ε</i>需要满足正交约束<b><i>PP</i></b><sup>T</sup>=<b><i>I</i></b>, 其中<b><i>I</i></b>为单位矩阵。PCA算法将该优化问题转化为协方差矩阵的特征值求解问题, 协方差矩阵<b><i>C</i></b>为</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">C</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>n</mi></munder><mi mathvariant="bold-italic">u</mi></mstyle><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo><mo>⋅</mo><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">将<mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>∼</mo></mover></mrow></math></mathml>个最大的特征值所对应的特征向量按行排列, 得到的矩阵即为投影矩阵<b><i>P</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="86">然后, 通过构建平移相关滤波器<b><i>h</i></b><sub>trans</sub>, 使相关响应输出与期望输出<b><i>g</i></b>之间的误差最小化, 就可得到最优的滤波器模型<b><i>h</i></b><sup>*</sup><sub>trans</sub>, 即</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msubsup><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow><mo>*</mo></msubsup><mo>=</mo><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">g</mi><mo>-</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mn>1</mn></msub><mo>⋅</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">式中:<i>λ</i>为正则化参数, 实验中取<i>λ</i>=0.01。<b><i>g</i></b>满足二维高斯分布, 对应二维矩阵中的元素记为<i>g</i> (<i>x</i>, <i>y</i>) , 设目标样本区域的大小为<i>M</i>×<i>N</i>, 其中<i>M</i>和<i>N</i>分别为目标样本区域的宽度和高度, 则</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>g</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>exp</mi><mrow><mo>[</mo><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>-</mo><mi>Μ</mi><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false"> (</mo><mi>y</mi><mo>-</mo><mi>Ν</mi><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>]</mo></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">式中:自变量<i>x</i>=0, 1, 2, …, <i>M</i>-1;自变量<i>y</i>=0, 1, 2, …, <i>N</i>-1;<i>σ</i>为高斯核的带宽, 实验中取<mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><mo>=</mo><mfrac><mrow><msqrt><mrow><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mi>h</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msqrt></mrow><mrow><mn>3</mn><mn>2</mn></mrow></mfrac></mrow></math></mathml>, 其中<i>w</i><sub>1</sub>和<i>h</i><sub>1</sub>分别为目标初始尺寸的宽度和高度。 (4) 式中的优化问题可以转化到频域中进行求解, 令<i>ε</i>′=‖<b><i>g</i></b>-<b><i>f</i></b><sub>1</sub>·<b><i>h</i></b><sub>trans</sub>‖<sup>2</sup>+<i>λ</i>‖<b><i>h</i></b><sub>trans</sub>‖<sup>2</sup>, 则根据帕萨瓦尔 (Parseval) 定理, 可以得到<i>ε</i>′的频域表示, 即</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>ε</mi></mstyle><mo>∼</mo></mover><mo>´</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Μ</mi><mi>Ν</mi></mrow></mfrac><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">G</mi><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">F</mi><mo>¯</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo>⊙</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">式中:⊙表示对应元素相乘;<b><i>G</i></b>、<b><i>F</i></b><sub>1</sub>和<b><i>H</i></b><sub>trans</sub>分别为<b><i>g</i></b>、<b><i>f</i></b><sub>1</sub>和<b><i>h</i></b><sub>trans</sub>经快速傅里叶变换 (FFT) 后的频域表示;<mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">F</mi><mo>¯</mo></mover><msub><mrow></mrow><mn>1</mn></msub></mrow></math></mathml>为<b><i>F</i></b><sub>1</sub>的复共轭。由<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>ε</mi></mstyle><mo>∼</mo></mover><mo>´</mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow></msub></mrow></mfrac><mo>=</mo><mn>0</mn></mrow></math></mathml>, 可得平移滤波器模型为</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">G</mi><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">F</mi><mo>¯</mo></mover><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mn>1</mn></msub><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">F</mi><mo>¯</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>λ</mi></mrow></mfrac><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">滤波器的尺寸和目标样本区域 (初始目标经过扩大后) 的尺寸相同, 均为<i>M</i>×<i>N</i>。</p>
                </div>
                <div class="p1">
                    <p id="98">将训练好的滤波器在第<i>t</i>帧中进行目标定位 (由于存在滤波器模型的更新操作, 因此使用的是在<i>t</i>-1帧得到的滤波器<b><i>H</i></b><sub><i>t</i>-1, trans</sub>) 。先以<i>t</i>-1帧的目标位置为中心, 采样一个大小同样为<i>M</i>×<i>N</i>的搜索区域, 并经过特征提取、融合和降维, 得到特征向量<b><i>f</i></b><sub><i>t</i></sub>, 经FFT处理后为<b><i>F</i></b><sub><i>t</i></sub>, 再通过相关滤波操作得到第<i>t</i>帧的平移相关响应图</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn><mo>, </mo><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow></msub><mo>⊙</mo><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">式中:<font face="EU-HT">F</font><sup>-1</sup>为逆快速傅里叶变换。最后, 将响应图中最大的响应值所对应的坐标位置标记为当前帧新的目标中心<mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>m</mi><mo>^</mo></mover><mo>, </mo><mover accent="true"><mi>n</mi><mo>^</mo></mover><mo stretchy="false">) </mo></mrow></math></mathml>, 即</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>m</mi><mo>^</mo></mover><mo>, </mo><mover accent="true"><mi>n</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></munder><mo stretchy="false">[</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">式中:<i>y</i><sub><i>t</i>, trans</sub> (<i>m</i>, <i>n</i>) 为<b><i>y</i></b><sub><i>t</i>, trans</sub>中坐标为 (<i>m</i>, <i>n</i>) 的像素点对应的响应值;<i>m</i>、<i>n</i>的取值分别为<i>m</i>=0, 1, 2, …, <i>M</i>-1和<i>n</i>=0, 1, 2, …, <i>N</i>-1。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104"><b>2.2 高置信度更新策略及尺度估计</b></h4>
                <div class="p1">
                    <p id="105">相关滤波跟踪方法通常需要进行模型更新, 以适应跟踪过程中目标可能发生的形变、旋转等变化, 使得滤波器模型能够保持对目标外观的刻画。目前, 大多数跟踪算法在每一帧 (或间隔几帧) 以一定的比例固定地更新滤波器模型, 并更新目标的尺度。这样的更新机制在背景单一、目标能够被准确定位时是简单有效的。但是在现实环境下, 目标在运动过程中可能会在较长一段时间内被部分甚至完全遮挡, 此时如果仍然保持对滤波器模型进行既定的更新操作, 就会持续地引入背景噪声, 从而造成滤波器模型的污染和快速退化, 最终导致跟踪失败。图1所示为Jogging-2序列的跟踪结果及所提算法对应的响应图, 其中proposed (NHU) 表示不使用高置信度更新策略, 在每一帧都更新尺度因子和滤波器模型。由图1 (a) 可知:目标在第51帧几乎被完全遮挡, 传统的更新策略使得模型受到遮挡物的影响而退化, 在后续的跟踪过程中偏离了目标;并且, 在遮挡情况下所进行的尺度估计也可以认为是冗余操作。为了解决模型退化问题和去冗余, 本课题组创新性地将尺度估计与目标定位时的置信度进行捆绑, 进而提出了一种高置信度更新策略, 只有在满足高置信度更新条件时才进行尺度估计和滤波器模型的更新。置信度的判定依赖于两个指标值的计算。对平移滤波器得到的响应图分别计算其<i>y</i><sub>max</sub>值和APCE值, 其中:<i>y</i><sub>max</sub>值只考虑峰值, 忽略响应图的旁瓣情况;而APCE值反映的是响应图整体的波形抖动情况, 用变量<i>E</i><sub>APCE</sub>表示APCE值, 则<i>E</i><sub>APCE</sub>为</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mfrac><mn>1</mn><mrow><mi>Μ</mi><mi>Ν</mi></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></munder><mo stretchy="false">[</mo></mstyle><mi>y</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">式中:<i>y</i><sub>max</sub>、<i>y</i><sub>min</sub>和<i>y</i><sub>trans</sub> (<i>m</i>, <i>n</i>) 分别为响应图<i>y</i><sub>trans</sub> (响应图的尺寸为<i>M</i>×<i>N</i>) 的最高响应值、最低响应值和坐标为 (<i>m</i>, <i>n</i>) 的像素点对应的响应值。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904033_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Jogging-2序列的 (a) 跟踪结果及 (b) 所提算法对应的响应图。 (a) 跟踪结果; (b) 响应图" src="Detail/GetImg?filename=images/GXXB201904033_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Jogging-2序列的 (a) 跟踪结果及 (b) 所提算法对应的响应图。 (a) 跟踪结果; (b) 响应图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904033_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Tracking results and response maps corresponding to proposed algorithm on Jogging-2 sequence. (a) Tracking results; (b) response maps</p>

                </div>
                <div class="p1">
                    <p id="109">由图1 (b) 可知, APCE值较小的情况表明响应图呈现出“多峰”的形式, 可以认为此时计算出的目标中心位置的置信度是较低的。在目标被遮挡的情况下, <i>y</i><sub>max</sub>值也会显著减小, <i>y</i><sub>max</sub>和APCE这两个指标分别从局部和整体的角度反映置信度, 具有一定的互补性。另外, 考虑到视频序列中目标的不断变化会对响应图产生持续影响, 用两个集合<i>S</i><sub><i>y</i><sub>max</sub></sub>和<i>S</i><sub>APCE</sub>分别保存<i>y</i><sub>max</sub>和APCE的历史指标值, 只有在当前第<i>t</i>帧的<i>y</i><sub><i>t</i>, max</sub>和<i>E</i><sub><i>t</i>, APCE</sub>都以一定的比例<i>α</i><sub>1</sub>和<i>α</i><sub>2</sub>超过历史指标的均值时, 才认为对当前帧的目标中心位置的预测具有高置信度, 即需要同时满足以下两个条件:</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mi>max</mi></mrow></msub><mo>&gt;</mo><mi>α</mi><msub><mrow></mrow><mn>1</mn></msub><mfrac><mn>1</mn><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>y</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>max</mi></mrow></msub><mo>, </mo><mspace width="0.25em" /><mspace width="0.25em" /><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>max</mi></mrow></msub><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub></mrow></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>E</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mtext>A</mtext><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext></mrow></msub><mo>&gt;</mo><mi>α</mi><msub><mrow></mrow><mn>2</mn></msub><mfrac><mn>1</mn><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>E</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mo>, </mo><mtext>A</mtext><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext></mrow></msub><mo>, </mo><mspace width="0.25em" /><mspace width="0.25em" /><mi>E</mi><msub><mrow></mrow><mrow><mi>j</mi><mo>, </mo><mtext>A</mtext><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext></mrow></msub><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext></mrow></msub></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">当<i>α</i><sub>1</sub>和<i>α</i><sub>2</sub>分别取0.6和0.45时, 可以得到较优的跟踪结果。置信度判别完成后, 将<i>y</i><sub><i>t</i>, max</sub>和<i>E</i><sub><i>t</i>, APCE</sub>分别加入到集合<i>S</i><sub><i>y</i><sub>max</sub></sub>和<i>S</i><sub>APCE</sub>中。</p>
                </div>
                <div class="p1">
                    <p id="112">在跟踪过程中, 高置信度跟踪策略的具体体现如图2所示 (目标在第40～60帧时处于被遮挡状态) 。</p>
                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904033_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Jogging-2序列跟踪过程中高置信度更新策略的具体体现" src="Detail/GetImg?filename=images/GXXB201904033_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Jogging-2序列跟踪过程中高置信度更新策略的具体体现  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904033_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Specific embodiment of high-confidence updating strategy in tracking process on Jogging-2 sequence</p>

                </div>
                <div class="p1">
                    <p id="114">由<i>y</i><sub>max</sub>指标的历史均值与<i>α</i><sub>1</sub>的乘积得到的曲线称为<i>y</i><sub>max</sub>指标的置信度曲线, 由APCE指标的历史均值与<i>α</i><sub>2</sub>的乘积得到的曲线称为APCE指标的置信度曲线。当表示当前帧<i>y</i><sub>max</sub>值的实线位于<i>y</i><sub>max</sub>指标的置信度曲线上方时, 认为当前帧满足 (11) 式中的第1个条件;类似地, 当表示当前帧APCE值的实线位于APCE指标的置信度曲线上方时, 认为当前帧满足 (11) 式中的第2个条件。由图2可知, 在第20～70帧中能同时满足两个条件的帧数区间是[20, 45]和[58, 70], 紧接着需要进行尺度估计和滤波器模型更新操作;而第46～57帧不能满足高置信度更新条件, 处于低置信度区间, 跟踪算法不进行任何更新操作而直接处理下一帧。</p>
                </div>
                <div class="p1">
                    <p id="115">所提算法使用独立的尺度滤波器对目标进行尺度估计。按照文献<citation id="184" type="reference">[<a class="sup">10</a>]</citation>中的方法, 围绕初始的目标样本区域, 通过放缩的方式, 构建一个含<i>L</i>个尺度因子的金字塔尺度空间模型 (对应<i>L</i>个不同大小的图像子块, 本实验中<i>L</i>=17) 。对所有图像子块提取HOG特征 (为了保持尺度计算的高效性, 没有使用灰度特征和CN特征) , 经过特征降维并融合后, 得到一个<i>d</i>′维的特征向量<b><i>f</i></b><sub>1, sc</sub> (从31维降到<i>d</i>′维, 实验中<i>d</i>′取17) 。设<b><i>g</i></b>′满足一维高斯分布, 对应的一维矩阵中的元素记为<i>g</i>′ (<i>s</i>) , 则</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>g</mi><mo>′</mo></msup><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>=</mo><mi>exp</mi><mrow><mo>[</mo><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>s</mi><mo>-</mo><mi>L</mi><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><msup><mi>σ</mi><mo>′</mo></msup><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>]</mo></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">式中:<i>s</i>的取值为<i>s</i>=0, 1, 2, …, <i>L</i>-1;<i>σ</i>′为高斯核的带宽, 实验中取<i>σ</i>′=2。然后对<b><i>f</i></b><sub>1, sc</sub>和<b><i>g</i></b>′进行FFT, 得到它们对应的频域表示<b><i>F</i></b><sub>1, sc</sub>和<b><i>G</i></b>′。将尺度滤波器的频域表示记为<b><i>H</i></b><sub>scale</sub>, 其计算过程与平移滤波器<b><i>H</i></b><sub>trans</sub>类似, 可得到</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext><mtext>e</mtext></mrow></msub><mo>=</mo><mfrac><mrow><msup><mi mathvariant="bold-italic">G</mi><mo>′</mo></msup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">F</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mn>1</mn><mo>, </mo><mtext>s</mtext><mtext>c</mtext></mrow></msub></mrow><mrow><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>, </mo><mtext>s</mtext><mtext>c</mtext></mrow></msub><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">F</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mn>1</mn><mo>, </mo><mtext>s</mtext><mtext>c</mtext></mrow></msub><mo>+</mo><mi>λ</mi></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">式中:<mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">F</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mn>1</mn><mo>, </mo><mtext>s</mtext><mtext>c</mtext></mrow></msub></mrow></math></mathml>为<b><i>F</i></b><sub>1, sc</sub>的复共轭。</p>
                </div>
                <div class="p1">
                    <p id="121">同样地, 将尺度滤波器<b><i>H</i></b><sub>trans</sub>应用到当前第<i>t</i>帧, 可以得到响应图<b><i>y</i></b><sub><i>t</i>, scale</sub>。<b><i>y</i></b><sub><i>t</i>, scale</sub>中最大响应值对应的尺度因子即为最优的尺度因子<mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>s</mi><mo>^</mo></mover></math></mathml>。最后, 分别更新平移滤波器和尺度滤波器, 即</p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>β</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn><mo>, </mo><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow></msub><mo>+</mo><mi>β</mi><mfrac><mrow><mi mathvariant="bold-italic">G</mi><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">F</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mi>t</mi></msub><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">F</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>λ</mi></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="left"><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mtext>s</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext><mtext>e</mtext></mrow></msub><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>β</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn><mo>, </mo><mtext>s</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext><mtext>e</mtext></mrow></msub><mo>+</mo><mi>β</mi><mfrac><mrow><msup><mi mathvariant="bold-italic">G</mi><mo>′</mo></msup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">F</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mtext>s</mtext><mtext>c</mtext></mrow></msub></mrow><mrow><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mtext>s</mtext><mtext>c</mtext></mrow></msub><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">F</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mtext>s</mtext><mtext>c</mtext></mrow></msub><mo>+</mo><mi>λ</mi></mrow></mfrac></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="124">式中:<b><i>H</i></b><sub><i>t</i>, trans</sub>、<b><i>H</i></b><sub><i>t</i>, scale</sub>分别为更新后第<i>t</i>帧的平移滤波器和尺度滤波器模型;<b><i>H</i></b><sub><i>t</i>-1, trans</sub>、<b><i>H</i></b><sub><i>t</i>-1, scale</sub>分别为<i>t</i>-1帧的平移滤波器和尺度滤波器模型;<i>β</i>为模型更新学习率, 实验中取<i>β</i>=0.025 (同文献<citation id="185" type="reference">[<a class="sup">10</a>]</citation>中<i>β</i>的设置) , <i>β</i>越大表示当前帧目标变化对滤波器模型的影响越大;在第<i>t</i>帧进行目标特征提取、降维和FFT处理后, <b><i>F</i></b><sub><i>t</i></sub>为当前帧用于估计目标位置中心的特征向量的频域表示;<b><i>F</i></b><sub><i>t</i>, sc</sub>为用于估计目标尺度的特征向量的频域表示 (多尺度的特征向量, 区别于<b><i>F</i></b><sub><i>t</i></sub>) ;<mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">F</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>和<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">F</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mtext>s</mtext><mtext>c</mtext></mrow></msub></mrow></math></mathml>分别为<b><i>F</i></b><sub><i>t</i></sub>和<b><i>F</i></b><sub><i>t</i>, sc</sub>的复共轭。</p>
                </div>
                <div class="p1">
                    <p id="127">如图1 (a) 中第51帧所示, 在不满足高置信度更新的条件下, 所提算法不进行尺度估计和模型更新操作, 对于遮挡等情况能够有效地缓解滤波器模型的退化。综上, 算法的总体流程如图3所示。在提高算法稳健性的同时, 从特征的降维、通过高置信度更新策略去除冗余操作两方面实现对算法的加速处理, 以确保所提算法高效地应对在线跟踪场景。</p>
                </div>
                <h3 id="128" name="128" class="anchor-tag">3 结果与分析</h3>
                <div class="p1">
                    <p id="129">为了测试所提算法的有效性, 在大规模数据集OTB2015<citation id="186" type="reference"><link href="58" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>上进行充分的实验验证与比较分析。实验环境为i5-7300HQ CPU (主频为2.5 GHz) 、内存为8 GB的计算机, 在64位Win10操作系统、MATLAB R2017a平台下编写代码。首先, 对所提高置信度更新策略中两个关键参数<i>α</i><sub>1</sub>和<i>α</i><sub>2</sub>的设定组织实验, 以确定最优的参数;然后, 选取7种在线跟踪算法与所提算法分别进行定量比较和定性比较, 7种在线跟踪算法包括CSK<citation id="187" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、KCF<citation id="188" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、CN<citation id="189" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、DSST<citation id="190" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、fDSST<citation id="191" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、CFLB<citation id="192" type="reference"><link href="60" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>和SWCF<citation id="193" type="reference"><link href="62" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>。所选算法均为近年来主流的相关滤波跟踪算法, 且跟踪速度能够满足在线跟踪场景的实时性需求, 源码均由其作者提供。</p>
                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904033_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 所提算法的流程图" src="Detail/GetImg?filename=images/GXXB201904033_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 所提算法的流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904033_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Flowchart of proposed algorithm</p>

                </div>
                <div class="p1">
                    <p id="132">实验中, 主要以生成精度图和成功图两种曲线图的方式, 展现不同的参数设定对算法跟踪结果的影响, 以及将本课题组所提算法与7种对比算法的性能进行比较。需要说明的是, 与精度图相关的评价指标是中心误差, 即跟踪器输出的矩形框中心与目标实际中心位置的像素点距离, 中心误差小于给定阈值时, 认为跟踪成功;与成功图相关的评价指标是重叠率, 即跟踪器输出的矩形区域与目标实际区域的交集占这两个区域并集的比例, 重叠率大于给定阈值时, 认为跟踪成功。上述两阈值在一定范围内变动时, 会得到由一系列成功率数值构成的曲线, 对应中心误差时的曲线图为精度图, 对应重叠率时的曲线图为成功图<citation id="194" type="reference"><link href="64" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="133" name="133"><b>3.1 参数</b><i>α</i><sub><b>1</b></sub><b>和</b><i>α</i><sub><b>2</b></sub><b>的设定</b></h4>
                <div class="p1">
                    <p id="134">所提算法中置信度条件的严格程度由 (11) 式中<i>α</i><sub>1</sub>和<i>α</i><sub>2</sub>这两个参数决定:<i>α</i><sub>1</sub>和<i>α</i><sub>2</sub>取值越大, 表示置信度条件越严格, 尺度估计和滤波器模型更新的次数会随之减少 (如图2所示, 落在低置信度区间范围内的帧数越多) ; <i>α</i><sub>1</sub>和<i>α</i><sub>2</sub>取值越小, 表示置信度条件越松弛。为了找到最优的参数组合<mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>α</mi><mo>^</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mover accent="true"><mi>α</mi><mo>^</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>, 首先设定两个集合<i>S</i><sub>1</sub>和<i>S</i><sub>2</sub>分别表示<i>α</i><sub>1</sub>和<i>α</i><sub>2</sub>取值的估计范围, 然后取<i>S</i><sub>1</sub>和<i>S</i><sub>2</sub>的笛卡儿积<mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mn>1</mn></msub><mo>×</mo><mi>S</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo stretchy="false"> (</mo><mover accent="true"><mi>α</mi><mo>^</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mover accent="true"><mi>α</mi><mo>^</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>应为集合S<sub>1</sub>×S<sub>2</sub>中跟踪效果最好的元素。实验中, 取S<sub>1</sub>={0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8}, S<sub>2</sub>={0.4, 0.45, 0.5, 0.55, 0.6}, 则通过笛卡儿积可得到35种α<sub>1</sub>和α<sub>2</sub>的组合方式, 分别在<i>OTB</i>2015数据集上的100组视频序列上进行测试, 得到目前通用的一次性运行评价 (<i>OPE</i>) 模式下的精度图和成功图, 如图4所示 (图中只显示前10位的跟踪结果) 。图4 (<i>a</i>) 的图例中各项按照中心误差阈值取20 <i>pixel</i>时的平均成功率 (图例中括号内的数值) 对各参数组合对应的跟踪结果进行排序, 图4 (<i>b</i>) 的图例中各项按照成功率曲线与坐标轴围成的面积 (<i>AUC</i>) 占比 (图例中括号内数值) 进行排序。<i>OPE</i>模式在第1帧初始化目标位置, 整个运行过程中不进行任何干预。由图4可知, 精度图中跟踪效果最优的参数组合是α<sub>1</sub>=0.60, α<sub>2</sub>=0.55 (该组参数在成功图中排名第5) , 而成功图中跟踪效果最优的参数组合是α<sub>1</sub>=0.60, α<sub>2</sub>=0.45 (该组参数在精度图中排名第3) 。相比于精度图, 成功图既考虑了目标中心位置, 又考虑了目标尺度, 能够更全面地体现跟踪算法的准确性。因此, 最终参数选择的原则可以概括为在注重成功图结果的基础上兼顾精度图。综合评定后, 将最优的参数组合<mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>α</mi><mo>^</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mover accent="true"><mi>α</mi><mo>^</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>设定为 (0.60, 0.45) , 以下比较实验均在此参数下运行。</p>
                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904033_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 所提算法在不同参数下的跟踪结果。 (a) OPE模式下的精度图; (b) OPE模式下的成功图" src="Detail/GetImg?filename=images/GXXB201904033_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 所提算法在不同参数下的跟踪结果。 (a) OPE模式下的精度图; (b) OPE模式下的成功图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904033_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Tracking results of proposed algorithm with different parameter settings. (a) Precision plot obtained at OPE mode; (b) success plot obtained at OPE mode</p>

                </div>
                <h4 class="anchor-tag" id="140" name="140"><b>3.2 定量比较</b></h4>
                <div class="p1">
                    <p id="141">除了OPE模式, 还有文献<citation id="195" type="reference">[<a class="sup">25</a>]</citation>提出的另外两种运行模式, 即空间稳健性评价 (SRE) 模式和时间稳健性评价 (TRE) 模式。不同于OPE模式, SRE模式在第1帧初始化时通过对跟踪框位置进行不同程度的平移、缩放等微小扰动来引入误差, 以此使跟踪场景更趋近于现实场景, 测试跟踪算法是否对初始化位置误差敏感, 每一段视频序列对每个跟踪器测试12次;TRE模式则不是对第1帧进行初始化, 而是对序列中随机的一帧进行初始化, 以此来衡量跟踪算法在时间轴上的稳健性, 每一段序列对每个跟踪器测试20次。为了全面比较所提算法与其他7种相关滤波跟踪算法的总体跟踪性能, 采用OPE、SRE、TRE这3种模式分别进行测试, 生成100组视频序列的精度图和成功图, 如图5所示。由图5可知, 所提算法在各种运行模式下的精度图和成功图中均取得了最优的跟踪结果, 说明所提算法在跟踪准确性方面具有一定优势, 且具有更好的空间稳健性和时间稳健性。</p>
                </div>
                <div class="p1">
                    <p id="142">为了进一步分析所提算法在不同挑战因素下的跟踪性能, 将各算法在11种不同属性的视频序列集合上分别进行测试 (每组视频序列对应多个不同的属性) 。这些属性包括:含38组序列的光照变化 (IV) 属性、含65组序列的尺度变化 (SV) 属性、含49组序列的遮挡 (OCC) 属性、含44组序列的形变 (DEF) 属性、含31组序列的运动模糊 (MB) 属性、含42组序列的快速运动 (FM) 属性、含50组序列的平面内旋转 (IPR) 属性、含64组序列的平面外旋转 (OPR) 属性、含14组序列的运动出视野 (OV) 属性、含31组序列的背景繁杂 (BC) 属性和含10组序列的低分辨率 (LR) 属性。对OPE模式下运行的精度图和成功图数据进行归纳, 跟踪结果如表1和表2所示, 其中加粗的数据表示单个属性下的最优结果。</p>
                </div>
                <div class="p1">
                    <p id="143">由表1和表2可知:所提算法在尺度变化、遮挡、形变、运动模糊、平面外旋转、运动出视野和低分辨率共7个属性的精度图和成功图中均取得了最优的跟踪结果。相比于排名第2的fDSST算法, 在尺度变化属性方面, 所提算法在减少尺度更新次数的情况下取得了更好的效果, 说明所提算法通过高置信度更新策略去除冗余尺度更新操作的做法是切实有效的;在遮挡、形变、运动出视野、低分辨率等属性方面, 所提算法具有明显优势, 其中遮挡属性的精度图和成功图分别提升了5.61%和5.2%, 形变属性的精度图和成功图分别提升了4.27%和3.77%, 运动出视野属性的精度图和成功图分别提升了3.98%和4.08%, 低分辨率属性的精度图和成功图分别提升了4.91%和5.21%, 说明所提算法只有在满足高置信度条件下才更新滤波器模型的处理方式, 能够提升跟踪算法的抗干扰性。另外, 所提算法在其余4个属性条件下取得了次优的跟踪结果 (光照变化和平面内旋转两个属性的精度图结果除外) 。总体来说, 所提算法在各种复杂的跟踪场景下具有更好的稳健性, 尤其是对于目标被遮挡等情况具有更好的适应性。</p>
                </div>
                <div class="p1">
                    <p id="144">表3所示为所提算法在不同阶段的性能评估结果 (在100组视频序列上以OPE模式运行) , 其中Proposed (NDR &amp; NHU) 表示既不使用高置信度更新策略, 也不使用特征降维的方法。由表3可知, 所提算法在各阶段的测试中跟踪准确性稳步提升, 平均跟踪速度得到了成倍提升, 验证了所提算法采用特征降维和高置信度更新等加速方法的有效性。</p>
                </div>
                <div class="area_img" id="145">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904033_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 各算法在100组视频序列上的跟踪结果。 (a) OPE模式下的精度图; (b) OPE模式下的成功图; (c) SRE模式下的精度图; (d) SRE模式下的成功图; (e) TRE模式下的精度图; (f) TRE模式下的成功图" src="Detail/GetImg?filename=images/GXXB201904033_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 各算法在100组视频序列上的跟踪结果。 (a) OPE模式下的精度图; (b) OPE模式下的成功图; (c) SRE模式下的精度图; (d) SRE模式下的成功图; (e) TRE模式下的精度图; (f) TRE模式下的成功图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904033_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Tracking results of different algorithms on 100 video sequences. (a) Precision plot obtained at OPE mode; (b) success plot obtained at OPE mode; (c) precision plot obtained at SRE mode; (d) success plot obtained at SRE mode; (e) precision plot obtained at TRE mode; (f) success plot obtained at TRE mode</p>

                </div>
                <div class="area_img" id="146">
                    <p class="img_tit">表1 OPE模式下各算法对不同属性的视频序列集合进行测试后得到的精度图数据 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Precision plot values correspond to different algorithms which are used to test sets of video sequences with different attributes at OPE mode </p>
                    <p class="img_note">%</p>
                    <table id="146" border="1"><tr><td><br />Tracker</td><td>IV</td><td>SV</td><td>OCC</td><td>DEF</td><td>MB</td><td>FM</td><td>IPR</td><td>OPR</td><td>OV</td><td>BC</td><td>LR</td></tr><tr><td><br />Proposed</td><td>66.70</td><td><b>64.73</b></td><td><b>64.07</b></td><td><b>60.99</b></td><td><b>66.76</b></td><td>63.75</td><td>63.78</td><td><b>63.75</b></td><td><b>57.27</b></td><td>65.49</td><td><b>64.19</b></td></tr><tr><td><br />fDSST</td><td><b>68.39</b></td><td>62.82</td><td>58.46</td><td>56.72</td><td>64.82</td><td><b>64.23</b></td><td><b>67.24</b></td><td>61.60</td><td>53.29</td><td><b>71.13</b></td><td>59.28</td></tr><tr><td><br />DSST</td><td>68.01</td><td>61.72</td><td>56.89</td><td>53.20</td><td>56.85</td><td>55.04</td><td>64.45</td><td>61.15</td><td>46.29</td><td>64.54</td><td>56.62</td></tr><tr><td><br />SWCF</td><td>67.02</td><td>61.35</td><td>58.37</td><td>53.71</td><td>55.82</td><td>52.12</td><td>63.14</td><td>59.88</td><td>45.21</td><td>63.08</td><td>53.89</td></tr><tr><td><br />CN</td><td>54.28</td><td>51.07</td><td>51.44</td><td>50.16</td><td>45.70</td><td>46.30</td><td>60.38</td><td>57.14</td><td>42.84</td><td>57.06</td><td>47.01</td></tr><tr><td><br />CFLB</td><td>37.05</td><td>44.16</td><td>41.02</td><td>39.56</td><td>39.92</td><td>40.09</td><td>45.29</td><td>41.77</td><td>33.74</td><td>38.44</td><td>55.62</td></tr><tr><td><br />CSK</td><td>47.29</td><td>44.92</td><td>42.01</td><td>42.54</td><td>36.52</td><td>38.99</td><td>48.99</td><td>47.13</td><td>27.66</td><td>52.72</td><td>41.06</td></tr><tr><td><br />KCF</td><td>64.20</td><td>58.56</td><td>58.13</td><td>56.84</td><td>56.40</td><td>57.52</td><td>63.24</td><td>61.74</td><td>47.91</td><td>64.58</td><td>51.14</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="147">
                    <p class="img_tit">表2 OPE模式下各算法对不同属性的视频序列集合进行测试后得到的成功图数据 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Success plot values correspond to different algorithms which are used to test sets of video sequences with different attributes at OPE mode </p>
                    <p class="img_note">%</p>
                    <table id="147" border="1"><tr><td><br />Tracker</td><td>IV</td><td>SV</td><td>OCC</td><td>DEF</td><td>MB</td><td>FM</td><td>IPR</td><td>OPR</td><td>OV</td><td>BC</td><td>LR</td></tr><tr><td><br />Proposed</td><td>56.58</td><td><b>52.48</b></td><td><b>53.56</b></td><td><b>50.54</b></td><td><b>58.67</b></td><td>54.44</td><td>52.20</td><td><b>51.79</b></td><td><b>49.84</b></td><td>55.09</td><td><b>49.82</b></td></tr><tr><td><br />fDSST</td><td><b>56.78</b></td><td>51.08</td><td>48.36</td><td>46.77</td><td>56.30</td><td><b>55.49</b></td><td><b>55.00</b></td><td>50.17</td><td>45.76</td><td><b>58.58</b></td><td>44.61</td></tr><tr><td><br />DSST</td><td>56.11</td><td>48.59</td><td>46.10</td><td>43.43</td><td>49.20</td><td>47.10</td><td>51.00</td><td>48.28</td><td>38.48</td><td>52.40</td><td>38.94</td></tr><tr><td><br />SWCF</td><td>55.29</td><td>48.08</td><td>46.70</td><td>43.43</td><td>48.35</td><td>44.83</td><td>49.98</td><td>47.18</td><td>37.88</td><td>51.01</td><td>36.82</td></tr><tr><td><br />CN</td><td>41.55</td><td>35.94</td><td>39.63</td><td>39.61</td><td>37.85</td><td>37.76</td><td>45.49</td><td>42.10</td><td>35.08</td><td>43.88</td><td>29.45</td></tr><tr><td><br />CFLB</td><td>29.63</td><td>32.84</td><td>31.30</td><td>31.49</td><td>34.69</td><td>34.28</td><td>35.20</td><td>31.66</td><td>27.58</td><td>31.96</td><td>35.64</td></tr><tr><td><br />CSK</td><td>36.85</td><td>32.39</td><td>33.13</td><td>33.70</td><td>31.39</td><td>32.63</td><td>38.05</td><td>35.39</td><td>24.96</td><td>41.00</td><td>26.33</td></tr><tr><td><br />KCF</td><td>47.92</td><td>39.86</td><td>44.30</td><td>43.62</td><td>45.56</td><td>44.84</td><td>47.22</td><td>45.12</td><td>39.33</td><td>49.77</td><td>30.69</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="148">
                    <p class="img_tit">表3 OPE模式下所提算法在不同阶段的性能评估 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Performance evaluation of proposed algorithm in different stages at OPE mode</p>
                    <p class="img_note"></p>
                    <table id="148" border="1"><tr><td><br />Tracker</td><td>Score on precision plot /%</td><td>Score on success plot /%</td><td>Average speed / (frame·s<sup>-1</sup>) </td></tr><tr><td><br />Proposed (NDR &amp; NHU) </td><td>64.01</td><td>52.16</td><td>37.6</td></tr><tr><td><br />Proposed (NHU) </td><td>66.22</td><td>54.40</td><td>88.1</td></tr><tr><td><br />Proposed</td><td>68.61</td><td>56.81</td><td>122.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="150">表4所示为包括本课题组所提算法在内的8种实时性较好的在线跟踪算法的平均跟踪速度 (实测均大于20 frame/s) , 可知:跟踪速度大于100 frame/s的高速跟踪算法包括所提算法、fDSST、CFLB、CSK和KCF共5种算法;所提算法的跟踪速度为122.3 frame/s, 与fDSST算法102.2 frame/s的跟踪速度相比, 提升了约20%, 其他4种高速跟踪算法虽然运行更快, 但整体性能均与所提算法存在较大差距。</p>
                </div>
                <h4 class="anchor-tag" id="151" name="151"><b>3.3 定性比较</b></h4>
                <div class="p1">
                    <p id="152">选取8组具有各种挑战因素的典型视频序列对所提算法进行定性分析, 各视频的挑战因素对应的属性及其他的相关信息如表5所示, 所提算法与其他对比算法对其中部分帧的跟踪结果如图6所示。</p>
                </div>
                <div class="area_img" id="153">
                    <p class="img_tit">表4 各算法的平均跟踪速度 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Average tracking speed of different algorithms</p>
                    <p class="img_note"></p>
                    <table id="153" border="1"><tr><td>Tracker</td><td>Proposed</td><td>fDSST</td><td>DSST</td><td>SWCF</td><td>CN</td><td>CFLB</td><td>CSK</td><td>KCF</td></tr><tr><td><br />Average speed / (frame·s<sup>-1</sup>) </td><td>122.3</td><td>102.2</td><td>42.6</td><td>22.0</td><td>239.3</td><td>190.1</td><td>523.1</td><td>291.1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="154">
                    <p class="img_tit">表5 8组视频序列的属性及相关信息 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Attributes and relevant information of eight video sequences</p>
                    <p class="img_note"></p>
                    <table id="154" border="1"><tr><td><br />Sequence</td><td>Attributes</td><td>Frame</td><td>Object size / (pixel) </td></tr><tr><td><br />Tiger1</td><td>IV, OCC, DEF, MB, FM, IPR, OPR</td><td>354</td><td>84×67</td></tr><tr><td><br />DragonBoy</td><td>SV, OCC, MB, FM, IPR, OPR, OV</td><td>113</td><td>65×56</td></tr><tr><td><br />Bird2</td><td>OCC, DEF, FM, IPR, OPR</td><td>99</td><td>73×69</td></tr><tr><td><br />Board</td><td>SV, MB, FM, OPR, OV, BC</td><td>698</td><td>173×198</td></tr><tr><td><br />Panda</td><td>SV, OCC, DEF, IPR, OPR, OV, LR</td><td>1000</td><td>23×28</td></tr><tr><td><br />Jogging-1</td><td>OCC, DEF, OPR</td><td>307</td><td>101×25</td></tr><tr><td><br />Girl2</td><td>SV, OCC, DEF, MB, OPR</td><td>1500</td><td>171×44</td></tr><tr><td><br />Human6</td><td>SV, OCC, DEF, FM, OPR, OV</td><td>792</td><td>55×18</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904033_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 8组视频序列上的部分跟踪结果。 (a) Tiger1; (b) DragonBaby; (c) Bird2; (d) Board; (e) Panda; (f) Jogging-1; (g) Girl2; (h) Human6" src="Detail/GetImg?filename=images/GXXB201904033_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 8组视频序列上的部分跟踪结果。 (a) Tiger1; (b) DragonBaby; (c) Bird2; (d) Board; (e) Panda; (f) Jogging-1; (g) Girl2; (h) Human6  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904033_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Partial tracking results on eight video sequences. (a) Tiger1; (b) DragonBaby; (c) Bird2; (d) Board; (e) Panda; (f) Jogging-1; (g) Girl2; (h) Human6</p>

                </div>
                <div class="p1">
                    <p id="156">由图6 (a) 可知, 在Tiger1视频序列中, 目标在第349帧被严重遮挡, 在第354帧重新出现后发生了明显的形变, 只有所提算法能准确跟踪到目标;由图6 (b) 可知, 在DragonBoy视频序列中, 目标在很短的时间内出现了快速运动和频繁的旋转变化, 除所提算法外, 其他算法均偏离了目标;由图6 (c) 可知, 在Bird2视频序列中, 所提算法和CN算法对目标的定位最准确;由图6 (d) 可知, 在Board视频序列中, 背景较繁杂, 且目标在第470～670帧过程中出现了明显的由小到大的尺度变化, 只有所提算法能够稳健地跟踪到目标, 且对尺度的处理较准确;由图6 (e) 可知, Panda视频序列的分辨率较低, 且视频中的目标尺寸很小, 所提算法和CFLB算法可以保持对目标的跟踪, 但是由第494帧和第532帧的结果可以看出, 所提算法的跟踪精度更高;由图6 (f) 可知, 在Jogging-1视频序列中, 目标经历了完全遮挡, 只有所提算法能够自始至终跟踪成功;由图6 (g) 可知, Girl2视频序列属于长时视频, 目标在运动过程中存在运动模糊、遮挡、尺度变化等挑战, 所提算法、SWCF和CN这三种算法能够保持对目标的跟踪, 其中所提算法的精度最高, 且对尺度变化的预测更准确;由图6 (h) 可知, 在Human6视频序列中, 只有所提算法和fDSST算法能成功跟踪到目标, 但是由第792帧结果可知, fDSST最后也偏离了目标。综上所述, 定性分析结果表明所提算法在应对跟踪过程中出现的形变、遮挡、快速运动、尺度变化等各种挑战时, 具有较好的稳健性。</p>
                </div>
                <h3 id="157" name="157" class="anchor-tag">4 结 论</h3>
                <div class="p1">
                    <p id="158">为了解决在线跟踪场景中存在的目标形变、遮挡、实时性需求等问题, 本课题组提出了一种基于高置信度更新策略的相关滤波跟踪算法。首先融合灰度、HOG和CN特征, 并降维加速, 利用相关滤波模型对目标进行快速定位;然后计算响应图的最高响应值和APCE值, 只有在满足高置信度的条件下才进行进一步的尺度估计和滤波器模型的更新操作。实验结果表明, 所提算法在100组视频序列上测试的结果具有较高的准确性和较好的稳健性, 尤其是在遮挡等情况下能够有效地缓解模型漂移。在跟踪速度方面, 所提算法的平均速度达到122.3 frame/s, 能够满足实际应用中所需的实时性需求。但所提算法也存在不足之处, 在光照变化、平面内旋转等情况下的适应性还有待提高。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201801008&amp;v=MjcxODI3WWJMRzRIOW5Ncm85RmJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtVYjNNS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Lu H C, Li P X, Wang D.Visual object tracking:a survey[J].Pattern Recognition and Artificial Intelligence, 2018, 31 (1) :61-76.卢湖川, 李佩霞, 王栋.目标跟踪算法综述[J].模式识别与人工智能, 2018, 31 (1) :61-76.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Visual Object Tracking VOT2017 Challenge Results">

                                <b>[2]</b> Kristan M, Leonardis A, Matas J, <i>et al</i>.The visual object tracking VOT2017 challenge results[C]//Proceedings of IEEE International Conference on Computer Vision Workshop, October 22-29, 2017, Venice, Italy.New York:IEEE, 2017:1949-1972.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">

                                <b>[3]</b> Bolme D S, Beveridge J R, Draper B A, <i>et al</i>.Visual object tracking using adaptive correlation filters[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 13-18, 2010, San Francisco, CA, USA.New York:IEEE, 2010:2544-2550.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">

                                <b>[4]</b> Henriques J F, Caseiro R, Martins P, <i>et al</i>.Exploiting the circulant structure of tracking-by-detection with kernels[C]//Proceedings of the 12th European Conference on Computer Vision, October 7-13, 2012, Florence, Italy.Heidelberg:Springer-Verlag, 2012:702-715.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" >
                                    <b>[5]</b>
                                 Henriques J F, Caseiro R, Martins P, <i>et al</i>.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive color attributes for real-time visual tracking">

                                <b>[6]</b> Danelljan M, Khan F S, Felsberg M, <i>et al</i>.Adaptive color attributes for real-time visual tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:1090-1097.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201705021&amp;v=MDc5OTFNcW85SFpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtVYjNNSWpYVGJMRzRIOWI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Shen Q, Yan X L, Liu L F, <i>et al</i>.Multi-scale correlation filtering tracker based on adaptive feature selection[J].Acta Optica Sinica, 2017, 37 (5) :0515001.沈秋, 严小乐, 刘霖枫, 等.基于自适应特征选择的多尺度相关滤波跟踪[J].光学学报, 2017, 37 (5) :0515001.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                 Li Y, Zhu J K.A scale adaptive kernel correlation filter tracker with feature integration[C]//Proceedings of the 13th European Conference on Computer Vision, September 6-7, 12, 2014, Zurich, Switzerland.Switzerland:Springer, 2014:254-265.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate scale estimation for robust visual tracking">

                                <b>[9]</b> Danelljan M, Häger G, Khan F S, <i>et al</i>.Accurate scale estimation for robust visual tracking[C]//Proceedings of British Machine Vision Conference, September 1-5, 2014, Nottingham, UK.Durham:BMVA Press, 2014:1-11.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 Danelljan M, Hager G, Khan F S, <i>et al</i>.Discriminative scale space tracking[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (8) :1561-1575.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201807026&amp;v=MDIxNDdlWmVWdUZ5SGtVYjNNSWpYVGJMRzRIOW5NcUk5SFlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Liao X F, Hou Z Q, Yu W S, <i>et al</i>.A scale adapted tracking algorithm based on kernelized correlation[J].Acta Optica Sinica, 2018, 38 (7) :0715002.廖秀峰, 侯志强, 余旺盛, 等.基于核相关的尺度自适应视觉跟踪[J].光学学报, 2018, 38 (7) :0715002.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                 Zhang T Z, Xu C S, Yang M H.Multi-task correlation particle filter for robust object tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4819-4827.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 Wang M M, Liu Y, Huang Z Y.Large margin object tracking with circulant feature maps[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:4800-4808.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201806041&amp;v=MDc0NzdCdEdGckNVUkxPZVplVnVGeUhrVWIzTUx5clBaTEc0SDluTXFZOUJaWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Ma X H.Updating method of improved gradient threshold in object tracking[J].Laser &amp; Optoelectronics Progress, 2018, 55 (6) :061502.马晓虹.目标跟踪中增强梯度阈值的更新方法[J].激光与光电子学进展, 2018, 55 (6) :061502.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long-term correlation tracking">

                                <b>[15]</b> Ma C, Yang X K, Zhang C Y, <i>et al</i>.Long-term correlation tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:5388-5396.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical convolutional features for visual tracking">

                                <b>[16]</b> Ma C, Huang J B, Yang X K, <i>et al</i>.Hierarchical convolutional features for visual tracking[C]//Proceedings of IEEE International Conference on Computer Vision, December 7-13, 2015, Santiago, Chile.New York:IEEE, 2015:3074-3082.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond correlation filters:Learning continuous convolution operators for visual tracking">

                                <b>[17]</b> Danelljan M, Robinson A, Khan F S, <i>et al</i>.Beyond correlation filters:learning continuous convolution operators for visual tracking[C]//Proceedings of the 14th European Conference on Computer Vision, October 11-14, 2016, Amsterdam, The Netherlands.Switzerland:Springer, 2016:472-488.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201711027&amp;v=MjkyMjhIOWJOcm85SFk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtVYjNNSWpYVGJMRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Wang X, Hou Z Q, Yu W S, <i>et al</i>.Target scale adaptive robust tracking based on fusion of multilayer convolutional features[J].Acta Optica Sinica, 2017, 37 (11) :1115005.王鑫, 侯志强, 余旺盛, 等.基于多层卷积特征融合的目标尺度自适应稳健跟踪[J].光学学报, 2017, 37 (11) :1115005.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_19" >
                                    <b>[19]</b>
                                 Danelljan M, Bhat G, Khan F S, <i>et al</i>.ECO:efficient convolution operators for tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, July 21-26, 2017, Honolulu, HI, USA.New York:IEEE, 2017:6931-6939.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201810031&amp;v=Mjc1NzA1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIa1ViM01JalhUYkxHNEg5bk5yNDlHWllRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> Xiong C Z, Che M Q, Wang R L, <i>et al</i>.Robust and real-time visual tracking via dual model adaptive switching[J].Acta Optica Sinica, 2018, 38 (10) :1015002.熊昌镇, 车满强, 王润玲, 等.稳健的双模型自适应切换实时跟踪算法[J].光学学报, 2018, 38 (10) :1015002.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning multi-domain convolutional neural networks for visual tracking">

                                <b>[21]</b> Nam H, Han B.Learning multi-domain convolutional neural networks for visual tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:4293-4302.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_22" >
                                    <b>[22]</b>
                                 Bertinetto L, Valmadre J, Henriques J F, <i>et al</i>.Fully-convolutional siamese networks for object tracking[C]//Proceedings of the 14th European Conference on Computer Vision, October 11-14, 2016, Amsterdam, The Netherlands.Switzerland:Springer, 2016:850-865.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep relative tracking">

                                <b>[23]</b> Gao J Y, Zhang T Z, Yang X S, <i>et al</i>.Deep relative tracking[J].IEEE Transactions on Image Processing, 2017, 26 (4) :1845-1858.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatial-aware regressions for visual tracking">

                                <b>[24]</b> Sun C, Wang D, Lu H C, <i>et al</i>.Learning spatial-aware regressions for visual tracking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:8962-8970.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_25" >
                                    <b>[25]</b>
                                 Wu Y, Lim J, Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1834-1848.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Correlation Filters with Limited Boundaries">

                                <b>[26]</b> Galoogahi H K, Sim T, Lucey S.Correlation filters with limited boundaries[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, June 7-12, 2015, Boston, MA, USA.New York:IEEE, 2015:4630-4638.
                            </a>
                        </p>
                        <p id="62">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial windowing for correlation filter based visual tracking">

                                <b>[27]</b> Gundogdu E, Alatan A A.Spatial windowing for correlation filter based visual tracking[C]//Proceedings of IEEE International Conference on Image Processing, September 25-28, 2016, Phoenix, AZ, USA.New York:IEEE, 2016:1684-1688.
                            </a>
                        </p>
                        <p id="64">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606004&amp;v=MTU4MjdlWmVWdUZ5SGtVYjNNS0NMZlliRzRIOWZNcVk5RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b> Guan H, Xue X Y, An Z Y.Advances on application of deep learning for video object tracking[J].Acta Automatica Sinica, 2016, 42 (6) :834-847.管皓, 薛向阳, 安志勇.深度学习在视频目标跟踪中的应用进展与展望[J].自动化学报, 2016, 42 (6) :834-847.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201904033" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201904033&amp;v=MjEyNDZHNEg5ak1xNDlHWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIa1ViM01JalhUYkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

