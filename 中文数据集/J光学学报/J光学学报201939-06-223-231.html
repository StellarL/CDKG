

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133922324658750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201906028%26RESULT%3d1%26SIGN%3dtkvz6kn8U6WXHrVYDT5u%252f9FiUvY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201906028&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201906028&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201906028&amp;v=MDM4ODNVUkxPZVplVnVGeXZuVXIzSklqWFRiTEc0SDlqTXFZOUhiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="2 SSD检测框架 ">2 SSD检测框架</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="&lt;b&gt;2.1 SSD模型结构&lt;/b&gt;"><b>2.1 SSD模型结构</b></a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;2.2 特征层映射和损失函数&lt;/b&gt;"><b>2.2 特征层映射和损失函数</b></a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;2.3 经典SSD在目标检测中的不足&lt;/b&gt;"><b>2.3 经典SSD在目标检测中的不足</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#94" data-title="3 增强的SSD模型 ">3 增强的SSD模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#96" data-title="&lt;b&gt;3.1 特征层双向融合&lt;/b&gt;"><b>3.1 特征层双向融合</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;3.2 浅层特征增强分支&lt;/b&gt;"><b>3.2 浅层特征增强分支</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#111" data-title="4 实验与分析 ">4 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#113" data-title="&lt;b&gt;4.1 PASCAL-VOC2007小目标数据集测试结果&lt;/b&gt;"><b>4.1 PASCAL-VOC2007小目标数据集测试结果</b></a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;4.2 空中红外目标数据集测试结果&lt;/b&gt;"><b>4.2 空中红外目标数据集测试结果</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;4.3 增强SSD的实时性分析&lt;/b&gt;"><b>4.3 增强SSD的实时性分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#131" data-title="5 结  论 ">5 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="图1 SSD网络结构框图">图1 SSD网络结构框图</a></li>
                                                <li><a href="#93" data-title="表1 SSD_300&#215;300卷积感受野、默认框映射图像区域">表1 SSD_300×300卷积感受野、默认框映射图像区域</a></li>
                                                <li><a href="#104" data-title="图2 多种特征融合方法示意图">图2 多种特征融合方法示意图</a></li>
                                                <li><a href="#105" data-title="图3 语义分割分支结构图">图3 语义分割分支结构图</a></li>
                                                <li><a href="#116" data-title="图4 原始SSD与改进SSD检测小目标的结果对比">图4 原始SSD与改进SSD检测小目标的结果对比</a></li>
                                                <li><a href="#117" data-title="表2 VOC2007数据集小目标检测结果">表2 VOC2007数据集小目标检测结果</a></li>
                                                <li><a href="#121" data-title="图5 空中红外目标检测结果">图5 空中红外目标检测结果</a></li>
                                                <li><a href="#122" data-title="表3 红外数据集空中目标检测结果">表3 红外数据集空中目标检测结果</a></li>
                                                <li><a href="#124" data-title="图6 空中红外目标召回率-准确率曲线对比">图6 空中红外目标召回率-准确率曲线对比</a></li>
                                                <li><a href="#129" data-title="表4 每个分类网络的预测框数目及运行速度对比">表4 每个分类网络的预测框数目及运行速度对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" Erhan D, Szegedy C, Toshev A, &lt;i&gt;et al&lt;/i&gt;.Scalable object detection using deep neural networks[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:2155-2162." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalable Object Detection Using Deep Neural Networks">
                                        <b>[1]</b>
                                         Erhan D, Szegedy C, Toshev A, &lt;i&gt;et al&lt;/i&gt;.Scalable object detection using deep neural networks[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:2155-2162.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" Borji A, Cheng M M, Jiang H Z, &lt;i&gt;et al&lt;/i&gt;.Salient object detection:a benchmark[J].IEEE Transactions on Image Processing, 2015, 24 (12) :5706-5722." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Salient Object Detection:A Benchmark">
                                        <b>[2]</b>
                                         Borji A, Cheng M M, Jiang H Z, &lt;i&gt;et al&lt;/i&gt;.Salient object detection:a benchmark[J].IEEE Transactions on Image Processing, 2015, 24 (12) :5706-5722.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" Singla N.Motion detection based on frame difference method[J].International Journal of Information &amp;amp; Computation Technology, 2014, 4 (15) :1559-1565." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Motion detection based on frame difference method">
                                        <b>[3]</b>
                                         Singla N.Motion detection based on frame difference method[J].International Journal of Information &amp;amp; Computation Technology, 2014, 4 (15) :1559-1565.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" Horn B K P, Schunck B G.Determining optical flow[J].Artificial Intelligence, 1981, 17 (1/2/3) :185-203." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Determining optical flow">
                                        <b>[4]</b>
                                         Horn B K P, Schunck B G.Determining optical flow[J].Artificial Intelligence, 1981, 17 (1/2/3) :185-203.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" Barinova O, Lempitsky V, Kholi P.On detection of multiple object instances using hough transforms[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (9) :1773-1784." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On Detection of Multiple Object Instances Using Hough Transforms">
                                        <b>[5]</b>
                                         Barinova O, Lempitsky V, Kholi P.On detection of multiple object instances using hough transforms[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (9) :1773-1784.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjgwNDF0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RlNqbFVMM0pJbDQ9Tmo3QmFyTzRI&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" Dalal N, Triggs B.Histograms of oriented gradients for human detection[C]//2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR′05) , June 20-25, 2005, San Diego, CA, USA.New York:IEEE, 2005:886-893." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">
                                        <b>[7]</b>
                                         Dalal N, Triggs B.Histograms of oriented gradients for human detection[C]//2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR′05) , June 20-25, 2005, San Diego, CA, USA.New York:IEEE, 2005:886-893.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" Burges C J C.A tutorial on support vector machines for pattern recognition[J].Data Mining and Knowledge Discovery, 1998, 2 (2) :121-167." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002157166&amp;v=MDg4MjlsND1OajdCYXJPNEh0SE9yb3BDWmUwSlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RlNqbFVMM0pJ&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Burges C J C.A tutorial on support vector machines for pattern recognition[J].Data Mining and Knowledge Discovery, 1998, 2 (2) :121-167.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" Hastie T, Rosset S, Zhu J, &lt;i&gt;et al&lt;/i&gt;.Multi-class AdaBoost[J].Statistics and Its Interface, 2009, 2 (3) :349-360." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJQI&amp;filename=SJQIF3BBEE8CB3D8F8F091DF44E2969FFE8E&amp;v=MTg2NjFiUTM1ZGhoeDdxK3dxZz1OaWZhWjhXN2JLTzUyb2MyRnVoN0JBb3h1UllhNjBzTFRIdVhyaHN6Y01UaU1MTHFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Hastie T, Rosset S, Zhu J, &lt;i&gt;et al&lt;/i&gt;.Multi-class AdaBoost[J].Statistics and Its Interface, 2009, 2 (3) :349-360.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     Ren S Q, He K M, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.</a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" Redmon J, Farhadi A.YOLOv3:an incremental improvement[EB/OL]. (2018-04-08) [2018-12-01].https://arxiv.org/abs/1804.02767." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLOv3 an incremental improvement">
                                        <b>[11]</b>
                                         Redmon J, Farhadi A.YOLOv3:an incremental improvement[EB/OL]. (2018-04-08) [2018-12-01].https://arxiv.org/abs/1804.02767.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                     Liu W, Anguelov D, Erhan D, &lt;i&gt;et al&lt;/i&gt;.SSD:single shot multibox dtector[M]//Leibe B, Matas J, Sebe N, &lt;i&gt;et al&lt;/i&gt;.Computer Vision:ECCV 2016.Cham:Springer, 2016, 9905:21-37.</a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic seg-mentation">
                                        <b>[13]</b>
                                         Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" He K M, Gkioxari G, Doll&#225;r P, &lt;i&gt;et al&lt;/i&gt;.Mask R-CNN[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018:2844175." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mask r-cnn">
                                        <b>[14]</b>
                                         He K M, Gkioxari G, Doll&#225;r P, &lt;i&gt;et al&lt;/i&gt;.Mask R-CNN[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018:2844175.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_15" title=" Li Z X, Zhou F Q.FSSD:feature fusion single shot multibox detector[EB/OL]. (2018-05-17) [2018-12-01].https://arxiv.org/abs/1712.00960." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FSSD:feature fusion single shot multibox detector">
                                        <b>[15]</b>
                                         Li Z X, Zhou F Q.FSSD:feature fusion single shot multibox detector[EB/OL]. (2018-05-17) [2018-12-01].https://arxiv.org/abs/1712.00960.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_16" title=" Cao G M, Xie X M, Yang W Z, &lt;i&gt;et al&lt;/i&gt;.Feature-fused SSD:fast detection for small objects[J].Proceedings of SPIE, 2018, 10615:106151E." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature-fused SSD:fast detection for small objects">
                                        <b>[16]</b>
                                         Cao G M, Xie X M, Yang W Z, &lt;i&gt;et al&lt;/i&gt;.Feature-fused SSD:fast detection for small objects[J].Proceedings of SPIE, 2018, 10615:106151E.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_17" title=" Fu C Y, Liu W, Ranga A, &lt;i&gt;et al&lt;/i&gt;.DSSD:deconvolutional single shot detector[EB/OL]. (2017-01-23) [2018-12-01].https://arxiv.org/abs/1701.06659." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dssd:Deconvolutional single shot detector">
                                        <b>[17]</b>
                                         Fu C Y, Liu W, Ranga A, &lt;i&gt;et al&lt;/i&gt;.DSSD:deconvolutional single shot detector[EB/OL]. (2017-01-23) [2018-12-01].https://arxiv.org/abs/1701.06659.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_18" title=" Jeong J, Park H, Kwak N.Enhancement of SSD by concatenating feature maps for object detection[EB/OL]. (2017-05-26) [2018-12-01].https://arxiv.org/abs/1705.09587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enhancement of SSD by concatenating feature maps for object detection">
                                        <b>[18]</b>
                                         Jeong J, Park H, Kwak N.Enhancement of SSD by concatenating feature maps for object detection[EB/OL]. (2017-05-26) [2018-12-01].https://arxiv.org/abs/1705.09587.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_19" title=" Tang C, Ling Y S, Zheng K D, &lt;i&gt;et al&lt;/i&gt;.Object detection method of multi-view SSD based on deep learning[J].Infrared and Laser Engineering, 2018, 47 (1) :126003.唐聪, 凌永顺, 郑科栋, 等.基于深度学习的多视窗SSD目标检测方法[J].红外与激光工程, 2018, 47 (1) :126003." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HWYJ201801042&amp;v=MDk2MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnl2blVyM0lMVHJTWkxHNEg5bk1ybzlCWm8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         Tang C, Ling Y S, Zheng K D, &lt;i&gt;et al&lt;/i&gt;.Object detection method of multi-view SSD based on deep learning[J].Infrared and Laser Engineering, 2018, 47 (1) :126003.唐聪, 凌永顺, 郑科栋, 等.基于深度学习的多视窗SSD目标检测方法[J].红外与激光工程, 2018, 47 (1) :126003.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_20" title=" Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2018-12-01].https://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[20]</b>
                                         Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2018-12-01].https://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_21" title=" He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.Deep residual learning for image recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[21]</b>
                                         He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;.Deep residual learning for image recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_22" title=" Lin T Y, Goyal P, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Focal loss for dense object detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017:2999-3007." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Focal Loss for Dense Object Detection">
                                        <b>[22]</b>
                                         Lin T Y, Goyal P, Girshick R, &lt;i&gt;et al&lt;/i&gt;.Focal loss for dense object detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017:2999-3007.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_23" title=" Xin P, Xu Y L, Tang H, &lt;i&gt;et al&lt;/i&gt;.Fast airplane detection based on multi-layer feature fusion of fully convolutional networks[J].Acta Optica Sinica, 2018, 38 (3) :0315003.辛鹏, 许悦雷, 唐红, 等.全卷积网络多层特征融合的飞机快速检测[J].光学学报, 2018, 38 (3) :0315003." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201803036&amp;v=MDAyMDdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnl2blVyM0lJalhUYkxHNEg5bk1ySTlHWW9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         Xin P, Xu Y L, Tang H, &lt;i&gt;et al&lt;/i&gt;.Fast airplane detection based on multi-layer feature fusion of fully convolutional networks[J].Acta Optica Sinica, 2018, 38 (3) :0315003.辛鹏, 许悦雷, 唐红, 等.全卷积网络多层特征融合的飞机快速检测[J].光学学报, 2018, 38 (3) :0315003.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_24" title=" Zhang Z S, Qiao S Y, Xie C H, &lt;i&gt;et al&lt;/i&gt;.Single-shot object detection with enriched semantics[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:5813-5821." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-shot object detection with enriched semantics">
                                        <b>[24]</b>
                                         Zhang Z S, Qiao S Y, Xie C H, &lt;i&gt;et al&lt;/i&gt;.Single-shot object detection with enriched semantics[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:5813-5821.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_25" title=" Feng X Y, Mei W, Hu D S.Aerialtarget detection based on improved faster R-CNN[J].Acta Optica Sinica, 2018, 38 (6) :0615004.冯小雨, 梅卫, 胡大帅.基于改进Faster R-CNN的空中目标检测[J].光学学报, 2018, 38 (6) :0615004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806034&amp;v=MTQ0NTZVcjNJSWpYVGJMRzRIOW5NcVk5R1lJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5dm4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         Feng X Y, Mei W, Hu D S.Aerialtarget detection based on improved faster R-CNN[J].Acta Optica Sinica, 2018, 38 (6) :0615004.冯小雨, 梅卫, 胡大帅.基于改进Faster R-CNN的空中目标检测[J].光学学报, 2018, 38 (6) :0615004.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_26" title=" Wang W X, Fu Y T, Dong F, &lt;i&gt;et al&lt;/i&gt;.Infrared ship target detection method based on deep convolution neural network[J].Acta Optica Sinica, 2018, 38 (7) :0712006.王文秀, 傅雨田, 董峰, 等.基于深度卷积神经网络的红外船只目标检测方法[J].光学学报, 2018, 38 (7) :0712006." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201807020&amp;v=MDc2MzFNcUk5SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5dm5VcjNJSWpYVGJMRzRIOW4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         Wang W X, Fu Y T, Dong F, &lt;i&gt;et al&lt;/i&gt;.Infrared ship target detection method based on deep convolution neural network[J].Acta Optica Sinica, 2018, 38 (7) :0712006.王文秀, 傅雨田, 董峰, 等.基于深度卷积神经网络的红外船只目标检测方法[J].光学学报, 2018, 38 (7) :0712006.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-02-25 09:20</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(06),223-231 DOI:10.3788/AOS201939.0615001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>用于空中红外目标检测的增强单发多框检测器方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%A2%E6%B1%9F%E8%8D%A3&amp;code=36524103&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谢江荣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E8%8C%83%E9%B8%A3&amp;code=09619181&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李范鸣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8D%AB%E7%BA%A2&amp;code=15704392&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卫红</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%86%B0&amp;code=28604097&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李冰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B5%E4%BF%9D%E6%B3%B0&amp;code=39751552&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邵保泰</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E4%B8%8A%E6%B5%B7%E6%8A%80%E6%9C%AF%E7%89%A9%E7%90%86%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0028796&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院上海技术物理研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E7%BA%A2%E5%A4%96%E6%8E%A2%E6%B5%8B%E4%B8%8E%E6%88%90%E5%83%8F%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0165468&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院红外探测与成像技术重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出了一种用于空中红外目标检测的增强单发多框检测器 (SSD) 方法。分析了感受野与特征图层数的关系, 同时采用池化和转置卷积操作的特征图双向融合机制, 从整体上增强了特征的表达能力。通过引入浅层特征图的语义增强分支, 并在高分辨率特征图上增加预测框, 可提升小尺寸目标的定位精度。在VOC2007小目标和空中红外目标数据集上进行了对比测试, 平均精度分别提高了7.1%和8.7%, 此时检测速度略有下降。结果表明, 增强SSD可在空中红外目标检测中获得较好的性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%95%E5%8F%91%E5%A4%9A%E6%A1%86%E6%A3%80%E6%B5%8B%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">单发多框检测器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A9%BA%E4%B8%AD%E7%BA%A2%E5%A4%96%E7%9B%AE%E6%A0%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空中红外目标;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义分割;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李范鸣 E-mail:lfmjws@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-18</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家十三五国防预研项目 (Jzx2016-0404/Y72-2);</span>
                                <span>上海市现场物证重点实验室基金 (2017xcwzk08);</span>
                    </p>
            </div>
                    <h1><b>Enhancement of Single Shot Multibox Detector for Aerial Infrared Target Detection</b></h1>
                    <h2>
                    <span>Xie Jiangrong</span>
                    <span>Li Fanming</span>
                    <span>Wei Hong</span>
                    <span>Li Bing</span>
                    <span>Shao Baotai</span>
            </h2>
                    <h2>
                    <span>Shanghai Institute of Technical Physics, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
                    <span>Key Laboratory of Infrared System Detection and Imaging Technology, Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A method for enhancement of a single shot multibox detector (SSD) for aerial infrared target detection is proposed. Herein, the relationship between the sensing field and number of feature layers is analyzed, and a bidirectional feature map fusion mechanism that uses both pooling and deconvolution operations is proposed to enhance the feature expression ability. The semantic enhancement branch of the shallow feature map is introduced and the prediction boxes on the high-resolution feature map are increased, so that the positing accuracy of small-size targets is improved. Comparative experiments on the VOC2007 small object dataset and an aerial infrared target dataset reveal that the mean average precisions increase by 7.1% and 8.7%, respectively, accompanied by a slight decrease in detection speed. The results demonstrate that SSD enhancements can achieve good performance in aerial infrared target detection.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=single%20shot%20multibox%20detector&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">single shot multibox detector;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=aerial%20infrared%20target&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">aerial infrared target;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=target%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">target detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semantic%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semantic segmentation;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-12-18</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="63" name="63" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="64">目标检测一直是计算机视觉领域的研究重点和热点<citation id="133" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>, 在无人驾驶、安防监控、机器人视觉、防空光电跟踪等领域具有不可替代的作用。在红外光电跟踪系统中, 空域背景变化多样, 并且空中目标占有的像素数较少、特征结构稀疏;另外, 高机动性的飞行器难以捕捉定位, 成像容易出现模糊失真的现象, 加大了算法识别的难度。因此, 有效的空中目标检测算法成为了提高跟踪精度和实时性的关键技术。</p>
                </div>
                <div class="p1">
                    <p id="65">目标检测要求同时获得目标的类别信息和定位信息。传统的模式识别方法大多依赖先验特性来建立数学模型并完成求解匹配, 如帧差法<citation id="134" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、光流法<citation id="135" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、Hough变换法<citation id="136" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等;另一种主流的方法采用特征算子[如SIFT (Scale-Invariant Feature Transform) <citation id="137" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、方向梯度直方图<citation id="138" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>]加分类器 (如支持向量机<citation id="139" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、AdaBoost<citation id="140" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>) 的模式, 对数据量要求较少, 在某些项目中能获得不错的效果。随着深度网络在计算机视觉领域获得突破, 卷积神经网络 (CNN) 被用于提取更高层、表达能力更强的特征, 并形成两种主流的深度网络检测框架:一类是结合区域提名和CNN的基于分类的R-CNN (Regions with CNN features) 系列目标检测框架;另一类是将目标检测算法转换为回归问题的单步算法。</p>
                </div>
                <div class="p1">
                    <p id="66">Faster R-CNN采用RPN (Region Proposal Networks) 结构, 能够在一个网络框架内完成候选区域、特征提取、分类和定位修正等操作, 真正实现了端到端的网络计算, 检测精度达到73.2%mAP (mean average precision) /5 frames·s<sup>-1</sup> (以下若无特殊说明, 数据集均指VOC2007) <citation id="141" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>;YOLOv3 (You Only Look Once) 将图片划分成多个网格, 在每个网格上一次性完成目标bounding box、置信度以及类别概率的预测, 牺牲了精度但换取了检测速度的大幅度提升, 最新版本引入多尺度特征融合的方式, 对小目标检测效果提升明显, 检测精度达到了57.9%mAP/20 frames·s<sup>-1</sup> (COCO数据集) , 基本代表了业界的最高水平<citation id="142" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>;单发多框检测器 (SSD) 结合了以上两者的优势, 可以实现高准确率和实时检测<citation id="143" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 在300 pixel×300 pixel分辨率时, 准确率为74.3%mAP/59 frames·s<sup>-1</sup>, 在512 pixel×512 pixel分辨率时, SSD获得了80%mAP/19 frames·s<sup>-1</sup>的结果, 超过了Faster R-CNN, 但仍存在小目标容易漏检、多个边界框重复检出的问题, 针对此类问题, 主要从改进网络结构和多尺度特征融合两个方向进行改善。全卷积网络的应用开创了语义分割的先河<citation id="144" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 与之后的Mask-RCNN<citation id="145" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>都可以实现目标像素级别的分类, 对于小目标能做出更精细的位置划分;另一方面, 文献<citation id="149" type="reference">[<a class="sup">15</a>,<a class="sup">16</a>]</citation>借鉴了FPN (Feature Pyramid Networks) 的思想, 提出的FSSD (Feature fusion Single Shot multibox Detector) 模型可以连接不同尺度的特征图, 重构出一组特征金字塔, 在小物体上获得了比原始SSD更高的检测精度;文献<citation id="146" type="reference">[<a class="sup">17</a>]</citation>中的DSSD (Deconvolutional Single Shot Detector) 模型将基础网络从VGG换成了表征能力更强的ResNet, 并引入反卷积层作为编解码器传递上下文信息, 在小物体检测上实现了优异的效果, 但是模型复杂度的增加削弱了实时性能;文献<citation id="147" type="reference">[<a class="sup">18</a>]</citation>提出改进的R-SSD模型, 将特征图采用简单的连接和转置卷积两种方式进行融合, 充分利用了特征图的方向信息, 改进了小目标的检测能力;文献<citation id="148" type="reference">[<a class="sup">19</a>]</citation>以经典SSD模型为基础, 从开辟多视窗结构出发, 通过融合5个预设视窗的输出结果, 在相应的小目标数据集上获得了一定的性能提升。</p>
                </div>
                <div class="p1">
                    <p id="67">红外探测器的工艺水平限制了响应率和分辨率, 与可见光图像相比, 一定距离外的红外目标图像不具备丰富的纹理细节、边界比较模糊, 只能利用热辐射几何分布和拓扑关系、飞行姿态与辐亮度的关联等浅层特征。本文基于SSD深度网络框架, 针对空中大视场背景下, 红外目标有效像素数少、特征稀疏, 伴随云朵遮挡、大幅度的姿态变化等不利因素, 提出了双向的特征图融合方法, 增强了各尺度上特征图的表达能力;同时, 引入语义分割支路, 通过增强更浅层特征层的语义, 获得更多的预测框。实验结果表明, 在VOC2007的小目标数据集和空中红外数据集中, mAP分别增加了7.1%和8.7%。</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag">2 SSD检测框架</h3>
                <div class="p1">
                    <p id="69">SSD是一种能够直接预测目标类别和位置的多目标检测方法<citation id="150" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。不同于Faster R-CNN等采用两阶段实现的深度检测框架, 它采用了回归的思想来解决多目标的检测问题, 大大地简化了网络结构和训练难度, 提高了算法的实时性能;引入的anchors机制有利于提取多尺度、多比例的特征, 相对于YOLO等全局特征提取的方法, 获得了更高的定位精度。</p>
                </div>
                <h4 class="anchor-tag" id="70" name="70"><b>2.1 SSD模型结构</b></h4>
                <div class="p1">
                    <p id="71">SSD的检测模型主要由两部分组成:1) 用于提取多层图像特征的深度CNN, 位于整个结构的前端, 可采用去除分类层后的图像分类网络, 如VGG_16<citation id="151" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、ResNet-101<citation id="152" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>等, 其中原先骨干网络中的全连接层 (FC6、FC7) 替换为相应的卷积层 (conv) , 卷积核尺寸和输出通道数详见图1下方的数据标注;2) 附加的级联网络, 由几个卷积层和最后一层的均值池化层 (Avg-pooling) 构成, 它针对前端获得的特征层, 进一步提取出在不同尺寸条件下的特征信息;将多级特征同时送入检测器中, 进行回归计算和极大值抑制后输出最终结果。SSD网络的结构框架如图1所示。图1中<i>c</i><sub>lasses</sub>为目标种类的数量。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906028_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 SSD网络结构框图" src="Detail/GetImg?filename=images/GXXB201906028_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 SSD网络结构框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906028_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Structure of SSD network</p>

                </div>
                <h4 class="anchor-tag" id="73" name="73"><b>2.2 特征层映射和损失函数</b></h4>
                <div class="p1">
                    <p id="74">在SSD的基础网络结构之后, 通过增加额外的卷积层, 能够产生信息更全面的多尺度特征图, 然后在各个特征图上分别进行目标预测。假设检测模型采用了<i>m</i>层特征图, 则第<i>k</i>个特征图上默认框占输入图像尺寸的比例为</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>+</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mo stretchy="false"> (</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mi>k</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>m</mi><mo stretchy="false">}</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">式中:<i>S</i><sub>min</sub>为特征层默认框占输入图像的最小比例, 一般取0.2;<i>S</i><sub>max</sub>为默认特征框占输入图像的最大比例, 一般取0.9。</p>
                </div>
                <div class="p1">
                    <p id="77">另外, SSD引入了Faster R-CNN的anchors机制, 在同一特征层上的默认框采用多个长宽比, 能够增强默认框对不同形状物体的稳健性。通常采用5种高宽比:<i>r</i>={<i>r</i><sub><i>n</i></sub>}={1, 2, 3, 1/2, 1/3} (<i>n</i>为序号, <i>n</i>=1, 2, 3, 4, 5) , 当高宽比<i>r</i><sub>1</sub>=1时, 添加<mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>S</mi><mo>′</mo></msup><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><msqrt><mrow><mi>S</mi><msub><mrow></mrow><mi>k</mi></msub><mi>S</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></msqrt></mrow></math></mathml>, 则所有默认框的宽<i>w</i><mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>n</mi></msubsup></mrow></math></mathml>和高h<mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>n</mi></msubsup></mrow></math></mathml>可以分别表示为</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msubsup><mrow></mrow><mi>k</mi><mi>n</mi></msubsup><mo>=</mo><mi>S</mi><msub><mrow></mrow><mi>k</mi></msub><msqrt><mrow><mi>r</mi><msub><mrow></mrow><mi>n</mi></msub></mrow></msqrt><mo>, </mo><mspace width="0.25em" /><mi>h</mi><msubsup><mrow></mrow><mi>k</mi><mi>n</mi></msubsup><mo>=</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><msqrt><mrow><mi>r</mi><msub><mrow></mrow><mi>n</mi></msub></mrow></msqrt></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><mi>n</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mn>3</mn><mo>, </mo><mn>4</mn><mo>, </mo><mn>5</mn><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">设定默认框的中心坐标为<mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>a</mi><mo>+</mo><mn>0</mn><mo>.</mo><mn>5</mn></mrow><mrow><mo stretchy="false">|</mo><mi>f</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">|</mo></mrow></mfrac><mo>, </mo><mfrac><mrow><mi>b</mi><mo>+</mo><mn>0</mn><mo>.</mo><mn>5</mn></mrow><mrow><mo stretchy="false">|</mo><mi>f</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">|</mo></mrow></mfrac></mrow><mo>) </mo></mrow></mrow></math></mathml>, 其中, |<i>f</i><sub><i>k</i></sub>|为第<i>k</i>个特征图的尺寸大小, <i>a</i>和<i>b</i>为锚点的横纵坐标, <i>a</i>, <i>b</i>∈{0, 1, 2, …, |<i>f</i><sub><i>k</i></sub>|-1}, 并截取默认框的坐标以保证其在[0, 1]的范围内。通过坐标转换, 实现特征图上的默认框与原始图像的映射。</p>
                </div>
                <div class="p1">
                    <p id="84">SSD在训练时对边界位置和目标种类同时进行回归, 其损失函数<i>L</i> (<i>x</i>, <i>c</i>, <i>l</i>, <i>g</i>) 定义为位置误差<i>L</i><sub>loc</sub>与置信度误差<i>L</i><sub>conf</sub>的加权和, 即</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>c</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>g</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mo stretchy="false">[</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>f</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>c</mi><mo stretchy="false">) </mo><mo>+</mo><mi>α</mi><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>g</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">式中:<i>N</i>为先验框正样本的个数;<i>x</i>∈{0, 1}表示预测框与真实框在某一类别上是否匹配<i>x</i>=0表示不匹配, <i>x</i>=1表示匹配;<i>c</i>为类别置信度的预测值;<i>l</i>为先验框对应边界位置的预测值;<i>g</i>为ground truth的位置参数;<i>L</i><sub>conf</sub> (<i>x</i>, <i>c</i>) 为置信度误差, 采用softmax loss;<i>L</i><sub>loc</sub> (<i>x</i>, <i>l</i>, <i>g</i>) 为位置误差, 采用Smooth-L1 loss;权重系数<i>α</i>通过交叉验证设置为1。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87"><b>2.3 经典SSD在目标检测中的不足</b></h4>
                <div class="p1">
                    <p id="88">SSD对不同的特征图取不同尺寸的先验框, 通过回归得出目标类别的置信度和先验框与ground truth间的偏差。SSD选取IOU (Intersection Over Union) 大于0.5的先验框作为正样本, 小于0.5的先验框作为负样本, 由于大尺寸物体覆盖IOU大于0.5的先验框多, 此时正负样本数目均衡;而小尺度目标的正样本数较少, 导致正负样本失衡难以训练。采用Focal Loss可以有效地解决上述问题<citation id="153" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="89">随着CNN层数的增加, 特征图的维度越来越小, 特征越来越抽象, 语义特征越来越明显, 而位置信息越来越模糊<citation id="154" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。假设采用SSD_300×300的模型, 即处理的图像分辨率为300 pixel×300 pixel, 其特征层为conv4_3、conv7、conv8_2、conv9_2、conv10_2、conv11_2。小目标的检测, 主要依赖于conv4_3的浅层特征, 它具备38 pixel×38 pixel的高分辨率, 包含的先验框尺寸也与目标尺寸较接近, 但其特征表达能力仅来源于前10层的卷积层, 不能捕捉深层次的语义信息;另一方面, 随着卷积层数的增加, 输入图像的感受野也迅速上升, 卷积层感受野的计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mrow><mtext>R</mtext><mtext>F</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">[</mo><mi>S</mi><msub><mrow></mrow><mrow><mtext>R</mtext><mtext>F</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn><mo stretchy="false">]</mo><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>Κ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">式中:<i>S</i><sub>RF</sub> (<i>i</i>) 为第<i>i</i>个卷积层的感受野大小;<i>S</i><sub><i>i</i></sub>为累乘后的卷积步长;<i>K</i><sub><i>i</i></sub>为卷积核的尺寸大小。</p>
                </div>
                <div class="p1">
                    <p id="92">通过计算, 得到各卷积层的感受野大小及各特征层默认框映射的图像区域如表1所示。可以发现从特征层conv9_2往后, 卷积感受野的大小就超过了原始图像的尺寸, 即每个特征点均由整个图像作为输入产生响应, 由此降低了精确定位目标的性能;另外, 特征层的默认框在图像上的映射区域, 在conv9_2中就已经超过了输入图像的一半, 当该区域包含多个目标时, 无法实现有效地区分。</p>
                </div>
                <div class="area_img" id="93">
                    <p class="img_tit">表1 SSD_300×300卷积感受野、默认框映射图像区域 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Convolution receptive field and mapping image region of default boxes of SSD_300×300</p>
                    <p class="img_note"></p>
                    <table id="93" border="1"><tr><td><br />Convolutional layer</td><td>Convolutional receptive <br />field / (pixel×pixel) </td><td>Output scale of feature <br />layer / (pixel×pixel) </td><td>Default boxes ratio</td><td>Mapping region scale /<br /> (pixel×pixel) </td></tr><tr><td><br />conv4_3</td><td>92×92</td><td>38×38</td><td>0.10</td><td>30×30</td></tr><tr><td><br />conv7</td><td>276×276</td><td>19×19</td><td>0.20</td><td>60×60</td></tr><tr><td><br />conv8_2</td><td>340×340</td><td>10×10</td><td>0.37</td><td>111×111</td></tr><tr><td><br />conv9_2</td><td>468×468</td><td>5×5</td><td>0.54</td><td>162×162</td></tr><tr><td><br />conv10_2</td><td>724×724</td><td>3×3</td><td>0.71</td><td>213×213</td></tr><tr><td><br />conv11_2</td><td>980×980</td><td>1×1</td><td>0.88</td><td>264×264</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="94" name="94" class="anchor-tag">3 增强的SSD模型</h3>
                <div class="p1">
                    <p id="95">为了提升SSD对小目标的检测能力, 应尽量使用较高分辨率的特征图, 并进一步挖掘浅层特征的表达能力, 同时将浅层的细节特征和高层的语义特征结合起来, 有利于为小目标提供精确的定位信息和目标类别信息。本文提出一种基于语义分割的浅层特征增强方法, 以及双向的特征层融合机制, 用于改进原始SSD的检测性能。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96"><b>3.1 特征层双向融合</b></h4>
                <div class="p1">
                    <p id="97">原始SSD在多层特征图上进行目标预测, 虽然能够起到类似于图像金字塔的作用, 但每次只利用了一层的特征图, 浅层特征缺乏类别识别的语义信息, 而深层特征随着感受野的增大, 分辨率降低, 不利于精确定位。为此, 在SSD的主干网络的基础上, 加入特征融合以提升检测性能。</p>
                </div>
                <div class="p1">
                    <p id="98">在不同特征层合并的过程中, 需要保持分辨率的一致。池化层结构简单, 高分辨率特征层通过池化后, 便获得降采样的特征;转置卷积与上采样的作用相同, 但卷积核参数可在训练过程中调整, 使上采样参数更加合理, 对低分辨率特征进行转置卷积后, 完成上采样。将统一分辨率的多层特征按通道连接, 便获得融合后的多尺度特征图。</p>
                </div>
                <div class="p1">
                    <p id="99">图2为多种特征融合方法示意图。当单独采用池化或者转置卷积操作用于特征层连接时, 特征信息的传递只能从左往右或从右往左传播, 方向都是单一的, 无法利用其他方向上的信息<citation id="155" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。本文采用双向融合的方法, 在多个尺度上获得了包含基本模式信息以及高级语义信息的特征图。结合SSD网络结构的实际特征图分辨率情况, 采用的max-pooling窗口为2×2, 步长为2;转置卷积的卷积核为3×3, 步长为2。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100"><b>3.2 浅层特征增强分支</b></h4>
                <div class="p1">
                    <p id="101">为了进一步提升SSD模型预测小目标的效果, 将75 pixel×75 pixel高分辨率特征图添加进检测层, 由于其拥有较少的语义信息, 本文模型加入语义分割支路, 用以增强浅层特征图的语义信息。分支结构如图3所示, 模块的输入是SSD主干网络的conv3_3特征层, 输出返回到主干网络替换原来的特征层, 该过程通过默认框级别的ground-truth进行监督学习, 不需要额外的标注数据<citation id="156" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="102">具体地, 模块的输入<i>X</i>为conv3_3特征层, 记为<i>X</i>∈<b>R</b><sup><i>C</i>×<i>H</i>×<i>W</i></sup>, 其中<i>C</i>为输入特征图的通道数, <i>H</i>和<i>W</i>为输入特征图的高和宽, 语义分割采用边界框级的标记ground-truth, 记为<i>G</i>∈{0, 1, 2, …, <i>N</i>}<sup><i>H</i>×<i>W</i></sup>, <i>N</i>为类别数。语义分割网络采用4个空洞卷积层, 得到中间结果<i>g</i> (<i>X</i>) , 之后用于产生两条分支结果。右侧分割支路使用1×1卷积之后, 经过softmax层, 将以上非线性变换记为<i>F</i>, 那么得到预测的语义分割结果为<i>Y</i>=<i>F</i>[<i>g</i> (<i>X</i>) ], 其中, <i>Y</i>∈<b>R</b><sup> (<i>N</i>+1) ×<i>H</i>×<i>W</i></sup>满足:<mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Y</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>, </mo><mn>1</mn><mo stretchy="false">]</mo><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>Ν</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo>×</mo><mi>Η</mi><mo>×</mo><mi>W</mi></mrow></msup><mo>, </mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><msup><mi>c</mi><mo>′</mo></msup><mo>=</mo><mn>0</mn></mrow><mi>Ν</mi></munderover><mi>Y</mi></mstyle><msub><mrow></mrow><mrow><msup><mi>c</mi><mo>′</mo></msup><mo>, </mo><mi>h</mi><mo>, </mo><mi>w</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow></mrow></math></mathml>, 其中<i>c</i>′为特征图的通道数量, <i>h</i>和<i>w</i>为语义分割结果图中每个点的横纵坐标。<i>g</i> (<i>X</i>) 的另一个分支用于生成带有语义信息的掩模, 使用1×1卷积, 将该过程表示为<i>H</i>, 进而获得<i>Z</i>=<i>H</i>[<i>g</i> (<i>X</i>) ]∈<b>R</b><sup><i>C</i>×<i>H</i>×<i>W</i></sup>, <i>Z</i>的尺寸和通道数与<i>X</i>相同, 通过按元素相乘, 得到语义信息丰富的<i>X</i>′, <i>X</i>′=<i>X</i>⊙<i>Z</i>, ⊙代表每个像素点的点乘运算。接下来用输出的<i>X</i>′替换原来的<i>X</i>, 用于后续预测。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906028_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 多种特征融合方法示意图" src="Detail/GetImg?filename=images/GXXB201906028_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 多种特征融合方法示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906028_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Schematics of multiple feature fusion methods</p>
                                <p class="img_note"> (a) 池化; (b) 转置卷积; (c) 双向融合</p>
                                <p class="img_note"> (a) Pooling; (b) transposed deconvolution; (c) bi-direction fusion</p>

                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906028_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 语义分割分支结构图" src="Detail/GetImg?filename=images/GXXB201906028_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 语义分割分支结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906028_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Diagram of semantic segmentation branch</p>

                </div>
                <div class="p1">
                    <p id="106">采用联合训练的策略, 在目标检测损失函数<i>L</i><sub>det</sub> (<i>I</i>, <i>B</i>) 上, 加入语义分割任务的交叉熵损失函数:</p>
                </div>
                <div class="p1">
                    <p id="107" class="code-formula">
                        <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>Ι</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mrow><mi>Η</mi><mi>W</mi></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>h</mi><mo>, </mo><mi>w</mi></mrow></munder><mtext>l</mtext></mstyle><mtext>n</mtext><mo stretchy="false">[</mo><mi>Y</mi><msub><mrow></mrow><mrow><mi>h</mi><mo>, </mo><mi>w</mi></mrow></msub><mo stretchy="false"> (</mo><mi>G</mi><msub><mrow></mrow><mrow><mi>h</mi><mo>, </mo><mi>w</mi></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="108">式中:<i>I</i>为输入处理的图像。最终的损失函数定义如下:</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false"> (</mo><mi>Ι</mi><mo>, </mo><mi>B</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mi>L</mi><msub><mrow></mrow><mrow><mi>det</mi></mrow></msub><mo stretchy="false"> (</mo><mi>Ι</mi><mo>, </mo><mi>B</mi><mo stretchy="false">) </mo><mo>+</mo><mi>β</mi><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>Ι</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">式中:<i>B</i>为边界框的ground-truth;<i>β</i>为两个任务的平衡系数, 本文模型取0.1。</p>
                </div>
                <h3 id="111" name="111" class="anchor-tag">4 实验与分析</h3>
                <div class="p1">
                    <p id="112">在实际的物体检测中, 分类性能体现在预测框的置信度高低上, 定位的准确性由预测框的坐标偏差衡量<citation id="157" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>。本文提出的改进模型, 着重于改善小目标的检测, 为验证本文算法的有效性, 将其与原始SSD方法在具有代表性的小目标数据集上的mAP进行对比, 并将置信度阈值统一设为0.5。所采用的实验平台操作系统为Ubuntu 16.04LTS, 深度学习软件框架为TensorFlow-GPU, CUDA-9.0, Cudnn-7.3.1, 主要硬件配置如下:NVIDIA GTX1080Ti×2 GPU, Intel i7-8700k CPU, DDR4 32G Memory。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113"><b>4.1 PASCAL-VOC2007小目标数据集测试结果</b></h4>
                <div class="p1">
                    <p id="114">为了对比本文提出的增强SSD与原始SSD模型的性能, 挑选了VOC2007数据集中137张具有代表性的小目标图片, 涉及的物体类别有8种, 包括飞机 (aero plane) 、鸟 (bird) 、船 (boat) 、瓶子 (bottle) 、小汽车 (car) 、狗 (dog) 、羊 (sheep) 、人, 经过相应处理之后, 标注物体的ground truth共计1164个。分别采用原始SSD算法和本文方法进行目标检测实验, 部分场景下的检测结果如图4所示, 其中边界框左上角的数字分别代表类别标签和相应的置信概率。</p>
                </div>
                <div class="p1">
                    <p id="115">从图4中可见本文方法对检测效果的改进。图4 (a) 为原始SSD_300×300模型的检测效果, 尽管其对近处物体取得较高的类别置信度, 但是定位精确性有待进一步提高;图4 (b) 为本文方法的检测效果, 可以看出不仅大目标的定位更加精确, 而且小目标的检测能力显著提升。表2为测试各类目标的结果, 可见小尺寸物体 (如:bottle) 类别的平均精度 (AP) 提升最大, mAP相比提升了7.1%。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906028_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 原始SSD与改进SSD检测小目标的结果对比" src="Detail/GetImg?filename=images/GXXB201906028_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 原始SSD与改进SSD检测小目标的结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906028_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Comparison of detection results of small targets obtained by original SSD and improved SSD</p>
                                <p class="img_note"> (a) 原始SSD; (b) 改进模型</p>
                                <p class="img_note"> (a) Original SSD; (b) improved model</p>

                </div>
                <div class="area_img" id="117">
                    <p class="img_tit">表2 VOC2007数据集小目标检测结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Small object detection results of VOC2007 dataset</p>
                    <p class="img_note"></p>
                    <table id="117" border="1"><tr><td rowspan="2"><br />Method</td><td rowspan="2">mAP</td><td colspan="8"><br />Detection result</td></tr><tr><td>Aero plane</td><td>Bird</td><td>Boat</td><td>Bottle</td><td>Car</td><td>Dog</td><td>Sheep</td><td>Person</td></tr><tr><td>SSD_300×300</td><td>0.537</td><td>0.601</td><td>0.525</td><td>0.426</td><td>0.374</td><td>0.720</td><td>0.556</td><td>0.538</td><td>0.563</td></tr><tr><td><br />Proposed method</td><td>0.608</td><td>0.685</td><td>0.570</td><td>0.534</td><td>0.503</td><td>0.748</td><td>0.597</td><td>0.652</td><td>0.591</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="118" name="118"><b>4.2 空中红外目标数据集测试结果</b></h4>
                <div class="p1">
                    <p id="119">在空中红外目标识别中, 特别是地基红外光电跟踪系统中, 目标往往只占据少量的有效像素, 另外云朵遮挡、大幅度的姿态变化等因素, 也增加了目标跟踪识别的难度。自建的空中红外目标数据集筛选自外场实验视频流, 分辨率为320 pixel×256 pixel和640 pixel×512 pixel的图像, 包含了J型战斗机 (fighter_J) 、直升机 (helicopter) 、S型战斗机 (fighter_S) 、民航客机 (airliner) 、飞鸟 (bird) 共5类目标, 并以海天线、城市天际线、云朵背景为主。各类目标数量均衡, 约为550张图, 全部采用多目标标记, 随机选取总数的30%作为测试集, 剩余的又划分为训练集和验证集。</p>
                </div>
                <div class="p1">
                    <p id="120">采用VGG_16基础网络的卷积层参数, 并在红外数据集上进行微调, 部分场景检测结果如图5所示, 可见, 该方法对于大尺寸物体的定位偏差小、类别置信度普遍较高;在某些复杂场景下, 如释放干扰弹时, 仍能捕获目标本身的红外辐射特征, 进行正确的识别。将该方法与原始SSD、YOLOv3进行对比, 各类别的检测结果见表3, 最终mAP获得8.7%的提升。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906028_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 空中红外目标检测结果" src="Detail/GetImg?filename=images/GXXB201906028_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 空中红外目标检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906028_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Detection results of infrared aerial targets</p>

                </div>
                <div class="area_img" id="122">
                    <p class="img_tit">表3 红外数据集空中目标检测结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Aerial target detection results of infrared dataset</p>
                    <p class="img_note"></p>
                    <table id="122" border="1"><tr><td rowspan="2"><br />Method</td><td rowspan="2">mAP</td><td colspan="5"><br />Detection result</td></tr><tr><td><br />Fighter_J</td><td>Helicopter</td><td>Fighter_S</td><td>Airliner</td><td>Bird</td></tr><tr><td>SSD_300×300</td><td>0.618</td><td>0.659</td><td>0.615</td><td>0.266</td><td>0.819</td><td>0.732</td></tr><tr><td><br />YOLOv3-320</td><td>0.641</td><td>0.730</td><td>0.548</td><td>0.314</td><td>0.867</td><td>0.747</td></tr><tr><td><br />Proposed method</td><td>0.705</td><td>0.784</td><td>0.636</td><td>0.485</td><td>0.822</td><td>0.796</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="123">为了进一步验证本文模型的有效性, 利用召回率-准确率曲线对最终检测结果进行定量评价。准确率衡量被模型提取的目标中真实标记的比例, 召回率反映了被正确提取的标记占总目标标记的比重<citation id="158" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>。图6为原始SSD模型、YOLOv3、本文增强SSD算法在空中红外目标数据集上的召回率与准确率的关系对比, 其中召回率和准确率都是多个类别的平均数据。可见, 相较于前面两种方法, 增强SSD算法的召回率-准确率曲线所围成的面积更大, 反映到数据上就是AP整体提升明显。</p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201906028_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 空中红外目标召回率-准确率曲线对比" src="Detail/GetImg?filename=images/GXXB201906028_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 空中红外目标召回率-准确率曲线对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201906028_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Comparison of recall-precision curve of infrared aerial targets</p>

                </div>
                <h4 class="anchor-tag" id="125" name="125"><b>4.3 增强SSD的实时性分析</b></h4>
                <div class="p1">
                    <p id="126">浅层特征的语义增强分支将更高分辨率的特征层引入检测中, 不仅在语义增强过程中增加了卷积运算量, 还扩大了预测框的总数, 必然会导致预测速度的降低。但是, 相对于简单的加深基础网络中特征层通道的方法, 浅层特征的语义增强分支具有更少的网络参数。</p>
                </div>
                <div class="p1">
                    <p id="127">双向特征融合采用最大池化和转置卷积, 将高低特征层归一化到相同尺寸, 再通过串联获得良好的表征能力, 并且没有带来太多的计算开销。通过拼接后的特征金字塔, 每一层都有3072个通道 (串联256, 512, 1024, 512, 256, 256, 256的通道) , 每个特征层具有相同通道数, 为不同层之间共享分类网络权重提供了可能。进一步地, 修改原始SSD模型中各分类网络中预测框数目, 使得每个特征层统一, 实现权重的共享。</p>
                </div>
                <div class="p1">
                    <p id="128">对原始SSD和增强SSD方法的速度进行定量评估, 为了测试的公平性, 统一将batchsize设置为8, 同时去除了批归一化层, 以缩短预测时间和减少内存消耗。表4为预测框总数和运行速度的对比, 其中FPS代表算法运行的帧率, 可见浅层预测框带来较多的时间消耗, 代码实现还存在效率提升的空间。</p>
                </div>
                <div class="area_img" id="129">
                    <p class="img_tit">表4 每个分类网络的预测框数目及运行速度对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Number of predictive boxes for each classiﬁed network and speed comparison</p>
                    <p class="img_note"></p>
                    <table id="129" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="7"><br />Number of boxes</td><td rowspan="2">Total boxes</td><td rowspan="2">FPS</td></tr><tr><td>75×75</td><td>38×38</td><td>19×19</td><td>10×10</td><td>5×5</td><td>3×3</td><td>1×1</td></tr><tr><td>Original SSD</td><td>0</td><td>4</td><td>6</td><td>6</td><td>6</td><td>4</td><td>4</td><td>8732</td><td>25.2</td></tr><tr><td><br />Modified SSD</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td><td>30260</td><td>9.4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="130">综上所述, 增强后的SSD较原始SSD方法对小目标检测效果提升明显, 在VOC2007的小目标数据集和空中红外数据集中, mAP分别增加了7.1%和8.7%, 在检测速率上FPS达到了9.4 frames·s<sup>-1</sup>, 验证了本文算法能够实现精度和实时性的平衡。分析其主要原因, 在于引入语义增强后的conv3_3层用于检测, 同时增加了预测框的数量, 增强了小目标的检测能力;另外, 双向特征融合机制较好地结合了语义信息与定位信息, 从整体上提升了物体检测性能。</p>
                </div>
                <h3 id="131" name="131" class="anchor-tag">5 结  论</h3>
                <div class="p1">
                    <p id="132">首先阐述了经典SSD深度网络检测框架的结构和机理, 并通过进一步分析卷积核感受野与默认特征框在输入图像上的映射关系, 指出小目标检测能力不足的原因。在原始SSD模型基础上, 引入浅层特征图语义增强分支, 并提出了一种双向的特征融合机制, 在VOC2007的小目标数据集和空中红外数据集上进行测试, 得到的mAP分别提高了7.1%和8.7%, 获得了较好的检测效果。未来的工作将围绕模型简化、参数压缩展开, 以改善该模型的实时性能, 并针对特定应用场景寻求更加合理的特征利用方式。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalable Object Detection Using Deep Neural Networks">

                                <b>[1]</b> Erhan D, Szegedy C, Toshev A, <i>et al</i>.Scalable object detection using deep neural networks[C]//2014 IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2014, Columbus, OH, USA.New York:IEEE, 2014:2155-2162.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Salient Object Detection:A Benchmark">

                                <b>[2]</b> Borji A, Cheng M M, Jiang H Z, <i>et al</i>.Salient object detection:a benchmark[J].IEEE Transactions on Image Processing, 2015, 24 (12) :5706-5722.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Motion detection based on frame difference method">

                                <b>[3]</b> Singla N.Motion detection based on frame difference method[J].International Journal of Information &amp; Computation Technology, 2014, 4 (15) :1559-1565.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Determining optical flow">

                                <b>[4]</b> Horn B K P, Schunck B G.Determining optical flow[J].Artificial Intelligence, 1981, 17 (1/2/3) :185-203.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On Detection of Multiple Object Instances Using Hough Transforms">

                                <b>[5]</b> Barinova O, Lempitsky V, Kholi P.On detection of multiple object instances using hough transforms[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (9) :1773-1784.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjAzNThPNEh0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RlNqbFVMM0pJbDQ9Tmo3QmFy&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">

                                <b>[7]</b> Dalal N, Triggs B.Histograms of oriented gradients for human detection[C]//2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR′05) , June 20-25, 2005, San Diego, CA, USA.New York:IEEE, 2005:886-893.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002157166&amp;v=MTY2NzNYcVJyeG94Y01IN1I3cWVidWR0RlNqbFVMM0pJbDQ9Tmo3QmFyTzRIdEhPcm9wQ1plMEpZM2s1ekJkaDRqOTlT&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Burges C J C.A tutorial on support vector machines for pattern recognition[J].Data Mining and Knowledge Discovery, 1998, 2 (2) :121-167.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJQI&amp;filename=SJQIF3BBEE8CB3D8F8F091DF44E2969FFE8E&amp;v=MDUxNzlhNjBzTFRIdVhyaHN6Y01UaU1MTHFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh4N3Erd3FnPU5pZmFaOFc3YktPNTJvYzJGdWg3QkFveHVSWQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Hastie T, Rosset S, Zhu J, <i>et al</i>.Multi-class AdaBoost[J].Statistics and Its Interface, 2009, 2 (3) :349-360.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 Ren S Q, He K M, Girshick R, <i>et al</i>.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLOv3 an incremental improvement">

                                <b>[11]</b> Redmon J, Farhadi A.YOLOv3:an incremental improvement[EB/OL]. (2018-04-08) [2018-12-01].https://arxiv.org/abs/1804.02767.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                 Liu W, Anguelov D, Erhan D, <i>et al</i>.SSD:single shot multibox dtector[M]//Leibe B, Matas J, Sebe N, <i>et al</i>.Computer Vision:ECCV 2016.Cham:Springer, 2016, 9905:21-37.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic seg-mentation">

                                <b>[13]</b> Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mask r-cnn">

                                <b>[14]</b> He K M, Gkioxari G, Dollár P, <i>et al</i>.Mask R-CNN[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018:2844175.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FSSD:feature fusion single shot multibox detector">

                                <b>[15]</b> Li Z X, Zhou F Q.FSSD:feature fusion single shot multibox detector[EB/OL]. (2018-05-17) [2018-12-01].https://arxiv.org/abs/1712.00960.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature-fused SSD:fast detection for small objects">

                                <b>[16]</b> Cao G M, Xie X M, Yang W Z, <i>et al</i>.Feature-fused SSD:fast detection for small objects[J].Proceedings of SPIE, 2018, 10615:106151E.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dssd:Deconvolutional single shot detector">

                                <b>[17]</b> Fu C Y, Liu W, Ranga A, <i>et al</i>.DSSD:deconvolutional single shot detector[EB/OL]. (2017-01-23) [2018-12-01].https://arxiv.org/abs/1701.06659.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enhancement of SSD by concatenating feature maps for object detection">

                                <b>[18]</b> Jeong J, Park H, Kwak N.Enhancement of SSD by concatenating feature maps for object detection[EB/OL]. (2017-05-26) [2018-12-01].https://arxiv.org/abs/1705.09587.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HWYJ201801042&amp;v=MTE5MjllVnVGeXZuVXIzSUxUclNaTEc0SDluTXJvOUJab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> Tang C, Ling Y S, Zheng K D, <i>et al</i>.Object detection method of multi-view SSD based on deep learning[J].Infrared and Laser Engineering, 2018, 47 (1) :126003.唐聪, 凌永顺, 郑科栋, 等.基于深度学习的多视窗SSD目标检测方法[J].红外与激光工程, 2018, 47 (1) :126003.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[20]</b> Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2018-12-01].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[21]</b> He K M, Zhang X Y, Ren S Q, <i>et al</i>.Deep residual learning for image recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 27-30, 2016, Las Vegas, NV, USA.New York:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Focal Loss for Dense Object Detection">

                                <b>[22]</b> Lin T Y, Goyal P, Girshick R, <i>et al</i>.Focal loss for dense object detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017:2999-3007.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201803036&amp;v=MTY5MzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeXZuVXIzSUlqWFRiTEc0SDluTXJJOUdZb1E=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> Xin P, Xu Y L, Tang H, <i>et al</i>.Fast airplane detection based on multi-layer feature fusion of fully convolutional networks[J].Acta Optica Sinica, 2018, 38 (3) :0315003.辛鹏, 许悦雷, 唐红, 等.全卷积网络多层特征融合的飞机快速检测[J].光学学报, 2018, 38 (3) :0315003.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-shot object detection with enriched semantics">

                                <b>[24]</b> Zhang Z S, Qiao S Y, Xie C H, <i>et al</i>.Single-shot object detection with enriched semantics[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 18-23, 2018, Salt Lake City, UT, USA.New York:IEEE, 2018:5813-5821.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806034&amp;v=MTgwMTVHWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnl2blVyM0lJalhUYkxHNEg5bk1xWTk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> Feng X Y, Mei W, Hu D S.Aerialtarget detection based on improved faster R-CNN[J].Acta Optica Sinica, 2018, 38 (6) :0615004.冯小雨, 梅卫, 胡大帅.基于改进Faster R-CNN的空中目标检测[J].光学学报, 2018, 38 (6) :0615004.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201807020&amp;v=MDIxMDRiTEc0SDluTXFJOUhaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeXZuVXIzSUlqWFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> Wang W X, Fu Y T, Dong F, <i>et al</i>.Infrared ship target detection method based on deep convolution neural network[J].Acta Optica Sinica, 2018, 38 (7) :0712006.王文秀, 傅雨田, 董峰, 等.基于深度卷积神经网络的红外船只目标检测方法[J].光学学报, 2018, 38 (7) :0712006.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201906028" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201906028&amp;v=MDM4ODNVUkxPZVplVnVGeXZuVXIzSklqWFRiTEc0SDlqTXFZOUhiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV4ZEdibEtXKzVXTDBsc0dtdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

