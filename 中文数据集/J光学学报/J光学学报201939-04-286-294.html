

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134122492315000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201904034%26RESULT%3d1%26SIGN%3dsFJX8Ah2wLSoXI000E0mswhlqNw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201904034&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201904034&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201904034&amp;v=MDcxNzMzenFxQnRHRnJDVVJMT2VaZVZ1RnlIa1ZyN09JalhUYkxHNEg5ak1xNDlHWUlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#54" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="2 所提算法 ">2 所提算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#67" data-title="&lt;b&gt;2.1 加权相关滤波&lt;/b&gt;"><b>2.1 加权相关滤波</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;2.2 其他改进&lt;/b&gt;"><b>2.2 其他改进</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#101" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#104" data-title="&lt;b&gt;3.1 算法改进实验&lt;/b&gt;"><b>3.1 算法改进实验</b></a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;3.2 与其他优秀算法的对比实验&lt;/b&gt;"><b>3.2 与其他优秀算法的对比实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#124" data-title="4 结 论 ">4 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="图1 算法总体框架图">图1 算法总体框架图</a></li>
                                                <li><a href="#108" data-title="表1 不同特征权重的跟踪结果">表1 不同特征权重的跟踪结果</a></li>
                                                <li><a href="#111" data-title="表2 三种上下文特征的跟踪结果">表2 三种上下文特征的跟踪结果</a></li>
                                                <li><a href="#114" data-title="图2 不同搜索区域尺度的精确度曲线图。 (a) OTB-100; (b) Temple-color-128">图2 不同搜索区域尺度的精确度曲线图。 (a) OTB-100; (b) Temple-color-......</a></li>
                                                <li><a href="#117" data-title="表3 依次加入各项策略的跟踪结果">表3 依次加入各项策略的跟踪结果</a></li>
                                                <li><a href="#121" data-title="表4 OTB-100中9种算法的对比结果">表4 OTB-100中9种算法的对比结果</a></li>
                                                <li><a href="#122" data-title="图3 9种算法在OTB-100中的精度图和成功率曲线图。 (a) 精确度曲线; (b) 成功率曲线">图3 9种算法在OTB-100中的精度图和成功率曲线图。 (a) 精确度曲线; (b) 成功率曲线</a></li>
                                                <li><a href="#126" data-title="图4 典型视频跟踪结果对比。 (a) Human3; (b) girl2; (c) soccer; (d) lemming">图4 典型视频跟踪结果对比。 (a) Human3; (b) girl2; (c) soccer; ......</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="12">


                                    <a id="bibliography_1" title=" Bolme D S, Beveridge J R, Draper B A, &lt;i&gt;et al&lt;/i&gt;.Visual object tracking using adaptive correlation filters[C].IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010:2544-2550." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">
                                        <b>[1]</b>
                                         Bolme D S, Beveridge J R, Draper B A, &lt;i&gt;et al&lt;/i&gt;.Visual object tracking using adaptive correlation filters[C].IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010:2544-2550.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_2" title=" Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;.Exploiting the circulant structure of tracking-by-detection with kernels[C].European Conference on Computer Vision, 2012:702-715." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">
                                        <b>[2]</b>
                                         Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;.Exploiting the circulant structure of tracking-by-detection with kernels[C].European Conference on Computer Vision, 2012:702-715.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_3" >
                                        <b>[3]</b>
                                     Henriques J F, Caseiro R, Martins P, &lt;i&gt;et al&lt;/i&gt;.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596.</a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_4" title=" Danelljan M, Khan F S, Felsberg M, &lt;i&gt;et al&lt;/i&gt;.Adaptive color attributes for real-time visual tracking[C].IEEE Conference on Computer Vision and Pattern Recognition, 2014:1090-1097." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive color attributes for Real-Time visual tracking">
                                        <b>[4]</b>
                                         Danelljan M, Khan F S, Felsberg M, &lt;i&gt;et al&lt;/i&gt;.Adaptive color attributes for real-time visual tracking[C].IEEE Conference on Computer Vision and Pattern Recognition, 2014:1090-1097.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_5" title=" Bertinetto L, Valmadre J, Golodetz S, &lt;i&gt;et al&lt;/i&gt;.Staple:complementary learners for real-time tracking[C].IEEE Conference on Computer Vision and Pattern Recognition, 2016:1401-1409." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">
                                        <b>[5]</b>
                                         Bertinetto L, Valmadre J, Golodetz S, &lt;i&gt;et al&lt;/i&gt;.Staple:complementary learners for real-time tracking[C].IEEE Conference on Computer Vision and Pattern Recognition, 2016:1401-1409.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                     Ma C, Huang J B, Yang X K, &lt;i&gt;et al&lt;/i&gt;.Hierarchical convolutional features for visual tracking[C].IEEE International Conference on Computer Vision, 2015:3074-3082.</a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_7" title=" Ma C, Huang J B, Yang X K, &lt;i&gt;et al&lt;/i&gt;.Robust visual tracking via hierarchical convolutional features[EB/OL]. (2018-08-11) [2018-10-25].https:∥arxiv.org/abs/1707.03816." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust visual tracking via hierarchical convolutional features[OL]">
                                        <b>[7]</b>
                                         Ma C, Huang J B, Yang X K, &lt;i&gt;et al&lt;/i&gt;.Robust visual tracking via hierarchical convolutional features[EB/OL]. (2018-08-11) [2018-10-25].https:∥arxiv.org/abs/1707.03816.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_8" title=" Li S S, Zhao G P, Wang J N.Distractor-aware object tracking based on multi-feature fusion and scale-adaption[J].Acta Optica Sinica, 2017, 37 (5) :0515005.李双双, 赵高鹏, 王建宇.基于特征融合和尺度自适应的干扰感知目标跟踪[J].光学学报, 2017, 37 (5) :0515005." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201705025&amp;v=MzAzMTFNcW85SFlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtWcjdPSWpYVGJMRzRIOWI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Li S S, Zhao G P, Wang J N.Distractor-aware object tracking based on multi-feature fusion and scale-adaption[J].Acta Optica Sinica, 2017, 37 (5) :0515005.李双双, 赵高鹏, 王建宇.基于特征融合和尺度自适应的干扰感知目标跟踪[J].光学学报, 2017, 37 (5) :0515005.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_9" title=" He Z Q, Fan Y R, Zhuang J F, &lt;i&gt;et al&lt;/i&gt;.Correlation filters with weighted convolution responses[C].IEEE International Conference on Computer Vision Workshops, 2017:1992-2000." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Correlation filters with weighted convolution responses">
                                        <b>[9]</b>
                                         He Z Q, Fan Y R, Zhuang J F, &lt;i&gt;et al&lt;/i&gt;.Correlation filters with weighted convolution responses[C].IEEE International Conference on Computer Vision Workshops, 2017:1992-2000.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_10" title=" Xiong C Z, Zhao L L, Guo F H.Kernelized correlation filters tracking based on adaptive feature fusion[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics, 2017, 29 (6) :1068-1074.熊昌镇, 赵璐璐, 郭芬红.自适应特征融合的核相关滤波跟踪算法[J].计算机辅助设计与图形学学报, 2017, 29 (6) :1068-1074." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201706012&amp;v=MjgwNDdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIa1ZyN09MejdCYUxHNEg5Yk1xWTlFWm9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Xiong C Z, Zhao L L, Guo F H.Kernelized correlation filters tracking based on adaptive feature fusion[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics, 2017, 29 (6) :1068-1074.熊昌镇, 赵璐璐, 郭芬红.自适应特征融合的核相关滤波跟踪算法[J].计算机辅助设计与图形学学报, 2017, 29 (6) :1068-1074.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_11" title=" Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Learning spatially regularized correlation filters for visual tracking[C].IEEE International Conference on Computer Vision, 2015:4310-4318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">
                                        <b>[11]</b>
                                         Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Learning spatially regularized correlation filters for visual tracking[C].IEEE International Conference on Computer Vision, 2015:4310-4318.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_12" title=" Galoogahi H K, Fagg A, Lucey S.Learning background-aware correlation filters for visual tracking[C].IEEE International Conference on Computer Vision, 2017:1144-1152." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Background-Aware Correlation Filters for Visual Tracking">
                                        <b>[12]</b>
                                         Galoogahi H K, Fagg A, Lucey S.Learning background-aware correlation filters for visual tracking[C].IEEE International Conference on Computer Vision, 2017:1144-1152.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_13" title=" Wang M M, Liu Y, Huang Z Y.Large margin object tracking with circulant feature maps[C].IEEE Conference on Computer Vision and Pattern Recognition, 2017:4800-4808." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large Margin Object Tracking with Circulant Feature Maps">
                                        <b>[13]</b>
                                         Wang M M, Liu Y, Huang Z Y.Large margin object tracking with circulant feature maps[C].IEEE Conference on Computer Vision and Pattern Recognition, 2017:4800-4808.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_14" title=" Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Discriminative scale space tracking[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (8) :1561-1575." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative scale space tracking">
                                        <b>[14]</b>
                                         Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Discriminative scale space tracking[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (8) :1561-1575.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_15" title=" Li Y, Zhu J K.A scale adaptive kernel correlation filter tracker with feature integration[C].European Conference on Computer Vision, 2014:254-265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A scale adaptive kernel correlation filter tracker with feature integration">
                                        <b>[15]</b>
                                         Li Y, Zhu J K.A scale adaptive kernel correlation filter tracker with feature integration[C].European Conference on Computer Vision, 2014:254-265.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_16" >
                                        <b>[16]</b>
                                     Danelljan M, Bhat G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.ECO:efficient convolution operators for tracking[C].IEEE Conference on Computer Vision and Pattern Recognition, 2017:6931-6939.</a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_17" title=" Xiong C Z, Che M Q, Wang R L, &lt;i&gt;et al&lt;/i&gt;.Robust real-time visual tracking via dual model adaptive switching[J].Acta Optica Sinica, 2018, 38 (10) :1015002.熊昌镇, 车满强, 王润玲, 等.稳健的双模型自适应切换实时跟踪算法[J].光学学报, 2018, 38 (10) :1015002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201810031&amp;v=MDY4NzlHRnJDVVJMT2VaZVZ1RnlIa1ZyN09JalhUYkxHNEg5bk5yNDlHWllRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         Xiong C Z, Che M Q, Wang R L, &lt;i&gt;et al&lt;/i&gt;.Robust real-time visual tracking via dual model adaptive switching[J].Acta Optica Sinica, 2018, 38 (10) :1015002.熊昌镇, 车满强, 王润玲, 等.稳健的双模型自适应切换实时跟踪算法[J].光学学报, 2018, 38 (10) :1015002.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_18" title=" Wu Y, Lim J, Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1834-1848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">
                                        <b>[18]</b>
                                         Wu Y, Lim J, Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1834-1848.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_19" title=" Liang P P, Blasch E, Ling H B.Encoding color information for visual tracking:algorithms and benchmark[J].IEEE Transactions on Image Processing, 2015, 24 (12) :5630-5644." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Encoding color information for visual tracking:Algorithms and bechmark,&amp;quot;">
                                        <b>[19]</b>
                                         Liang P P, Blasch E, Ling H B.Encoding color information for visual tracking:algorithms and benchmark[J].IEEE Transactions on Image Processing, 2015, 24 (12) :5630-5644.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_20" title=" Lukežic A, Voj&#237;r T, Zajc L C, &lt;i&gt;et al&lt;/i&gt;.Discriminative correlation filter with channel and spatial reliability[C].IEEE Conference on Computer Vision and Pattern Recognition, 2017:4847-4856." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative correlation filter with channel and spatial reliability">
                                        <b>[20]</b>
                                         Lukežic A, Voj&#237;r T, Zajc L C, &lt;i&gt;et al&lt;/i&gt;.Discriminative correlation filter with channel and spatial reliability[C].IEEE Conference on Computer Vision and Pattern Recognition, 2017:4847-4856.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_21" >
                                        <b>[21]</b>
                                     Danelljan M, H&#228;ger G, Khan F S, &lt;i&gt;et al&lt;/i&gt;.Convolutional features for correlation filter based visual tracking[C].IEEE International Conference on Computer Vision Workshop, 2015:621-629.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-17 10:58</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(04),286-294 DOI:10.3788/AOS201939.0415004            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>融合上下文和重定位的加权相关滤波跟踪算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%86%8A%E6%98%8C%E9%95%87&amp;code=06228157&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">熊昌镇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8D%A2%E9%A2%9C&amp;code=39939900&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卢颜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%97%AB%E4%BD%B3%E5%BA%86&amp;code=36619616&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">闫佳庆</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E6%96%B9%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E5%9F%8E%E5%B8%82%E9%81%93%E8%B7%AF%E4%BA%A4%E9%80%9A%E6%99%BA%E8%83%BD%E6%8E%A7%E5%88%B6%E6%8A%80%E6%9C%AF%E5%8C%97%E4%BA%AC%E5%B8%82%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0226398&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北方工业大学城市道路交通智能控制技术北京市重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为提升融合梯度直方图特征和颜色属性特征的有效卷积操作跟踪算法 (ECO-HC) 的跟踪精度和速度, 提出一种融合上下文和重定位的加权相关滤波跟踪方法。根据梯度直方图和颜色属性的不同特性加权融合相关滤波响应值, 采用自适应迭代方法预测目标位置;融合多尺度搜索区域, 目标上下文特征和目标预测失败时重定位方法进一步提高跟踪精度。在标准数据集OTB-100上进行算法评估, 实验结果表明, 所提算法的平均距离精度为89.2%, 平均重叠率精度为80.6%, 比ECO-HC算法分别高3.6%和2.1%。中央处理器的跟踪速度达65.2 frame/s, 优于实验中对比的其他跟踪算法。所提算法有效地提高了跟踪精度, 在严重遮挡、光照变化等干扰下, 仍能较好地跟踪目标。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%85%B3%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相关滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A0%E6%9D%83%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">加权融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E9%80%82%E5%BA%94%E8%BF%AD%E4%BB%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自适应迭代;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%8A%E4%B8%8B%E6%96%87%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上下文特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%87%8D%E5%AE%9A%E4%BD%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重定位;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *熊昌镇, E-mail:xczkiong@163.com;;
                                </span>
                                <span>
                                    *卢颜, E-mail:1825650885@qq.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-25</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划 (2017YFC0821102);</span>
                                <span>北京市优秀人才培养资助 (2017000020124G287);</span>
                    </p>
            </div>
                    <h1><b>Weighted Correlation Filter Tracking Algorithm Based on Context and Relocation</b></h1>
                    <h2>
                    <span>Xiong Changzhen</span>
                    <span>Lu Yan</span>
                    <span>Yan Jiaqing</span>
            </h2>
                    <h2>
                    <span>Beijing Key Laboratory of Urban Intelligent Control Technologies, North China University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to improve both the tracking accuracy and speed of the efficient convolution operators based tracking algorithm fusing the histogram of oriented gradient and color names features (ECO-HC) , a weighted correlation filtering algorithm based on context and relocation is proposed. Considering the differences between the histogram of oriented gradient and color names features, the responses of two features are fused in different weights. The adaptive iterative method is used to predict the position of a target, which combines with the multi-scale search area, the contextual features and the relocation method when the target prediction is failure to further improve the tracking accuracy. The algorithm is evaluated on the OTB-100 dataset. The experimental results show that the average distance accuracy of the proposed algorithm is 89.2% and the average overlap rate is 80.6%, 3.6% and 2.1% higher than those of the ECO-HC method, respectively. In addition, the tracking speed on the central processing unit is 65.2 frame/s, superior to that of the other tracking algorithms compared in the experiments. The proposed algorithm effectively improves the tracking accuracy and can track the objects well under the condition of severe occlusion, illumination variation and other interferences.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=correlation%20filtering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">correlation filtering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=weighted%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">weighted fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adaptive%20iteration&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adaptive iteration;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=contextual%20features&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">contextual features;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=relocation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">relocation;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-25</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="54" name="54" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="55">随着计算机视觉的快速发展, 视频跟踪技术已渗透进监控、人机交互、医学诊断等多个领域, 并受到广泛重视。但是, 由于实际跟踪过程会遇到目标变形、外物遮挡、光照变化等多种干扰, 视频跟踪技术仍需深入研究以提高跟踪精度和速度。</p>
                </div>
                <div class="p1">
                    <p id="56">Blome等<citation id="129" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出的相关滤波跟踪方法突破传统跟踪方法的束缚, 提升了跟踪算法的速度, 成为目标跟踪发展的转折点。在此基础上研究者们提取灰度<citation id="130" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、方向梯度直方图 (HOG) <citation id="131" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、颜色属性 (CN) <citation id="132" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、颜色直方图<citation id="133" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等人为设计的特征、以及通过深度学习得到的卷积神经网络 (CNN) 特征<citation id="137" type="reference"><link href="22" rel="bibliography" /><link href="24" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>作为训练相关滤波跟踪器的显著特征, 有效地提升相关滤波跟踪算法的跟踪精度。为进一步提高跟踪的精度, 出现大量使用多特征融合的相关滤波跟踪方法, 如李双双等<citation id="134" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>融合三原色特征和改进的HOG特征得到干扰感知目标模型, He等<citation id="135" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>对卷积特征高层与低层设置不同权重并进行加权融合, 熊昌镇等<citation id="136" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>根据最大响应值计算特征权重自适应加权融合HOG和CN特征。</p>
                </div>
                <div class="p1">
                    <p id="57">相关滤波算法需要大量样本训练跟踪器, 但视觉跟踪只在第一帧指定真实的跟踪目标, 因此多数算法使用循环移位获取足够多的近似训练样本, 但该方法产生的边界效应一定程度上影响跟踪的性能。对此, Danelljan等<citation id="138" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>在分布式相关滤波器的基础上加入空间正则化惩罚项抑制边界效应的影响, 提高跟踪精度;Galoogahi等<citation id="139" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>通过提取真实的背景信息与目标特征共同训练背景感知滤波器以使跟踪器区分正负样本, 抑制边界效应的影响。视频跟踪遭遇严重遮挡、变形、光线变化等多重干扰时, 易出现目标定位失败, 所以需要及时检测目标跟丢的情况并对模型进行重新训练以重新定位到目标, Wang等<citation id="140" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>使用多峰目标检测提高跟踪定位精度, 并通过高置信度模型更新策略, 解决跟踪失败的问题, 提高算法的稳健性。视觉跟踪不仅需要目标定位准确, 还要求跟踪尺度尽可能接近实际目标尺度, 为提升尺度预测的准确性, Danelljan等<citation id="141" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出一种拥有平移滤波器和尺度滤波器的判别式尺度空间跟踪算法, 通过自适应尺度选择提高跟踪重叠率;Li等<citation id="142" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>运用尺度池化技术确定响应值最大的尺度预测, 该算法同样以设计尺度自适应为主提升跟踪性能。</p>
                </div>
                <div class="p1">
                    <p id="58">上述相关滤波跟踪算法在目标定位和尺度预测的准确度方面都得到一定提升, 但现有算法仍普遍存在着精度高而速度慢或者速度快而精度低的现象, 所以如何平衡跟踪精度与速度是此领域研究的重点之一。为解决上述问题, 本文选取目前跟踪性能比较稳定的有效卷积操作跟踪算法 (ECO-HC) <citation id="143" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>作为基础算法, 并做出以下几点改进:</p>
                </div>
                <div class="p1">
                    <p id="59">1) 加权融合。通过分析HOG与CN的各自特点及对跟踪性能的影响程度确定特征融合权重, 提高跟踪精确度。</p>
                </div>
                <div class="p1">
                    <p id="60">2) 自适应牛顿迭代法。ECO-HC算法使用固定牛顿迭代次数预测目标新位置, 而本文设计一种自适应选择迭代次数的牛顿迭代方法, 节省计算资源, 提高跟踪速度。</p>
                </div>
                <div class="p1">
                    <p id="61">3) 多尺度搜索区域。视频跟踪目标大小差异较大, 通过研究标准视频序列, 将目标分为特小目标、小目标和普通目标三类, 并对应使用不同比例的目标搜索区域, 提高跟踪性能。</p>
                </div>
                <div class="p1">
                    <p id="62">4) 上下文特征。真实目标区域放大固定倍数融入目标周围适当背景信息, 提取含有目标和背景的上下文特征训练滤波器, 提高跟踪算法稳健性。</p>
                </div>
                <div class="p1">
                    <p id="63">5) 目标重定位。由于视频跟踪存在多重干扰现象, 易出现跟丢目标的问题。本文通过计算峰旁比判断目标是否定位失败, 并在定位失败后利用帧差重新计算目标新位置。</p>
                </div>
                <h3 id="64" name="64" class="anchor-tag">2 所提算法</h3>
                <div class="p1">
                    <p id="65">所提算法的总体框架如图1所示。对第一帧给定的跟踪目标, 采用多尺度搜索区域和上下文特征策略获取新的目标区域和搜索区域, 然后提取HOG和CN特征计算响应图。根据特征使用加权融合公式得到最终滤波响应图, 寻找图中最大响应位置为目标预测位置, 利用响应图峰旁比评估预测结果。若出现图中虚线框预测失败的结果时, 使用目标重定位技术进行校正得到如实线所示的预测结果。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904034_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 算法总体框架图" src="Detail/GetImg?filename=images/GXXB201904034_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 算法总体框架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904034_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Overall framework of algorithm</p>

                </div>
                <h4 class="anchor-tag" id="67" name="67"><b>2.1 加权相关滤波</b></h4>
                <div class="p1">
                    <p id="68">ECO-HC算法只融合HOG和CN两种传统特征, 避免了提取CNN特征的严重耗时, 在中央处理器 (CPU) 上可以满足实时跟踪的要求;在目前仅使用人为设计的特征的算法中, 该算法跟踪性能最优。但是, ECO-HC算法在面对复杂环境时仍会跟丢目标, 所以本文通过研究特征优缺点设置相应的权重融合比例以提高跟踪效果, 并构造自适应牛顿迭代优化算法提高跟踪速度。</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69">2.1.1 加权融合</h4>
                <div class="p1">
                    <p id="70">目标特征类型的选择很大程度上影响着目标跟踪的性能。一般来说, 选择的特征越能表征目标, 训练的滤波器跟踪性能越好。实际跟踪过程存在着各种干扰因素, 目前也并没有可以应对所有干扰情况的特征, 所以只选取一种特征的跟踪算法并不能达到很好的跟踪精度。因此, 可提取不同特点的目标特征, 面对不同环境充分发挥各自的潜能, 达到较好的跟踪效果。</p>
                </div>
                <div class="p1">
                    <p id="71">目前, 跟踪算法所选用的目标特征大致分为两类, 即人为设计的传统特征和深度学习获取的特征。其中, 以HOG和CN为主的传统特征最为流行;而利用神经网络提取的深度特征能很好地表征目标外观和语义信息, 但是其提取过程相当复杂且耗时, 在CPU中运行速度极慢, 需要使用图形处理器进行额外加速操作。HOG特征通过计算局部区域的边缘及梯度表征目标, 对图像几何与光学形变都能保持良好的不变性, 尤其在检测行人时, 可以忽略行人的一些细微肢体动作, 提高跟踪效果。CN特征则通过提取物体CN全局性表征目标, 对几何形变具有很好的不变性。HOG与CN具有一定的互补性, 融合两者可以更好地应对不同环境, 另外两者提取过程简单, 在CPU中也可以满足实时跟踪的效果。</p>
                </div>
                <div class="p1">
                    <p id="72">ECO-HC算法在响应层中心融合HOG与CN特征得到最终响应图, 并确定融合响应值最大处为目标位置, 其融合权重比例为1∶1。CN特征提取整张图片的CN, HOG特征在局部区域内计算HOG。当目标与背景拥有相似的颜色信息时, CN特征响应图将会出现多个峰值;当目标出现严重变形或者遮挡等问题时, HOG特征将无法从局部区域正确把握全局的变化。在跟踪过程中, 不同特征面对不同干扰时对跟踪效果的影响力度是不同的, 这就需要设置合理的融合权重以充分发挥特征的表征作用, 保证跟踪器的性能。为此设计如下特征加权融合公式</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>c</mtext></mrow></msub><mo>=</mo><mi>w</mi><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>o</mtext><mtext>g</mtext></mrow></msub><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>o</mtext><mtext>g</mtext></mrow></msub><mo>+</mo><mi>w</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>n</mtext></mrow></msub><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>n</mtext></mrow></msub><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">式中:<i>w</i><sub>hog</sub>、<i>w</i><sub>cn</sub>分别为HOG和CN特征的权重;<b><i>f</i></b><sub>hog</sub>、<b><i>f</i></b><sub>cn</sub>分别为HOG和CN的特征响应。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75">2.1.2 自适应牛顿迭代法</h4>
                <div class="p1">
                    <p id="76">目标跟踪领域不仅需要优秀的创新策略提升跟踪的性能, 还需要对算法参数进行调优操作, 这样才能完全发挥算法的作用。但是, 参数调优操作虽然看似简单, 但却需要大量的时间与经验才能找到较好的参数选择及其参数间的配置。</p>
                </div>
                <div class="p1">
                    <p id="77">ECO-HC算法采用固定迭代次数的牛顿法进行迭代优化, 得到目标预测位置, 迭代次数的多少影响着位置预测的准确性和目标跟踪的速度。一般地, 迭代次数越多, 位置预测越准确, 时间花费也越高。视频跟踪过程既含有多种干扰因素组成的复杂场景, 也存在没有干扰或干扰较少的跟踪环境, 在两种完全不同的环境中预测目标位置的难易程度有很大的区别。复杂场景预测难度较大, 需要更多的迭代次数才能更准确地得到目标位置;而简单跟踪环境中干扰较少, 只需要很少的次数就能得到理想的结果。所以, 采用固定的迭代次数会使高难度情况预测结果不够准确或使低难度跟踪产生多余的迭代步骤, 造成资源与时间的浪费。为此, 本文引进一种自适应牛顿迭代法, 针对不同情况选用不同的迭代次数, 该方法使用的终止条件为<image id="78" type="formula" href="images/GXXB201904034_07800.jpg" display="inline" placement="inline"><alt></alt></image>, 即第<i>t</i>帧预测的中心位置<b><i>L</i></b><sub><i>t</i></sub>与第<i>t</i>-1帧预测的中心位置<b><i>L</i></b><sub><i>t</i>-1</sub>之间的距离值小于位置阈值<i>l</i>时, 停止本次迭代优化操作, 进入下一步任务。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79"><b>2.2 其他改进</b></h4>
                <h4 class="anchor-tag" id="80" name="80">2.2.1 多尺度搜索区域</h4>
                <div class="p1">
                    <p id="81">提高跟踪算法效率的方法之一是在一帧图像中快速并准确地提取正负样本特征以用于滤波器训练。目标跟踪发展初期, 大部分算法使用在整张图片中进行循环移位的方法获取训练样本, 但是由于搜索范围过大, 循环移位得到的正样本数量远少于负样本数量, 滤波器性能降低, 易产生漂移现象;此外, 搜索区域过大导致过多无用背景信息被提取, 浪费计算资源。随着跟踪算法的发展, 出现多种计算搜索区域的方法, 一种是目标尺度放大固定倍数, 另一种是目标长与宽加上固定值, 而目前最常用的是正方形搜索区域, 即对目标放大固定倍数后再将其变成面积相同的正方形搜索区域, 此区域可以较好地适应目标的外观变化, 有利于提高算法的稳健性。</p>
                </div>
                <div class="p1">
                    <p id="82">ECO-HC算法选用正方形搜索区域, 通过改变放大倍数调整搜索区域的大小, 从而影响跟踪性能, 该算法在所有跟踪过程中采用同一种固定倍数得到搜索区域, 这对不同大小的目标跟踪有不同的影响。例如, 通过航拍等方式拍摄的小目标与正常摄像机拍摄的普通大小的目标在选择搜索区域放大倍数时有很大的不同。一般情况, 面积较小的目标信息量少、运动过程易超出搜索范围, 根据经验会选择较大的搜索倍数;而普通大小的目标纹理、形状等特征明显, 可选用稍小一些的搜索区域放大倍数, 减少背景冗余。有针对性地分配不同参数, 解决不同目标跟踪问题, 提高跟踪性能。为此, 本文设计了多尺度搜索区域策略以确定不同尺寸目标的搜索区域, 使算法更具稳健性。</p>
                </div>
                <div class="p1">
                    <p id="83">通过计算第一帧目标面积并根据经验将目标分为三种, 并赋予不同大小的搜索区域。当目标面积<i>s</i>&lt;<i>s</i><sub>1</sub>时, 被定义为特小目标, 选用放大<i>γ</i><sub>1</sub>倍的目标搜索区域;而当<i>s</i><sub>1</sub>≤<i>s</i>&lt;<i>s</i><sub>2</sub>时, 被定义为小目标, 此时将其放大<i>γ</i><sub>2</sub>倍得到搜索区域;其他情况被视为普通目标, 将其放大<i>γ</i><sub>3</sub>倍得到搜索区域。目标搜索区域放大倍数可表示为</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mi>γ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo></mtd><mtd><mi>s</mi><mo>&lt;</mo><mi>s</mi><msub><mrow></mrow><mn>1</mn></msub></mtd></mtr><mtr><mtd><mi>γ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo></mtd><mtd><mi>s</mi><msub><mrow></mrow><mn>1</mn></msub><mo>≤</mo><mi>s</mi><mo>&lt;</mo><mi>s</mi><msub><mrow></mrow><mn>2</mn></msub></mtd></mtr><mtr><mtd><mi>γ</mi><msub><mrow></mrow><mn>3</mn></msub><mo>, </mo></mtd><mtd><mi>s</mi><mo>≥</mo><mi>s</mi><msub><mrow></mrow><mn>2</mn></msub></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">式中:<i>γ</i><sub>1</sub>&gt;<i>γ</i><sub>2</sub>&gt;<i>γ</i><sub>3</sub>。由于面积较大的目标搜索区域通常是整张图片, 所以将其归为普通目标行列;为了提高小目标的跟踪效果, 判断面积很小的目标给予更大的搜索区域。最终, 不同目标跟踪拥有不同大小的搜索区域。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86">2.2.2 上下文特征</h4>
                <div class="p1">
                    <p id="87">相关滤波器具有强大的计算能力且对光度、几何形变有较好的稳健性, 这使其被广泛应用于视频跟踪领域。视频第一帧提取目标显著特征, 训练相关滤波器需要大量的有效正样本以保证滤波器性能。ECO-HC算法在第一帧目标搜索区域中通过循环移位法得到高质量、高数量的近似样本, 避免了稠密采样的资源浪费, 提高了采样速度;但近似操作引入了边界效应, 影响跟踪性能。</p>
                </div>
                <div class="p1">
                    <p id="88">目标跟踪过程部分目标长宽相差较大, 这类目标在变形以及快速运动方面极易受到边界效应的影响, 根据文献<citation id="144" type="reference">[<a class="sup">12</a>]</citation>使用背景信息解决边界效应的思想, 本文对初始帧长宽比大于2的长条形目标进行跟踪时, 将目标尺度放大固定倍数, 引入适当的背景信息, 并提取含有前景与背景的上下文特征训练滤波器。目标与背景特征具有一定差异, 理想情况下可将上下文特征分为两部分, 即中间区域的目标特征和周围边界区域的背景特征。上下文特征的引入使滤波器能更大程度地区分前景和背景区域, 提高跟踪精确度;另外, 放大目标区域加入周围背景使目标样本特征更加完整, 在面对目标快速运动时, 更大范围的保留运动到边界的目标特征, 降低了边界效应的影响。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89">2.2.3 目标重定位</h4>
                <div class="p1">
                    <p id="90">视频跟踪中, 相机快速抖动、严重遮挡、目标快速移动和严重变形等复杂情况都会导致模型漂移, 从而使目标定位失败, 这就需要跟踪算法能够及时检测出漂移现象并给定相应措施重新找到目标的准确位置。一般情况下, 视频序列的前10帧是没有任何干扰因素的, 可以从第11帧图像开始判断跟踪失败现象, 减少计算冗余, 同时保证前10帧内没有重定位算法的干扰, 更准确地定位目标。</p>
                </div>
                <div class="p1">
                    <p id="91">相关滤波算法通过特征响应图中的最大峰值来预测目标位置, 当遇到复杂环境时响应图中会出现多个峰值干扰目标的定位。跟踪算法定位是否准确, 可以使用响应图峰旁比进行量化判断。假设第<i>t</i>帧的特征响应图为<b><i>f</i></b><sub><i>t</i></sub>, 其峰旁比为</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>max</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mi>σ</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">式中:<i>μ</i><sub><i>t</i></sub>、<i>σ</i><sub><i>t</i></sub>分别为特征响应图的均值和方差;max (·) 表示取最大值。当跟踪算法定位准确时, 峰旁比很大, 反之峰旁比很小<citation id="145" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="94">在实际跟踪过程中, 使用峰旁比判断定位失败并不十分准确。为减少误判, 使用多个限制条件判断失败情况:</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mo>&lt;</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mfrac><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mi>m</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo>&lt;</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>|</mo></mrow><mo>&gt;</mo><mi>p</mi><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">式中:<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>t</mi></mfrac><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>τ</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><mi>Ρ</mi></mstyle><msub><mrow></mrow><mi>τ</mi></msub></mrow></math></mathml>为跟踪前<i>t</i>帧峰旁比均值;<i>λ</i><sub>1</sub>、<i>λ</i><sub>2</sub>、<i>p</i>分别为第<i>t</i>帧峰旁比与第一帧峰旁比的比值阈值、前<i>t</i>帧峰旁比均值的比值阈值以及两帧间预测位置误差的阈值。目标跟踪第1帧没有干扰存在, 定位十分准确, 对应峰旁比<i>P</i><sub>1</sub>很大。结合两个峰旁比比值不等式条件[ (4) 式和 (5) 式]准确地表达定位可信度较低的情况。相邻两帧预测位置误差过大而超过阈值, 则说明很大程度上存在定位失败, 需要进行重新定位操作:</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi mathvariant="bold-italic">e</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msub></mrow></msub></mtd></mtr><mtr><mtd columnalign="left"><mi mathvariant="bold-italic">e</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msub></mrow></msub></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">式中:<b><i>A</i></b><sub><b><i>L</i></b><sub><i>t</i></sub></sub>为第<i>t</i>帧预测位置<b><i>L</i></b><sub><i>t</i></sub>对应的目标区域;<b><i>e</i></b><sub><i>t</i></sub>为两帧图片预测目标框之间对应像素的误差。误差越小, 说明预测位置越准确。所以, 重定位的基本思想是<mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">L</mi></munder><mo stretchy="false">{</mo><mi mathvariant="bold-italic">e</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi mathvariant="bold-italic">e</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">}</mo></mrow></math></mathml>, 即将<b><i>e</i></b><sub><i>t</i></sub>与<b><i>e</i></b><sub><i>t</i>-1</sub>中像素误差最小值所对应的预测目标位置记为当前帧的目标位置。</p>
                </div>
                <h3 id="101" name="101" class="anchor-tag">3 实验结果与分析</h3>
                <div class="p1">
                    <p id="102">为了验证本文改进算法的整体效果, 选取标准视频序列OTB-100<citation id="146" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>作为测试数据集。该数据集包含11种典型干扰, 每个视频序列都存在一种或几种干扰组成的复杂情况, 100组视频长短不一, 具有代表性。实验平台是ubuntu16.04系统下的MATLAB R2015b, 所有跟踪实验均在配置为Intel Core i7-4790K CPU、内存为16 GB的计算机上完成。实验中采用中心位置误差 (CLE) 、距离精度 (DP) 以及重叠精度 (OP) 三种评估参数对跟踪性能进行评价, 使用每秒钟处理的帧数 (frame/s) 评估跟踪速度。</p>
                </div>
                <div class="p1">
                    <p id="103">实验均在ECO-HC算法代码上改进, 基础参数与原始算法相同, 而由于实验需要, 改动几项参数:高斯函数带宽因子为1/12, 学习率为0.01, 滤波器稀疏更新间隔为2, 初始帧中高斯-牛顿法的迭代次数为2, 共轭梯度迭代次数为2, 第一帧共轭梯度迭代次数为15。原ECO-HC算法使用5次牛顿迭代预测目标位置, 性能较好, 为保证复杂场景下预测位置的准确性, 自适应牛顿迭代次数最大值也选用5次;自适应牛顿迭代的位置阈值取<i>l</i>=10<sup>-6</sup>, 表示两次迭代的位置误差趋近于零;目标重定位中判断定位失败的阈值分别设置为<i>λ</i><sub>1</sub>=0.385、<i>λ</i><sub>2</sub>=0.52和<i>p</i>=20, 表示目标两帧之间的位移较大且峰旁比与历史值相差大, 目标发生了定位错误, 此时启用重定位技术对目标位置进行校正。实验中通过经验调节方法给定基本参数的阈值。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104"><b>3.1 算法改进实验</b></h4>
                <div class="p1">
                    <p id="105">本部分实验对比特征融合过程不同权重比例对跟踪过程的影响和引入不同程度的上下文特征对结果的影响。此外, 在ECO-HC算法的基础上, 逐步引入各种策略, 观察每种策略对整体算法的贡献。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106">3.1.1 不同融合权重对比实验</h4>
                <div class="p1">
                    <p id="107">ECO-HC算法使用CN与HOG的权重比为1∶1, 本次实验增加4组CN与HOG权重比例关系, 使用平均DP和平均OP进行评估, 实验结果如表1所示。当CN与HOG的权重比为1∶2时, 跟踪效果最好, 比ECO-HC算法的平均DP和平均OP分别提升了1.1%和2.1%, 而OP提升幅度较大说明HOG权重的增加有利于跟踪稳健性的提升。</p>
                </div>
                <div class="area_img" id="108">
                    <p class="img_tit">表1 不同特征权重的跟踪结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Tracking results under different feature weights</p>
                    <p class="img_note"></p>
                    <table id="108" border="1"><tr><td>CN</td><td>HOG</td><td>Average DP /%</td><td>Average OP /%</td></tr><tr><td><br />1</td><td>0.5</td><td>85.6</td><td>79.1</td></tr><tr><td><br />1</td><td>1.0</td><td>85.7</td><td>78.2</td></tr><tr><td><br />1</td><td>1.5</td><td>84.8</td><td>78.0</td></tr><tr><td><br />1</td><td>2.0</td><td>86.8</td><td>80.3</td></tr><tr><td><br />1</td><td>2.5</td><td>84.7</td><td>77.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: best results in table are in the fourth row, and results of the ECO-HC algorithm are in the second row.</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109">3.1.2 引入上下文特征对比实验</h4>
                <div class="p1">
                    <p id="110">放大目标尺度, 引入周围背景并提取上下文特征训练相关滤波器, 使算法在一定程度上正确区分背景和前景区域, 避免跟踪失败。ECO-HC算法跟踪girl2视频中的目标时遭遇干扰物遮挡, 之后跟踪失败且定位在干扰物上, 将固定倍数设置为1.1、1.2、1.3, 引入少许背景信息后, 跟踪精度分别为81.7%、97.9%、98.4%, 跟踪效果大幅度提升, 呈现不断上升趋势。为进一步研究上下文特征的作用, 在引入自适应牛顿迭代法、特征加权和多尺度搜索区域的改进算法上进行相应实验, 其结果如表2所示。由表2数据得到三种放大倍数均可以提升跟踪性能, 但目标放大1.1倍时跟踪性能最好, 这说明引入背景信息需要适量, 否则会适得其反, 所以最终选择放大倍数为1.1。</p>
                </div>
                <div class="area_img" id="111">
                    <p class="img_tit">表2 三种上下文特征的跟踪结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Tracking results of three contextual features</p>
                    <p class="img_note"></p>
                    <table id="111" border="1"><tr><td><br />Magnification times of target size</td><td>1.1</td><td>1.2</td><td>1.3</td></tr><tr><td><br />Average CLE /pixel</td><td>19.1</td><td>19.2</td><td>23.3</td></tr><tr><td><br />Average DP /%</td><td>88.5</td><td>88.3</td><td>86.9</td></tr><tr><td><br />Average OP /%</td><td>80.0</td><td>79.7</td><td>78.6</td></tr><tr><td><br />Speed / (frame·s<sup>-1</sup>) </td><td>65.2</td><td>65.0</td><td>64.8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="112" name="112">3.1.3 多尺度搜索区域对比实验</h4>
                <div class="p1">
                    <p id="113">搜索区域对相关滤波跟踪有重要影响, 原ECO-HC算法中搜索区域采用固定大小的搜索区域, 然而不同大小的目标应具有不同的搜索区域。因此根据目标对象的面积将目标分为三类, 即特小目标、小目标、普通目标, 实验中选用的分类阈值为<i>s</i><sub>1</sub>=20<sup>2</sup>, <i>s</i><sub>2</sub>=36<sup>2</sup>。为得到三种大小的目标对应的最佳搜索区域, 在[3.0, 4.5]的搜索区域范围内进行阈值选择。图2 (a) 给出了数据集OTB-100中三类大小不同的目标尺度的精确度结果, 由图2 (a) 可以看出最优搜索区域尺度为<i>γ</i><sub>1</sub>=3.7、<i>γ</i><sub>2</sub>=3.6、<i>γ</i><sub>3</sub>=3.5。根据实验的测试结果得到在引入加权融合、上下文特征和目标重定位的算法基础上增加该多尺度搜索区域策略, 可使算法的跟踪精度由88.2%提升到89.2%。为验证此策略的普及性, 在数据集Temple-color-128<citation id="147" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>中进行同样的实验, 结果如图2 (b) 所示, 在相同目标分类阈值下, 最优搜索区域尺度为<i>γ</i><sub>1</sub>=4.4、<i>γ</i><sub>2</sub>=3.7、<i>γ</i><sub>3</sub>=3.4。由数据集Temple-color-128中的实验结果得到多尺度搜索区域策略可使ECO-HC算法的平均跟踪精确度从71.8%提升到75.9%, 从而验证了多尺度搜索区域策略的有效性。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904034_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同搜索区域尺度的精确度曲线图。 (a) OTB-100; (b) Temple-color-128" src="Detail/GetImg?filename=images/GXXB201904034_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 不同搜索区域尺度的精确度曲线图。 (a) OTB-100; (b) Temple-color-128  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904034_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Precision plots under different search area scales. (a) OTB-100; (b) Temple-color-128</p>

                </div>
                <h4 class="anchor-tag" id="115" name="115">3.1.4 各策略对跟踪算法的影响</h4>
                <div class="p1">
                    <p id="116">确定各种策略的参数之后, 本文在ECO-HC算法基础上逐步加入相应策略, 比较各个策略的性能, 如表3所示。表中自适应牛顿迭代法主要用于提升速度, 其他4种策略则提升目标跟踪准确性与稳健性, 最终算法比ECO-HC算法的平均DP和平均OP分别提升了3.6%和2.1%, 速度提升5.2 frame/s。从表3中可以看出自适应牛顿迭代策略使跟踪速度提升了6.8 frame/s;多尺度搜索区域和上下文特征仅增加了条件判断, 且ECO-HC算法中获取的搜索区域图像将会归一化到相应的大小进行特征提取, 对计算速度影响不大;加权特征融合过程仅增加了响应图的数值乘法, 也基本不影响跟踪速度;目标重定位采用帧差方法, 主要增加了两帧图像之间的相减操作, 对整体的跟踪速度影响不大, 且重定位操作只在定位失败后进行, 前三种策略有效提升了跟踪精确度, 减少了目标定位失败的情况。总体上, 这四种策略使得跟踪速度略有下降, 从66.8 frame/s降到65.2 frame/s。</p>
                </div>
                <div class="area_img" id="117">
                    <p class="img_tit">表3 依次加入各项策略的跟踪结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Tracking results of each strategy added in turn</p>
                    <p class="img_note"></p>
                    <table id="117" border="1"><tr><td>ECO-HC<br />algorithm</td><td>Adaptive Newton <br />iteration</td><td>Weighted <br />fusion</td><td>Multi-scale <br />search area</td><td>Contextual <br />features</td><td>Object <br />relocation</td><td>Average <br />DP /%</td><td>Average <br />OP /%</td><td>Speed /<br /> (frame·s<sup>-1</sup>) </td></tr><tr><td><br />√</td><td></td><td></td><td></td><td></td><td></td><td>85.6</td><td>78.5</td><td>60.0</td></tr><tr><td><br />√</td><td>√</td><td></td><td></td><td></td><td></td><td>85.6</td><td>78.5</td><td>66.8</td></tr><tr><td><br />√</td><td>√</td><td>√</td><td></td><td></td><td></td><td>86.8</td><td>80.3</td><td>65.7</td></tr><tr><td><br />√</td><td>√</td><td>√</td><td>√</td><td></td><td></td><td>87.7</td><td>80.4</td><td>65.8</td></tr><tr><td><br />√</td><td>√</td><td>√</td><td>√</td><td>√</td><td></td><td>88.5</td><td>80.0</td><td>65.2</td></tr><tr><td><br />√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>89.2</td><td>80.6</td><td>65.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="118" name="118"><b>3.2 与其他优秀算法的对比实验</b></h4>
                <div class="p1">
                    <p id="119">为进一步验证所提算法的性能, 选取了现有的8种优秀跟踪算法与其进行比较。这8种算法分别是:ECO-HC算法<citation id="148" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、多特征融合的视觉跟踪算法 (Staple) <citation id="149" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、改进版分层卷积相关滤波跟踪算法 (HCFTS) <citation id="150" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、使用通道和空间置信度的判别式相关滤波器 (CSR-DCF) <citation id="151" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、空间正则化相关滤波跟踪算法 (SRDCF) <citation id="152" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>及其使用深度卷积特征的SRDCF (D_SRDCF) <citation id="153" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、基于多峰检测的目标跟踪算法 (LMCF) <citation id="154" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>以及使用深度特征的增强版LMCF (D_LMCF) <citation id="155" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。8种算法均属于相关滤波法, 其中ECO-HC算法、Staple使用多种传统特征融合, HCFTS、D_LMCF、D_SRDCF使用深度卷积特征。</p>
                </div>
                <div class="p1">
                    <p id="120">在OTB-100数据集中对9种跟踪算法进行比较, 如表4所示。表中数据显示所提算法平均DP为89.2%, 平均OP为80.6%, 均优于其他8种对比算法;所提算法跟踪速度达到65.2 frame/s左右, 相对于ECO-HC算法提高了5.2 frame/s, 跟踪实时性效果更好。综合比较, 所提算法仅使用两种传统特征, 跟踪性能却优于D_LMCF和D_SRDCF等, 而在CPU中的跟踪速度依然满足实时性要求。另外, 画出9种算法的精确度曲线和成功率曲线, 如图3所示, 从成功率曲线中得到所提算法成功率为65.8%, 比ECO-HC算法高1.5%。</p>
                </div>
                <div class="area_img" id="121">
                    <p class="img_tit">表4 OTB-100中9种算法的对比结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Comparison results of 9 algorithms in OTB-100</p>
                    <p class="img_note"></p>
                    <table id="121" border="1"><tr><td><br /></td><td>Proposed<br />algorithm</td><td>ECO-HC<br />algorithm</td><td>HCFTS</td><td>CSR-DCF</td><td>Staple</td><td>LMCF</td><td>D_LMCF</td><td>SRDCF</td><td>D_SRDCF</td></tr><tr><td><br />Average CLE /pixel</td><td>17.0</td><td>22.7</td><td>16.6</td><td>24.6</td><td>31.5</td><td>39.0</td><td>21.3</td><td>38.6</td><td>21.4</td></tr><tr><td><br />Average DP /%</td><td>89.2</td><td>85.6</td><td>87.0</td><td>79.9</td><td>78.4</td><td>78.9</td><td>85.9</td><td>78.9</td><td>85.1</td></tr><tr><td><br />Average OP /%</td><td>80.6</td><td>78.5</td><td>72.1</td><td>70.2</td><td>70.9</td><td>71.9</td><td>76.1</td><td>72.9</td><td>77.3</td></tr><tr><td><br />Speed / (frame·s<sup>-1</sup>) </td><td>65.2</td><td>60.0</td><td>6.0</td><td>6.5</td><td>26.4</td><td>65.5</td><td>6.9</td><td>3.0</td><td>0.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904034_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 9种算法在OTB-100中的精度图和成功率曲线图。 (a) 精确度曲线; (b) 成功率曲线" src="Detail/GetImg?filename=images/GXXB201904034_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 9种算法在OTB-100中的精度图和成功率曲线图。 (a) 精确度曲线; (b) 成功率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904034_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Precision and success plots of nine algorithms on OTB-100. (a) Precision curves; (b) success curves</p>

                </div>
                <div class="p1">
                    <p id="123">图4是9种跟踪算法在human3, girl2, soccer和lemming 4组典型视频下的可视化结果, 4组视频均存在多种干扰条件, 具有很好的代表性。图4 (a) human3视频中第41帧左右遭遇其他人物和标志杆的遮挡, 在第77帧看到只有所提算法、ECO-HC算法和D_SRDCF三种算法跟踪到目标, 其他算法均跟踪失败;随后遇到摄像机调焦等干扰后, 在第737帧D_SRDCF的尺度预测明显比所提算法和ECO-HC算法的效果差。图4 (b) girl2视频在第105帧遇到外物遮挡, 之后只有所提算法准确跟踪到目标并拥有较好的尺度预测;随着目标人物移动和形变, 多个算法重新找到目标, 但尺度预测却比所提算法差。图4 (c) soccer视频存在着严重的光线变化、大面积红色外物遮挡、快速运动等干扰, 从随机选取的4帧图片中看出所提算法跟踪目标具有很好的稳健性, 尺度预测也是几种算法中最优的。图4 (d) lemming视频中目标发生平面外旋转、严重遮挡、运动模糊、尺度变化等干扰, 在选取的图片中看到多种算法因遮挡而跟丢目标, 而在跟踪到目标的算法中, 所提算法的尺度预测优于其他算法。综上所述, 所提算法拥有较好的跟踪性能, 在面对多重干扰时具有较高的稳健性和更好的尺度预测。</p>
                </div>
                <h3 id="124" name="124" class="anchor-tag">4 结 论</h3>
                <div class="p1">
                    <p id="125">ECO-HC算法跟踪效果优于其他使用传统特征和部分使用深度特征的跟踪算法, 性能比较稳定, 但面对严重遮挡等干扰时仍出现跟踪失败的现象, 因此做出以下改进:首先通过研究HOG与CN特征的优缺点设计特征加权融合方法提高跟踪精度, 然后通过自适应牛顿迭代优化方法预测目标位置, 其次对目标大小精细划分得到多尺度搜索区域提高小目标跟踪精度, 通过放大目标尺度提取上下文特征缓解边界效应, 最后利用峰旁比判定目标定位失败的情况并进行重定位操作。为验证所提算法的性能, 在标准数据集OTB-100中做了综合性实验对比。实验结果表明, 改进算法的平均DP比ECO-HC算法高3.6%, 平均OP比ECO-HC算法高2.1%, 平均成功率比ECO-HC算法高1.5%, 跟踪速度保持在65.2 frame/s左右, 满足实时跟踪的要求。</p>
                </div>
                <div class="area_img" id="126">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201904034_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 典型视频跟踪结果对比。 (a) Human3; (b) girl2; (c) soccer; (d) lemming" src="Detail/GetImg?filename=images/GXXB201904034_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 典型视频跟踪结果对比。 (a) Human3; (b) girl2; (c) soccer; (d) lemming  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201904034_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Comparison of typical video tracking results. (a) Human3; (b) girl2; (c) soccer; (d) lemming</p>

                </div>
                <div class="p1">
                    <p id="128">所提算法取得了较好的跟踪效果, 但使用实践经验方法设定算法的参数, 需要在不同数据集中进行设置, 因此后续将重点研究各参数的自适应选择算法。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="12">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">

                                <b>[1]</b> Bolme D S, Beveridge J R, Draper B A, <i>et al</i>.Visual object tracking using adaptive correlation filters[C].IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010:2544-2550.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">

                                <b>[2]</b> Henriques J F, Caseiro R, Martins P, <i>et al</i>.Exploiting the circulant structure of tracking-by-detection with kernels[C].European Conference on Computer Vision, 2012:702-715.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_3" >
                                    <b>[3]</b>
                                 Henriques J F, Caseiro R, Martins P, <i>et al</i>.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive color attributes for Real-Time visual tracking">

                                <b>[4]</b> Danelljan M, Khan F S, Felsberg M, <i>et al</i>.Adaptive color attributes for real-time visual tracking[C].IEEE Conference on Computer Vision and Pattern Recognition, 2014:1090-1097.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">

                                <b>[5]</b> Bertinetto L, Valmadre J, Golodetz S, <i>et al</i>.Staple:complementary learners for real-time tracking[C].IEEE Conference on Computer Vision and Pattern Recognition, 2016:1401-1409.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                 Ma C, Huang J B, Yang X K, <i>et al</i>.Hierarchical convolutional features for visual tracking[C].IEEE International Conference on Computer Vision, 2015:3074-3082.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust visual tracking via hierarchical convolutional features[OL]">

                                <b>[7]</b> Ma C, Huang J B, Yang X K, <i>et al</i>.Robust visual tracking via hierarchical convolutional features[EB/OL]. (2018-08-11) [2018-10-25].https:∥arxiv.org/abs/1707.03816.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201705025&amp;v=MTA2MzQ5SFlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtWcjdPSWpYVGJMRzRIOWJNcW8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Li S S, Zhao G P, Wang J N.Distractor-aware object tracking based on multi-feature fusion and scale-adaption[J].Acta Optica Sinica, 2017, 37 (5) :0515005.李双双, 赵高鹏, 王建宇.基于特征融合和尺度自适应的干扰感知目标跟踪[J].光学学报, 2017, 37 (5) :0515005.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Correlation filters with weighted convolution responses">

                                <b>[9]</b> He Z Q, Fan Y R, Zhuang J F, <i>et al</i>.Correlation filters with weighted convolution responses[C].IEEE International Conference on Computer Vision Workshops, 2017:1992-2000.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201706012&amp;v=MDMzMDNCYUxHNEg5Yk1xWTlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIa1ZyN09Mejc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Xiong C Z, Zhao L L, Guo F H.Kernelized correlation filters tracking based on adaptive feature fusion[J].Journal of Computer-Aided Design &amp; Computer Graphics, 2017, 29 (6) :1068-1074.熊昌镇, 赵璐璐, 郭芬红.自适应特征融合的核相关滤波跟踪算法[J].计算机辅助设计与图形学学报, 2017, 29 (6) :1068-1074.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">

                                <b>[11]</b> Danelljan M, Häger G, Khan F S, <i>et al</i>.Learning spatially regularized correlation filters for visual tracking[C].IEEE International Conference on Computer Vision, 2015:4310-4318.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Background-Aware Correlation Filters for Visual Tracking">

                                <b>[12]</b> Galoogahi H K, Fagg A, Lucey S.Learning background-aware correlation filters for visual tracking[C].IEEE International Conference on Computer Vision, 2017:1144-1152.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large Margin Object Tracking with Circulant Feature Maps">

                                <b>[13]</b> Wang M M, Liu Y, Huang Z Y.Large margin object tracking with circulant feature maps[C].IEEE Conference on Computer Vision and Pattern Recognition, 2017:4800-4808.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative scale space tracking">

                                <b>[14]</b> Danelljan M, Häger G, Khan F S, <i>et al</i>.Discriminative scale space tracking[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (8) :1561-1575.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A scale adaptive kernel correlation filter tracker with feature integration">

                                <b>[15]</b> Li Y, Zhu J K.A scale adaptive kernel correlation filter tracker with feature integration[C].European Conference on Computer Vision, 2014:254-265.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_16" >
                                    <b>[16]</b>
                                 Danelljan M, Bhat G, Khan F S, <i>et al</i>.ECO:efficient convolution operators for tracking[C].IEEE Conference on Computer Vision and Pattern Recognition, 2017:6931-6939.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201810031&amp;v=MzE1MjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SGtWcjdPSWpYVGJMRzRIOW5OcjQ5R1pZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> Xiong C Z, Che M Q, Wang R L, <i>et al</i>.Robust real-time visual tracking via dual model adaptive switching[J].Acta Optica Sinica, 2018, 38 (10) :1015002.熊昌镇, 车满强, 王润玲, 等.稳健的双模型自适应切换实时跟踪算法[J].光学学报, 2018, 38 (10) :1015002.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">

                                <b>[18]</b> Wu Y, Lim J, Yang M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1834-1848.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Encoding color information for visual tracking:Algorithms and bechmark,&amp;quot;">

                                <b>[19]</b> Liang P P, Blasch E, Ling H B.Encoding color information for visual tracking:algorithms and benchmark[J].IEEE Transactions on Image Processing, 2015, 24 (12) :5630-5644.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative correlation filter with channel and spatial reliability">

                                <b>[20]</b> Lukežic A, Vojír T, Zajc L C, <i>et al</i>.Discriminative correlation filter with channel and spatial reliability[C].IEEE Conference on Computer Vision and Pattern Recognition, 2017:4847-4856.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_21" >
                                    <b>[21]</b>
                                 Danelljan M, Häger G, Khan F S, <i>et al</i>.Convolutional features for correlation filter based visual tracking[C].IEEE International Conference on Computer Vision Workshop, 2015:621-629.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201904034" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201904034&amp;v=MDcxNzMzenFxQnRHRnJDVVJMT2VaZVZ1RnlIa1ZyN09JalhUYkxHNEg5ak1xNDlHWUlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

