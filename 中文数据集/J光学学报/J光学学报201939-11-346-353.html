

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133074997940000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dGXXB201911042%26RESULT%3d1%26SIGN%3dXANhHHoZavmMMo%252bFKhOztKkDkBQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201911042&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201911042&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201911042&amp;v=MTA4NjFqWFRiTEc0SDlqTnJvOUJab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXZoV3IvQUk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#65" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#69" data-title="2 MSCNN检测框架 ">2 MSCNN检测框架</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="&lt;b&gt;2.1 MSCNN网络结构&lt;/b&gt;"><b>2.1 MSCNN网络结构</b></a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;2.2 EFPN&lt;/b&gt;"><b>2.2 EFPN</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;2.3 损失函数设计&lt;/b&gt;"><b>2.3 损失函数设计</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#87" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#88" data-title="&lt;b&gt;3.1 数据集与评价指标&lt;/b&gt;"><b>3.1 数据集与评价指标</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;3.2 参数设置&lt;/b&gt;"><b>3.2 参数设置</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;3.3 主要结果&lt;/b&gt;"><b>3.3 主要结果</b></a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;3.4 消融实验&lt;/b&gt;"><b>3.4 消融实验</b></a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;3.5 多尺度目标检测&lt;/b&gt;"><b>3.5 多尺度目标检测</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#120" data-title="4 结  论 ">4 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="图1 MSCNN目标检测框架结构">图1 MSCNN目标检测框架结构</a></li>
                                                <li><a href="#79" data-title="图2 EFPN网络构型和膨胀瓶颈结构。">图2 EFPN网络构型和膨胀瓶颈结构。</a></li>
                                                <li><a href="#91" data-title="表1 基于实例尺度分布的边界框面积定义方法">表1 基于实例尺度分布的边界框面积定义方法</a></li>
                                                <li><a href="#101" data-title="表2 不同算法在NWPU VHR-10数据集上的检测精度对比">表2 不同算法在NWPU VHR-10数据集上的检测精度对比</a></li>
                                                <li><a href="#108" data-title="表3 MSCNN消融实验参数">表3 MSCNN消融实验参数</a></li>
                                                <li><a href="#115" data-title="图3 &lt;i&gt;EFPN&lt;/i&gt;-&lt;i&gt;NoProj&lt;/i&gt;的网络结构">图3 <i>EFPN</i>-<i>NoProj</i>的网络结构</a></li>
                                                <li><a href="#116" data-title="表4 不同&lt;i&gt;IOU&lt;/i&gt;阈值和不同边框尺度下的平均检测精度和召回率">表4 不同<i>IOU</i>阈值和不同边框尺度下的平均检测精度和召回率</a></li>
                                                <li><a href="#119" data-title="图4 基于&lt;i&gt;MSCNN&lt;/i&gt;的可视化检测结果">图4 基于<i>MSCNN</i>的可视化检测结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="9">


                                    <a id="bibliography_1" title=" &lt;i&gt;Han J W&lt;/i&gt;,&lt;i&gt;Zhang D W&lt;/i&gt;,&lt;i&gt;Cheng G&lt;/i&gt;,et al.&lt;i&gt;Object detection in optical remote sensing images based on weakly supervised learning and high&lt;/i&gt;-&lt;i&gt;level feature learning&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Geoscience and Remote Sensing&lt;/i&gt;,2015,53(6):3325-3337." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object detection in optical remote sensing images based on weakly supervised learning and high-level feature Learning">
                                        <b>[1]</b>
                                         &lt;i&gt;Han J W&lt;/i&gt;,&lt;i&gt;Zhang D W&lt;/i&gt;,&lt;i&gt;Cheng G&lt;/i&gt;,et al.&lt;i&gt;Object detection in optical remote sensing images based on weakly supervised learning and high&lt;/i&gt;-&lt;i&gt;level feature learning&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Geoscience and Remote Sensing&lt;/i&gt;,2015,53(6):3325-3337.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_2" title=" &lt;i&gt;Cheng G&lt;/i&gt;,&lt;i&gt;Han J W&lt;/i&gt;.&lt;i&gt;A survey on object detection in optical remote sensing images&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;ISPRS Journal of Photogrammetry and Remote Sensing&lt;/i&gt;,2016,117:11-28." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES4CCC753F8FC6467291EA53DC443DBE26&amp;v=MjI0MzFDcGJRMzVkbGh4N3kyd0tBPU5pZk9mYmZMYmFMTHFvd3piSjE4Q25nL3lCUWE2MG9NVFh5VzN4WXhlc2JtTUxpWkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         &lt;i&gt;Cheng G&lt;/i&gt;,&lt;i&gt;Han J W&lt;/i&gt;.&lt;i&gt;A survey on object detection in optical remote sensing images&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;ISPRS Journal of Photogrammetry and Remote Sensing&lt;/i&gt;,2016,117:11-28.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_3" title=" &lt;i&gt;Cheng G&lt;/i&gt;,&lt;i&gt;Han J W&lt;/i&gt;,&lt;i&gt;Zhou P C&lt;/i&gt;,et al.&lt;i&gt;Multi&lt;/i&gt;-&lt;i&gt;class geospatial object detection and geographic image classification based on collection of part detectors&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;ISPRS Journal of Photogrammetry and Remote Sensing&lt;/i&gt;,2014,98:119-132." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700308532&amp;v=MjkwODJud1plWnVIeWptVWI3SUkxb2RhaG89TmlmT2ZiSzhIOURNcUk5Rlorc0hDWDg3b0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         &lt;i&gt;Cheng G&lt;/i&gt;,&lt;i&gt;Han J W&lt;/i&gt;,&lt;i&gt;Zhou P C&lt;/i&gt;,et al.&lt;i&gt;Multi&lt;/i&gt;-&lt;i&gt;class geospatial object detection and geographic image classification based on collection of part detectors&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;ISPRS Journal of Photogrammetry and Remote Sensing&lt;/i&gt;,2014,98:119-132.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_4" title=" &lt;i&gt;Deng Z P&lt;/i&gt;,&lt;i&gt;Sun H&lt;/i&gt;,&lt;i&gt;Zhou S L&lt;/i&gt;,et al.&lt;i&gt;Multi&lt;/i&gt;-&lt;i&gt;scale object detection in remote sensing imagery with convolutional neural networks&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;ISPRS Journal of Photogrammetry and Remote Sensing&lt;/i&gt;,2018,145:3-22." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESD9E8A1F886C5F554CD4DAFCE569A68B5&amp;v=MTI0NTVSMlJjemNNT1NUY2lhQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxoeDd5MndLQT1OaWZPZmNleGE5bTlydmxOYk8xOENRbzh5aEpnbmpzSk9RbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         &lt;i&gt;Deng Z P&lt;/i&gt;,&lt;i&gt;Sun H&lt;/i&gt;,&lt;i&gt;Zhou S L&lt;/i&gt;,et al.&lt;i&gt;Multi&lt;/i&gt;-&lt;i&gt;scale object detection in remote sensing imagery with convolutional neural networks&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;ISPRS Journal of Photogrammetry and Remote Sensing&lt;/i&gt;,2018,145:3-22.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_5" title=" &lt;i&gt;Zhong Y F&lt;/i&gt;,&lt;i&gt;Han X B&lt;/i&gt;,&lt;i&gt;Zhang L P&lt;/i&gt;.&lt;i&gt;Multi&lt;/i&gt;-&lt;i&gt;class geospatial object detection based on a position&lt;/i&gt;-&lt;i&gt;sensitive balancing framework for high spatial resolution remote sensing imagery&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;ISPRS Journal of Photogrammetry and Remote Sensing&lt;/i&gt;,2018,138:281-294." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6C08A5B3BD027EF86DE47E5DC1275010&amp;v=MTQxOTdrcDVUd3JuMkdFMGU3V1JSYnVmQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxoeDd5MndLQT1OaWZPZmJYTEh0bTlxdjFHRnA4UERudE11UjRWbg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         &lt;i&gt;Zhong Y F&lt;/i&gt;,&lt;i&gt;Han X B&lt;/i&gt;,&lt;i&gt;Zhang L P&lt;/i&gt;.&lt;i&gt;Multi&lt;/i&gt;-&lt;i&gt;class geospatial object detection based on a position&lt;/i&gt;-&lt;i&gt;sensitive balancing framework for high spatial resolution remote sensing imagery&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;ISPRS Journal of Photogrammetry and Remote Sensing&lt;/i&gt;,2018,138:281-294.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_6" title=" &lt;i&gt;Lowe D G&lt;/i&gt;.&lt;i&gt;Distinctive image features from scale&lt;/i&gt;-&lt;i&gt;invariant keypoints&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;International Journal of Computer Vision&lt;/i&gt;,2004,60(2):91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjIxMDZkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTbmxVTHZCSUZZPU5qN0Jhck80SHRIT3A0eEZiZXNPWTNrNXpC&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         &lt;i&gt;Lowe D G&lt;/i&gt;.&lt;i&gt;Distinctive image features from scale&lt;/i&gt;-&lt;i&gt;invariant keypoints&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;International Journal of Computer Vision&lt;/i&gt;,2004,60(2):91-110.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_7" title=" &lt;i&gt;Dalal N&lt;/i&gt;,&lt;i&gt;Triggs B&lt;/i&gt;.&lt;i&gt;Histograms of oriented gradients for human&lt;/i&gt;&lt;i&gt;detection&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2005 &lt;i&gt;IEEE Computer Society Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;′05),&lt;i&gt;June&lt;/i&gt; 20-25,2005,&lt;i&gt;San Diego&lt;/i&gt;,&lt;i&gt;CA&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2005:8588935." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">
                                        <b>[7]</b>
                                         &lt;i&gt;Dalal N&lt;/i&gt;,&lt;i&gt;Triggs B&lt;/i&gt;.&lt;i&gt;Histograms of oriented gradients for human&lt;/i&gt;&lt;i&gt;detection&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2005 &lt;i&gt;IEEE Computer Society Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;′05),&lt;i&gt;June&lt;/i&gt; 20-25,2005,&lt;i&gt;San Diego&lt;/i&gt;,&lt;i&gt;CA&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2005:8588935.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_8" title=" &lt;i&gt;Felzenszwalb P&lt;/i&gt;,&lt;i&gt;McAllester D&lt;/i&gt;,&lt;i&gt;Ramanan D&lt;/i&gt;.&lt;i&gt;A discriminatively trained&lt;/i&gt;,&lt;i&gt;multiscale&lt;/i&gt;,&lt;i&gt;deformable part model&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]∥2008 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;,&lt;i&gt;June&lt;/i&gt; 23-28,2008,&lt;i&gt;Anchorage&lt;/i&gt;,&lt;i&gt;AK&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2008:10139902." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A discriminatively trained,multiscale,deformable part model">
                                        <b>[8]</b>
                                         &lt;i&gt;Felzenszwalb P&lt;/i&gt;,&lt;i&gt;McAllester D&lt;/i&gt;,&lt;i&gt;Ramanan D&lt;/i&gt;.&lt;i&gt;A discriminatively trained&lt;/i&gt;,&lt;i&gt;multiscale&lt;/i&gt;,&lt;i&gt;deformable part model&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]∥2008 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;,&lt;i&gt;June&lt;/i&gt; 23-28,2008,&lt;i&gt;Anchorage&lt;/i&gt;,&lt;i&gt;AK&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2008:10139902.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_9" title=" &lt;i&gt;Fu C Y&lt;/i&gt;,&lt;i&gt;Liu W&lt;/i&gt;,&lt;i&gt;Ranga A&lt;/i&gt;,et al.&lt;i&gt;Dssd&lt;/i&gt;:&lt;i&gt;deconvolutional&lt;/i&gt;&lt;i&gt;single shot detector&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].(2017-01-23)[2019-04-07].&lt;i&gt;https&lt;/i&gt;://&lt;i&gt;arxiv&lt;/i&gt;.&lt;i&gt;org&lt;/i&gt;/&lt;i&gt;abs&lt;/i&gt;/1701.06659." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dssd:Deconvolutional single shot detector">
                                        <b>[9]</b>
                                         &lt;i&gt;Fu C Y&lt;/i&gt;,&lt;i&gt;Liu W&lt;/i&gt;,&lt;i&gt;Ranga A&lt;/i&gt;,et al.&lt;i&gt;Dssd&lt;/i&gt;:&lt;i&gt;deconvolutional&lt;/i&gt;&lt;i&gt;single shot detector&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].(2017-01-23)[2019-04-07].&lt;i&gt;https&lt;/i&gt;://&lt;i&gt;arxiv&lt;/i&gt;.&lt;i&gt;org&lt;/i&gt;/&lt;i&gt;abs&lt;/i&gt;/1701.06659.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     &lt;i&gt;Lin T Y&lt;/i&gt;,&lt;i&gt;Goyal P&lt;/i&gt;,&lt;i&gt;Girshick R&lt;/i&gt;,et al.&lt;i&gt;Focal loss for dense object detection&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2017 &lt;i&gt;IEEE International Conference on Computer Vision&lt;/i&gt; (&lt;i&gt;ICCV&lt;/i&gt;),&lt;i&gt;October&lt;/i&gt; 22-29,2017,&lt;i&gt;Venice&lt;/i&gt;,&lt;i&gt;Italy&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2017:2999-3007.</a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_11" title=" &lt;i&gt;Zhang S F&lt;/i&gt;,&lt;i&gt;Wen L Y&lt;/i&gt;,&lt;i&gt;Bian X&lt;/i&gt;,et al.&lt;i&gt;Single&lt;/i&gt;-&lt;i&gt;shot refinement neural network for object detection&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2018 &lt;i&gt;IEEE&lt;/i&gt;/&lt;i&gt;CVF Conference on Computer Vision and Pattern Recognition&lt;/i&gt;,&lt;i&gt;June&lt;/i&gt; 18-23,2018,&lt;i&gt;Salt Lake City&lt;/i&gt;,&lt;i&gt;UT&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2018:4203-4212." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-shot Refinement Neural Network for Object Detection">
                                        <b>[11]</b>
                                         &lt;i&gt;Zhang S F&lt;/i&gt;,&lt;i&gt;Wen L Y&lt;/i&gt;,&lt;i&gt;Bian X&lt;/i&gt;,et al.&lt;i&gt;Single&lt;/i&gt;-&lt;i&gt;shot refinement neural network for object detection&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2018 &lt;i&gt;IEEE&lt;/i&gt;/&lt;i&gt;CVF Conference on Computer Vision and Pattern Recognition&lt;/i&gt;,&lt;i&gt;June&lt;/i&gt; 18-23,2018,&lt;i&gt;Salt Lake City&lt;/i&gt;,&lt;i&gt;UT&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2018:4203-4212.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_12" title=" &lt;i&gt;Chen Z&lt;/i&gt;,&lt;i&gt;Zhang T&lt;/i&gt;,&lt;i&gt;Ouyang C&lt;/i&gt;.&lt;i&gt;End&lt;/i&gt;-&lt;i&gt;to&lt;/i&gt;-&lt;i&gt;end airplane detection using transfer learning in remote sensing images&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Remote Sensing&lt;/i&gt;,2018,10(1):139." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-End Airplane Detection Using Transfer Learning in Remote Sensing Images">
                                        <b>[12]</b>
                                         &lt;i&gt;Chen Z&lt;/i&gt;,&lt;i&gt;Zhang T&lt;/i&gt;,&lt;i&gt;Ouyang C&lt;/i&gt;.&lt;i&gt;End&lt;/i&gt;-&lt;i&gt;to&lt;/i&gt;-&lt;i&gt;end airplane detection using transfer learning in remote sensing images&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Remote Sensing&lt;/i&gt;,2018,10(1):139.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     &lt;i&gt;Liu W&lt;/i&gt;,&lt;i&gt;Anguelov D&lt;/i&gt;,&lt;i&gt;Erhan D&lt;/i&gt;,et al.&lt;i&gt;SSD&lt;/i&gt;:&lt;i&gt;single shot multibox detector&lt;/i&gt;[&lt;i&gt;M&lt;/i&gt;]//&lt;i&gt;Leibe B&lt;/i&gt;,&lt;i&gt;Matas J&lt;/i&gt;,&lt;i&gt;Sebe N&lt;/i&gt;,et al.&lt;i&gt;Computer vision&lt;/i&gt;-&lt;i&gt;ECCV&lt;/i&gt; 2016.&lt;i&gt;Lecture notes in computer science&lt;/i&gt;.&lt;i&gt;Cham&lt;/i&gt;:&lt;i&gt;Springer&lt;/i&gt;,2016,9905:21-37.</a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_14" title=" &lt;i&gt;Xia G S&lt;/i&gt;,&lt;i&gt;Bai X&lt;/i&gt;,&lt;i&gt;Ding J&lt;/i&gt;,et al.&lt;i&gt;DOTA&lt;/i&gt;:&lt;i&gt;a large&lt;/i&gt;-&lt;i&gt;scale dataset for object detection in aerial images&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2018 &lt;i&gt;IEEE&lt;/i&gt;/&lt;i&gt;CVF Conference on Computer Vision and Pattern Recognition&lt;/i&gt;,&lt;i&gt;June&lt;/i&gt; 18-23,2018,&lt;i&gt;Salt Lake City&lt;/i&gt;,&lt;i&gt;UT&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2018:3974-3983." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DOTA:A Largescale Dataset for Object Detection in Aerial Images">
                                        <b>[14]</b>
                                         &lt;i&gt;Xia G S&lt;/i&gt;,&lt;i&gt;Bai X&lt;/i&gt;,&lt;i&gt;Ding J&lt;/i&gt;,et al.&lt;i&gt;DOTA&lt;/i&gt;:&lt;i&gt;a large&lt;/i&gt;-&lt;i&gt;scale dataset for object detection in aerial images&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2018 &lt;i&gt;IEEE&lt;/i&gt;/&lt;i&gt;CVF Conference on Computer Vision and Pattern Recognition&lt;/i&gt;,&lt;i&gt;June&lt;/i&gt; 18-23,2018,&lt;i&gt;Salt Lake City&lt;/i&gt;,&lt;i&gt;UT&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2018:3974-3983.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     &lt;i&gt;Redmon J&lt;/i&gt;,&lt;i&gt;Farhadi A&lt;/i&gt;.&lt;i&gt;YOLO&lt;/i&gt;9000:&lt;i&gt;better&lt;/i&gt;,&lt;i&gt;faster&lt;/i&gt;,&lt;i&gt;stronger&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2017 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;July&lt;/i&gt; 21-26,2017,&lt;i&gt;Honolulu&lt;/i&gt;,&lt;i&gt;HI&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2017:6517-6525.</a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_16" title=" &lt;i&gt;Ren S Q&lt;/i&gt;,&lt;i&gt;He K M&lt;/i&gt;,&lt;i&gt;Girshick R&lt;/i&gt;,et al.&lt;i&gt;Faster R&lt;/i&gt;-&lt;i&gt;CNN&lt;/i&gt;:&lt;i&gt;towards real&lt;/i&gt;-&lt;i&gt;time object detection with region proposal&lt;/i&gt;&lt;i&gt;networks&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//&lt;i&gt;Advances in Neural Information&lt;/i&gt;&lt;i&gt;Processing Systems&lt;/i&gt; 28(&lt;i&gt;NIPS&lt;/i&gt; 2015),&lt;i&gt;December&lt;/i&gt; 7-12,2015,&lt;i&gt;Palais des Congr&lt;/i&gt;&#232;&lt;i&gt;s de Montr&lt;/i&gt;&#233;&lt;i&gt;al&lt;/i&gt;,&lt;i&gt;Montr&lt;/i&gt;&#233;&lt;i&gt;al Canada&lt;/i&gt;.&lt;i&gt;Canada&lt;/i&gt;:&lt;i&gt;NIPS&lt;/i&gt;,2015:91-99." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards real-time object detection with region proposal networks">
                                        <b>[16]</b>
                                         &lt;i&gt;Ren S Q&lt;/i&gt;,&lt;i&gt;He K M&lt;/i&gt;,&lt;i&gt;Girshick R&lt;/i&gt;,et al.&lt;i&gt;Faster R&lt;/i&gt;-&lt;i&gt;CNN&lt;/i&gt;:&lt;i&gt;towards real&lt;/i&gt;-&lt;i&gt;time object detection with region proposal&lt;/i&gt;&lt;i&gt;networks&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//&lt;i&gt;Advances in Neural Information&lt;/i&gt;&lt;i&gt;Processing Systems&lt;/i&gt; 28(&lt;i&gt;NIPS&lt;/i&gt; 2015),&lt;i&gt;December&lt;/i&gt; 7-12,2015,&lt;i&gt;Palais des Congr&lt;/i&gt;&#232;&lt;i&gt;s de Montr&lt;/i&gt;&#233;&lt;i&gt;al&lt;/i&gt;,&lt;i&gt;Montr&lt;/i&gt;&#233;&lt;i&gt;al Canada&lt;/i&gt;.&lt;i&gt;Canada&lt;/i&gt;:&lt;i&gt;NIPS&lt;/i&gt;,2015:91-99.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_17" title=" &lt;i&gt;He K M&lt;/i&gt;,&lt;i&gt;Gkioxari G&lt;/i&gt;,&lt;i&gt;Dollar P&lt;/i&gt;,et al.&lt;i&gt;Mask R&lt;/i&gt;-&lt;i&gt;CNN&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]∥2017 &lt;i&gt;IEEE International Conference on Computer Vision&lt;/i&gt; (&lt;i&gt;ICCV&lt;/i&gt;),&lt;i&gt;October&lt;/i&gt; 22-29,2017,&lt;i&gt;Venice&lt;/i&gt;,&lt;i&gt;Italy&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2017:2980-2988." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">
                                        <b>[17]</b>
                                         &lt;i&gt;He K M&lt;/i&gt;,&lt;i&gt;Gkioxari G&lt;/i&gt;,&lt;i&gt;Dollar P&lt;/i&gt;,et al.&lt;i&gt;Mask R&lt;/i&gt;-&lt;i&gt;CNN&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]∥2017 &lt;i&gt;IEEE International Conference on Computer Vision&lt;/i&gt; (&lt;i&gt;ICCV&lt;/i&gt;),&lt;i&gt;October&lt;/i&gt; 22-29,2017,&lt;i&gt;Venice&lt;/i&gt;,&lt;i&gt;Italy&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2017:2980-2988.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_18" title=" &lt;i&gt;Lin T Y&lt;/i&gt;,&lt;i&gt;Dollar P&lt;/i&gt;,&lt;i&gt;Girshick R&lt;/i&gt;,et al.&lt;i&gt;Feature pyramid networks for object detection&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]∥2017 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;July&lt;/i&gt; 21-26,2017,&lt;i&gt;Honolulu&lt;/i&gt;,&lt;i&gt;HI&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2017:936-944." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature pyramid networks for object detection">
                                        <b>[18]</b>
                                         &lt;i&gt;Lin T Y&lt;/i&gt;,&lt;i&gt;Dollar P&lt;/i&gt;,&lt;i&gt;Girshick R&lt;/i&gt;,et al.&lt;i&gt;Feature pyramid networks for object detection&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]∥2017 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;July&lt;/i&gt; 21-26,2017,&lt;i&gt;Honolulu&lt;/i&gt;,&lt;i&gt;HI&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2017:936-944.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_19" title=" &lt;i&gt;Cai Z W&lt;/i&gt;,&lt;i&gt;Vasconcelos N&lt;/i&gt;.&lt;i&gt;Cascade R&lt;/i&gt;-&lt;i&gt;CNN&lt;/i&gt;:&lt;i&gt;delving into high quality object detection&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2018 &lt;i&gt;IEEE&lt;/i&gt;/&lt;i&gt;CVF Conference on Computer Vision and Pattern Recognition&lt;/i&gt;,&lt;i&gt;June&lt;/i&gt; 18-23,2018,&lt;i&gt;Salt Lake City&lt;/i&gt;,&lt;i&gt;UT&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2018:6154-6162." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cascade R-CNN:delving into high quality object detection">
                                        <b>[19]</b>
                                         &lt;i&gt;Cai Z W&lt;/i&gt;,&lt;i&gt;Vasconcelos N&lt;/i&gt;.&lt;i&gt;Cascade R&lt;/i&gt;-&lt;i&gt;CNN&lt;/i&gt;:&lt;i&gt;delving into high quality object detection&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2018 &lt;i&gt;IEEE&lt;/i&gt;/&lt;i&gt;CVF Conference on Computer Vision and Pattern Recognition&lt;/i&gt;,&lt;i&gt;June&lt;/i&gt; 18-23,2018,&lt;i&gt;Salt Lake City&lt;/i&gt;,&lt;i&gt;UT&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2018:6154-6162.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_20" title=" &lt;i&gt;Han X B&lt;/i&gt;,&lt;i&gt;Zhong Y F&lt;/i&gt;,&lt;i&gt;Zhang L P&lt;/i&gt;.&lt;i&gt;An efficient and robust integrated geospatial object detection framework for high spatial resolution remote sensing imagery&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Remote Sensing&lt;/i&gt;,2017,9(7):666." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Efficient and Robust Integrated Geospatial Object Detection Framework for High Spatial Resolution Remote Sensing Imagery">
                                        <b>[20]</b>
                                         &lt;i&gt;Han X B&lt;/i&gt;,&lt;i&gt;Zhong Y F&lt;/i&gt;,&lt;i&gt;Zhang L P&lt;/i&gt;.&lt;i&gt;An efficient and robust integrated geospatial object detection framework for high spatial resolution remote sensing imagery&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Remote Sensing&lt;/i&gt;,2017,9(7):666.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_21" title=" &lt;i&gt;Ren Z J&lt;/i&gt;,&lt;i&gt;Lin S Z&lt;/i&gt;,&lt;i&gt;Li D W&lt;/i&gt;,et al.&lt;i&gt;Mask R&lt;/i&gt;-&lt;i&gt;CNN object detection method based on improved feature pyramid&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Laser&lt;/i&gt; &amp;amp; &lt;i&gt;Optoelectronics Progress&lt;/i&gt;,2019,56(4):041502.任之俊,蔺素珍,李大威,等.基于改进特征金字塔的&lt;i&gt;Mask R&lt;/i&gt;-&lt;i&gt;CNN&lt;/i&gt;目标检测方法[&lt;i&gt;J&lt;/i&gt;].激光与光电子学进展,2019,56(4):041502." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201904019&amp;v=MDg3NTJGeXZoV3IvQUx5clBaTEc0SDlqTXE0OUViWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         &lt;i&gt;Ren Z J&lt;/i&gt;,&lt;i&gt;Lin S Z&lt;/i&gt;,&lt;i&gt;Li D W&lt;/i&gt;,et al.&lt;i&gt;Mask R&lt;/i&gt;-&lt;i&gt;CNN object detection method based on improved feature pyramid&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Laser&lt;/i&gt; &amp;amp; &lt;i&gt;Optoelectronics Progress&lt;/i&gt;,2019,56(4):041502.任之俊,蔺素珍,李大威,等.基于改进特征金字塔的&lt;i&gt;Mask R&lt;/i&gt;-&lt;i&gt;CNN&lt;/i&gt;目标检测方法[&lt;i&gt;J&lt;/i&gt;].激光与光电子学进展,2019,56(4):041502.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_22" title=" &lt;i&gt;Zhu M M&lt;/i&gt;,&lt;i&gt;Xu Y L&lt;/i&gt;,&lt;i&gt;Ma S P&lt;/i&gt;,et al.&lt;i&gt;Airport detection method with improved region&lt;/i&gt;-&lt;i&gt;based convolutional neural network&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Optica Sinica&lt;/i&gt;,2018,38(7):0728001.朱明明,许悦雷,马时平,等.改进区域卷积神经网络的机场检测方法[&lt;i&gt;J&lt;/i&gt;].光学学报,2018,38(7):0728001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201807042&amp;v=MTU4MjFyQ1VSTE9lWmVWdkZ5dmhXci9BSWpYVGJMRzRIOW5NcUk5QlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         &lt;i&gt;Zhu M M&lt;/i&gt;,&lt;i&gt;Xu Y L&lt;/i&gt;,&lt;i&gt;Ma S P&lt;/i&gt;,et al.&lt;i&gt;Airport detection method with improved region&lt;/i&gt;-&lt;i&gt;based convolutional neural network&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Optica Sinica&lt;/i&gt;,2018,38(7):0728001.朱明明,许悦雷,马时平,等.改进区域卷积神经网络的机场检测方法[&lt;i&gt;J&lt;/i&gt;].光学学报,2018,38(7):0728001.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_23" title=" &lt;i&gt;Long J&lt;/i&gt;,&lt;i&gt;Shelhamer E&lt;/i&gt;,&lt;i&gt;Darrell T&lt;/i&gt;.&lt;i&gt;Fully convolutional networks for semantic segmentation&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2015 &lt;i&gt;IEEE&lt;/i&gt;&lt;i&gt;Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;June&lt;/i&gt; 7-12,2015,&lt;i&gt;Boston&lt;/i&gt;,&lt;i&gt;MA&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2015:3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[23]</b>
                                         &lt;i&gt;Long J&lt;/i&gt;,&lt;i&gt;Shelhamer E&lt;/i&gt;,&lt;i&gt;Darrell T&lt;/i&gt;.&lt;i&gt;Fully convolutional networks for semantic segmentation&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2015 &lt;i&gt;IEEE&lt;/i&gt;&lt;i&gt;Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;June&lt;/i&gt; 7-12,2015,&lt;i&gt;Boston&lt;/i&gt;,&lt;i&gt;MA&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2015:3431-3440.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_24" title=" &lt;i&gt;Chen L C&lt;/i&gt;,&lt;i&gt;Papandreou G&lt;/i&gt;,&lt;i&gt;Kokkinos I&lt;/i&gt;,et al.&lt;i&gt;Semantic image segmentation with deep convolutional nets and fully connected CRFs&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].(2016-06-07)[2019-04-07].&lt;i&gt;https&lt;/i&gt;://&lt;i&gt;arxiv&lt;/i&gt;.&lt;i&gt;org&lt;/i&gt;/&lt;i&gt;abs&lt;/i&gt;/1412.7062." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic image segmentation with deep convolutional nets and fully connected crfs">
                                        <b>[24]</b>
                                         &lt;i&gt;Chen L C&lt;/i&gt;,&lt;i&gt;Papandreou G&lt;/i&gt;,&lt;i&gt;Kokkinos I&lt;/i&gt;,et al.&lt;i&gt;Semantic image segmentation with deep convolutional nets and fully connected CRFs&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].(2016-06-07)[2019-04-07].&lt;i&gt;https&lt;/i&gt;://&lt;i&gt;arxiv&lt;/i&gt;.&lt;i&gt;org&lt;/i&gt;/&lt;i&gt;abs&lt;/i&gt;/1412.7062.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_25" title=" &lt;i&gt;Cheng G&lt;/i&gt;,&lt;i&gt;Zhou P C&lt;/i&gt;,&lt;i&gt;Han J W&lt;/i&gt;.&lt;i&gt;Learning rotation&lt;/i&gt;-&lt;i&gt;invariant convolutional neural networks for object detection in VHR optical remote sensing images&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Geoscience and Remote Sensing&lt;/i&gt;,2016,54(12):7405-7415." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR OpticalRemote Sensing Images">
                                        <b>[25]</b>
                                         &lt;i&gt;Cheng G&lt;/i&gt;,&lt;i&gt;Zhou P C&lt;/i&gt;,&lt;i&gt;Han J W&lt;/i&gt;.&lt;i&gt;Learning rotation&lt;/i&gt;-&lt;i&gt;invariant convolutional neural networks for object detection in VHR optical remote sensing images&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Geoscience and Remote Sensing&lt;/i&gt;,2016,54(12):7405-7415.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_26" title=" &lt;i&gt;Dai J F&lt;/i&gt;,&lt;i&gt;Li Y&lt;/i&gt;,&lt;i&gt;He K M&lt;/i&gt;,et al.&lt;i&gt;R&lt;/i&gt;-&lt;i&gt;FCN&lt;/i&gt;:&lt;i&gt;object detection via region&lt;/i&gt;-&lt;i&gt;based&lt;/i&gt;&lt;i&gt;fully convolutional networks&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]∥&lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt; 29(&lt;i&gt;NIPS&lt;/i&gt; 2016),&lt;i&gt;December&lt;/i&gt; 5-10,2016,&lt;i&gt;Centre Convencions Internacional Barcelona&lt;/i&gt;,&lt;i&gt;Barcelona Spain&lt;/i&gt;.&lt;i&gt;Canada&lt;/i&gt;:&lt;i&gt;NIPS&lt;/i&gt;,2016:379-387." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=R-fcn:Object detection via region-based fully convolutional networks">
                                        <b>[26]</b>
                                         &lt;i&gt;Dai J F&lt;/i&gt;,&lt;i&gt;Li Y&lt;/i&gt;,&lt;i&gt;He K M&lt;/i&gt;,et al.&lt;i&gt;R&lt;/i&gt;-&lt;i&gt;FCN&lt;/i&gt;:&lt;i&gt;object detection via region&lt;/i&gt;-&lt;i&gt;based&lt;/i&gt;&lt;i&gt;fully convolutional networks&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]∥&lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt; 29(&lt;i&gt;NIPS&lt;/i&gt; 2016),&lt;i&gt;December&lt;/i&gt; 5-10,2016,&lt;i&gt;Centre Convencions Internacional Barcelona&lt;/i&gt;,&lt;i&gt;Barcelona Spain&lt;/i&gt;.&lt;i&gt;Canada&lt;/i&gt;:&lt;i&gt;NIPS&lt;/i&gt;,2016:379-387.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_27" title=" &lt;i&gt;Dai J F&lt;/i&gt;,&lt;i&gt;Qi H Z&lt;/i&gt;,&lt;i&gt;Xiong Y W&lt;/i&gt;,et al.&lt;i&gt;Deformable convolutional networks&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2017 &lt;i&gt;IEEE International Conference on Computer Vision&lt;/i&gt; (&lt;i&gt;ICCV&lt;/i&gt;),&lt;i&gt;October&lt;/i&gt; 22-29,2017,&lt;i&gt;Venice&lt;/i&gt;,&lt;i&gt;Italy&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2017:764-773." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deformable Convolutional Networks">
                                        <b>[27]</b>
                                         &lt;i&gt;Dai J F&lt;/i&gt;,&lt;i&gt;Qi H Z&lt;/i&gt;,&lt;i&gt;Xiong Y W&lt;/i&gt;,et al.&lt;i&gt;Deformable convolutional networks&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2017 &lt;i&gt;IEEE International Conference on Computer Vision&lt;/i&gt; (&lt;i&gt;ICCV&lt;/i&gt;),&lt;i&gt;October&lt;/i&gt; 22-29,2017,&lt;i&gt;Venice&lt;/i&gt;,&lt;i&gt;Italy&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2017:764-773.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_28" title=" &lt;i&gt;Deng Z P&lt;/i&gt;,&lt;i&gt;Sun H&lt;/i&gt;,&lt;i&gt;Lei L&lt;/i&gt;,et al.&lt;i&gt;Object detection in remote sensing imagery with multi&lt;/i&gt;-&lt;i&gt;scale deformable convolutional networks&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Geodaetica et Cartographica Sinica&lt;/i&gt;,2018,47(9):1216-1227.邓志鹏,孙浩,雷琳,等.基于多尺度形变特征卷积网络的高分辨率遥感影像目标检测[&lt;i&gt;J&lt;/i&gt;].测绘学报,2018,47(9):1216-1227." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201809008&amp;v=MTcyOTZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXZoV3IvQUppWFRiTEc0SDluTXBvOUY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                         &lt;i&gt;Deng Z P&lt;/i&gt;,&lt;i&gt;Sun H&lt;/i&gt;,&lt;i&gt;Lei L&lt;/i&gt;,et al.&lt;i&gt;Object detection in remote sensing imagery with multi&lt;/i&gt;-&lt;i&gt;scale deformable convolutional networks&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Geodaetica et Cartographica Sinica&lt;/i&gt;,2018,47(9):1216-1227.邓志鹏,孙浩,雷琳,等.基于多尺度形变特征卷积网络的高分辨率遥感影像目标检测[&lt;i&gt;J&lt;/i&gt;].测绘学报,2018,47(9):1216-1227.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-07-29 09:22</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(11),346-353 DOI:10.3788/AOS201939.1128002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于多尺度卷积神经网络的遥感目标检测研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9A%E7%BE%A4%E5%8A%9B&amp;code=39904532&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姚群力</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E6%98%BE&amp;code=39904534&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡显</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%9B%B7%E5%AE%8F&amp;code=09542040&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">雷宏</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E7%94%B5%E5%AD%90%E5%AD%A6%E7%A0%94%E7%A9%B6%E6%89%80%E8%88%AA%E5%A4%A9%E5%BE%AE%E6%B3%A2%E9%81%A5%E6%84%9F%E7%B3%BB%E7%BB%9F%E9%83%A8&amp;code=0227399&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院电子学研究所航天微波遥感系统部</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E7%94%B5%E6%B0%94%E4%B8%8E%E9%80%9A%E4%BF%A1%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学电子电气与通信工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对现有遥感图像目标检测算法对于复杂场景下多尺度目标检测精度较低、泛化能力差的问题,提出了一种多尺度卷积神经网络遥感目标检测框架——MSCNN。该方法通过构造一种深度特征金字塔,增强了网络对多尺度目标特征的提取能力;引入聚焦分类损失作为分类损失函数,加强了网络对难样本的学习能力。所提方法在NWPU VHR-10公开数据集上取得了0.960的平均检测精度(mAP),相较于RetinaNet检测框架,MSCNN对小尺度以及中等尺度目标的平均检测精度分别提高了1.5%和1.9%,实现了对多尺度目标的高精度稳健检测。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%A5%E6%84%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遥感;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%9B%AE%E6%A0%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度目标;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    姚群力,E-mail:yaoqunli15@mails.ucas.ac.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-08</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金青年基金(61422113,61601437);</span>
                                <span>国家重点研发计划(2017YFB0502700);</span>
                    </p>
            </div>
                    <h1><b>Object Detection in Remote Sensing Images Using Multiscale Convolutional Neural Networks</b></h1>
                    <h2>
                    <span>Yao Qunli</span>
                    <span>Hu Xian</span>
                    <span>Lei Hong</span>
            </h2>
                    <h2>
                    <span>Department of Space Microwave Remote Sensing Systems, Institute of Electronics,Chinese Academy of Sciences</span>
                    <span>School of Electronics, Electrical and Communication Engineering,University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A novel detection framework is proposed based on a multiscale convolutional neural network(MSCNN) to overcome low precision and insufficient generalization ability associated with existing object detection methods for multiscale objects with complex scenes. First, an essence feature pyramid network is constructed to enhance the extraction ability of multiscale features. Then, the focal classification loss is introduced as classification loss function to enhance the learning capability of the MSCNN over complex samples. The proposed method achieves a mean average precision(mAP) of 0.960 over the challenging NWPU VHR-10 dataset. In comparison with the RetinaNet detection method, the mAP of the proposed MSCNN on small-and medium-scale objects increases by 1.5% and 1.9%, respectively. The proposed method is found to be accurate and robust for multiscale objects.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=remote%20sensing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">remote sensing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=object%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">object detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20networks&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural networks;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale%20objects&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale objects;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-04-08</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="65" name="65" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="66">遥感目标自动检测技术不仅是一种实现遥感目标自动分类和定位的智能化数据分析方法,还是遥感图像解译领域的重要研究方向之一<citation id="122" type="reference"><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。遥感目标自动检测在城市规划、交通安全和环境监测等方面发挥着重要作用。然而,在遥感目标检测中,除了要考虑诸如光照、遮挡和几何形变等客观因素外,目标实例的多尺度性、密集分布和形状差异等亦是需要重点考虑的。因此,复杂场景下多尺度的地理空间目标检测需要更具体的探测。</p>
                </div>
                <div class="p1">
                    <p id="67">传统的遥感图像目标检测方法,如尺度不变特征变换(SIFT)<citation id="123" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、梯度方向直方图(HOG)<citation id="124" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>,以及可变形部件模型(DPM)<citation id="125" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等,都是根据人工经验设计特征的,虽然在特定的应用场景下能取得较好的检测效果,但该类方法对先验知识的依赖性强,导致检测模型的自适应性与泛化能力较差。</p>
                </div>
                <div class="p1">
                    <p id="68">深度卷积神经网络(Deep ConvNets)能够从数据中主动学习特征,且不依赖于人工经验,为遥感图像目标检测研究提供了一种新的思路和框架。基于Deep ConvNets的目标检测模型在研究理念上可以分为两类:1)基于回归的Single-stage目标检测框架(如DSSD<citation id="126" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、RetinaNet<citation id="127" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,以及RefineDet<citation id="128" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>等)。该类检测方法从回归的角度出发,直接在图像上回归出目标的边框位置和物体类别。如Chen等<citation id="129" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>在SSD(Single Short MultiBox Detector)<citation id="130" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>检测框架的基础上提出了一种输入图像尺度可变的检测方法,Xia等<citation id="131" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了基于YOLOv2<citation id="132" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>的遥感图像目标检测。但该类方法对于多尺度遥感目标的检测精度较差。2)基于区域建议的Two-stage目标探测框架(如Faster R-CNN<citation id="133" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、Mask RCNN<citation id="134" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、FPN<citation id="135" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,以及Cascade RCNN<citation id="136" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>等)。该类方法基于原始图像生成一系列区域建议,并将区域建议和特征图输入感兴趣区域池化(ROI Pooling)层,最终实现目标的分类与定位。如Han等<citation id="137" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>基于Faster R-CNN<citation id="138" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>体系结构提出的R-P-Faster R-CNN,不仅实现了端到端的训练与检测,还提高了模型的检测精度和效率。Deng等<citation id="139" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出了一种基于多尺度目标建议网络(MS-OPN)和高精度目标检测网络(AODN)的多尺度遥感目标检测框架,进一步提高了多尺度目标的检测精度。任之俊等<citation id="140" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提出了一种基于改进特征金字塔的MaskR-CNN目标检测方法,该方法有效提高了中小尺度目标的检测精度。朱明明等<citation id="141" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>也提出了基于级联区域建议网络的机场目标检测算法。但该类算法的高计算成本限制了目标检测的速度。此外,在FPN体系结构中,较大的步长易造成小尺度目标在深层特征图中丢失的现象,进而导致上下文线索被削弱。基于此,本文先设计了一种多尺度特征金字塔网络EFPN(Essence Feature Pyramid Networks)。通过控制基础网络的采样步长,引入膨胀瓶颈结构,增强了网络对多尺度特征的表达能力。引入了聚焦分类损失作为分类损失函数,加强了网络对遥感图像中难样本的学习能力,并通过极小化多任务损失函数端到端优化整个网络。本文基于EFPN构造了一种基于多尺度卷积神经网络的遥感目标检测框架(MSCNN),有效地提高了多尺度遥感目标的预测能力。</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag">2 MSCNN检测框架</h3>
                <h4 class="anchor-tag" id="70" name="70"><b>2.1 MSCNN网络结构</b></h4>
                <div class="p1">
                    <p id="71">本文以RetinaNet目标检测网络作为基线结构,提出了一种多尺度目标检测框架——MSCNN,整体结构框架如图1所示,其中A为膨胀瓶颈结构,B为带有1×1卷积的膨胀瓶颈结构,<i>W</i>、<i>H</i>为卷积核尺度。首先以ResNet-50作为基础网络构造了一种新的特征金字塔网络EFPN,生成了新的融合预测特征,提高了传统特征提取网络对多尺度特征的表达能力。之后通过目标检测子网络,分别给出多尺度目标的分类得分和边框位置。最后再采用非极大值抑制原理将相似结果进行合并,输出最终检测结果。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911042_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 MSCNN目标检测框架结构" src="Detail/GetImg?filename=images/GXXB201911042_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 MSCNN目标检测框架结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911042_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Target detection framework of MSCNN</p>

                </div>
                <h4 class="anchor-tag" id="73" name="73"><b>2.2 EFPN</b></h4>
                <div class="p1">
                    <p id="74">本文以ResNet-50作为基线,构建了EFPN。为了构建自底向上的通路,从主干中选取卷积块{<i>C</i><sub>3</sub>,<i>C</i><sub>4</sub>,<i>C</i><sub>5</sub>}作为基础层级结构。添加特征映射<i>C</i><sub>6</sub>和<i>C</i><sub>7</sub>,以获得更精确的语义信息,特征层<i>C</i><sub>6</sub>和<i>C</i><sub>7</sub>的计算式为</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>C</mi><msub><mrow></mrow><mn>6</mn></msub><mo>=</mo><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>2</mn><mtext>D</mtext><mo stretchy="false">[</mo><mi>k</mi><mo>=</mo><mn>2</mn><mn>5</mn><mn>6</mn><mo>,</mo><mi>s</mi><mo>=</mo><mo stretchy="false">(</mo><mn>3</mn><mo>,</mo><mn>3</mn><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>C</mi><msub><mrow></mrow><mn>5</mn></msub><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>C</mi><msub><mrow></mrow><mn>7</mn></msub><mo>=</mo><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>2</mn><mtext>D</mtext><mo stretchy="false">[</mo><mi>k</mi><mo>=</mo><mn>2</mn><mn>5</mn><mn>6</mn><mo>,</mo><mi>s</mi><mo>=</mo><mo stretchy="false">(</mo><mn>3</mn><mo>,</mo><mn>3</mn><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mtext>R</mtext><mtext>e</mtext><mtext>L</mtext><mtext>U</mtext><mo stretchy="false">(</mo><mi>C</mi><msub><mrow></mrow><mn>6</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">式中:Conv2D为二维卷积算子,它将给定的特征图与预定义的卷积核进行卷积;<i>k</i>为卷积核的数量;<i>s</i>为内核的尺度;ReLU为激活函数。因此,通过自底向上的路径生成了特征图{<i>C</i><sub>3</sub>,<i>C</i><sub>4</sub>,<i>C</i><sub>5</sub>,<i>C</i><sub>6</sub>,<i>C</i><sub>7</sub>}。事实上,为了进一步提高网络的特征表达能力,如图2(a)所示,本文在ResNet-50的第4阶段以后,首先将步长锁定为16×,然后将通道维度控制为256,接着在每个阶段之后均部署了一个低复杂度的膨胀瓶颈结构<citation id="142" type="reference"><link href="53" rel="bibliography" /><link href="55" rel="bibliography" /><sup>[<a class="sup">23</a>,<a class="sup">24</a>]</sup></citation>,如图2(c)所示,以保障网络能生成更深的特征图,且能保持较高的分辨率。最后通过自上而下的通路,构建出了特征金字塔网络层级{<i>P</i>3,<i>P</i>4,<i>P</i>5,<i>P</i>6,<i>P</i>7}。新的特征金字塔EFPN将包含<i>P</i>3～<i>P</i>7共5层预测层。这些特征层级的计算方法为</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo>=</mo><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>2</mn><mtext>D</mtext><mo stretchy="false">[</mo><mi>k</mi><mo>=</mo><mn>2</mn><mn>5</mn><mn>6</mn><mo>,</mo><mi>s</mi><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mspace width="0.25em" /><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>C</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><msup><mi>Τ</mi><mo>′</mo></msup><msub><mrow></mrow><mi>Ν</mi></msub><mo>=</mo><mi>R</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>Ν</mi><mo>+</mo><mn>1</mn></mrow><mn>7</mn></munderover><mi>U</mi></mstyle><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>,</mo><mi>C</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo>=</mo><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>2</mn><mtext>D</mtext><mo stretchy="false">[</mo><mi>k</mi><mo>=</mo><mn>2</mn><mn>5</mn><mn>6</mn><mo>,</mo><mi>s</mi><mo>=</mo><mo stretchy="false">(</mo><mn>3</mn><mo>,</mo><mspace width="0.25em" /><mn>3</mn><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo stretchy="false">(</mo><msup><mi>Τ</mi><mo>′</mo></msup><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">)</mo><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">式中:序数<i>N</i>=3,4,5,6,7;序数<i>N</i>′=3,4,5,6,且<i>j</i>为由<i>N</i>生成的索引;<i>R</i><sub><i>N</i></sub>为自底向上路径经过卷积降维得到的特征层;<i>T</i><sub><i>N</i></sub>为经过特征堆叠得到的新特征;<i>U</i>(·)为将<i>T</i><sub><i>j</i></sub>的尺度调整到<i>C</i><sub><i>N</i></sub>大小的算子;<i>P</i><sub><i>N</i></sub>为自顶向下构造的输出特征。所有的预测特征最后都将被送入检测网络进行多尺度目标预测。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911042_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 EFPN网络构型和膨胀瓶颈结构。" src="Detail/GetImg?filename=images/GXXB201911042_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 EFPN网络构型和膨胀瓶颈结构。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911042_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Structures of EFPN and dilated bottleneck. </p>
                                <p class="img_note">(a) EFPN模块结构;(b)膨胀瓶颈结构;(c)带有1×1卷积的膨胀瓶颈结构</p>
                                <p class="img_note">(a) Structure of EFPN module; (b) dilated bottleneck structure; 
(c) dilated bottleneck structure with 1×1 Conv</p>

                </div>
                <h4 class="anchor-tag" id="80" name="80"><b>2.3 损失函数设计</b></h4>
                <div class="p1">
                    <p id="81">在实际的遥感影像数据中,正、负样本的不平衡现象普遍存在。然而,极端的正、负样本不平衡将会导致网络训练过程中正样本在数据中占比较少,从而使网络训练效率降低,使得一些难以学习的正样本得不到充分的学习,严重制约了检测网络在遥感目标检测任务中的检测性能。为此,本文引入聚焦分类损失<citation id="143" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,通过动态缩放交叉熵,快速地将模型的训练集中在难样本稀疏集上,用以加强网络对于遥感图像中难样本的学习和挖掘。本文所提出的多任务联合损失函数为</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>p</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>,</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">t</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>L</mi></mstyle><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>p</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>α</mi><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>p</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">t</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">式中:<i>N</i><sub>cls</sub>为批量尺度;<i>N</i><sub>loc</sub>为锚点框数量;<i>α</i>为平衡超参数;<i>p</i><sup>*</sup><sub><i>i</i></sub>为目标的真实类别标签;<i>p</i><sub><i>i</i></sub>为预测相应类别的概率;<i><b>t</b></i><sub><i>i</i></sub>为预测的4个参数化坐标向量;<i><b>t</b></i><sup>*</sup><sub><i>i</i></sub>为真实边框参数;<i>i</i>为批量数据中锚点框的索引;<i>L</i><sub>fl</sub>和<i>L</i><sub>loc</sub>分别为聚焦分类和边框回归损失,其中<i>L</i><sub>fl</sub>可定义为</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mtext>Τ</mtext></msub><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mi>α</mi><msub><mrow></mrow><mtext>t</mtext></msub><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>p</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mi>γ</mi></msup><mrow><mi>log</mi></mrow><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">式中:<i>α</i><sub>t</sub>∈[0,1],为平衡超参数;(1-<i>p</i><sub>t</sub>)<sup><i>γ</i></sup>为调制因子,<i>p</i><sub>t</sub>为前景目标对应的预测概率;<i>γ</i>∈[0,5],为超参数,当<i>γ</i>&gt;0时意味着模型将更专注于难样本的训练。<i>L</i><sub>loc</sub>通常采用Smooth L1损失,其表达式为</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext></mrow></msub><mo stretchy="false">(</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">t</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mi>L</mi><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">t</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>s</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mi>L</mi><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>0</mn><mo>.</mo><mn>5</mn><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>,</mo></mtd><mtd columnalign="left"><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left"><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow><mo>-</mo><mn>0</mn><mo>.</mo><mn>5</mn><mo>,</mo></mtd><mtd columnalign="left"><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h3 id="87" name="87" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="88" name="88"><b>3.1 数据集与评价指标</b></h4>
                <div class="p1">
                    <p id="89">本文在NWPU VHR-10<citation id="144" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>公开数据集上进行多尺度目标检测实验。NWPU VHR-10数据集是一个用于多类多尺度目标检测的地理空间目标检测公开数据集。该数据集共包含650幅光学遥感图像,平均尺度约为600×800,共标注了757架飞机、302艘船只、655个油罐、390个棒球场、524个网球场、159个篮球场、163个田径场、224个港口、124座桥梁,以及477辆车。图像分辨率在0.5～2.0 m之间,每一幅图像至少包含一个目标。</p>
                </div>
                <div class="p1">
                    <p id="90">为了避免过拟合,本文采用旋转、翻转进行数据扩充。旋转变换采用的步长为90°,将图像从0°旋转到270°进行数据扩充。翻转变换是分别对原图进行水平翻转和垂直翻转。在训练过程中,本文将数据集随机分配60%用于训练,余下的40%用于测试。根据NWPU VHR-10数据分布信息统计,定义了相应的边界框尺度量(small, medium, and large),如表1所示。</p>
                </div>
                <div class="area_img" id="91">
                    <p class="img_tit">表1 基于实例尺度分布的边界框面积定义方法 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Definition of bounding box areas based on distribution of instance scales</p>
                    <p class="img_note"></p>
                    <table id="91" border="1"><tr><td><br />Small</td><td>Medium</td><td>Large</td></tr><tr><td><br />(0,60]</td><td>(60,120]</td><td>(120,+∞)</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="92">本文采用平均检测精度(mAP)作为目标检测的评价指标,该指标衡量了所有类别的检测精度的均值。事实上,AP计算的是召回率(AR)在0到1区间内精度的平均值,即精度-召回率曲线所包络的面积,因此AP值越高代表检测性能越好。精度<i>p</i>和召回率<i>r</i>指标可以表示为</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><msub><mrow></mrow><mtext>Ρ</mtext></msub></mrow><mrow><mi>Τ</mi><msub><mrow></mrow><mtext>Ρ</mtext></msub><mo>+</mo><mi>F</mi><msub><mrow></mrow><mtext>Ρ</mtext></msub></mrow></mfrac><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>r</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><msub><mrow></mrow><mtext>Ρ</mtext></msub></mrow><mrow><mi>Τ</mi><msub><mrow></mrow><mtext>Ρ</mtext></msub><mo>+</mo><mi>F</mi><msub><mrow></mrow><mtext>Ν</mtext></msub></mrow></mfrac><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">式中:<i>T</i><sub>P</sub>、<i>F</i><sub>P</sub>、<i>F</i><sub>N</sub>分别为真正例、假正例,以及假反例。此外,本文还评估了在不同IOU阈值和不同边界框尺度(small、medium,and large)下目标的检测精度和召回率,分析了所提方法对多尺度目标的检测能力。IOU代表了检测框与真值框的交并比,其定义可以表示为</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><msub><mrow></mrow><mrow><mtext>Ο</mtext><mtext>U</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>G</mi><msub><mrow></mrow><mtext>Τ</mtext></msub><mstyle displaystyle="true"><mo>∩</mo><mi>D</mi></mstyle><msub><mrow></mrow><mtext>R</mtext></msub></mrow><mrow><mi>G</mi><msub><mrow></mrow><mtext>Τ</mtext></msub><mstyle displaystyle="true"><mo>∪</mo><mi>D</mi></mstyle><msub><mrow></mrow><mtext>R</mtext></msub></mrow></mfrac><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">式中:<i>G</i><sub>T</sub>为真值框;<i>D</i><sub>R</sub>为检测结果。</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97"><b>3.2 参数设置</b></h4>
                <div class="p1">
                    <p id="98">本文训练和测试采用的硬件平台为NVIDIA Titan Xp GPUs,利用Pytorch开源深度学习框架完成实验的构建。实验过程采用端到端训练方式,初始学习率设置为0.001,优化方法为随机梯度下降,动量设置为0.9,正则化系数设置为0.0005,批处理大小设置为1。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>3.3 主要结果</b></h4>
                <div class="p1">
                    <p id="100">本文所有检测实验均在NWPU VHR-10公开数据集上进行训练和测试,结果如表2所示,表中给出了本文所提方法与当前一些典型目标检测网络的对比结果。</p>
                </div>
                <div class="area_img" id="101">
                    <p class="img_tit">表2 不同算法在NWPU VHR-10数据集上的检测精度对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Comparison of the detection precision of different algorithms on NWPU VHR-10 dataset</p>
                    <p class="img_note"></p>
                    <table id="101" border="1"><tr><td>Method</td><td>RICNN<sup>[25]</sup></td><td>FRCN-VGG-16<sup>[16]</sup></td><td>YOLO<sup>[15]</sup></td><td>SSD<sup>[13]</sup></td><td>R-FCN<sup>[26]</sup></td><td>FRCN-Deform<sup>[27]</sup></td><td>FPN<sup>[18]</sup></td><td>MSDN<sup>[28]</sup></td><td>MSCNN</td></tr><tr><td><br />Airplane</td><td>0.884</td><td>0.830</td><td>0.874</td><td>0.956</td><td>0.961</td><td>0.983</td><td>0.964</td><td><b>0.998</b></td><td>0.994</td></tr><tr><td><br />Ship</td><td>0.773</td><td>0.776</td><td>0.847</td><td>0.937</td><td><b>0.983</b></td><td>0.892</td><td>0.931</td><td>0.972</td><td>0.953</td></tr><tr><td><br />Storage tank</td><td>0.853</td><td>0.525</td><td>0.427</td><td>0.617</td><td>0.725</td><td>0.817</td><td>0.914</td><td>0.838</td><td><b>0.918</b></td></tr><tr><td><br />Baseball diamond</td><td>0.881</td><td>0.963</td><td>0.931</td><td>0.995</td><td><b>0.994</b></td><td>0.984</td><td>0.947</td><td>0.991</td><td>0.963</td></tr><tr><td><br />Tennis court</td><td>0.408</td><td>0.629</td><td>0.658</td><td>0.860</td><td>0.907</td><td>0.859</td><td>0.944</td><td><b>0.973</b></td><td>0.954</td></tr><tr><td><br />Basketball court</td><td>0.585</td><td>0.688</td><td>0.870</td><td>0.944</td><td>0.978</td><td>0.927</td><td>0.959</td><td><b>0.999</b></td><td>0.967</td></tr><tr><td><br />Ground track field</td><td>0.867</td><td>0.984</td><td>0.975</td><td>0.987</td><td>0.981</td><td>0.988</td><td>0.990</td><td>0.986</td><td><b>0.993</b></td></tr><tr><td><br />Harbor</td><td>0.686</td><td>0.819</td><td>0.800</td><td>0.950</td><td>0.924</td><td>0.946</td><td>0.921</td><td><b>0.972</b></td><td>0.955</td></tr><tr><td><br />Bridge</td><td>0.615</td><td>0.793</td><td>0.903</td><td>0.966</td><td>0.934</td><td>0.947</td><td>0.838</td><td>0.927</td><td><b>0.972</b></td></tr><tr><td><br />Vehicle</td><td>0.711</td><td>0.639</td><td>0.704</td><td>0.745</td><td>0.884</td><td>0.816</td><td>0.900</td><td>0.901</td><td><b>0.933</b></td></tr><tr><td><br />mAP</td><td>0.726</td><td>0.764</td><td>0.799</td><td>0.894</td><td>0.928</td><td>0.917</td><td>0.931</td><td>0.956</td><td><b>0.960</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="102">通过分析不同算法的检测精度可知,基于VGG-16的FRCN网络的检测精度相对较低。事实上,FRCN目标检测网络仅基于固定感受野尺度的末端特征图生成预测,然而较大的步长降低了末端特征图的分辨率,导致在复杂背景下FRCN对多尺度数据特征的表征能力严重不足,严重制约了检测器的检测性能。</p>
                </div>
                <div class="p1">
                    <p id="103">相较于YOLO网络,SSD的目标检测精度得到了大幅提高,这再一次证明了多尺度特征金字塔构型能够有效提升网络对多尺度目标的建模能力,从而达到提升多尺度目标检测精度的目的。对比分析FRCN-Deform网络与FRCN-VGG-16网络可知,带有可变形卷积结构和可变形池化结构的FRCN-Deform目标检测网络相较于基于传统的卷积和池化操作的检测网络具有更强的特征建模能力,以及对目标多尺度和形变的适应能力。因此,FRCN-Deform较之FRCN-VGG-16,检测精度也就更高。</p>
                </div>
                <div class="p1">
                    <p id="104">与R-FCN相比较,FPN的检测精度提升了0.3%。事实上,FPN网络构造了多尺度特征金字塔,对多尺度特征进行融合,显著增强了网络对多尺度特征的学习能力。因此,采用FPN网络可显著提升对多尺度遥感目标检测的效果。本文提出的MSCNN对于大多数类别的目标均取得了最佳的检测效果,对于小尺度目标,如:飞机目标,较MSDN检测精度提高了0.4%;油罐目标,较FPN提高了0.4%;车辆目标,较MSDN提高了3.2%。对于较大尺度目标,如:田径场目标,较MSDN检测精度提高了0.7%;桥梁目标,较SSD提高了0.6%。MSCNN在NWPU VHR-10公开数据集上的平均检测精度相较于MSDN提升了0.4%,相较于FPN提升了2.9%。由上述分析可知,本文提出的基于EFPN的MSCNN能有效提高多尺度目标的检测性能。</p>
                </div>
                <h4 class="anchor-tag" id="105" name="105"><b>3.4 消融实验</b></h4>
                <h4 class="anchor-tag" id="106" name="106">1) EFPN</h4>
                <div class="p1">
                    <p id="107">在目标检测框架MSCNN中,EFPN对于提升多尺度目标检测精度具有至关重要的作用。为了证明EFPN组件在检测框架中所发挥的性能,本文设计了一组对比实验,实验结果如表3第2～3行所示。EFPN取得了0.960@AP<sub>50</sub>以及0.824@AP<sub>75</sub>的平均检测精度,相较于RetinaNet取得了1.5%@AP<sub>50</sub>和1.5%@AP<sub>75</sub>的增益,表现出了更高的检测精度。此外,EFPN还取得了0.547@small、0.578@medium和0.701@large的多尺度平均检测精度,相较于RetinaNet取得1.5%@small、1.9%@medium和1.9%@large的增益,表现出了对多尺度目标检测的优越性。因此,在多尺度遥感目标检测中,EFPN组件相较于RetinaNet具有更为明显的优势。</p>
                </div>
                <div class="area_img" id="108">
                    <p class="img_tit">表3 MSCNN消融实验参数 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Ablation experimental parameters of MSCNN</p>
                    <p class="img_note"></p>
                    <table id="108" border="1"><tr><td rowspan="2"><br />Backbone</td><td colspan="6"><br />Average precision</td></tr><tr><td><i>I</i><sub>OU</sub>=0.5:0.95</td><td><i>I</i><sub>OU</sub>=0.5</td><td><i>I</i><sub>OU</sub>=0.75</td><td>Small</td><td>Medium</td><td>Large</td></tr><tr><td>RetinaNet</td><td>0.690</td><td>0.945</td><td>0.809</td><td>0.532</td><td>0.559</td><td>0.682</td></tr><tr><td><br />EFPN</td><td>0.706</td><td>0.960</td><td>0.824</td><td>0.547</td><td>0.578</td><td>0.701</td></tr><tr><td><br />EFPN-NoProj</td><td>0.700</td><td>0.950</td><td>0.819</td><td>0.544</td><td>0.562</td><td>0.698</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="109" name="109">2) EFPN-NoProj</h4>
                <div class="p1">
                    <p id="110">相较于RetinaNet目标检测网络,EFPN保持了特征图的更高空间分辨率,并且生成了新的阶段。为验证EFPN的确生成了新的阶段<i>P</i>6、<i>P</i>7,设计对比实验,其结果如表3第3～4行所示。采用EFPN作为基础网络进行训练和测试,接着修改EFPN的<i>P</i>6和<i>P</i>7,即采用图2(c)中包含1×1 Conv映射的Dilated bottleNeck结构代替图2(a)中的结构,并将新的修改置于<i>P</i>5之后。为方便起见,新的网络记为EFPN-NoProj。EFPN-NoProj的网络结构如图3所示。</p>
                </div>
                <div class="p1">
                    <p id="111">由表3第3～4行的检测结果可知,EFPN较EFPN-NoProj的检测精度有所提升。同时,实验结果还证明了含有1×1 Conv映射的Dilated bottleNeck结构的EFPN网络在保证特征图有较高分辨率的同时,确实生成了新的特征阶段。</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112"><b>3.5 多尺度目标检测</b></h4>
                <div class="p1">
                    <p id="113">本文评估了所提目标检测框架RetinaNet、MSCNN,以及EFPN-NoProj网络在不同IOU阈值和不同边框尺度下的平均检测精度和召回率,结果如表4所示。</p>
                </div>
                <div class="p1">
                    <p id="114">AP<sub>50</sub>是评价模型分类能力的有效指标,AP<sub>75</sub>能够体现出检测框架对边界框位置回归的能力。如表4所示,MSCNN取得了0.960@AP<sub>50</sub>以及0.824@<i>AP</i><sub>75</sub>的平均检测精度,相较于<i>RetinaNet</i>取得了1.5%@<i>AP</i><sub>50</sub>和1.5%@<i>AP</i><sub>75</sub>的增益。即<i>MSCNN</i>目标检测框架具有更高的分类能力和边框回归精度。此外,<i>MSCNN</i>取得了0.600@<i>small</i>、0.605@<i>medium</i>和0.755@<i>large</i>的平均召回率,相较于<i>RetinaNet</i>网络取得了2.7%@<i>small</i>、1.9%@<i>medium</i>和0.1%@<i>large</i>的增益,说明其对小尺度目标在召回率上具有一定优势。事实上,与<i>RetinaNet</i>网络相比,<i>MSCNN</i>在深层阶段的分辨率更高,同时还生成了更新的阶段。因此,<i>MSCNN</i>可以在更深阶段检测到更小尺度的目标。</p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911042_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 EFPN-NoProj的网络结构" src="Detail/GetImg?filename=images/GXXB201911042_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 <i>EFPN</i>-<i>NoProj</i>的网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911042_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Network structure of EFPN</i>-<i>NoProj</i></p>

                </div>
                <div class="area_img" id="116">
                    <p class="img_tit">表4 不同<i>IOU</i>阈值和不同边框尺度下的平均检测精度和召回率 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Table</i> 4 <i>Average precision and average recall under different IOU thresholds and different bounding box areas</i></p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td rowspan="2"><i>Backbone</i></td><td colspan="3"><br /><i>Average precision</i></td><td colspan="3"><br /><i>Average precision</i><br />(I<sub><i>OU</i></sub> =0.5:0.95)</td><td colspan="3"><br /><i>Average recall</i><br />(I<sub><i>OU</i></sub>=0.5:0.95)</td></tr><tr><td><br />I<sub><i>OU</i></sub>=0.5:0.95</td><td>I<sub><i>OU</i></sub>=0.5</td><td>I<sub><i>OU</i></sub>=0.75</td><td><br /><i>Small</i></td><td><i>Medium</i></td><td><i>Large</i></td><td><br /><i>Small</i></td><td><i>Medium</i></td><td><i>Large</i></td></tr><tr><td><i>RetinaNet</i></td><td>0.690</td><td>0.945</td><td>0.809</td><td>0.532</td><td>0.559</td><td>0.682</td><td>0.573</td><td>0.586</td><td>0.754</td></tr><tr><td><br /><i>MSCNN</i></td><td>0.706</td><td>0.960</td><td>0.824</td><td>0.547</td><td>0.578</td><td>0.701</td><td>0.600</td><td>0.605</td><td>0.755</td></tr><tr><td><br /><i>EFPN</i>-<i>NoProj</i></td><td>0.700</td><td>0.950</td><td>0.819</td><td>0.544</td><td>0.562</td><td>0.698</td><td>0.597</td><td>0.600</td><td>0.753</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="118">图4为部分实例检测结果,从图中可以看出,所提方法能够同时对多类多尺度目标进行准确检测。对于高分辨率光学遥感图像中的飞机、舰船、油管等排布相对密集的小目标,以及车辆和网球场等包含复杂背景的目标, 所提方法均能给出准确的检测结果,进一步证明了本文所提方法的有效性。</p>
                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201911042_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 基于MSCNN的可视化检测结果" src="Detail/GetImg?filename=images/GXXB201911042_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 基于<i>MSCNN</i>的可视化检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201911042_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Visual detection results of MSCNN</i></p>

                </div>
                <h3 id="120" name="120" class="anchor-tag">4 结  论</h3>
                <div class="p1">
                    <p id="121">针对现有目标检测算法对复杂场景下多尺度目标检测精度低、 泛化能力不足的问题, 本文提出了一种基于多尺度卷积神经网络的遥感目标检测框架 <i>MSCNN</i>。该方法首先设计了一种多尺度特征提取骨架,并在此基础上构造出一种深度特征金字塔<i>EFPN</i>, 从而能够更加有效地对遥感图像中的多尺度目标特征的学习。此外,本文引入聚焦分类损失作为分类损失函数, 进一步改善了目标检测网络对于遥感图像中难样本的挖掘能力。所提方法在 <i>NWPU VHRG</i>10公开数据集上获得了0.960的平均检测精度, 相较于其他遥感目标检测框架, 实现了对多尺度遥感目标的高精度稳健检测。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="9">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object detection in optical remote sensing images based on weakly supervised learning and high-level feature Learning">

                                <b>[1]</b> <i>Han J W</i>,<i>Zhang D W</i>,<i>Cheng G</i>,et al.<i>Object detection in optical remote sensing images based on weakly supervised learning and high</i>-<i>level feature learning</i>[<i>J</i>].<i>IEEE Transactions on Geoscience and Remote Sensing</i>,2015,53(6):3325-3337.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES4CCC753F8FC6467291EA53DC443DBE26&amp;v=MjYyODFGcG1hQnVIWWZPR1FsZkNwYlEzNWRsaHg3eTJ3S0E9TmlmT2ZiZkxiYUxMcW93emJKMThDbmcveUJRYTYwb01UWHlXM3hZeGVzYm1NTGlaQ09OdkZTaVdXcjdKSQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> <i>Cheng G</i>,<i>Han J W</i>.<i>A survey on object detection in optical remote sensing images</i>[<i>J</i>].<i>ISPRS Journal of Photogrammetry and Remote Sensing</i>,2016,117:11-28.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700308532&amp;v=MjYwMjFNbndaZVp1SHlqbVViN0lJMW9kYWhvPU5pZk9mYks4SDlETXFJOUZaK3NIQ1g4N29CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> <i>Cheng G</i>,<i>Han J W</i>,<i>Zhou P C</i>,et al.<i>Multi</i>-<i>class geospatial object detection and geographic image classification based on collection of part detectors</i>[<i>J</i>].<i>ISPRS Journal of Photogrammetry and Remote Sensing</i>,2014,98:119-132.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESD9E8A1F886C5F554CD4DAFCE569A68B5&amp;v=MjQ0NTVDcGJRMzVkbGh4N3kyd0tBPU5pZk9mY2V4YTltOXJ2bE5iTzE4Q1FvOHloSmduanNKT1FtUjJSY3pjTU9TVGNpYUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> <i>Deng Z P</i>,<i>Sun H</i>,<i>Zhou S L</i>,et al.<i>Multi</i>-<i>scale object detection in remote sensing imagery with convolutional neural networks</i>[<i>J</i>].<i>ISPRS Journal of Photogrammetry and Remote Sensing</i>,2018,145:3-22.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6C08A5B3BD027EF86DE47E5DC1275010&amp;v=MTU5NDc1VHdybjJHRTBlN1dSUmJ1ZkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRsaHg3eTJ3S0E9TmlmT2ZiWExIdG05cXYxR0ZwOFBEbnRNdVI0Vm5rcA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> <i>Zhong Y F</i>,<i>Han X B</i>,<i>Zhang L P</i>.<i>Multi</i>-<i>class geospatial object detection based on a position</i>-<i>sensitive balancing framework for high spatial resolution remote sensing imagery</i>[<i>J</i>].<i>ISPRS Journal of Photogrammetry and Remote Sensing</i>,2018,138:281-294.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjcxMjRGU25sVUx2QklGWT1OajdCYXJPNEh0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> <i>Lowe D G</i>.<i>Distinctive image features from scale</i>-<i>invariant keypoints</i>[<i>J</i>].<i>International Journal of Computer Vision</i>,2004,60(2):91-110.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">

                                <b>[7]</b> <i>Dalal N</i>,<i>Triggs B</i>.<i>Histograms of oriented gradients for human</i><i>detection</i>[<i>C</i>]//2005 <i>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</i> (<i>CVPR</i>′05),<i>June</i> 20-25,2005,<i>San Diego</i>,<i>CA</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2005:8588935.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A discriminatively trained,multiscale,deformable part model">

                                <b>[8]</b> <i>Felzenszwalb P</i>,<i>McAllester D</i>,<i>Ramanan D</i>.<i>A discriminatively trained</i>,<i>multiscale</i>,<i>deformable part model</i>[<i>C</i>]∥2008 <i>IEEE Conference on Computer Vision and Pattern Recognition</i>,<i>June</i> 23-28,2008,<i>Anchorage</i>,<i>AK</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2008:10139902.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dssd:Deconvolutional single shot detector">

                                <b>[9]</b> <i>Fu C Y</i>,<i>Liu W</i>,<i>Ranga A</i>,et al.<i>Dssd</i>:<i>deconvolutional</i><i>single shot detector</i>[<i>J</i>/<i>OL</i>].(2017-01-23)[2019-04-07].<i>https</i>://<i>arxiv</i>.<i>org</i>/<i>abs</i>/1701.06659.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 <i>Lin T Y</i>,<i>Goyal P</i>,<i>Girshick R</i>,et al.<i>Focal loss for dense object detection</i>[<i>C</i>]//2017 <i>IEEE International Conference on Computer Vision</i> (<i>ICCV</i>),<i>October</i> 22-29,2017,<i>Venice</i>,<i>Italy</i>.<i>New York</i>:<i>IEEE</i>,2017:2999-3007.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-shot Refinement Neural Network for Object Detection">

                                <b>[11]</b> <i>Zhang S F</i>,<i>Wen L Y</i>,<i>Bian X</i>,et al.<i>Single</i>-<i>shot refinement neural network for object detection</i>[<i>C</i>]//2018 <i>IEEE</i>/<i>CVF Conference on Computer Vision and Pattern Recognition</i>,<i>June</i> 18-23,2018,<i>Salt Lake City</i>,<i>UT</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2018:4203-4212.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-End Airplane Detection Using Transfer Learning in Remote Sensing Images">

                                <b>[12]</b> <i>Chen Z</i>,<i>Zhang T</i>,<i>Ouyang C</i>.<i>End</i>-<i>to</i>-<i>end airplane detection using transfer learning in remote sensing images</i>[<i>J</i>].<i>Remote Sensing</i>,2018,10(1):139.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 <i>Liu W</i>,<i>Anguelov D</i>,<i>Erhan D</i>,et al.<i>SSD</i>:<i>single shot multibox detector</i>[<i>M</i>]//<i>Leibe B</i>,<i>Matas J</i>,<i>Sebe N</i>,et al.<i>Computer vision</i>-<i>ECCV</i> 2016.<i>Lecture notes in computer science</i>.<i>Cham</i>:<i>Springer</i>,2016,9905:21-37.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DOTA:A Largescale Dataset for Object Detection in Aerial Images">

                                <b>[14]</b> <i>Xia G S</i>,<i>Bai X</i>,<i>Ding J</i>,et al.<i>DOTA</i>:<i>a large</i>-<i>scale dataset for object detection in aerial images</i>[<i>C</i>]//2018 <i>IEEE</i>/<i>CVF Conference on Computer Vision and Pattern Recognition</i>,<i>June</i> 18-23,2018,<i>Salt Lake City</i>,<i>UT</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2018:3974-3983.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 <i>Redmon J</i>,<i>Farhadi A</i>.<i>YOLO</i>9000:<i>better</i>,<i>faster</i>,<i>stronger</i>[<i>C</i>]//2017 <i>IEEE Conference on Computer Vision and Pattern Recognition</i> (<i>CVPR</i>),<i>July</i> 21-26,2017,<i>Honolulu</i>,<i>HI</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2017:6517-6525.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards real-time object detection with region proposal networks">

                                <b>[16]</b> <i>Ren S Q</i>,<i>He K M</i>,<i>Girshick R</i>,et al.<i>Faster R</i>-<i>CNN</i>:<i>towards real</i>-<i>time object detection with region proposal</i><i>networks</i>[<i>C</i>]//<i>Advances in Neural Information</i><i>Processing Systems</i> 28(<i>NIPS</i> 2015),<i>December</i> 7-12,2015,<i>Palais des Congr</i>è<i>s de Montr</i>é<i>al</i>,<i>Montr</i>é<i>al Canada</i>.<i>Canada</i>:<i>NIPS</i>,2015:91-99.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">

                                <b>[17]</b> <i>He K M</i>,<i>Gkioxari G</i>,<i>Dollar P</i>,et al.<i>Mask R</i>-<i>CNN</i>[<i>C</i>]∥2017 <i>IEEE International Conference on Computer Vision</i> (<i>ICCV</i>),<i>October</i> 22-29,2017,<i>Venice</i>,<i>Italy</i>.<i>New York</i>:<i>IEEE</i>,2017:2980-2988.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature pyramid networks for object detection">

                                <b>[18]</b> <i>Lin T Y</i>,<i>Dollar P</i>,<i>Girshick R</i>,et al.<i>Feature pyramid networks for object detection</i>[<i>C</i>]∥2017 <i>IEEE Conference on Computer Vision and Pattern Recognition</i> (<i>CVPR</i>),<i>July</i> 21-26,2017,<i>Honolulu</i>,<i>HI</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2017:936-944.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cascade R-CNN:delving into high quality object detection">

                                <b>[19]</b> <i>Cai Z W</i>,<i>Vasconcelos N</i>.<i>Cascade R</i>-<i>CNN</i>:<i>delving into high quality object detection</i>[<i>C</i>]//2018 <i>IEEE</i>/<i>CVF Conference on Computer Vision and Pattern Recognition</i>,<i>June</i> 18-23,2018,<i>Salt Lake City</i>,<i>UT</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2018:6154-6162.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Efficient and Robust Integrated Geospatial Object Detection Framework for High Spatial Resolution Remote Sensing Imagery">

                                <b>[20]</b> <i>Han X B</i>,<i>Zhong Y F</i>,<i>Zhang L P</i>.<i>An efficient and robust integrated geospatial object detection framework for high spatial resolution remote sensing imagery</i>[<i>J</i>].<i>Remote Sensing</i>,2017,9(7):666.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201904019&amp;v=MTgzMThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl2aFdyL0FMeXJQWkxHNEg5ak1xNDlFYlk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> <i>Ren Z J</i>,<i>Lin S Z</i>,<i>Li D W</i>,et al.<i>Mask R</i>-<i>CNN object detection method based on improved feature pyramid</i>[<i>J</i>].<i>Laser</i> &amp; <i>Optoelectronics Progress</i>,2019,56(4):041502.任之俊,蔺素珍,李大威,等.基于改进特征金字塔的<i>Mask R</i>-<i>CNN</i>目标检测方法[<i>J</i>].激光与光电子学进展,2019,56(4):041502.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201807042&amp;v=MDkxMzVCWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl2aFdyL0FJalhUYkxHNEg5bk1xSTk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> <i>Zhu M M</i>,<i>Xu Y L</i>,<i>Ma S P</i>,et al.<i>Airport detection method with improved region</i>-<i>based convolutional neural network</i>[<i>J</i>].<i>Acta Optica Sinica</i>,2018,38(7):0728001.朱明明,许悦雷,马时平,等.改进区域卷积神经网络的机场检测方法[<i>J</i>].光学学报,2018,38(7):0728001.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[23]</b> <i>Long J</i>,<i>Shelhamer E</i>,<i>Darrell T</i>.<i>Fully convolutional networks for semantic segmentation</i>[<i>C</i>]//2015 <i>IEEE</i><i>Conference on Computer Vision and Pattern Recognition</i> (<i>CVPR</i>),<i>June</i> 7-12,2015,<i>Boston</i>,<i>MA</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2015:3431-3440.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic image segmentation with deep convolutional nets and fully connected crfs">

                                <b>[24]</b> <i>Chen L C</i>,<i>Papandreou G</i>,<i>Kokkinos I</i>,et al.<i>Semantic image segmentation with deep convolutional nets and fully connected CRFs</i>[<i>J</i>/<i>OL</i>].(2016-06-07)[2019-04-07].<i>https</i>://<i>arxiv</i>.<i>org</i>/<i>abs</i>/1412.7062.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR OpticalRemote Sensing Images">

                                <b>[25]</b> <i>Cheng G</i>,<i>Zhou P C</i>,<i>Han J W</i>.<i>Learning rotation</i>-<i>invariant convolutional neural networks for object detection in VHR optical remote sensing images</i>[<i>J</i>].<i>IEEE Transactions on Geoscience and Remote Sensing</i>,2016,54(12):7405-7415.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=R-fcn:Object detection via region-based fully convolutional networks">

                                <b>[26]</b> <i>Dai J F</i>,<i>Li Y</i>,<i>He K M</i>,et al.<i>R</i>-<i>FCN</i>:<i>object detection via region</i>-<i>based</i><i>fully convolutional networks</i>[<i>C</i>]∥<i>Advances in Neural Information Processing Systems</i> 29(<i>NIPS</i> 2016),<i>December</i> 5-10,2016,<i>Centre Convencions Internacional Barcelona</i>,<i>Barcelona Spain</i>.<i>Canada</i>:<i>NIPS</i>,2016:379-387.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deformable Convolutional Networks">

                                <b>[27]</b> <i>Dai J F</i>,<i>Qi H Z</i>,<i>Xiong Y W</i>,et al.<i>Deformable convolutional networks</i>[<i>C</i>]//2017 <i>IEEE International Conference on Computer Vision</i> (<i>ICCV</i>),<i>October</i> 22-29,2017,<i>Venice</i>,<i>Italy</i>.<i>New York</i>:<i>IEEE</i>,2017:764-773.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201809008&amp;v=MTEzNzZxQnRHRnJDVVJMT2VaZVZ2Rnl2aFdyL0FKaVhUYkxHNEg5bk1wbzlGYklRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b> <i>Deng Z P</i>,<i>Sun H</i>,<i>Lei L</i>,et al.<i>Object detection in remote sensing imagery with multi</i>-<i>scale deformable convolutional networks</i>[<i>J</i>].<i>Acta Geodaetica et Cartographica Sinica</i>,2018,47(9):1216-1227.邓志鹏,孙浩,雷琳,等.基于多尺度形变特征卷积网络的高分辨率遥感影像目标检测[<i>J</i>].测绘学报,2018,47(9):1216-1227.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201911042" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201911042&amp;v=MTA4NjFqWFRiTEc0SDlqTnJvOUJab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXZoV3IvQUk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01aTVQrV2x1a2h6OTd3KzRlTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

