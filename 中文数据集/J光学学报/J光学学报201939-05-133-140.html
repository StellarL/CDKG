

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134008586377500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201905017%26RESULT%3d1%26SIGN%3dzeeVrdxWlnUZRHiCavsn0zfiW08%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201905017&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201905017&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201905017&amp;v=MTI2NDFyQ1VSTE9lWmVWdUZ5M2hWci9PSWpYVGJMRzRIOWpNcW85RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#1" data-title="1 引言 ">1 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#5" data-title="2 基于有效区域的多视角特征点检测 ">2 基于有效区域的多视角特征点检测</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#8" data-title="2.1 视角变换及有效区域标记">2.1 视角变换及有效区域标记</a></li>
                                                <li><a href="#19" data-title="2.2 非线性尺度空间">2.2 非线性尺度空间</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#24" data-title="3 特征点描述与匹配 ">3 特征点描述与匹配</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#32" data-title="3.1 多视角加权匹配">3.1 多视角加权匹配</a></li>
                                                <li><a href="#40" data-title="3.2 多视角最近邻匹配">3.2 多视角最近邻匹配</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#48" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#50" data-title="4.1 匹配算法的有效性分析">4.1 匹配算法的有效性分析</a></li>
                                                <li><a href="#57" data-title="4.2 匹配算法的综合性评价">4.2 匹配算法的综合性评价</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="5 结论 ">5 结论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#7" data-title="图1 特征提取流程图">图1 特征提取流程图</a></li>
                                                <li><a href="#13" data-title="图2 空间变换参数图">图2 空间变换参数图</a></li>
                                                <li><a href="#17" data-title="图3 视角变换图与掩模图。 (a) 变换图; (b) 掩模图">图3 视角变换图与掩模图。 (a) 变换图; (b) 掩模图</a></li>
                                                <li><a href="#29" data-title="图4 多视角描述子">图4 多视角描述子</a></li>
                                                <li><a href="#52" data-title="图5 不同算法的匹配结果。 (a) (c) ASIFT算法, Freiburg＿center1-4和Graffiti1-4图像对; (b) (d) 本文算法, Freiburg＿center1-4和Graffiti1-4图像对">图5 不同算法的匹配结果。 (a) (c) ASIFT算法, Freiburg＿center1-4和......</a></li>
                                                <li><a href="#55" data-title="表1 特征点检测时间表">表1 特征点检测时间表</a></li>
                                                <li><a href="#62" data-title="图6 不同算法在图像变换下对不同图像对的Recall-1-Precision匹配结果。 (a) Boat1-2; (b) Bark1-2; (c) Graffiti1-2; (d) Graffiti1-4; (e) Wall1-2; (f) Wall1-4">图6 不同算法在图像变换下对不同图像对的Recall-1-Precision匹配结果。 (a) Bo......</a></li>
                                                <li><a href="#63" data-title="图7 不同算法在部分图像上的匹配结果">图7 不同算法在部分图像上的匹配结果</a></li>
                                                <li><a href="#69" data-title="图8 本文算法对Graffiti1-6的匹配结果">图8 本文算法对Graffiti1-6的匹配结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="70">


                                    <a id="bibliography_1" title="Lowe D G.Distinctive image features from scaleinvariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MzE3OTlsVnJ2TklGZz1OajdCYXJPNEh0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RlNq&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        Lowe D G.Distinctive image features from scaleinvariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                                    </a>
                                </li>
                                <li id="72">


                                    <a id="bibliography_2" title="Bay H, Ess A, Tuytelaars T, et al.Speeded-up robust features (SURF) [J].Computer Vision and Image Understanding, 2008, 110 (3) :346-359." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501083888&amp;v=MTI3NjJud1plWnVIeWptVWIvSUpWb1JhaFE9TmlmT2ZiSzdIdEROcW85RVpPTU1CSFF4b0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        Bay H, Ess A, Tuytelaars T, et al.Speeded-up robust features (SURF) [J].Computer Vision and Image Understanding, 2008, 110 (3) :346-359.
                                    </a>
                                </li>
                                <li id="74">


                                    <a id="bibliography_3" title="Alcantarilla P F, Bartoli A, Davison A J.KAZE features[M]//Computer Vision-ECCV 2012.Berlin, Heidelberg:Springer Berlin Heidelberg, 2012:214-227." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=KAZE features">
                                        <b>[3]</b>
                                        Alcantarilla P F, Bartoli A, Davison A J.KAZE features[M]//Computer Vision-ECCV 2012.Berlin, Heidelberg:Springer Berlin Heidelberg, 2012:214-227.
                                    </a>
                                </li>
                                <li id="76">


                                    <a id="bibliography_4" title="Alcantarilla P F, Solutions T.Fast explicit diffusion for accelerated features in nonlinear scale spaces[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 34 (7) :1281-1298." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast explicit diffusion for accelerated features in nonlinear scale spaces">
                                        <b>[4]</b>
                                        Alcantarilla P F, Solutions T.Fast explicit diffusion for accelerated features in nonlinear scale spaces[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 34 (7) :1281-1298.
                                    </a>
                                </li>
                                <li id="78">


                                    <a id="bibliography_5" title="Yu G S, Morel J M.ASIFT:an algorithm for fully affine invariant comparison[J].Image Processing on Line, 2011, 1:11-38." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ASIFT:An Algorithm for Fully Affine Invariant Comparison">
                                        <b>[5]</b>
                                        Yu G S, Morel J M.ASIFT:an algorithm for fully affine invariant comparison[J].Image Processing on Line, 2011, 1:11-38.
                                    </a>
                                </li>
                                <li id="80">


                                    <a id="bibliography_6" title="Wang G, Sun X L, Shang Y, et al.A robust template matching algorithm based on best-buddies similarity[J].Acta Optica Sinica, 2017, 37 (3) :0315003.王刚, 孙晓亮, 尚洋, 等.一种基于最佳相似点对的稳健模板匹配算法[J].光学学报, 2017, 37 (3) :0315003." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201703033&amp;v=MjIxOTZWci9PSWpYVGJMRzRIOWJNckk5R1o0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5M2g=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        Wang G, Sun X L, Shang Y, et al.A robust template matching algorithm based on best-buddies similarity[J].Acta Optica Sinica, 2017, 37 (3) :0315003.王刚, 孙晓亮, 尚洋, 等.一种基于最佳相似点对的稳健模板匹配算法[J].光学学报, 2017, 37 (3) :0315003.
                                    </a>
                                </li>
                                <li id="82">


                                    <a id="bibliography_7" title="Tong G F, Li Y, Liu N, et al.Mixed feature extraction and matching for large affine scene[J].Acta Optica Sinica, 2017, 37 (11) :1115003.佟国峰, 李勇, 刘楠, 等.大仿射场景的混合特征提取与匹配[J].光学学报, 2017, 37 (11) :1115003." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201711025&amp;v=MTU5NDhaZVZ1RnkzaFZyL09JalhUYkxHNEg5Yk5ybzlIWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        Tong G F, Li Y, Liu N, et al.Mixed feature extraction and matching for large affine scene[J].Acta Optica Sinica, 2017, 37 (11) :1115003.佟国峰, 李勇, 刘楠, 等.大仿射场景的混合特征提取与匹配[J].光学学报, 2017, 37 (11) :1115003.
                                    </a>
                                </li>
                                <li id="84">


                                    <a id="bibliography_8" title="Calonder M, Lepetit V, Strecha C, et al.BRIEF:binary robust independent elementary features[M]//Computer Vision-ECCV 2010.Berlin, Heidelberg:Springer Berlin Heidelberg, 2010:778-792." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Brief:Binary robust independent elementary features">
                                        <b>[8]</b>
                                        Calonder M, Lepetit V, Strecha C, et al.BRIEF:binary robust independent elementary features[M]//Computer Vision-ECCV 2010.Berlin, Heidelberg:Springer Berlin Heidelberg, 2010:778-792.
                                    </a>
                                </li>
                                <li id="86">


                                    <a id="bibliography_9" title="Rublee E, Rabaud V, Konolige K, et al.ORB:an efficient alternative to SIFT or SURF[C]//2011International Conference on Computer Vision, 6-13Nov.2011, Barcelona, Spain, 2011:2564-2571." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;ORB:An efficient alternative to SIFT or SURF, &amp;quot;">
                                        <b>[9]</b>
                                        Rublee E, Rabaud V, Konolige K, et al.ORB:an efficient alternative to SIFT or SURF[C]//2011International Conference on Computer Vision, 6-13Nov.2011, Barcelona, Spain, 2011:2564-2571.
                                    </a>
                                </li>
                                <li id="88">


                                    <a id="bibliography_10" title="Zhu B, Dai X Z, Li X D, et al.An ASIFT algorithm with masks for feature extraction[J].Chinese Journal of Computers, 2015, 38 (6) :1202-1211.朱博, 戴先中, 李新德, 等.一种带有“遮罩”的ASIFT特征提取算法[J].计算机学报, 2015, 38 (6) :1202-1211." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201506009&amp;v=MjAxNTlPTHo3QmRyRzRIOVRNcVk5RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5M2hWci8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        Zhu B, Dai X Z, Li X D, et al.An ASIFT algorithm with masks for feature extraction[J].Chinese Journal of Computers, 2015, 38 (6) :1202-1211.朱博, 戴先中, 李新德, 等.一种带有“遮罩”的ASIFT特征提取算法[J].计算机学报, 2015, 38 (6) :1202-1211.
                                    </a>
                                </li>
                                <li id="90">


                                    <a id="bibliography_11" title="Lindeberg T.Scale selection properties of generalized scale-space interest point detectors[J].Journal of Mathematical Imaging and Vision, 2013, 46 (2) :177-210." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130513006396&amp;v=MTA5MTQ3bjN4RTlmYnZuS3JpZlp1OXVGQ3JsVTdyTUpWNFNOajdCYXJLN0h0VE5ySTlGWXVnR0NoTTh6eFVTbURkOVNI&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        Lindeberg T.Scale selection properties of generalized scale-space interest point detectors[J].Journal of Mathematical Imaging and Vision, 2013, 46 (2) :177-210.
                                    </a>
                                </li>
                                <li id="92">


                                    <a id="bibliography_12" title="Mikolajczyk K, Mikolajczyk K.Scale&amp;amp;affine invariant interest point detectors[J].International Journal of Computer Vision, 2004, 60 (1) :63-86." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830896&amp;v=MDE2NTRqbFZydk5JRmc9Tmo3QmFyTzRIdEhPcDR4RmJPSUpZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZT&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        Mikolajczyk K, Mikolajczyk K.Scale&amp;amp;affine invariant interest point detectors[J].International Journal of Computer Vision, 2004, 60 (1) :63-86.
                                    </a>
                                </li>
                                <li id="94">


                                    <a id="bibliography_13" title="Yang T Y, Lin Y Y, Chuang Y Y.Accumulated stability voting:a robust descriptor from descriptors of multiple scales[C]//2016IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 27-30June2016, Las Vegas, NV, USA, 2016:327-335." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accumulated stability voting:a robust descriptor from descriptors of multiple scales">
                                        <b>[13]</b>
                                        Yang T Y, Lin Y Y, Chuang Y Y.Accumulated stability voting:a robust descriptor from descriptors of multiple scales[C]//2016IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 27-30June2016, Las Vegas, NV, USA, 2016:327-335.
                                    </a>
                                </li>
                                <li id="96">


                                    <a id="bibliography_14" title="Brown M, Hua G, Winder S.Discriminative learning of local image descriptors[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (1) :43-57." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative learning of local image descriptors">
                                        <b>[14]</b>
                                        Brown M, Hua G, Winder S.Discriminative learning of local image descriptors[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (1) :43-57.
                                    </a>
                                </li>
                                <li id="98">


                                    <a id="bibliography_15" title="Affine covariant features[EB/OL]. (2007-06-15) [2018-10-15].http://www.robots.ox.ac.uk/~vgg/research/affine/." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Affine covariant features">
                                        <b>[15]</b>
                                        Affine covariant features[EB/OL]. (2007-06-15) [2018-10-15].http://www.robots.ox.ac.uk/~vgg/research/affine/.
                                    </a>
                                </li>
                                <li id="100">


                                    <a id="bibliography_16" title="Fischer P, Dosovitskiy A, Brox T.Descriptor matching with convolutional neural networks:a comparison to sift[J].Computer Science, 2004:arXiv:1405.5769." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Descriptor matching with convolutional neural networks:a comparison to sift">
                                        <b>[16]</b>
                                        Fischer P, Dosovitskiy A, Brox T.Descriptor matching with convolutional neural networks:a comparison to sift[J].Computer Science, 2004:arXiv:1405.5769.
                                    </a>
                                </li>
                                <li id="102">


                                    <a id="bibliography_17" title="Mikolajczyk K, Schmid C.A performance evaluation of local descriptors[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (10) :1615-1630." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A performance evaluation of local descriptors">
                                        <b>[17]</b>
                                        Mikolajczyk K, Schmid C.A performance evaluation of local descriptors[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (10) :1615-1630.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-08 10:05</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(05),133-140 DOI:10.3788/AOS201939.0510002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于局部特征的大视角图像匹配</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E9%B9%8F%E5%9B%BE&amp;code=42448472&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵鹏图</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%BE%BE%E9%A3%9E%E9%B9%8F&amp;code=06593251&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">达飞鹏</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%9C%E5%8D%97%E5%A4%A7%E5%AD%A6%E5%A4%8D%E6%9D%82%E5%B7%A5%E7%A8%8B%E7%B3%BB%E7%BB%9F%E6%B5%8B%E9%87%8F%E4%B8%8E%E6%8E%A7%E5%88%B6%E6%95%99%E8%82%B2%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0220478&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">东南大学复杂工程系统测量与控制教育部重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%9C%E5%8D%97%E5%A4%A7%E5%AD%A6%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">东南大学自动化学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对全空间仿射不变 (ASIFT) 算法在解决大视角场景下的图像匹配问题时存在稳健性差、匹配效率低的缺点, 提出了一种新的图像匹配算法。采用非线性扩散滤波代替高斯线性滤波对图像进行预处理, 以提高检测到的特征点的稳健性;在图像模拟变换过程中采用掩模算子对有效区域进行标记, 以提高特征点检测效率。根据视角模拟变换原理, 提取特征点不同视角变换下的邻域信息, 提出了多视角最近邻匹配与加权匹配规则, 建立了多视角描述子, 提升了匹配效率。实验结果证明, 相比现有的特征匹配算法, 所提算法不仅对视角变化具有很好的稳健性, 还提高了图像匹配效率与准确度。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%92%E5%8F%98%E6%8D%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视角变换;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E8%BF%91%E9%82%BB%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最近邻匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A0%E6%9D%83%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">加权匹配;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    赵鹏图, E-mail:220161501@seu.edu.cn;
                                </span>
                                <span>
                                    达飞鹏, E-mail:dafp@seu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-22</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (51475092);</span>
                    </p>
            </div>
                    <h1>Image Matching with Large Viewing Angle Based on Local Features</h1>
                    <h2>
                    <span>Zhao Pengtu</span>
                    <span>Da Feipeng</span>
            </h2>
                    <h2>
                    <span>Key Laboratory of Measurement and Control of Complex System of Engineering, Ministry of Education, Southeast University</span>
                    <span>School of Automation, Southeast University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>An image matching algorithm is proposed to address the poor robustness and low matching efficiency exhibited by the affine scale-invariant feature transform algorithm used for image matching with a large viewing angle.The proposed algorithm employs nonlinear diffusion filtering to preprocess images instead of Gaussian linear filtering, thereby improving the robustness of the detected feature points.Further, a mask operator is employed to denote the effective region in the image simulation transformation process for improving the detection efficiency of feature points.According to the angle simulation transformation principle, the neighborhood information can be extracted from the feature points at different view transformation angles, and the multi-view nearest neighbor matching and weighted matching rules are proposed to establish a multi-view descriptor, thereby improving the matching efficiency.The experimental results show that the proposed algorithm not only exhibits good robustness when the viewing angle changes, but also improves the image matching efficiency and accuracy compared with the existing feature matching algorithms.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=view%20transformation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">view transformation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=nearest%20neighbor%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">nearest neighbor matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=weighted%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">weighted matching;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-22</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="1" name="1" class="anchor-tag">1 引言</h3>
                <div class="p1">
                    <p id="2">图像匹配即通过提取输入图像的具体特征, 度量特征之间的相似性, 进而得出图像之间的对应关系。图像匹配是视觉应用的核心问题, 广泛应用于三维重建、立体匹配、图像检索、图像拼接等领域。该过程的重点在于建立图像可区分性强的特征, 使其具有尺度、平移、旋转不变性等特点, 并使其对光照变化、视角变化、噪声保持较好的稳定性。</p>
                </div>
                <div class="p1">
                    <p id="3">基于局部特征的图像匹配算法近年来发展迅速。比较经典的是2004年由Lowe<citation id="104" type="reference"><link href="70" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出的尺度不变特征匹配算法 (SIFT) , 通过建立高斯线性金字塔尺度空间 (DOG) 来确保特征点的尺度不变性, 同时使提取的特征具有一定的抗噪性与稳定性。随之衍生出的加速稳健特征算法 (SURF) <citation id="105" type="reference"><link href="72" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>使用积分图与盒子滤波的方式加快特征点的检测, 相比于SIFT算法, 匹配效率得到了提高。但这两种算法均采用高斯滤波对图像进行同等程度的平滑, 使得在构建尺度不变空间时, 会出现不同尺度下细节的丢失, 导致检测到的特征点稳健性较差。KAZE<citation id="106" type="reference"><link href="74" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>算法及改进的AKAZE (accelerated-KAZE) <citation id="107" type="reference"><link href="76" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>算法引入非线性扩散滤波代替高斯滤波, 从而检测到性能稳定的特征点。但是, 上述算法不具备完全仿射不变性, 因此不适用于较大视角变化下的图像匹配。针对该问题, 提出了全空间仿射不变 (ASIFT) 算法<citation id="108" type="reference"><link href="78" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>, 该算法利用空间投影变换模型模拟视角变换, 取得了较好的匹配效果, 但该算法的匹配复杂度较高。随着应用场景越来越复杂, 针对特殊应用场景的匹配算法<citation id="109" type="reference"><link href="80" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>也相继得到发展。佟国峰等<citation id="110" type="reference"><link href="82" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>通过提取ASIFT特征点和最大稳定极值区域 (MSER) , 并采用多特征融合方法进行匹配, 一定程度上提升了匹配能力;但是其部分并行计算仍使用ASIFT算法, 并没有解决算法本身的复杂度。图像匹配中的另一个重要环节是特征点描述, 描述子的可区分性决定了特征能否被正确匹配。目前, 常用的描述子主要是SIFT算法所采用的实值描述子, 以及二值特征提取算法 (BRIEF) <citation id="111" type="reference"><link href="84" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和快速特征提取算法 (ORB) <citation id="112" type="reference"><link href="86" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>所采用的二值描述子。但是, 单一视角下的特征描述导致实值描述子和二值描述子无法适用于大视角场景下的特征匹配。</p>
                </div>
                <div class="p1">
                    <p id="4">针对上述匹配算法在大视角场景应用中检测特征点稳健性差、匹配效率低的问题, 本文结合非线性扩散滤波和视角变换, 对输入图像进行预处理, 生成非线性尺度空间, 提高特征点检测的稳健性, 并在图像视角变换的同时, 标记变换后的图像区域与非图像区域, 从而避免无效计算, 提升效率。基于特征点邻域对应的变换区域来提取二值描述子, 得到特征点在不同视角下的特征描述, 并提出最近邻搜索和多视角加权匹配两种匹配规则, 建立新的描述子向量, 提高特征点匹配的准确性。</p>
                </div>
                <h3 id="5" name="5" class="anchor-tag">2 基于有效区域的多视角特征点检测</h3>
                <div class="p1">
                    <p id="6">尺度空间的有效构建会大幅改善特征点检测的效果。非线性扩散滤波<citation id="113" type="reference"><link href="74" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>能够根据图像梯度自适应扩散, 在不同尺度下滤除噪声的同时还能保留细节;ASIFT算法<citation id="114" type="reference"><link href="78" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>通过模拟视角变换可得到不同视角下的图像信息。本文将非线性扩散滤波和模拟视角变换方法相结合, 提出一种优于ASIFT且适用于大视角变化下图像匹配的特征点检测方法, 具体步骤如下:1) 基于视角变换原理, 对输入图像进行变换, 同时对有效区域进行标记;2) 对每一张变换图进行降采样, 构建O组降采样图像, 并对每组降采样图像采用不同的滤波尺度进行S次非线性扩散滤波, 得到S层, 最终构建包含O×S张滤波图像的非线性尺度空间, 然后在该空间中进行特征点检测。特征提取流程如图1所示。</p>
                </div>
                <div class="area_img" id="7">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201905017_00700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 特征提取流程图" src="Detail/GetImg?filename=images/GXXB201905017_00700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 特征提取流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201905017_00700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Flow chart of feature extraction</p>

                </div>
                <h4 class="anchor-tag" id="8" name="8">2.1 视角变换及有效区域标记</h4>
                <div class="p1">
                    <p id="9">模拟视角变换的原理主要基于ASIFT算法<citation id="115" type="reference"><link href="78" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。根据成像理论, 将因相机运动引起的平面物体的投影变形简化为投影变换矩阵A:</p>
                </div>
                <div class="area_img" id="10">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201905017_01000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="11">式中:λ为缩放尺度因子, λ&gt;0;t为倾斜因子;为摄像机的倾斜视角;ψ为相机的旋转角。各参量在简化的空间模型中的表示如图2所示。</p>
                </div>
                <div class="p1">
                    <p id="12">图2中u表示理想平面物体正视图, l表示相机平面, 全空间视角参数包含θ和φ两个参数, 二者可分别表示为</p>
                </div>
                <div class="area_img" id="13">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201905017_01300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 空间变换参数图" src="Detail/GetImg?filename=images/GXXB201905017_01300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 空间变换参数图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201905017_01300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Schematic of spatial transformation parameters</p>

                </div>
                <div class="area_img" id="14">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201905017_01400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="15">式中:k为满足条件下对φ的采样次数;b=72°<citation id="116" type="reference"><link href="78" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>;t=1, <image id="135" type="formula" href="images/GXXB201905017_13500.jpg" display="inline" placement="inline"><alt></alt></image>, 其中s为对t的采样次数。通过对t和采样来模拟图像的全空间视角变换。由 (2) 式可以看出, t的采样数量决定了模拟变换图的数量, 模拟变换图数量越多, 匹配效率就越低。文献<citation id="117" type="reference">[<a class="sup">5</a>]</citation>中, s≥5即可取得较好的匹配结果, 而所提改进描述子有较强的可区分性, s=4即可获得较好的匹配效果。</p>
                </div>
                <div class="p1">
                    <p id="16">在研究过程中发现, 由于每张变换图都是对原图的投影变换, 因此ASIFT算法产生的变换图边缘存在大量的非图像填充区域。文献<citation id="118" type="reference">[<a class="sup">10</a>]</citation>对变换图的非图像区域占比进行了分析, 研究发现所有非图像区域面积占总计算区域面积的40%左右, 引入大量无效计算 (图3) 。</p>
                </div>
                <div class="area_img" id="17">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201905017_01700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 视角变换图与掩模图。 (a) 变换图; (b) 掩模图" src="Detail/GetImg?filename=images/GXXB201905017_01700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 视角变换图与掩模图。 (a) 变换图; (b) 掩模图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201905017_01700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 View transformation and mask maps. (a) Transformation map; (b) mask map</p>

                </div>
                <div class="p1">
                    <p id="18">为了避免无效区域的计算, 在图像变换的同时, 加入由“1” (标记图像区域) 和“0” (标记非图像区域) 构成的掩模图, 白色表示变换后的图像区域[图3 (a) 中图像区域], 灰色表示变换后的非图像区域[图3 (a) 中EFGH非图像区域]。图3 (b) 中标记的非图像区域不参与随后的特征点检测计算。该标记方式可能导致图像边缘区域特征点的检测产生一定的误差, 但通常情况下, 边缘特征点稳定度不高, 不参与后续的图像匹配等工作, 因此这种简单的标记方式可以提高算法效率。</p>
                </div>
                <h4 class="anchor-tag" id="19" name="19">2.2 非线性尺度空间</h4>
                <div class="p1">
                    <p id="20">非线性尺度空间构造的重点是非线性扩散滤波<citation id="119" type="reference"><link href="74" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 其将图像的灰度在不同尺度上的局部结构变化当作一种流动函数的散度, 一般的方程形式为</p>
                </div>
                <div class="area_img" id="21">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201905017_02100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="22">式中:L为得到的模拟图像;div和#分别为散度和梯度算子;x和y为像素位置;t<sub>i</sub>为时间, 一般转换为图像滤波尺度参数σ<sub>i</sub>, 即t<sub>i</sub>=σ<sub>i</sub><sup>2</sup>/2, 其值越大图像结构越平滑, i表示尺度空间中的第i层图像;c (x, y, t<sub>i</sub>) 为传导函数<citation id="120" type="reference"><link href="74" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 该函数能够根据图像梯度分布来自适应调节扩散参数, 具体表现为图像梯度大的地方扩散慢, 图像梯度小的地方扩散快, 即滤除噪声和光滑不稳定区域的同时保留图像的细节信息。</p>
                </div>
                <div class="p1">
                    <p id="23">根据上述原理构建非线性尺度空间, 利用特征点检测算子进行多尺度特征点检测。Lindeberg等<citation id="121" type="reference"><link href="90" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出的第一种特征点检测算子, 相对于传统的DOG算子<citation id="122" type="reference"><link href="70" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、Hessian-Laplace算子<citation id="123" type="reference"><link href="92" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等, 具有更好的性能。</p>
                </div>
                <h3 id="24" name="24" class="anchor-tag">3 特征点描述与匹配</h3>
                <div class="p1">
                    <p id="25">如何为特征点建立可区分性强的描述子是图像匹配中更为重要的内容。现有的描述子提取方法已经相当成熟, 尤其是二值描述子的提出, 大大提高了匹配速度。其主要思想就是在特征点邻域内以某种方式<citation id="124" type="reference"><link href="84" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>采样n对点, 根据灰度测试生成二值描述子。假设X和Y是特征点邻域内的一对采样点, 描述子生成准则如下:</p>
                </div>
                <div class="area_img" id="26">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201905017_02600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="27">式中:I (X) 和I (Y) 分别表示像素X和Y的灰度值。n对采样点依照规则就会生成n维二值描述子向量, 进一步依照一定的匹配规则实现最终的图像匹配。传统算法的匹配规则大多不考虑图像视角变化所带来的特征点邻域信息变化, 仅简单地进行单一视角下穷举式的相似匹配, 无法用于大视角变化的图像匹配。</p>
                </div>
                <div class="p1">
                    <p id="28">本文提出的匹配策略会对采样点对的优劣进行自适应权重分配, 因此采用尽可能分布均匀的平均采样方式<citation id="125" type="reference"><link href="84" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 多视角描述子如图4所示。</p>
                </div>
                <div class="area_img" id="29">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201905017_02900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 多视角描述子" src="Detail/GetImg?filename=images/GXXB201905017_02900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 多视角描述子  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201905017_02900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Multi-view descriptor</p>

                </div>
                <div class="p1">
                    <p id="30">假设拥有m个视角变换, 根据视角变换理论, 可以得到每个特征点的m个不同视角的邻域 (patch) , 对每一个patch以相同的平均采样方式进行点对采样, 然后根据 (4) 式建立二值描述子向量h<sub>ui</sub>, u<sub>i</sub>表示特征点的第i个邻域块。最终得到特征点多视角描述矩阵[h<sub>u1</sub>h<sub>u2</sub>…h<sub>um</sub>]。</p>
                </div>
                <div class="p1">
                    <p id="31">图4所示的描述子构建过程包含了特征点邻域的信息变化, 具体可从以下两方面反映出来:1) 邻域采样点可能引起不同视角变换下产生的描述信息发生变化, 产生相似性度量误差, 该信息变化需要被考虑在内;2) 分别用多个视角变换处理两个视角下的图像, 所得的两组变换图中的某一空间点, 必然存在一定数量可匹配的特征点邻域。</p>
                </div>
                <h4 class="anchor-tag" id="32" name="32">3.1 多视角加权匹配</h4>
                <div class="p1">
                    <p id="33">加权匹配是基于不同采样点对在不同视角变换下为反映特征点邻域信息能力的不同而提出的, 即生成描述子向量的每一维信息的每一对采样点具有相对该特征点稳定或者不稳定的特点, 所以多视角加权思想就是对采样点对进行加权, 对特征点的多视角描述向量进行内部求和, 得到综合多视角信息的实值描述子向量M<sup>∧</sup>, 然后对M<sup>∧</sup>进行加权得到<image id="136" type="formula" href="images/GXXB201905017_13600.jpg" display="inline" placement="inline"><alt></alt></image> (图4) 。假设特征点p的多视角描述矩阵为[h<sup>p</sup><sub>u1</sub>h<sup>p</sup><sub>u2</sub>…h<sup>p</sup><sub>um</sub>], 则M<image id="137" type="formula" href="images/GXXB201905017_13700.jpg" display="inline" placement="inline"><alt></alt></image>可表示为</p>
                </div>
                <div class="area_img" id="34">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201905017_03400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="35">在求和的同时统计每一维“0”、“1”的变化次数, 变化次数较大说明不稳定, 反之认为稳定。根据变化频率对求和后的各分量进行一维高斯加权, 变化频率越小, 高斯权重越大, α表示求和后向量各分量的权重。对于加权后得到的实值向量<image id="138" type="formula" href="images/GXXB201905017_13800.jpg" display="inline" placement="inline"><alt></alt></image>, 仍希望将其转换成二值描述子向量, 因此需要确定一个阈值。文献<citation id="126" type="reference">[<a class="sup">13</a>]</citation>采用最大熵原理对特征向量的可区分性进行最大化处理。最大熵可以衡量信息的不确定性。根据最大熵的定义, 二元事件发生等概率时信息熵最大, 因此阈值取特征向量所有分量的中位数是最简单也是较优的选择。假设<image id="139" type="formula" href="images/GXXB201905017_13900.jpg" display="inline" placement="inline"><alt></alt></image>为n维向量, 阈值为T<sub>p</sub>, 则有</p>
                </div>
                <div class="area_img" id="36">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201905017_03600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="37">式中:M <sub>p</sub><sup>∧ (v) </sup>为M <sub>p</sub><sup>∧</sup>的第v维分量;mid (·) 表示求括号内序列的中位数。根据阈值T<sub>p</sub>, 对实值向量<image id="140" type="formula" href="images/GXXB201905017_14000.jpg" display="inline" placement="inline"><alt></alt></image>进行二值化, 得到所提算法的最终二值描述子向量M<sub>p</sub>。M<sub>p</sub>的第v维分量可表示为</p>
                </div>
                <div class="area_img" id="38">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201905017_03800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="39">M<sub>p</sub>具有采样点对在视角变化中的稳定程度信息, 且具有较强的可区分性。</p>
                </div>
                <h4 class="anchor-tag" id="40" name="40">3.2 多视角最近邻匹配</h4>
                <div class="p1">
                    <p id="41">二值描述子矩阵中行 (列) 向量是不同视角变换下对特征点邻域信息的描述, 最近邻匹配就是在描述子矩阵中寻找相似的行 (列) 向量。相似性匹配的本质就是搜索具有相似视角的匹配。假设具有m种视角变换, 设参考图像中对特征点p提取到的多视角描述矩阵为<image id="141" type="formula" href="images/GXXB201905017_14100.jpg" display="inline" placement="inline"><alt></alt></image>, 待匹配图像中对特征点q提取到的多视角描述矩阵为<image id="142" type="formula" href="images/GXXB201905017_14200.jpg" display="inline" placement="inline"><alt></alt></image><image id="142" type="formula" href="images/GXXB201905017_14201.jpg" display="inline" placement="inline"><alt></alt></image>。对于<image id="143" type="formula" href="images/GXXB201905017_14300.jpg" display="inline" placement="inline"><alt></alt></image>, 在<image id="144" type="formula" href="images/GXXB201905017_14400.jpg" display="inline" placement="inline"><alt></alt></image>中根据汉明距离寻找最佳匹配的特征向量<image id="145" type="formula" href="images/GXXB201905017_14500.jpg" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <div class="area_img" id="42">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201905017_04200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="43">式中:<image id="146" type="formula" href="images/GXXB201905017_14600.jpg" display="inline" placement="inline"><alt></alt></image>的汉明距离, 这里采用不重复查找方式, 即一旦<image id="147" type="formula" href="images/GXXB201905017_14700.jpg" display="inline" placement="inline"><alt></alt></image>中某一个向量被匹配到<image id="148" type="formula" href="images/GXXB201905017_14800.jpg" display="inline" placement="inline"><alt></alt></image>中的向量, 再次进行搜索时已匹配向量不再参与 (8) 式的计算。根据该匹配规则, 对于参考图像特征点p所提取到的所有描述子向量, 在对特征点q提取到的多视角描述矩阵中均有对应的最佳匹配向量, 而对于得到的m对向量对, 还要根据其汉明距离判断是否真正具有相似性, 即有下面两种情况:</p>
                </div>
                <div class="p1">
                    <p id="44">1) 若p与q对应于不同的空间点, 所有这些得到的最佳匹配对理论上不具有相似性, 即根据二值描述子的相似性判断准则, 所有最佳匹配对都不具有相似性, 但偶尔会出现部分最佳匹配对被判定为相似的情况;</p>
                </div>
                <div class="p1">
                    <p id="45">2) 若p与q对应于同一空间点, 其本质上是匹配点, 理论上所有得到的最佳匹配对都具有相似性, 但也可能会出现不相似的情况。</p>
                </div>
                <div class="p1">
                    <p id="46">因此, 仍然需要一个阈值限定:当相似性最佳匹配对个数低于某一个阈值时, 认为特征点p与q不匹配, 高于该阈值时认为匹配。文献<citation id="127" type="reference">[<a class="sup">14</a>]</citation>提供的patch数据集包含了大量特征点在各种变换下的局部图像块, 根据上述匹配规则在该数据集中进行测试, 得出一个可区分性最强的最优值作为阈值。在数据集中选取5000个特征点, 由于本文产生26种视角变换, 所以为每个特征点提取26个表征其邻域视角变换信息的二值向量, 然后按照最近邻匹配规则进行随机组合匹配, 根据最佳匹配对中真实匹配对的数量与所设阈值, 判断特征点对是否匹配, 对该阈值下正确匹配与误匹配的数量进行分析, 得到最佳阈值为14。</p>
                </div>
                <div class="p1">
                    <p id="47">综上, 由于多视角加权匹配综合了视角信息, 具有较高的可信度, 而最近邻匹配是穷举式判定, 复杂度较高;因此本文判定准则如下:若加权匹配规则判定为两点匹配, 则再次利用最近邻规则判断, 若仍匹配, 则认为是匹配点对, 若不匹配, 即判定为非匹配点对并舍弃;若加权匹配规则判定为不匹配, 即判定为非匹配点对, 直接舍弃, 不再进行最近邻规则的验证。实验结果证明, 所提匹配规则可以在不增加时间开销的同时增强匹配算法对视角变化的稳健性。</p>
                </div>
                <h3 id="48" name="48" class="anchor-tag">4 实验结果与分析</h3>
                <div class="p1">
                    <p id="49">采用两个数据集进行实验。Oxford数据集<citation id="128" type="reference"><link href="98" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>包括了图像模糊、尺度缩放、旋转变化、视角变化、光照变化等图片序列。本次实验主要针对视角变化的图片序列, 每一种序列均包含6张图片, 第一张作为参考图像, 认为视角为0°, 其余5张图片相对于第一张图片的视角依次变化10°, 最大近似为60°的视角变化。Oxford数据集还具有图像对之间的单应性矩阵, 可用于求解衡量算法性能的指标参数。Fischer数据集<citation id="129" type="reference"><link href="100" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>包含了更为丰富的图像序列, 本文展示了其中部分匹配结果。实验测试平台为Windows10 64位操作系统, 处理器为Intel core i5-7500, 内存为8GB, 编程环境为VS2013和OpenCV3.0.0, 开发语言为C++。</p>
                </div>
                <h4 class="anchor-tag" id="50" name="50">4.1 匹配算法的有效性分析</h4>
                <div class="p1">
                    <p id="51">在特征点检测中提出的算法思想主要是针对ASIFT算法和KAZE算法的缺陷, 在速度与特征点的稳定程度上进行提升, 而后面提出的匹配策略能使特征点匹配准确率得到提升。图5显示的是算法得到的匹配点对, 红色表示经过随机抽样一致性 (RANSAC) 算法剔除掉的错误匹配点, 绿色表示正确匹配点对。</p>
                </div>
                <div class="area_img" id="52">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201905017_05200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同算法的匹配结果。 (a) (c) ASIFT算法, Freiburg＿center1-4和Graffiti1-4图像对; (b) (d) 本文算法, Freiburg＿center1-4和Graffiti1-4图像对" src="Detail/GetImg?filename=images/GXXB201905017_05200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同算法的匹配结果。 (a) (c) ASIFT算法, Freiburg＿center1-4和Graffiti1-4图像对; (b) (d) 本文算法, Freiburg＿center1-4和Graffiti1-4图像对  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201905017_05200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Matching results of different algorithms. (a) (c) ASIFT algorithm, image pair of Freiburg＿center1-4and Graffiti1-4; (b) (d) proposed algorithm, image pair of Freiburg＿center1-4and Graffiti1-4</p>

                </div>
                <div class="p1">
                    <p id="53">对于具有视角变化的图像对, Freiburg＿center1-4和Graffiti1-4, ASIFT算法较本文算法提取特征的分散度更高。对于图5中圆圈标注的区域, ASIFT算法无法对该区域检测到的特征点进行匹配回收, 本文算法则避免了这些区域的特征提取, 并很好地对检测到的特征点进行了匹配回收。因此, 本文算法较ASIFT算法具有更好的匹配性能。这里未给出其他三种匹配算法的匹配效果, 因为它们并不适用于较大视角变化的图像匹配问题。</p>
                </div>
                <div class="p1">
                    <p id="54">本文算法相对于ASIFT算法最重要的改进是基于有效区域进行特征点检测。ASIFT算法是穷举所有变换图像的特征点并逐一进行相似性度量, 由于采用实值描述子, 复杂度很高;而本文算法在有效区域检测的基础上提取二值描述子, 根据匹配规则构建多视角描述子, 然后进行相似性匹配, 在降低了时间复杂度的情况下, 匹配性能也得到了较好提升。不同算法在Oxford数据集中对具有视角变化的Graffiti序列图像的匹配时间如表1所示。</p>
                </div>
                <div class="area_img" id="55">
                                            <p class="img_tit">
                                                表1 特征点检测时间表
                                                    <br />
                                                Table 1 Time schedule for feature point detection
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201905017_05500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/GXXB201905017_05500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201905017_05500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 特征点检测时间表" src="Detail/GetImg?filename=images/GXXB201905017_05500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="56">算法的检测复杂度表示特征点检测输入计算的图像组数。从表1可以看到, SIFT、KAZE和ORB算法明显较ASIFT和本文算法的速度快, 这是因为它们都只对输入的图像进行特征提取与匹配, 而ASIFT算法与本文算法均是在空间模拟变换理论下进行图像匹配, 复杂度较高。但是, 本文所提的掩模标记算法仅基于有效区域进行检测, 可以看出, 其相对于ASIFT算法在检测速度上有所提升, 而且本文提出利用多视角描述子进行特征匹配, 较ASIFT算法在匹配速度上也得到了提升。</p>
                </div>
                <h4 class="anchor-tag" id="57" name="57">4.2 匹配算法的综合性评价</h4>
                <div class="p1">
                    <p id="58">在基于局部特征的图像匹配中, 有两项很重要的指标可以衡量匹配算法的优劣, 即查全率 (Recall) 和查错率 (1-Precision) 。这两项指标被广泛应用于信息检索和统计学分类领域。用于特征点匹配时, 文献<citation id="130" type="reference">[<a class="sup">17</a>]</citation>具体定义了其计算形式。理想的特征描述子具有较高的查全率和较低的查错率, 表现在结果图中是Recall-1-Precision曲线在左上方。通过比较本文算法与ASIFT<citation id="131" type="reference"><link href="78" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、KAZE<citation id="132" type="reference"><link href="74" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、SIFT<citation id="133" type="reference"><link href="70" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、ORB<citation id="134" type="reference"><link href="86" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>算法在Oxford标准数据集上的匹配效果来说明本文算法的有效性。本文主要针对数据集中具有旋转、缩放和视角变化的若干对图像进行分析比较, 如图6所示。</p>
                </div>
                <div class="p1">
                    <p id="59">可以看出, 对于视角变化较大的Graffiti和Wall图像对, 由于ORB、SIFT和KAZE都采用单一视角下的单一描述子向量进行匹配, 因此失去匹配能力, 而本文算法和ASIFT算法则表现出了很好的性能。同时, 如前所述, 本文算法有两方面优势:1) 本文算法采取了非线性扩散滤波的方式, 检测到的特征点具有较好的稳健性;2) 在提取特征点描述子时, 本文算法根据两种匹配规则, 充分融合了特征点邻域发生视角变化带来的信息变化, 提高了匹配准确率。</p>
                </div>
                <div class="p1">
                    <p id="60">匹配算法在保证准确率的同时还需要保证一定的匹配点对数量。匹配点对的统计规则是:在特征点检测阶段, 将所有视角变换图像中检测到的特征点非重复地转换到原图, 然后统计匹配算法得到的准确匹配点对 (匹配点对的准确性可由图像之间的单应性矩阵进行验证) 。上述匹配实验中, Graffiti和Wall图像对的匹配点对数量统计如图7所示。</p>
                </div>
                <div class="p1">
                    <p id="61">从图7可以看出, 除了本文算法和ASIFT算法, 其他三种算法均不适用于视角变化较大的图像匹配。但结合前面的匹配时效分析可知, 在视角变化不大且实时性要求较高的图像匹配场景中, ORB、SIFT和KAZE等复杂度较低的算法要优于ASIFT和本文算法。对于大视角变化场景下的图像匹配问题, 根据视角变化40°的Graffiti1-4和Wall1-4的匹配结果, 本文算法具有发现潜在匹配点对的能力。同时由图8可知, 本文算法对于视角变化60°的Graffiti1-6的匹配能力下降, 但是在实际应用中, 视角变化越大, 图像间的相关信息越难提取。考虑到计算复杂性, 通常会避免获取很大广角的图像序列。本文算法的视角容限为60°, 足以适用于很多应用场景。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201905017_06200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同算法在图像变换下对不同图像对的Recall-1-Precision匹配结果。 (a) Boat1-2; (b) Bark1-2; (c) Graffiti1-2; (d) Graffiti1-4; (e) Wall1-2; (f) Wall1-4" src="Detail/GetImg?filename=images/GXXB201905017_06200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同算法在图像变换下对不同图像对的Recall-1-Precision匹配结果。 (a) Boat1-2; (b) Bark1-2; (c) Graffiti1-2; (d) Graffiti1-4; (e) Wall1-2; (f) Wall1-4  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201905017_06200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Recall-1-Precision matching results of different image pairs by different algorithms under image transformation. (a) Boat1-2; (b) Bark1-2; (c) Graffiti1-2; (d) Graffiti1-4; (e) Wall1-2; (f) Wall1-4</p>

                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201905017_06300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同算法在部分图像上的匹配结果" src="Detail/GetImg?filename=images/GXXB201905017_06300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同算法在部分图像上的匹配结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201905017_06300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Matching results of different algorithms on partial images</p>

                </div>
                <h3 id="64" name="64" class="anchor-tag">5 结论</h3>
                <div class="p1">
                    <p id="65">针对视角变化较大场景下的图像匹配问题, 结合非线性扩散滤波和模拟视角理论, 提出了具有视角变化信息的描述子。实验证明, 本文算法在视角变化、噪声影响及尺度变化的匹配实验中均具有较好的稳健性, 并且在匹配效率上较ASIFT算法也有提升, 但是仍存在一些有待改进的地方。为了解决较大视角变化下的图像匹配问题, 采用了视角模拟理论, 中间产生了大量的变换图, 增加了计算量。虽然本文算法采用了一些改进措施, 但是相比于SIFT、ORB等算法, 其匹配速度仍较慢, 无法在地图重建等实时应用中使用, 而只能在一些离线应用中使用。随着实时性要求越来越高, 本文算法还需要在匹配效率上进行改进。</p>
                </div>
                <div class="area_img" id="69">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201905017_06900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 本文算法对Graffiti1-6的匹配结果" src="Detail/GetImg?filename=images/GXXB201905017_06900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 本文算法对Graffiti1-6的匹配结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201905017_06900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Matching result of proposed algorithm on Graffiti1-6</p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="70">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MDc5MjZqbFZydk5JRmc9Tmo3QmFyTzRIdEhPcDR4RmJlc09ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZT&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>Lowe D G.Distinctive image features from scaleinvariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                            </a>
                        </p>
                        <p id="72">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501083888&amp;v=MDg3NTJaZVp1SHlqbVViL0lKVm9SYWhRPU5pZk9mYks3SHRETnFvOUVaT01NQkhReG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>Bay H, Ess A, Tuytelaars T, et al.Speeded-up robust features (SURF) [J].Computer Vision and Image Understanding, 2008, 110 (3) :346-359.
                            </a>
                        </p>
                        <p id="74">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=KAZE features">

                                <b>[3]</b>Alcantarilla P F, Bartoli A, Davison A J.KAZE features[M]//Computer Vision-ECCV 2012.Berlin, Heidelberg:Springer Berlin Heidelberg, 2012:214-227.
                            </a>
                        </p>
                        <p id="76">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast explicit diffusion for accelerated features in nonlinear scale spaces">

                                <b>[4]</b>Alcantarilla P F, Solutions T.Fast explicit diffusion for accelerated features in nonlinear scale spaces[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 34 (7) :1281-1298.
                            </a>
                        </p>
                        <p id="78">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ASIFT:An Algorithm for Fully Affine Invariant Comparison">

                                <b>[5]</b>Yu G S, Morel J M.ASIFT:an algorithm for fully affine invariant comparison[J].Image Processing on Line, 2011, 1:11-38.
                            </a>
                        </p>
                        <p id="80">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201703033&amp;v=MDAwNjVMRzRIOWJNckk5R1o0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5M2hWci9PSWpYVGI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>Wang G, Sun X L, Shang Y, et al.A robust template matching algorithm based on best-buddies similarity[J].Acta Optica Sinica, 2017, 37 (3) :0315003.王刚, 孙晓亮, 尚洋, 等.一种基于最佳相似点对的稳健模板匹配算法[J].光学学报, 2017, 37 (3) :0315003.
                            </a>
                        </p>
                        <p id="82">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201711025&amp;v=MDEyODBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5M2hWci9PSWpYVGJMRzRIOWJOcm85SFlZUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>Tong G F, Li Y, Liu N, et al.Mixed feature extraction and matching for large affine scene[J].Acta Optica Sinica, 2017, 37 (11) :1115003.佟国峰, 李勇, 刘楠, 等.大仿射场景的混合特征提取与匹配[J].光学学报, 2017, 37 (11) :1115003.
                            </a>
                        </p>
                        <p id="84">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Brief:Binary robust independent elementary features">

                                <b>[8]</b>Calonder M, Lepetit V, Strecha C, et al.BRIEF:binary robust independent elementary features[M]//Computer Vision-ECCV 2010.Berlin, Heidelberg:Springer Berlin Heidelberg, 2010:778-792.
                            </a>
                        </p>
                        <p id="86">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;ORB:An efficient alternative to SIFT or SURF, &amp;quot;">

                                <b>[9]</b>Rublee E, Rabaud V, Konolige K, et al.ORB:an efficient alternative to SIFT or SURF[C]//2011International Conference on Computer Vision, 6-13Nov.2011, Barcelona, Spain, 2011:2564-2571.
                            </a>
                        </p>
                        <p id="88">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201506009&amp;v=MjIzNjhaZVZ1RnkzaFZyL09MejdCZHJHNEg5VE1xWTlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>Zhu B, Dai X Z, Li X D, et al.An ASIFT algorithm with masks for feature extraction[J].Chinese Journal of Computers, 2015, 38 (6) :1202-1211.朱博, 戴先中, 李新德, 等.一种带有“遮罩”的ASIFT特征提取算法[J].计算机学报, 2015, 38 (6) :1202-1211.
                            </a>
                        </p>
                        <p id="90">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130513006396&amp;v=MTc5NDFoTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkNybFU3ck1KVjRTTmo3QmFySzdIdFROckk5Rll1Z0dD&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>Lindeberg T.Scale selection properties of generalized scale-space interest point detectors[J].Journal of Mathematical Imaging and Vision, 2013, 46 (2) :177-210.
                            </a>
                        </p>
                        <p id="92">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830896&amp;v=MjkwMzBveGNNSDdSN3FlYnVkdEZTamxWcnZOSUZnPU5qN0Jhck80SHRIT3A0eEZiT0lKWTNrNXpCZGg0ajk5U1hxUnJ4&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>Mikolajczyk K, Mikolajczyk K.Scale&amp;affine invariant interest point detectors[J].International Journal of Computer Vision, 2004, 60 (1) :63-86.
                            </a>
                        </p>
                        <p id="94">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accumulated stability voting:a robust descriptor from descriptors of multiple scales">

                                <b>[13]</b>Yang T Y, Lin Y Y, Chuang Y Y.Accumulated stability voting:a robust descriptor from descriptors of multiple scales[C]//2016IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 27-30June2016, Las Vegas, NV, USA, 2016:327-335.
                            </a>
                        </p>
                        <p id="96">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative learning of local image descriptors">

                                <b>[14]</b>Brown M, Hua G, Winder S.Discriminative learning of local image descriptors[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (1) :43-57.
                            </a>
                        </p>
                        <p id="98">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Affine covariant features">

                                <b>[15]</b>Affine covariant features[EB/OL]. (2007-06-15) [2018-10-15].http://www.robots.ox.ac.uk/~vgg/research/affine/.
                            </a>
                        </p>
                        <p id="100">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Descriptor matching with convolutional neural networks:a comparison to sift">

                                <b>[16]</b>Fischer P, Dosovitskiy A, Brox T.Descriptor matching with convolutional neural networks:a comparison to sift[J].Computer Science, 2004:arXiv:1405.5769.
                            </a>
                        </p>
                        <p id="102">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A performance evaluation of local descriptors">

                                <b>[17]</b>Mikolajczyk K, Schmid C.A performance evaluation of local descriptors[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (10) :1615-1630.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201905017" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201905017&amp;v=MTI2NDFyQ1VSTE9lWmVWdUZ5M2hWci9PSWpYVGJMRzRIOWpNcW85RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralVvUTBSTDAybXhOTGFHZWhJcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

