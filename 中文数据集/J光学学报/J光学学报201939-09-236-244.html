

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133151869346250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201909028%26RESULT%3d1%26SIGN%3dUDRIVnono%252b5yE3zmQiCTpljljnQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201909028&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201909028&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201909028&amp;v=MTA1MjVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXprVkx6UElqWFRiTEc0SDlqTXBvOUhiSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#58" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="2 本文算法 ">2 本文算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#64" data-title="&lt;b&gt;2.1 核相关滤波器&lt;/b&gt;"><b>2.1 核相关滤波器</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;2.2 自适应特征融合&lt;/b&gt;"><b>2.2 自适应特征融合</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;2.3 自适应模型更新策略&lt;/b&gt;"><b>2.3 自适应模型更新策略</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#106" data-title="3 实验分析 ">3 实验分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#107" data-title="&lt;b&gt;3.1 算法分析&lt;/b&gt;"><b>3.1 算法分析</b></a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;3.2 实验环境及设置&lt;/b&gt;"><b>3.2 实验环境及设置</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;3.3 定量评估&lt;/b&gt;"><b>3.3 定量评估</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;3.4 定性评估&lt;/b&gt;"><b>3.4 定性评估</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#133" data-title="4 结  论 ">4 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="图1 本文跟踪算法框架">图1 本文跟踪算法框架</a></li>
                                                <li><a href="#111" data-title="图2 &lt;i&gt;HOG&lt;/i&gt;和&lt;i&gt;MIX&lt;/i&gt;自适应权重值">图2 <i>HOG</i>和<i>MIX</i>自适应权重值</a></li>
                                                <li><a href="#114" data-title="图3 singer2视频序列第179帧与对应的响应图。">图3 singer2视频序列第179帧与对应的响应图。</a></li>
                                                <li><a href="#122" data-title="图4 David3中最大响应值与自适应权重以及部分帧。">图4 David3中最大响应值与自适应权重以及部分帧。</a></li>
                                                <li><a href="#123" data-title="表1 8种跟踪算法的平均跟踪性能指标(36组彩色序列)">表1 8种跟踪算法的平均跟踪性能指标(36组彩色序列)</a></li>
                                                <li><a href="#124" data-title="图5 8种跟踪算法的距离精度曲线和成功率曲线比较（36组彩色序列）。">图5 8种跟踪算法的距离精度曲线和成功率曲线比较（36组彩色序列）。</a></li>
                                                <li><a href="#128" data-title="图6 6种算法的跟踪结果。">图6 6种算法的跟踪结果。</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" title=" &lt;i&gt;Yin H P&lt;/i&gt;,&lt;i&gt;Chen B&lt;/i&gt;,&lt;i&gt;Chai Y&lt;/i&gt;,et al.&lt;i&gt;Vision&lt;/i&gt;-&lt;i&gt;based object detection&lt;/i&gt;&lt;i&gt;and tracking&lt;/i&gt;:&lt;i&gt;a review&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Automatica&lt;/i&gt;&lt;i&gt;Sinica&lt;/i&gt;,2016,42(10):1466-1489.尹宏鹏,陈波,柴毅,等.基于视觉的目标检测与跟踪综述[&lt;i&gt;J&lt;/i&gt;].自动化学报,2016,42(10):1466-1489." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201610002&amp;v=MzAxMzZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXprVkx6UEtDTGZZYkc0SDlmTnI0OUY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         &lt;i&gt;Yin H P&lt;/i&gt;,&lt;i&gt;Chen B&lt;/i&gt;,&lt;i&gt;Chai Y&lt;/i&gt;,et al.&lt;i&gt;Vision&lt;/i&gt;-&lt;i&gt;based object detection&lt;/i&gt;&lt;i&gt;and tracking&lt;/i&gt;:&lt;i&gt;a review&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Automatica&lt;/i&gt;&lt;i&gt;Sinica&lt;/i&gt;,2016,42(10):1466-1489.尹宏鹏,陈波,柴毅,等.基于视觉的目标检测与跟踪综述[&lt;i&gt;J&lt;/i&gt;].自动化学报,2016,42(10):1466-1489.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" &lt;i&gt;Comaniciu D&lt;/i&gt;,&lt;i&gt;Ramesh V&lt;/i&gt;,&lt;i&gt;Meer P&lt;/i&gt;.&lt;i&gt;Real&lt;/i&gt;-&lt;i&gt;time tracking of non&lt;/i&gt;-&lt;i&gt;rigid objects using mean shift&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2000 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;June&lt;/i&gt; 15,2000,&lt;i&gt;Hilton Head Island&lt;/i&gt;,&lt;i&gt;SC&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2000:142-149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time tracking of non-rigid objects using mean shift">
                                        <b>[2]</b>
                                         &lt;i&gt;Comaniciu D&lt;/i&gt;,&lt;i&gt;Ramesh V&lt;/i&gt;,&lt;i&gt;Meer P&lt;/i&gt;.&lt;i&gt;Real&lt;/i&gt;-&lt;i&gt;time tracking of non&lt;/i&gt;-&lt;i&gt;rigid objects using mean shift&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2000 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;June&lt;/i&gt; 15,2000,&lt;i&gt;Hilton Head Island&lt;/i&gt;,&lt;i&gt;SC&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2000:142-149.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" title=" &lt;i&gt;Nummiaro K&lt;/i&gt;,&lt;i&gt;Koller&lt;/i&gt;-&lt;i&gt;Meier E&lt;/i&gt;,&lt;i&gt;van Gool L&lt;/i&gt;.&lt;i&gt;An adaptive color&lt;/i&gt;-&lt;i&gt;based particle filter&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Image and Vision Computing&lt;/i&gt;,2003,21(1):99-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349994&amp;v=MTQ4NDhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViN0lKRjhUYVJVPU5pZk9mYks3SHRET3JZOUVaKzhHQlhVOQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         &lt;i&gt;Nummiaro K&lt;/i&gt;,&lt;i&gt;Koller&lt;/i&gt;-&lt;i&gt;Meier E&lt;/i&gt;,&lt;i&gt;van Gool L&lt;/i&gt;.&lt;i&gt;An adaptive color&lt;/i&gt;-&lt;i&gt;based particle filter&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Image and Vision Computing&lt;/i&gt;,2003,21(1):99-110.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" >
                                        <b>[4]</b>
                                     &lt;i&gt;Hare S&lt;/i&gt;,&lt;i&gt;Golodetz S&lt;/i&gt;,&lt;i&gt;Saffari A&lt;/i&gt;,et al.&lt;i&gt;Struck&lt;/i&gt;:&lt;i&gt;structured output tracking with kernels&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/i&gt;,2016,38(10):2096-2109.</a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" title=" &lt;i&gt;Henriques J F&lt;/i&gt;,&lt;i&gt;Caseiro R&lt;/i&gt;,&lt;i&gt;Martins P&lt;/i&gt;,et al.&lt;i&gt;Exploiting the circulant structure of tracking&lt;/i&gt;-&lt;i&gt;by&lt;/i&gt;-&lt;i&gt;detection with kernels&lt;/i&gt;[&lt;i&gt;M&lt;/i&gt;]//&lt;i&gt;Fitzgibbon A&lt;/i&gt;,&lt;i&gt;Lazebnik S&lt;/i&gt;,&lt;i&gt;Perona P&lt;/i&gt;,et al.&lt;i&gt;Computer vision&lt;/i&gt;-&lt;i&gt;ECCV&lt;/i&gt; 2012.&lt;i&gt;Berlin&lt;/i&gt;,&lt;i&gt;Heidelberg&lt;/i&gt;:&lt;i&gt;Springer&lt;/i&gt;,2012:702-715." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">
                                        <b>[5]</b>
                                         &lt;i&gt;Henriques J F&lt;/i&gt;,&lt;i&gt;Caseiro R&lt;/i&gt;,&lt;i&gt;Martins P&lt;/i&gt;,et al.&lt;i&gt;Exploiting the circulant structure of tracking&lt;/i&gt;-&lt;i&gt;by&lt;/i&gt;-&lt;i&gt;detection with kernels&lt;/i&gt;[&lt;i&gt;M&lt;/i&gt;]//&lt;i&gt;Fitzgibbon A&lt;/i&gt;,&lt;i&gt;Lazebnik S&lt;/i&gt;,&lt;i&gt;Perona P&lt;/i&gt;,et al.&lt;i&gt;Computer vision&lt;/i&gt;-&lt;i&gt;ECCV&lt;/i&gt; 2012.&lt;i&gt;Berlin&lt;/i&gt;,&lt;i&gt;Heidelberg&lt;/i&gt;:&lt;i&gt;Springer&lt;/i&gt;,2012:702-715.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" title=" &lt;i&gt;Bolme D&lt;/i&gt;,&lt;i&gt;Beveridge J R&lt;/i&gt;,&lt;i&gt;Draper B A&lt;/i&gt;,et al.&lt;i&gt;Visual object tracking using adaptive correlation filters&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2010 &lt;i&gt;IEEE Computer Society Conference on Computer Vision and Pattern Recognition&lt;/i&gt;,&lt;i&gt;June&lt;/i&gt; 13-18,2010,&lt;i&gt;San Francisco&lt;/i&gt;,&lt;i&gt;CA&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2010:2544-2550." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">
                                        <b>[6]</b>
                                         &lt;i&gt;Bolme D&lt;/i&gt;,&lt;i&gt;Beveridge J R&lt;/i&gt;,&lt;i&gt;Draper B A&lt;/i&gt;,et al.&lt;i&gt;Visual object tracking using adaptive correlation filters&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2010 &lt;i&gt;IEEE Computer Society Conference on Computer Vision and Pattern Recognition&lt;/i&gt;,&lt;i&gt;June&lt;/i&gt; 13-18,2010,&lt;i&gt;San Francisco&lt;/i&gt;,&lt;i&gt;CA&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2010:2544-2550.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" &lt;i&gt;Henriques J F&lt;/i&gt;,&lt;i&gt;Caseiro R&lt;/i&gt;,&lt;i&gt;Martins P&lt;/i&gt;,et al.&lt;i&gt;High&lt;/i&gt;-&lt;i&gt;speed tracking with kernelized correlation filters&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/i&gt;,2015,37(3):583-596." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">
                                        <b>[7]</b>
                                         &lt;i&gt;Henriques J F&lt;/i&gt;,&lt;i&gt;Caseiro R&lt;/i&gt;,&lt;i&gt;Martins P&lt;/i&gt;,et al.&lt;i&gt;High&lt;/i&gt;-&lt;i&gt;speed tracking with kernelized correlation filters&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/i&gt;,2015,37(3):583-596.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" title=" &lt;i&gt;Danelljan M&lt;/i&gt;,&lt;i&gt;Khan F S&lt;/i&gt;,&lt;i&gt;Felsberg M&lt;/i&gt;,et al.&lt;i&gt;Adaptive color attributes for real&lt;/i&gt;-&lt;i&gt;time visual tracking&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2014 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;June&lt;/i&gt; 23-28,2014,&lt;i&gt;Columbus&lt;/i&gt;,&lt;i&gt;OH&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2014:1090-1097." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive color attributes for real-time visual tracking">
                                        <b>[8]</b>
                                         &lt;i&gt;Danelljan M&lt;/i&gt;,&lt;i&gt;Khan F S&lt;/i&gt;,&lt;i&gt;Felsberg M&lt;/i&gt;,et al.&lt;i&gt;Adaptive color attributes for real&lt;/i&gt;-&lt;i&gt;time visual tracking&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2014 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;June&lt;/i&gt; 23-28,2014,&lt;i&gt;Columbus&lt;/i&gt;,&lt;i&gt;OH&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2014:1090-1097.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" &lt;i&gt;Li Y&lt;/i&gt;,&lt;i&gt;Zhu J K&lt;/i&gt;.&lt;i&gt;A scale adaptive kernel correlation filter tracker with feature integration&lt;/i&gt;[&lt;i&gt;M&lt;/i&gt;]∥&lt;i&gt;Agapito L&lt;/i&gt;,&lt;i&gt;Bronstein M M&lt;/i&gt;,&lt;i&gt;Rother C&lt;/i&gt;.&lt;i&gt;Computer vision&lt;/i&gt;-&lt;i&gt;ECCV&lt;/i&gt; 2014 &lt;i&gt;workshops&lt;/i&gt;.&lt;i&gt;Cham&lt;/i&gt;:&lt;i&gt;Springer&lt;/i&gt;,2015:254-265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A scale adaptive kernel correlation filter tracker with feature integration">
                                        <b>[9]</b>
                                         &lt;i&gt;Li Y&lt;/i&gt;,&lt;i&gt;Zhu J K&lt;/i&gt;.&lt;i&gt;A scale adaptive kernel correlation filter tracker with feature integration&lt;/i&gt;[&lt;i&gt;M&lt;/i&gt;]∥&lt;i&gt;Agapito L&lt;/i&gt;,&lt;i&gt;Bronstein M M&lt;/i&gt;,&lt;i&gt;Rother C&lt;/i&gt;.&lt;i&gt;Computer vision&lt;/i&gt;-&lt;i&gt;ECCV&lt;/i&gt; 2014 &lt;i&gt;workshops&lt;/i&gt;.&lt;i&gt;Cham&lt;/i&gt;:&lt;i&gt;Springer&lt;/i&gt;,2015:254-265.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" title=" &lt;i&gt;Bertinetto L&lt;/i&gt;,&lt;i&gt;Valmadre J&lt;/i&gt;,&lt;i&gt;Golodetz S&lt;/i&gt;,et al.&lt;i&gt;Staple&lt;/i&gt;:&lt;i&gt;complementary learners for real&lt;/i&gt;-&lt;i&gt;time tracking&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2016 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;June&lt;/i&gt; 27-30,2016,&lt;i&gt;Las Vegas&lt;/i&gt;,&lt;i&gt;NV&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2016:1401-1409." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">
                                        <b>[10]</b>
                                         &lt;i&gt;Bertinetto L&lt;/i&gt;,&lt;i&gt;Valmadre J&lt;/i&gt;,&lt;i&gt;Golodetz S&lt;/i&gt;,et al.&lt;i&gt;Staple&lt;/i&gt;:&lt;i&gt;complementary learners for real&lt;/i&gt;-&lt;i&gt;time tracking&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2016 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;June&lt;/i&gt; 27-30,2016,&lt;i&gt;Las Vegas&lt;/i&gt;,&lt;i&gt;NV&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2016:1401-1409.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" title=" &lt;i&gt;Ma C&lt;/i&gt;,&lt;i&gt;Huang J B&lt;/i&gt;,&lt;i&gt;Yang X K&lt;/i&gt;,et al.&lt;i&gt;Hierarchical convolutional features for visual tracking&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]∥2015 &lt;i&gt;IEEE International Conference on Computer Vision&lt;/i&gt; (&lt;i&gt;ICCV&lt;/i&gt;),&lt;i&gt;December&lt;/i&gt; 7-13,2015,&lt;i&gt;Santiago&lt;/i&gt;,&lt;i&gt;Chile&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2015:3074-3082." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical Convolutional Features for Visual Tracking">
                                        <b>[11]</b>
                                         &lt;i&gt;Ma C&lt;/i&gt;,&lt;i&gt;Huang J B&lt;/i&gt;,&lt;i&gt;Yang X K&lt;/i&gt;,et al.&lt;i&gt;Hierarchical convolutional features for visual tracking&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]∥2015 &lt;i&gt;IEEE International Conference on Computer Vision&lt;/i&gt; (&lt;i&gt;ICCV&lt;/i&gt;),&lt;i&gt;December&lt;/i&gt; 7-13,2015,&lt;i&gt;Santiago&lt;/i&gt;,&lt;i&gt;Chile&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2015:3074-3082.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" title=" &lt;i&gt;Danelljan M&lt;/i&gt;,&lt;i&gt;Robinson A&lt;/i&gt;,&lt;i&gt;Khan F S&lt;/i&gt;,et al.&lt;i&gt;Beyond correlation filters&lt;/i&gt;:&lt;i&gt;learning continuous convolution operators for visual tracking&lt;/i&gt;[&lt;i&gt;M&lt;/i&gt;]//&lt;i&gt;Leibe B&lt;/i&gt;,&lt;i&gt;Matas J&lt;/i&gt;,&lt;i&gt;Sebe N&lt;/i&gt;,et al.&lt;i&gt;Computer vision&lt;/i&gt;-&lt;i&gt;ECCV&lt;/i&gt; 2016.&lt;i&gt;Cham&lt;/i&gt;:&lt;i&gt;Springer&lt;/i&gt;,2016:472-488." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond Correlation Filters:Learning Continuous Convolution Operators for Visual Tracking">
                                        <b>[12]</b>
                                         &lt;i&gt;Danelljan M&lt;/i&gt;,&lt;i&gt;Robinson A&lt;/i&gt;,&lt;i&gt;Khan F S&lt;/i&gt;,et al.&lt;i&gt;Beyond correlation filters&lt;/i&gt;:&lt;i&gt;learning continuous convolution operators for visual tracking&lt;/i&gt;[&lt;i&gt;M&lt;/i&gt;]//&lt;i&gt;Leibe B&lt;/i&gt;,&lt;i&gt;Matas J&lt;/i&gt;,&lt;i&gt;Sebe N&lt;/i&gt;,et al.&lt;i&gt;Computer vision&lt;/i&gt;-&lt;i&gt;ECCV&lt;/i&gt; 2016.&lt;i&gt;Cham&lt;/i&gt;:&lt;i&gt;Springer&lt;/i&gt;,2016:472-488.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" title=" &lt;i&gt;Liu W&lt;/i&gt;,&lt;i&gt;Zhao W J&lt;/i&gt;,&lt;i&gt;Li C&lt;/i&gt;.&lt;i&gt;Long&lt;/i&gt;-&lt;i&gt;term visual tracking based on spatio&lt;/i&gt;-&lt;i&gt;temporal context&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Optica Sinica&lt;/i&gt;,2016,36(1):0115001.刘威,赵文杰,李成.时空上下文学习长时目标跟踪[&lt;i&gt;J&lt;/i&gt;].光学学报,2016,36(1):0115001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201601023&amp;v=MjM1MDllVnZGeXprVkx6UElqWFRiTEc0SDlmTXJvOUhaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         &lt;i&gt;Liu W&lt;/i&gt;,&lt;i&gt;Zhao W J&lt;/i&gt;,&lt;i&gt;Li C&lt;/i&gt;.&lt;i&gt;Long&lt;/i&gt;-&lt;i&gt;term visual tracking based on spatio&lt;/i&gt;-&lt;i&gt;temporal context&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Optica Sinica&lt;/i&gt;,2016,36(1):0115001.刘威,赵文杰,李成.时空上下文学习长时目标跟踪[&lt;i&gt;J&lt;/i&gt;].光学学报,2016,36(1):0115001.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" &lt;i&gt;Xiong C Z&lt;/i&gt;,&lt;i&gt;Zhao L L&lt;/i&gt;,&lt;i&gt;Guo F H&lt;/i&gt;.&lt;i&gt;Kernelized correlation filters tracking based on adaptive feature fusion&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Journal of Computer&lt;/i&gt;-&lt;i&gt;Aided Design&lt;/i&gt; &amp;amp; &lt;i&gt;Computer Graphics&lt;/i&gt;,2017,29(6):1068-1074.熊昌镇,赵璐璐,郭芬红.自适应特征融合的核相关滤波跟踪算法[&lt;i&gt;J&lt;/i&gt;].计算机辅助设计与图形学学报,2017,29(6):1068-1074." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201706012&amp;v=MDkwODg2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXprVkx6UEx6N0JhTEc0SDliTXFZOUVab1FLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         &lt;i&gt;Xiong C Z&lt;/i&gt;,&lt;i&gt;Zhao L L&lt;/i&gt;,&lt;i&gt;Guo F H&lt;/i&gt;.&lt;i&gt;Kernelized correlation filters tracking based on adaptive feature fusion&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Journal of Computer&lt;/i&gt;-&lt;i&gt;Aided Design&lt;/i&gt; &amp;amp; &lt;i&gt;Computer Graphics&lt;/i&gt;,2017,29(6):1068-1074.熊昌镇,赵璐璐,郭芬红.自适应特征融合的核相关滤波跟踪算法[&lt;i&gt;J&lt;/i&gt;].计算机辅助设计与图形学学报,2017,29(6):1068-1074.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" &lt;i&gt;Wang M M&lt;/i&gt;,&lt;i&gt;Liu Y&lt;/i&gt;,&lt;i&gt;Huang Z Y&lt;/i&gt;.&lt;i&gt;Large margin object tracking with circulant feature maps&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2017 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;July&lt;/i&gt; 21-26,2017,&lt;i&gt;Honolulu&lt;/i&gt;,&lt;i&gt;HI&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2017:4021-4029." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large Margin Object Tracking with Circulant Feature Maps">
                                        <b>[15]</b>
                                         &lt;i&gt;Wang M M&lt;/i&gt;,&lt;i&gt;Liu Y&lt;/i&gt;,&lt;i&gt;Huang Z Y&lt;/i&gt;.&lt;i&gt;Large margin object tracking with circulant feature maps&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2017 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;July&lt;/i&gt; 21-26,2017,&lt;i&gt;Honolulu&lt;/i&gt;,&lt;i&gt;HI&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2017:4021-4029.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" >
                                        <b>[16]</b>
                                     &lt;i&gt;Ojala T&lt;/i&gt;,&lt;i&gt;Pietik&lt;/i&gt;&#228;&lt;i&gt;inen M&lt;/i&gt;,&lt;i&gt;M&lt;/i&gt;&#228;&lt;i&gt;enp&lt;/i&gt;&#228;&#228; &lt;i&gt;T&lt;/i&gt;.&lt;i&gt;Multiresolution gray&lt;/i&gt;-&lt;i&gt;scale and rotation invariant texture classification with local binary patterns&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Pattern Analysis&lt;/i&gt; &amp;amp; &lt;i&gt;Machine Intelligence&lt;/i&gt;,2002,24(7):971-987.</a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" title=" &lt;i&gt;Felzenszwalb P F&lt;/i&gt;,&lt;i&gt;Girshick R B&lt;/i&gt;,&lt;i&gt;McAllester D&lt;/i&gt;,et al.&lt;i&gt;Object detection with discriminatively trained part&lt;/i&gt;-&lt;i&gt;based models&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/i&gt;,2010,32(9):1627-1645." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object Detection with Discriminatively Trained Part-Based Models">
                                        <b>[17]</b>
                                         &lt;i&gt;Felzenszwalb P F&lt;/i&gt;,&lt;i&gt;Girshick R B&lt;/i&gt;,&lt;i&gt;McAllester D&lt;/i&gt;,et al.&lt;i&gt;Object detection with discriminatively trained part&lt;/i&gt;-&lt;i&gt;based models&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/i&gt;,2010,32(9):1627-1645.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" &lt;i&gt;Shen Q&lt;/i&gt;,&lt;i&gt;Yan X L&lt;/i&gt;,&lt;i&gt;Liu L F&lt;/i&gt;,et al.&lt;i&gt;Multi&lt;/i&gt;-&lt;i&gt;scale correlation filtering tracker based on adaptive feature selection&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Optica Sinica&lt;/i&gt;,2017,37(5):0515001.沈秋,严小乐,刘霖枫,等.基于自适应特征选择的多尺度相关滤波跟踪[&lt;i&gt;J&lt;/i&gt;].光学学报,2017,37(5):0515001." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201705021&amp;v=MDY1MTJPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5emtWTHpQSWpYVGJMRzRIOWJNcW85SFpZUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         &lt;i&gt;Shen Q&lt;/i&gt;,&lt;i&gt;Yan X L&lt;/i&gt;,&lt;i&gt;Liu L F&lt;/i&gt;,et al.&lt;i&gt;Multi&lt;/i&gt;-&lt;i&gt;scale correlation filtering tracker based on adaptive feature selection&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Optica Sinica&lt;/i&gt;,2017,37(5):0515001.沈秋,严小乐,刘霖枫,等.基于自适应特征选择的多尺度相关滤波跟踪[&lt;i&gt;J&lt;/i&gt;].光学学报,2017,37(5):0515001.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_19" title=" &lt;i&gt;Possegger H&lt;/i&gt;,&lt;i&gt;Mauthner T&lt;/i&gt;,&lt;i&gt;Bischof H&lt;/i&gt;.&lt;i&gt;In defense of color&lt;/i&gt;-&lt;i&gt;based model&lt;/i&gt;-&lt;i&gt;free tracking&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]∥2015 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;June&lt;/i&gt; 7-12,2015,&lt;i&gt;Boston&lt;/i&gt;,&lt;i&gt;MA&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2015:2113-2120." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=In defense of color-based model-free tracking">
                                        <b>[19]</b>
                                         &lt;i&gt;Possegger H&lt;/i&gt;,&lt;i&gt;Mauthner T&lt;/i&gt;,&lt;i&gt;Bischof H&lt;/i&gt;.&lt;i&gt;In defense of color&lt;/i&gt;-&lt;i&gt;based model&lt;/i&gt;-&lt;i&gt;free tracking&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]∥2015 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;June&lt;/i&gt; 7-12,2015,&lt;i&gt;Boston&lt;/i&gt;,&lt;i&gt;MA&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2015:2113-2120.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_20" title=" &lt;i&gt;Wang W&lt;/i&gt;,&lt;i&gt;Wang C P&lt;/i&gt;,&lt;i&gt;Li J&lt;/i&gt;,et al.&lt;i&gt;Correlation filter tracking based on feature fusing and model adaptive updating&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Optics and Precision Engineering&lt;/i&gt;,2016,24(8):2059-2066.王暐,王春平,李军,等.特征融合和模型自适应更新相结合的相关滤波目标跟踪[&lt;i&gt;J&lt;/i&gt;].光学精密工程,2016,24(8):2059-2066." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201608029&amp;v=MDQ5ODhIOWZNcDQ5SGJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5emtWTHpQSWpYQlk3RzQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         &lt;i&gt;Wang W&lt;/i&gt;,&lt;i&gt;Wang C P&lt;/i&gt;,&lt;i&gt;Li J&lt;/i&gt;,et al.&lt;i&gt;Correlation filter tracking based on feature fusing and model adaptive updating&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Optics and Precision Engineering&lt;/i&gt;,2016,24(8):2059-2066.王暐,王春平,李军,等.特征融合和模型自适应更新相结合的相关滤波目标跟踪[&lt;i&gt;J&lt;/i&gt;].光学精密工程,2016,24(8):2059-2066.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_21" title=" &lt;i&gt;Wu Y&lt;/i&gt;,&lt;i&gt;Lim J&lt;/i&gt;,&lt;i&gt;Yang M H&lt;/i&gt;.&lt;i&gt;Online object tracking&lt;/i&gt;:&lt;i&gt;a benchmark&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2013 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;June&lt;/i&gt; 23-28,2013,&lt;i&gt;Portland&lt;/i&gt;,&lt;i&gt;OR&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2013:2411-2418." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online object tracking:A benchmark">
                                        <b>[21]</b>
                                         &lt;i&gt;Wu Y&lt;/i&gt;,&lt;i&gt;Lim J&lt;/i&gt;,&lt;i&gt;Yang M H&lt;/i&gt;.&lt;i&gt;Online object tracking&lt;/i&gt;:&lt;i&gt;a benchmark&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//2013 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (&lt;i&gt;CVPR&lt;/i&gt;),&lt;i&gt;June&lt;/i&gt; 23-28,2013,&lt;i&gt;Portland&lt;/i&gt;,&lt;i&gt;OR&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2013:2411-2418.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_22" title=" &lt;i&gt;Danelljan M&lt;/i&gt;,&lt;i&gt;H&lt;/i&gt;&#228;&lt;i&gt;ger G&lt;/i&gt;,&lt;i&gt;Khan F S&lt;/i&gt;,et al.&lt;i&gt;Accurate scale estimation for robust visual tracking&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]∥&lt;i&gt;The British Machine Vision Conference&lt;/i&gt; 2014,&lt;i&gt;September&lt;/i&gt;,2014,&lt;i&gt;Nottingham&lt;/i&gt;,&lt;i&gt;UK&lt;/i&gt;.&lt;i&gt;Guildford&lt;/i&gt;:&lt;i&gt;BMVA Press&lt;/i&gt;,2014:1-11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate scale estimation for robust visual tracking">
                                        <b>[22]</b>
                                         &lt;i&gt;Danelljan M&lt;/i&gt;,&lt;i&gt;H&lt;/i&gt;&#228;&lt;i&gt;ger G&lt;/i&gt;,&lt;i&gt;Khan F S&lt;/i&gt;,et al.&lt;i&gt;Accurate scale estimation for robust visual tracking&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]∥&lt;i&gt;The British Machine Vision Conference&lt;/i&gt; 2014,&lt;i&gt;September&lt;/i&gt;,2014,&lt;i&gt;Nottingham&lt;/i&gt;,&lt;i&gt;UK&lt;/i&gt;.&lt;i&gt;Guildford&lt;/i&gt;:&lt;i&gt;BMVA Press&lt;/i&gt;,2014:1-11.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_23" title=" &lt;i&gt;Everingham M&lt;/i&gt;,&lt;i&gt;van Gool L&lt;/i&gt;,&lt;i&gt;Williams C K I&lt;/i&gt;,et al.&lt;i&gt;The Pascal visual object classes&lt;/i&gt; (&lt;i&gt;VOC&lt;/i&gt;) &lt;i&gt;challenge&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;International Journal of Computer Vision&lt;/i&gt;,2010,88(2):303-338." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MDI0ODF1ZHRGU25sVjc3UEkxaz1OajdCYXJPNEh0SFBxWWRIWStJTFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVi&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         &lt;i&gt;Everingham M&lt;/i&gt;,&lt;i&gt;van Gool L&lt;/i&gt;,&lt;i&gt;Williams C K I&lt;/i&gt;,et al.&lt;i&gt;The Pascal visual object classes&lt;/i&gt; (&lt;i&gt;VOC&lt;/i&gt;) &lt;i&gt;challenge&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;International Journal of Computer Vision&lt;/i&gt;,2010,88(2):303-338.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_24" title=" &lt;i&gt;Ge B Y&lt;/i&gt;,&lt;i&gt;Zuo X Z&lt;/i&gt;,&lt;i&gt;Hu Y J&lt;/i&gt;.&lt;i&gt;Long&lt;/i&gt;-&lt;i&gt;term object tracking based on feature fusion&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Optica Sinica&lt;/i&gt;,2018,38(11):1115002.葛宝义,左宪章,胡永江.基于特征融合的长时目标跟踪算法[&lt;i&gt;J&lt;/i&gt;].光学学报,2018,38(11):1115002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811025&amp;v=MjEwNDllVnZGeXprVkx6UElqWFRiTEc0SDluTnJvOUhZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                         &lt;i&gt;Ge B Y&lt;/i&gt;,&lt;i&gt;Zuo X Z&lt;/i&gt;,&lt;i&gt;Hu Y J&lt;/i&gt;.&lt;i&gt;Long&lt;/i&gt;-&lt;i&gt;term object tracking based on feature fusion&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Acta Optica Sinica&lt;/i&gt;,2018,38(11):1115002.葛宝义,左宪章,胡永江.基于特征融合的长时目标跟踪算法[&lt;i&gt;J&lt;/i&gt;].光学学报,2018,38(11):1115002.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-05-08 11:51</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(09),236-244 DOI:10.3788/AOS201939.0915001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>自适应特征融合和模型更新的相关滤波跟踪</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="javascript:;">常敏</a>
                                <a href="javascript:;">沈凯</a>
                                <a href="javascript:;">张学典</a>
                                <a href="javascript:;">杜嘉</a>
                                <a href="javascript:;">李峰</a>
                </h2>
                    <h2>

                    <span>上海理工大学光电信息与计算机工程学院</span>
                    <span>上海市现代光学系统重点实验室</span>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对复杂场景下单个特征的稳健性差,以及目标存在背景干扰和目标遮挡时跟踪失败的问题,提出一种基于自适应特征融合和模型更新的相关滤波跟踪算法。该算法在核相关滤波的基础上,通过对不同特征的响应图采用平均峰值-相关能量的方法进行加权求和,实现了响应图层面的自适应特征融合。根据响应图的峰值特性计算自适应权重,以其作为置信度确定模型的更新率,进而设计自适应模型更新方法。实验结果表明,该算法能够很好地适应背景干扰、目标遮挡、旋转运动等复杂场景,与近年来优秀的相关滤波跟踪算法相比,所提算法的平均距离精度比其中最优的算法提高了2.64%,平均重叠精度提高了1.54%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标跟踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%85%B3%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相关滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E9%80%82%E5%BA%94%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自适应特征融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E5%9E%8B%E6%9B%B4%E6%96%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模型更新;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    常敏,E-mail:changmin@usst.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-05</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重大仪器专项(2016YFF0101400);</span>
                    </p>
            </div>
                    <h1><b>Correlation Filter Tracking Based on Adaptive Feature Fusion and Model Updating</b></h1>
                    <h2>
                    <span>Chang Min</span>
                    <span>Shen Kai</span>
                    <span>Zhang Xuedian</span>
                    <span>Du Jia</span>
                    <span>Li Feng</span>
            </h2>
                    <h2>
                    <span>School of Optical-Electrical and Computer Engineering,University of Shanghai for Science and Technology</span>
                    <span>Shanghai Key Lab of Modern Optical System</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To address the poor robustness of single feature in a complex scene and tracking failure caused by background interference and object occlusion, this study proposes a correlation filter tracking algorithm that combines adaptive feature fusion and adaptive model update. Based on kernel correlation filtering, the proposed algorithm performs weighted summation on the response maps of different features by adopting the average peak-correlation energy method to realize adaptive feature fusion of response maps. The adaptive weight is calculated as the confidence according to the peak characteristics of the response maps to determine the update rate of the model,thereby realizing the design of an adaptive model updating method. Experimental results demonstrate that the algorithm can adapt to complex scene changes, such as background disturbance, object occlusion, and rotational motion. Compared to popular correlation filtering tracking algorithms, the proposed algorithm increases the average distance and overlapping precision by 2.64% and 1.54%, respectively.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=object%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">object tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=correlation%20filter&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">correlation filter;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adaptive%20feature%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adaptive feature fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=model%20updating&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">model updating;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-05</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="58" name="58" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="59">目标跟踪技术是计算机视觉领域的基础性研究,在智能监控、人机交互和军事制导等领域具有广泛应用,但在目标变形、目标遮挡、快速或不规则运动等干扰下,目标精确定位仍然难以实现<citation id="144" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。为此,研究人员提出了大量的目标跟踪算法,Comaniciu等<citation id="145" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>将均值漂移理论应用到目标跟踪领域,采用将候选区域以概率密度函数的梯度方向移动到目标位置的局部最优方法,取得了很好的跟踪效果,但当目标快速运动时,概率密度函数梯度方向不明显,易导致跟踪失败;Nummiaro等<citation id="146" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>利用粒子滤波原理,将运动目标跟踪问题转化为贝叶斯估计问题,使得跟踪性能得到明显提升,但采用颜色统计直方图作为目标特征表达过于单一,结果受背景干扰的影响;Hare等<citation id="147" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出的核化结构支持向量机跟踪方法在结构化输出空间中引入基于结构化损失的约束,使得样本选择更可靠,跟踪效果得到提升,但速度方面有待提高。</p>
                </div>
                <div class="p1">
                    <p id="60">在基于学习的跟踪方法中,基于相关滤波与基于深度学习的方法成为主流跟踪算法。其中,基于相关滤波的跟踪方法具有速度快、精度高、稳健性好的特点<citation id="148" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,因而相关研究较深入。Bolme等<citation id="149" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>首次将相关滤波器引入到跟踪应用,并提出了最小均方误差和(MOSSE)滤波跟踪器,该算法利用图像灰度特征训练分类器,提高了跟踪的速度;Henriques等<citation id="150" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>通过介绍改进MOSSE算法的核函数,提出了循环结构核(CSK),利用循环矩阵性质实现了密集采样,解决了训练样本不足的问题;此后,Henriques等<citation id="151" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>又提出了核相关滤波(KCF)算法,在岭回归中引入核函数,利用多通道的方向梯度直方图(HOG)特征替代单通道的灰度特征,提高了跟踪的精度;Danelljan等<citation id="152" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>利用颜色属性特征(CN)扩展CSK算法,采用主成分分析将11维颜色特征降至2维,提升了跟踪的精度,在目标形变、运动模糊等干扰下具有良好的跟踪效果。</p>
                </div>
                <div class="p1">
                    <p id="61">然而,上述方法只基于单个特征对目标进行外观描述,当目标外观变化时,模型误差易积累,从而影响跟踪器的稳健性。因此,多个特征融合描述目标的方法被引入到相关滤波器中,Li等<citation id="153" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出尺度自适应多特征跟踪器(SAMF),串联灰度特征、CN和HOG,提升了跟踪器的整体性能;Bertinetto等<citation id="154" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出了结合模板和像素特征的分类器(Staple),分别训练颜色直方图和HOG分类器,并在响应阶段按固定比例进行融合,实现了实时稳健的跟踪;另外,大量基于深度学习的算法相继提出,Ma等<citation id="155" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>利用分层卷积神经网络(CNN)特征建立目标外观模型,对每一层卷积层分别训练相关滤波器,线性加权融合各分层响应,实现目标位置的准确定位;Danelljan等<citation id="156" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出了连续卷积相关滤波跟踪算法(CCOT),结合CNN、HOG、CN进行目标跟踪,跟踪精度得到了显著提升。上述研究表明,图像特征的选择对跟踪效果具有很大影响。基于相关滤波的算法尽管能够实时跟踪目标,但由于特征描述能力有限而影响跟踪器的性能。基于深度学习的算法尽管跟踪效果很好,但其复杂特征的提取增加了运算量,算法的实时性受到了影响。</p>
                </div>
                <div class="p1">
                    <p id="62">上述方法在准确性和稳健性方面都获得了很好的跟踪效果,但是,目标在出现局部遮挡、旋转运动以及背景干扰等复杂场景的情况下,仍会发生跟踪错误或跟丢的问题<citation id="157" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>,此外,基于简单线性插值的模型更新机制容易导致模型退化,从而导致跟踪器发生漂移<citation id="158" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。为保证跟踪实时性,同时考虑到算法性能,本文在核相关滤波跟踪算法的框架上,提出一种基于自适应特征融合和模型更新的相关滤波跟踪算法,对不同特征的响应图采用平均峰值-相关能量(APCE)方法<citation id="159" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>进行自适应融合,解决了单一特征稳健性差的问题;采用自适应模型更新方法确定相关滤波模型的更新率,使滤波器能够适应目标外观的变化,解决了目标遮挡造成的目标跟踪失败的问题。</p>
                </div>
                <h3 id="63" name="63" class="anchor-tag">2 本文算法</h3>
                <h4 class="anchor-tag" id="64" name="64"><b>2.1 核相关滤波器</b></h4>
                <div class="p1">
                    <p id="65">核相关滤波跟踪算法中,在前一帧中目标中心位置上,选取高度为<i>H</i>、宽度为<i>W</i>的矩形区域图像块<i><b>x</b></i>训练线性分类器模型<i>f</i>(<i><b>x</b></i>)=〈<i><b>w</b></i>,<i>φ</i>(<i><b>x</b></i>)〉,分类器将<i><b>x</b></i>的循环移位图像块作为训练样本<i><b>x</b></i><sub><i>i</i></sub>(<i>i</i>为样本序号),提取特征后,对应的标签<i><b>y</b></i><sub><i>i</i></sub>为高斯函数与<i><b>x</b></i><sub><i>i</i></sub>对应的期望输出。采用正则化最小二乘分类器并引入核函数,得到目标函数模型为</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">w</mi><mo>=</mo><mrow><mi>arg</mi></mrow><msub><mrow></mrow><mi>w</mi></msub><mi>min</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mrow><mo stretchy="false">[</mo><mo>〈</mo><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>φ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>〉</mo><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">]</mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">式中:<i><b>w</b></i>为分类器权重系数;〈·,·〉表示内积;<i>φ</i>(<i><b>x</b></i>)表示内核<i>k</i>从原始空间到Hilbert特征空间的映射;<i>λ</i>为控制过拟合的正则化项。</p>
                </div>
                <div class="p1">
                    <p id="68">因此,目标解为<mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>φ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>,引入核函数<i><b>k</b></i>(<i><b>x</b></i>,<i><b>x</b></i>′)=〈<i>φ</i>(<i><b>x</b></i>),<i>φ</i>(<i><b>x</b></i>′)〉求解(<i><b>x</b></i>′为图像块)。利用离散傅里叶变换和循环矩阵的性质,得到核化正则化最小二乘最优解为</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">α</mi><mo>=</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mrow><mo>[</mo><mrow><mfrac><mrow><mi mathvariant="script">F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="script">F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">k</mi><msup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><mi mathvariant="bold-italic">x</mi></mrow></msup><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi></mrow></mfrac></mrow><mo>]</mo></mrow><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">式中:<font face="EU-HT">F</font>和<font face="EU-HT">F</font><sup>(-1)</sup>分别表示离散傅里叶变换及其逆变换;<i><b>k</b></i><sup><i><b>xx</b></i></sup>=<i><b>k</b></i>(<i><b>x</b></i>,<i><b>x</b></i>)表示训练样本的自相关核。选用高斯核作为核函数,定义为</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">k</mi><msup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup></mrow></msup><mo>=</mo><mi>exp</mi><mo stretchy="false">{</mo><mo>-</mo><mfrac><mn>1</mn><mrow><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">{</mo><mrow><mo>|</mo><mi mathvariant="bold-italic">x</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo>|</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo></mtd></mtr><mtr><mtd><mn>2</mn><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">[</mo><mi mathvariant="script">F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>⊙</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo stretchy="false">}</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">式中:⊙表示元素点积;<i>σ</i>为高斯核函数带宽参数;*表示共轭。</p>
                </div>
                <div class="p1">
                    <p id="73">在新的一帧中,获取与前一帧大小相同的窗口图像块<i>z</i>预测目标的位置,分类器的输出响应为</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover><mo>=</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">[</mo><mi mathvariant="script">F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">k</mi><msup><mrow></mrow><mrow><mi mathvariant="bold-italic">z</mi><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover></mrow></msup><mo stretchy="false">)</mo><mo>⊙</mo><mi mathvariant="script">F</mi><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">式中:<mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>为输出响应值;<mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover></math></mathml>和<mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover></math></mathml>分别为学习得到的目标外观模型和滤波器参数;<mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">k</mi><msup><mrow></mrow><mrow><mi mathvariant="bold-italic">z</mi><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover></mrow></msup></mrow></math></mathml>表示候选区域<i><b>z</b></i>与目标外观模型<mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover></math></mathml>之间的核相关。</p>
                </div>
                <div class="p1">
                    <p id="76">本文跟踪算法为核相关滤波器的改进,主要差别体现在自适应特征融合模块与模型更新模块两部分。图1为跟踪算法框架。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909028_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文跟踪算法框架" src="Detail/GetImg?filename=images/GXXB201909028_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文跟踪算法框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909028_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Framework of proposed tracking algorithm</p>

                </div>
                <div class="p1">
                    <p id="78">首先,提取3种特征,分别为纹理特征<citation id="160" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>(LBP)、CN和HOG特征,HOG能够较好地适应目标的快速运动和形变,采用31通道HOG描述目标<citation id="161" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>,将两个互补特征LBP和CN串行融合得到融合特征(MIX,<i><b>M</b></i><sub>IX</sub>);然后,基于MIX和HOG分别训练相关滤波器,将两个相关滤波器得到的响应图通过APCE方法合并,得到最终响应图,进而确定目标的位置;此外,采用自适应模型更新方法确定跟踪器的模型更新率,即根据响应图计算自适应权重作为置信度,进一步确定模型的更新率以提升跟踪性能。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79"><b>2.2 自适应特征融合</b></h4>
                <div class="p1">
                    <p id="80">为了改善跟踪结果,将两个互补特征LBP和CN融合之后得到MIX,使用MIX和HOG分别训练相关滤波器,并构建两个独立的外观模型,采用APCE方法自适应地融合两个相关滤波器产生的响应图,最后在最终响应图中搜索峰值,确定目标位置。</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81">2.2.1 特征融合</h4>
                <div class="p1">
                    <p id="82">核相关滤波器仅需计算傅里叶域中特征的元素点积和向量范数,因此,可以提取图像的多通道特征,将其作为输入<citation id="162" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>,则(3)式可表示为</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">k</mi><msup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup></mrow></msup><mo>=</mo><mi>exp</mi><mrow><mo>{</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mrow><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">[</mo><mrow><mo>|</mo><mi mathvariant="bold-italic">x</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo>|</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo><mn>2</mn><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><mo>}</mo></mrow><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">式中:<i>D</i>=<font face="EU-HT">F</font>(<i><b>x</b></i>)⊙<font face="EU-HT">F</font><sup>*</sup>(<i><b>x</b></i>′),<i><b>x</b></i>=[<i><b>x</b></i><sub>1</sub><i><b>x</b></i><sub>2</sub> … <i><b>x</b></i><sub><i>d</i></sub>]表示图像的<i>d</i>通道特征。特征的选择是目标跟踪中非常关键的一步,选择合适的特征可以显著提高跟踪性能。根据(5)式,可以在LBP的基础上引入CN。</p>
                </div>
                <div class="p1">
                    <p id="85">LBP指局部二值模式,LBP算子定义在3×3的邻域内,以中心像素点的灰度值为阈值,将相邻的8个像素值与中心像素值进行比较,若大于中心像素值,则将该像素点的值记为1,否则为0。因此,该邻域内可产生8位二进制数,将其按特定顺序排列形成一组8位二进制数,这组二进制数(0～255)则表示中心像素点的LBP值。LBP值描述像素点周围区域的纹理信息,该特征计算速度快,具有灰度不变性和旋转不变性<citation id="163" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。CN颜色特征是将RGB空间的3维颜色特征映射到黑、蓝、棕、灰、绿、橙、粉、紫、红、白和黄的11维颜色空间特征<citation id="164" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。CN可以对不同颜色物体进行分离,对颜色区别显著且纹理形状相似的目标和背景具有很好的区分度。当目标与背景颜色相近或存在背景噪声时,CN无法对目标进行稳定描述,可以利用LBP对目标进行补充描述以弥补CN的缺点。</p>
                </div>
                <div class="p1">
                    <p id="86">采用LBP谱的统计直方图作为特征向量,通过VLFeat工具箱中的LBP等价模式进行LBP提取,采用8邻域像素的LBP算子,其特征维数为58维。CN采用自适应颜色属性算法将RGB空间映射到区分度明显的11维颜色空间中,得到11维颜色特征向量,再采用主成分分析将其降为2维颜色特征<citation id="165" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。因此,将LBP和CN串行组合成<i><b>M</b></i><sub>IX</sub>,假设LBP和CN向量分别为<i><b>L</b></i><sub>m</sub>(<i>m</i>=1,2,…,58)和<i><b>C</b></i><sub><i>n</i></sub>(<i>n</i>=1,2),<i><b>L</b></i><sub><i>m</i></sub>和<i><b>C</b></i><sub><i>n</i></sub>分别表示图像第<i>m</i>通道的LBP和第<i>n</i>通道的CN,则<i><b>M</b></i><sub>IX</sub>=[<i><b>L</b></i><sub>1</sub><i><b>L</b></i><sub>2</sub> … <i><b>L</b></i><sub>58</sub><i><b>C</b></i><sub>1</sub><i><b>C</b></i><sub>2</sub>],即将训练图像块提取的58通道的LBP和2通道的CN进行串行融合,得到60通道的MIX,将其作为(5)式的输入,对相关滤波器进行训练,构建具有稳健性的目标外观,从而实现优势互补并提高滤波器的性能。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87">2.2.2 自适应融合方法</h4>
                <div class="p1">
                    <p id="88">首先,使用两个由MIX和HOG训练的相关滤波器检测所有候选图像块中的最相关位置:</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">[</mo><mi mathvariant="script">F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">k</mi><msubsup><mrow></mrow><mrow><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow><mrow><mi mathvariant="bold-italic">x</mi><mi mathvariant="bold-italic">z</mi></mrow></msubsup><mo stretchy="false">)</mo><mo>⊙</mo><mi mathvariant="script">F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">a</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>Ι</mtext><mtext>X</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="script">F</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">[</mo><mi mathvariant="script">F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">k</mi><msubsup><mrow></mrow><mrow><mtext>Μ</mtext><mtext>Ι</mtext><mtext>X</mtext></mrow><mrow><mi mathvariant="bold-italic">x</mi><mi mathvariant="bold-italic">z</mi></mrow></msubsup><mo stretchy="false">)</mo><mo>⊙</mo><mi mathvariant="script">F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">a</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>Ι</mtext><mtext>X</mtext></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">式中:<i><b>a</b></i><sub>HOG</sub>和<i><b>a</b></i><sub>MIX</sub>分别表示由HOG和MIX训练的相关滤波器;<i><b>k</b></i><sup><i><b>xz</b></i></sup><sub>HOG</sub>和<i><b>k</b></i><sup><i><b>xz</b></i></sup><sub>MIX</sub>分别表示HOG和MIX的训练样本<i><b>x</b></i>与候选区域<i><b>z</b></i>之间的核相关;⊙表示点乘;<font face="EU-HT">F</font>和<font face="EU-HT">F</font><sup>-1</sup>分别代表离散傅里叶变换及其逆变换;<i>f</i><sub>HOG</sub>(<i><b>z</b></i>)和<i>f</i><sub>MIX</sub>(<i><b>z</b></i>)分别表示由HOG和MIX的滤波器产生的响应图,线性组合两个响应图即可得到最终响应图<i>f</i>(<i><b>z</b></i>):</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>=</mo><mi>ρ</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow></msub><mi>f</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>+</mo><mi>ρ</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>Ι</mtext><mtext>X</mtext></mrow></msub><mi>f</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>Ι</mtext><mtext>X</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">式中:<i>ρ</i><sub>HOG</sub>和<i>ρ</i><sub>MIX</sub>为融合权重值。本文将<i>A</i><sub>PCE</sub>作为每个输出响应图的置信度指标确定<i>ρ</i><sub>HOG</sub>和<i>ρ</i><sub>MIX</sub>,该指标有效反映了响应图的波动程度和可靠性。定义为</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ω</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">ω</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>,</mo><mi>j</mi></mrow></munder><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ω</mi><msub><mrow></mrow><mrow><mi>s</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">ω</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>]</mo></mrow></mrow></mfrac><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">式中:<i>ω</i><sub>max</sub>、<i>ω</i><sub>min</sub>和<i>ω</i><sub><i>s</i></sub><sub>,</sub><sub><i>j</i></sub>分别表示响应图中的最大值、最小值和坐标位置为(<i>s</i>,<i>j</i>)的响应值。进一步对其进行归一化,即将当前<i>A</i><sub>PCE</sub>值与之前的<i>A</i><sub>PCE</sub>值中的最大值进行归一化,得到</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>U</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext></mrow></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>A</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext></mrow></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mrow><mi>max</mi><mo stretchy="false">{</mo><mi>A</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mo>,</mo><mi>A</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext></mrow></msub><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>A</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext></mrow></msub><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow></mfrac><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">式中:<i>U</i><sub>APCE</sub>(<i>t</i>)表示归一化的<i>A</i><sub>PCE</sub>(<i>t</i>),<i>t</i>为帧的索引。最后,HOG和MIX的权重分别为</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>ρ</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>U</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext><mo>,</mo><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mrow><mi>U</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext><mo>,</mo><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mi>U</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext><mo>,</mo><mtext>Μ</mtext><mtext>Ι</mtext><mtext>X</mtext></mrow></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>ρ</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>Ι</mtext><mtext>X</mtext></mrow></msub><mo>=</mo><mn>1</mn><mo>-</mo><mi>ρ</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow></msub><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">式中:<i>U</i><sub>APCE,HOG</sub>(<i>t</i>)、<i>U</i><sub>APCE,MIX</sub>(<i>t</i>)分别表示HOG和MIX的<i>U</i><sub>APCE</sub>(<i>t</i>)。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>2.3 自适应模型更新策略</b></h4>
                <div class="p1">
                    <p id="100">在整个跟踪过程中,目标的外观模型应随时更新,这样才能使跟踪器对目标的外观变化具有稳健性<citation id="166" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。本文提出一种自适应更新方法以确定跟踪器的模型更新率,即定义一个自适应权重<i>β</i>,<i>β</i>定义为当前帧融合响应图的最大响应与之前帧融合响应图的最大响应之比可表示为</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mfrac><mrow><mi>max</mi><mo stretchy="false">[</mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><mrow><mi>max</mi><mo stretchy="false">{</mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo><mo>,</mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo><mo>,</mo><mo>⋯</mo><mo>,</mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow></mfrac><mo>,</mo></mtd><mtd columnalign="left"><mi>t</mi><mo>≥</mo><mn>4</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>1</mn><mo>,</mo></mtd><mtd columnalign="left"><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn></mtd></mtr></mtable></mrow></mrow><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">式中:<i><b>p</b></i>(<i>t</i>)表示当前帧<i>t</i>的融合响应图,第1～3帧为初始帧,第1帧中目标位置给定,所以第1帧中滤波器无输出响应;第2帧中滤波器根据第1帧给定的真实位置截取图像块,并计算输出响应,其不同于后续每一帧中滤波器根据前一帧预测位置截取图像块并计算输出响应,因为预测位置与真实位置存在一定的距离误差,且第2帧响应图的峰值较高,因此,为了使<i>β</i>能够更有效地根据目标外观模型而自适应变化,从第4帧开始计算。</p>
                </div>
                <div class="p1">
                    <p id="103">相关滤波器的跟踪模型由滤波器参数<mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover></math></mathml>和目标外观模型<mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover></math></mathml>组成,在第t帧中,<i>HOG</i>和<i>MIX</i>分别训练的相关滤波器跟踪模型<mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>,</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>的更新方式为</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>β</mi><mi>L</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false">)</mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>β</mi><mi>L</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext></mrow></msub><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>β</mi><mi>L</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false">)</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>β</mi><mi>L</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext></mrow></msub><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105">式中:L<sub><i>pre</i></sub>为初始学习率,取值为0.015。</p>
                </div>
                <h3 id="106" name="106" class="anchor-tag">3 实验分析</h3>
                <h4 class="anchor-tag" id="107" name="107"><b>3.1 算法分析</b></h4>
                <div class="p1">
                    <p id="108">以标准数据集<i>OTB</i>-50中<i>singer</i>2与<i>David</i>3视频序列为例,对提出的自适应融合方法与自适应模型更新策略进行实验分析。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109">3.1.1 特征自适应融合</h4>
                <div class="p1">
                    <p id="110">对于视频序列<i>singer</i>2,以<i>HOG</i>和<i>MIX</i>训练的相关滤波器对每一帧图像产生的响应图为基础,采用<i>APCE</i>作为置信度指标,计算<i>HOG</i>权重值和<i>MIX</i>权重值,图2所示为<i>HOG</i>和<i>MIX</i>在每帧的自适应权重值。</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909028_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 HOG和MIX自适应权重值" src="Detail/GetImg?filename=images/GXXB201909028_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 <i>HOG</i>和<i>MIX</i>自适应权重值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909028_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Adaptive weight values for HOG and MIX</i></p>

                </div>
                <div class="p1">
                    <p id="112">由图2可知,特征权重在跟踪过程中不断变化,<i>MIX</i>的权重值整体大于<i>HOG</i>的权重值,这是由于在<i>singer</i>2视频序列中,目标运动过程中存在光照剧烈变化和背景复杂多变的干扰,而以上的情况有利于<i>LBP</i>描述,同时整个过程中目标与背景的颜色区分明显,有利于<i>CN</i>实现不同颜色物体的分离,而目标在这些场景下不利于<i>HOG</i>描述。因此,采用<i>APCE</i>方法后,两者融合后<i>MIX</i>的权重值整体上大于<i>HOG</i>值。图3所示为<i>singer</i>2视频序列中第179帧及其对应的响应图。</p>
                </div>
                <div class="p1">
                    <p id="113">由图3可以看出,目标存在背景干扰,此时目标与背景颜色能够区分,<i>MIX</i>响应图的峰值较高且集中突出,此情况不利于<i>HOG</i>描述,从而导致该帧的<i>HOG</i>响应图分布情况较分散,响应值低。因此,采用<i>APCE</i>方法可以为<i>HOG</i>分配较小权重值,减小<i>HOG</i>的影响,而<i>MIX</i>自然就分配较大权重,融合后的响应图能够准确确定目标。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909028_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 singer2视频序列第179帧与对应的响应图。" src="Detail/GetImg?filename=images/GXXB201909028_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 singer2视频序列第179帧与对应的响应图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909028_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 179th frame and corresponding response maps of singer2video.</p>
                                <p class="img_note">(a）第179帧；（b)MIX;(c)HOG;(d）融合</p>
                                <p class="img_note">(a)179th frame;(b)MIX;(c)HOG;(d)fusion</p>

                </div>
                <h4 class="anchor-tag" id="115" name="115">3.1.2 模型自适应更新</h4>
                <div class="p1">
                    <p id="116">针对目标被遮挡的跟踪问题,对于相关滤波跟踪,其滤波响应图能够有效反映跟踪质量,当目标处于遮挡情况时,响应图的峰值会急剧下降<citation id="167" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。图4所示为视频序列<i>David</i>3中最大响应值与自适应权重以及部分帧。</p>
                </div>
                <div class="p1">
                    <p id="117">由图4可知,在<i>David</i>3中的第24,83,187帧中,由于广告牌和大树的遮挡,跟踪下的行人几乎消失,此时响应图的最大响应值急剧减小,如果模型继续按原始学习率更新,那么模型将会逐渐弱化,因此,此处的更新率应降低。当响应图的最大响应较小时,(13)式中的自适应权重β也会随之减小,从而使得模型更新率减小,这样可以有效处理目标的遮挡情况,从而避免模型漂移,提高跟踪器的稳健性。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118"><b>3.2 实验环境及设置</b></h4>
                <div class="p1">
                    <p id="119">实验硬件环境为<i>Intel Core i</i>5-3230<i>M</i>,主频2.6 <i>GHz</i>,内存4 <i>GB</i>配置的计算机,实验平台为<i>Matlab R</i>2016<i>b</i>软件。实验测试序列来源于标准跟踪数据集<i>OTB</i>-50<citation id="168" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>中的36组彩色序列,视频序列中包含目标跟踪中常见的复杂场景,包括快速运动、背景干扰、运动模糊、尺度变化、光照变化、目标旋转、低分辨率、目标遮挡等。将本文方法与<i>CCOT</i>、<i>SAMF</i>、<i>Staple</i>、<i>KCF</i>、<i>DSST</i><citation id="169" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、<i>CN</i>、<i>CSK</i>跟踪算法进行定量和定性比较。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120"><b>3.3 定量评估</b></h4>
                <div class="p1">
                    <p id="121">根据<i>OTB</i>数据集的评估方法,采用一次通过评估(<i>OPE</i>)模式,选择常用的评估指标评估跟踪器性能,即跟踪距离精度(<i>DP</i>)、跟踪重叠精度(<i>OP</i>)和算法时间复杂度。跟踪距离精度是指计算中心位置误差(<i>CLE</i>)低于某个阈值的帧数占总帧数的百分比,其中<i>CLE</i>定义为检测值与真实目标中心位置之间的平均欧氏距离,<i>CLE</i>数值越小表示跟踪精度越高;跟踪重叠精度即为跟踪成功率,是指跟踪重叠率大于某个阈值的帧数占视频总帧数的百分比,其中重叠率指目标跟踪框与真实目标框的重叠面积与两者并集的总面积之比,重叠率越大表示跟踪成功率越高。根据<i>PASCAL</i>评价指标<citation id="170" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>,设定距离精度阈值为20 <i>pixel</i>和重叠率阈值为0.5。算法时间复杂度采用跟踪算法每秒处理的图像帧数(<i>FPS</i>)衡量,<i>FPS</i>定义为目标跟踪的视频总帧数与跟踪算法的总耗时之比,<i>FPS</i>越高表示算法的时间复杂度越低<citation id="171" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>。表1给出了本文算法与其他7种算法在36组序列上的平均<i>DP</i>、<i>OP</i>、<i>FPS</i>,图5所示为8种跟踪算法相应的距离精度曲线和成功率曲线。</p>
                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909028_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 David3中最大响应值与自适应权重以及部分帧。" src="Detail/GetImg?filename=images/GXXB201909028_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 David3中最大响应值与自适应权重以及部分帧。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909028_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Maximum response value,adaptive weight,and partial frames in David3.</p>
                                <p class="img_note">(a）响应图的最大响应与自适应权重；（b）部分帧</p>
                                <p class="img_note">(a)Maximum response map and adaptive weight;(b)partial frames</p>

                </div>
                <div class="area_img" id="123">
                    <p class="img_tit">表1 8种跟踪算法的平均跟踪性能指标(36组彩色序列) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Table</i> 1 <i>Average tracking performance indexes of eight tracking algorithms on</i> 36 <i>color sequences</i></p>
                    <p class="img_note"></p>
                    <table id="123" border="1"><tr><td><i>Algorithm</i></td><td><i>Proposed</i></td><td><i>CCOT</i></td><td><i>SAMF</i></td><td><i>Staple</i></td><td><i>KCF</i></td><td><i>DSST</i></td><td><i>CN</i></td><td><i>CSK</i></td></tr><tr><td><i>Mean DP</i> /%</td><td><b>85.4</b></td><td>83.2</td><td>79.0</td><td>78.2</td><td>77.0</td><td>76.7</td><td>67.8</td><td>55.2</td></tr><tr><td><br /><i>Mean OP</i> /%</td><td><b>79.2</b></td><td>78.0</td><td>72.0</td><td>75.2</td><td>63.8</td><td>68.9</td><td>52.9</td><td>41.8</td></tr><tr><td><br /><i>Mean FPS</i> /(<i>frame</i>·<i>s</i><sup>-1</sup>)</td><td>28.39</td><td>31.42</td><td>17.53</td><td>29.26</td><td>149.60</td><td>28.12</td><td>63.23</td><td>264.33</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909028_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 8种跟踪算法的距离精度曲线和成功率曲线比较（36组彩色序列）。" src="Detail/GetImg?filename=images/GXXB201909028_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 8种跟踪算法的距离精度曲线和成功率曲线比较（36组彩色序列）。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909028_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Comparison of distance precision plots and success rate plots of eight tracking algorithms on 36color sequences.</p>
                                <p class="img_note">(a）距离精度曲线；（b）成功率曲线</p>
                                <p class="img_note">(a)Distance precision plots;(b)success rate plots</p>

                </div>
                <div class="p1">
                    <p id="125">由表1可知,在36组彩色视频序列上,本文方法的平均<i>DP</i>、平均<i>OP</i>分别为85.4%和79.2%,获得了最佳的结果,比采用融合<i>HOG</i>和<i>CN</i>的算法<i>CCOT</i>提高了2.64%和1.54%,跟踪性能具有一定程度的提升。在时间复杂度方面,跟踪速度是<i>KCF</i>和<i>CSK</i>算法的最大优势,分别达到149.60 <i>frame</i>/<i>s</i>和264.33 <i>frame</i>/<i>s</i>,但其跟踪精度表现不佳,因此其综合性能仍然不足。本文算法在基于核相关滤波跟踪算法的框架上融合了多个特征,继承了相关滤波跟踪的速度优势,在提高跟踪精度的前提下,其平均<i>FPS</i>为28.39 <i>frame</i>/<i>s</i>,能够应用于实时跟踪场合中。由图5可知,在所比较的跟踪算法中,本文算法所得结果最优,其次为<i>CCOT</i>算法,两者均明显优于<i>KCF</i>和<i>DSST</i>算法,这说明本文算法具备较好的跟踪性能。</p>
                </div>
                <h4 class="anchor-tag" id="126" name="126"><b>3.4 定性评估</b></h4>
                <div class="p1">
                    <p id="127">图6所示为6种算法的跟踪结果(左上角数字为帧序号)。</p>
                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201909028_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 6种算法的跟踪结果。" src="Detail/GetImg?filename=images/GXXB201909028_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 6种算法的跟踪结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201909028_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Tracking results of six tracking algorithms.</p>
                                <p class="img_note">(a)Couple;(b)Lemming;(c)Liquor;(d)Girl;(e)Motor Rolling</p>
                                <p class="img_note">(a)Couple;(b)Lemming;(c)Liquor;(d)Girl;(e)Motor Rolling</p>

                </div>
                <div class="p1">
                    <p id="129">背景干扰:<i>Couple</i>序列中,目标在行走过程中姿态发生变化,同时受到周围背景中来往车辆的干扰,本文算法和<i>CCOT</i>算法可以持续准确跟踪目标,而其他算法都已经发生严重漂移。第91帧处背景存在运动车辆和相近颜色的干扰,<i>SAMF</i>、<i>Staple</i>和<i>KCF</i>算法跟踪失败,在后续帧中只有本文算法和<i>CCOT</i>算法能够继续保持准确跟踪。</p>
                </div>
                <div class="p1">
                    <p id="130">局部遮挡:<i>Lemming</i>序列中,只有本文算法和<i>SAMF</i>算法能够长期准确跟踪目标,而其他跟踪算法都未能持续跟踪目标。第454帧小熊在运动过程中被游标卡尺局部遮挡,本文算法能够处理遮挡情况,准确跟踪到了目标。</p>
                </div>
                <div class="p1">
                    <p id="131">类目标干扰:<i>Liquor</i>和<i>Girl</i>序列中,目标在运动过程中背景复杂且多变,背景中存在相似目标的干扰。<i>Liquor</i>第1519帧中,在移动另一个酒瓶过程中,酒瓶对目标产生了干扰,除了本文算法、<i>CCOT</i>、<i>Staple</i>和<i>KCF</i>算法,其他算法均跟踪失败;<i>Girl</i>中第443帧和第471帧处出现相似人脸的干扰后,只有本文算法和<i>CCOT</i>算法可以持续准确跟踪目标。这是由于本文算法结合了多种特征,从而实现了目标的准确跟踪,说明本文算法具有很好的抗干扰能力和稳健性。</p>
                </div>
                <div class="p1">
                    <p id="132">旋转变形:<i>Motor Rolling</i>序列中,目标在快速运动过程中伴随姿态发生旋转变形和光照剧烈变化,只有本文算法能够始终准确稳定跟踪第89帧目标的旋转运动,其他跟踪算法都发生了漂移,本文算法能够持续准确跟踪目标。</p>
                </div>
                <h3 id="133" name="133" class="anchor-tag">4 结  论</h3>
                <div class="p1">
                    <p id="134">在核相关滤波跟踪算法的框架上,提出了一种基于特征自适应融合和模型自适应更新的相关滤波跟踪算法。提取了3种特征——<i>LBP</i>、<i>CN</i>、<i>HOG</i>,将互补特征<i>LBP</i>和<i>CN</i>进行串行融合得到<i>MIX</i>,对<i>MIX</i>和<i>HOG</i>的响应图采用<i>APCE</i>方法进行合并,得到最终的响应图,从而确定了目标位置,实现了复杂场景下目标的稳定跟踪。采用自适应模型更新方法确定相关滤波模型的更新率,优化了模型更新方式,滤波器能够适应目标外观的变化。实验结果表明,本文算法提高了目标跟踪的稳健性和准确性,进一步研究重点为算法优化问题,以及如何提高跟踪方法的实时性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201610002&amp;v=MTE5MzF2Rnl6a1ZMelBLQ0xmWWJHNEg5Zk5yNDlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> <i>Yin H P</i>,<i>Chen B</i>,<i>Chai Y</i>,et al.<i>Vision</i>-<i>based object detection</i><i>and tracking</i>:<i>a review</i>[<i>J</i>].<i>Acta Automatica</i><i>Sinica</i>,2016,42(10):1466-1489.尹宏鹏,陈波,柴毅,等.基于视觉的目标检测与跟踪综述[<i>J</i>].自动化学报,2016,42(10):1466-1489.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time tracking of non-rigid objects using mean shift">

                                <b>[2]</b> <i>Comaniciu D</i>,<i>Ramesh V</i>,<i>Meer P</i>.<i>Real</i>-<i>time tracking of non</i>-<i>rigid objects using mean shift</i>[<i>C</i>]//2000 <i>IEEE Conference on Computer Vision and Pattern Recognition</i> (<i>CVPR</i>),<i>June</i> 15,2000,<i>Hilton Head Island</i>,<i>SC</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2000:142-149.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349994&amp;v=MDQ0MDRIeWptVWI3SUpGOFRhUlU9TmlmT2ZiSzdIdERPclk5RVorOEdCWFU5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> <i>Nummiaro K</i>,<i>Koller</i>-<i>Meier E</i>,<i>van Gool L</i>.<i>An adaptive color</i>-<i>based particle filter</i>[<i>J</i>].<i>Image and Vision Computing</i>,2003,21(1):99-110.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" >
                                    <b>[4]</b>
                                 <i>Hare S</i>,<i>Golodetz S</i>,<i>Saffari A</i>,et al.<i>Struck</i>:<i>structured output tracking with kernels</i>[<i>J</i>].<i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>,2016,38(10):2096-2109.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">

                                <b>[5]</b> <i>Henriques J F</i>,<i>Caseiro R</i>,<i>Martins P</i>,et al.<i>Exploiting the circulant structure of tracking</i>-<i>by</i>-<i>detection with kernels</i>[<i>M</i>]//<i>Fitzgibbon A</i>,<i>Lazebnik S</i>,<i>Perona P</i>,et al.<i>Computer vision</i>-<i>ECCV</i> 2012.<i>Berlin</i>,<i>Heidelberg</i>:<i>Springer</i>,2012:702-715.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">

                                <b>[6]</b> <i>Bolme D</i>,<i>Beveridge J R</i>,<i>Draper B A</i>,et al.<i>Visual object tracking using adaptive correlation filters</i>[<i>C</i>]//2010 <i>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</i>,<i>June</i> 13-18,2010,<i>San Francisco</i>,<i>CA</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2010:2544-2550.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">

                                <b>[7]</b> <i>Henriques J F</i>,<i>Caseiro R</i>,<i>Martins P</i>,et al.<i>High</i>-<i>speed tracking with kernelized correlation filters</i>[<i>J</i>].<i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>,2015,37(3):583-596.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive color attributes for real-time visual tracking">

                                <b>[8]</b> <i>Danelljan M</i>,<i>Khan F S</i>,<i>Felsberg M</i>,et al.<i>Adaptive color attributes for real</i>-<i>time visual tracking</i>[<i>C</i>]//2014 <i>IEEE Conference on Computer Vision and Pattern Recognition</i> (<i>CVPR</i>),<i>June</i> 23-28,2014,<i>Columbus</i>,<i>OH</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2014:1090-1097.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A scale adaptive kernel correlation filter tracker with feature integration">

                                <b>[9]</b> <i>Li Y</i>,<i>Zhu J K</i>.<i>A scale adaptive kernel correlation filter tracker with feature integration</i>[<i>M</i>]∥<i>Agapito L</i>,<i>Bronstein M M</i>,<i>Rother C</i>.<i>Computer vision</i>-<i>ECCV</i> 2014 <i>workshops</i>.<i>Cham</i>:<i>Springer</i>,2015:254-265.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">

                                <b>[10]</b> <i>Bertinetto L</i>,<i>Valmadre J</i>,<i>Golodetz S</i>,et al.<i>Staple</i>:<i>complementary learners for real</i>-<i>time tracking</i>[<i>C</i>]//2016 <i>IEEE Conference on Computer Vision and Pattern Recognition</i> (<i>CVPR</i>),<i>June</i> 27-30,2016,<i>Las Vegas</i>,<i>NV</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2016:1401-1409.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical Convolutional Features for Visual Tracking">

                                <b>[11]</b> <i>Ma C</i>,<i>Huang J B</i>,<i>Yang X K</i>,et al.<i>Hierarchical convolutional features for visual tracking</i>[<i>C</i>]∥2015 <i>IEEE International Conference on Computer Vision</i> (<i>ICCV</i>),<i>December</i> 7-13,2015,<i>Santiago</i>,<i>Chile</i>.<i>New York</i>:<i>IEEE</i>,2015:3074-3082.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond Correlation Filters:Learning Continuous Convolution Operators for Visual Tracking">

                                <b>[12]</b> <i>Danelljan M</i>,<i>Robinson A</i>,<i>Khan F S</i>,et al.<i>Beyond correlation filters</i>:<i>learning continuous convolution operators for visual tracking</i>[<i>M</i>]//<i>Leibe B</i>,<i>Matas J</i>,<i>Sebe N</i>,et al.<i>Computer vision</i>-<i>ECCV</i> 2016.<i>Cham</i>:<i>Springer</i>,2016:472-488.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201601023&amp;v=Mjk5NzlHRnJDVVJMT2VaZVZ2Rnl6a1ZMelBJalhUYkxHNEg5Zk1ybzlIWjRRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> <i>Liu W</i>,<i>Zhao W J</i>,<i>Li C</i>.<i>Long</i>-<i>term visual tracking based on spatio</i>-<i>temporal context</i>[<i>J</i>].<i>Acta Optica Sinica</i>,2016,36(1):0115001.刘威,赵文杰,李成.时空上下文学习长时目标跟踪[<i>J</i>].光学学报,2016,36(1):0115001.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201706012&amp;v=MDM0NjFyQ1VSTE9lWmVWdkZ5emtWTHpQTHo3QmFMRzRIOWJNcVk5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> <i>Xiong C Z</i>,<i>Zhao L L</i>,<i>Guo F H</i>.<i>Kernelized correlation filters tracking based on adaptive feature fusion</i>[<i>J</i>].<i>Journal of Computer</i>-<i>Aided Design</i> &amp; <i>Computer Graphics</i>,2017,29(6):1068-1074.熊昌镇,赵璐璐,郭芬红.自适应特征融合的核相关滤波跟踪算法[<i>J</i>].计算机辅助设计与图形学学报,2017,29(6):1068-1074.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large Margin Object Tracking with Circulant Feature Maps">

                                <b>[15]</b> <i>Wang M M</i>,<i>Liu Y</i>,<i>Huang Z Y</i>.<i>Large margin object tracking with circulant feature maps</i>[<i>C</i>]//2017 <i>IEEE Conference on Computer Vision and Pattern Recognition</i> (<i>CVPR</i>),<i>July</i> 21-26,2017,<i>Honolulu</i>,<i>HI</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2017:4021-4029.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" >
                                    <b>[16]</b>
                                 <i>Ojala T</i>,<i>Pietik</i>ä<i>inen M</i>,<i>M</i>ä<i>enp</i>ää <i>T</i>.<i>Multiresolution gray</i>-<i>scale and rotation invariant texture classification with local binary patterns</i>[<i>J</i>].<i>IEEE Transactions on Pattern Analysis</i> &amp; <i>Machine Intelligence</i>,2002,24(7):971-987.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object Detection with Discriminatively Trained Part-Based Models">

                                <b>[17]</b> <i>Felzenszwalb P F</i>,<i>Girshick R B</i>,<i>McAllester D</i>,et al.<i>Object detection with discriminatively trained part</i>-<i>based models</i>[<i>J</i>].<i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>,2010,32(9):1627-1645.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201705021&amp;v=MDA2MDBJalhUYkxHNEg5Yk1xbzlIWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl6a1ZMelA=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> <i>Shen Q</i>,<i>Yan X L</i>,<i>Liu L F</i>,et al.<i>Multi</i>-<i>scale correlation filtering tracker based on adaptive feature selection</i>[<i>J</i>].<i>Acta Optica Sinica</i>,2017,37(5):0515001.沈秋,严小乐,刘霖枫,等.基于自适应特征选择的多尺度相关滤波跟踪[<i>J</i>].光学学报,2017,37(5):0515001.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=In defense of color-based model-free tracking">

                                <b>[19]</b> <i>Possegger H</i>,<i>Mauthner T</i>,<i>Bischof H</i>.<i>In defense of color</i>-<i>based model</i>-<i>free tracking</i>[<i>C</i>]∥2015 <i>IEEE Conference on Computer Vision and Pattern Recognition</i> (<i>CVPR</i>),<i>June</i> 7-12,2015,<i>Boston</i>,<i>MA</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2015:2113-2120.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201608029&amp;v=MDI2NTJwNDlIYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnl6a1ZMelBJalhCWTdHNEg5Zk0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> <i>Wang W</i>,<i>Wang C P</i>,<i>Li J</i>,et al.<i>Correlation filter tracking based on feature fusing and model adaptive updating</i>[<i>J</i>].<i>Optics and Precision Engineering</i>,2016,24(8):2059-2066.王暐,王春平,李军,等.特征融合和模型自适应更新相结合的相关滤波目标跟踪[<i>J</i>].光学精密工程,2016,24(8):2059-2066.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online object tracking:A benchmark">

                                <b>[21]</b> <i>Wu Y</i>,<i>Lim J</i>,<i>Yang M H</i>.<i>Online object tracking</i>:<i>a benchmark</i>[<i>C</i>]//2013 <i>IEEE Conference on Computer Vision and Pattern Recognition</i> (<i>CVPR</i>),<i>June</i> 23-28,2013,<i>Portland</i>,<i>OR</i>,<i>USA</i>.<i>New York</i>:<i>IEEE</i>,2013:2411-2418.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate scale estimation for robust visual tracking">

                                <b>[22]</b> <i>Danelljan M</i>,<i>H</i>ä<i>ger G</i>,<i>Khan F S</i>,et al.<i>Accurate scale estimation for robust visual tracking</i>[<i>C</i>]∥<i>The British Machine Vision Conference</i> 2014,<i>September</i>,2014,<i>Nottingham</i>,<i>UK</i>.<i>Guildford</i>:<i>BMVA Press</i>,2014:1-11.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MDczNjVyTzRIdEhQcVlkSFkrSUxZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTbmxWNzdQSTFrPU5qN0Jh&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> <i>Everingham M</i>,<i>van Gool L</i>,<i>Williams C K I</i>,et al.<i>The Pascal visual object classes</i> (<i>VOC</i>) <i>challenge</i>[<i>J</i>].<i>International Journal of Computer Vision</i>,2010,88(2):303-338.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811025&amp;v=MDMxNjZPZVplVnZGeXprVkx6UElqWFRiTEc0SDluTnJvOUhZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b> <i>Ge B Y</i>,<i>Zuo X Z</i>,<i>Hu Y J</i>.<i>Long</i>-<i>term object tracking based on feature fusion</i>[<i>J</i>].<i>Acta Optica Sinica</i>,2018,38(11):1115002.葛宝义,左宪章,胡永江.基于特征融合的长时目标跟踪算法[<i>J</i>].光学学报,2018,38(11):1115002.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201909028" />
        <input id="dpi" type="hidden" value="200" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201909028&amp;v=MTA1MjVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeXprVkx6UElqWFRiTEc0SDlqTXBvOUhiSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01EbTY1YWlNVTlZaDFrek84dz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

