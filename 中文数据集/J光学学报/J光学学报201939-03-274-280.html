

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134140695752500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201903032%26RESULT%3d1%26SIGN%3dx3sp0m%252b5%252bx9l7iYQ4117%252b3i4uHU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201903032&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201903032&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201903032&amp;v=MDU4ODJYVGJMRzRIOWpNckk5R1pvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SG5WcnpPSWo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#46" data-title="1 引 言 ">1 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#50" data-title="2 改进的YOLO目标检测模型 ">2 改进的YOLO目标检测模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="&lt;b&gt;2.1 网络结构&lt;/b&gt;"><b>2.1 网络结构</b></a></li>
                                                <li><a href="#55" data-title="&lt;b&gt;2.2 SP模块&lt;/b&gt;"><b>2.2 SP模块</b></a></li>
                                                <li><a href="#61" data-title="&lt;b&gt;2.3 训练策略&lt;/b&gt;"><b>2.3 训练策略</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="3 实 验 ">3 实 验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="&lt;b&gt;3.1 实验数据集&lt;/b&gt;"><b>3.1 实验数据集</b></a></li>
                                                <li><a href="#67" data-title="&lt;b&gt;3.2 评价标准&lt;/b&gt;"><b>3.2 评价标准</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;3.3 网络模型的训练与测试实验&lt;/b&gt;"><b>3.3 网络模型的训练与测试实验</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;3.4 网络模型向嵌入式平台的移植实验&lt;/b&gt;"><b>3.4 网络模型向嵌入式平台的移植实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#94" data-title="4 结 论 ">4 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="图1 网络结构示意图">图1 网络结构示意图</a></li>
                                                <li><a href="#69" data-title="图2 数据集的示例图片。">图2 数据集的示例图片。</a></li>
                                                <li><a href="#82" data-title="表1 第一组实验结果">表1 第一组实验结果</a></li>
                                                <li><a href="#83" data-title="表2 第二组实验结果">表2 第二组实验结果</a></li>
                                                <li><a href="#84" data-title="表3 第三组实验结果">表3 第三组实验结果</a></li>
                                                <li><a href="#86" data-title="图3 第三组实验的部分检测结果对比。">图3 第三组实验的部分检测结果对比。</a></li>
                                                <li><a href="#91" data-title="表4 目标检测的速度对比">表4 目标检测的速度对比</a></li>
                                                <li><a href="#92" data-title="图4 在嵌入式平台上的检测结果。">图4 在嵌入式平台上的检测结果。</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                     LeCun Y, Bottou L, Bengio Y, &lt;i&gt;et al&lt;/i&gt;. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86 (11) : 2278-2324.</a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" Girshick R, Donahue J, Darrell T, &lt;i&gt;et al&lt;/i&gt;. Rich feature hierarchies for accurate object detection and semantic segmentation [C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[2]</b>
                                         Girshick R, Donahue J, Darrell T, &lt;i&gt;et al&lt;/i&gt;. Rich feature hierarchies for accurate object detection and semantic segmentation [C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 580-587.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" title=" He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;. Spatial pyramid pooling in deep convolutional networks for visual recognition[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) : 1904-1916." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">
                                        <b>[3]</b>
                                         He K M, Zhang X Y, Ren S Q, &lt;i&gt;et al&lt;/i&gt;. Spatial pyramid pooling in deep convolutional networks for visual recognition[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) : 1904-1916.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" title=" Girshick R. Fast R-CNN[C]. IEEE International Conference on Computer Vision, 2015: 1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[4]</b>
                                         Girshick R. Fast R-CNN[C]. IEEE International Conference on Computer Vision, 2015: 1440-1448.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" >
                                        <b>[5]</b>
                                     Ren S Q, He K M, Girshick R, &lt;i&gt;et al&lt;/i&gt;. Faster R-CNN: Towards real-time object detection with region proposal networks[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) : 1137-1149.</a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" title=" Li Y, He K, Sun J. R-FCN: Object detection via region-based fully convolutional networks[C]. Advances in Neural Information Processing Systems, 2016: 379-387." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=R-FCN:Object Detection via Region-based Fully Convolutional Networks">
                                        <b>[6]</b>
                                         Li Y, He K, Sun J. R-FCN: Object detection via region-based fully convolutional networks[C]. Advances in Neural Information Processing Systems, 2016: 379-387.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" Liu W, Anguelov D, Erhan D, &lt;i&gt;et al&lt;/i&gt;. SSD: Single shot multibox detector[C]. European Conference on Computer Vision, 2016: 21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">
                                        <b>[7]</b>
                                         Liu W, Anguelov D, Erhan D, &lt;i&gt;et al&lt;/i&gt;. SSD: Single shot multibox detector[C]. European Conference on Computer Vision, 2016: 21-37.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" title=" Redmon J, Farhadi A. YOLO9000: Better, faster, stronger[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2017: 6517-6525." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better Faster,Stronger">
                                        <b>[8]</b>
                                         Redmon J, Farhadi A. YOLO9000: Better, faster, stronger[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2017: 6517-6525.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" Redmon J, Farhadi A. Yolov3: An incremental improvement[EB/OL]. (2018-04-08) [2018-09-07]. https://arxiv.org/abs/1804.02767." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLOv3 an incremental improvement">
                                        <b>[9]</b>
                                         Redmon J, Farhadi A. Yolov3: An incremental improvement[EB/OL]. (2018-04-08) [2018-09-07]. https://arxiv.org/abs/1804.02767.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     Redmon J, Divvala S, Girshick R, &lt;i&gt;et al&lt;/i&gt;. You only look once: Unified, real-time object detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 779-788.</a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" title=" Feng X Y, Mei W, Hu D S. Aerial target detection based on improved faster R-CNN[J]. Acta Optica Sinica, 2018, 38 (6) : 0615004. 冯小雨, 梅卫, 胡大帅. 基于改进Faster R-CNN的空中目标检测[J]. 光学学报, 2018, 38 (6) : 0615004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806034&amp;v=MTc2MjZPZVplVnVGeUhuVnJ6QklqWFRiTEc0SDluTXFZOUdZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Feng X Y, Mei W, Hu D S. Aerial target detection based on improved faster R-CNN[J]. Acta Optica Sinica, 2018, 38 (6) : 0615004. 冯小雨, 梅卫, 胡大帅. 基于改进Faster R-CNN的空中目标检测[J]. 光学学报, 2018, 38 (6) : 0615004.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" title=" Xin P, Xu Y L, Tang H, &lt;i&gt;et al&lt;/i&gt;. Fast airplane detection based on multi-layer feature fusion of fully convolutional networks[J]. Acta Optica Sinica, 2018, 38 (3) : 0315003.  辛鹏, 许悦雷, 唐红, 等. 全卷积网络多层特征融合的飞机快速检测[J]. 光学学报, 2018, 38 (3) : 0315003." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201803036&amp;v=MTY1MTJySTlHWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlIblZyekJJalhUYkxHNEg5bk0=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Xin P, Xu Y L, Tang H, &lt;i&gt;et al&lt;/i&gt;. Fast airplane detection based on multi-layer feature fusion of fully convolutional networks[J]. Acta Optica Sinica, 2018, 38 (3) : 0315003.  辛鹏, 许悦雷, 唐红, 等. 全卷积网络多层特征融合的飞机快速检测[J]. 光学学报, 2018, 38 (3) : 0315003.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" title=" Iandola F N, Han S, Moskewicz M W, &lt;i&gt;et al&lt;/i&gt;. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &amp;lt;0.5 MB model size[EB/OL]. (2016-11-04) [2018-09-07]. https://arxiv.org/abs/1602.07360." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Squeeze Net:Alex Net-level accuracy with 50x fewer parameters and&amp;lt;0.5MB model size">
                                        <b>[13]</b>
                                         Iandola F N, Han S, Moskewicz M W, &lt;i&gt;et al&lt;/i&gt;. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &amp;lt;0.5 MB model size[EB/OL]. (2016-11-04) [2018-09-07]. https://arxiv.org/abs/1602.07360.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" Howard A G, Zhu M, Chen B, &lt;i&gt;et al&lt;/i&gt;. Mobilenets: Efficient convolutional neural networks for mobile vision applications[EB/OL]. (2017-04-17) [2018-09-07]. https://arxiv.org/abs/ 1704.04861." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mobilenets:efficient convolutional neural networks for mobile vision applications">
                                        <b>[14]</b>
                                         Howard A G, Zhu M, Chen B, &lt;i&gt;et al&lt;/i&gt;. Mobilenets: Efficient convolutional neural networks for mobile vision applications[EB/OL]. (2017-04-17) [2018-09-07]. https://arxiv.org/abs/ 1704.04861.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" Sandler M, Howard A, Zhu M, &lt;i&gt;et al&lt;/i&gt;. Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation[EB/OL]. (2018-04-02) [2018-09-07]. https://arxiv.org/abs/1801.04381." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inverted residuals and linear bottlenecks:Mobile networks for classification,detection and segmentation">
                                        <b>[15]</b>
                                         Sandler M, Howard A, Zhu M, &lt;i&gt;et al&lt;/i&gt;. Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation[EB/OL]. (2018-04-02) [2018-09-07]. https://arxiv.org/abs/1801.04381.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" title=" Geiger A, Lenz P, Urtasun R. Are we ready for autonomous driving? The KITTI vision benchmark suite[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2012: 3354-3361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The KITTI vision benchmark suite">
                                        <b>[16]</b>
                                         Geiger A, Lenz P, Urtasun R. Are we ready for autonomous driving? The KITTI vision benchmark suite[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2012: 3354-3361.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" title=" Johnson-Roberson M, Barto C, Mehta R, &lt;i&gt;et al&lt;/i&gt;. Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?[C]. IEEE International Conference on Robotics and Automation, 2017: 746-753." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?">
                                        <b>[17]</b>
                                         Johnson-Roberson M, Barto C, Mehta R, &lt;i&gt;et al&lt;/i&gt;. Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?[C]. IEEE International Conference on Robotics and Automation, 2017: 746-753.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" Redmon J. YOLO-tiny[EB/OL]. (2018-08-16) [2018-09-07]. https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-tiny.cfg." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLO-tiny">
                                        <b>[18]</b>
                                         Redmon J. YOLO-tiny[EB/OL]. (2018-08-16) [2018-09-07]. https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-tiny.cfg.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-11-13 10:07</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(03),274-280 DOI:10.3788/AOS201939.0315005            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>应用于嵌入式图形处理器的实时目标检测方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%99%93%E9%9D%92&amp;code=35285941&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王晓青</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%90%91%E5%86%9B&amp;code=09961107&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王向军</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A9%E6%B4%A5%E5%A4%A7%E5%AD%A6%E7%B2%BE%E5%AF%86%E6%B5%8B%E8%AF%95%E6%8A%80%E6%9C%AF%E5%8F%8A%E4%BB%AA%E5%99%A8%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0246359&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">天津大学精密测试技术及仪器国家重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出了一种应用于嵌入式图形处理器 (GPU) 的实时目标检测算法。针对嵌入式平台计算单元较少、处理速度较慢的现状, 提出了一种基于YOLO-V3 (You Only Look Once-Version 3) 架构的改进的轻量目标检测模型, 对汽车目标进行了离线训练, 在嵌入式平台上部署训练好的模型, 实现了在线检测。实验结果表明, 在嵌入式平台上, 所提方法对分辨率为640 pixel×480 pixel的视频图像的检测速度大于23 frame/s。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%B9%B3%E5%8F%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">嵌入式平台;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%BD%A2%E5%A4%84%E7%90%86%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图形处理器;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *王向军 E-mail:tjuxjw@126.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-07</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金面上项目 (51575388);</span>
                    </p>
            </div>
                    <h1><b>Real-Time Target Detection Method Applied to Embedded Graphic Processing Unit</b></h1>
                    <h2>
                    <span>Wang Xiaoqing</span>
                    <span>Wang Xiangjun</span>
            </h2>
                    <h2>
                    <span>State Key Laboratory of Precision Measuring Technology and Instruments, Tianjin University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A real-time target detection algorithm is proposed and used in the embedded graphic processing unit (GPU) . In view of the lack of computing units and the slow processing speed for an embedded platform, an improved lightweight target detection model is proposed based on the YOLO-V3 (You Only Look Once-Version 3) structure. This model is first trained off-line with vehicle targets and then deployed on the embedded GPU platform to achieve the online prediction. The experimental results show that the processing speed of the proposed method on the embedded GPU platform reaches 23 frame/s for a 640 pixel×480 pixel video.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=target%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">target detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=embedded%20platform&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">embedded platform;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=graphic%20processing%20unit&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">graphic processing unit;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-09-07</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="46" name="46" class="anchor-tag">1 引 言</h3>
                <div class="p1">
                    <p id="47">目标检测是大量高级视觉任务的必备前提, 它要求算法不仅能够检验出图像中存在什么物体, 还需要确定物体在图片中的位置。传统目标检测的方法一般分为三个阶段:1) 在给定的图像上选择一些候选的区域;2) 对这些区域进行特征提取;3) 使用训练的分类器进行分类。传统目标检测存在的两个主要问题:1) 基于滑动窗口的区域选择策略时间复杂度高, 窗口冗余, 处理时间长;2) 手工设计的特征对于目标多样性的变化不具有很好的稳健性。</p>
                </div>
                <div class="p1">
                    <p id="48">近几年, 得益于基于深度学习的算法, 尤其是基于深度卷积神经网络<citation id="96" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation> (CNN) 的算法, 目标检测的准确率得到大幅提升。2014年, Girshick等<citation id="97" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>设计了R-CNN (Regions with CNN) 目标检测框架, 使得目标检测的效果取得巨大突破, 并开启了应用深度学习算法进行目标检测的研究热潮。最近几年, 学术界涌现出一系列基于R-CNN的检测算法, 如SPP-Net (Spatial Pyramid Pooling Net) <citation id="98" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、Fast-RCNN<citation id="99" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、Faster R-CNN<citation id="100" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、R-FCN (Region-based Fully Convolutional Networks) <citation id="101" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等, 检测精度得到大幅度提升。此外, 还出现了区别于R-CNN系列两步检测方法的端到端检测算法, 如SSD (Single Shot MultiBox Detector) <citation id="102" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和YOLO (You Only Look Once) <citation id="106" type="reference"><link href="24" rel="bibliography" /><link href="26" rel="bibliography" /><link href="28" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>等, 检测速度得到较大提升。早期的基于深度学习的目标检测算法R-CNN, 首先通过区域提名的方式识别出可能存在目标的区域, 将图片划分成若干可能包含目标的小图片送入CNN中, 由CNN确定是哪一类物体存在于这个小区域上, 从而完成检测。这种检测流程的一大缺陷是检测一幅图片要经过CNN处理数千次, 完成一幅图片的检测需要2 s。后来的Fast R-CNN和Faster R-CNN对此进行改进, 不再将整幅图片分成小块的图片一次次送入CNN提取特征, 而是将图片整体一次性送入CNN, 提取到特征图后, 再对特征图做进一步处理, 但这一系列算法的整体流程依旧是分为区域提取和目标分类两部分, 这样做虽然确保了精度, 但牺牲了速度, 于是以SSD和YOLO为主要代表的端到端的目标检测算法应运而生。YOLO算法被提出以来, 已经经过了3个版本的更替, 在精度和速度上均获得了巨大的提升, 在Titan X上检测速度可达到45 frame/s, 是目前检测速度最快的目标检测算法之一。然而, 目前基于CNN的目标检测模型的训练与测试均依赖如Titan X这样的台式GPU计算平台, 计算资源消耗大, 难以向嵌入式平台移植, 因此难以适应工业界对于目标检测实时性和便携性的需求<citation id="107" type="reference"><link href="30" rel="bibliography" /><link href="32" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>。只有解决CNN模型的效率问题, 设计轻量化网络模型, 才能让CNN走出实验室, 实现更广泛的应用。针对这一现状, Iandola等<citation id="103" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出SqueezeNet模型, 基于fire module思想, 采用1×1卷积对特征图的维数进行压缩, 从而达到减少权值参数的目的;Howard等<citation id="104" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出MobileNet模型, 采用深度可分离的卷积方式代替传统卷积方式, 以达到减少网络权值参数的目的, 并在2018年初又提出改进版MobileNet-V2<citation id="105" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="49">以上几种方法本质上都是针对特征提取与分类网络结构的优化, 目前将它们应用于目标检测中时普遍采用的是与SSD算法结合的方式。考虑到YOLO系列算法本身速度要优于SSD算法, 因此本文提出一种基于YOLO-V3的轻量型目标检测网络结构。在该网络结构中, 对特征提取部分的网络层数进行适度压缩, 在检测部分通过引入SP (Spatial Pooling) 模块配合1×1卷积进行降维的方式, 一方面使得网络能够融合不同尺度的特征, 另一方面使得网络可以对不同宽高比的图片进行训练, 从而提高网络的检测能力。在对车辆目标进行检测训练时, 通过使用仿真图片辅助训练的方式弥补真实场景数据库样本数量的不足, 进一步提升检测精度, 并在嵌入式GPU平台上部署了基于所提方法训练得到的检测模型, 实现了对目标的实时检测, 为将基于深度学习的目标检测技术应用到智能驾驶、智能安防和智能无人机等产品中提供了一套目标检测原型系统。</p>
                </div>
                <h3 id="50" name="50" class="anchor-tag">2 改进的YOLO目标检测模型</h3>
                <h4 class="anchor-tag" id="51" name="51"><b>2.1 网络结构</b></h4>
                <div class="p1">
                    <p id="52">为了将神经网络技术应用于嵌入式平台实现实时的目标检测, 需要设计轻量级的目标检测模型, 为此, 提出一种基于YOLO的目标检测网络模型改进方案, 简化后的模型结构如图1所示, 其中<i>N</i>为卷积核数量 (通道数) , <i>K</i>为卷积核大小, <i>S</i>为步长, Conv表示卷积层, Fc表示全连接层。提出的网络结构具有以下特点:</p>
                </div>
                <div class="p1">
                    <p id="53">1) 对于特征提取部分, 在Darknet-53结构的基础上对网络层数进行了压缩, 以减少网络的参数, 简化后的特征提取部分包括1个卷积层, 4个降采样卷积层和9个resnet block。</p>
                </div>
                <div class="p1">
                    <p id="54">2) 在预测目标部分, 采用了与YOLO不同的结构。预测过程中使用了从不同层中提取的2个不同尺度的特征图以提高对于小目标的检测效果。通过引入SP模块, 将特征图的大小统一为13 pixel×13 pixel。再通过1×1卷积对通道数进行降维处理, 得到最后用于目标检测的特征向量。</p>
                </div>
                <h4 class="anchor-tag" id="55" name="55"><b>2.2 SP模块</b></h4>
                <div class="p1">
                    <p id="56">初代YOLO的模型结构中存在全连接层, 因而输入图片的尺寸必须固定。为解决这个问题, YOLO-V3取消了全连接层的使用, 转而使用了全局平均池化的方式, 而所提方法则选择保留全连接层并加入SP模块。SP模块主要基于SPP原理, 对于输入为任意大小<i>a</i>×<i>b</i>的特征图, SP模块根据图像的大小动态地计算池化窗口的大小<i>w</i>×<i>h</i>、水平步长<i>t</i><sub>w</sub>和竖直步长<i>t</i><sub>h</sub>, 得到尺寸固定为<i>n</i>×<i>n</i>的特征图, 通过这种方式, 将输入的不同大小的特征图进行尺度归一化, 这样网络就可以对不同尺寸的图片进行训练, 从而提高网络的检测能力。池化窗口大小和池化步长分别为</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>w</mi><mo>=</mo><mtext>c</mtext><mtext>e</mtext><mtext>i</mtext><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>g</mtext><mo stretchy="false"> (</mo><mi>a</mi><mo>/</mo><mi>n</mi><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mi>h</mi><mo>=</mo><mtext>c</mtext><mtext>e</mtext><mtext>i</mtext><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>g</mtext><mo stretchy="false"> (</mo><mi>b</mi><mo>/</mo><mi>n</mi><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd columnalign="left"><mi>t</mi><msub><mrow></mrow><mtext>w</mtext></msub><mo>=</mo><mtext>f</mtext><mtext>l</mtext><mtext>o</mtext><mtext>o</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi>a</mi><mo>/</mo><mi>n</mi><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mi>t</mi><msub><mrow></mrow><mtext>h</mtext></msub><mo>=</mo><mtext>f</mtext><mtext>l</mtext><mtext>o</mtext><mtext>o</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi>b</mi><mo>/</mo><mi>n</mi><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">式中:ceiling (·) 为向上取整函数;floor (·) 为向下取整函数。</p>
                </div>
                <div class="p1">
                    <p id="59">在对网络模型进行训练时, 每隔10次迭代后就会对训练数据进行缩放, 调整网络的输入尺寸, 训练过程中使用的最小图像尺寸为416 pixel×416 pixel, 最大图像尺寸为672 pixel×672 pixel。</p>
                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903032_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 网络结构示意图" src="Detail/GetImg?filename=images/GXXB201903032_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 网络结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903032_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Structural diagram of network</p>

                </div>
                <h4 class="anchor-tag" id="61" name="61"><b>2.3 训练策略</b></h4>
                <div class="p1">
                    <p id="62">下面结合图1简述参数的确定方式以及网络的训练策略。以输入尺寸为416 pixel×416 pixel的图片为例, 经过特征提取后, 得到的两个特征图的大小分别是52 pixel×52 pixel和26 pixel×26 pixel, 其特征通道数分别为256和512。通过SP模块后两个通道尺度分别变为13×13×256和13×13×512, 再通过1×1卷积进行降维后, 最终得到维度为13×13×18的特征向量作为全连接层的输入。其中13×13表示 (1) 式中<i>n</i>=13 pixel, 对应将输入图像分为对应的13×13个栅格, 这里采用了与YOLO-V3一致的策略, 为每个栅格预测三个可能的目标边界框, 对每个边界框预测4个坐标值 (<i>t</i><sub><i>x</i></sub>, <i>t</i><sub><i>y</i></sub>, <i>t</i><sub><i>w</i></sub>, <i>t</i><sub><i>h</i></sub>) , 1个置信度和<i>m</i>个条件概率 (<i>m</i>为待识别目标的类别数量) 。在后续实验中, 只检测车辆目标, 因而<i>m</i>=1, 所以每一个栅格对应的特征维数是3× (4+1+1) =18。由此得到总的特征向量的维度是13×13×18。</p>
                </div>
                <div class="p1">
                    <p id="63">在预测边界框的4个坐标值时, 采用平方和距离误差作为损失函数。在预测某一个边界框的置信度时, 如果这个预测的边界框与真实的边界框的重合部分大于一定阈值 (实验中设定为0.5) 且比其他所有预测的要好, 那么这个置信度值就为1, 否则这个预测的边界框将会被忽略。使用经典的softmax函数对每个框的内容进行目标分类预测。</p>
                </div>
                <h3 id="64" name="64" class="anchor-tag">3 实 验</h3>
                <h4 class="anchor-tag" id="65" name="65"><b>3.1 实验数据集</b></h4>
                <div class="p1">
                    <p id="66">为测试所提网络结构的性能, 对车辆目标进行目标检测实验, 实验中使用的两个数据集分别是KITTI<citation id="108" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>和DIM<citation id="109" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。KITTI数据集由德国卡尔斯鲁厄理工学院和丰田美国技术研究院联合创办, 是目前国际上最大的自动驾驶场景下的计算机视觉算法评测数据集。KITTI包含市区、乡村和高速公路等场景采集的真实图像数据, 其中KITTI_detection子数据集用于二维图像中的目标检测, 目标类别细分为car, van, truck, pedestrian, pedestrian-sitting, cyclist, tram以及misc, 共8类。实验中, 使用全部7481张KITTI_detection数据集图片, 将其按照8\:2的比例分为训练集和测试集, 并将tram, truck, van和car这几类目标合并为汽车类作为待检测目标。由于KITTI数据集样本量较少, 为能进一步提高检测准确率, 实验中还使用了DIM数据集作为补充。DIM数据集是采集自游戏截图的仿真图片, 共有10<sup>5</sup>张样本图片, 其中只标注了汽车类, 在本实验中, 共使用其中的10<sup>4</sup>张图片。图2展示了两个数据集的示例图片。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67"><b>3.2 评价标准</b></h4>
                <div class="p1">
                    <p id="68">实验中使用平均准确率 (<i>P</i><sub>mAP</sub>) 和召回率 (<i>R</i>) 作为目标检测结果的评价指标。召回率表示被正确识别出来的目标个数 (<i>N</i><sub>True</sub>) 与测试集中所有目标的个数 (<i>N</i><sub>Total</sub>) 的比值:</p>
                </div>
                <div class="area_img" id="69">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903032_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 数据集的示例图片。" src="Detail/GetImg?filename=images/GXXB201903032_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 数据集的示例图片。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903032_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Sample images from datasets. </p>
                                <p class="img_note"> (a) KITTI; (b) DIM</p>
                                <p class="img_note"> (a) KITTI; (b) DIM</p>

                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>r</mtext><mtext>u</mtext><mtext>e</mtext></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>o</mtext><mtext>t</mtext><mtext>a</mtext><mtext>l</mtext></mrow></msub></mrow></mfrac><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">数据集中某个类别C在一张图片上的检测准确率<i>P</i><sub>C</sub>等于在该图片上正确识别出的类别C的个数<i>N</i><sub>True-C</sub>与该图片上识别出的类别C的总数 (包括正确识别<i>N</i><sub>True-C</sub>和误识别<i>N</i><sub>False-C</sub>) 的比值:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>r</mtext><mtext>u</mtext><mtext>e</mtext><mo>-</mo><mtext>C</mtext></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>r</mtext><mtext>u</mtext><mtext>e</mtext><mo>-</mo><mtext>C</mtext></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>a</mtext><mtext>l</mtext><mtext>s</mtext><mtext>e</mtext><mo>-</mo><mtext>C</mtext></mrow></msub></mrow></mfrac><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">判断某个目标是否正确识别的标准交并比 (<i>R</i><sub>IoU</sub>) 定义为预测产生的候选框A与真实目标的标记框B的交集与并集的比值:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>o</mtext><mtext>U</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>A</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mo>_</mo><mn>1</mn></mrow></msub></mrow><mrow><mi>A</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mo>_</mo><mn>2</mn></mrow></msub></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">式中:<i>A</i><sub>area_1</sub>表示A与B的相交面积;<i>A</i><sub>area_2</sub>表示A与B的集合面积。该比值越高说明预测的定位准确性越高, 本实验中设定阈值为0.5, <i>R</i><sub>IoU</sub>大于该阈值的预测是一个正确的预测。类别C在所有包含该类别的图片中的单类平均准确率<i>P</i><sub>AP-C</sub>等于每张图片上的<i>P</i><sub>C</sub>求和后除以含有类别C的图片数目<i>N</i><sub>Total-C</sub>:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext><mo>-</mo><mtext>C</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><mo>∑</mo><mi>Ρ</mi></mstyle><msub><mrow></mrow><mtext>C</mtext></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>o</mtext><mtext>t</mtext><mtext>a</mtext><mtext>l</mtext><mo>-</mo><mtext>C</mtext></mrow></msub></mrow></mfrac><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">对于包含<i>m</i>个类别的整个数据集, 其平均准确率<i>P</i><sub>mAP</sub>等于各个单类平均准确率求和后除以类别数<i>m</i>:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>A</mtext><mtext>Ρ</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><mo>∑</mo><mi>Ρ</mi></mstyle><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext><mo>-</mo><mtext>C</mtext></mrow></msub></mrow><mi>m</mi></mfrac><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">由于本实验中类别数<i>m</i>=1, 因此实际中<i>P</i><sub>mAP</sub>=<i>P</i><sub>AP</sub>。</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80"><b>3.3 网络模型的训练与测试实验</b></h4>
                <div class="p1">
                    <p id="81">下面通过实验对所提网络模型进行目标检测的训练与测试, 并将实验结果与另外两个网络模型 (YOLO-V3 和YOLO-tiny<citation id="110" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>) 进行对比。YOLO-V3是目前最优秀的目标检测模型之一, YOLO-tiny是YOLO的作者自己提出的简化版模型。本节中所述实验的训练与测试过程均在配有NVIDIA GTX1080显卡的工作站上进行, 实验中使用第3.1节提到的KITTI和DIM数据集对汽车目标进行检测。实验共分三组。第一组实验中, 分别使用YOLO-V3、YOLO-tiny以及所提网络结构, 在KITTI数据集上进行模型训练和测试, 得到的实验结果见表1。考虑到实际应用中, 像KITTI这样的真实场景数据集的采集和标记都会耗费大量的人力物力, 而仿真数据集较容易获取, 可作为真实数据集不足时的一种数据补充, 因此在第二组实验中, 使用10<sup>4</sup>张DIM数据集图片对模型进行训练, 并在KITTI测试数据集上进行测试, 得到的实验结果见表2。第三组实验中, 使用在DIM数据集上预训练好的模型, 在KITTI训练集上进行微调训练, 并在KITTI测试数据集上进行测试, 实验结果见表3。图3展示了第三组实验中, 使用完整的YOLO模型和所提模型得到的部分测试结果的对比图。</p>
                </div>
                <div class="area_img" id="82">
                    <p class="img_tit">表1 第一组实验结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Results of first group of experiments</p>
                    <p class="img_note"></p>
                    <table id="82" border="1"><tr><td><br />Model</td><td><i>P</i><sub>mAP</sub> /%</td><td><i>R</i> /%</td></tr><tr><td><br />YOLO-V3</td><td>86.20</td><td>85.49</td></tr><tr><td><br />YOLO-tiny</td><td>65.89</td><td>75.56</td></tr><tr><td><br />Proposed method</td><td>64.22</td><td>78.22</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="83">
                    <p class="img_tit">表2 第二组实验结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Results of second group of experiments</p>
                    <p class="img_note"></p>
                    <table id="83" border="1"><tr><td><br />Model</td><td><i>P</i><sub>mAP</sub> /%</td><td><i>R</i> /%</td></tr><tr><td><br />YOLO-V3</td><td>72.48</td><td>78.96</td></tr><tr><td><br />YOLO-tiny</td><td>56.32</td><td>67.77</td></tr><tr><td><br />Proposed method</td><td>52.87</td><td>69.13</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="84">
                    <p class="img_tit">表3 第三组实验结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Results of third group of experiments</p>
                    <p class="img_note"></p>
                    <table id="84" border="1"><tr><td><br />Model</td><td><i>P</i><sub>mAP</sub> /%</td><td><i>R</i> /%</td></tr><tr><td><br />YOLO-V3</td><td>89.29</td><td>81.76</td></tr><tr><td><br />YOLO-tiny</td><td>72.40</td><td>69.13</td></tr><tr><td><br />Proposed method</td><td>71.35</td><td>73.90</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="85">实验结果表明, 所提模型在检测准确率上与YOLO-tiny持平且召回率高于YOLO-tiny, 但与YOLO-V3相比, 检测准确率和召回率均有一定差距。这是由于网络结构简化以及网络参数的减少造成了精度损失。从图3的对比中也可以看出, 相比YOLO-V3网络模型, 所提网络模型对于小目标和有遮挡、有残缺的目标存在漏检现象。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903032_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 第三组实验的部分检测结果对比。" src="Detail/GetImg?filename=images/GXXB201903032_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 第三组实验的部分检测结果对比。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903032_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Comparison of partial detection results in third group of experiments. </p>
                                <p class="img_note"> (a) ～ (e) 使用YOLO-V3的结果; (f) ～ (j) 使用所提网络结构的结果</p>
                                <p class="img_note"> (a) - (e) Results by YOLO-V3; (f) - (j) results by proposed network structure</p>

                </div>
                <div class="p1">
                    <p id="87">对比三组实验结果, 还可以看出:第二组使用仿真数据训练的测试结果在三组实验中精度最低, 这是因为仿真数据与真实场景数据的特征分布存在一定差异, 直接跨数据集进行测试, 难以达到良好的检测效果;但第三组实验中, 使用对仿真数据进行预训练、对真实数据进行再训练的方法得到的检测效果是三组实验中效果最优的, 平均准确率在YOLO-V3上达到89.29%, 在所提网络模型上达到71.35%, 这说明即使是存在一定的特征差异, 使用仿真的数据集作为真实场景数据的补充, 配合适当的训练策略, 可以起到提升网络模型性能的作用, 从而弥补因网络结构简化带来的检测精度的降低。这一实验结果为诸如智能驾驶和智能无人机等真实训练数据集获取成本高、难度大的应用领域提供了一种解决思路。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88"><b>3.4 网络模型向嵌入式平台的移植实验</b></h4>
                <div class="p1">
                    <p id="89">目前基于深度学习的目标检测算法多数基于台式GPU计算机进行训练和测试, 这类计算平台的运算能力强, 可以胜任大规模网络模型的训练, 但由于体积相对庞大, 即使在这类平台上达到了实时的目标检测, 也难以实现工业应用。将该类算法向嵌入式平台进行移植, 是实现智能车载、机载目标识别的关键。在嵌入式平台的选择上, 使用NVIDIA Jetson TX1作为嵌入式GPU计算平台来搭建实时目标检测原型系统。Jeston TX1的CPU采用的是ARM Cortex-A57, GPU采用NVIDIA Maxwell架构, 内含256个CUDA (Compute Unified Device Architecture) 核心, 非常适合嵌入式人工智能计算。</p>
                </div>
                <div class="p1">
                    <p id="90">在本节实验中, 针对模型的检测速度进行实验, 将网络模型部署在NVIDIA Jetson TX1嵌入式计算平台上, 使用分辨率为640 pixel×480 pixel、包含200 frame图像的视频对所提网络模型进行了检测速度的测试, 并与其他轻量型网络模型的检测速度进行了对比, 部分实验过程如图4所示。MobileNet-V1-SSD和MobileNet-V2-SSD部署在Tensorflow框架上, 使用Tensorflow object detection API提供的模型;YOLO-V3, YOLO-tiny和所提模型部署在Darknet框架上, 使用第3.3节中训练得到的模型。实验结果见表4。</p>
                </div>
                <div class="area_img" id="91">
                    <p class="img_tit">表4 目标检测的速度对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Speed comparison of target detection</p>
                    <p class="img_note"></p>
                    <table id="91" border="1"><tr><td><br />Model</td><td>Average speed on TX1 /<br /> (frame·s<sup>-1</sup>) </td></tr><tr><td><br />MobileNet-V1-SSD</td><td>14</td></tr><tr><td><br />MobileNet-V2-SSD</td><td>13</td></tr><tr><td><br />YOLO-V3</td><td>2</td></tr><tr><td><br />YOLO-tiny</td><td>17</td></tr><tr><td><br />Proposed method</td><td>23</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201903032_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 在嵌入式平台上的检测结果。" src="Detail/GetImg?filename=images/GXXB201903032_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 在嵌入式平台上的检测结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201903032_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Detection results on embedded platform. </p>
                                <p class="img_note"> (a) YOLO-V3; (b) YOLO-tiny; (c) 所提网络结构; (d) MobileNet-V1-SSD; (e) MobileNet-V2-SSD</p>
                                <p class="img_note"> (a) YOLO-V3; (b) YOLO-tiny; (c) proposed network structure; (d) MobileNet-V1-SSD; (e) MobileNet-V2-SSD</p>

                </div>
                <div class="p1">
                    <p id="93">实验结果表明, 受到嵌入式平台计算能力的限制, 即使是在台式GPU显卡上已经可以实现实时目标检测的方法, 如YOLO-V3, 在嵌入式平台上的运行速度依然很慢, 检测帧率仅为2 frame/s。在对比实验中, 选取了针对SSD检测模型进行速度优化的方法MobileNet-V1-SSD和MobileNet-V2-SSD进行测试, 平均检测速度分别为14 frame/s和13 frame/s。YOLO-tiny是基于YOLO-V3的优化方案, 相比YOLO-V3在检测速度上有了大幅度提升, 达到17 frame/s。由于一般摄像机 (PAL制式) 的帧率为25 frame/s, 因此以上的优化方案均没有达到实时检测的标准。所提模型结构在检测准确率和召回率上与YOLO-tiny持平的情况下, 检测速度提升35%, 达到23 frame/s, 可基本实现在嵌入式平台上的实时目标检测。</p>
                </div>
                <h3 id="94" name="94" class="anchor-tag">4 结 论</h3>
                <div class="p1">
                    <p id="95">基于YOLO-V3提出了一种适用于嵌入式GPU平台的轻量化目标检测模型, 在两个公开车辆检测数据集上对网络模型进行了训练, 并将模型移植到嵌入式GPU计算平台上。实验证明, 所提方法初步实现了嵌入式平台上的实时目标检测, 为后期将基于深度学习的目标检测技术应用到智能驾驶、智能安防和智能无人机等产品中提供了一套目标检测原型系统。通过与复杂网络模型的对比实验可以看出, 简化的网络结构相较于复杂的网络结构, 其准确率和召回率均有所降低, 说明目标检测网络运行速度的提升是以牺牲一定的检测准确率为代价的, 如何进一步优化网络结构, 在保证检测速度的情况下进一步提升检测准确率, 将是下一步研究的重点。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                 LeCun Y, Bottou L, Bengio Y, <i>et al</i>. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86 (11) : 2278-2324.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[2]</b> Girshick R, Donahue J, Darrell T, <i>et al</i>. Rich feature hierarchies for accurate object detection and semantic segmentation [C]. IEEE Conference on Computer Vision and Pattern Recognition, 2014: 580-587.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">

                                <b>[3]</b> He K M, Zhang X Y, Ren S Q, <i>et al</i>. Spatial pyramid pooling in deep convolutional networks for visual recognition[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) : 1904-1916.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[4]</b> Girshick R. Fast R-CNN[C]. IEEE International Conference on Computer Vision, 2015: 1440-1448.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" >
                                    <b>[5]</b>
                                 Ren S Q, He K M, Girshick R, <i>et al</i>. Faster R-CNN: Towards real-time object detection with region proposal networks[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) : 1137-1149.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=R-FCN:Object Detection via Region-based Fully Convolutional Networks">

                                <b>[6]</b> Li Y, He K, Sun J. R-FCN: Object detection via region-based fully convolutional networks[C]. Advances in Neural Information Processing Systems, 2016: 379-387.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">

                                <b>[7]</b> Liu W, Anguelov D, Erhan D, <i>et al</i>. SSD: Single shot multibox detector[C]. European Conference on Computer Vision, 2016: 21-37.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better Faster,Stronger">

                                <b>[8]</b> Redmon J, Farhadi A. YOLO9000: Better, faster, stronger[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2017: 6517-6525.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLOv3 an incremental improvement">

                                <b>[9]</b> Redmon J, Farhadi A. Yolov3: An incremental improvement[EB/OL]. (2018-04-08) [2018-09-07]. https://arxiv.org/abs/1804.02767.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 Redmon J, Divvala S, Girshick R, <i>et al</i>. You only look once: Unified, real-time object detection[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2016: 779-788.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806034&amp;v=MjkyOTQ5R1lJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SG5WcnpCSWpYVGJMRzRIOW5NcVk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Feng X Y, Mei W, Hu D S. Aerial target detection based on improved faster R-CNN[J]. Acta Optica Sinica, 2018, 38 (6) : 0615004. 冯小雨, 梅卫, 胡大帅. 基于改进Faster R-CNN的空中目标检测[J]. 光学学报, 2018, 38 (6) : 0615004.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201803036&amp;v=MDQwODRiTEc0SDluTXJJOUdZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeUhuVnJ6QklqWFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Xin P, Xu Y L, Tang H, <i>et al</i>. Fast airplane detection based on multi-layer feature fusion of fully convolutional networks[J]. Acta Optica Sinica, 2018, 38 (3) : 0315003.  辛鹏, 许悦雷, 唐红, 等. 全卷积网络多层特征融合的飞机快速检测[J]. 光学学报, 2018, 38 (3) : 0315003.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Squeeze Net:Alex Net-level accuracy with 50x fewer parameters and&amp;lt;0.5MB model size">

                                <b>[13]</b> Iandola F N, Han S, Moskewicz M W, <i>et al</i>. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;0.5 MB model size[EB/OL]. (2016-11-04) [2018-09-07]. https://arxiv.org/abs/1602.07360.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mobilenets:efficient convolutional neural networks for mobile vision applications">

                                <b>[14]</b> Howard A G, Zhu M, Chen B, <i>et al</i>. Mobilenets: Efficient convolutional neural networks for mobile vision applications[EB/OL]. (2017-04-17) [2018-09-07]. https://arxiv.org/abs/ 1704.04861.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inverted residuals and linear bottlenecks:Mobile networks for classification,detection and segmentation">

                                <b>[15]</b> Sandler M, Howard A, Zhu M, <i>et al</i>. Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation[EB/OL]. (2018-04-02) [2018-09-07]. https://arxiv.org/abs/1801.04381.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The KITTI vision benchmark suite">

                                <b>[16]</b> Geiger A, Lenz P, Urtasun R. Are we ready for autonomous driving? The KITTI vision benchmark suite[C]. IEEE Conference on Computer Vision and Pattern Recognition, 2012: 3354-3361.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?">

                                <b>[17]</b> Johnson-Roberson M, Barto C, Mehta R, <i>et al</i>. Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?[C]. IEEE International Conference on Robotics and Automation, 2017: 746-753.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLO-tiny">

                                <b>[18]</b> Redmon J. YOLO-tiny[EB/OL]. (2018-08-16) [2018-09-07]. https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-tiny.cfg.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201903032" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201903032&amp;v=MDU4ODJYVGJMRzRIOWpNckk5R1pvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5SG5WcnpPSWo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNDOE1qZHpnTnRFYXZMeWZFaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

