

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133859468565000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dGXXB201907007%26RESULT%3d1%26SIGN%3dXCYB%252ftePcheI%252bTIGyaCV%252bJXDUKI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201907007&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GXXB201907007&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201907007&amp;v=MTcxNjk5ak1xSTlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnluZ1Y3ck1JalhUYkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#50" data-title="1 引  言 ">1 引  言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#54" data-title="2 算法描述 ">2 算法描述</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="&lt;b&gt;2.1 基于特征的人脸图像配准基本框架&lt;/b&gt;"><b>2.1 基于特征的人脸图像配准基本框架</b></a></li>
                                                <li><a href="#60" data-title="&lt;b&gt;2.2 基于局部特征描述子的初始配准&lt;/b&gt;"><b>2.2 基于局部特征描述子的初始配准</b></a></li>
                                                <li><a href="#66" data-title="&lt;b&gt;2.3 基于空间位置约束的精确配准&lt;/b&gt;"><b>2.3 基于空间位置约束的精确配准</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#112" data-title="3 实验结果与讨论 ">3 实验结果与讨论</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#114" data-title="&lt;b&gt;3.1 仿真数据集&lt;/b&gt;"><b>3.1 仿真数据集</b></a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;3.2 真实人脸图像配准&lt;/b&gt;"><b>3.2 真实人脸图像配准</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#129" data-title="4 结  论 ">4 结  论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="图1 基于特征的可见光和红外人脸图像配准流程">图1 基于特征的可见光和红外人脸图像配准流程</a></li>
                                                <li><a href="#68" data-title="图2 IDSC人脸轮廓特征提取示意图。">图2 IDSC人脸轮廓特征提取示意图。</a></li>
                                                <li><a href="#69" data-title="图3 不同自由度下的Student′s-T分布曲线">图3 不同自由度下的Student′s-T分布曲线</a></li>
                                                <li><a href="#111" data-title="图4 本文配准算法流程图">图4 本文配准算法流程图</a></li>
                                                <li><a href="#116" data-title="图5 汉字福仿真点集配准结果。">图5 汉字福仿真点集配准结果。</a></li>
                                                <li><a href="#122" data-title="图6 UTK-IRIS数据集下可见光和红外人脸图像配准结果。">图6 UTK-IRIS数据集下可见光和红外人脸图像配准结果。</a></li>
                                                <li><a href="#124" data-title="表1 各配准算法的平均匹配误差和运行时间比较">表1 各配准算法的平均匹配误差和运行时间比较</a></li>
                                                <li><a href="#125" data-title="图7 不同个体的多光谱人脸图像数据集的定量比较结果。">图7 不同个体的多光谱人脸图像数据集的定量比较结果。</a></li>
                                                <li><a href="#131" data-title="图8 自行采集的可见光和红外人脸图像配准和融合结果。">图8 自行采集的可见光和红外人脸图像配准和融合结果。</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="12">


                                    <a id="bibliography_1" title=" Ding W S, Bi D Y, He L Y, &lt;i&gt;et al&lt;/i&gt;.Fusion of infrared and visible images based on shearlet transform and neighborhood structure features[J].Acta Optica Sinica, 2017, 37 (10) :1010002.丁文杉, 毕笃彦, 何林远, 等.基于剪切波变换和邻域结构特征的红外与可见光图像融合[J].光学学报, 2017, 37 (10) :1010002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201710016&amp;v=MDgwMTA1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnluZ1Y3ck1JalhUYkxHNEg5Yk5yNDlFWW9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Ding W S, Bi D Y, He L Y, &lt;i&gt;et al&lt;/i&gt;.Fusion of infrared and visible images based on shearlet transform and neighborhood structure features[J].Acta Optica Sinica, 2017, 37 (10) :1010002.丁文杉, 毕笃彦, 何林远, 等.基于剪切波变换和邻域结构特征的红外与可见光图像融合[J].光学学报, 2017, 37 (10) :1010002.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_2" title=" Liu X H, Chen Z B.Fusion of infrared and visible images based on multi-scale directional guided filter and convolutional sparse representation[J].Acta Optica Sinica, 2017, 37 (11) :1110004.刘先红, 陈志斌.基于多尺度方向引导滤波和卷积稀疏表示的红外与可见光图像融合[J].光学学报, 2017, 37 (11) :1110004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201711013&amp;v=MzA1NDk5Yk5ybzlFWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnluZ1Y3ck1JalhUYkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Liu X H, Chen Z B.Fusion of infrared and visible images based on multi-scale directional guided filter and convolutional sparse representation[J].Acta Optica Sinica, 2017, 37 (11) :1110004.刘先红, 陈志斌.基于多尺度方向引导滤波和卷积稀疏表示的红外与可见光图像融合[J].光学学报, 2017, 37 (11) :1110004.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_3" title=" Pal A R, Singha A.A comparative analysis of visual and thermal face image fusion based on different wavelet family[C]//2017 International Conference on Innovations in Electronics, Signal Processing and Communication (IESC) , April 6-7, 2017, Shillong, India.New York:IEEE, 2017:8071895." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A comparative analysis of visual and thermal face image fusion based on different wavelet family">
                                        <b>[3]</b>
                                         Pal A R, Singha A.A comparative analysis of visual and thermal face image fusion based on different wavelet family[C]//2017 International Conference on Innovations in Electronics, Signal Processing and Communication (IESC) , April 6-7, 2017, Shillong, India.New York:IEEE, 2017:8071895.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_4" title=" Ma J Y, Ma Y, Li C.Infrared and visible image fusion methods and applications:a survey[J].Information Fusion, 2019, 45:153-178." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF1AA21F05383F7D583B754F0E37B5D1D&amp;v=MjE2OTZwYlEzNWRoaHhiMjd4YXc9TmlmT2ZjVzViNkRPcnZsRlllZ0hEd28rdXhNYjZVMTZUWHVVckdjMmZzQ1JNYnZyQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Ma J Y, Ma Y, Li C.Infrared and visible image fusion methods and applications:a survey[J].Information Fusion, 2019, 45:153-178.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_5" title=" Liu X C, Zhong T, Yu Q F, &lt;i&gt;et al&lt;/i&gt;.Multi-modal image registration based on local frequency information using modified simplex-simulated annealing algorithm[J].Acta Optica Sinica, 2013, 33 (6) :0615002.刘晓春, 钟涛, 于起峰, 等.基于局部频率信息和单纯型-模拟退火的异源图像配准[J].光学学报, 2013, 33 (6) :0615002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201306031&amp;v=MTg2MzlHRnJDVVJMT2VaZVZ1RnluZ1Y3ck1JalhUYkxHNEg5TE1xWTlHWllRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Liu X C, Zhong T, Yu Q F, &lt;i&gt;et al&lt;/i&gt;.Multi-modal image registration based on local frequency information using modified simplex-simulated annealing algorithm[J].Acta Optica Sinica, 2013, 33 (6) :0615002.刘晓春, 钟涛, 于起峰, 等.基于局部频率信息和单纯型-模拟退火的异源图像配准[J].光学学报, 2013, 33 (6) :0615002.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_6" title=" Lin Q, Jin W Q, Guo H, &lt;i&gt;et al&lt;/i&gt;.Confocal-window telescope objective design in visible and long-wave infrared[J].Acta Optica Sinica, 2012, 32 (9) :0922005.林青, 金伟其, 郭宏, 等.可见光/长波红外共聚焦窗口望远物镜设计[J].光学学报, 2012, 32 (9) :0922005." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201209038&amp;v=MjA3MDJYVGJMRzRIOVBNcG85R2JJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmdWN3JNSWo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Lin Q, Jin W Q, Guo H, &lt;i&gt;et al&lt;/i&gt;.Confocal-window telescope objective design in visible and long-wave infrared[J].Acta Optica Sinica, 2012, 32 (9) :0922005.林青, 金伟其, 郭宏, 等.可见光/长波红外共聚焦窗口望远物镜设计[J].光学学报, 2012, 32 (9) :0922005.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_7" title=" Kong S G, Heo J, Boughorbel F, &lt;i&gt;et al&lt;/i&gt;.Multiscale fusion of visible and thermal IR images for illumination-invariant face recognition[J].International Journal of Computer Vision, 2007, 71 (2) :215-233." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002831084&amp;v=MTIzOTF4Y01IN1I3cWVidWR0RlNqbFVyck1KVm89Tmo3QmFyTzRIdEhPcDR4RVpPTUxZM2s1ekJkaDRqOTlTWHFScnhv&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Kong S G, Heo J, Boughorbel F, &lt;i&gt;et al&lt;/i&gt;.Multiscale fusion of visible and thermal IR images for illumination-invariant face recognition[J].International Journal of Computer Vision, 2007, 71 (2) :215-233.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_8" title=" Boughorbel F, Mercimek M, Koschan A, &lt;i&gt;et al&lt;/i&gt;.A new method for the registration of three-dimensional point-sets:the Gaussian fields framework[J].Image and Vision Computing, 2010, 28 (1) :124-137." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201348988&amp;v=MzIwOTRZPU5pZk9mYks3SHRET3JZOUVaKzhIQlhReG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUlWc1FieA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Boughorbel F, Mercimek M, Koschan A, &lt;i&gt;et al&lt;/i&gt;.A new method for the registration of three-dimensional point-sets:the Gaussian fields framework[J].Image and Vision Computing, 2010, 28 (1) :124-137.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_9" title=" Myronenko A, Song X B.Point set registration:coherent point drift[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (12) :2262-2275." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Point Set Registration: Coherent Point Drift">
                                        <b>[9]</b>
                                         Myronenko A, Song X B.Point set registration:coherent point drift[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (12) :2262-2275.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_10" title=" Ma J Y, Zhao J, Ma Y, &lt;i&gt;et al&lt;/i&gt;.Non-rigid visible and infrared face registration via regularized Gaussian fields criterion[J].Pattern Recognition, 2015, 48 (3) :772-784." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700101420&amp;v=MDcwOTQ0NW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUlWc1FieFk9TmlmT2ZiSzhIOURNcUk5Rlplc09DSA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Ma J Y, Zhao J, Ma Y, &lt;i&gt;et al&lt;/i&gt;.Non-rigid visible and infrared face registration via regularized Gaussian fields criterion[J].Pattern Recognition, 2015, 48 (3) :772-784.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_11" title=" Tian T, Mei X G, Yu Y, &lt;i&gt;et al&lt;/i&gt;.Automatic visible and infrared face registration based on silhouette matching and robust transformation estimation[J].Infrared Physics &amp;amp; Technology, 2015, 69:145-154." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1D6971F6D9C6126779AA40880A072325&amp;v=MDY0NDBOaWZPZmJMTUdOakxydmxERU9KOENuMDd5UkVVNDA0TVRIL3FwQkpFZWJXV1JyaWFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh4YjI3eGF3PQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Tian T, Mei X G, Yu Y, &lt;i&gt;et al&lt;/i&gt;.Automatic visible and infrared face registration based on silhouette matching and robust transformation estimation[J].Infrared Physics &amp;amp; Technology, 2015, 69:145-154.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_12" title=" Gerogiannis D, Nikou C, Likas A.The mixtures of student′s &lt;i&gt;t&lt;/i&gt;-distributions as a robust framework for rigid registration[J].Image and Vision Computing, 2009, 27 (9) :1285-1294." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349053&amp;v=MDc4ODhOaWZPZmJLN0h0RE9yWTlFWis4R0RIazZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lJVnNRYnhZPQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Gerogiannis D, Nikou C, Likas A.The mixtures of student′s &lt;i&gt;t&lt;/i&gt;-distributions as a robust framework for rigid registration[J].Image and Vision Computing, 2009, 27 (9) :1285-1294.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_13" title=" Zhou Z Y, Zheng J, Dai Y K, &lt;i&gt;et al&lt;/i&gt;.Robust non-rigid point set registration using student′s-t mixture model[J].PLoS One, 2014, 9 (3) :e91381." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust non-rigid point set registration using student&amp;#39;&amp;#39;st mixture model">
                                        <b>[13]</b>
                                         Zhou Z Y, Zheng J, Dai Y K, &lt;i&gt;et al&lt;/i&gt;.Robust non-rigid point set registration using student′s-t mixture model[J].PLoS One, 2014, 9 (3) :e91381.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_14" title=" Maiseli B, Gu Y F, Gao H J.Recent developments and trends in point set registration methods[J].Journal of Visual Communication and Image Representation, 2017, 46:95-106." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESD21C7D537C972ED5652B143CACD47650&amp;v=MTU3NTZ1eE1WN3owUFNYdmgzMk5HRGJhVFE3K2ZDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh4YjI3eGF3PU5pZk9mY2U2SDZMTDI0cEdZNWdHQzM1TQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Maiseli B, Gu Y F, Gao H J.Recent developments and trends in point set registration methods[J].Journal of Visual Communication and Image Representation, 2017, 46:95-106.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_15" title=" Belongie S, Malik J, Puzicha J.Shape matching and object recognition using shape contexts[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002, 24 (4) :509-522." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape matching and object recognition using shape contexts">
                                        <b>[15]</b>
                                         Belongie S, Malik J, Puzicha J.Shape matching and object recognition using shape contexts[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002, 24 (4) :509-522.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_16" title=" Ling H B, Jacobs D W.Shape classification using the inner-distance[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (2) :286-299." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape Classification Using the Inner-Distance">
                                        <b>[16]</b>
                                         Ling H B, Jacobs D W.Shape classification using the inner-distance[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (2) :286-299.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_17" title=" Peel D, McLachlan G J.Robust mixture modelling using the &lt;i&gt;t&lt;/i&gt; distribution[J].Statistics and Computing, 2000, 10 (4) :339-348." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002792185&amp;v=MDU0MjBSN3FlYnVkdEZTamxVcnJNSlZvPU5qN0Jhck80SHRIT3FJWkhaZU1LWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         Peel D, McLachlan G J.Robust mixture modelling using the &lt;i&gt;t&lt;/i&gt; distribution[J].Statistics and Computing, 2000, 10 (4) :339-348.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_18" title=" Chui H L, Rangarajan A.A new point matching algorithm for non-rigid registration[J].Computer Vision and Image Understanding, 2003, 89 (2/3) :114-141." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501084334&amp;v=MjcxMzVPZmJLN0h0RE5xbzlFWk9NTEQzODlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lJVnNRYnhZPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Chui H L, Rangarajan A.A new point matching algorithm for non-rigid registration[J].Computer Vision and Image Understanding, 2003, 89 (2/3) :114-141.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_19" title=" Zhan L C, Zhuang Y, Huang L D.Infrared and visible images fusion method based on discrete wavelet transform[J].Journal of Computers, 2017, 28 (2) :57-71." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Infrared and visible images fusion method based on discrete wavelet transform">
                                        <b>[19]</b>
                                         Zhan L C, Zhuang Y, Huang L D.Infrared and visible images fusion method based on discrete wavelet transform[J].Journal of Computers, 2017, 28 (2) :57-71.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-16 17:32</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GXXB" target="_blank">光学学报</a>
                2019,39(07),64-74 DOI:10.3788/AOS201939.0710001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于T分布混合模型的多光谱人脸图像配准</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%B7%8D&amp;code=25246845&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李巍</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%91%A3%E6%98%8E%E5%88%A9&amp;code=11093092&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">董明利</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%95%E4%B9%83%E5%85%89&amp;code=21995738&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吕乃光</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A8%84%E5%B0%8F%E5%B9%B3&amp;code=21800361&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">娄小平</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%85%89%E5%AD%90%E5%AD%A6%E4%B8%8E%E5%85%89%E9%80%9A%E4%BF%A1%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=0041796&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京邮电大学信息光子学与光通信研究院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E4%BF%A1%E6%81%AF%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E5%85%89%E7%94%B5%E6%B5%8B%E8%AF%95%E6%8A%80%E6%9C%AF%E5%8F%8A%E4%BB%AA%E5%99%A8%E6%95%99%E8%82%B2%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0251832&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京信息科技大学光电测试技术及仪器教育部重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了降低多光谱人脸图像中出现的非刚性形变、噪声和离群点等因素对配准结果的准确性和稳健性的影响, 提出一种综合考虑特征点的空间几何结构和局部形状特征两方面信息的多光谱人脸图像配准方法。所提方法首先通过基于内部距离的形状上下文描述子来表述点集的局部特征信息, 建立可见光和红外图像相似性测度函数。然后利用Student′s-T分布混合模型来表示图像特征点集配准过程中变换模型估计问题, 并采用期望最大化算法对模型进行求解。仿真数据表明在点集存在非刚性形变、噪声和离群点的情况下, 所提方法仍可以实现点集间的精确配准。可见光和红外人脸真实图像数据表明所提方法的平均匹配误差和运算效率都优于对比算法, 配准融合后的多光谱人脸图像可以提高后续的人脸检测和识别性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%85%89%E8%B0%B1%E4%BA%BA%E8%84%B8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多光谱人脸;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E9%85%8D%E5%87%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像配准;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%86%85%E8%B7%9D%E7%A6%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">内距离;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Student%E2%80%B2s-T%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Student′s-T混合模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">期望最大化;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *董明利, E-mail:dongml@bistu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-07</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (51475046, 51475047);</span>
                    </p>
            </div>
                    <h1><b>Multispectral Face Image Registration Based on T-Distribution Mixture Model</b></h1>
                    <h2>
                    <span>Li Wei</span>
                    <span>Dong Mingli</span>
                    <span>Lü Naiguang</span>
                    <span>Lou Xiaoping</span>
            </h2>
                    <h2>
                    <span>Institute of Information Photonics and Optical Communications, Beijing University of Posts & Telecommunications</span>
                    <span>Key Laboratory of the Ministry of Education for Optoelectronic Measurement Technology and Instruments, Beijing Information Science & Technology University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to enhance the accuracy and robustness of multispectral face registration results suffering from non-rigid deformation, noise, and outliers, a multispectral face registration method based on the spatial geometrical structure and local shape features of feature points is proposed. On the one hand, we use inner-distance shape context as the local shape feature of the point set, and create the similarity measure function between visible and infrared images. On the other hand, a Student′s-T mixture model is used to represent the transformation model estimation in non-rigid point set registration process, and the model can be solved by using the expectation maximization algorithm. The simulation results show that the proposed method can realize exactly registration of point sets with deformation, noise, and outliers. The visible and infrared real image databases demonstrate that the matching error and computing efficiency of the proposed method outperform those of the comparison methods. As a result, the multispectral face images after registration and fusion will improve the performances of follow-up face detection and recognition.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=imaging%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">imaging processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multispectral%20face&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multispectral face;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20registration&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image registration;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=inner-distance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">inner-distance;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Student%E2%80%B2s-T%20mixture%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Student′s-T mixture model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=expectation%20maximization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">expectation maximization;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-07</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="50" name="50" class="anchor-tag">1 引  言</h3>
                <div class="p1">
                    <p id="51">图像融合技术能够分析提取多光谱图像传感器的互补信息, 从而合成一幅能详尽表达复杂目标场景信息的准确图像。由于图像融合过程更切合人眼的视觉特性, 对于研究目标检测和识别方法具有重要的意义, 因此被广泛应用于遥感图像处理、医学影像分析、军事目标检测、视频安防监控等领域<citation id="134" type="reference"><link href="12" rel="bibliography" /><link href="14" rel="bibliography" /><link href="16" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。人脸识别作为一种有效的非接触式生物识别手段, 一直以来都是模式识别领域的研究热点。鉴于可见光与红外图像反映的人脸特征信息不同, 例如:可见光图像分辨率高, 可以提供丰富的人脸细节信息;而红外图像具有较高的热对比度, 不易受到外界环境光照、姿态及伪装等因素对人脸识别性能的影响, 因此将可见光与红外图像融合应用到人脸识别领域是目前人脸识别技术的发展趋势<citation id="132" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。然而, 快速准确地实现异源图像的配准是多传感器图像融合成功的前提, 也是决定图像融合技术发展的关键环节<citation id="133" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="52">目前, 异源图像如红外和可见光图像的配准方法可以分为两种。第一种方法是建立双波段共轴光路系统, 利用分束器让同一个窗口输入的双波段光线分别在不同传感器上成像<citation id="135" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>, 这种通过搭建同轴光路的硬件配准方法虽然精度高, 但是光学系统结构复杂, 成本高, 体积大, 不具有普遍适用性。第二种方法是先建立双波段旁轴光路系统, 再利用后续的图像配准算法补偿平行光路系统带来的视差, 这种方法虽然没有前者配准精度高, 但是光学系统结构简单, 成本低廉, 体积小, 更适合应用于实际目标检测和场景识别领域<citation id="136" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。针对第二种通过后续软件进行图像校正的配准方法, 目前常用的策略是将图像配准视为一个概率密度估计问题, 并采用高斯混合模型 (GMM) 建模对图像变换关系进行求解。2010年Boughorbel等<citation id="137" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出一种基于高斯场准则刚性图像配准方法。随后, Myronenko等<citation id="138" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出一种基于高斯统计模型的一致性点漂移算法 (CPD) , 该算法正则化配准图像点集中的位置偏移场, 使之遵从运动一致性原理 (MCT) , 可以解决刚性以及非刚性图像配准问题, 对于噪声、离群点具有一定的抑制能力。2015年Ma等<citation id="139" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>在Boughorbel刚性配准模型基础上加入正则项, 将Boughorbel刚性配准模型扩展为非刚性配准模型。此外, 为加快可见光和红外人脸图像的配准速度, Tian等<citation id="140" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出一种仅利用人脸面部外围轮廓信息进行刚性配准的方法。考虑到高斯分布只能描述具有高斯特征的图像数据, 不能准确地描述具有重尾特征的图像数据, 而Student′s-T分布自身具有重尾的特点, 可通过自由度控制分布曲线形态及尾部厚度, 能够适应不同类型的图像数据需要。Gerogiannis等<citation id="141" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>选用Student′s-T分布代替高斯分布实现刚性图像的配准。Zhou等<citation id="142" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>参照CPD算法, 在Student′s-T分布混合模型的基础上加入变形场, 提出一种基于Student′s-T分布的非刚性图像配准方法, 但是该算法在图像配准中只考虑了特征点的空间约束信息, 并没有考虑特征点的局部形状特征。2017年Maiseli等<citation id="143" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>总结了近些年来图像配准技术的研究现状及发展趋势, 指出充分利用Student′s-T分布对噪声和离群点不敏感的优势进行建模, 将会是非刚性图像配准技术的一个发展方向。</p>
                </div>
                <div class="p1">
                    <p id="53">本文综合考虑特征点的空间几何结构和局部形状特征两方面信息, 解决非刚性人脸图像的配准过程中存在的问题, 在基于特征的图像配准方法框架基础上, 提出一种由粗到细的可见光和红外人脸图像配准方法。该方法首先利用基于内部距离的形状上下文 (IDSC) 描述子作为局部特征信息, 建立图像特征点的初始匹配关系;然后将非刚性点集的配准过程转化为Student′s-T分布概率密度函数的参数估计问题;最后通过最大化观测数据的Student′s-T分布后验概率求解点集的空间变换模型, 并利用薄板样条插值方法实现可见光和红外人脸图像的配准。实验结果表明, 该方法可以实现可见光和红外人脸图像的精确配准, 具有较强的稳健性。</p>
                </div>
                <h3 id="54" name="54" class="anchor-tag">2 算法描述</h3>
                <h4 class="anchor-tag" id="55" name="55"><b>2.1 基于特征的人脸图像配准基本框架</b></h4>
                <div class="p1">
                    <p id="56">与基于区域的图像配准方法不同, 基于特征的图像配准方法首先提取两幅图像的显著特征以形成特征点集, 然后确立模板点集与目标点集之间的对应关系, 并估计图像间空间坐标的变换参数, 从而实现图像配准。图1所示为基于特征的可见光和红外人脸图像配准过程。由图1可知, 基于特征的图像配准过程主要包括特征提取、特征描述、特征匹配及图像变换与插值, 其中关键的两个步骤为特征提取与特征匹配。</p>
                </div>
                <div class="p1">
                    <p id="57">特征提取的目的是在图像中尽可能多地提取一些显著性或辨识度较高的点。例如, 针对可见光和红外图像两种不同的成像模式, 常用的包含纹理信息的灰度或颜色特征并不适用于表示异源图像的共有信息, 而是采用特定的显著性结构特征 (如角点, 强边缘、具有高曲率的点、直线交叉点、结构性轮廓) 来表示异源图像的共同信息。其中, Canny边缘检测采用双阈值检测图像的强、弱边缘, 具有信噪比高、边界点定位性能好、边缘的误检率低等优点, 因此使用Canny算子提取可见光和红外人脸图像显著性特征。提取到图像的轮廓边界后, 在轮廓上均匀采样就可以得到可见光和红外图像对应的模板点集和目标点集。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907007_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于特征的可见光和红外人脸图像配准流程" src="Detail/GetImg?filename=images/GXXB201907007_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基于特征的可见光和红外人脸图像配准流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907007_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Infrared and visible face image registration process based on feature maps</p>

                </div>
                <div class="p1">
                    <p id="59">特征匹配的目的是建立模板点集与目标点集的正确对应关系, 并估计一个变换函数将模板点集对齐到目标点集。一般来说, 如果两个点集表示相似的形状, 那么对应点将会有相似的领域结构信息, 可以用直方图来描述这种局部特征。然而, 仅依赖相邻像素间的局部特征只能得到粗略的配准, 还需要结合特征点之间的空间位置约束信息进一步提高配准精度。因此, 针对可见光和红外人脸图像配准关键问题, 本文提出一种基于局部特征和Student′s-T混合模型 (SMM) 的图像配准方法, 该方法分为以下两个步骤: 1) 利用IDSC信息作为点集局部特征描述子, 建立点集间的初始配准关系;2) 采用Student′s-T分布构建包含点集空间位置相关性的概率统计模型, 剔除初始配准点集中的噪声和离群点, 实现可见光和红外图像特征点集间的精确配准。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60"><b>2.2 基于局部特征描述子的初始配准</b></h4>
                <div class="p1">
                    <p id="61">形状上下文 (SC) <citation id="144" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>是一种常用的基于形状轮廓的局部特征描述算法, 其本质是把轮廓边界采样点中该点相对于其他特征点的角度及距离的统计直方图作为轮廓上每个点的形状上下文信息。而IDSC<citation id="145" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>是在SC算法的基础上引入对连接、部分结构不敏感的内部距离来取代采样点间的欧氏距离, 与传统的SC算法相比, IDSC算法对目标存在结构上或者关联上的失真以及非刚性变形具有良好的稳健性, 因此采用IDSC算法作为局部特征描述子来建立点集间的初始配准。其中, 内部距离可以理解为在形状内部两点间的最短距离, 如果连接两点之间的线段都位于特征形状的内部, 则两点之间的内部距离就等于欧氏距离。如果连接两点之间的线段不全位于特征形状内部, 则内部距离实际上是各个采样点之间欧氏距离的代数和, 可以利用Bellman-Ford最短路径算法来计算任意两点之间的最短路径。</p>
                </div>
                <div class="p1">
                    <p id="62">对于每个采样点, 计算该点到其余点的内部距离和角度, 从而得到一个基于距离和角度分割的二维统计直方图, 称之为每个采样点的IDSC局部特征描述子。设<i>n</i>为轮廓边界采样点的数量, <i>n</i><sub><i>θ</i></sub>表示直方图横坐标轴中划分的角度区间数量, <i>n</i><sub>d</sub>表示直方图纵坐标轴中划分的对数距离区间数量。图2所示为IDSC人脸轮廓特征提取示例, 当<i>n</i>=100, <i>n</i><sub><i>θ</i></sub>=12, <i>n</i><sub>d</sub> =5时, 图2 (c) 标出了图2 (b) 轮廓上4个采样点所对应的IDSC特征直方图。</p>
                </div>
                <div class="p1">
                    <p id="63">从图2可以看出, 对于轮廓上不同的采样点, 对应的IDSC特征直方图不相同, 具有较好的区分性, 因此可以利用该特征描述子之间的相似性建立粗略的点集配准关系。考虑到IDSC是利用直方图分布的统计特征进行描述的, 采用卡方检验作为两幅图像IDSC相似性匹配测度, 即</p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>q</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mfrac><mrow><mo stretchy="false">[</mo><mi>Η</mi><msub><mrow></mrow><mrow><mi>p</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo><mo>-</mo><mi>Η</mi><msub><mrow></mrow><mrow><mi>q</mi><mo>, </mo><mi>j</mi></mrow></msub><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>Η</mi><msub><mrow></mrow><mrow><mi>p</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo><mo>+</mo><mi>Η</mi><msub><mrow></mrow><mrow><mi>q</mi><mo>, </mo><mi>j</mi></mrow></msub><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></mstyle><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">式中:<i>K</i>为统计直方图分割的区间数量;<i>k</i>为序号;<i>H</i><sub><i>p</i>, <i>i</i></sub> (<i>k</i>) 为模板集中点<i>p</i><sub><i>i</i></sub>对应的IDSC直方图;<i>H</i><sub><i>q</i>, <i>j</i></sub> (<i>k</i>) 为目标集中点<i>q</i><sub><i>j</i></sub>对应的IDSC直方图。卡方值<i>C</i> (<i>p</i><sub><i>i</i></sub>, <i>q</i><sub><i>j</i></sub>) 越小, 表示<i>p</i><sub><i>i</i></sub>和<i>q</i><sub><i>j</i></sub>两点在形状或结构上特征越相似。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66"><b>2.3 基于空间位置约束的精确配准</b></h4>
                <div class="p1">
                    <p id="67">为进一步提高图像特征点集的配准精度, 在图像配准过程中还需考虑特征点的空间位置分布信息。高斯混合模型作为一种常用的聚类方法, 广泛应用于图像的分割和配准领域<citation id="146" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。但是由于高斯分布属于轻尾分布, 噪声和离群点的存在会对模型参数估计的结果有较大影响, 因此需要一种更稳健的概率模型来尽量减少这种影响。与高斯分布相比, Student′s-T分布属于重尾分布, 具有较好的稳健性, 更适合处理数据集中、存在噪声和离群点等少概率事件的情况<citation id="147" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。如图3所示, Student′s-T分布曲线是随自由度<i>υ</i>变化的一组曲线, 自由度<i>υ</i>越小, Student′s-T分布曲线越分散, 曲线中间越低, 重尾特征越明显;自由度<i>υ</i>越大, Student′s-T分布曲线越逼近高斯分布。</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907007_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 IDSC人脸轮廓特征提取示意图。" src="Detail/GetImg?filename=images/GXXB201907007_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 IDSC人脸轮廓特征提取示意图。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907007_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Diagrams of face silhouette feature extraction based on IDSC. </p>
                                <p class="img_note"> (a) 人脸轮廓采样点构成的Bellman-Ford最短路径图; (b) 标记的4个采样点; (c) 对应的IDSC特征直方图</p>
                                <p class="img_note"> (a) Bellman-Ford shortest path graph built using face silhouette landmark points; (b) four marked points; (c) their IDSC feature histograms</p>

                </div>
                <div class="area_img" id="69">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907007_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同自由度下的Student′s-T分布曲线" src="Detail/GetImg?filename=images/GXXB201907007_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 不同自由度下的Student′s-T分布曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907007_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Student′s T distribution for various degrees of freedom</p>

                </div>
                <div class="p1">
                    <p id="70">假设目标点集和模板点集分别表示为<b><i>X</i></b><sub><i>N</i>×<i>D</i></sub>= (<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>N</i></sub>) <sup>T</sup>, <b><i>Y</i></b><sub><i>M</i>×<i>D</i></sub>= (<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>M</i></sub>) <sup>T</sup>, 其中<i>D</i>表示点集中特征点的维数大小, <i>N</i>表示目标点集的大小, <i>M</i>表示模板点集的大小, <i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>N</i></sub>为目标点, <i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>M</i></sub>为模板点。以模板点集<b><i>Y</i></b><sub><i>M</i>×<i>D</i></sub>中各点作为SMM中各分量Student′s-T分布的质心, 而目标点集<b><i>X</i></b><sub><i>N</i>×<i>D</i></sub>表示SMM所生成的观测样本数据, 则定义多元SMM概率密度函数为</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">ψ</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>ω</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>Σ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>υ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>ω</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mfrac><mrow><mtext>Γ</mtext><mrow><mo> (</mo><mrow><mfrac><mrow><mi>υ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>D</mi></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mrow><mo>|</mo><mrow><mi>Σ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup></mrow><mrow><mo stretchy="false"> (</mo><mtext>π</mtext><mi>υ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mfrac><mi>D</mi><mn>2</mn></mfrac></mrow></msup><mtext>Γ</mtext><mrow><mo> (</mo><mrow><mfrac><mrow><mi>υ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mrow><mo>[</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><mi>d</mi><mrow><mo> (</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>;</mo><mi>Σ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>) </mo></mrow></mrow><mrow><mi>υ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow><mo>]</mo></mrow><msup><mrow></mrow><mrow><mfrac><mrow><mi>υ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>D</mi></mrow><mn>2</mn></mfrac></mrow></msup></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">式中:<b><i>x</i></b>为目标点集组成的列向量;<i>d</i> (<i>x</i><sub><i>j</i></sub>, <i>y</i><sub><i>i</i></sub>;<mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Σ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mi>Σ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mrow><mo> (</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>) </mo></mrow></mrow></math></mathml>表示点x<sub>j</sub>与y<sub>i</sub>马氏距离的平方, j为目标点序号, i为模板点序号;<i>Γ</i> (•) 为<i>Gamma</i>函数;参数集合<i>ψ</i>= (<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, …, <i>w</i><sub><i>m</i></sub>, <i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>m</i></sub>, <i>Σ</i><sub>1</sub>, <i>Σ</i><sub>2</sub>, …, <i>Σ</i><sub><i>m</i></sub><sub>, </sub><i>υ</i><sub>1</sub>, <i>υ</i><sub>2</sub>, …, <i>υ</i><sub><i>m</i></sub>) , 其中<i>w</i>、<i>y</i>、<i>Σ</i>、<i>υ</i>分别代表SMM中各个Student′s-T分布分量的权重系数、质心位置、协方差和自由度, 下标<i>m</i>为T分布分量的个数。<i>f</i> (<i>x</i><sub><i>j</i></sub>| <i>y</i><sub><i>i</i></sub><sub>, </sub><i>Σ</i><sub><i>i</i></sub><sub>, </sub><i>υ</i><sub><i>i</i></sub>) 是SMM中第<i>i</i>个分量的类条件概率密度函数。设<i>T</i> (<b><i>Y</i></b>, <i>θ</i>) 表示模板点集<b><i>Y</i></b>到目标点集<b><i>X</i></b>的空间变换关系, 其中<i>θ</i>表示变换模型中的参数集合, 将 (2) 式SMM中的各个Student′s-T分布分量的质心<i>y</i><sub><i>i</i></sub>按照参数集<i>θ</i>进行空间变换得到<i>T</i> (<i>y</i><sub><i>i</i></sub>, <i>θ</i>) , 即空间变换参数<i>θ</i>的计算过程可以视为SMM中的各个Student′s-T分布分量的质心<i>y</i><sub><i>i</i></sub>逐渐向目标点集中的对应点<i>x</i><sub><i>i</i></sub>靠拢的过程, 通过最大化观测数据的对数似然函数可以将SMM中质心拟合至目标点集中的对应点, 则图像特征点集间的配准问题可以转化为多元SMM概率模型的参数估计问题求解。</p>
                </div>
                <div class="p1">
                    <p id="74">为了方便计算多元SMM的极大似然估计, 需要引入观测数据<i>X</i><sub><i>j</i></sub>的索引变量<i>U</i><sub><i>j</i></sub>组成完全数据<i>Z</i><sub><i>j</i></sub>= (<i>X</i><sub><i>j</i></sub>, <i>U</i><sub><i>j</i></sub>) , 标记每个观测数据<i>x</i><sub><i>j</i></sub>来自哪一成分。当标记变量<i>z</i><sub><i>ij</i></sub>= (<i>z</i><sub><i>j</i></sub>) <sub><i>i</i></sub>=1时, 样本<i>X</i><sub><i>j</i></sub>服从均值为<i>y</i><sub><i>i</i></sub>、方差为<i>Σ</i><sub><i>i</i></sub>/<i>u</i><sub><i>j</i></sub>的正态分布, 索引变量<i>U</i><sub><i>j</i></sub>服从形状和尺度参数都为<i>υ</i><sub><i>i</i></sub>/2的Gamma分布, 即</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>∼</mo><mi>Ν</mi><mrow><mo> (</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>Σ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>/</mo><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>) </mo></mrow></mtd></mtr><mtr><mtd columnalign="left"><mi>U</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>z</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>∼</mo><mtext>Γ</mtext><mrow><mo> (</mo><mrow><mfrac><mrow><mi>υ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac><mo>, </mo><mfrac><mrow><mi>υ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow></mtd></mtr></mtable></mrow></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">则完全数据的对数似然函数ln <i>L</i><sub>C</sub> (<i>ψ</i>) 可以表示为完全数据<i>Z</i><sub><i>j</i></sub>的边缘密度函数<i>L</i><sub>1C</sub> (<b><i>w</i></b>) 、给定完全数据分量<i>z</i><sub><i>ij</i></sub>后索引变量<i>U</i><sub><i>j</i></sub>的条件密度函数<i>L</i><sub>2C</sub> (<i>υ</i>) 以及给定<i>z</i><sub><i>ij</i></sub>和<i>u</i><sub><i>j</i></sub>后样本<i>X</i><sub><i>j</i></sub>的条件密度函数<i>L</i><sub>3C</sub> (<b><i>y</i></b>, <i>Σ</i>) 三者的乘积, 即</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ln</mi><mspace width="0.25em" /><mi>L</mi><msub><mrow></mrow><mtext>C</mtext></msub><mo stretchy="false"> (</mo><mi>ψ</mi><mo stretchy="false">) </mo><mo>=</mo><mi>ln</mi><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>ω</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>Σ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>υ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow><mo>=</mo><mi>ln</mi><mspace width="0.25em" /><mi>L</mi><msub><mrow></mrow><mrow><mn>1</mn><mtext>C</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">) </mo><mo>+</mo><mi>ln</mi><mspace width="0.25em" /><mi>L</mi><msub><mrow></mrow><mrow><mn>2</mn><mtext>C</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">υ</mi><mo stretchy="false">) </mo><mo>+</mo><mi>ln</mi><mspace width="0.25em" /><mi>L</mi><msub><mrow></mrow><mrow><mn>3</mn><mtext>C</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><mo>, </mo><mi>Σ</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">其中</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>ln</mi><mspace width="0.25em" /><mi>L</mi><msub><mrow></mrow><mrow><mn>1</mn><mtext>C</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>z</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>ln</mi><mspace width="0.25em" /><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>ln</mi><mspace width="0.25em" /><mi>L</mi><msub><mrow></mrow><mrow><mn>2</mn><mtext>C</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">υ</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>z</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mrow><mo>[</mo><mrow><mo>-</mo><mi>ln</mi><mspace width="0.25em" /><mtext>Γ</mtext><mrow><mo> (</mo><mrow><mfrac><mrow><mi>υ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mo>+</mo><mfrac><mrow><mi>υ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac><mi>ln</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mi>υ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mo>+</mo><mfrac><mrow><mi>υ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac><mo stretchy="false"> (</mo><mi>ln</mi><mspace width="0.25em" /><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>ln</mi><mspace width="0.25em" /><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>]</mo></mrow></mtd></mtr><mtr><mtd columnalign="left"><mi>ln</mi><mspace width="0.25em" /><mi>L</mi><msub><mrow></mrow><mrow><mn>3</mn><mtext>C</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><mo>, </mo><mi>Σ</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>z</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mrow><mo>[</mo><mrow><mo>-</mo><mfrac><mi>D</mi><mn>2</mn></mfrac><mi>ln</mi><mo stretchy="false"> (</mo><mn>2</mn><mtext>π</mtext><mo stretchy="false">) </mo><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>ln</mi><mrow><mo>|</mo><mrow><mi>Σ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow><mo>-</mo><mfrac><mrow><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub><mrow><mo stretchy="false">∥</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>Σ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow><mo>]</mo></mrow></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">利用期望最大化算法 (EM) 求解 (5) 式参数集合<i>ψ</i>, 求解步骤如下。</p>
                </div>
                <div class="p1">
                    <p id="81">1) E-步求期望, 根据 (2) 式, 利用当前的混合参数值<i>ψ</i><sup> (<i>k</i>) </sup>计算观测数据<i>x</i><sub><i>j</i></sub>属于第<i>i</i>个混合模型分量的后验概率:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>τ</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>ω</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mtext>Σ</mtext><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi>υ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo></mrow><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>ψ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="left"><mi>u</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>υ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mi>D</mi></mrow><mrow><mi>υ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mi>d</mi><mrow><mo> (</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>;</mo><mi>Σ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mo>) </mo></mrow></mrow></mfrac></mtd></mtr></mtable></mrow></mrow><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">计算完全数据的对数似然函数<i>L</i><sub>C</sub> (<i>ψ</i>) 关于参数集合<i>ψ</i>和观测数据<i>x</i><sub><i>j</i></sub>的条件期望, 得到辅助函数:</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Q</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">ψ</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">ψ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><mo>=</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">ψ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">υ</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">ψ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mn>3</mn></msub><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">y</mi><mo>, </mo><mi>Σ</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">ψ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup></mrow><mo>) </mo></mrow><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">辅助函数<b><i>Q</i></b><sub>1</sub>, <b><i>Q</i></b><sub>2</sub>, <b><i>Q</i></b><sub>3</sub>分别表示为</p>
                </div>
                <div class="area_img" id="154">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201907007_15400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="88">式中:<i>γ</i> (•) 表示双Gamma函数。设非刚性变形情况下点集的空间变换定义为<b><i>X</i></b>=<i>T</i> (<b><i>Y</i></b>, <i>ρ</i>) =<b><i>Y</i></b>+<i>ρ</i> (<b><i>Y</i></b>) , 其中<i>ρ</i>表示模板点集<b><i>Y</i></b>相对于目标点集<b><i>X</i></b>的平移参量, 根据MCT, 为保持点集配准过程中偏移参数的平滑性, 需在辅助函数<b><i>Q</i></b><sub>3</sub>中添加平滑正则项<i>φ</i> (<i>ρ</i>) , 表示为</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Q</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mn>3</mn></msub><mo>=</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mn>3</mn></msub><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">ρ</mi><mo>, </mo><mi>Σ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">ψ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup></mrow><mo>) </mo></mrow><mo>+</mo><mfrac><mi>λ</mi><mn>2</mn></mfrac><mi mathvariant="bold-italic">φ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">ρ</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">式中:<i>λ</i>为正则化参数, 用来控制 (9) 式中两函数项权重系数。参照文献<citation id="148" type="reference">[<a class="sup">9</a>]</citation>, 引入再生核希尔伯特空间理论, 将平移参量进行傅里叶变换, 即</p>
                </div>
                <div class="area_img" id="91">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201907007_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="92">式中:<i>h</i><sub><i>i</i></sub>为高斯核矩阵<b><i>G</i></b>的权重系数。将 (10) 式表示为矩阵形式<i>ρ</i> (<b><i>Y</i></b>) =<b><i>GH</i></b>, 其中系数矩阵<b><i>H</i></b><sub><i>M</i>×<i>D</i></sub>= (<i>h</i><sub>1</sub>, <i>h</i><sub>2</sub>, …, <i>h</i><sub><i>m</i></sub>) , 高斯核函数<b><i>G</i></b>在频域中可以看成一个低通滤波器, 将其代入 (9) 式得到点集存在非刚性变形情况下的辅助函数<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Q</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mn>3</mn></msub></mrow></math></mathml>:</p>
                </div>
                <div class="area_img" id="94">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201907007_09400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="95">2) M-步求期望最大化, 通过最大化辅助函数<b><i>Q</i></b><sub>1</sub>, <b><i>Q</i></b><sub>2</sub>和<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Q</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mn>3</mn></msub></mrow></math></mathml>, 获得更新的参数集合<i>ψ</i><sup> (<i>k</i>+1) </sup>。对<b><i>Q</i></b><sub>1</sub>求导取极值, 权重系数更新值表示为每个样本点属于SMM第<i>i</i>个分量的后验概率平均值, 即</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mfrac><mrow><mi>τ</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mi>Ν</mi></mfrac></mrow></mstyle><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>m</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">式中:<i>N</i>为目标点集<b><i>X</i></b><sub><i>N</i>×<i>D</i></sub>的大小。对<b><i>Q</i></b><sub>2</sub>求导取极值, 自由度<i>υ</i>更新值表示为如下方程的解:</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>1</mn><mo>-</mo><mi>γ</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mi>υ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mo>+</mo><mi>ln</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mi>υ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mo>+</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>τ</mi></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ln</mi><mspace width="0.25em" /><mi>u</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>-</mo><mi>u</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>τ</mi></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></mfrac><mo>+</mo><mi>γ</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mi>υ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mi>D</mi></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mo>-</mo><mi>ln</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mi>υ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mi>D</mi></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mo>=</mo><mn>0</mn><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">对<mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Q</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mn>3</mn></msub></mrow></math></mathml>求导取极值, 系数矩阵<b><i>H</i></b>和协方差矩阵内元素∑<sub><i>i</i></sub>更新值表示为</p>
                </div>
                <div class="area_img" id="155">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/GXXB201907007_15500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="104">式中:<b><i>P</i></b>为由 (6) 式得到的后验概率<i>τ</i><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>和u<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>所确定的概率密度矩阵; <b>1</b>表示元素都为1的列向量; <b><i>I</i></b>表示单位矩阵; <b><i>G</i></b> (<i>i</i>, •) 表示核矩阵<b><i>G</i></b>的第<i>i</i>行向量。</p>
                </div>
                <div class="p1">
                    <p id="107">交替执行上述E-步和M-步, 更新后验概率<i>τ</i><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, u<mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, 参数集合<i>ψ</i><sup> (<i>k</i>+1) </sup>和系数矩阵<b><i>H</i></b><sup> (<i>k</i>+1) </sup>直至满足收敛条件<image id="110" type="formula" href="images/GXXB201907007_11000.jpg" display="inline" placement="inline"><alt></alt></image>时停止迭代, 其中ε为收敛阈值。本文算法流程如图4所示。终止迭代后, 利用 (14) 式得到特征点集间的变换矩阵<b><i>T</i></b> (<b><i>Y</i></b>, <i>ρ</i>) , 就可以将模板点集对齐到目标点集, 并采用图像插值算法<citation id="149" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>将可见光图像变换到红外图像坐标系下, 实现多光谱人脸图像的配准。算法执行过程中具体参数设置为<i>λ</i>=3, <i>β</i>=2, <i>ε</i>=10<sup>-5</sup>。</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907007_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 本文配准算法流程图" src="Detail/GetImg?filename=images/GXXB201907007_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 本文配准算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907007_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Flow chart of proposed registration algorithm</p>

                </div>
                <h3 id="112" name="112" class="anchor-tag">3 实验结果与讨论</h3>
                <div class="p1">
                    <p id="113">为验证本文图像配准方法的有效性, 在Inter© CoreTM i5-4590 CPU@3.3 GHz、4 GB内存的PC机上, 基于MATLAB R2014a (8.3) 软件在仿真点集数据和真实人脸图像上进行了图像配准实验。将基于局部特征和T分布混合模型的图像配准算法 ( L-SMM) 与高斯混合模型加形状上下文特征图像配准算法 (RPM-GMM<citation id="150" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>) 和SMM<citation id="151" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>等处于领先水平的非刚性点集配准算法进行比较分析。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114"><b>3.1 仿真数据集</b></h4>
                <div class="p1">
                    <p id="115">采用文献<citation id="152" type="reference">[<a class="sup">18</a>]</citation>中的数据集进行点集配准仿真, 考虑到汉字“福”的模型比较复杂, 具有连接、部分结构更符合人脸特征的特点, 因此选择数据集中的汉字“福”的形状点集进行配准仿真。图5所示为该点集存在变形、噪声和离群点3种退化情况下使用不同算法进行点集配准仿真的结果, 在仿真模型中使用召回率曲线来定量评估不同算法的配准精度, 针对每种退化情况进行了100次独立重复仿真。召回率定义为所有预设匹配点集中被算法识别为正确匹配点的数量所占的比例, 被算法识别为正确匹配点的数量是指配准仿真中匹配误差满足阈值要求的点数量, 其中匹配误差为独立重复仿真中通过变换模型将模板点集“+”对齐到目标点集“○”后对应点间的欧氏距离。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907007_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 汉字福仿真点集配准结果。" src="Detail/GetImg?filename=images/GXXB201907007_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 汉字福仿真点集配准结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907007_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Registration results on synthetic Chinese character point sets. </p>
                                <p class="img_note"> (a) ～ (c) 配准前的点集; (d) ～ (f) 本文算法的配准结果; (g) ～ (i) 3种配准算法的召回率曲线</p>
                                <p class="img_note"> (a) - (c) Point sets before registration; (d) - (f) registration results of proposed algorithm; (g) - (i) recall curves of three registration algorithms</p>

                </div>
                <div class="p1">
                    <p id="117">图5中的3列分别表示不同退化情况下的配准结果比较;图5 (a) ～ (c) 表示不同退化情况下的原始点集数据, 图5 (d) ～ (f) 表示使用L-SMM算法将模板点集“+”对齐到目标点集“○”上的配准结果, 图5 (g) ～ (i) 则表示在给定的匹配精度阈值下每种算法的召回率曲线, 曲线越凸向左上角, 表明点集配准效果越好。由图5所示的不同退化程度下点集的召回率曲线可知, L-SMM的配准结果略优于RPM-GMM的配准结果, 明显优于仅利用空间位置相关性的SMM的配准结果。因此, 有必要在SMM方法中加入IDSC描述子, 作为局部特征来提高配准算法的精度和稳健性。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118"><b>3.2 真实人脸图像配准</b></h4>
                <div class="p1">
                    <p id="119">在真实图像数据仿真中, 采用公开的红外/可见光人脸图像数据库UTK-IRIS进行多光谱人脸图像配准仿真。该数据库包含了不同光照、不同姿态、不同表情以及不同种族的红外/可见光人脸图像。从数据集中随机抽取了部分个体的人脸图像 (Charles, Heo, Gribok, Sharon) 进行配准仿真, 每个测试个体包含不同角度和光照强度下拍摄的多光谱人脸图像, 配准结果如图6所示。</p>
                </div>
                <div class="p1">
                    <p id="120">图6 (a) 和图6 (b) 分别表示配准前的可见光和红外原始图像。图6 (c) 表示利用Canny边缘检测算子提取到的人脸图像离散特征点集。其中, 红色代表可见光图像点集, 蓝色代表红外图像点集。由于成像模式的不同, 可见光和红外人脸图像点集存在非刚性的形变、噪声及离群点等影响因素。图6 (d) 和图6 (e) 分别为使用SMM和L-SMM配准算法得到的可见光和红外图像变形叠加后生成的人脸棋盘格图像, 棋盘格图像是指利用配准算法将可见光图像变换叠加到红外图像坐标系后, 两种图像相间叠加的可见光和红外人脸图像配准效果。如图6 (d) 中标注的红色方框处, SMM算法得到的多光谱叠加图像接缝处出现明显的错位, 而本文提出的L-SMM配准算法在图6 (e) 中图像接缝处连接比较自然, 无明显的错位情况, 因此图6可以定性地说明L-SMM算法人脸图像配准效果优于SMM算法。</p>
                </div>
                <div class="p1">
                    <p id="121">考虑到L-SMM与RPM-GMM算法配准效果相差不明显, 从棋盘格图像无法直观地比较优劣, 因此采用Recall曲线作为实际多光谱人脸图像配准精度的定量评测标准, 在图6中每组个体的可见光和红外图像中人工选取20对不同的标记对应点作为模板点集和目标点集进行比较分析, 选取的标记点主要集中在五官和眼睛等面貌特征明显的区域。图7所示为图6中不同个体 (Charles, Heo, Gribok, Sharon) 的多光谱人脸图像数据集的定量比较结果。每组个体数据集中包含不同光照、不同姿态以及不同表情的约200张可见光和红外人脸图像。</p>
                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907007_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 UTK-IRIS数据集下可见光和红外人脸图像配准结果。" src="Detail/GetImg?filename=images/GXXB201907007_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 UTK-IRIS数据集下可见光和红外人脸图像配准结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907007_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Visible and infrared face registration results in UTK-IRIS database. </p>
                                <p class="img_note"> (a) (b) 原始的可见光和红外图像; (c) 人脸边缘图; (d) (e) 可见光和红外图像变形叠加后生成的棋盘格图像</p>
                                <p class="img_note"> (a) (b) Original visible and infrared images; (c) face edge maps; (d) (e) checkerboards of warping visible image into and infrared image</p>

                </div>
                <div class="p1">
                    <p id="123">在图7中, Recall曲线越凸向左上角, 说明算法的图像配准精度越高。正如图6中的定性评价结果所述, 结合局部特征的RPM-GMM和L-SMM图像配准算法的人脸图像配准结果明显优于未考虑局部特征信息的SMM图像配准方法, 因此在图像配准过程中加入合适的特征描述子作为局部特征信息, 可以提高图像的配准精度。此外, 从结合局部特征的两种图像配准算法比较结果中可以看出, 在Charles、Heo、Gribok和Sharon的人脸图像配准结果中, 所提的L-SMM算法的Recall曲线始终在RPM-GMM算法的Recall曲线的上方, 说明在配准图像存在噪声点和离群点的情况下, Student′s-T分布混合概率模型比高斯混合概率模型更适合被用于点集的配准过程。表1所示为3种配准算法在图7中20对人工标记点对应的平均匹配误差和运行时间比较。</p>
                </div>
                <div class="area_img" id="124">
                    <p class="img_tit">表1 各配准算法的平均匹配误差和运行时间比较 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Comparison of average matching errors and running time with different registration algorithms</p>
                    <p class="img_note"></p>
                    <table id="124" border="1"><tr><td rowspan="2"><br />Algorithm</td><td colspan="4"><br />Average matching error /pixel</td><td rowspan="2">Mean running time /s</td></tr><tr><td><br />Charles</td><td>Heo</td><td>Gribok</td><td>Sharon</td></tr><tr><td><br />SMM<sup>[13]</sup></td><td>1.8</td><td>1.5</td><td>1.6</td><td>1.7</td><td>1.1</td></tr><tr><td><br />RPM-GMM<sup>[10]</sup></td><td>1.5</td><td>1.4</td><td>1.4</td><td>1.2</td><td>1.5</td></tr><tr><td><br />L-SMM</td><td>1.2</td><td>1.1</td><td>1.0</td><td>0.9</td><td>0.9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907007_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同个体的多光谱人脸图像数据集的定量比较结果。" src="Detail/GetImg?filename=images/GXXB201907007_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同个体的多光谱人脸图像数据集的定量比较结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907007_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Quantitative result comparison of multispectral face image pairs of different individuals. </p>
                                <p class="img_note"> (a) Charles; (b) Heo; (c) Gribok; (d) Sharon</p>
                                <p class="img_note"> (a) Charles; (b) Heo; (c) Gribok; (d) Sharon</p>

                </div>
                <div class="p1">
                    <p id="126">由表1可知, 无论在匹配精度还是计算效率上, L-SMM算法都优于SMM和RPM-GMM两种对比算法, 这是由于预先利用IDSC局部邻域结构特征建立特征点集的初始匹配关系后, 在一定程度上减少了SMM配准模型参数估计过程中的平均迭代次数。在RPM-GMM配准算法中, 如果不采用快速高斯变换近似目标函数中的高斯函数加权和, 时间的复杂度为<i>O</i> (<i>MN</i>) , 运算过程最为耗时。</p>
                </div>
                <div class="p1">
                    <p id="127">此外, 为了检验本文算法在实际应用场景下的配准效果, 利用比利时Xenics红外相机和大恒图像公司水星系列彩色工业相机构建了一套简易的多光谱成像系统。图8所示为利用自主构建的双波段旁轴光路系统采集到的多光谱人脸图像及配准和融合结果。</p>
                </div>
                <div class="p1">
                    <p id="128">图8 (a) 和图8 (b) 分别表示采用自主构建的可见光和红外平行光路成像设备采集到的多光谱人脸原始图像, 图8 (c) 表示使用L-SMM配准算法得到的可见光和红外图像变形叠加后生成的人脸棋盘格图像, 图8 (d) 表示使用文献<citation id="153" type="reference">[<a class="sup">19</a>]</citation>中的离散小波变换方法对可见光和红外人脸图像重构得到的融合图像。可以看出, 融合后的图像既保留了可见光图像部分细节和纹理特征, 也包含了部分红外图像热辐射信息, 因此可以保证后续人脸识别方法的全天候工作能力。</p>
                </div>
                <h3 id="129" name="129" class="anchor-tag">4 结  论</h3>
                <div class="p1">
                    <p id="130">提出了一种基于局部特征和Student′s-T混合模型的多光谱人脸图像配准方法, 即在图像配准过程中采用IDSC描述子与Student′s-T分布混合概率密度函数相结合的策略, 降低了多光谱人脸图像中出现非刚性形变、噪声及离群点等因素对配准结果的影响, 并通过仿真和实测实验验证了所提方法的有效性。通过可见光和红外人脸图像数据的定性和定量评测, 与2种处于领先水平的非刚性图像配准方法相比, 所提方法在平均匹配误差和运算效率方面都具有一定的优势, 可为后续多光谱人脸图像融合和全天候检测识别研究提供精度保证。鉴于图像配准最终目的是提升多光谱图像融合质量, 因此在后续的研究工作中, 将结合多种图像融合算法对所提图像配准方法精度进行全面、科学的评价, 并扩大测试样本在自主构建的多光谱人脸数据集中进行系统测试。</p>
                </div>
                <div class="area_img" id="131">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GXXB201907007_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 自行采集的可见光和红外人脸图像配准和融合结果。" src="Detail/GetImg?filename=images/GXXB201907007_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 自行采集的可见光和红外人脸图像配准和融合结果。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GXXB201907007_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Visible and infrared face image registration and fusion results captured by ourselves. </p>
                                <p class="img_note"> (a) 原始可见光图像; (b) 原始红外图像; (c) 棋盘格图像; (d) 融合图像</p>
                                <p class="img_note"> (a) Original visible images; (b) original infrared images; (c) checkerboard images; (d) fusion images</p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="12">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201710016&amp;v=MTMwMTlNSWpYVGJMRzRIOWJOcjQ5RVlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmdWN3I=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Ding W S, Bi D Y, He L Y, <i>et al</i>.Fusion of infrared and visible images based on shearlet transform and neighborhood structure features[J].Acta Optica Sinica, 2017, 37 (10) :1010002.丁文杉, 毕笃彦, 何林远, 等.基于剪切波变换和邻域结构特征的红外与可见光图像融合[J].光学学报, 2017, 37 (10) :1010002.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201711013&amp;v=MTMwNTJPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmdWN3JNSWpYVGJMRzRIOWJOcm85RVo0UUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Liu X H, Chen Z B.Fusion of infrared and visible images based on multi-scale directional guided filter and convolutional sparse representation[J].Acta Optica Sinica, 2017, 37 (11) :1110004.刘先红, 陈志斌.基于多尺度方向引导滤波和卷积稀疏表示的红外与可见光图像融合[J].光学学报, 2017, 37 (11) :1110004.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A comparative analysis of visual and thermal face image fusion based on different wavelet family">

                                <b>[3]</b> Pal A R, Singha A.A comparative analysis of visual and thermal face image fusion based on different wavelet family[C]//2017 International Conference on Innovations in Electronics, Signal Processing and Communication (IESC) , April 6-7, 2017, Shillong, India.New York:IEEE, 2017:8071895.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF1AA21F05383F7D583B754F0E37B5D1D&amp;v=MjA1OTJwYlEzNWRoaHhiMjd4YXc9TmlmT2ZjVzViNkRPcnZsRlllZ0hEd28rdXhNYjZVMTZUWHVVckdjMmZzQ1JNYnZyQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Ma J Y, Ma Y, Li C.Infrared and visible image fusion methods and applications:a survey[J].Information Fusion, 2019, 45:153-178.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201306031&amp;v=MjE2OTE0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5nVjdyTUlqWFRiTEc0SDlMTXFZOUdaWVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Liu X C, Zhong T, Yu Q F, <i>et al</i>.Multi-modal image registration based on local frequency information using modified simplex-simulated annealing algorithm[J].Acta Optica Sinica, 2013, 33 (6) :0615002.刘晓春, 钟涛, 于起峰, 等.基于局部频率信息和单纯型-模拟退火的异源图像配准[J].光学学报, 2013, 33 (6) :0615002.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201209038&amp;v=MjAzMDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnluZ1Y3ck1JalhUYkxHNEg5UE1wbzlHYklRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Lin Q, Jin W Q, Guo H, <i>et al</i>.Confocal-window telescope objective design in visible and long-wave infrared[J].Acta Optica Sinica, 2012, 32 (9) :0922005.林青, 金伟其, 郭宏, 等.可见光/长波红外共聚焦窗口望远物镜设计[J].光学学报, 2012, 32 (9) :0922005.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002831084&amp;v=MTI1MTJTWHFScnhveGNNSDdSN3FlYnVkdEZTamxVcnJNSlZvPU5qN0Jhck80SHRIT3A0eEVaT01MWTNrNXpCZGg0ajk5&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Kong S G, Heo J, Boughorbel F, <i>et al</i>.Multiscale fusion of visible and thermal IR images for illumination-invariant face recognition[J].International Journal of Computer Vision, 2007, 71 (2) :215-233.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201348988&amp;v=MDg1MDRmYks3SHRET3JZOUVaKzhIQlhReG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUlWc1FieFk9TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Boughorbel F, Mercimek M, Koschan A, <i>et al</i>.A new method for the registration of three-dimensional point-sets:the Gaussian fields framework[J].Image and Vision Computing, 2010, 28 (1) :124-137.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Point Set Registration: Coherent Point Drift">

                                <b>[9]</b> Myronenko A, Song X B.Point set registration:coherent point drift[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (12) :2262-2275.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700101420&amp;v=MTM4OTZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lJVnNRYnhZPU5pZk9mYks4SDlETXFJOUZaZXNPQ0g0NQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Ma J Y, Zhao J, Ma Y, <i>et al</i>.Non-rigid visible and infrared face registration via regularized Gaussian fields criterion[J].Pattern Recognition, 2015, 48 (3) :772-784.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1D6971F6D9C6126779AA40880A072325&amp;v=MDU0MzdOakxydmxERU9KOENuMDd5UkVVNDA0TVRIL3FwQkpFZWJXV1JyaWFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh4YjI3eGF3PU5pZk9mYkxNRw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Tian T, Mei X G, Yu Y, <i>et al</i>.Automatic visible and infrared face registration based on silhouette matching and robust transformation estimation[J].Infrared Physics &amp; Technology, 2015, 69:145-154.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349053&amp;v=Mjc4MDl4WT1OaWZPZmJLN0h0RE9yWTlFWis4R0RIazZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lJVnNRYg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Gerogiannis D, Nikou C, Likas A.The mixtures of student′s <i>t</i>-distributions as a robust framework for rigid registration[J].Image and Vision Computing, 2009, 27 (9) :1285-1294.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust non-rigid point set registration using student&amp;#39;&amp;#39;st mixture model">

                                <b>[13]</b> Zhou Z Y, Zheng J, Dai Y K, <i>et al</i>.Robust non-rigid point set registration using student′s-t mixture model[J].PLoS One, 2014, 9 (3) :e91381.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESD21C7D537C972ED5652B143CACD47650&amp;v=MjM5NjFiYVRRNytmQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGhoeGIyN3hhdz1OaWZPZmNlNkg2TEwyNHBHWTVnR0MzNU11eE1WN3owUFNYdmgzMk5HRA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Maiseli B, Gu Y F, Gao H J.Recent developments and trends in point set registration methods[J].Journal of Visual Communication and Image Representation, 2017, 46:95-106.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape matching and object recognition using shape contexts">

                                <b>[15]</b> Belongie S, Malik J, Puzicha J.Shape matching and object recognition using shape contexts[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002, 24 (4) :509-522.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape Classification Using the Inner-Distance">

                                <b>[16]</b> Ling H B, Jacobs D W.Shape classification using the inner-distance[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (2) :286-299.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002792185&amp;v=MjcwMDFyck1KVm89Tmo3QmFyTzRIdEhPcUlaSFplTUtZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTamxV&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> Peel D, McLachlan G J.Robust mixture modelling using the <i>t</i> distribution[J].Statistics and Computing, 2000, 10 (4) :339-348.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501084334&amp;v=MDc0MTRmT2ZiSzdIdEROcW85RVpPTUxEMzg5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JSVZzUWJ4WT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Chui H L, Rangarajan A.A new point matching algorithm for non-rigid registration[J].Computer Vision and Image Understanding, 2003, 89 (2/3) :114-141.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Infrared and visible images fusion method based on discrete wavelet transform">

                                <b>[19]</b> Zhan L C, Zhuang Y, Huang L D.Infrared and visible images fusion method based on discrete wavelet transform[J].Journal of Computers, 2017, 28 (2) :57-71.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GXXB201907007" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201907007&amp;v=MTcxNjk5ak1xSTlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnluZ1Y3ck1JalhUYkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9rakVxV3hmUTNNTFVERjZkUE9iUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

