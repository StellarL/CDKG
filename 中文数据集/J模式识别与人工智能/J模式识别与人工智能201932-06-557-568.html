<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131451174717500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201906009%26RESULT%3d1%26SIGN%3dYknKLyspUKmWsFm5Q4KJDw9Kzh0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201906009&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201906009&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201906009&amp;v=MDA2MzVxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emhVN3ZPS0Q3WWJMRzRIOWpNcVk5RmJZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="1 3D人脸重构 ">1 3D人脸重构</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="2 基于人脸标准化的3D人脸重构 ">2 基于人脸标准化的3D人脸重构</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#104" data-title="&lt;b&gt;2.1&lt;/b&gt; 基于&lt;b&gt;3D&lt;/b&gt;形变模型的&lt;b&gt;2D&lt;/b&gt;人脸正则化"><b>2.1</b> 基于<b>3D</b>形变模型的<b>2D</b>人脸正则化</a></li>
                                                <li><a href="#147" data-title="&lt;b&gt;2.2 3D&lt;/b&gt;人脸的肤色纹理重构"><b>2.2 3D</b>人脸的肤色纹理重构</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#174" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#217" data-title="4 结 束 语 ">4 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#96" data-title="图1 本文算法流程图">图1 本文算法流程图</a></li>
                                                <li><a href="#249" data-title="图2 人脸图像分片">图2 人脸图像分片</a></li>
                                                <li><a href="#250" data-title="图3 使用泊松图片编辑填充不可见区域">图3 使用泊松图片编辑填充不可见区域</a></li>
                                                <li><a href="#251" data-title="图4 人脸形状和光照估计">图4 人脸形状和光照估计</a></li>
                                                <li><a href="#252" data-title="图5 2种算法得到的人脸标准化结果对比">图5 2种算法得到的人脸标准化结果对比</a></li>
                                                <li><a href="#252" data-title="图5 2种算法得到的人脸标准化结果对比">图5 2种算法得到的人脸标准化结果对比</a></li>
                                                <li><a href="#253" data-title="图6 2D-3D点对应关系">图6 2D-3D点对应关系</a></li>
                                                <li><a href="#253" data-title="图6 2D-3D点对应关系">图6 2D-3D点对应关系</a></li>
                                                <li><a href="#254" data-title="图7 使用原图像和标准化人脸图像的纹理渲染对比">图7 使用原图像和标准化人脸图像的纹理渲染对比</a></li>
                                                <li><a href="#255" data-title="图8 2种方法的人脸纹理重构结果对比">图8 2种方法的人脸纹理重构结果对比</a></li>
                                                <li><a href="#256" data-title="图9 不同算法的人脸重建结果对比">图9 不同算法的人脸重建结果对比</a></li>
                                                <li><a href="#256" data-title="图9 不同算法的人脸重建结果对比">图9 不同算法的人脸重建结果对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="257">


                                    <a id="bibliography_1" title=" BLANZ V, VETTER T.A Morphable Model for the Synthesis of 3D Faces // Proc of the 26th Annual Conference on Computer Graphics and Interactive Techniques.New York, USA:ACM, 1999:187-194." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A morphable model for the synthesis of 3D faces">
                                        <b>[1]</b>
                                         BLANZ V, VETTER T.A Morphable Model for the Synthesis of 3D Faces // Proc of the 26th Annual Conference on Computer Graphics and Interactive Techniques.New York, USA:ACM, 1999:187-194.
                                    </a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_2" title=" 梁荣华, 陈纯, 张慧.一个三维人脸真实感模型重建算法.模式识别与人工智能, 2003, 16 (1) :116-121. (LIANG R H, CHEN C, ZHANG H.An Algorithm for 3D Realistic-Looking Facial Model Reconstruction.Pattern Recognition and Artificial Intelligence, 2003, 16 (1) :116-121.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB200301020&amp;v=MTA3ODVMT2VaZVJuRnl6aFU3dk9LRDdZYkxHNEh0TE1ybzlIWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         梁荣华, 陈纯, 张慧.一个三维人脸真实感模型重建算法.模式识别与人工智能, 2003, 16 (1) :116-121. (LIANG R H, CHEN C, ZHANG H.An Algorithm for 3D Realistic-Looking Facial Model Reconstruction.Pattern Recognition and Artificial Intelligence, 2003, 16 (1) :116-121.) 
                                    </a>
                                </li>
                                <li id="261">


                                    <a id="bibliography_3" title=" CHU B, ROMDHANI S, CHEN L M.3D-Aided Face Recognition Robust to Expression and Pose Variations // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:1907-1914." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D-aided face recognition robust to expression and pose variations">
                                        <b>[3]</b>
                                         CHU B, ROMDHANI S, CHEN L M.3D-Aided Face Recognition Robust to Expression and Pose Variations // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:1907-1914.
                                    </a>
                                </li>
                                <li id="263">


                                    <a id="bibliography_4" title=" AMBERG B, KNOTHE R, VETTER T.Expression Invariant 3D Face Recognition with a Morphable Model // Proc of the 8th IEEE International Conference on Automatic Face and Gesture Recognition.Washington, USA:IEEE, 2008.DOI:10.1109/AFGR.2008.4813376." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Expression invariant3D face recognition with amorphable model">
                                        <b>[4]</b>
                                         AMBERG B, KNOTHE R, VETTER T.Expression Invariant 3D Face Recognition with a Morphable Model // Proc of the 8th IEEE International Conference on Automatic Face and Gesture Recognition.Washington, USA:IEEE, 2008.DOI:10.1109/AFGR.2008.4813376.
                                    </a>
                                </li>
                                <li id="265">


                                    <a id="bibliography_5" title=" CAO C, WENG Y L, ZHOU S, et al.Facewarehouse:A 3D Facial Expression Database for Visual Computing.IEEE Transactions on Visualization and Computer Graphics, 2014, 20 (3) :413-425." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A 3D facial expression database for visual computing">
                                        <b>[5]</b>
                                         CAO C, WENG Y L, ZHOU S, et al.Facewarehouse:A 3D Facial Expression Database for Visual Computing.IEEE Transactions on Visualization and Computer Graphics, 2014, 20 (3) :413-425.
                                    </a>
                                </li>
                                <li id="267">


                                    <a id="bibliography_6" title=" BLANZ V, MEHL A, VETTER T, et al.A Statistical Method for Robust 3D Surface Reconstruction from Sparse Data // Proc of the 2nd International Symposium on 3D Data Processing, Visualization and Transmission.Washington, USA:IEEE, 2004:293-300." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A statistical method forrobust 3D surface reconstruction from sparse data">
                                        <b>[6]</b>
                                         BLANZ V, MEHL A, VETTER T, et al.A Statistical Method for Robust 3D Surface Reconstruction from Sparse Data // Proc of the 2nd International Symposium on 3D Data Processing, Visualization and Transmission.Washington, USA:IEEE, 2004:293-300.
                                    </a>
                                </li>
                                <li id="269">


                                    <a id="bibliography_7" title=" ROMDHANI S, VETTER T.Estimating 3D Shape and Texture Using Pixel Intensity, Edges, Specular Highlights, Texture Constraints and a Prior // Proc of the IEEE Computer Society Confe-rence on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2005, II:986-993." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Estimating 3D shape and textureusing pixel intensity,edges,specular highlights,textureconstraints and a prior">
                                        <b>[7]</b>
                                         ROMDHANI S, VETTER T.Estimating 3D Shape and Texture Using Pixel Intensity, Edges, Specular Highlights, Texture Constraints and a Prior // Proc of the IEEE Computer Society Confe-rence on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2005, II:986-993.
                                    </a>
                                </li>
                                <li id="271">


                                    <a id="bibliography_8" title=" ZHU X Y, YAN J J, YI D, et al.Discriminative 3D Morphable Model Fitting // Proc of the 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition.Washington, USA:IEEE, 2015.DOI:10.1109/FG.2015.7163096." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative3D morphable model fitting">
                                        <b>[8]</b>
                                         ZHU X Y, YAN J J, YI D, et al.Discriminative 3D Morphable Model Fitting // Proc of the 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition.Washington, USA:IEEE, 2015.DOI:10.1109/FG.2015.7163096.
                                    </a>
                                </li>
                                <li id="273">


                                    <a id="bibliography_9" title=" LIU F, ZHAO Q J, LIU X M, et al.Joint Face Alignment and 3D Face Reconstruction with Application to Face Recognition[J/OL].[2019-01-13].http://cvlab.cse.msu.edu/pdfs/Liu_Zeng_Zhao_Liu_ECCV2016.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint Face Alignment and 3D Face Reconstruction with Application to Face Recognition">
                                        <b>[9]</b>
                                         LIU F, ZHAO Q J, LIU X M, et al.Joint Face Alignment and 3D Face Reconstruction with Application to Face Recognition[J/OL].[2019-01-13].http://cvlab.cse.msu.edu/pdfs/Liu_Zeng_Zhao_Liu_ECCV2016.pdf.
                                    </a>
                                </li>
                                <li id="275">


                                    <a id="bibliography_10" title=" HUBER P, FENG Z H, CHRISTMAS W, et al.Fitting 3D Morphable Face Models Using Local Features // Proc of the IEEE International Conference on Image Processing.Washington, USA:IEEE, 2015:1195-1199." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fitting 3Dmorphable face models using local features">
                                        <b>[10]</b>
                                         HUBER P, FENG Z H, CHRISTMAS W, et al.Fitting 3D Morphable Face Models Using Local Features // Proc of the IEEE International Conference on Image Processing.Washington, USA:IEEE, 2015:1195-1199.
                                    </a>
                                </li>
                                <li id="277">


                                    <a id="bibliography_11" title=" JOURABLOO A, LIU X M.Pose-Invariant 3D Face Alignment[C/OL].[2019-01-13].https://arxiv.org/pdf/1506.03799.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pose-Invariant 3D Face Alignment[C/OL]">
                                        <b>[11]</b>
                                         JOURABLOO A, LIU X M.Pose-Invariant 3D Face Alignment[C/OL].[2019-01-13].https://arxiv.org/pdf/1506.03799.pdf.
                                    </a>
                                </li>
                                <li id="279">


                                    <a id="bibliography_12" title=" ZHU X Y, LEI Z, LIU X M, et al.Face Alignment across Large Poses:A 3D Solution[C/OL].[2019-01-13].https://arxiv.org/pdf/1511.07212.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face Alignment across Large Poses:A 3D Solution[C/OL]">
                                        <b>[12]</b>
                                         ZHU X Y, LEI Z, LIU X M, et al.Face Alignment across Large Poses:A 3D Solution[C/OL].[2019-01-13].https://arxiv.org/pdf/1511.07212.pdf.
                                    </a>
                                </li>
                                <li id="281">


                                    <a id="bibliography_13" title=" DOU P F, SHAH S K, KAKADIARIS I A.End-to-End 3D Face Reconstruction with Deep Neural Networks[C/OL].[2019-01-13].https://arxiv.org/pdf/1704.05020.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-End 3D Face Reconstruction with Deep Neural Networks[C/OL]">
                                        <b>[13]</b>
                                         DOU P F, SHAH S K, KAKADIARIS I A.End-to-End 3D Face Reconstruction with Deep Neural Networks[C/OL].[2019-01-13].https://arxiv.org/pdf/1704.05020.pdf.
                                    </a>
                                </li>
                                <li id="283">


                                    <a id="bibliography_14" title=" JOURABLOO A, LIU X M.Large-Pose Face Alignment via CNN-Based Dense 3D Model Fitting // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:4188-4196." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-pose face alignment via CNN-based dense 3D model fitting">
                                        <b>[14]</b>
                                         JOURABLOO A, LIU X M.Large-Pose Face Alignment via CNN-Based Dense 3D Model Fitting // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:4188-4196.
                                    </a>
                                </li>
                                <li id="285">


                                    <a id="bibliography_15" title=" FENG Y, WU F, SHAO X H, et al.Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network[C/OL].[2019-01-13].https://arxiv.org/pdf/1803.07835.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network[C/OL]">
                                        <b>[15]</b>
                                         FENG Y, WU F, SHAO X H, et al.Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network[C/OL].[2019-01-13].https://arxiv.org/pdf/1803.07835.pdf.
                                    </a>
                                </li>
                                <li id="287">


                                    <a id="bibliography_16" title=" BLANZ V, SCHERBAUM K, VETTER T, et al.Exchanging Faces in Images.Computer Graphics Forum, 2004, 23 (3) :669-676." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exchanging Faces in Images">
                                        <b>[16]</b>
                                         BLANZ V, SCHERBAUM K, VETTER T, et al.Exchanging Faces in Images.Computer Graphics Forum, 2004, 23 (3) :669-676.
                                    </a>
                                </li>
                                <li id="289">


                                    <a id="bibliography_17" title=" HU G S, YAN F, CHAN C H, et al.Face Recognition Using a Unified 3D Morphable Model // Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:73-89." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face Recognition Using a Unified 3D Morphable Model">
                                        <b>[17]</b>
                                         HU G S, YAN F, CHAN C H, et al.Face Recognition Using a Unified 3D Morphable Model // Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:73-89.
                                    </a>
                                </li>
                                <li id="291">


                                    <a id="bibliography_18" title=" MA M Y, HU X Y, XU Y Q, et al.A Lighting Robust Fitting Approach of 3D Morphable Model Using Spherical Harmonic Illumination // Proc of the 22nd International Conference on Pattern Recognition.Washington, USA:IEEE, 2014:2101-2106." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Lighting Robust Fitting Approach of 3D Morphable Model Using Spherical Harmonic Illumination">
                                        <b>[18]</b>
                                         MA M Y, HU X Y, XU Y Q, et al.A Lighting Robust Fitting Approach of 3D Morphable Model Using Spherical Harmonic Illumination // Proc of the 22nd International Conference on Pattern Recognition.Washington, USA:IEEE, 2014:2101-2106.
                                    </a>
                                </li>
                                <li id="293">


                                    <a id="bibliography_19" title=" ZHU X Y, LEI Z, YAN J J, et al.High-Fidelity Pose and Expre-ssion Normalization for Face Recognition in the Wild // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:787-796." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-fidelity pose and expression normalization for face recognition in the wild">
                                        <b>[19]</b>
                                         ZHU X Y, LEI Z, YAN J J, et al.High-Fidelity Pose and Expre-ssion Normalization for Face Recognition in the Wild // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:787-796.
                                    </a>
                                </li>
                                <li id="295">


                                    <a id="bibliography_20" title=" P&#201;REZ P, GANGNET M, BLAKE A.Poisson Image Editing.ACM Transactions on Graphics, 2003, 22 (3) :313-318." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000097999&amp;v=MjQ3MDd3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZvVWJoUT1OaWZJWTdLN0h0ak5yNDlGWk9JSUJYVQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         P&#201;REZ P, GANGNET M, BLAKE A.Poisson Image Editing.ACM Transactions on Graphics, 2003, 22 (3) :313-318.
                                    </a>
                                </li>
                                <li id="297">


                                    <a id="bibliography_21" title=" ZHANG L, SAMARAS D.Face Recognition from a Single Training Image under Arbitrary Unknown Lighting Using Spherical Harmonics.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006, 28 (3) :351-363." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face recognition from a single training image under arbitrary unknown lighting using spherical harmonics">
                                        <b>[21]</b>
                                         ZHANG L, SAMARAS D.Face Recognition from a Single Training Image under Arbitrary Unknown Lighting Using Spherical Harmonics.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006, 28 (3) :351-363.
                                    </a>
                                </li>
                                <li id="299">


                                    <a id="bibliography_22" title=" ZHU X X, RAMANAN D.Face Detection, Pose Estimation, and Landmark Localization in the Wild // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2012:2879-2886." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face detection,pose estimation,and landmark localization in the wild">
                                        <b>[22]</b>
                                         ZHU X X, RAMANAN D.Face Detection, Pose Estimation, and Landmark Localization in the Wild // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2012:2879-2886.
                                    </a>
                                </li>
                                <li id="301">


                                    <a id="bibliography_23" title=" BELHUMEUR P N, JACOBS D W, KRIEGMAN D J, et al.Localizing Parts of Faces Using a Consensus of Exemplars.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (12) :2930-2940." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Localizing Parts of Faces Using a Consensus of Exemplars">
                                        <b>[23]</b>
                                         BELHUMEUR P N, JACOBS D W, KRIEGMAN D J, et al.Localizing Parts of Faces Using a Consensus of Exemplars.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (12) :2930-2940.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(06),557-568 DOI:10.16451/j.cnki.issn1003-6059.201906008            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于人脸标准化的纹理和光照保持</b><b>3</b><b>D人脸重构</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%98%B3%E7%91%9C&amp;code=42201398&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">阳瑜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E5%B0%8F%E4%BF%8A&amp;code=07780300&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴小俊</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E5%8D%97%E5%A4%A7%E5%AD%A6%E7%89%A9%E8%81%94%E7%BD%91%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0074200&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江南大学物联网工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>现有人脸纹理重建方法对于人脸的皱纹、胡须、瞳孔颜色等重建效果往往不够细致.为了解决此问题, 文中提出基于人脸标准化的纹理和光照保持3D人脸重构.首先对2D人脸图像标准化, 使用光照信息和对称纹理重构人脸自遮挡区域的纹理.然后依据2D-3D点对应关系从标准化的2D人脸图像获取相应的3D人脸纹理, 结合人脸形状重构和纹理信息, 得到最终的3D人脸重构结果.实验表明文中方法有效保留原始2D图像的纹理和光照信息, 重构的人脸更自然, 具有更丰富的人脸细节.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=3D%E5%BD%A2%E5%8F%98%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">3D形变模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=3D%E4%BA%BA%E8%84%B8%E9%87%8D%E6%9E%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">3D人脸重构;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E8%84%B8%E6%A0%87%E5%87%86%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人脸标准化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%89%E7%85%A7%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">光照模型;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    阳瑜, 硕士研究生, 主要研究方向为计算机视觉、三维人脸重建、深度学习.E-mail:yuyang899@foxmail.com.;
                                </span>
                                <span>
                                    *吴小俊 (通讯作者) , 博士, 教授, 主要研究方向为人工智能、模式识别、计算机视觉.E-mail:xiaojun_wu_jnu@163.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-06</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.61672265, U1836218) 资助;</span>
                    </p>
            </div>
                    <h1><b>Texture and Illumination Preserving 3D Face Reconstruction Based on Face Normalization</b></h1>
                    <h2>
                    <span>YANG Yu</span>
                    <span>WU Xiaojun</span>
            </h2>
                    <h2>
                    <span>School of Internet of Things Engineering, Jiangnan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The results of the existing texture recovery methods are not detailed enough for the face features such as wrinkles, beards and pupil colors. Therefore, the texture and illumination preserving 3 D face reconstruction based on face normalization is proposed. Firstly, the 2 D facial image is normalized to reconstruct the self-occlusion area using illumination information and symmetric texture. Then, the corresponding 3 D face texture is obtained by the normalized 2 D image according to the 2 D-3 D point pairs. Finally, combining the face shape reconstruction and texture information, the final 3 D face reconstruction results are generated. The experimental results show that the proposed method preserves the texture and the illumination information of the original 2 D image effectively, and the reconstructed faces are more natural with more facial details.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=3D%20Morphable%20Model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">3D Morphable Model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=3D%20Face%20Reconstruction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">3D Face Reconstruction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Face%20Normalization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Face Normalization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Illumination%20Model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Illumination Model;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YANG Yu, master student. His research interests include computer vision, 3D face reconstruction and deep learning.;
                                </span>
                                <span>
                                    WU Xiaojun (Corresponding author) , Ph.D., professor. His research interests include artificial intelligence, pattern recognition and computer vision.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-06</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.61672265, U1836218);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="49">相比2D人脸, 3D人脸包含更多的生物信息, 具有对姿态、光照等生物特征的不变性, 因此使用3D信息进行人脸识别具有更好的鉴别性.Blanz等<citation id="303" type="reference"><link href="257" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出三维形变模型 (3D Morphable Face Model, 3DMM) , 这是基于先验知识的三维人脸模型, 可以从单幅2D人脸图像重构对应的3D人脸.3DMM将人脸形状和人脸纹理投影到PCA空间, 使用人脸形状参数和纹理参数的变化表达模型的形变.人脸重构的过程可看作是最小化3D模型和2D人脸图像之间差异的优化问题.在此基础上.梁荣华等<citation id="304" type="reference"><link href="259" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出根据人头部运动的多张照片恢复三维人脸的方法.随着算法的发展和数据采集精度的提升, 3DMM增加估计人脸表情信息的功能<citation id="305" type="reference"><link href="261" rel="bibliography" /><link href="263" rel="bibliography" /><link href="265" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>.由于采集样本精度的提高, 3D模型的样本数量和分辨率也大幅提高.</p>
                </div>
                <div class="p1">
                    <p id="50">对于人脸形状重建, 3D人脸形状重构算法参数众多, 整个3D模型上的点都参与优化, 计算过程非常耗时.为了缩短计算时间, Blanz等<sup></sup><citation id="306" type="reference"><link href="267" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出只在人脸上选取一组稀疏的特征点 (Landmarks) 优化求解3D形状参数, 可提高3D重构的计算效率.Romdhani等<citation id="307" type="reference"><link href="269" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出多特征框架 (Multi-features Framework, MFF) 解决优化过程中的局部最小化问题, 在稀疏人脸特征点的基础上, 在目标函数中增加人脸边缘特征 (Edge Feature) 、镜面光强 (Specular Highlights) 及像素强度 (Pixel Intensity) 等特征.近年来, 使用局部特征和级联回归函数预测3D参数的方法可进一步提高3D人脸重构的速度和精确度<citation id="311" type="reference"><link href="271" rel="bibliography" /><link href="273" rel="bibliography" /><link href="275" rel="bibliography" /><link href="277" rel="bibliography" /><link href="279" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>.随着深度学习的发展, 深度神经网络也应用到3D人脸重构中, Dou等<citation id="308" type="reference"><link href="281" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和Jourabloo等<citation id="309" type="reference"><link href="283" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>使用神经网络估计3D参数.Feng等<citation id="310" type="reference"><link href="285" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>使用神经网络直接估计3D人脸点坐标.</p>
                </div>
                <div class="p1">
                    <p id="51">对于人脸纹理颜色重构, 基于立体的方法<citation id="312" type="reference"><link href="287" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>需同时采集一个人在同一地点、同一时间的不同角度的多张照片.Hu等<citation id="313" type="reference"><link href="289" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>基于Phong光照模型估计2D图像的光照情况, 重构3D人脸模型的光照参数.Ma等<citation id="314" type="reference"><link href="291" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出球谐光照模型 (Sphere Harmonic Illumina-tion Model, SHIM) .</p>
                </div>
                <div class="p1">
                    <p id="52">3D人脸重构得到的人脸纹理不够细致, 如皱纹、眉毛颜色、脸部伤痕、胡须等特征缺少或不自然.本文提出基于人脸标准化的3D人脸重构算法.将自然状态下的2D人脸标准化, 得到正面无遮挡的2D人脸图像, 用于获取完整的人脸纹理.Zhu等<citation id="315" type="reference"><link href="293" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出高分辨率人脸图像的姿态和表情标准化算法, 在该算法的基础上本文改进自遮挡区域重建效果, 得到更自然的标准化图像.标准化过程基于3DMM, 所以, 同时得到相应的3D形状、表情、姿态等信息, 通过这些参数可重构3D人脸的形状.根据2D-3D匹配点对信息从标准化的2D人脸图像中获取相应3D点的肤色纹理信息.本文方法直接从2D原图像获取人脸纹理, 可完整保留最终得到的3D重构人脸的光照信息及人脸细节信息.实验表明, 本文方法可有效重构完整的3D人脸形状和纹理信息, 相比已有方法, 能得到更丰富的人脸纹理细节.</p>
                </div>
                <h3 id="53" name="53" class="anchor-tag">1 3D人脸重构</h3>
                <div class="p1">
                    <p id="54">3DMM由两个向量组成, 分别为:形状向量</p>
                </div>
                <div class="p1">
                    <p id="55"><b><i>S</i></b>′= (<i>X</i><sub>1</sub>, <i>Y</i><sub>1</sub>, <i>Z</i><sub>1</sub>, <i>X</i><sub>2</sub>, <i>Y</i><sub>2</sub>, <i>Z</i><sub>2</sub>, …, <i>X</i><sub><i>n</i></sub>, <i>Y</i><sub><i>n</i></sub>, <i>Z</i><sub><i>n</i></sub>) <sup>T</sup>∈<b>R</b><sup>3<i>n</i></sup>, </p>
                </div>
                <div class="p1">
                    <p id="56">表示笛卡尔坐标系中3D点的坐标;纹理向量</p>
                </div>
                <div class="p1">
                    <p id="57"><b><i>T</i></b>′ = (<i>R</i><sub>1</sub>, <i>G</i><sub>1</sub>, <i>B</i><sub>1</sub>, <i>R</i><sub>2</sub>, <i>G</i><sub>2</sub>, <i>B</i><sub>2</sub>, …, <i>R</i><sub><i>n</i></sub>, <i>G</i><sub><i>n</i></sub>, <i>B</i><sub><i>n</i></sub>) <sup>T</sup>∈<b>R</b><sup>3<i>n</i></sup>, </p>
                </div>
                <div class="p1">
                    <p id="58">表示3D模型对应点的<i>R</i>、<i>G</i>、<i>B</i>值.将多个训练样本集的形状向量和纹理向量投影到PCA空间, 建立形状模型<i>S</i><sub>model</sub> 和纹理模型<i>T</i><sub>model</sub>:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo>=</mo><mover accent="true"><mi mathvariant="bold-italic">S</mi><mo>¯</mo></mover><mspace width="0.25em" /><mo>+</mo><mspace width="0.25em" /><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo>=</mo><mover accent="true"><mi mathvariant="bold-italic">Τ</mi><mo>¯</mo></mover><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">其中, <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">S</mi><mo>¯</mo></mover></math></mathml>、<mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Τ</mi><mo>¯</mo></mover></math></mathml>分别表示3D模型的形状和纹理的平均值, <i>m</i>表示对应形状和纹理的特征向量维数, <b><i>s</i></b><sub><i>i</i></sub>表示对应的形状特征向量, <b><i>t</i></b><sub><i>i</i></sub>表示对应的纹理特征向量, </p>
                </div>
                <div class="p1">
                    <p id="63"><i>α</i>= (<i>α</i><sub>1</sub>, <i>α</i><sub>2</sub>, …, <i>α</i><sub><i>m</i></sub>) </p>
                </div>
                <div class="p1">
                    <p id="64">表示形状参数, </p>
                </div>
                <div class="p1">
                    <p id="65"><i>β</i>= (<i>β</i><sub>1</sub>, <i>β</i><sub>2</sub>, …, <i>β</i><sub><i>m</i></sub>) </p>
                </div>
                <div class="p1">
                    <p id="66">表示纹理参数.</p>
                </div>
                <div class="p1">
                    <p id="67">考虑到人脸表情变化, 表情参数也可添加到形状模型中, 带有表情的人脸可看作是无表情模型和表情变化参数的线性组合:</p>
                </div>
                <div class="p1">
                    <p id="68"><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo>=</mo><mover accent="true"><mi mathvariant="bold-italic">S</mi><mo>¯</mo></mover><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>i</mi><mi>d</mi></mrow></msub><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>i</mi><mi>d</mi></mrow></msub><mspace width="0.25em" /><mo>+</mo><mspace width="0.25em" /><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mo>, </mo><mi>e</mi><mi>x</mi><mi>p</mi></mrow></msub><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mrow><mi>j</mi><mo>, </mo><mi>e</mi><mi>x</mi><mi>p</mi></mrow></msub></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="70">其中, <b><i>s</i></b><sub><i>i</i>, <i>id</i></sub> 表示中性表情的第<i>i</i>个特征向量, </p>
                </div>
                <div class="p1">
                    <p id="71"><i>α</i><sub><i>id</i></sub>= (<i>α</i><sub>1, <i>id</i></sub>, <i>α</i><sub>2, <i>id</i></sub>, …, <i>α</i><sub><i>m</i>, <i>id</i></sub>) </p>
                </div>
                <div class="p1">
                    <p id="72">表示形状参数, <b><i>s</i></b><sub><i>j</i>, <i>exp</i></sub>表示第<i>j</i>个表情特征向量, </p>
                </div>
                <div class="p1">
                    <p id="73"><i>α</i><sub><i>exp</i></sub>= (<i>α</i><sub>1, <i>exp</i></sub>, <i>α</i><sub>2, <i>exp</i></sub>, …, <i>α</i><sub><i>k</i>, <i>exp</i></sub>) </p>
                </div>
                <div class="p1">
                    <p id="74">表示表情参数向量.</p>
                </div>
                <div class="p1">
                    <p id="75">形状拟合过程需先将3D模型投影到2D平面, 这里使用弱透视投影:</p>
                </div>
                <div class="area_img" id="76">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201906009_07600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="78">其中, <b><i>S</i></b><sub>2d</sub>表示3D模型点向量投影到2D平面的向量, <i>f</i>表示相机焦距, <b><i>R</i></b>∈<b>R</b><sup>3×3</sup>表示3D模型的旋转矩阵, 可使模型在<i>x</i>、<i>y</i>、<i>z</i>方向旋转, <b><i>t</i></b><sub>3d</sub>表示模型的平移向量.拟合的目的是使投影得到的2D形状向量<b><i>S</i></b><sub>2d</sub>和给定的2D图像对应点位置<b><i>S</i></b><sub>2d, <i>t</i></sub>误差最小以方便进行拟合:</p>
                </div>
                <div class="p1">
                    <p id="79"><mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>f</mi><mo>, </mo><mi mathvariant="bold-italic">R</mi><mo>, </mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>d</mi></mrow></msub><mo>, </mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mrow><mi>e</mi><mi>x</mi><mi>p</mi></mrow></msub><mo>, </mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mrow><mn>3</mn><mtext>d</mtext></mrow></msub></mrow></munder><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mrow><mn>2</mn><mtext>d</mtext><mo>, </mo><mi>t</mi></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mrow><mn>2</mn><mtext>d</mtext></mrow></msub></mrow><mo>|</mo></mrow></mrow></math></mathml>.      (2) </p>
                </div>
                <div class="p1">
                    <p id="81">由于计算整个3D模型上所有的点太耗时, 为了使优化过程快速收敛, 在式 (2) 的基础上, 求解3D参数只在事先选取的特征点上进行计算.优化过程需要预先建立2D-3D特征点的对应关系, 但是自然条件的2D人脸图像特征点定位会将人脸轮廓的特征点定位到视觉可见的人脸边缘位置, 导致2D-3D点对匹配产生误差.为了解决这个问题, Zhu等<citation id="316" type="reference"><link href="293" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出姿态自适应的3DMM拟合算法 (Pose Adaptive 3DMM Fitting) , 将3D模型上的相应特征点根据人脸姿态平移到和2D特征点对应的视觉可见边缘, 重新建立2D-3D特征点匹配对.随着最近2D特征点定位精度提高, 式 (2) 优化得到的3D参数也足够精确.改进后形状模型优化目标如下:</p>
                </div>
                <div class="p1">
                    <p id="82"><mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>f</mi><mo>, </mo><mi mathvariant="bold-italic">R</mi><mo>, </mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>d</mi></mrow></msub><mo>, </mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mrow><mi>e</mi><mi>x</mi><mi>p</mi></mrow></msub><mo>, </mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mrow><mn>3</mn><mtext>d</mtext></mrow></msub></mrow></munder><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mrow><mn>2</mn><mtext>d</mtext><mo>, </mo><mi>t</mi></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mrow><mn>2</mn><mtext>d</mtext></mrow></msub></mrow><mo>|</mo></mrow></mrow><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>a</mtext><mtext>n</mtext><mtext>d</mtext><mtext>m</mtext><mtext>a</mtext><mtext>r</mtext><mtext>k</mtext><mtext>s</mtext></mrow></msub></mrow></math></mathml>.      (3) </p>
                </div>
                <div class="p1">
                    <p id="84">对于人脸纹理模型, 3DMM假设自然条件下的3D人脸符合Phong光照模型特性.3DMM将整个模型分解成若干个三角块, 假设第<i>k</i>个三角块的中心坐标为<mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub><mo>, </mo><mover accent="true"><mi>Y</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub><mo>, </mo><mover accent="true"><mi>Ζ</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></math></mathml>, 纹理为<mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>R</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub><mo>, </mo><mover accent="true"><mi>G</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub><mo>, </mo><mover accent="true"><mi>B</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></math></mathml>, 值为所在最小三角块的平均值.该点通过弱透视投影到2D平面相应位置<mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>p</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>k</mi></mrow></msub><mo>, </mo><mover accent="true"><mi>p</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>y</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></math></mathml>.根据Phong光照模型, 定义第<i>k</i>个三角块的纹理颜色<i>I</i><sub><i>r</i>, model, <i>k</i></sub>, <i>I</i><sub><i>g</i>, model, <i>k</i></sub>和<i>I</i><sub><i>b</i>, model, <i>k</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="88"><mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><msub><mrow></mrow><mrow><mi>r</mi><mo>, </mo><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>l</mtext><mo>, </mo><mi>k</mi></mrow></msub><mo>=</mo><mo stretchy="false"> (</mo><mi>i</mi><msub><mrow></mrow><mrow><mi>r</mi><mo>, </mo><mtext>a</mtext><mtext>m</mtext><mtext>b</mtext></mrow></msub><mo>+</mo><mi>i</mi><msub><mrow></mrow><mrow><mi>r</mi><mo>, </mo><mtext>d</mtext><mtext>i</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">n</mi><msub><mrow></mrow><mi>k</mi></msub><mi mathvariant="bold-italic">l</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mover accent="true"><mi>R</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mi>i</mi><msub><mrow></mrow><mrow><mi>r</mi><mo>, </mo><mtext>d</mtext><mtext>i</mtext><mtext>r</mtext></mrow></msub><mi>s</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mi>k</mi></msub><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mi>v</mi></msup><mo>, </mo></mrow></math></mathml>      (4) </p>
                </div>
                <div class="p1">
                    <p id="90">其中, <i>i</i><sub><i>r</i>, amb</sub>、<i>i</i><sub><i>r</i>, dir</sub>分别表示环境散射光和直射光中红色分量的强度, <b><i>n</i></b><sub><i>k</i></sub>表示第<i>k</i>个三角块的法向量, <b><i>l</i></b>表示光照入射方向, <b><i>v</i></b><sub><i>k</i></sub>表示相机视角, <b><i>r</i></b><sub><i>k</i></sub>表示反射方向, <i>s</i>表示表面反光强度, <i>v</i>表示镜面反射的角分布.<i>I</i><sub><i>g</i>, model, <i>k</i></sub>和<i>I</i><sub><i>b</i>, model, <i>k</i></sub>的表达式类似.</p>
                </div>
                <div class="p1">
                    <p id="91">由此, 对于包含<i>K</i>个三角块的3D模型的纹理目标函数可表示为</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>∈</mo><mi>Κ</mi></mrow></munder><mrow><mrow><mo>|</mo><mrow><mi>Ι</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>n</mtext><mtext>p</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false"> (</mo><mover accent="true"><mi>p</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>k</mi></mrow></msub><mo>, </mo><mover accent="true"><mi>p</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>y</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false">) </mo><mo>-</mo><mi>Ι</mi><msub><mrow></mrow><mrow><mo>, </mo><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>l</mtext><mo>, </mo><mi>k</mi></mrow></msub></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <h3 id="93" name="93" class="anchor-tag">2 基于人脸标准化的3D人脸重构</h3>
                <div class="p1">
                    <p id="94">3D模型的光照模型存在参数较多、模型复杂、优化计算过程耗时等问题, 通过光照参数估计得到3D整体纹理色彩, 并未考虑人脸局部细节特征.因此, 本文提出基于人脸标准化3D人脸重构算法, 直接从2D图像获取人脸纹理信息, 有效提高3D模型纹理重构速度, 最大程度保留原图的光照和细节信息.</p>
                </div>
                <div class="p1">
                    <p id="95">本文算法的三维人脸重建过程如图1所示.</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906009_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文算法流程图" src="Detail/GetImg?filename=images/MSSB201906009_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906009_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Flow chart of the proposed method</p>

                </div>
                <div class="p1">
                    <p id="97">本文算法具体步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="98">1) 使用3DMM和球谐光照模型 (Spherical Har-monics Lighting) 估计输入人脸图像的三维人脸形状和光照条件.</p>
                </div>
                <div class="p1">
                    <p id="99">2) 使用Delaunay三角剖分对原图像进行三角划分 (人脸、人脸周围、背景区域) , 根据1) 得到的三维姿态参数, 对输入图像进行人脸姿态标准化, 得到正面的人脸图像.在自然姿态情况下, 人脸存在自遮挡区域, 旋转到正面的人脸在自遮挡的地方会产生局部纹理缺失.若要得到完整的标准化人脸, 需对残缺的自遮挡区域进行恢复重建.</p>
                </div>
                <div class="p1">
                    <p id="100">3) 对原图像镜像操作获取自遮挡区域的对称纹理.</p>
                </div>
                <div class="p1">
                    <p id="101">4) 在重建模型上获取相应自遮挡区域, 对人脸图像进行粗填充.</p>
                </div>
                <div class="p1">
                    <p id="102">5) 使用泊松图片编辑 (Poisson Image Edi-ting) <citation id="317" type="reference"><link href="295" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>将镜像区域的细节纹理融合到粗填充自遮挡区域, 得到标准化的人脸图像.</p>
                </div>
                <div class="p1">
                    <p id="103">6) 将标准化的人脸图像纹理渲染到重建的形状模型上, 得到最终的三维人脸重建结果.</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104"><b>2.1</b> 基于<b>3D</b>形变模型的<b>2D</b>人脸正则化</h4>
                <div class="p1">
                    <p id="105">为了重建更精确、自然的三维人脸纹理, 在重建三维人脸形状后对2D人脸图像进行标准化处理, 得到正面无遮挡的人脸图像.然后在标准化的2D人脸图像中获取完整的3D纹理信息.本文在文献<citation id="318" type="reference">[<a class="sup">19</a>]</citation>基础上进行优化改进.</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106"><b>2.1.1</b> 图像<b>3D</b>分片及旋转</h4>
                <div class="p1">
                    <p id="107">为了保证最大限度保留原图像的信息, 将2D图像分为3个区域:人脸区域、人脸周围区域、背景区域, 使用狄洛尼三角剖分将2D人脸图像划分为若干三角块, 如图2所示.人脸标准化不仅对人脸区域进行转换, 而且对整个图像进行转换.使用式 (3) 估计原始2D图像的人脸3D信息<i>f</i>, <b><i>R</i></b>, <i>α</i><sub>id</sub>, <i>α</i><sub><i>exp</i></sub>, <i>t</i><sub>3d</sub>.显而易见, 对比自然状态有姿势变化的人脸区域, 人脸周围区域和背景区域所需的标准化变化依次减小.根据区域位置以人脸鼻尖为参考点向三角区域添加深度信息, 然后根据人脸旋转矩阵<b><i>R</i></b>将3D深度图像逆旋转得到正面的人脸图像:</p>
                </div>
                <div class="p1">
                    <p id="108"><b><i>I</i></b><sub>img_rn</sub>=<b><i>R</i></b><sup>-1</sup><b><i>I</i></b><sub>img</sub>.      (5) </p>
                </div>
                <div class="area_img" id="249">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906009_24900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 人脸图像分片" src="Detail/GetImg?filename=images/MSSB201906009_24900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 人脸图像分片  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906009_24900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Facial image meshing</p>

                </div>
                <h4 class="anchor-tag" id="113" name="113"><b>2.1.2</b> 自遮挡区域填充</h4>
                <div class="p1">
                    <p id="114">自然条件下的人脸因人脸姿态存在自遮挡问题, 2D人脸图像旋转至正面后, 缺失原始图像中自遮挡区域.为了解决此问题, 本文改进自遮挡区域填充算法, 对不可见的残缺区域填充更自然平滑.自然环境中的人脸纹理可看作均匀光照条件下的人脸肤色添加特定光照的结果.如图3所示, 均匀光照环境下的人脸图像 (a) 在 (b) 的光照条件下视觉效果为 (c) , 在 (c) 人脸上添加一个遮挡区域得到 (d) , 使用对应的光照模型 (b) 粗填充得到结果 (e) , 使遮挡区域包含真实光照信息, 然后在粗填充的基础上使用泊松图像编辑在遮挡区域添加细节纹理, 得到最终的遮挡区域重建结果 (g) .由此可见, 整个过程是借助光照信息和真实的纹理细节, 最终得到自然的遮挡区域重建结果.</p>
                </div>
                <div class="area_img" id="250">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906009_25000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 使用泊松图片编辑填充不可见区域" src="Detail/GetImg?filename=images/MSSB201906009_25000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 使用泊松图片编辑填充不可见区域  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906009_25000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Recovering invisible area by Poisson image editing</p>

                </div>
                <div class="p1">
                    <p id="125">由于人脸具有左右近似对称的内在生物特性, 可从对称的人脸区域获取相应纹理信息, 但是自然环境下的人脸具有特定的光照条件.由图3可看出, 通常情况下人脸左右的光照情况并不相同.简单的使用人脸对称信息填充不可见区域会因为光照条件造成填充区域和相邻区域衔接不自然, 结果很差, 如图3 (f) 所示.由此首先需要估计不可见区域的光照情况, 再根据人脸的对称性将人脸对称位置的细节信息添加到该区域.这里使用球谐光照模型<citation id="319" type="reference"><link href="297" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>估计不可见区域的光照情况:</p>
                </div>
                <div class="p1">
                    <p id="126" class="code-formula">
                        <mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow></msub><mo>=</mo><mi>ρ</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>9</mn></munderover><mi>h</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mspace width="0.25em" /><mover accent="true"><mi>n</mi><mo>→</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow></msub><mo stretchy="false">) </mo><mi>γ</mi><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="127">其中, <i>I</i><sub><i>x</i>, <i>y</i></sub>表示图像中 (<i>x</i>, <i>y</i>) 处的像素, <i>ρ</i><sub><i>x</i>, <i>y</i></sub>表示人脸表面肤色纹理, <i>γ</i>表示光照参数, <i>h</i><sub><i>i</i></sub>表示第<i>i</i>个球谐基底:</p>
                </div>
                <div class="p1">
                    <p id="128" class="code-formula">
                        <mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>h</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mfrac><mn>1</mn><mrow><msqrt><mrow><mn>4</mn><mtext>π</mtext></mrow></msqrt></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><mi>h</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mi>n</mi><msub><mrow></mrow><mi>x</mi></msub><msqrt><mrow><mfrac><mn>3</mn><mrow><mn>4</mn><mtext>π</mtext></mrow></mfrac></mrow></msqrt><mo>, </mo><mspace width="0.25em" /><mi>h</mi><msub><mrow></mrow><mn>3</mn></msub><mo>=</mo><mi>n</mi><msub><mrow></mrow><mi>y</mi></msub><msqrt><mrow><mfrac><mn>3</mn><mrow><mn>4</mn><mtext>π</mtext></mrow></mfrac></mrow></msqrt><mo>, </mo></mtd></mtr><mtr><mtd><mi>h</mi><msub><mrow></mrow><mn>4</mn></msub><mo>=</mo><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub><msqrt><mrow><mfrac><mn>3</mn><mrow><mn>4</mn><mtext>π</mtext></mrow></mfrac></mrow></msqrt><mo>, </mo><mspace width="0.25em" /><mi>h</mi><msub><mrow></mrow><mn>5</mn></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msqrt><mrow><mfrac><mn>5</mn><mrow><mn>4</mn><mtext>π</mtext></mrow></mfrac></mrow></msqrt><mo stretchy="false"> (</mo><mn>2</mn><mi>n</mi><msubsup><mrow></mrow><mi>z</mi><mn>2</mn></msubsup><mo>-</mo><mi>n</mi><msubsup><mrow></mrow><mi>x</mi><mn>2</mn></msubsup><mo>-</mo><mi>n</mi><msubsup><mrow></mrow><mi>y</mi><mn>2</mn></msubsup><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mi>h</mi><msub><mrow></mrow><mn>6</mn></msub><mo>=</mo><mn>3</mn><msqrt><mrow><mfrac><mn>5</mn><mrow><mn>1</mn><mn>2</mn><mtext>π</mtext></mrow></mfrac></mrow></msqrt><mi>n</mi><msub><mrow></mrow><mi>y</mi></msub><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>h</mi><msub><mrow></mrow><mn>7</mn></msub><mo>=</mo><mn>3</mn><msqrt><mrow><mfrac><mn>5</mn><mrow><mn>1</mn><mn>2</mn><mtext>π</mtext></mrow></mfrac></mrow></msqrt><mi>n</mi><msub><mrow></mrow><mi>x</mi></msub><mi>n</mi><msub><mrow></mrow><mi>z</mi></msub><mo>, </mo></mtd></mtr><mtr><mtd><mi>h</mi><msub><mrow></mrow><mn>8</mn></msub><mo>=</mo><mn>3</mn><msqrt><mrow><mfrac><mn>5</mn><mrow><mn>1</mn><mn>2</mn><mtext>π</mtext></mrow></mfrac></mrow></msqrt><mi>n</mi><msub><mrow></mrow><mi>x</mi></msub><mi>n</mi><msub><mrow></mrow><mi>y</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>h</mi><msub><mrow></mrow><mn>9</mn></msub><mo>=</mo><mfrac><mn>3</mn><mn>2</mn></mfrac><msqrt><mrow><mfrac><mn>5</mn><mrow><mn>1</mn><mn>2</mn><mtext>π</mtext></mrow></mfrac></mrow></msqrt><mo stretchy="false"> (</mo><mi>n</mi><msubsup><mrow></mrow><mi>x</mi><mn>2</mn></msubsup><mo>-</mo><mi>n</mi><msubsup><mrow></mrow><mi>y</mi><mn>2</mn></msubsup><mo stretchy="false">) </mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="129">在计算过程中, 将<i>ρ</i>设置为3DMM模型的平均肤色值, 即<mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">ρ</mi><mo>=</mo><mover accent="true"><mi mathvariant="bold-italic">Τ</mi><mo>¯</mo></mover></mrow></math></mathml>.表面法向量<b><i>n</i></b>通过人脸形状重构得到的3D点坐标计算.估计3D人脸的光照情况:</p>
                </div>
                <div class="p1">
                    <p id="131"><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">γ</mi><mo>=</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">γ</mi></munder><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ι</mi><mo>-</mo><mi mathvariant="bold-italic">Η</mi><mi mathvariant="bold-italic">γ</mi></mrow><mo>|</mo></mrow></mrow></math></mathml>,      (6) </p>
                </div>
                <div class="p1">
                    <p id="133">其中, <b><i>I</i></b>为输入的2D人脸图像, <b><i>H</i></b>= (<i>ρh</i><sub>1</sub>, <i>ρh</i><sub>2</sub>, …, <i>ρh</i><sub>9</sub>) , <b><i>H</i></b>表示在光照参数<i>γ</i>下的人脸纹理信息.3D人脸光照估计结果如图4所示.</p>
                </div>
                <div class="area_img" id="251">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906009_25100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 人脸形状和光照估计" src="Detail/GetImg?filename=images/MSSB201906009_25100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 人脸形状和光照估计  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906009_25100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Facial shape and illumination estimation</p>

                </div>
                <div class="p1">
                    <p id="138">使用3DMM和光照模型重建带有光照信息的三维人脸, 得到的三维人脸纹理来自三维模型的平均人脸纹理, 缺少目标人脸的真实细节信息, 如图4 (d) 所示.根据图3, 先使用带有光照信息的人脸形状重建结果粗填充自遮挡区域, 根据人脸的对称性, 选取人脸自遮挡区域对称位置的可见区域, 使用泊松图像编辑将细节纹理融入其中, 得到最终的人脸标准化结果.</p>
                </div>
                <div class="p1">
                    <p id="139">图5展示部分人脸标准化结果.从图中可看出, 相比文献<citation id="320" type="reference">[<a class="sup">19</a>]</citation>方法, 本文算法对自遮挡区域的重建效果具有明显提升作用.</p>
                </div>
                <div class="area_img" id="252">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906009_25200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 2种算法得到的人脸标准化结果对比" src="Detail/GetImg?filename=images/MSSB201906009_25200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 2种算法得到的人脸标准化结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906009_25200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Comparison of face normalization results by 2 algorithms</p>

                </div>
                <div class="area_img" id="252">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906009_25201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 2种算法得到的人脸标准化结果对比" src="Detail/GetImg?filename=images/MSSB201906009_25201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 2种算法得到的人脸标准化结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906009_25201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Comparison of face normalization results by 2 algorithms</p>

                </div>
                <h4 class="anchor-tag" id="147" name="147"><b>2.2 3D</b>人脸的肤色纹理重构</h4>
                <div class="p1">
                    <p id="148">根据人脸标准化过程中得到的3D形状参数及姿态等3D参数, 建立标准化人脸图像和3D模型的2D-3D点对应.依据点对应关系, 使用式 (1) 的弱透视投影将三维人脸点坐标投影到2D标准人脸图像, 获取相应的2D人脸纹理信息, 将对应的纹理信息渲染到三维形状模型上, 最终得到完整的3D重构结果.如图6所示, (a) 为带有特征点的彩色人脸图像, 使用式 (6) 重建三维人脸的形状, 得到对应的三维人脸点云 (见 (b) ) .人脸形状重建的原理是最小化投影到2D平面的三维模型特征点和输入图像上的特征点之间的误差, 进而求得三维形状和姿态参数, 根据这些三维参数得到一个确定的三维模型, 即对应于输入图像的重建结果.三维人脸模型上的特征点如 (d) 所示.根据形状估计过程得到的人脸姿态参数, 将三维人脸点云旋转到正面, 使用式 (6) 对人脸图像进行姿态的标准化处理.在直角坐标系中同时画出正则化后的人脸图像和三维人脸点云, 重建人脸上三维点坐标 (<i>x</i>, <i>y</i>, <i>z</i>) 对应标准化人脸图像中相同语义位置的坐标为 (<i>x</i>, <i>y</i>) . (f) 为它们在同一坐标系中的2D-3D对应关系.根据三维点云和标准化人脸图像在坐标系中的对应关系,  在标准化人脸图像中直接选取对应像素重建三维人脸的纹理.</p>
                </div>
                <div class="area_img" id="253">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906009_25300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 2D-3D点对应关系" src="Detail/GetImg?filename=images/MSSB201906009_25300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 2D-3D点对应关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906009_25300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Corresponding relationship of 2D-3D vertex</p>

                </div>
                <div class="area_img" id="253">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906009_25301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 2D-3D点对应关系" src="Detail/GetImg?filename=images/MSSB201906009_25301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 2D-3D点对应关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906009_25301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Corresponding relationship of 2D-3D vertex</p>

                </div>
                <div class="p1">
                    <p id="160">本文提出的人脸重建算法具体步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="161"><b>算法</b> 基于人脸标准化的纹理和光照保持3D人脸重构</p>
                </div>
                <div class="p1">
                    <p id="163"><b>输入</b> 人脸图像, 特征点坐标 (<i>S</i><sub>2d, <i>t</i></sub>) </p>
                </div>
                <div class="p1">
                    <p id="164"><b>输出</b> 3D人脸重建结果</p>
                </div>
                <div class="p1">
                    <p id="165">step 1 使用式 (3) 估计三维参数<i>α</i><sub>id</sub>, <i>α</i><sub><i>exp</i></sub>, <i>f</i>, <b><i>R</i></b>, <i>t</i><sub>3d</sub>, 得到重建三维人脸形状和姿态信息;</p>
                </div>
                <div class="p1">
                    <p id="166">step 2 使用式 (5) 估计光照参数<i>γ</i>, 得到三维人脸光照信息;</p>
                </div>
                <div class="p1">
                    <p id="167">step 3 使用Delaunay对输入图像进行分片, 增加深度信息;</p>
                </div>
                <div class="p1">
                    <p id="168">step 4 使用姿态参数<b><i>R</i></b>对分片的人脸图像进行姿态标准化;</p>
                </div>
                <div class="p1">
                    <p id="169">step 5 将带有光照信息的三维人脸投影到2D平面, 获取自遮挡对应区域;</p>
                </div>
                <div class="p1">
                    <p id="170">step 6 将step 5中获取的自遮挡区域填充到姿态标准化的人脸图像中, 得到粗填充结果;</p>
                </div>
                <div class="p1">
                    <p id="171">step 7 对输入图像的人脸执行镜像操作, 获取自遮挡区域的对称纹理;</p>
                </div>
                <div class="p1">
                    <p id="172">step 8 使用泊松图像编辑将对称区域的纹理细节融合到粗填充的自遮挡区域;</p>
                </div>
                <div class="p1">
                    <p id="173">step 9 使用step 8中得到的完整标准化人脸纹理渲染重建的人脸形状, 得到最终的三维人脸重建结果.</p>
                </div>
                <h3 id="174" name="174" class="anchor-tag">3 实验及结果分析</h3>
                <div class="p1">
                    <p id="175">为了验证本文算法的有效性, 在公开的数据集AFW<citation id="321" type="reference"><link href="299" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、LFPW<citation id="322" type="reference"><link href="301" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>上进行测试.这2个人脸数据库包含带有landmarks标签的自然条件下不同姿态的人脸图像.</p>
                </div>
                <div class="p1">
                    <p id="176">AFW人脸数据集包含205幅人脸图像, 每幅图像手工标记有6个landmarks及其可见性.</p>
                </div>
                <div class="p1">
                    <p id="177">LFPW人脸数据集包含1 432幅来源网络的人脸图像, 每张照片手工标定29个特征点.本文实验随机选取若干幅人脸图像, 每幅人脸图像增加landmarks至68个.</p>
                </div>
                <div class="p1">
                    <p id="178">本文使用的3DMM为Basel Face Model, 另外, 在模型中添加人脸表情模型<citation id="323" type="reference"><link href="261" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>.该模型除包含中立表情之外, 还包含150人的19个人脸表情训练样本.</p>
                </div>
                <div class="p1">
                    <p id="179">对比实验分为两部分.1) 验证人脸标准化对人脸纹理重建的必要性和有效性, 在其它条件相同时, 使用原始人脸图像和人脸标准化之后的图像分别提取纹理信息, 验证人脸标准化的必要性.实验中使用Basel Face Model模型重构3D人脸形状, 根据2D-3D点对应关系在原始图像及标准化之后的图像中获取相应的3D纹理信息, 将获取的3D纹理替代3DMM的平均纹理.</p>
                </div>
                <div class="p1">
                    <p id="180">在使用相同的形状重建方法的基础上, 对比光照模型和本文算法, 验证本文算法在人脸纹理细节和精确度上的提升和有效性.实验中使用球谐光照模型重建人脸的纹理信息, 光照模型通过人脸表面法向量<b><i>n</i></b>、肤色参数<i>ρ</i>估计环境光照, 根据光照信息调节人脸纹理.实验中人脸表面法向量及肤色参数初始化为3D平均模型的表面法向量和平均肤色参数.</p>
                </div>
                <div class="p1">
                    <p id="181">2) 对比本文算法、3D稠密人脸对齐 (3D Dense Face Alignment, 3DDFA) <citation id="324" type="reference"><link href="279" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>和位置映射回归网络 (Position Map Regression Network, PRNet) <citation id="325" type="reference"><link href="281" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>.3DD-FA在3DMM的基础上使用卷积神经网络估计三维人脸形状参数和姿态等信息, 根据这些参数重建对应的三维人脸.PRNet为端到端的深度神经网络, 直接从2D图像估计三维人脸的点坐标, 重建对应的三维人脸.在实验中, 在相同的设备环境下使用预先训练好的3DDFA和PRNet的卷积神经网络重建三维人脸, 对比本文算法的重建结果.</p>
                </div>
                <div class="p1">
                    <p id="182">图7为使用原图像和人脸标准化的图像进行3D人脸纹理重构得到的结果, 图中将重建的三维人脸旋转到自遮挡区域的可视角度. (b) 为直接使用原图像根据2D-3D点对应重构得到的3D纹理, 由于人脸自遮挡的影响, 在2D人脸不可见的区域无法有效获取正确的3D纹理信息, 导致重构3D人脸对应区域纹理失真. (c) 为经过2D人脸标准化之后获取的3D人脸纹理的重建结果, 人脸标准化过程恢复自遮挡区域的实际环境光照及相应的细节纹理, 使用标准化的人脸图像获取三维纹理得到的自遮挡区域和周围纹理衔接得自然、平滑.相比直接使用原始图像, 本文算法得到的3D纹理重构结果更自然, 由此验证人脸标准化的必要性.</p>
                </div>
                <div class="area_img" id="254">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906009_25400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 使用原图像和标准化人脸图像的纹理渲染对比" src="Detail/GetImg?filename=images/MSSB201906009_25400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 使用原图像和标准化人脸图像的纹理渲染对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906009_25400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Texture rendering comparison using original images and normalized facial images</p>

                </div>
                <div class="p1">
                    <p id="191">由图7可见, 人脸标准化处理之后的3D重构人脸在耳朵附近区域的纹理失真, 这是由于人脸标准化后, 原来自然姿态下的人脸旋转到正面, 耳朵邻近的区域同样会造成自遮挡, 导致无法正确获取该区域的真实纹理信息.但是人脸的主要生物信息位于脸部, 耳朵区域包含的生物鉴别信息可忽略, 即使耳朵周围的区域纹理信息缺失, 对后续的人脸识别等应用影响也较小.综上所述, 通过人脸标准化可获取尽可能完整的人脸面部纹理, 使重建的三维人脸的纹理更自然、真实.</p>
                </div>
                <div class="p1">
                    <p id="192">本文提出的3D重构方法和使用球谐光照模型估计三维人脸纹理的3D重构算法的实验结果如图8所示. (b) 为传统方法得到的重构结果, 重建的人脸纹理缺少细节. (c) 为本文算法从标准化后的2D人脸图像中获取纹理信息重建的三维人脸, 重建结果包含更丰富的纹理细节和更真实的光照信息.由于直接在图像中根据2D-3D点对获取纹理, 三维人脸纹理重建速度较快, 传统的光照模型需要复杂的计算估计光照参数.本文算法不仅能最大程度保留原图像中人脸的纹理细节和光照条件, 得到的重构结果更自然, 计算速度更快.传统光照模型首先估计3D环境光照参数, 根据光照参数调整3D模型平均纹理, 得到最终的三维人脸纹理.传统方法受限于模型的纹理表达能力, 在模型表达能力不足时, 人脸重建的结果不能较好地表现真实图像中人的胡须、皱纹, 眼镜等细节特征.然而, 在生物识别应用中, 人脸的细节纹理往往表达更多的鉴别信息.相比传统方法, 本文算法从真实的图像环境中提取人脸纹理信息, 最大限度保留人脸细节信息, 有效保留原始图像中人脸细节特征.</p>
                </div>
                <div class="area_img" id="255">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906009_25500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 2种方法的人脸纹理重构结果对比" src="Detail/GetImg?filename=images/MSSB201906009_25500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 2种方法的人脸纹理重构结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906009_25500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Comparison of texture reconstruction results of 2 methods</p>

                </div>
                <div class="p1">
                    <p id="200">不同算法三维人脸重建结果如图9所示, 可以看出PRNet重建得到的三维人脸在嘴巴、眼镜、鼻子等包含细节变化较多的区域, 形状细节不够清晰, 相比本文算法和3DDFA的结果, PRNet重建的三维人脸分辨率较低.对于不同人脸图像样本, 3DDFA三维重建结果的身份鉴别不够明显, 不同个体之间人脸形状差异较小.本文算法的人脸形状重建结果能更好地表现人脸眼睛、嘴巴、鼻子等区域的细节.相比端到端的PRNet, 本文算法三维重建结果具有更好的细节表现.相比3DDFA, 算法重建的三维人脸包含更准确的人脸身份信息.鉴别信息对于人脸至关重要, 结合标准化人脸的纹理信息, 最大程度地保留原图像中人脸细节, 同时保留人脸的光照信息.从实验结果可看出, 相比现有通用的三维人脸重建算法, 本文算法得到的重建结果在人脸形状细节和纹理细节方面都有更好的表现.</p>
                </div>
                <div class="area_img" id="256">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906009_25600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 不同算法的人脸重建结果对比" src="Detail/GetImg?filename=images/MSSB201906009_25600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 不同算法的人脸重建结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906009_25600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Comparison of face reconstruction results by different algorithms</p>

                </div>
                <div class="area_img" id="256">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906009_25601.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 不同算法的人脸重建结果对比" src="Detail/GetImg?filename=images/MSSB201906009_25601.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 不同算法的人脸重建结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906009_25601.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Comparison of face reconstruction results by different algorithms</p>

                </div>
                <h3 id="217" name="217" class="anchor-tag">4 结 束 语</h3>
                <div class="p1">
                    <p id="218">本文提出基于人脸标准化的3D人脸重构算法.对比现有算法, 本文算法从真实的人脸图像中获取人脸纹理信息, 相比现有的光照模型, 得到的纹理信息包含更多的细节信息, 如眼睛瞳孔颜色、人脸皱纹、胡须等重要生物特征.由于纹理信息直接从2D人脸图像根据点对应关系获取, 相比传统方法需要计算复杂的环境光及皮肤纹理参数, 本文算法更简单高效.实验结果可以看出本文算法在不同姿态、光照条件下具有较好的鲁棒性.</p>
                </div>
                <div class="p1">
                    <p id="219">本文算法依然有需要改进的地方和不足.例如, 标准化之后的人脸依然存在脸颊后面部分自遮挡的情况, 这个问题可以通过人脸标准化的方法获取多角度的多幅人脸图像, 使用多角度的图像重构3D纹理信息, 获取脸颊区域的纹理细节, 解决正面人脸存在的少量自遮挡区域的问题.另外, 在戴口罩或其它大范围的可视遮挡情况下, 无法通过人脸对称性重建恢复遮挡区域的真实纹理, 在这种情况下, 人脸标准化获取的人脸纹理包含遮挡物等不需要的特征信息.这个问题也是今后的研究目标和方向.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="247" type="formula" href="images/MSSB201906009_24700.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">阳瑜</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="248" type="formula" href="images/MSSB201906009_24800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">吴小俊</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="257">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A morphable model for the synthesis of 3D faces">

                                <b>[1]</b> BLANZ V, VETTER T.A Morphable Model for the Synthesis of 3D Faces // Proc of the 26th Annual Conference on Computer Graphics and Interactive Techniques.New York, USA:ACM, 1999:187-194.
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB200301020&amp;v=MDM0MDZPZVplUm5GeXpoVTd2QktEN1liTEc0SHRMTXJvOUhaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 梁荣华, 陈纯, 张慧.一个三维人脸真实感模型重建算法.模式识别与人工智能, 2003, 16 (1) :116-121. (LIANG R H, CHEN C, ZHANG H.An Algorithm for 3D Realistic-Looking Facial Model Reconstruction.Pattern Recognition and Artificial Intelligence, 2003, 16 (1) :116-121.) 
                            </a>
                        </p>
                        <p id="261">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D-aided face recognition robust to expression and pose variations">

                                <b>[3]</b> CHU B, ROMDHANI S, CHEN L M.3D-Aided Face Recognition Robust to Expression and Pose Variations // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:1907-1914.
                            </a>
                        </p>
                        <p id="263">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Expression invariant3D face recognition with amorphable model">

                                <b>[4]</b> AMBERG B, KNOTHE R, VETTER T.Expression Invariant 3D Face Recognition with a Morphable Model // Proc of the 8th IEEE International Conference on Automatic Face and Gesture Recognition.Washington, USA:IEEE, 2008.DOI:10.1109/AFGR.2008.4813376.
                            </a>
                        </p>
                        <p id="265">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A 3D facial expression database for visual computing">

                                <b>[5]</b> CAO C, WENG Y L, ZHOU S, et al.Facewarehouse:A 3D Facial Expression Database for Visual Computing.IEEE Transactions on Visualization and Computer Graphics, 2014, 20 (3) :413-425.
                            </a>
                        </p>
                        <p id="267">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A statistical method forrobust 3D surface reconstruction from sparse data">

                                <b>[6]</b> BLANZ V, MEHL A, VETTER T, et al.A Statistical Method for Robust 3D Surface Reconstruction from Sparse Data // Proc of the 2nd International Symposium on 3D Data Processing, Visualization and Transmission.Washington, USA:IEEE, 2004:293-300.
                            </a>
                        </p>
                        <p id="269">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Estimating 3D shape and textureusing pixel intensity,edges,specular highlights,textureconstraints and a prior">

                                <b>[7]</b> ROMDHANI S, VETTER T.Estimating 3D Shape and Texture Using Pixel Intensity, Edges, Specular Highlights, Texture Constraints and a Prior // Proc of the IEEE Computer Society Confe-rence on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2005, II:986-993.
                            </a>
                        </p>
                        <p id="271">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative3D morphable model fitting">

                                <b>[8]</b> ZHU X Y, YAN J J, YI D, et al.Discriminative 3D Morphable Model Fitting // Proc of the 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition.Washington, USA:IEEE, 2015.DOI:10.1109/FG.2015.7163096.
                            </a>
                        </p>
                        <p id="273">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint Face Alignment and 3D Face Reconstruction with Application to Face Recognition">

                                <b>[9]</b> LIU F, ZHAO Q J, LIU X M, et al.Joint Face Alignment and 3D Face Reconstruction with Application to Face Recognition[J/OL].[2019-01-13].http://cvlab.cse.msu.edu/pdfs/Liu_Zeng_Zhao_Liu_ECCV2016.pdf.
                            </a>
                        </p>
                        <p id="275">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fitting 3Dmorphable face models using local features">

                                <b>[10]</b> HUBER P, FENG Z H, CHRISTMAS W, et al.Fitting 3D Morphable Face Models Using Local Features // Proc of the IEEE International Conference on Image Processing.Washington, USA:IEEE, 2015:1195-1199.
                            </a>
                        </p>
                        <p id="277">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pose-Invariant 3D Face Alignment[C/OL]">

                                <b>[11]</b> JOURABLOO A, LIU X M.Pose-Invariant 3D Face Alignment[C/OL].[2019-01-13].https://arxiv.org/pdf/1506.03799.pdf.
                            </a>
                        </p>
                        <p id="279">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face Alignment across Large Poses:A 3D Solution[C/OL]">

                                <b>[12]</b> ZHU X Y, LEI Z, LIU X M, et al.Face Alignment across Large Poses:A 3D Solution[C/OL].[2019-01-13].https://arxiv.org/pdf/1511.07212.pdf.
                            </a>
                        </p>
                        <p id="281">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-End 3D Face Reconstruction with Deep Neural Networks[C/OL]">

                                <b>[13]</b> DOU P F, SHAH S K, KAKADIARIS I A.End-to-End 3D Face Reconstruction with Deep Neural Networks[C/OL].[2019-01-13].https://arxiv.org/pdf/1704.05020.pdf.
                            </a>
                        </p>
                        <p id="283">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-pose face alignment via CNN-based dense 3D model fitting">

                                <b>[14]</b> JOURABLOO A, LIU X M.Large-Pose Face Alignment via CNN-Based Dense 3D Model Fitting // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:4188-4196.
                            </a>
                        </p>
                        <p id="285">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network[C/OL]">

                                <b>[15]</b> FENG Y, WU F, SHAO X H, et al.Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network[C/OL].[2019-01-13].https://arxiv.org/pdf/1803.07835.pdf.
                            </a>
                        </p>
                        <p id="287">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exchanging Faces in Images">

                                <b>[16]</b> BLANZ V, SCHERBAUM K, VETTER T, et al.Exchanging Faces in Images.Computer Graphics Forum, 2004, 23 (3) :669-676.
                            </a>
                        </p>
                        <p id="289">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face Recognition Using a Unified 3D Morphable Model">

                                <b>[17]</b> HU G S, YAN F, CHAN C H, et al.Face Recognition Using a Unified 3D Morphable Model // Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:73-89.
                            </a>
                        </p>
                        <p id="291">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Lighting Robust Fitting Approach of 3D Morphable Model Using Spherical Harmonic Illumination">

                                <b>[18]</b> MA M Y, HU X Y, XU Y Q, et al.A Lighting Robust Fitting Approach of 3D Morphable Model Using Spherical Harmonic Illumination // Proc of the 22nd International Conference on Pattern Recognition.Washington, USA:IEEE, 2014:2101-2106.
                            </a>
                        </p>
                        <p id="293">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-fidelity pose and expression normalization for face recognition in the wild">

                                <b>[19]</b> ZHU X Y, LEI Z, YAN J J, et al.High-Fidelity Pose and Expre-ssion Normalization for Face Recognition in the Wild // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:787-796.
                            </a>
                        </p>
                        <p id="295">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000097999&amp;v=MjAzMDByUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRm9VYmhzPU5pZklZN0s3SHRqTnI0OUZaT0lJQlhVd29CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> PÉREZ P, GANGNET M, BLAKE A.Poisson Image Editing.ACM Transactions on Graphics, 2003, 22 (3) :313-318.
                            </a>
                        </p>
                        <p id="297">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face recognition from a single training image under arbitrary unknown lighting using spherical harmonics">

                                <b>[21]</b> ZHANG L, SAMARAS D.Face Recognition from a Single Training Image under Arbitrary Unknown Lighting Using Spherical Harmonics.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006, 28 (3) :351-363.
                            </a>
                        </p>
                        <p id="299">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face detection,pose estimation,and landmark localization in the wild">

                                <b>[22]</b> ZHU X X, RAMANAN D.Face Detection, Pose Estimation, and Landmark Localization in the Wild // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2012:2879-2886.
                            </a>
                        </p>
                        <p id="301">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Localizing Parts of Faces Using a Consensus of Exemplars">

                                <b>[23]</b> BELHUMEUR P N, JACOBS D W, KRIEGMAN D J, et al.Localizing Parts of Faces Using a Consensus of Exemplars.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (12) :2930-2940.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201906009" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201906009&amp;v=MDA2MzVxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emhVN3ZPS0Q3WWJMRzRIOWpNcVk5RmJZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
