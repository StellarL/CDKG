<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131458797530000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dMSSB201909001%26RESULT%3d1%26SIGN%3dqbbCt%252bU9g9DocdVD8i%252bxdjyCUfw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201909001&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201909001&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201909001&amp;v=MjQ4MzdyM0FLRDdZYkxHNEg5ak1wbzlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#75" data-title="1 相关知识 ">1 相关知识</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#78" data-title="&lt;b&gt;1.1 定义&lt;/b&gt;"><b>1.1 定义</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;1.2 最大均值差异&lt;/b&gt;"><b>1.2 最大均值差异</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="2 两阶段领域自适应方法 ">2 两阶段领域自适应方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="&lt;b&gt;2.1 方法思路&lt;/b&gt;"><b>2.1 方法思路</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;2.2 低维投影变换&lt;/b&gt;"><b>2.2 低维投影变换</b></a></li>
                                                <li><a href="#134" data-title="&lt;b&gt;2.3 自适应分类器学习&lt;/b&gt;"><b>2.3 自适应分类器学习</b></a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;2.4 算法步骤&lt;/b&gt;"><b>2.4 算法步骤</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#178" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#179" data-title="&lt;b&gt;3.1 实验数据&lt;/b&gt;"><b>3.1 实验数据</b></a></li>
                                                <li><a href="#185" data-title="&lt;b&gt;3.2 实验设置&lt;/b&gt;"><b>3.2 实验设置</b></a></li>
                                                <li><a href="#203" data-title="&lt;b&gt;3.3 分类准确率对比&lt;/b&gt;"><b>3.3 分类准确率对比</b></a></li>
                                                <li><a href="#216" data-title="&lt;b&gt;3.4 Kappa系数对比&lt;/b&gt;"><b>3.4 Kappa系数对比</b></a></li>
                                                <li><a href="#225" data-title="&lt;b&gt;3.5 有效性分析&lt;/b&gt;"><b>3.5 有效性分析</b></a></li>
                                                <li><a href="#233" data-title="&lt;b&gt;3.6 参数敏感性分析与算法收敛性分析&lt;/b&gt;"><b>3.6 参数敏感性分析与算法收敛性分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#241" data-title="4 结 束 语 ">4 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="&lt;b&gt;表1 本文常用的符号及描述&lt;/b&gt;"><b>表1 本文常用的符号及描述</b></a></li>
                                                <li><a href="#177" data-title="图1 本文方法框架">图1 本文方法框架</a></li>
                                                <li><a href="#184" data-title="&lt;b&gt;表2 3个基准数据集&lt;/b&gt;"><b>表2 3个基准数据集</b></a></li>
                                                <li><a href="#207" data-title="&lt;b&gt;表3 13种方法在Office-Caltech10数据集上分类准确率&lt;/b&gt;"><b>表3 13种方法在Office-Caltech10数据集上分类准确率</b></a></li>
                                                <li><a href="#211" data-title="&lt;b&gt;表4 13种方法在MNIST-USPS数据集上分类准确率&lt;/b&gt;"><b>表4 13种方法在MNIST-USPS数据集上分类准确率</b></a></li>
                                                <li><a href="#215" data-title="&lt;b&gt;表5 13种方法在ImageNet-VOC2007数据集上分类准确率&lt;/b&gt;"><b>表5 13种方法在ImageNet-VOC2007数据集上分类准确率</b></a></li>
                                                <li><a href="#218" data-title="&lt;b&gt;表6 13种方法在Office-Caltech10数据集上Kappa系数&lt;/b&gt;"><b>表6 13种方法在Office-Caltech10数据集上Kappa系数</b></a></li>
                                                <li><a href="#221" data-title="&lt;b&gt;表7 13种方法在MNIST-USPS数据集上Kappa系数&lt;/b&gt;"><b>表7 13种方法在MNIST-USPS数据集上Kappa系数</b></a></li>
                                                <li><a href="#224" data-title="&lt;b&gt;表8 13种方法在ImageNet-VOC2007数据集Kappa系数&lt;/b&gt;"><b>表8 13种方法在ImageNet-VOC2007数据集Kappa系数</b></a></li>
                                                <li><a href="#228" data-title="&lt;b&gt;表9 原始空间和&lt;/b&gt;Z&lt;b&gt;中分类准确率&lt;/b&gt;"><b>表9 原始空间和</b>Z<b>中分类准确率</b></a></li>
                                                <li><a href="#230" data-title="&lt;b&gt;表10 最近邻分类器与自适应分类器的分类准确率&lt;/b&gt;"><b>表10 最近邻分类器与自适应分类器的分类准确率</b></a></li>
                                                <li><a href="#232" data-title="&lt;b&gt;表11 不考虑样本标签或数据结构判别信息时的分类准确率&lt;/b&gt;"><b>表11 不考虑样本标签或数据结构判别信息时的分类准确率</b></a></li>
                                                <li><a href="#239" data-title="图2 &lt;i&gt;k&lt;/i&gt;、 &lt;i&gt;β&lt;/i&gt;、&lt;i&gt;ε&lt;/i&gt;和&lt;i&gt;λ&lt;/i&gt;的敏感性分析">图2 <i>k</i>、 <i>β</i>、<i>ε</i>和<i>λ</i>的敏感性分析</a></li>
                                                <li><a href="#240" data-title="图3 TSDA收敛性分析">图3 TSDA收敛性分析</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="333">


                                    <a id="bibliography_1" title=" 庄福振,罗平,何清,等.迁移学习研究进展.软件学报,2015,26(1):26-39.(ZHUANG F Z,LUO P,HE Q,et al.Survey on Transfer Learning Research.Journal of Software,2015,26(1):26-39.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201501003&amp;v=MjA0OTA1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1ZyM0FOeWZUYkxHNEg5VE1ybzlGWjRRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         庄福振,罗平,何清,等.迁移学习研究进展.软件学报,2015,26(1):26-39.(ZHUANG F Z,LUO P,HE Q,et al.Survey on Transfer Learning Research.Journal of Software,2015,26(1):26-39.)
                                    </a>
                                </li>
                                <li id="335">


                                    <a id="bibliography_2" title=" GOPALAN R,LI R N,CHELLAPPA R.Domain Adaptation for Object Recognition:An Unsupervised Approach // Proc of the IEEE Conference on Computer Vision.Washington,USA:IEEE,2011:999-1006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Domain Adaptation for Object Recognition:An Unsupervised Approach">
                                        <b>[2]</b>
                                         GOPALAN R,LI R N,CHELLAPPA R.Domain Adaptation for Object Recognition:An Unsupervised Approach // Proc of the IEEE Conference on Computer Vision.Washington,USA:IEEE,2011:999-1006.
                                    </a>
                                </li>
                                <li id="337">


                                    <a id="bibliography_3" title=" GONG B Q,SHI Y,SHA F,et al.Geodesic Flow Kernel for Unsupervised Domain Adaptation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2012:2066-2073." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Geodesic flow kernel for unsupervised domain adaptation">
                                        <b>[3]</b>
                                         GONG B Q,SHI Y,SHA F,et al.Geodesic Flow Kernel for Unsupervised Domain Adaptation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2012:2066-2073.
                                    </a>
                                </li>
                                <li id="339">


                                    <a id="bibliography_4" title=" FERNANDO B,HABRAD A,SEBBAN M,et al.Unsupervised Visual Domain Adaptation Using Subspace Alignment // Proc of the IEEE Conference on Computer Vision.Washington,USA:IEEE,2013:2960-2967." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised Visual Domain Adaptation Using Subspace Alignment">
                                        <b>[4]</b>
                                         FERNANDO B,HABRAD A,SEBBAN M,et al.Unsupervised Visual Domain Adaptation Using Subspace Alignment // Proc of the IEEE Conference on Computer Vision.Washington,USA:IEEE,2013:2960-2967.
                                    </a>
                                </li>
                                <li id="341">


                                    <a id="bibliography_5" title=" PAN S J,TSANG I W,KWOK J T,et al.Domain Adaptation via Transfer Component Analysis.IEEE Transaction on Neural Networks,2011,22(2):199-210." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Domain Adaptation via Transfer Component Analysis">
                                        <b>[5]</b>
                                         PAN S J,TSANG I W,KWOK J T,et al.Domain Adaptation via Transfer Component Analysis.IEEE Transaction on Neural Networks,2011,22(2):199-210.
                                    </a>
                                </li>
                                <li id="343">


                                    <a id="bibliography_6" title=" LONG M S,WANG J M,DING G G,et al.Transfer Feature Lear-ning with Joint Distribution Adaptation // Proc of the IEEE Confe-rence on Computer Vision.Washington,USA:IEEE,2013:2200-2207." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transfer feature learning with joint distribution adaptation">
                                        <b>[6]</b>
                                         LONG M S,WANG J M,DING G G,et al.Transfer Feature Lear-ning with Joint Distribution Adaptation // Proc of the IEEE Confe-rence on Computer Vision.Washington,USA:IEEE,2013:2200-2207.
                                    </a>
                                </li>
                                <li id="345">


                                    <a id="bibliography_7" title=" VENKATESWARA H,CHAKRABORTY S,MCDANIEL T,et al.Model Selection with Nonlinear Embedding for Unsupervised Domain Adaptation[C/OL].[2019-05-15].https://arxiv.org/pdf/1706.07527.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Model Selection with Nonlinear Embedding for Unsupervised Domain Adaptation[C/OL]">
                                        <b>[7]</b>
                                         VENKATESWARA H,CHAKRABORTY S,MCDANIEL T,et al.Model Selection with Nonlinear Embedding for Unsupervised Domain Adaptation[C/OL].[2019-05-15].https://arxiv.org/pdf/1706.07527.pdf.
                                    </a>
                                </li>
                                <li id="347">


                                    <a id="bibliography_8" title=" ZHANG J,LI W Q,OGUNBONA P.Joint Geometrical and Statistical Alignment for Visual Domain Adaptation // Proc of the IEEE Conference on Computer Vision.Washington,USA:IEEE,2017:5150-5158." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint Geometrical and Statistical Alignment for Visual Domain Adaptation">
                                        <b>[8]</b>
                                         ZHANG J,LI W Q,OGUNBONA P.Joint Geometrical and Statistical Alignment for Visual Domain Adaptation // Proc of the IEEE Conference on Computer Vision.Washington,USA:IEEE,2017:5150-5158.
                                    </a>
                                </li>
                                <li id="349">


                                    <a id="bibliography_9" title=" LI S,SONG S J,HUANG G,et al.Domain Invariant and Class Discriminative Feature Learning for Visual Domain Adaptation.IEEE Transaction on Image Processing,2018,27(9):4260-4273." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Domain Invariant and Class Discriminative Feature Learning for Visual Domain Adaptation">
                                        <b>[9]</b>
                                         LI S,SONG S J,HUANG G,et al.Domain Invariant and Class Discriminative Feature Learning for Visual Domain Adaptation.IEEE Transaction on Image Processing,2018,27(9):4260-4273.
                                    </a>
                                </li>
                                <li id="351">


                                    <a id="bibliography_10" title=" YANG J,YAN R,HAUPTMANN A G.Cross-Domain Video Concept Detection Using Adaptive SVMs // Proc of the 15th ACM International Conference on Multimedia.New York,USA:ACM,2007:188-197." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-domain video concept detection using adaptive SVMS">
                                        <b>[10]</b>
                                         YANG J,YAN R,HAUPTMANN A G.Cross-Domain Video Concept Detection Using Adaptive SVMs // Proc of the 15th ACM International Conference on Multimedia.New York,USA:ACM,2007:188-197.
                                    </a>
                                </li>
                                <li id="353">


                                    <a id="bibliography_11" title=" YAO T,PAN Y W,NGO C W,et al.Semi-supervised Domain Adaptation with Subspace Learning for Visual Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:2142-2150." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised Domain Adaptation with Subspace Learning for Visual Recognition">
                                        <b>[11]</b>
                                         YAO T,PAN Y W,NGO C W,et al.Semi-supervised Domain Adaptation with Subspace Learning for Visual Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:2142-2150.
                                    </a>
                                </li>
                                <li id="355">


                                    <a id="bibliography_12" title=" ZHANG L,ZHANG D.Robust Visual Knowledge Transfer via Extreme Learning Machine-Based Domain Adaptation.IEEE Transaction on Image Processing,2016,25(10):4959-4973." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust visual knowledge transfer via extreme learning machinebased domain adaptation">
                                        <b>[12]</b>
                                         ZHANG L,ZHANG D.Robust Visual Knowledge Transfer via Extreme Learning Machine-Based Domain Adaptation.IEEE Transaction on Image Processing,2016,25(10):4959-4973.
                                    </a>
                                </li>
                                <li id="357">


                                    <a id="bibliography_13" title=" DUAN L X,TSANG I W,XU D,et al.Domain Transfer SVM for Video Concept Detection // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2009:1375-1381." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Maybank.Domain transfer svm for video concept detection">
                                        <b>[13]</b>
                                         DUAN L X,TSANG I W,XU D,et al.Domain Transfer SVM for Video Concept Detection // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2009:1375-1381.
                                    </a>
                                </li>
                                <li id="359">


                                    <a id="bibliography_14" title=" LONG M S,WANG J M,DING G G,et al.Adaptation Regulari-zation:A General Framework for Transfer Learning.IEEE Transa-ction on Knowledge and Data Engineering,2014,26(5):1076-1089." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptation regularization:A general framework for transfer learning">
                                        <b>[14]</b>
                                         LONG M S,WANG J M,DING G G,et al.Adaptation Regulari-zation:A General Framework for Transfer Learning.IEEE Transa-ction on Knowledge and Data Engineering,2014,26(5):1076-1089.
                                    </a>
                                </li>
                                <li id="361">


                                    <a id="bibliography_15" title=" WANG J D,FENG W J,CHEN Y Q,et al.Visual Domain Adaptation with Manifold Embedded Distribution Alignment // Proc of the 26th ACM International Conference on Multimedia.New York,USA:ACM,2018:402-410." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual Domain Adaptation with Manifold Embedded Distribution Alignment">
                                        <b>[15]</b>
                                         WANG J D,FENG W J,CHEN Y Q,et al.Visual Domain Adaptation with Manifold Embedded Distribution Alignment // Proc of the 26th ACM International Conference on Multimedia.New York,USA:ACM,2018:402-410.
                                    </a>
                                </li>
                                <li id="363">


                                    <a id="bibliography_16" title=" PAN S J,YANG Q.A Survey on Transfer Learning.IEEE Trans-actions on Knowledge and Data Engineering,2010,22(10):1345-1359." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Survey on Transfer Learning">
                                        <b>[16]</b>
                                         PAN S J,YANG Q.A Survey on Transfer Learning.IEEE Trans-actions on Knowledge and Data Engineering,2010,22(10):1345-1359.
                                    </a>
                                </li>
                                <li id="365">


                                    <a id="bibliography_17" title=" PAN S J,KWOK J T,YANG Q.Transfer Learning via Dimensionality Reduction // Proc of the 23rd National Conference on Artificial Intelligence.Palo Alto,USA:AAAI Press,2008,II:677-682." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transfer learning via dimensionality reduction">
                                        <b>[17]</b>
                                         PAN S J,KWOK J T,YANG Q.Transfer Learning via Dimensionality Reduction // Proc of the 23rd National Conference on Artificial Intelligence.Palo Alto,USA:AAAI Press,2008,II:677-682.
                                    </a>
                                </li>
                                <li id="367">


                                    <a id="bibliography_18" title=" DONAHUE J,JIA Y Q,VINYALS O,et al.DeCAF:A Deep Convolutional Activation Feature for Generic Visual Recognition // Proc of the 31st International Conference on Machine Learning.New York,USA:ACM,2014:647-655." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=De CAF:A Deep Convolutional Activation Feature for Generic Visual Recognition">
                                        <b>[18]</b>
                                         DONAHUE J,JIA Y Q,VINYALS O,et al.DeCAF:A Deep Convolutional Activation Feature for Generic Visual Recognition // Proc of the 31st International Conference on Machine Learning.New York,USA:ACM,2014:647-655.
                                    </a>
                                </li>
                                <li id="369">


                                    <a id="bibliography_19" title=" 龙明盛.迁移学习问题与方法研究.博士学位论文.北京:清华大学,2014.(LONG M S.Transfer Learning:Problems and Methods.Ph.D.Dissertation.Beijing,China:Tsinghua University,2014.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1015039180.nh&amp;v=MjcwMjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVnIzQVZGMjZHN083RjlERXI1RWJQSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         龙明盛.迁移学习问题与方法研究.博士学位论文.北京:清华大学,2014.(LONG M S.Transfer Learning:Problems and Methods.Ph.D.Dissertation.Beijing,China:Tsinghua University,2014.)
                                    </a>
                                </li>
                                <li id="371">


                                    <a id="bibliography_20" title=" FANG C,XU Y,ROCKMORE D N.Unbiased Metric Learning:On the Utilization of Multiple Datasets and Web Images for Softe-ning Bias // Proc of the IEEE International Conference on Compu-ter Vision.Washington,USA:IEEE,2013:1657-1664." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unbiased Metric Learning:On the Utilization of Multiple Datasets and Web Images for Softe-ning Bias">
                                        <b>[20]</b>
                                         FANG C,XU Y,ROCKMORE D N.Unbiased Metric Learning:On the Utilization of Multiple Datasets and Web Images for Softe-ning Bias // Proc of the IEEE International Conference on Compu-ter Vision.Washington,USA:IEEE,2013:1657-1664.
                                    </a>
                                </li>
                                <li id="373">


                                    <a id="bibliography_21" title=" FUKUNAGA K,NARENDRA P M.A Branch and Bound Algorithm for Computing k-Nearest Neighbors.IEEE Transactions on Computers,1975,24(7):750-753." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A branch and bound algorithm for computing k-nearest neighbors">
                                        <b>[21]</b>
                                         FUKUNAGA K,NARENDRA P M.A Branch and Bound Algorithm for Computing k-Nearest Neighbors.IEEE Transactions on Computers,1975,24(7):750-753.
                                    </a>
                                </li>
                                <li id="375">


                                    <a id="bibliography_22" title=" VAPNIK V N.An Overview of Statistical Learning Theory.IEEE Transaction on Neural Networks,1999,10(5):988-999." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Overview of statistical learning theory">
                                        <b>[22]</b>
                                         VAPNIK V N.An Overview of Statistical Learning Theory.IEEE Transaction on Neural Networks,1999,10(5):988-999.
                                    </a>
                                </li>
                                <li id="377">


                                    <a id="bibliography_23" title=" SUN B C,FENG J S,SAENKO K.Return of Frustratingly Easy Domain Adaptation // Proc of the 30th Conference on Artificial Intelligence.Palo Alto,USA:AAAI Press,2016:2058-2065." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Return of Frustratingly Easy Domain Adaptation">
                                        <b>[23]</b>
                                         SUN B C,FENG J S,SAENKO K.Return of Frustratingly Easy Domain Adaptation // Proc of the 30th Conference on Artificial Intelligence.Palo Alto,USA:AAAI Press,2016:2058-2065.
                                    </a>
                                </li>
                                <li id="379">


                                    <a id="bibliography_24" title=" LIANG J,HE R,SUN Z N,et al.Aggregating Randomized Clustering-Promoting Invariant Projections for Domain Adaptation.IEEE Transaction on Pattern Analysis and Machine Intelligence,2019,41(5):1027-1042." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Aggregating Randomized Clustering-Promoting Invariant Projections for Domain Adaptation">
                                        <b>[24]</b>
                                         LIANG J,HE R,SUN Z N,et al.Aggregating Randomized Clustering-Promoting Invariant Projections for Domain Adaptation.IEEE Transaction on Pattern Analysis and Machine Intelligence,2019,41(5):1027-1042.
                                    </a>
                                </li>
                                <li id="381">


                                    <a id="bibliography_25" title=" VOGADO L H S,VERAS R M S,ARAUJO F H D,et al.Leuke-mia Diagnosis in Blood Slides Using Transfer Learning in CNNs and SVM for Classification.Engineering Applications of Artificial Inte-lligence,2018,72:415-422." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES4CA015BECF2CC4750F6554B8BD8DC0E7&amp;v=MTczMzZjY2JuUmMrWUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHc3bTZ3cUE9TmlmT2ZiZkxiOUhOcXYwd0Y1ME5mdzg5eUJNVG5EbDRUWHVRcEdCQg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         VOGADO L H S,VERAS R M S,ARAUJO F H D,et al.Leuke-mia Diagnosis in Blood Slides Using Transfer Learning in CNNs and SVM for Classification.Engineering Applications of Artificial Inte-lligence,2018,72:415-422.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(09),773-784 DOI:10.16451/j.cnki.issn1003-6059.201909001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>两阶段领域自适应学习</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%94%B0%E7%A3%8A&amp;code=42910561&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">田磊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%94%90%E6%B0%B8%E5%BC%BA&amp;code=42394634&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">唐永强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%96%87%E7%94%9F&amp;code=11028945&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张文生</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E8%87%AA%E5%8A%A8%E5%8C%96%E7%A0%94%E7%A9%B6%E6%89%80%E7%B2%BE%E5%AF%86%E6%84%9F%E7%9F%A5%E4%B8%8E%E6%8E%A7%E5%88%B6%E4%B8%AD%E5%BF%83&amp;code=0143551&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院自动化研究所精密感知与控制中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E9%99%A2&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学人工智能学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对领域自适应问题中源域和目标域的联合分布差异最小化问题,提出两阶段领域自适应学习方法.在第一阶段考虑样本标签和数据结构的判别信息,通过学习一个共享投影变换,使投影后的共享空间中边缘分布的差异最小.第二阶段利用源域标记数据和目标域非标记数据学习一个带结构风险的自适应分类器,不仅能最小化源域和目标域条件分布差异,还能进一步保持源域和目标域边缘分布的流形一致性.在3个基准数据集上的实验表明,文中方法在平均分类准确率和Kappa系数两项评价指标上均表现较优.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">领域自适应;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%A4%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">两阶段学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BE%B9%E7%BC%98%E5%88%86%E5%B8%83%E9%80%82%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">边缘分布适配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9D%A1%E4%BB%B6%E5%88%86%E5%B8%83%E9%80%82%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">条件分布适配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%A4%E5%88%AB%E4%BF%A1%E6%81%AF%E4%BF%9D%E7%95%99&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">判别信息保留;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    田磊，博士研究生，主要研究方向为模式识别、数据挖掘．E-mail:tianlei2017@ia.ac.cn.&lt;image id="329" type="formula" href="images/MSSB201909001_32900.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    唐永强，博士，助理研究员，主要研究方向为计算机视觉、数据挖掘、机器学习．E-mail:tangyongqiang2014@ia.ac.cn.&lt;image id="330" type="formula" href="images/MSSB201909001_33000.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    *张文生(通讯作者)，博士，教授，主要研究方向为人工智能、机器学习、数据挖掘．E-mail:zhangwenshengia@hotmail.com.&lt;image id="331" type="formula" href="images/MSSB201909001_33100.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-12</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(No.U1636220,61472423)资助;</span>
                    </p>
            </div>
                    <h1><b>Two Stage Domain Adaptation Learning</b></h1>
                    <h2>
                    <span>TIAN Lei</span>
                    <span>TANG Yongqiang</span>
                    <span>ZHANG Wensheng</span>
            </h2>
                    <h2>
                    <span>Research Center of Precision Sensing and Control,Institute of Automation,Chinese Academy of Sciences</span>
                    <span>School of Artificial Intelligence,University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at minimization of the joint distribution difference between source domain and target domain in domain adaptation, a two-stage domain adaptation learning method is proposed. In the first stage, the discriminative information of sample labels and the data structure are considered, and a shared projection transformation is learned to minimize the difference of marginal distribution in the shared-projected space. In the second stage, an adaptive classifier with structural risk is learned by the labeled source data and unlabeled target data. The classifier minimizes the difference of conditional distribution of source domain and target domain as well as maintains the manifold consistency underlying the marginal distributions. Experiments on three benchmark datasets show that the method achieves better results on average classification accuracy and the Kappa coefficient.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Domain%20Adaptation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Domain Adaptation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Two-Stage%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Two-Stage Learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Marginal%20Distribution%20Alignment&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Marginal Distribution Alignment;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Conditional%20Distribution%20Alignment&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Conditional Distribution Alignment;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Discriminant%20Information%20Preservation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Discriminant Information Preservation;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    TIAN Lei,Ph. D. candidate. His research interests include pattern recognition and data mining.;
                                </span>
                                <span>
                                    TANG Yongqiang,Ph. D. ,assistant professor. His research interests include computer vision, data mining and machine learning.;
                                </span>
                                <span>
                                    ZHANG Wensheng(Corresponding author),Ph. D.,professor. His research inte-rests include artificial intelligence,machine learning and data mining.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-12</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China(No.U1636220,61472423);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="65">为了保证训练得到的分类模型具有可信的分类效果,传统的机器学习方法通常需要满足两个基本假设:1)足够的带标注的训练样本,2)训练样本和测试样本满足独立同分布条件.然而,在实际应用中通常无法满足这两个假设条件.一方面,获取的数据大多缺乏完善的标注,而标注大量的样本又费时费力<citation id="383" type="reference"><link href="333" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>;另一方面,随着时间的推移,原先可用的训练样本与新测试样本的分布会逐渐出现偏离.迁移学习的出现为这两个问题提供一种有效的解决途径,其基本思想是利用源域和目标域之间的关联关系,将从有标注的源域数据中学习得到的知识迁移和复用到缺少标注的目标域,使传统的从零开始学习变成可累积学习,进而提升机器学习效果.</p>
                </div>
                <div class="p1">
                    <p id="66">近些年,学者们从不同角度对迁移学习展开广泛研究.其中,领域自适应任务受到较多关注,该任务假设源域和目标域的标签和标签预测函数相同.领域自适应问题的根本原因是域间联合概率分布不同,难点在于如何减小域间联合概率分布差异.</p>
                </div>
                <div class="p1">
                    <p id="67">针对此问题,现有的非深度学习方法可分为3类:1)实例权重法,通过调整源域样本权重使分布相似;2)特征适配法,通过学习共享子空间或特征表示以减小联合分布差异;3)分类器适配法,在源域上学习一个可泛化到目标域的分类器或学习两个相似的分类器.本文主要关注特征适配法和分类器适配法.</p>
                </div>
                <div class="p1">
                    <p id="68">特征适配法主要通过学习共享子空间或特征表示以减小域间分布差异.在共享子空间学习方面,Gopalan等<citation id="384" type="reference"><link href="335" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>在源域和目标域数据的格拉斯曼流形的测地路径上采样一组子空间,找到跨域不变的中间表示.Gong等<citation id="385" type="reference"><link href="337" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>在上述基础上,利用测地路径上无限子空间的积分构建测地流核,建立域漂移模型.</p>
                </div>
                <div class="p1">
                    <p id="69">与上述基于流形变换的方法不同,Fernando等<citation id="386" type="reference"><link href="339" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>考虑统计特征变换,直接使用一个矩阵以适配源域和目标域数据的PCA主成分向量张成的子空间.在共享特征表示学习方面,Pan等<citation id="387" type="reference"><link href="341" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>通过最小化源域数据和目标域数据在一个再生希尔伯特空间上的最大均值偏差,学习跨域不变的迁移成分.Long等<citation id="388" type="reference"><link href="343" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>在上述基础上最小化源域数据和目标域数据的条件分布差异,使源域和目标域联合概率分布差异最小.</p>
                </div>
                <div class="p1">
                    <p id="70">为使学习的特征表示更有利于分类问题,一些研究者也关注如何保留源域、目标域数据的判别信息.Venkateswara等<citation id="389" type="reference"><link href="345" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>对源域和目标域数据进行非线性嵌入变换,同时限制同类样本聚到一簇以保留源域样本标签的判别性.Zhang等<citation id="390" type="reference"><link href="347" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>结合几何和统计适配,对源域和目标域分别学习一个投影矩阵,并通过最小化源域样本类内散度和最大化源域样本类间散度保留源域样本标签的判别性.Li等<citation id="391" type="reference"><link href="349" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>还考虑目标域样本,通过最小化源域、目标域的同类样本距离和最大化源域、目标域的不同类样本距离,保留两域样本标签的判别性.</p>
                </div>
                <div class="p1">
                    <p id="71">分类器适配法旨在利用源域数据学习一个可泛化到目标域的分类器或学习两个相似的分类器.在学习两个相似的分类器上,Yang等<citation id="392" type="reference"><link href="351" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>假设源域和目标域分类器存在偏置并对偏置进行线性建模求解.Yao等<citation id="393" type="reference"><link href="353" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>考虑到数据的结构特性,通过最小化源域数据的分类误差、保持两域数据的几何结构并结合流形正则化为源域和目标域分别学习一个分类器.Zhang等<citation id="394" type="reference"><link href="355" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>注意到目标域数据伪标签不准确会对结果产生巨大影响,利用标签预计算和精修,在源域和目标域上分别学习基于流形正则化的最小二乘分类器.在学习一个可泛化到目标域的分类器方面,Duan等<citation id="395" type="reference"><link href="357" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>通过最小化SVM分类器的结构误差函数和两域的边缘分布差异同时学习一个核函数和SVM分类器.Long等<citation id="396" type="reference"><link href="359" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>通过最小化源域数据结构风险、最小化两域联合分布差异和最大化两域边缘分布的流形一致性,学习跨域不变的分类器.Wang等<citation id="397" type="reference"><link href="361" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>利用文献<citation id="398" type="reference">[<a class="sup">3</a>]</citation>的方法,将两域数据映射到一个格拉斯曼流形上,并在这个流形上利用文献<citation id="399" type="reference">[<a class="sup">14</a>]</citation>方法学习一个跨域不变的分类器,同时进行边缘分布和条件分布的动态适配.</p>
                </div>
                <div class="p1">
                    <p id="72">现有的领域自适应方法可减小域间联合分布差异,但也存在如下两个问题:1)特征适配法计算条件分布差异时使用<mathml id="243"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">|</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></math></mathml>近似<mathml id="244"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math></mathml>,在实际样本分布不平衡时这一近似无法最有效减少域间联合分布差异;2)分类器适配法通过直接训练分类器拟合条件分布差异<mathml id="245"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math></mathml>,避免使用<mathml id="246"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">|</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></math></mathml>作近似替代,但在训练分类器的过程中缺乏对特征边缘分布<i>P</i>(<i>x</i>)适配的考虑,因此限制性能进一步提升.</p>
                </div>
                <div class="p1">
                    <p id="73">针对上述问题,本文提出两阶段领域自适应方法(Two Stage Domain Adaptation, TSDA).TSDA在第一阶段学习一个共享投影变换,使得在变换后的共享空间中源域和目标域的边缘分布<i>P</i>(<i>x</i>)差异最小.进一步考虑样本标签和数据结构的判别性,通过最小化投影后同类样本距离和投影前后<i>p</i>近邻样本的流形差异保留判别信息.在第二阶段利用源域标记数据和目标域非标记数据学习一个带结构风险的自适应分类器,在最小化源域和目标域条件分布<mathml id="247"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math></mathml>差异的同时,又能维持源域和目标域边缘分布的流形一致性.上述两个阶段交替进行,最终使源域和目标域的联合分布差异最小.在国际上广泛使用的Office-Caltech10、MNIST-USPS和ImageNet-VOC-2007基准图像数据集上的实验表明本文方法的有效性.</p>
                </div>
                <h3 id="75" name="75" class="anchor-tag">1 相关知识</h3>
                <div class="p1">
                    <p id="76">本文常用符号及描述总结如表1所示.</p>
                </div>
                <div class="area_img" id="77">
                    <p class="img_tit"><b>表1 本文常用的符号及描述</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Symbols and their descriptions</p>
                    <p class="img_note"></p>
                    <table id="77" border="1"><tr><td><br />符号</td><td>描述</td></tr><tr><td><br />D<sub><i>s</i></sub></td><td>源域</td></tr><tr><td><br />D<sub><i>t</i></sub></td><td>目标域</td></tr><tr><td><br /><b><i>X</i></b><sub><i>s</i></sub>∈<b>R</b><sup><i>d</i>×<i>m</i></sup></td><td>源域样本集</td></tr><tr><td><br /><b><i>y</i></b><sub><i>s</i></sub></td><td>源域样本标签</td></tr><tr><td><br /><b><i>X</i></b><sub><i>t</i></sub>∈<b>R</b><sup><i>d</i>×<i>m</i></sup></td><td>目标域样本集</td></tr><tr><td><br /><b><i>X</i></b></td><td>[<b><i>X</i></b><sub><i>s</i></sub>,<b><i>X</i></b><sub><i>t</i></sub>]</td></tr><tr><td><br />Z</td><td>投影后的低维空间</td></tr><tr><td><br /><i>d</i></td><td>原始空间特征维度</td></tr><tr><td><br /><i>k</i></td><td>低维空间特征维度</td></tr><tr><td><br /><b><i>L</i></b></td><td>归一化拉普拉斯矩阵</td></tr><tr><td><br /><i>m</i></td><td>源域样本数</td></tr><tr><td><br /><i>n</i></td><td>目标域样本数</td></tr><tr><td><br /><i>N</i><sub><i>p</i></sub>(<b><i>x</i></b><sub><i>i</i></sub>)</td><td>点<b><i>x</i></b><sub><i>i</i></sub>的<i>p</i>近邻</td></tr><tr><td><br /><b><i>I</i></b><sub><i>k</i></sub></td><td><i>k</i>维单位矩阵</td></tr><tr><td><br /><b><i>W</i></b></td><td>图邻接矩阵</td></tr><tr><td><br /><b><i>A</i></b></td><td>低维空间投影矩阵</td></tr><tr><td><br /><b><i>Z</i></b><sub><i>s</i></sub>∈<b>R</b><sup><i>d</i>×<i>m</i></sup></td><td>Z中源域样本集</td></tr><tr><td><br /><b><i>Z</i></b><sub><i>t</i></sub>∈<b>R</b><sup><i>d</i>×<i>m</i></sup></td><td>Z中目标域样本集</td></tr><tr><td><br /><i>m</i><sup>(<i>c</i>)</sup></td><td>目标域中类别为<i>c</i>的样本总数</td></tr><tr><td><br /><i>n</i><sup>(<i>c</i>)</sup></td><td>源域中类别为<i>c</i>的样本总数</td></tr><tr><td><br /><b>1</b><sub><i>p</i>×<i>q</i></sub></td><td>所有元素都是1的<i>p</i>×<i>q</i>阶矩阵</td></tr><tr><td><br /><b><i>Z</i></b></td><td>[<b><i>Z</i></b><sub><i>s</i></sub>,<b><i>Z</i></b><sub><i>t</i></sub>]</td></tr><tr><td><br />H</td><td>再生希尔伯特空间</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="78" name="78"><b>1.1 定义</b></h4>
                <div class="p1">
                    <p id="79"><b>定义 1</b>领域,任务<citation id="400" type="reference"><link href="363" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation> 一个领域D包含特征空间X和边缘概率分布<i>P</i>(<i><b>X</b></i>),其中<i><b>X</b></i>={<i><b>x</b></i><sub>1</sub>,<i><b>x</b></i><sub>2</sub>,…,<i><b>x</b></i><sub><i>n</i></sub>}.给定一个领域D={X,<i>P</i>(<i><b>X</b></i>)},一个任务T由标签空间Y和目标预测函数<i>f</i>(·)组成,记为T={Y,<i>f</i>(·)}.使用函数<i>f</i>(·)预测一个实例<i><b>x</b></i>的标签<i>f</i>(<i><b>x</b></i>),从概率观点<i>f</i>(<i><b>x</b></i>)可视为<mathml id="248"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>.</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="80">有标签的领域称为源域,记为</p>
                </div>
                <div class="p1">
                    <p id="81">D<sub><i>s</i></sub>={(<i><b>x</b></i><sub><i>s</i></sub><sub>1</sub>,<i><b>y</b></i><sub><i>s</i></sub><sub>1</sub>),(<i><b>x</b></i><sub><i>s</i></sub><sub>2</sub>,<i><b>y</b></i><sub><i>s</i></sub><sub>2</sub>),…,(<i><b>x</b></i><sub><i>sn</i></sub>,<i><b>y</b></i><sub><i>sn</i></sub>)}={<i><b>X</b></i><sub><i>s</i></sub>,<i><b>y</b></i><sub><i>s</i></sub>},</p>
                </div>
                <div class="p1">
                    <p id="82">其中<i><b>y</b></i><sub><i>si</i></sub>∈<i><b>y</b></i><sub><i>s</i></sub>为样本标签.无标签的领域称为目标域,记为D<sub><i>t</i></sub>={<i><b>x</b></i><sub><i>t</i></sub><sub>1</sub>,<i><b>x</b></i><sub><i>t</i></sub><sub>2</sub>,…,<i><b>x</b></i><sub><i>tm</i></sub>}={<i><b>X</b></i><sub><i>t</i></sub>}.</p>
                </div>
                <div class="p1">
                    <p id="83"><b>定义 2</b>迁移学习<citation id="401" type="reference"><link href="363" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation> 给定一个源域D<sub><i>s</i></sub>和源域任务T<sub><i>s</i></sub>,一个目标域D<sub><i>t</i></sub>和目标任务T<sub><i>t</i></sub>,迁移学习旨在利用源域和目标域的知识改进目标预测函数<i>f</i><sub><i>t</i></sub>(·)的学习,其中D<sub><i>s</i></sub>≠D<sub><i>t</i></sub>或T<sub><i>s</i></sub>≠T<sub><i>t</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="84"><b>定义 3</b>直推式迁移学习<citation id="402" type="reference"><link href="363" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation> 给定一个源域D<sub><i>s</i></sub>和源域任务T<sub><i>s</i></sub>,一个目标域D<sub><i>t</i></sub>和目标任务T<sub><i>t</i></sub>,直推式迁移学习旨在利用源域和目标域的知识改进目标预测函数<i>f</i><sub><i>t</i></sub>(·)的学习,其中D<sub><i>s</i></sub>≠D<sub><i>t</i></sub>且T<sub><i>s</i></sub>=T<sub><i>t</i></sub>.此外,训练时也必须使用一些无标签的目标域数据.直推式迁移学习也被认为是领域自适应.</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85"><b>1.2 最大均值差异</b></h4>
                <div class="p1">
                    <p id="86">最大均值差异(Maximum Mean Discrepancy, MMD)<citation id="403" type="reference"><link href="365" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>是在再生希尔伯特空间H中衡量两种分布差异的方法.假设</p>
                </div>
                <div class="p1">
                    <p id="87"><i><b>X</b></i>={<i><b>x</b></i><sub>1</sub>,<i><b>x</b></i><sub>2</sub>,…,<i><b>x</b></i><sub><i>r</i></sub>}, <i><b>Y</b></i>={<i><b>y</b></i><sub>1</sub>,<i><b>y</b></i><sub>2</sub>,…,<i><b>y</b></i><sub><i>s</i></sub>}</p>
                </div>
                <div class="p1">
                    <p id="88">为采自分布<i>P</i>和<i>Q</i>的随机样本集,由MMD定义的分布<i>P</i>和<i>Q</i>的经验距离为</p>
                </div>
                <div class="p1">
                    <p id="89"><mathml id="249"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo>,</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>sup</mi></mrow></mstyle><mrow><mrow><mo stretchy="false">∥</mo><mi>f</mi><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mtext>Η</mtext></msub><mo>≤</mo><mn>1</mn></mrow></munder><mo stretchy="false">(</mo><mfrac><mn>1</mn><mi>r</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>r</mi></munderover><mi>f</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mfrac><mn>1</mn><mi>s</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><mi>f</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></math></mathml>,      (1)</p>
                </div>
                <div class="p1">
                    <p id="90">当<i>r</i>→∞,<i>s</i>→∞时,<i>Dist</i>(<i><b>X</b></i>,<i><b>Y</b></i>)=0当且仅当<i>P</i>=<i>Q</i>.在H中, <i>f</i>(<i><b>x</b></i>)=〈ϕ(<i><b>x</b></i>), <i>f</i>〉,其中ϕ(<i><b>x</b></i>)∶X→H,故式(1)可重写为</p>
                </div>
                <div class="p1">
                    <p id="91"><mathml id="250"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo>,</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>|</mo><mrow><mfrac><mn>1</mn><mi>r</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>r</mi></munderover><mtext>ϕ</mtext></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mfrac><mn>1</mn><mi>s</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><mtext>ϕ</mtext></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><msub><mrow></mrow><mtext>Η</mtext></msub></mrow></math></mathml>.</p>
                </div>
                <h3 id="92" name="92" class="anchor-tag">2 两阶段领域自适应方法</h3>
                <h4 class="anchor-tag" id="93" name="93"><b>2.1 方法思路</b></h4>
                <div class="p1">
                    <p id="94">领域自适应的目标是最小化源域和目标域的联合概率分布差异,但是,由于目标域数据缺乏标注,因此很难直接最小化联合概率分布差异.目前较通用的做法是将联合分布转换为边缘分布和条件分布乘积的形式,即将联合概率分布差异最小化问题进行如下转换:</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mrow><mo>|</mo><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>s</mi></msub><mo>,</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo>,</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mrow><mo>|</mo><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">)</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">经过上式的变换,联合分布最小化问题可分解为如下两个需同时成立的子问题:1)边缘分布<i>P</i>(<i><b>z</b></i><sub><i>s</i></sub>)与<i>P</i>(<i><b>z</b></i><sub><i>t</i></sub>)差异最小化;2)条件分布<mathml id="251"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>与<mathml id="252"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>差异最小化.现有的特征适配方法在计算条件分布差异时,通常利用<mathml id="253"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">)</mo></mrow></math></mathml>近似<mathml id="254"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo></mrow></math></mathml>,然而,根据概率公式</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mrow><mo>|</mo><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>s</mi></msub><mo>,</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo>,</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mo stretchy="false">|</mo><mfrac><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">)</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mo>-</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">|</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">可知,在实际样本分布不平衡(即<i>P</i>(<i><b>y</b></i><sub><i>s</i></sub>)与<i>P</i>(<i><b>y</b></i><sub><i>t</i></sub>)差异较大)时,现有的特征适配方法不能有效减少域间的联合分布差异.分类器适配方法通过训练一个分类器预测目标域标签,从概率角度上看,将分类器函数视为条件分布<mathml id="255"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo></mrow></math></mathml>.因此,此类方法可看作对<mathml id="256"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>和<mathml id="257"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>的适配.虽然分类器适配法避免对<mathml id="258"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo></mrow></math></mathml>的近似替代,但是缺乏对边缘分布<i>P</i>(<i><b>z</b></i>)适配的考虑.基于上述讨论,本文提出两阶段领域自适应方法,能同时最小化边缘分布(<i>P</i>(<i><b>z</b></i><sub><i>s</i></sub>)与<i>P</i>(<i><b>z</b></i><sub><i>t</i></sub>))差异及条件分布<mathml id="259"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>与<mathml id="260"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></math></mathml>差异.</p>
                </div>
                <div class="p1">
                    <p id="99">具体地,针对边缘分布差异最小化问题,本文首先对源域和目标域数据特征进行统一的低维投影变换,在投影后的低维空间Z中满足<i>P</i>(<i><b>z</b></i><sub><i>s</i></sub>)≈<i>P</i>(<i><b>z</b></i><sub><i>t</i></sub>).针对条件分布差异最小化问题,本文在第二阶段学习一个自适应分类器,使条件分布</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">)</mo><mo>≈</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">上述两个阶段交替迭代,最终实现</p>
                </div>
                <div class="p1">
                    <p id="102"><i>P</i>(<i><b>z</b></i><sub><i>s</i></sub>,<i><b>y</b></i><sub><i>s</i></sub>)≈<i>P</i>(<i><b>z</b></i><sub><i>t</i></sub>,<i><b>y</b></i><sub><i>t</i></sub>).</p>
                </div>
                <div class="p1">
                    <p id="103">在第一阶段进行特征变换时,判别信息的保留至关重要.首先,本文通过最小化投影后的同类样本间距离,保留样本标签的判别性.进一步地,通过最小化投影前后<i>p</i>近邻样本的流形差异,保留样本结构的判别性.在第二阶段利用源域标记数据和目标域非标记数据学习一个带结构风险的自适应分类器,在最小化源域和目标域条件分布<mathml id="261"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>与<mathml id="262"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></math></mathml>差异的同时,进一步维持源域和目标域边缘分布的流形一致性.</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104"><b>2.2 低维投影变换</b></h4>
                <div class="p1">
                    <p id="105">低维投影变换的优化目标为</p>
                </div>
                <div class="area_img" id="106">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201909001_10600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="108">其中:<i>α</i>、 <i>β</i>、<i>ε</i>表示正则化参数;<i><b>H</b></i>表示中心矩阵,</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><mo>=</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>+</mo><mi>m</mi></mrow></msub><mo>-</mo><mfrac><mrow><mn>1</mn><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>+</mo><mi>m</mi><mo stretchy="false">)</mo><mo>×</mo><mo stretchy="false">(</mo><mi>n</mi><mo>+</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msub></mrow><mrow><mi>n</mi><mo>+</mo><mi>m</mi></mrow></mfrac><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">限制条件旨在最大化映射后的数据方差以尽量保持数据特性;<i>L</i><sub>MMD</sub>表示边缘分布适配损失;<i>aL</i><sub>dis</sub>+<i>εL</i><sub>nei</sub>表示判别信息损失;‖<i><b>A</b></i>‖<sup>2</sup><sub>F</sub>控制模型复杂度.</p>
                </div>
                <div class="p1">
                    <p id="111">采用低维空间的MMD准则<citation id="404" type="reference"><link href="349" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>,边缘分布适配损失为</p>
                </div>
                <div class="p1">
                    <p id="112"><i>L</i><sub>MMD</sub>=tr(<i><b>A</b></i><sup>T</sup><i><b>XBX</b></i><sup>T</sup><i><b>A</b></i>),</p>
                </div>
                <div class="p1">
                    <p id="113">其中</p>
                </div>
                <div class="p1">
                    <p id="114" class="code-formula">
                        <mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">B</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mfrac><mrow><mn>1</mn><msub><mrow></mrow><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow></msub></mrow><mrow><mi>n</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mtd><mtd><mfrac><mrow><mn>1</mn><msub><mrow></mrow><mrow><mi>n</mi><mo>×</mo><mi>m</mi></mrow></msub></mrow><mrow><mi>n</mi><mi>m</mi></mrow></mfrac></mtd></mtr><mtr><mtd><mfrac><mrow><mn>1</mn><msub><mrow></mrow><mrow><mi>n</mi><mo>×</mo><mi>m</mi></mrow></msub></mrow><mrow><mi>n</mi><mi>m</mi></mrow></mfrac></mtd><mtd><mfrac><mrow><mn>1</mn><msub><mrow></mrow><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow></msub></mrow><mrow><mi>m</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="115">判别信息损失的第一项<citation id="405" type="reference"><link href="349" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>为</p>
                </div>
                <div class="p1">
                    <p id="116"><i>L</i><sub>dis</sub>=tr(<i><b>A</b></i><sup>T</sup><i><b>XRX</b></i><sup>T</sup><i><b>A</b></i>),</p>
                </div>
                <div class="p1">
                    <p id="117">其中</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">R</mi><mo>=</mo><mtext>d</mtext><mtext>i</mtext><mtext>a</mtext><mtext>g</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>s</mi></msub><mo>,</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">)</mo></mrow><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>n</mi><mo>,</mo></mtd><mtd columnalign="left"><mi>i</mi><mo>=</mo><mi>j</mi></mtd></mtr><mtr><mtd columnalign="left"><mo>-</mo><mfrac><mi>n</mi><mrow><mi>n</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mo>,</mo></mtd><mtd columnalign="left"><mi>i</mi><mo>≠</mo><mi>j</mi><mo>,</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>s</mi><mi>i</mi></mrow></msub><mo>=</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>s</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>c</mi></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>,</mo></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>它</mtext></mtd></mtr></mtable></mrow></mrow><mo>,</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>m</mi><mo>,</mo></mtd><mtd columnalign="left"><mi>i</mi><mo>=</mo><mi>j</mi></mtd></mtr><mtr><mtd columnalign="left"><mo>-</mo><mfrac><mi>m</mi><mrow><mi>m</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mo>,</mo></mtd><mtd columnalign="left"><mi>i</mi><mo>≠</mo><mi>j</mi><mo>,</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>i</mi></mrow></msub><mo>=</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>c</mi></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>,</mo></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>它</mtext></mtd></mtr></mtable></mrow></mrow><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">对于源域样本,<i>c</i>为真实标签.对于目标域样本,<i>c</i>为第二阶段的自适应分类器输出的“伪标签”.判别信息损失的第二项为</p>
                </div>
                <div class="p1">
                    <p id="121"><mathml id="263"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>i</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mover></mrow><mrow><mi>n</mi><mo>+</mo><mi>m</mi></mrow></munderover><mo stretchy="false">(</mo></mstyle><mi mathvariant="bold-italic">A</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">A</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mi>W</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mtext>t</mtext><mtext>r</mtext></mrow></math></mathml>(<i><b>A</b></i><sup>T</sup><i><b>XLX</b></i><sup>T</sup><i><b>A</b></i>),</p>
                </div>
                <div class="p1">
                    <p id="122">其中,<i><b>L</b></i>为归一化拉普拉斯矩阵,</p>
                </div>
                <div class="p1">
                    <p id="123"><i><b>L</b></i>=<i><b>I</b></i>-<i><b>D</b></i><sup>-1/2</sup><i><b>WD</b></i><sup>-1/2</sup>,</p>
                </div>
                <div class="p1">
                    <p id="124"><i><b>D</b></i>为对角矩阵,满足<mathml id="264"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>W</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><mi mathvariant="bold-italic">W</mi></mrow></math></mathml>定义为</p>
                </div>
                <div class="p1">
                    <p id="125" class="code-formula">
                        <mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>cos</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mtd><mtd columnalign="left"><mspace width="0.25em" /><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>∨</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>它</mtext></mtd></mtr></mtable></mrow></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="126">式(2)的拉格朗日函数:</p>
                </div>
                <div class="area_img" id="127">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201909001_12700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="129">其中<i>Θ</i>表示广义拉格朗日算子.记</p>
                </div>
                <div class="p1">
                    <p id="130"><i>Ω</i>=<i><b>B</b></i>+<i>α</i><i><b>R</b></i>+<i>ε</i><i><b>L</b></i>,</p>
                </div>
                <div class="p1">
                    <p id="131">令式(3)关于<i><b>A</b></i>的梯度为0,得</p>
                </div>
                <div class="p1">
                    <p id="132">(<i><b>XΩX</b></i><sup>T</sup>+<i>β</i><i><b>I</b></i><sub><i>d</i></sub>)<i><b>A</b></i>=<i><b>XHX</b></i><sup>T</sup><i>Θ</i>,      (4)</p>
                </div>
                <div class="p1">
                    <p id="133">计算式(4)最小的<i>k</i>个特征向量即可获得投影矩阵<i><b>A</b></i>.</p>
                </div>
                <h4 class="anchor-tag" id="134" name="134"><b>2.3 自适应分类器学习</b></h4>
                <div class="p1">
                    <p id="135">根据文献<citation id="406" type="reference">[<a class="sup">14</a>]</citation>,自适应分类器学习的框架为</p>
                </div>
                <div class="area_img" id="136">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201909001_13600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="138">其中,<i>σ</i>、<i>λ</i>和<i>γ</i>表示正则化参数,K表示由ϕ诱导的核函数,满足K(<i><b>x</b></i><sub><i>i</i></sub>,<i><b>x</b></i><sub><i>j</i></sub>)=〈ϕ(<i><b>x</b></i><sub><i>i</i></sub>),ϕ(<i><b>x</b></i><sub><i>j</i></sub>)〉,H<sub>K</sub>表示假设空间,‖<i>f</i>‖<sup>2</sup><sub>K</sub>表示<i>f</i>的平方范数,<i>l</i>表示平方损失函数.式(5)前两项为源域数据的结构风险,第三项为联合分布适配损失,第四项为流形正则化损失.联合分布适配损失为</p>
                </div>
                <div class="p1">
                    <p id="139" class="code-formula">
                        <mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>D</mi><msub><mrow></mrow><mrow><mi>f</mi><mo>,</mo><mtext>Κ</mtext></mrow></msub><mo stretchy="false">(</mo><mi>J</mi><msub><mrow></mrow><mi>s</mi></msub><mo>,</mo><mi>J</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>f</mi><mo>,</mo><mtext>Κ</mtext></mrow></msub><mo stretchy="false">(</mo><mi>Ρ</mi><msub><mrow></mrow><mi>s</mi></msub><mo>,</mo><mi>Ρ</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi>D</mi></mstyle><msubsup><mrow></mrow><mrow><mi>f</mi><mo>,</mo><mtext>Κ</mtext></mrow><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>Q</mi><msub><mrow></mrow><mi>s</mi></msub><mo>,</mo><mi>Q</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mo>|</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>f</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>+</mo><mi>m</mi></mrow></munderover><mi>f</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mtext>Η</mtext><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mrow><mrow><mo>|</mo><mrow><mfrac><mn>1</mn><mrow><mi>n</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>D</mi><msubsup><mrow></mrow><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></munder><mi>f</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mfrac><mn>1</mn><mrow><mi>m</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>D</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></munder><mi>f</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow></mrow></mstyle><msubsup><mrow></mrow><mtext>Η</mtext><mn>2</mn></msubsup><mo>,</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="140">其中</p>
                </div>
                <div class="p1">
                    <p id="141"><i>D</i><mathml id="265"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>={<i><b>z</b></i><sub><i>i</i></sub>∶<i><b>z</b></i><sub><i>i</i></sub>∈<i><b>Z</b></i><sub><i>s</i></sub>∧<i>y</i>(<i><b>z</b></i><sub><i>i</i></sub>)=<i>c</i>}</p>
                </div>
                <div class="p1">
                    <p id="142">表示Z中属于<i>c</i>类的源域样本集,<i>y</i>(<i><b>z</b></i><sub><i>i</i></sub>)表示源域样本<i><b>z</b></i><sub><i>i</i></sub>的真实标签,</p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>D</mtext><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mo stretchy="false">{</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∶</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>∧</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>c</mi><mo stretchy="false">}</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144">表示Z中属于<i>c</i>类的目标域样本集,<mathml id="266"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>y</mi><mo>^</mo></mover></math></mathml>(<i><b>z</b></i><sub><i>j</i></sub>)表示目标域样本<i><b>z</b></i><sub><i>j</i></sub>的伪标签.流形正则化项为</p>
                </div>
                <div class="p1">
                    <p id="145" class="code-formula">
                        <mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>Μ</mi><msub><mrow></mrow><mrow><mi>f</mi><mo>,</mo><mtext>Κ</mtext></mrow></msub><mo stretchy="false">(</mo><mi>Ρ</mi><msub><mrow></mrow><mi>s</mi></msub><mo>,</mo><mi>Ρ</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mover></mrow><mrow><mi>n</mi><mo>+</mo><mi>m</mi></mrow></munderover><mo stretchy="false">(</mo></mstyle><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mi>W</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mover></mrow><mrow><mi>n</mi><mo>+</mo><mi>m</mi></mrow></munderover><mspace width="0.25em" /></mstyle><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mi>L</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="146">根据表示定理,式(5)的最优解具有如下展开式:</p>
                </div>
                <div class="p1">
                    <p id="147"><mathml id="267"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>+</mo><mi>m</mi></mrow></munderover><mi>θ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mtext>Κ</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo></mrow></math></mathml>,      (6)</p>
                </div>
                <div class="p1">
                    <p id="148">其中,<i>θ</i>=(<i>θ</i><sub>1</sub>,<i>θ</i><sub>2</sub>,…,<i>θ</i><sub><i>n</i></sub><sub>+</sub><sub><i>m</i></sub>)<sup>T</sup>为系数向量.根据文献<citation id="407" type="reference">[<a class="sup">14</a>]</citation>方法,可求得</p>
                </div>
                <div class="p1">
                    <p id="149"><i>θ</i>=((<i><b>E</b></i>+<i>λ</i><i><b>M</b></i>+<i>γ</i><i><b>L</b></i>)<i><b>K</b></i>+<i>σ</i><i><b>I</b></i>)<sup>-1</sup><i><b>EY</b></i><sup>T</sup>,      (7)</p>
                </div>
                <div class="p1">
                    <p id="150">其中,<i><b>Y</b></i>=[<i><b>y</b></i><sub>1</sub>,<i><b>y</b></i><sub>2</sub>,…,<i><b>y</b></i><sub><i>n</i></sub><sub>+</sub><sub><i>m</i></sub>]为标签矩阵,<i><b>E</b></i>为对角形式的标签指示矩阵,满足当且仅当<i><b>x</b></i><sub><i>i</i></sub>∈D<sub><i>s</i></sub>时有<mathml id="268"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant="bold-italic">Μ</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>c</mi></munderover><mi mathvariant="bold-italic">Μ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></math></mathml>与<i><b>M</b></i><sub><i>c</i></sub>定义为</p>
                </div>
                <div class="p1">
                    <p id="151" class="code-formula">
                        <mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mfrac><mn>1</mn><mrow><mi>n</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo>,</mo></mtd><mtd columnalign="left"><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mi>s</mi></msub><mo>,</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mi>s</mi></msub></mtd><mtd columnalign="left"></mtd></mtr><mtr><mtd columnalign="left"><mfrac><mn>1</mn><mrow><mi>m</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo>,</mo></mtd><mtd columnalign="left"><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>,</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mi>t</mi></msub></mtd><mtd columnalign="left"></mtd></mtr><mtr><mtd columnalign="left"><mo>-</mo><mfrac><mn>1</mn><mrow><mi>m</mi><mi>n</mi></mrow></mfrac><mo>,</mo></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>它</mtext></mtd><mtd columnalign="left"></mtd></mtr></mtable></mrow></mrow><mspace width="0.25em" /></mtd></mtr><mtr><mtd><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">)</mo></mrow><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mfrac><mn>1</mn><mrow><mi>n</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msup><mi>n</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mo>,</mo></mtd><mtd columnalign="left"><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd columnalign="left"></mtd><mtd columnalign="left"></mtd></mtr><mtr><mtd columnalign="left"><mfrac><mn>1</mn><mrow><mi>m</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msup><mi>m</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mo>,</mo></mtd><mtd columnalign="left"><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd columnalign="left"></mtd><mtd columnalign="left"></mtd></mtr><mtr><mtd columnalign="left"><mo>-</mo><mfrac><mn>1</mn><mrow><mi>m</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msup><mi>n</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mo>,</mo></mtd><mtd columnalign="left"><mtable><mtr><mtd columnalign="left"><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup><mtext>或</mtext></mtd><mtd columnalign="left"></mtd></mtr><mtr><mtd columnalign="left"><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd columnalign="left"></mtd></mtr></mtable></mtd><mtd columnalign="left"></mtd><mtd columnalign="left"></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>,</mo></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>它</mtext></mtd><mtd columnalign="left"></mtd><mtd columnalign="left"></mtd></mtr></mtable></mrow></mrow></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="152">求得系数向量后,代入式(6)即可获取分类器<i>f</i>.</p>
                </div>
                <h4 class="anchor-tag" id="153" name="153"><b>2.4 算法步骤</b></h4>
                <div class="p1">
                    <p id="154">本文算法步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="155"><b>算法</b> TSDA</p>
                </div>
                <div class="p1">
                    <p id="156"><b>输入</b> 源域数据{<i><b>X</b></i><sub><i>s</i></sub>,<i><b>y</b></i><sub><i>s</i></sub>},目标域数据{<i><b>X</b></i><sub><i>t</i></sub>},低维空间维度<i>k</i>,最大迭代次数<i>T</i>,正则化参数<i>α</i>,<i>β</i>,<i>ε</i>,<i>σ</i>,<i>λ</i>,<i>γ</i>,<i>p</i></p>
                </div>
                <div class="p1">
                    <p id="159"><b>输出</b> 自适应分类器<i>f</i></p>
                </div>
                <div class="p1">
                    <p id="160">1. 根据定义构建矩阵<i><b>B</b></i>,<i><b>L</b></i>,初始化<i>R</i>=<i>zeros</i>(<i>n</i><sub><i>s</i></sub>+<i>n</i><sub><i>t</i></sub>,<i>n</i><sub><i>s</i></sub>+<i>n</i><sub><i>t</i></sub>),根据式(4)计算投影矩阵<i><b>A</b></i>.令[<i><b>Z</b></i><sub><i>s</i></sub>,<i><b>Z</b></i><sub><i>t</i></sub>]=<i><b>A</b></i><sup>T</sup>[<i><b>X</b></i><sub><i>s</i></sub>,<i><b>X</b></i><sub><i>t</i></sub>],在<i><b>Z</b></i><sub><i>s</i></sub>上训练一个最近邻分类器,预测<i><b>Z</b></i><sub><i>t</i></sub>的伪标签<i>Cls</i>.</p>
                </div>
                <div class="p1">
                    <p id="164">2. 当<i>t</i>≤<i>T</i>时</p>
                </div>
                <div class="p1">
                    <p id="165">3. 利用<i><b>Z</b></i><sub><i>s</i></sub>和<i><b>Z</b></i><sub><i>t</i></sub>计算核矩阵K</p>
                </div>
                <div class="p1">
                    <p id="166">4. 根据定义计算<i><b>M</b></i><sub>0</sub>和<i><b>M</b></i><sub><i>c</i></sub></p>
                </div>
                <div class="p1">
                    <p id="167">5. 根据式(7)计算<i>θ</i>,根据式(6)获得自适应分类器<i>f</i></p>
                </div>
                <div class="p1">
                    <p id="169">6. 更新目标域数据的伪标签<i>Cls</i>=<i>f</i>(<i><b>Z</b></i><sub><i>t</i></sub>)</p>
                </div>
                <div class="p1">
                    <p id="170">7. 根据定义计算<i><b>R</b></i></p>
                </div>
                <div class="p1">
                    <p id="171">8. 根据式(4)计算投影矩阵<i><b>A</b></i>,更新</p>
                </div>
                <div class="p1">
                    <p id="172">[<i><b>Z</b></i><sub><i>s</i></sub>,<i><b>Z</b></i><sub><i>t</i></sub>]=<i><b>A</b></i><sup>T</sup>[<i><b>X</b></i><sub><i>s</i></sub>,<i><b>X</b></i><sub><i>t</i></sub>]</p>
                </div>
                <div class="p1">
                    <p id="173">9. <i>t</i>=<i>t</i>+1</p>
                </div>
                <div class="p1">
                    <p id="174">10. 返回自适应分类器<i>f</i></p>
                </div>
                <div class="p1">
                    <p id="175">步骤1旨在获取目标域数据的初始伪标签.步骤 3～步骤 6对应于自适应分类器学习阶段,用于减小源域和目标域的条件分布差异.步骤 7、步骤 8对应于低维投影变换阶段,用于减小源域和目标域的边缘分布差异.上述两个阶段交替迭代,最终得到适应于源域和目标域的分类器.</p>
                </div>
                <div class="p1">
                    <p id="176">本文算法的研究框架如图1所示.</p>
                </div>
                <div class="area_img" id="177">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909001_177.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文方法框架" src="Detail/GetImg?filename=images/MSSB201909001_177.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文方法框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909001_177.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Framework of the proposed method</p>

                </div>
                <h3 id="178" name="178" class="anchor-tag">3 实验及结果分析</h3>
                <h4 class="anchor-tag" id="179" name="179"><b>3.1 实验数据</b></h4>
                <div class="p1">
                    <p id="180">实验数据集为Office-Caltech10、MNIST-USPS、ImageNet-VOC2007这3个基准图像数据集.表2列出这3个基准数据集的详细描述.</p>
                </div>
                <div class="p1">
                    <p id="181">Office-Caltech10数据集包含Office31、Caltech256数据集的10个重叠类图像.本文采用800维加速稳健特征(Speeded up Robust Features, SURF)<citation id="408" type="reference"><link href="337" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和4 096维第6全连接层深度卷积激活特征(Deep Convolution Activation Features of fc6, DeCAF)<sub>6</sub><citation id="409" type="reference"><link href="367" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,一共构造12个任务:A→W、A→D、…、C→D.</p>
                </div>
                <div class="p1">
                    <p id="182">在MNIST-USPS数据集中,USPS数据集图像大小为16×16,MNIST数据集图像大小为28×28.每个数据集都包含10个类别,每个类别是10以内的某个字符.从MNIST中随机选取2 000幅图像,从USPS中随机选取1 800幅图像<citation id="410" type="reference"><link href="369" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,分别作为源域和目标域,构造2个任务:U→M,M→U.</p>
                </div>
                <div class="p1">
                    <p id="183">ImageNet-VOC2007数据集包含ImageNet(I)和VOC2007(V),每个数据集都可以视为一个域.本文采用Fang等<citation id="411" type="reference"><link href="371" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>提供的子数据集构建跨域任务,包含5类图像:鸟、猫、椅子、狗和人.本文采用4 096维DeCAF<sub>6</sub>特征<citation id="412" type="reference"><link href="367" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,一共构造2个任务:I→V,V→I.</p>
                </div>
                <div class="area_img" id="184">
                    <p class="img_tit"><b>表2 3个基准数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Statistics of 3 benchmark datasets</p>
                    <p class="img_note"></p>
                    <table id="184" border="1"><tr><td><br />名称</td><td>子集</td><td>样本数</td><td>特征(大小)</td><td>类别数</td></tr><tr><td><br />Office-<br />Caltech10</td><td>Amazon(A)<br />Webcam(W)<br />DSLR(D)<br />Caltech(C)</td><td>958<br />295<br />157<br />1123</td><td>SURF(800)<br />DeCAF<sub>6</sub>(4096)</td><td>10</td></tr><tr><td><br />MNIST-<br />USPS</td><td>MNIST(M)<br />USPS(U)</td><td>2000<br />1800</td><td>Pixel(256)</td><td>10</td></tr><tr><td><br />ImangeNet-<br />VOC2007</td><td>ImageNet(I)<br />VOC2007(V)</td><td>7341<br />3376</td><td>DeCAF<sub>6</sub>(4096)</td><td>5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="185" name="185"><b>3.2 实验设置</b></h4>
                <div class="p1">
                    <p id="186">按照文献<citation id="413" type="reference">[<a class="sup">9</a>]</citation>方法,训练时利用所有的源域样本,对Office-Caltech10数据集的每维特征进行减均值除以方差的归一化处理,对其它数据集的每维特征进行<i>l</i><sub>2</sub>归一化处理.</p>
                </div>
                <div class="p1">
                    <p id="187">本文考虑如下12种对比方法:最近邻分类器(1-Nearest Neighbor, 1-NN)<citation id="414" type="reference"><link href="373" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>,支持向量机(Support Vector Machine, SVM)<citation id="415" type="reference"><link href="375" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>,迁移主成分分析(Transfer Component Analysis, TCA)<citation id="416" type="reference"><link href="341" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,测地流核方法(Geodesic Flow Kernel, GFK)<citation id="417" type="reference"><link href="337" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>,联合分布适配法(Joint Distribution Adaptation, JDA)<citation id="418" type="reference"><link href="343" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>,子空间对齐法(Subspace Alignment, SA)<citation id="419" type="reference"><link href="339" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>,相关性对齐法(Correlation Alignment, CORAL)<citation id="420" type="reference"><link href="377" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>,自适应正则化的迁移学习(Adaptation Regularization Based Transfer Learning, ARTL)<citation id="421" type="reference"><link href="359" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,域不变和类判别特征学习(Domain Invariant and Class Discrimina-tive Feature Learning,DICD)<citation id="422" type="reference"><link href="349" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>，几何与统计联合适配法(Joint Geometrical and Statistical Alignment,JGSA)<citation id="423" type="reference"><link href="347" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>，流形嵌入分布适配(Manifold Embedded Distribution Alignment,MEDA)<citation id="424" type="reference"><link href="361" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>，域无关聚类法(Domain-Irrelevant Class Clustering,DICE)<citation id="425" type="reference"><link href="379" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="189">1-NN、JDA、DICD、JGSA、DICE通过最近邻分类器获取伪标签,SVM、SA、CORAL通过SVM分类器获取伪标签,ATRL、MEDA通过自适应分类器获取伪标签,DICD、JGSA、DICE考虑源域、目标域样本标签判别信息的保留.</p>
                </div>
                <div class="p1">
                    <p id="190">文中对<i><b>B</b></i>和<i><b>R</b></i>均进行F范数的归一化处理,使得<i>L</i><sub>MMD</sub>和<i>L</i><sub>dis</sub>尺度大致相同,因此设置<i>α</i>=1.按照文献<citation id="426" type="reference">[<a class="sup">14</a>]</citation>方法,设置<i>T</i>=10,<i>σ</i>=0.1,<i>p</i>=10,<i>γ</i>=1.0,并且第二阶段中的核函数也与其保持一致.本文在第一阶段进行边缘分布适配,减小域间分布差异,相比文献<citation id="427" type="reference">[<a class="sup">14</a>]</citation>方法,参数<i>λ</i>应取更小的值,故设置<i>λ</i>=1.对于Office-Caltech10数据集,设置<i>k</i>=20<citation id="428" type="reference"><link href="361" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>;对于其它数据集,设置<i>k</i>=100<citation id="429" type="reference"><link href="343" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>.剩余的参数<i>β</i>,<i>ε</i>是可调的,本文在</p>
                </div>
                <div class="p1">
                    <p id="191"><i>β</i>∈{0.001,0.005,0.01,0.05,0.1,0.5,1.0,5.0,10.0},</p>
                </div>
                <div class="p1">
                    <p id="192"><i>ε</i>∈{0.001,0.005,0.01,0.05,0.1,0.5,1.0,5.0,10.0}</p>
                </div>
                <div class="p1">
                    <p id="193">内搜索最优参数.由于目标域样本无标签,无法使用标准的交叉验证选取正则化参数,故按照文献<citation id="430" type="reference">[<a class="sup">9</a>]</citation>方法,在参数空间进行网格搜索,选择性能最优的参数.</p>
                </div>
                <div class="p1">
                    <p id="194">本文提供不同数据集的最优参数.Office-Cal-tech10(SURF特征:<i>β</i>=0.5,<i>ε</i>=0.005,DeCAF<sub>6</sub>特征:<i>β</i>=0.5,<i>ε</i>=0.1),MNIST-USPS(<i>β</i>=0.5,<i>ε</i>=10.0),ImageNet-VOC2007(<i>β</i>=0.1,<i>ε</i>=0.01).</p>
                </div>
                <div class="p1">
                    <p id="196">MEDA和DICE是两个最关键的对比算法.在Office-Caltech10和MNIST-USPS数据集上,MEDA和DICE的参数设置与原论文保持一致.在ImageNet-VOC2007数据集上,MEDA的低维空间维度与原论文保持一致,设置为40.DICE的低维空间维度设置为100,<i>γ</i>在原论文给出的范围内搜索,并报告最优结果(<i>γ</i>=1.0).</p>
                </div>
                <div class="p1">
                    <p id="197">评价指标如下.1)目标域无标签测试数据的平均分类准确率:</p>
                </div>
                <div class="p1">
                    <p id="198" class="code-formula">
                        <mathml id="198"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">x</mi><mo>∶</mo><mi mathvariant="bold-italic">x</mi><mo>∈</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>t</mi></msub><mo>∧</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>y</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">x</mi><mo>∶</mo><mi mathvariant="bold-italic">x</mi><mo>∈</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>|</mo></mrow></mrow></mfrac><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="199">其中,<i>y</i>(<i><b>x</b></i>)和<mathml id="269"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>y</mi><mo>^</mo></mover></math></mathml>(<i><b>x</b></i>)为目标域测试数据<i><b>x</b></i>的真实标签和预测标签.</p>
                </div>
                <div class="p1">
                    <p id="200">2)Kappa系数:</p>
                </div>
                <div class="p1">
                    <p id="201"><mathml id="270"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><mo>=</mo><mfrac><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>o</mi></msub><mo>-</mo><mi>Ρ</mi><msub><mrow></mrow><mi>e</mi></msub></mrow><mrow><mn>1</mn><mo>-</mo><mi>Ρ</mi><msub><mrow></mrow><mi>e</mi></msub></mrow></mfrac></mrow></math></mathml>,</p>
                </div>
                <div class="p1">
                    <p id="202">其中,<i>P</i><sub><i>o</i></sub>为正确分类的样本占总样本的比例,<i>P</i><sub><i>e</i></sub>为每类真实样本数乘预测样本数之和除以总样本的平方.Kappa系数是一种对精确度更全面的度量,考虑混淆矩阵中的所有元素,而不是仅考虑正确分类数<citation id="431" type="reference"><link href="381" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>.</p>
                </div>
                <h4 class="anchor-tag" id="203" name="203"><b>3.3 分类准确率对比</b></h4>
                <div class="p1">
                    <p id="204">TSDA和12种对比方法在Office-Caltech10数据集上的分类准确率如表3所示,部分结果引自文献<citation id="432" type="reference">[<a class="sup">9</a>]</citation>、文献<citation id="433" type="reference">[<a class="sup">15</a>]</citation>和文献<citation id="434" type="reference">[<a class="sup">24</a>]</citation>,每个任务的最优结果使用黑体数字表示.</p>
                </div>
                <div class="p1">
                    <p id="205">由表3可看出,相比1-NN、SVM,绝大多数的领域自适应方法均有性能提升,表明11种领域自适应方法的有效性.对于SURF特征,TSDA在总共12个任务中的7个任务上取得最优的分类准确率,在所有任务上的平均分类准确率达到55.0%,比最优对比方法MEDA提升2.3%.</p>
                </div>
                <div class="p1">
                    <p id="206">对于DeCAF<sub>6</sub>特征, TSDA在总共12个任务中的6个任务上取得最优的分类准确率, 在所有任务上的平均分类准确率达到93.1%,比最优对比方法MEDA提升0.3%.</p>
                </div>
                <div class="area_img" id="207">
                                            <p class="img_tit">
                                                <b>表3 13种方法在Office-Caltech10数据集上分类准确率</b>
                                                    <br />
                                                Table 3 Classification accuracy of 13 methods on Office-Caltech10 dataset 
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909001_20700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201909001_20700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909001_20700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 13种方法在Office-Caltech10数据集上分类准确率" src="Detail/GetImg?filename=images/MSSB201909001_20700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="208">在所有对比方法中,MEDA与TSDA最相关.相比ARTL,MEDA进行边缘分布适配,相比JDA、DICD和JGSA,MEDA进行自适应分类器学习.由表3可看出,MEDA的性能表现明显优于上述对比方法,表明同时进行边缘分布适配和自适应分类器学习能更有效减小域间差异.</p>
                </div>
                <div class="p1">
                    <p id="209">相比MEDA,TSDA进一步考虑样本标签和数据结构的判别信息,由表3可看出,TSDA的性能表现优于MEDA,这表明样本标签和数据结构判别信息保留的有效性.</p>
                </div>
                <div class="p1">
                    <p id="210">TSDA和12种对比方法在MNIST-USPS数据集上的分类准确率如表4所示,部分结果引自文献<citation id="435" type="reference">[<a class="sup">15</a>]</citation>和文献<citation id="436" type="reference">[<a class="sup">24</a>]</citation>,每个任务的最优结果使用黑体数字标出.</p>
                </div>
                <div class="area_img" id="211">
                    <p class="img_tit"><b>表4 13种方法在MNIST-USPS数据集上分类准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Classification accuracy of 13 methods on MNIST-USPS dataset </p>
                    <p class="img_note"></p>
                    <table id="211" border="1"><tr><td>任务</td><td>1-NN</td><td>SVM</td><td>TCA</td><td>GFK</td><td>JDA</td><td>SA</td><td>ARTL</td><td>CORAL</td><td>DICD</td><td>JGSA</td><td>MEDA</td><td>DICE</td><td>TSDA</td></tr><tr><td>M→U</td><td>65.9</td><td>48.8</td><td>62.3</td><td>68.6</td><td>70.6</td><td>45.3</td><td><b>88.8</b></td><td>35.8</td><td>77.8</td><td>80.4</td><td>88.2</td><td>76.9</td><td>88.4</td></tr><tr><td><br />U→M</td><td>44.7</td><td>27.8</td><td>50.3</td><td>50.1</td><td>60.0</td><td>29.5</td><td>67.7</td><td>36.4</td><td>65.2</td><td>68.2</td><td>67.5</td><td>64.8</td><td><b>74.1</b></td></tr><tr><td><br />平均值</td><td>55.3</td><td>38.3</td><td>56.3</td><td>59.4</td><td>65.3</td><td>37.4</td><td>78.3</td><td>36.1</td><td>71.5</td><td>74.3</td><td>77.9</td><td>70.9</td><td><b>81.3</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="212">由表4可看出,TSDA在2个任务上的平均分类准确率达到81.3%,相比最优对比方法ARTL提升3.0%.ARTL与MEDA均进行自适应分类器学习,由表可看出,这两种方法的性能表现明显优于其它对比方法,表明自适应分类器学习在该数据集上的重要性.</p>
                </div>
                <div class="p1">
                    <p id="213">相比ARTL,MEDA性能未提升,表明MEDA学得的低维特征表示在一定程度上丢失原始数据信息. 相 比ARTL,TSDA虽 然 也 在 一定程度上丢失原始数据信息,但是,由于TSDA考虑样本标签和数据结构的判别信息,能取得更优的性能表现,这进一步验证判别信息保留的重要性.</p>
                </div>
                <div class="p1">
                    <p id="214">TSDA和12种对比方法在ImageNet-VOC2007数据集上的分类准确率如表5所示,每个任务的最优结果使用黑体数字标出.由表可看出,TSDA在V→I任务上的表现显著优于对比方法,在这2个任务上的平均分类准确率达到75.7%,比最优对比方法提升1.4%.</p>
                </div>
                <div class="area_img" id="215">
                    <p class="img_tit"><b>表5 13种方法在ImageNet-VOC2007数据集上分类准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Classification accuracy of 13 methods on ImageNet-VOC2007 dataset </p>
                    <p class="img_note"></p>
                    <table id="215" border="1"><tr><td>任务</td><td>1-NN</td><td>SVM</td><td>TCA</td><td>GFK</td><td>JDA</td><td>SA</td><td>ARTL</td><td>CORAL</td><td>DICD</td><td>JGSA</td><td>MEDA</td><td>DICE</td><td>TSDA</td></tr><tr><td>I→V</td><td>65.4</td><td>68.2</td><td>62.3</td><td>61.7</td><td>61.8</td><td>69.7</td><td>64.7</td><td><b>69.9</b></td><td>54.4</td><td>65.8</td><td>64.7</td><td>65.6</td><td>66.0</td></tr><tr><td><br />V→I</td><td>73.7</td><td>80.5</td><td>71.1</td><td>71.1</td><td>68.7</td><td>76.4</td><td>78.5</td><td>78.7</td><td>75.8</td><td>72.2</td><td>76.4</td><td>75.8</td><td><b>85.3</b></td></tr><tr><td><br />平均值</td><td>69.6</td><td>74.3</td><td>66.7</td><td>66.4</td><td>65.3</td><td>73.1</td><td>71.6</td><td>74.3</td><td>70.4</td><td>69.0</td><td>70.6</td><td>70.7</td><td><b>75.7</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="216" name="216"><b>3.4 Kappa系数对比</b></h4>
                <div class="p1">
                    <p id="217">TSDA和12种对比方法在Office-Caltech10数据集上的Kappa系数如表6所示.对于SURF特征和DeCAF<sub>6</sub>特征, TSDA在所有任务上的平均Kappa系数比最优对比方法MEDA分别提升2.6%和1.4%.由表2可以看出,Office-Caltech10存在较明显的样本不平衡问题,在这种情况下,TSDA仍能取得最优的性能表现,进一步表明TSDA的优异性.</p>
                </div>
                <div class="area_img" id="218">
                                            <p class="img_tit">
                                                <b>表6 13种方法在Office-Caltech10数据集上Kappa系数</b>
                                                    <br />
                                                Table 6 Kappa index of 13 methods on Office-Caltech10 dataset
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909001_21800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201909001_21800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note"> %</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909001_21800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表6 13种方法在Office-Caltech10数据集上Kappa系数" src="Detail/GetImg?filename=images/MSSB201909001_21800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="219">TSDA和12种对比方法在MNIST-USPS数据集上的Kappa系数如表7所示.</p>
                </div>
                <div class="p1">
                    <p id="220">由表7可看出,TSDA在U→M任务上的表现显著优于对比方法,在这2个任务上的平均Kappa系数达到79.0%,相比最优对比方法ARTL,提升3.3%.</p>
                </div>
                <div class="area_img" id="221">
                    <p class="img_tit"><b>表7 13种方法在MNIST-USPS数据集上Kappa系数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 7 Kappa index of 13 methods on MNIST-USPS dataset </p>
                    <p class="img_note"></p>
                    <table id="221" border="1"><tr><td>任务</td><td>1-NN</td><td>SVM</td><td>TCA</td><td>GFK</td><td>JDA</td><td>SA</td><td>ARTL</td><td>CORAL</td><td>DICD</td><td>JGSA</td><td>MEDA</td><td>DICE</td><td>TSDA</td></tr><tr><td>M→U</td><td>61.4</td><td>43.0</td><td>57.5</td><td>64.5</td><td>66.9</td><td>44.9</td><td><b>87.3</b></td><td>44.7</td><td>74.9</td><td>77.9</td><td>86.6</td><td>74.1</td><td>86.9</td></tr><tr><td><br />U→M</td><td>38.4</td><td>19.7</td><td>44.9</td><td>44.5</td><td>55.6</td><td>21.8</td><td>64.1</td><td>17.5</td><td>61.3</td><td>64.6</td><td>63.9</td><td>60.9</td><td><b>71.1</b></td></tr><tr><td><br />平均值</td><td>49.9</td><td>31.4</td><td>51.2</td><td>54.5</td><td>61.3</td><td>33.3</td><td>75.7</td><td>31.1</td><td>68.1</td><td>71.2</td><td>75.3</td><td>67.5</td><td><b>79.0</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="222">TSDA和12种对比方法在ImageNet-VOC2007数据集上的Kappa系数如表8所示.</p>
                </div>
                <div class="p1">
                    <p id="223">由表8可看出, TSDA在V→I任务上显 著 优 于对比方法,在这2个任务上的平均Kappa系数达到68.9%,比最优对比方法CORAL提升2.2%,表明TSDA的优异性.</p>
                </div>
                <div class="area_img" id="224">
                    <p class="img_tit"><b>表8 13种方法在ImageNet-VOC2007数据集Kappa系数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 8 Kappa index of 13 methods on ImageNet-VOC2007 dataset</p>
                    <p class="img_note"></p>
                    <table id="224" border="1"><tr><td>任务</td><td>1-NN</td><td>SVM</td><td>TCA</td><td>GFK</td><td>JDA</td><td>SA</td><td>ARTL</td><td>CORAL</td><td>DICD</td><td>JGSA</td><td>MEDA</td><td>DICE</td><td>TSDA</td></tr><tr><td>I→V</td><td>54.6</td><td>58.5</td><td>50.9</td><td>50.3</td><td>50.5</td><td>60.1</td><td>54.9</td><td><b>60.3</b></td><td>54.3</td><td>55.4</td><td>55.1</td><td>55.0</td><td>56.4</td></tr><tr><td>V→I</td><td>66.6</td><td>75.2</td><td>63.7</td><td>63.3</td><td>60.7</td><td>70.2</td><td>73.1</td><td>73.0</td><td>68.7</td><td>65.2</td><td>70.5</td><td>69.5</td><td><b>81.4</b></td></tr><tr><td><br />平均值</td><td>60.6</td><td>66.9</td><td>57.3</td><td>56.8</td><td>55.6</td><td>65.2</td><td>64.0</td><td>66.7</td><td>61.5</td><td>62.4</td><td>62.8</td><td>62.3</td><td><b>68.9</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="225" name="225"><b>3.5 有效性分析</b></h4>
                <div class="p1">
                    <p id="226">本节首先验证低维投影变换和自适应分类器学习两个阶段的有效性,然后验证样本标签和数据结构判别信息保留的有效性.</p>
                </div>
                <div class="p1">
                    <p id="227">低维投影变换的有效性验证如下.首先从3个数据集中随机选择4个任务(从Office-Caltech10数据集的SURF特征、DeCAF<sub>6</sub>特征、MNIST-USPS数据集和ImageNet-VOC2007数据集上各选择一个任务),然后在原始空间和Z中运行TSDA.表9给出各个任务的分类准确率和性能提升效果(注意,此处性能提升指的是相对提升,例如,在C→A任务上性能提升为(61.0-50.7)/50.7×100%≈20.3%).由表可看出,所有任务在低维投影变换后,均能获得性能提升.上述结果表明本文方法的有效性.</p>
                </div>
                <div class="area_img" id="228">
                    <p class="img_tit"><b>表9 原始空间和</b>Z<b>中分类准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 9 Classification accuracy in original space and Z space </p>
                    <p class="img_note"></p>
                    <table id="228" border="1"><tr><td><br />任务</td><td>原始空间</td><td>低维空间</td><td>性能提升</td></tr><tr><td><br />C→A</td><td>50.7</td><td>61.0</td><td>20.3</td></tr><tr><td><br />A→D(DeCAF<sub>6</sub>)</td><td>88.5</td><td>96.2</td><td>8.7</td></tr><tr><td><br />U→M</td><td>69.0</td><td>74.1</td><td>7.4</td></tr><tr><td><br />V→I</td><td>82.3</td><td>85.3</td><td>3.6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="229">自适应分类器学习的有效性验证如下.对于上述4个任务,使用最近邻分类器代替自适应分类器.表10给出各任务的分类准确率和性能提升效果.由表可看出,所有任务在经过自适应分类器学习后,均能获得性能提升,在U→M任务上的性能提升甚至达到59.0%.上述结果表明自适应分类器学习的有效性.</p>
                </div>
                <div class="area_img" id="230">
                    <p class="img_tit"><b>表10 最近邻分类器与自适应分类器的分类准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 10 Classification accuracy of nearest neighbor classifier and adaptive classifier </p>
                    <p class="img_note"></p>
                    <table id="230" border="1"><tr><td><br />任务</td><td>最近邻分类器</td><td>自适应分类器</td><td>性能提升</td></tr><tr><td><br />C→A</td><td>50.4</td><td>61.0</td><td>21.0</td></tr><tr><td><br />A→D(DeCAF6)</td><td>86.0</td><td>96.2</td><td>11.9</td></tr><tr><td><br />U→M</td><td>46.6</td><td>74.1</td><td>59.0</td></tr><tr><td><br />V→I</td><td>77.6</td><td>85.3</td><td>9.9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="231">样本标签和数据结构判别信息保留的有效性验证如下.在上述4个任务上运行如下两种算法:不考虑最小化<i>p</i>近邻样本距离和不考虑最小化同类样本距离.实验结果如表11所示.由表可看出,TSDA在4个任务上的平均分类准确率最高,表明样本标签和数据结构判别信息保留的有效性.</p>
                </div>
                <div class="area_img" id="232">
                    <p class="img_tit"><b>表11 不考虑样本标签或数据结构判别信息时的分类准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 11 Classification accuracy without consideration of sample labels or data structure</p>
                    <p class="img_note"></p>
                    <table id="232" border="1"><tr><td><br />任务</td><td>不考虑最小化<i>p</i><br />近邻样本距离</td><td>不考虑最小化<br />同类样本距离</td><td>TSDA</td></tr><tr><td><br />C→A</td><td>60.8</td><td>55.3</td><td>61.0</td></tr><tr><td><br />A→D<br />(DeCAF<sub>6</sub>)</td><td>90.4</td><td>91.7</td><td>96.2</td></tr><tr><td><br />U→M</td><td>63.2</td><td>75.0</td><td>74.1</td></tr><tr><td><br />V→I</td><td>83.2</td><td>85.8</td><td>85.3</td></tr><tr><td><br />平均值</td><td>74.4</td><td>77.0</td><td>79.2<br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="233" name="233"><b>3.6 参数敏感性分析与算法收敛性分析</b></h4>
                <div class="p1">
                    <p id="234">低维空间维度<i>k</i>和正则化参数<i>β</i>、<i>ε</i>、<i>λ</i>为本文方法中较重要参数,参数灵敏度分析结果如图2所示.</p>
                </div>
                <div class="p1">
                    <p id="235">图2中虚线表示最优对比方法的性能表现,由图可看出,TSDA对<i>β</i>、<i>ε</i>较敏感,而对<i>k</i>、<i>λ</i>的敏感度较低,能在较大范围(<i>k</i>∈[20,100],<i>λ</i>∈[0.5,10])内获得比最优对比方法更好的分类准确率.</p>
                </div>
                <div class="p1">
                    <p id="236">图3给出TSDA的收敛性分析,由图可看出,本文方法的收敛速率较快.</p>
                </div>
                <div class="area_img" id="239">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909001_23900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 k、 β、ε和λ的敏感性分析" src="Detail/GetImg?filename=images/MSSB201909001_23900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 <i>k</i>、 <i>β</i>、<i>ε</i>和<i>λ</i>的敏感性分析  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909001_23900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Sensitivity analysis of <i>k</i>,<i>β</i>,<i>ε</i> and <i>λ</i></p>

                </div>
                <div class="area_img" id="240">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909001_240.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 TSDA收敛性分析" src="Detail/GetImg?filename=images/MSSB201909001_240.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 TSDA收敛性分析  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909001_240.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Convergence analysis of TSDA</p>

                </div>
                <h3 id="241" name="241" class="anchor-tag">4 结 束 语</h3>
                <div class="p1">
                    <p id="242">针对领域自适应中联合分布差异最小化问题,本文提出两阶段方法(TSDA),对联合分布差异进行更精准的估计.本文方法包含特征适配和分类器适配两部分,特征适配能通过特征的低维投影最小化边缘分布差异.在此过程中,本文进一步通过最小化投影后的同类样本间距离,保留样本标签的判别性.通过最小化投影前后<i>p</i>近邻样本的流形差异,保留样本结构的判别性.不同于现有近似替代方法,分类器适配能直接最小化条件分布差异.在3个领域自适应公开数据集(Office-Caltech10、MNIST-USPS、ImageNet-VOC2007)上进行的实验表明,TSDA性能优于现有大多数对比方法.此外,有效性分析表明,两个阶段中每个都必不可少,均能有效促进域间联合分布差异的缩小.今后进一步研究方向包括:1)邻接矩阵计算方式的影响;2)将该模型推广至非线性变换的情况.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="333">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201501003&amp;v=MjkxMzJPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tWcjNBTnlmVGJMRzRIOVRNcm85Rlo0UUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 庄福振,罗平,何清,等.迁移学习研究进展.软件学报,2015,26(1):26-39.(ZHUANG F Z,LUO P,HE Q,et al.Survey on Transfer Learning Research.Journal of Software,2015,26(1):26-39.)
                            </a>
                        </p>
                        <p id="335">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Domain Adaptation for Object Recognition:An Unsupervised Approach">

                                <b>[2]</b> GOPALAN R,LI R N,CHELLAPPA R.Domain Adaptation for Object Recognition:An Unsupervised Approach // Proc of the IEEE Conference on Computer Vision.Washington,USA:IEEE,2011:999-1006.
                            </a>
                        </p>
                        <p id="337">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Geodesic flow kernel for unsupervised domain adaptation">

                                <b>[3]</b> GONG B Q,SHI Y,SHA F,et al.Geodesic Flow Kernel for Unsupervised Domain Adaptation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2012:2066-2073.
                            </a>
                        </p>
                        <p id="339">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised Visual Domain Adaptation Using Subspace Alignment">

                                <b>[4]</b> FERNANDO B,HABRAD A,SEBBAN M,et al.Unsupervised Visual Domain Adaptation Using Subspace Alignment // Proc of the IEEE Conference on Computer Vision.Washington,USA:IEEE,2013:2960-2967.
                            </a>
                        </p>
                        <p id="341">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Domain Adaptation via Transfer Component Analysis">

                                <b>[5]</b> PAN S J,TSANG I W,KWOK J T,et al.Domain Adaptation via Transfer Component Analysis.IEEE Transaction on Neural Networks,2011,22(2):199-210.
                            </a>
                        </p>
                        <p id="343">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transfer feature learning with joint distribution adaptation">

                                <b>[6]</b> LONG M S,WANG J M,DING G G,et al.Transfer Feature Lear-ning with Joint Distribution Adaptation // Proc of the IEEE Confe-rence on Computer Vision.Washington,USA:IEEE,2013:2200-2207.
                            </a>
                        </p>
                        <p id="345">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Model Selection with Nonlinear Embedding for Unsupervised Domain Adaptation[C/OL]">

                                <b>[7]</b> VENKATESWARA H,CHAKRABORTY S,MCDANIEL T,et al.Model Selection with Nonlinear Embedding for Unsupervised Domain Adaptation[C/OL].[2019-05-15].https://arxiv.org/pdf/1706.07527.pdf.
                            </a>
                        </p>
                        <p id="347">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint Geometrical and Statistical Alignment for Visual Domain Adaptation">

                                <b>[8]</b> ZHANG J,LI W Q,OGUNBONA P.Joint Geometrical and Statistical Alignment for Visual Domain Adaptation // Proc of the IEEE Conference on Computer Vision.Washington,USA:IEEE,2017:5150-5158.
                            </a>
                        </p>
                        <p id="349">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Domain Invariant and Class Discriminative Feature Learning for Visual Domain Adaptation">

                                <b>[9]</b> LI S,SONG S J,HUANG G,et al.Domain Invariant and Class Discriminative Feature Learning for Visual Domain Adaptation.IEEE Transaction on Image Processing,2018,27(9):4260-4273.
                            </a>
                        </p>
                        <p id="351">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-domain video concept detection using adaptive SVMS">

                                <b>[10]</b> YANG J,YAN R,HAUPTMANN A G.Cross-Domain Video Concept Detection Using Adaptive SVMs // Proc of the 15th ACM International Conference on Multimedia.New York,USA:ACM,2007:188-197.
                            </a>
                        </p>
                        <p id="353">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised Domain Adaptation with Subspace Learning for Visual Recognition">

                                <b>[11]</b> YAO T,PAN Y W,NGO C W,et al.Semi-supervised Domain Adaptation with Subspace Learning for Visual Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:2142-2150.
                            </a>
                        </p>
                        <p id="355">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust visual knowledge transfer via extreme learning machinebased domain adaptation">

                                <b>[12]</b> ZHANG L,ZHANG D.Robust Visual Knowledge Transfer via Extreme Learning Machine-Based Domain Adaptation.IEEE Transaction on Image Processing,2016,25(10):4959-4973.
                            </a>
                        </p>
                        <p id="357">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Maybank.Domain transfer svm for video concept detection">

                                <b>[13]</b> DUAN L X,TSANG I W,XU D,et al.Domain Transfer SVM for Video Concept Detection // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2009:1375-1381.
                            </a>
                        </p>
                        <p id="359">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptation regularization:A general framework for transfer learning">

                                <b>[14]</b> LONG M S,WANG J M,DING G G,et al.Adaptation Regulari-zation:A General Framework for Transfer Learning.IEEE Transa-ction on Knowledge and Data Engineering,2014,26(5):1076-1089.
                            </a>
                        </p>
                        <p id="361">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual Domain Adaptation with Manifold Embedded Distribution Alignment">

                                <b>[15]</b> WANG J D,FENG W J,CHEN Y Q,et al.Visual Domain Adaptation with Manifold Embedded Distribution Alignment // Proc of the 26th ACM International Conference on Multimedia.New York,USA:ACM,2018:402-410.
                            </a>
                        </p>
                        <p id="363">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Survey on Transfer Learning">

                                <b>[16]</b> PAN S J,YANG Q.A Survey on Transfer Learning.IEEE Trans-actions on Knowledge and Data Engineering,2010,22(10):1345-1359.
                            </a>
                        </p>
                        <p id="365">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transfer learning via dimensionality reduction">

                                <b>[17]</b> PAN S J,KWOK J T,YANG Q.Transfer Learning via Dimensionality Reduction // Proc of the 23rd National Conference on Artificial Intelligence.Palo Alto,USA:AAAI Press,2008,II:677-682.
                            </a>
                        </p>
                        <p id="367">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=De CAF:A Deep Convolutional Activation Feature for Generic Visual Recognition">

                                <b>[18]</b> DONAHUE J,JIA Y Q,VINYALS O,et al.DeCAF:A Deep Convolutional Activation Feature for Generic Visual Recognition // Proc of the 31st International Conference on Machine Learning.New York,USA:ACM,2014:647-655.
                            </a>
                        </p>
                        <p id="369">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1015039180.nh&amp;v=MjQ1MTlHRnJDVVJMT2VaZVJuRnkva1ZyM0FWRjI2RzdPN0Y5REVyNUViUElRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> 龙明盛.迁移学习问题与方法研究.博士学位论文.北京:清华大学,2014.(LONG M S.Transfer Learning:Problems and Methods.Ph.D.Dissertation.Beijing,China:Tsinghua University,2014.)
                            </a>
                        </p>
                        <p id="371">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unbiased Metric Learning:On the Utilization of Multiple Datasets and Web Images for Softe-ning Bias">

                                <b>[20]</b> FANG C,XU Y,ROCKMORE D N.Unbiased Metric Learning:On the Utilization of Multiple Datasets and Web Images for Softe-ning Bias // Proc of the IEEE International Conference on Compu-ter Vision.Washington,USA:IEEE,2013:1657-1664.
                            </a>
                        </p>
                        <p id="373">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A branch and bound algorithm for computing k-nearest neighbors">

                                <b>[21]</b> FUKUNAGA K,NARENDRA P M.A Branch and Bound Algorithm for Computing k-Nearest Neighbors.IEEE Transactions on Computers,1975,24(7):750-753.
                            </a>
                        </p>
                        <p id="375">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Overview of statistical learning theory">

                                <b>[22]</b> VAPNIK V N.An Overview of Statistical Learning Theory.IEEE Transaction on Neural Networks,1999,10(5):988-999.
                            </a>
                        </p>
                        <p id="377">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Return of Frustratingly Easy Domain Adaptation">

                                <b>[23]</b> SUN B C,FENG J S,SAENKO K.Return of Frustratingly Easy Domain Adaptation // Proc of the 30th Conference on Artificial Intelligence.Palo Alto,USA:AAAI Press,2016:2058-2065.
                            </a>
                        </p>
                        <p id="379">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Aggregating Randomized Clustering-Promoting Invariant Projections for Domain Adaptation">

                                <b>[24]</b> LIANG J,HE R,SUN Z N,et al.Aggregating Randomized Clustering-Promoting Invariant Projections for Domain Adaptation.IEEE Transaction on Pattern Analysis and Machine Intelligence,2019,41(5):1027-1042.
                            </a>
                        </p>
                        <p id="381">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES4CA015BECF2CC4750F6554B8BD8DC0E7&amp;v=MjU0MzgrWUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHc3bTZ3cUE9TmlmT2ZiZkxiOUhOcXYwd0Y1ME5mdzg5eUJNVG5EbDRUWHVRcEdCQmNjYm5SYw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> VOGADO L H S,VERAS R M S,ARAUJO F H D,et al.Leuke-mia Diagnosis in Blood Slides Using Transfer Learning in CNNs and SVM for Classification.Engineering Applications of Artificial Inte-lligence,2018,72:415-422.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201909001" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201909001&amp;v=MjQ4MzdyM0FLRDdZYkxHNEg5ak1wbzlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
