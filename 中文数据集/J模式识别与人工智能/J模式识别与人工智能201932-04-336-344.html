<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131444052373750%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dMSSB201904008%26RESULT%3d1%26SIGN%3dLagfNoBmiUazUvNrsIF967jjEqg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201904008&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201904008&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201904008&amp;v=MjYwMDdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6Z1VyN01LRDdZYkxHNEg5ak1xNDlGYklRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#80" data-title="1 稀疏化双线性卷积神经网络模型 ">1 稀疏化双线性卷积神经网络模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#85" data-title="&lt;b&gt;1.1&lt;/b&gt; 双线性卷积神经网络模型"><b>1.1</b> 双线性卷积神经网络模型</a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;1.2&lt;/b&gt; 通道级稀疏与剪枝的实现"><b>1.2</b> 通道级稀疏与剪枝的实现</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#136" data-title="2 实验及结果分析 ">2 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#140" data-title="&lt;b&gt;2.1&lt;/b&gt; 在&lt;b&gt;FGVC-aircraft&lt;/b&gt;数据集上的分类实验"><b>2.1</b> 在<b>FGVC-aircraft</b>数据集上的分类实验</a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;2.2&lt;/b&gt; 在&lt;b&gt;Stanford dogs&lt;/b&gt; 数据集上的分类实验"><b>2.2</b> 在<b>Stanford dogs</b> 数据集上的分类实验</a></li>
                                                <li><a href="#156" data-title="&lt;b&gt;2.3&lt;/b&gt; 在&lt;b&gt;Stanford cars&lt;/b&gt;数据集上的分类实验"><b>2.3</b> 在<b>Stanford cars</b>数据集上的分类实验</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#161" data-title="3 结 束 语 ">3 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#83" data-title="图1 稀疏化的双线性卷积神经网络结构">图1 稀疏化的双线性卷积神经网络结构</a></li>
                                                <li><a href="#87" data-title="图2 双线性卷积神经网络结构">图2 双线性卷积神经网络结构</a></li>
                                                <li><a href="#200" data-title="图3 双线性卷积神经网络的前向运算方式">图3 双线性卷积神经网络的前向运算方式</a></li>
                                                <li><a href="#122" data-title="图4 稀疏剪枝操作过程">图4 稀疏剪枝操作过程</a></li>
                                                <li><a href="#133" data-title="图5 双线性卷积神经网络梯度计算图">图5 双线性卷积神经网络梯度计算图</a></li>
                                                <li><a href="#135" data-title="图6 双线性卷积神经网络算法改进流程图">图6 双线性卷积神经网络算法改进流程图</a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;表1 细粒度数据集&lt;/b&gt;"><b>表1 细粒度数据集</b></a></li>
                                                <li><a href="#145" data-title="图7 第一步粗训练曲线图 (仅训练B-CNN最后一层分类层) ">图7 第一步粗训练曲线图 (仅训练B-CNN最后一层分类层) </a></li>
                                                <li><a href="#146" data-title="图8 第二步微调曲线图 (训练B-CNN全部权重和比例因子) ">图8 第二步微调曲线图 (训练B-CNN全部权重和比例因子) </a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;表2 各方法在FGVC-aircraft数据集上的准确率对比&lt;/b&gt;"><b>表2 各方法在FGVC-aircraft数据集上的准确率对比</b></a></li>
                                                <li><a href="#154" data-title="&lt;b&gt;表3 各方法在Stanford Dogs数据集上的准确率对比&lt;/b&gt;"><b>表3 各方法在Stanford Dogs数据集上的准确率对比</b></a></li>
                                                <li><a href="#160" data-title="&lt;b&gt;表4 各方法在Stanford Cars数据集上的准确率对比&lt;/b&gt;"><b>表4 各方法在Stanford Cars数据集上的准确率对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" FARRELL R, OZA O, ZHANG N, &lt;i&gt;et al&lt;/i&gt;.Birdlets:Subordinate Categorization Using Volumetric Primitives and Pose-Normalized Appearance // Proc of the International Conference on Computer Vision.Washington, USA:IEEE, 2011:161-168." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Birdlets: Subordinate categorization using volumetric primi-tives and pose-normalized appearance">
                                        <b>[1]</b>
                                         FARRELL R, OZA O, ZHANG N, &lt;i&gt;et al&lt;/i&gt;.Birdlets:Subordinate Categorization Using Volumetric Primitives and Pose-Normalized Appearance // Proc of the International Conference on Computer Vision.Washington, USA:IEEE, 2011:161-168.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" ZHANG N, FARRELL R, DARRELL T.Pose Pooling Kernels for Sub-category Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2012:3665-3672." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pose Pooling Kernels for Sub-Category Recognition">
                                        <b>[2]</b>
                                         ZHANG N, FARRELL R, DARRELL T.Pose Pooling Kernels for Sub-category Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2012:3665-3672.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 罗建豪, 吴建鑫.基于深度卷积特征的细粒度图像分类研究综述.自动化学报, 2017, 43 (8) :1306-1318. (LUO J H, WU J X.A Survey on Fine-Grained Image Categorization Using Deep Convolutional Features.Acta Automatica Sinica, 2017, 43 (8) :1306-1318.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201708002&amp;v=MTM2NTh0R0ZyQ1VSTE9lWmVSbkZ5emdVcjdNS0NMZlliRzRIOWJNcDQ5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         罗建豪, 吴建鑫.基于深度卷积特征的细粒度图像分类研究综述.自动化学报, 2017, 43 (8) :1306-1318. (LUO J H, WU J X.A Survey on Fine-Grained Image Categorization Using Deep Convolutional Features.Acta Automatica Sinica, 2017, 43 (8) :1306-1318.) 
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" CUI Y, SONG Y, SUN C, &lt;i&gt;et al&lt;/i&gt;.Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:4109-4118." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning">
                                        <b>[4]</b>
                                         CUI Y, SONG Y, SUN C, &lt;i&gt;et al&lt;/i&gt;.Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:4109-4118.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" WU L, WANG Y, LI X, &lt;i&gt;et al&lt;/i&gt;.Deep Attention-Based Spatially Recursive Networks for Fine-Grained Visual Recognition.IEEE Tran-sactions on Cybernetics, 2019, 49 (5) :1791-1802." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Attention-Based Spatially Recursive Networks for Fine-Grained Visual Recognition">
                                        <b>[5]</b>
                                         WU L, WANG Y, LI X, &lt;i&gt;et al&lt;/i&gt;.Deep Attention-Based Spatially Recursive Networks for Fine-Grained Visual Recognition.IEEE Tran-sactions on Cybernetics, 2019, 49 (5) :1791-1802.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" LIN T Y, ROYCHOWDHURY A, MAJI S.Bilinear CNN Models for Fine-Grained Visual Recognition // Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:1449-1457." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bilinear CNN models for fine-grained visual recognition">
                                        <b>[6]</b>
                                         LIN T Y, ROYCHOWDHURY A, MAJI S.Bilinear CNN Models for Fine-Grained Visual Recognition // Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:1449-1457.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" >
                                        <b>[7]</b>
                                     SIMONYAN K, ZISSERMAN A.Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL].[2018-12-06].https://arxiv.org/pdf/1409.1556.pdf.</a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" LECUN Y, DENKER J S, SOLLA S A.Optimal Brain Damage // TOURETZKY D S, ed.Advances in Neural Information Processing Systems 2.San Francisco, USA:Morgan Kaufmann Publishers, 1990:598-605." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimal brain damage">
                                        <b>[8]</b>
                                         LECUN Y, DENKER J S, SOLLA S A.Optimal Brain Damage // TOURETZKY D S, ed.Advances in Neural Information Processing Systems 2.San Francisco, USA:Morgan Kaufmann Publishers, 1990:598-605.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" HINTON G E, SRIVASTAVA N, KRIZHEVSKY A, &lt;i&gt;et al&lt;/i&gt;.Improving Neural Networks by Preventing Co-adaptation of Feature Detectors[C/OL].[2018-12-06].https://arxiv.org/pdf/1207.0580.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving Neural Networks by Preventing Co-adaptation of Feature Detectors">
                                        <b>[9]</b>
                                         HINTON G E, SRIVASTAVA N, KRIZHEVSKY A, &lt;i&gt;et al&lt;/i&gt;.Improving Neural Networks by Preventing Co-adaptation of Feature Detectors[C/OL].[2018-12-06].https://arxiv.org/pdf/1207.0580.pdf.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" QUINLAN J R.Bagging, Boosting, and C4.5 // Proc of the 13th National Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 1996, I:725-730." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bagging, boosting, and C4. 5">
                                        <b>[10]</b>
                                         QUINLAN J R.Bagging, Boosting, and C4.5 // Proc of the 13th National Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 1996, I:725-730.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" IOFFE S, SZEGEDY C.Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift[C/OL].[2018-12-06].https://arxiv.org/pdf/1502.03167v3.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:accelerating deep network training by reducing internal covariate shift">
                                        <b>[11]</b>
                                         IOFFE S, SZEGEDY C.Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift[C/OL].[2018-12-06].https://arxiv.org/pdf/1502.03167v3.pdf.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 曹文龙, 芮建武, 李敏.神经网络模型压缩方法综述.计算机应用研究, 2019, 36 (3) :649-656. (CAO W L, BING J W, LI M.Survey on Neural Network Model Compression Methods.Application Research of Computers, 2019, 36 (3) :649-656.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201903003&amp;v=MDc0MzlNTHo3U1pMRzRIOWpNckk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emdVcjc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         曹文龙, 芮建武, 李敏.神经网络模型压缩方法综述.计算机应用研究, 2019, 36 (3) :649-656. (CAO W L, BING J W, LI M.Survey on Neural Network Model Compression Methods.Application Research of Computers, 2019, 36 (3) :649-656.) 
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" DENIL M, SHAKIBI B, DINH L, &lt;i&gt;et al&lt;/i&gt;.Predicting Parameters in Deep Learning[C/OL].[2018-12-06].https://arxiv.org/pdf/1306.0543.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Predicting Parameters in Deep Learning[C/OL]">
                                        <b>[13]</b>
                                         DENIL M, SHAKIBI B, DINH L, &lt;i&gt;et al&lt;/i&gt;.Predicting Parameters in Deep Learning[C/OL].[2018-12-06].https://arxiv.org/pdf/1306.0543.pdf.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" HAN S, MAO H Z, DALLY W J.Deep Compression:Compre-ssing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding[C/OL].[2018-12-06].https://arxiv.org/pdf/1510.00149.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep compression:compressing deep neural networks with pruning trained quantization and huffman coding">
                                        <b>[14]</b>
                                         HAN S, MAO H Z, DALLY W J.Deep Compression:Compre-ssing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding[C/OL].[2018-12-06].https://arxiv.org/pdf/1510.00149.pdf.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" WEN W, WU C P, WANG Y D, &lt;i&gt;et al&lt;/i&gt;.Learning Structured Sparsity in Deep Neural Networks[C/OL].[2018-12-06].https://arxiv.org/pdf/1608.03665.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Structured Sparsity in Deep Neural Networks[C/OL]">
                                        <b>[15]</b>
                                         WEN W, WU C P, WANG Y D, &lt;i&gt;et al&lt;/i&gt;.Learning Structured Sparsity in Deep Neural Networks[C/OL].[2018-12-06].https://arxiv.org/pdf/1608.03665.pdf.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" LIU Z, LI J G, SHEN Z Q, &lt;i&gt;et al&lt;/i&gt;.Learning Efficient Convolutional Networks through Network Slimming // Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2017:2755-2763." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning efficient convolutional networks through network slimming">
                                        <b>[16]</b>
                                         LIU Z, LI J G, SHEN Z Q, &lt;i&gt;et al&lt;/i&gt;.Learning Efficient Convolutional Networks through Network Slimming // Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2017:2755-2763.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" LI H, KADAV A, DURDANOVIC I, &lt;i&gt;et al&lt;/i&gt;.Pruning Filters for Efficient Convents[C/OL].[2018-12-06].https://arxiv.org/pdf/1608.08710.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pruning Filters for Efficient ConvNets[C/OL]">
                                        <b>[17]</b>
                                         LI H, KADAV A, DURDANOVIC I, &lt;i&gt;et al&lt;/i&gt;.Pruning Filters for Efficient Convents[C/OL].[2018-12-06].https://arxiv.org/pdf/1608.08710.pdf.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" MAJI S, RAHTU E, KANNALA J, &lt;i&gt;et al&lt;/i&gt;.Fine-Grained Visual Classification of Aircraft[C/OL].[2018-12-06].https://arxiv.org/pdf/1306.5151.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fine-Grained Visual Classification of Aircraft[C/OL]">
                                        <b>[18]</b>
                                         MAJI S, RAHTU E, KANNALA J, &lt;i&gt;et al&lt;/i&gt;.Fine-Grained Visual Classification of Aircraft[C/OL].[2018-12-06].https://arxiv.org/pdf/1306.5151.pdf.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" KHOSLA A, JAYADEVAPRAKASH N, YAO B P, &lt;i&gt;et al&lt;/i&gt;.Novel Dataset for Fine-Grained Image Categorization:Stanford Dogs[C/OL].[2018-12-06].http://59.80.44.98/people.csail.mit.edu/khosla/papers/fgvc2011.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Novel Dataset for Fine-Grained Image Categorization:Stanford Dogs[C/OL]">
                                        <b>[19]</b>
                                         KHOSLA A, JAYADEVAPRAKASH N, YAO B P, &lt;i&gt;et al&lt;/i&gt;.Novel Dataset for Fine-Grained Image Categorization:Stanford Dogs[C/OL].[2018-12-06].http://59.80.44.98/people.csail.mit.edu/khosla/papers/fgvc2011.pdf.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" KRAUSE J, STARK M, DENG J, &lt;i&gt;et al&lt;/i&gt;.3D Object Representations for Fine-Grained Categorization // Proc of the IEEE International Conference on Computer Vision Workshops.Washington, USA:IEEE, 2013:554-561." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3d object representations for fine-grained categorization">
                                        <b>[20]</b>
                                         KRAUSE J, STARK M, DENG J, &lt;i&gt;et al&lt;/i&gt;.3D Object Representations for Fine-Grained Categorization // Proc of the IEEE International Conference on Computer Vision Workshops.Washington, USA:IEEE, 2013:554-561.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" CHATFIELD K, SIMONYAN K, VEDALDI A, &lt;i&gt;et al&lt;/i&gt;.Return of the Devil in the Details:Delving Deep into Convolutional Nets[C/OL].[2018-12-06].https://arxiv.org/pdf/1405.3531.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Return of the Devil in the Details:Delving Deep into Convolutional Nets[C/OL]">
                                        <b>[21]</b>
                                         CHATFIELD K, SIMONYAN K, VEDALDI A, &lt;i&gt;et al&lt;/i&gt;.Return of the Devil in the Details:Delving Deep into Convolutional Nets[C/OL].[2018-12-06].https://arxiv.org/pdf/1405.3531.pdf.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" GOSSELIN P H, MURRAY N, J&#201;GOU H, &lt;i&gt;et al&lt;/i&gt;.Revisiting the Fisher Vector for Fine-Grained Classification.Pattern Recognition Letters, 2014, 49:92-98." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700171523&amp;v=MTg3ODdPQ1g0Nm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGc1ZheFk9TmlmT2ZiSzhIOURNcUk5Rlpldw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         GOSSELIN P H, MURRAY N, J&#201;GOU H, &lt;i&gt;et al&lt;/i&gt;.Revisiting the Fisher Vector for Fine-Grained Classification.Pattern Recognition Letters, 2014, 49:92-98.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" 冯语姗, 王子磊.自上而下注意图分割的细粒度图像分类.中国图象图形学报, 2016, 21 (9) :1147-1154. (FENG Y S, WANG Z L.Fine-Grained Image Categorization with Segmentation Based on Top-Down Attention Map.Journal of Image and Graphics, 2016, 21 (9) :1147-1154.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201609004&amp;v=MDgzODRSTE9lWmVSbkZ5emdVcjdNUHlyZmJMRzRIOWZNcG85RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         冯语姗, 王子磊.自上而下注意图分割的细粒度图像分类.中国图象图形学报, 2016, 21 (9) :1147-1154. (FENG Y S, WANG Z L.Fine-Grained Image Categorization with Segmentation Based on Top-Down Attention Map.Journal of Image and Graphics, 2016, 21 (9) :1147-1154.) 
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" SIMON M, RODNER E.Neural Activation Constellations:Unsupervised Part Model Discovery with Convolutional Networks // Proc of the IEEE International Conference on Computer Vision.Wa-shington, USA:IEEE, 2015:1143-1151." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural activation constellations:unsupervised part model discovery with convolutional networks">
                                        <b>[24]</b>
                                         SIMON M, RODNER E.Neural Activation Constellations:Unsupervised Part Model Discovery with Convolutional Networks // Proc of the IEEE International Conference on Computer Vision.Wa-shington, USA:IEEE, 2015:1143-1151.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" ZHANG X P, XIONG H K, ZHOU W G, &lt;i&gt;et al&lt;/i&gt;.Picking Deep Filter Responses for Fine-Grained Image Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:1134-1142." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Picking deep filter responses for fine-grained image recognition">
                                        <b>[25]</b>
                                         ZHANG X P, XIONG H K, ZHOU W G, &lt;i&gt;et al&lt;/i&gt;.Picking Deep Filter Responses for Fine-Grained Image Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:1134-1142.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_26" title=" ZHAO B, WU X, FENG J S, &lt;i&gt;et al&lt;/i&gt;.Diversified Visual Attention Networks for Fine-Grained Object Classification.IEEE Transactions on Multimedia, 2017, 19 (6) :1245-1256." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Diversified visual attention networks for fine-grained object classification">
                                        <b>[26]</b>
                                         ZHAO B, WU X, FENG J S, &lt;i&gt;et al&lt;/i&gt;.Diversified Visual Attention Networks for Fine-Grained Object Classification.IEEE Transactions on Multimedia, 2017, 19 (6) :1245-1256.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_27" title=" LIU X, XIA T, WANG J, &lt;i&gt;et al&lt;/i&gt;.Fully Convolutional Attention Networks for Fine-Grained Recognition[C/OL].[2018-12-06].https://arxiv.org/pdf/1603.06765.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully Convolutional Attention Networks for Fine-Grained Recognition[C/OL]">
                                        <b>[27]</b>
                                         LIU X, XIA T, WANG J, &lt;i&gt;et al&lt;/i&gt;.Fully Convolutional Attention Networks for Fine-Grained Recognition[C/OL].[2018-12-06].https://arxiv.org/pdf/1603.06765.pdf.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_28" title=" GIRSHICK R, DONAHUE J, DARRELL T, &lt;i&gt;et al&lt;/i&gt;.Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[28]</b>
                                         GIRSHICK R, DONAHUE J, DARRELL T, &lt;i&gt;et al&lt;/i&gt;.Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:580-587.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_29" title=" KRAUSE J, JIN H L, YANG J C, &lt;i&gt;et al&lt;/i&gt;.Fine-Grained Recognition without Part Annotations // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:5546-5555." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fine-grained recognition without part annotations">
                                        <b>[29]</b>
                                         KRAUSE J, JIN H L, YANG J C, &lt;i&gt;et al&lt;/i&gt;.Fine-Grained Recognition without Part Annotations // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:5546-5555.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(04),336-344 DOI:10.16451/j.cnki.issn1003-6059.201904006            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于稀疏化双线性卷积神经网络的细粒度图像分类</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E5%8A%9B&amp;code=26399742&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马力</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%B0%B8%E9%9B%84&amp;code=33084411&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王永雄</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E5%85%89%E7%94%B5%E4%BF%A1%E6%81%AF%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0256814&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海理工大学光电信息与计算机工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E5%BA%B7%E5%A4%8D%E5%99%A8%E6%A2%B0%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海康复器械工程技术研究中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对双线性卷积神经网络 (B-CNN) 在细粒度图像分类中因参数过多、复杂度过高而导致的过拟合问题, 提出稀疏化B-CNN.首先对B-CNN的每个特征通道引入比例因子, 在训练中采用正则化方法对其稀疏.然后利用比例因子的大小判别特征通道的重要性.最后将不重要特征通道按一定比例裁剪, 消除网络过拟合, 提高关键特征的显著性.稀疏化B-CNN属于弱监督学习, 可实现端到端训练.在FGVC-aircraft、Stanford dogs、Stanford cars这3个细粒度图像数据集上的实验表明, 稀疏化B-CNN的准确率高于B-CNN, 也优于或基本接近其它通用的细粒度图像分类算法.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%86%E7%B2%92%E5%BA%A6%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">细粒度图像分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E7%BA%BF%E6%80%A7%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20(B-CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双线性卷积神经网络 (B-CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%87%E6%8B%9F%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">过拟合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BD%91%E7%BB%9C%E7%A8%80%E7%96%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">网络稀疏;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BD%91%E7%BB%9C%E5%89%AA%E6%9E%9D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">网络剪枝;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    马力, 硕士研究生, 主要研究方向为计算机视觉、图像处理.E-mail:mali1906@163.com.;
                                </span>
                                <span>
                                    *王永雄 (通讯作者) , 博士, 教授, 主要研究方向为智能机器人及视觉.E-mail:wyxiong@usst.edu.cn.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-03</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.61673276, 61703277) 资助;</span>
                    </p>
            </div>
                    <h1><b>Fine-Grained Visual Classification Based on Sparse Bilinear Convolutional Neural Network</b></h1>
                    <h2>
                    <span>MA Li</span>
                    <span>WANG Yongxiong</span>
            </h2>
                    <h2>
                    <span>School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology</span>
                    <span>Shanghai Engineering Research Center of Assistive Devices</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The overfitting problem of bilinear convolutional neural network (B-CNN) for fine-grained visual recognition is caused by the large number of parameters and its complex structure. In this paper, a sparse B-CNN is proposed to handle the problem. Firstly, a scaling factor is introduced into each feature channel of B-CNN, and regularization of sparsity is applied to the scaling factors during the training. Then, the feature channels in B-CNN with low contribution to the final classification are identified by small scaling factors. Finally, these channels are pruned in a certain proportion to prevent overfitting and increase the significance of key features. The learning of sparse B-CNN is weakly supervised and end-to-end. The verification experiments on FGVC-aircraft, Stanford dogs and Stanford cars fine-grained image datasets show that the accuracy of sparse B-CNN is higher than that of the original B-CNN. Moreover, compared with other advanced algorithms for fine-grained visual recognition, the performance of sparse B-CNN is same or even better.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Fine-Grained%20Visual%20Recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Fine-Grained Visual Recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Bilinear%20Convolutional%20Neural%20Network%20(B-CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Bilinear Convolutional Neural Network (B-CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Overfitting&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Overfitting;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Network%20Sparsity&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Network Sparsity;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Network%20Pruning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Network Pruning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    MA Li, master student. His research interests include computer vision and image processing.;
                                </span>
                                <span>
                                    WANG Yongxiong ( Corresponding author) , Ph. D., professor. His research interests include intelligent robot and vision.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-01-03</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.61673276, 61703277);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="75">细粒度图像分类是计算机视觉领域一个重要的研究方向, 不同于粗粒度图像分类, 细粒度图像分类主要目的是对图像进行细致的子类划分.相比粗粒度图像分类, 细粒度图像分类更关注图像中微小但十分重要的局部特征, 因此细粒度图像分类的难度更大.</p>
                </div>
                <div class="p1">
                    <p id="76">早期的细粒度图像分类主要依靠人工标注细微的差异区域<citation id="166" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>, 人工开销过大, 实用性不佳.目前仅使用图像标签 (弱监督) 进行细粒度图像分类是该领域研究的趋势<citation id="163" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>.卷积神经网络 (Convolutional Neural Network, CNN) 的快速发展及在计算机视觉领域取得的重要突破也推动细粒度图像分类技术的快速进步.基于深度卷积神经网络提取图像特征的算法<citation id="167" type="reference"><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>提取的特征比人工特征拥有更强的描述能力, 分类的准确度更高.Lin等<citation id="164" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出的双线性卷积神经网络 (Bilinear CNN, B-CNN) 为表现优异的弱监督方法, 在细粒度图像数据集FGVC-aircraft上取得84.1%的准确率.B-CNN主要依靠2个D-Net (Deep CNN) VGG-16<citation id="165" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>同时提取图像中某一图像块的图像特征与位置特征, 组合成为一个双线性特征用于分类, 因此B-CNN适用于细粒度图像分类任务.</p>
                </div>
                <div class="p1">
                    <p id="77">但是, B-CNN的特征提取网络采用VGG-16, 网络深度较深, 参数较多, 容易造成过拟合现象<citation id="168" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 模型在测试集上准确率与训练集相差较大.目前许多方法可以避免过拟合, 但细粒度图像分类关注于局部关键特征, 一些防止过拟合的方法容易造成关键特征丢失, 如在CNN中广泛使用的随机失活一定比例神经元方法 (Dropout) <citation id="169" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation> .该方法由于随机失效网络中的神经元, 无法区分重要的神经元, 容易造成关键特征丢失.基于自助采样法的并行式集成学习方法 (Bootstrap Aggregating, Bagging) <citation id="170" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>在深度神经网络中会导致训练时间过长.批量归一化 (Batch Normalization, BN) <citation id="171" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>是深度学习中获得快速收敛和更好泛化性能的常用方法, 可以避免过拟合, 但防止过拟合的效果不明显, 无法区分重要的神经元.</p>
                </div>
                <div class="p1">
                    <p id="78">网络剪枝<citation id="172" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>可通过筛选B-CNN中不重要的神经元并裁剪, 减小模型复杂度和模型存储空间, 不仅有助于加速模型训练和预测, 还可以去除不重要特征对分类的影响并保留关键特征.Denil等<citation id="173" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>也提出许多深度卷积神经网络中存在显著冗余, 通过训练较少的权重并预测其余权重可减少冗余, 因此对裁剪后的网络进行微调, 提高算法准确度.网络剪枝技术主要分为权重级、通道级及层级的裁剪.权重级裁剪拥有高度的裁剪灵活性, 但需要额外的软件和硬件的加速<citation id="174" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 在B-CNN上不易实现.而层级的裁剪虽然不需要额外的辅助, 易于实现, 但缺少裁剪的灵活性, 需要移除整层网络<citation id="175" type="reference"><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>, 丢失重要特征, 不利于细粒度图像分类任务.相比上述2种方法, 通道级的裁剪兼顾灵活性和易实现性, 但传统的通道裁剪无法直接在预训练模型上使用.</p>
                </div>
                <div class="p1">
                    <p id="79">综上所述, 本文利用BN中的比例因子构造稀疏层, 直接嵌入在卷积层后实现通道级裁剪.稀疏层中的比例因子与卷积层的每个特征通道相对应, 利用L1正则化操作对BN中的比例因子<i>γ</i>进行通道级的稀疏, 促使不重要通道的<i>γ</i>值更小, 从而自动辨别不重要的通道.最后按一定比例裁剪部分不重要的通道, 减少不重要神经元的数量, 同时加大关键特征对分类的影响, 以此解决B-CNN过拟合问题, 提高在细粒度图像数据集上的分类精度.</p>
                </div>
                <h3 id="80" name="80" class="anchor-tag">1 稀疏化双线性卷积神经网络模型</h3>
                <div class="p1">
                    <p id="82">稀疏化B-CNN如图1所示, 利用BN层的比例因子组成稀疏层, 嵌入在B-CNN每个卷积层后, 稀疏层中每个比例因子<i>γ</i>与卷积层的每个输出在BN层中相乘, 达到筛选特征通道的目的.该模型手动设置裁剪阈值, 使一定数量的不重要通道失效, 最后经过网络微调消除通道裁剪的影响, 提高准确率.</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904008_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 稀疏化的双线性卷积神经网络结构" src="Detail/GetImg?filename=images/MSSB201904008_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 稀疏化的双线性卷积神经网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904008_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Architecture of sparse bilinear convolutional neural network</p>

                </div>
                <h4 class="anchor-tag" id="85" name="85"><b>1.1</b> 双线性卷积神经网络模型</h4>
                <div class="p1">
                    <p id="86">B-CNN的基本结构如图2所示, 输入图像通过2个特征提取网络<i>A</i>和<i>B</i>, 通过外积组合两个网络的输出, 使用求和池化获得双线性特征, 送入分类层进行预测.双线性模型优点是以平移不变的方式对局部成对特征相互作用进行建模, 可推广至各种无序纹理描述符.B-CNN可表示成一组四元函数 <i>B</i>=<i>F</i> (<i>f</i><sub><i>A</i></sub>, <i>f</i><sub><i>B</i></sub>, <i>P</i>, <i>C</i>) , 其中:<i>f</i><sub><i>A</i></sub>和<i>f</i><sub><i>B</i></sub>为特征提取函数, 文中两者都是基于VGG-16;<i>P</i>为池化函数, 池化可使用简单的相加也可使用最大池化函数;<i>C</i>为分类函数, 可使用softmax或线性SVM.</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904008_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 双线性卷积神经网络结构" src="Detail/GetImg?filename=images/MSSB201904008_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 双线性卷积神经网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904008_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Architecture of B-CNN</p>

                </div>
                <div class="p1">
                    <p id="88">特征提取函数<i>f</i><sub><i>A</i></sub>和<i>f</i><sub><i>B</i></sub>为一种映射<i>f</i>∶<b><i>L</i></b>×<b><i>I</i></b>→<b>R</b><sup><i>c</i>×<i>D</i></sup>, 其中, <b><i>L</i></b>为输入图像的位置区域, <b><i>I</i></b>为输入的图像, 将两者映射成一个<i>c</i>×<i>D</i>维的特征.最后<i>f</i><sub><i>A</i></sub>和<i>f</i><sub><i>B</i></sub>两者的输出特征通过矩阵的外积操作进行汇聚, 得到一个双线性特征.双线性特征</p>
                </div>
                <div class="p1">
                    <p id="89"><i>b</i> (<b><i>l</i></b>, <b><i>i</i></b>, <i>f</i><sub><i>A</i></sub>, <i>f</i><sub><i>B</i></sub>) =<i>f</i><sub><i>A</i></sub> (<b><i>l</i></b>, <b><i>i</i></b>) <sup>T</sup><i>f</i><sub><i>B</i></sub> (<b><i>l</i></b>, <b><i>i</i></b>) , </p>
                </div>
                <div class="p1">
                    <p id="90">其中, <b><i>i</i></b>∈<b><i>I</i></b>, <b><i>l</i></b>∈<b><i>L</i></b>.<i>f</i><sub><i>A</i></sub>和<i>f</i><sub><i>B</i></sub>必须拥有相同的特征维度<i>c</i>, <i>c</i>的维度由模型决定.池化函数<i>P</i>过程如下所示:</p>
                </div>
                <div class="p1">
                    <p id="91"><mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Φ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">l</mi><mo>∈</mo><mi mathvariant="bold-italic">L</mi></mrow></munder><mi>b</mi></mstyle><mi>i</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>a</mi><mi>r</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">l</mi><mo>, </mo><mi mathvariant="bold-italic">i</mi><mo>, </mo><mi>f</mi><msub><mrow></mrow><mi>A</mi></msub><mo>, </mo><mi>f</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="93">采用将图像中所有位置的双线性特征相加聚合以获得全局图像表示.若<i>f</i><sub><i>A</i></sub>和<i>f</i><sub><i>B</i></sub>提取的特征为<i>C</i>×<i>M</i>和<i>C</i>×<i>N</i>, 则上式中输出的双线性特征<i>Φ</i> (<b><i>I</i></b>) 维度为<i>M</i>×<i>N</i>.</p>
                </div>
                <div class="p1">
                    <p id="94">将<i>Φ</i> (<b><i>I</i></b>) 特征转化为一个<i>MN</i>×1的列向量, 记作<b><i>x</i></b>, 作为最后提取的特征.将<i>MN</i>×1的特征向量输入最后的分类函数 <i>C</i> 进行分类, <i>C</i>采用softmax函数分类:</p>
                </div>
                <div class="p1">
                    <p id="95"><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mi>i</mi></msup><mo>=</mo><mfrac><mrow><mrow><mi>e</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mi>i</mi></msup></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>e</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mi>j</mi></msup></mrow></mfrac></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="97">softmax函数常用于多任务分类, 将特征值映射至 (0, 1) 内, 其中, <i>e</i> (<i>x</i>) <sup><i>i</i></sup>为类别<i>i</i>的权重值, <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>e</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mi>j</mi></msup></mrow></math></mathml>为所有类别权重值总和, <i>C</i> (<i>x</i>) <sup><i>i</i></sup>为网络输出属于类别<i>i</i>的概率值.</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>1.1.1 B-CNN</b>的特征函数</h4>
                <div class="p1">
                    <p id="100">迁移学习在利用深度学习方法的计算机视觉任务中被广泛采用, 在B-CNN中特征函数<i>f</i><sub><i>A</i></sub>和<i>f</i><sub><i>B</i></sub>由VGG-16在ImageNet数据集上的预训练网络构成.由于细粒度图像数据集一般较小, 在实验中通过使用预训练网络VGG-16的relu5_3层提取的特征, 可以使B-CNN在细粒度图像分类上取得更优的表现.另外网络可以共享特征并能处理任意大小的图像, 产生由图像位置和特征通道索引的输出.如图3所示, 在网络的前向运算过程中, 特征函数<i>f</i><sub><i>A</i></sub>和<i>f</i><sub><i>B</i></sub>可以选择独立、完全共享或部分共享的模型, 本文采用完全共享的模型.</p>
                </div>
                <div class="area_img" id="200">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904008_20000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 双线性卷积神经网络的前向运算方式" src="Detail/GetImg?filename=images/MSSB201904008_20000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 双线性卷积神经网络的前向运算方式  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904008_20000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Forward operation ways of B-CNN</p>

                </div>
                <h4 class="anchor-tag" id="108" name="108"><b>1.1.2 B-CNN</b>的正则化和分类</h4>
                <div class="p1">
                    <p id="109">通过特征函数输出获得双线性特征<i>Φ</i> (<b><i>I</i></b>) , 然后采用有符号的平方根</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">y</mi><mo>=</mo><mi>s</mi><mi>i</mi><mi>g</mi><mi>n</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Φ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msqrt><mrow><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Φ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">) </mo><mo stretchy="false">|</mo></mrow></msqrt></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">及L2正则化处理</p>
                </div>
                <div class="p1">
                    <p id="112"><b><i>z</i></b>=<i>y</i>/‖<i>y</i>‖<sub>2</sub></p>
                </div>
                <div class="p1">
                    <p id="113">进一步提高B-CNN的性能.在分类层, softmax或线性SVM都可作为良好的分类器.线性SVM能取得最优效果, 但需要单独训练时间, 开销较大, 故本文使用softmax.</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114"><b>1.2</b> 通道级稀疏与剪枝的实现</h4>
                <h4 class="anchor-tag" id="115" name="115"><b>1.2.1 B-CNN</b>通道级稀疏的困难</h4>
                <div class="p1">
                    <p id="116">实现通道级稀疏的主要方法是修剪与特征通道关联的连接, 但在预训练模型上的输入或输出的权重不可能为零或接近零, 因此通道级的稀疏无法直接作用于预训练模型上.B-CNN为基于VGG-16在ImageNet数据集上的预训练模型, 常采用分组最小角回归算法 (group LASSO) , 训练期间将所有对应于相同特征通道的权重滤波器逼近于零, 从而实现预训练模型上的稀疏过程<citation id="176" type="reference"><link href="31" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">17</a>]</sup></citation>.但是, 这需要额外计算针对所有权重滤波器附加的正则化梯度, 模型训练时间开销大, 而利用BN层比例因子对B-CNN通道级稀疏的方案可以解决这些困难.</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117"><b>1.2.2</b> 比例因子和稀疏惩罚项</h4>
                <div class="p1">
                    <p id="118">对每个特征通道引入一个对应的比例因子<i>γ</i> (<i>γ</i>≥0) , 如图4所示, <i>γ</i>组成的稀疏层实现特征通道筛选功能.稀疏层的构造利用BN层的正则化激活方式.可以设计一种简单有效的方法用于合并通道的比例因子.BN层对小批量输入进行归一化操作, 使内部激活标准化, 若令<i>x</i><sub>in</sub>和<i>x</i><sub>out</sub>作为BN层的输入和输出, <i>B</i>为当前的小批量, BN层执行的转换如下:</p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>μ</mi><msub><mrow></mrow><mi>B</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo>, </mo></mtd></mtr><mtr><mtd><mi>σ</mi><msubsup><mrow></mrow><mi>B</mi><mn>2</mn></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>x</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo></mtd></mtr><mtr><mtd><mover accent="true"><mi>X</mi><mo>^</mo></mover><mo>=</mo><mfrac><mrow><mi>x</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mrow><msqrt><mrow><mi>σ</mi><msubsup><mrow></mrow><mi>B</mi><mn>2</mn></msubsup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mrow></mfrac><mo>, </mo></mtd></mtr><mtr><mtd><mi>x</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub><mo>=</mo><mi>γ</mi><mover accent="true"><mi>X</mi><mo>^</mo></mover><mo>+</mo><mi>β</mi><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">其中, <i>μ</i><sub><i>B</i></sub>、<i>σ</i><sub><i>B</i></sub>为小批量<i>B</i>输入激活的平均值、标准偏差值, <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>X</mi><mo>^</mo></mover></math></mathml>为对输入<i>x</i><sub>in</sub>标准化处理后的输出, 比例因子<i>γ</i>、 <i>β</i>为可训练的仿射变换参数, 能将标准化激活线性变换到任何尺度.</p>
                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904008_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 稀疏剪枝操作过程" src="Detail/GetImg?filename=images/MSSB201904008_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 稀疏剪枝操作过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904008_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Process of sparsity and pruning</p>

                </div>
                <div class="p1">
                    <p id="123">将拥有通道级的比例与移位参数的BN层插入到卷积层之后, 可直接利用BN层中的<i>γ</i>进行网络稀疏化.这种方法不需要引入任何额外开销, 实验中发现这是通道比例因子剪枝的有效方法.理由如下:1) 如果未利用BN层实现稀疏化, 则比例因子对于评估特征通道的重要性无意义, 因为卷积层和稀疏层都是线性变换.通过在卷积层中放大权重的同时减小比例因子, 可获得相同的结果.2) 如果将含有比例因子的稀疏层插入在BN层之前, 缩放层的缩放效果将被BN归一化处理而失去作用.3) 如果将含有比例因子的稀疏层插入在BN层之后, 则每个特征通道会有两个连续的比例因子.</p>
                </div>
                <div class="p1">
                    <p id="124">为了控制比例因子在训练过程中的稀疏性, 在B-CNN的训练目标函数中添加稀疏惩罚项.训练目标函数</p>
                </div>
                <div class="p1">
                    <p id="125"><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></munder><mi>l</mi></mstyle><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>W</mi><mo stretchy="false">) </mo><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>γ</mi><mo>∈</mo><mi>Γ</mi></mrow></munder><mi>g</mi></mstyle><mo stretchy="false"> (</mo><mi>γ</mi><mo stretchy="false">) </mo></mrow></math></mathml>.      (1) </p>
                </div>
                <div class="p1">
                    <p id="127">其中:第1项<i>l</i>为原B-CNN的损失函数, 这里采用交叉熵损失函数</p>
                </div>
                <div class="p1">
                    <p id="128"><mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><mo>=</mo><mi>Η</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>q</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>x</mi></munder><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mrow><mi>log</mi></mrow><msub><mrow></mrow><mn>2</mn></msub><mspace width="0.25em" /><mi>q</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="130"><i>p</i> (<i>x</i>) 为交叉熵函数准确值, <i>q</i> (<i>x</i>) 为交叉熵函数预测值, <i>l</i>交叉熵计算值为两者概率分布的距离. (<i>x</i>, <i>y</i>) 为输入图像和真实标签;<i>W</i>为可训练的权重.式 (1) 中第二项为稀疏惩罚项, <i>g</i> (<i>γ</i>) 为对比例因子<i>γ</i>的正则化操作, <i>g</i> (·) 可选用L1或L2正则化, 实验中对比两种正则化方法, L2比L1更具有稀疏化的功能, 但会丢失部分通道特征, 而L2为权重衰减, 可保留更多通道特征, 当采用L1正则化时, 需利用次梯度法优化非平滑的L1惩罚项, 也可以利用平滑的L1进行替换.<i>λ</i>为控制稀疏程度的参数, 防止稀疏比例因子过多而丢失重要通道特征, 根据实验得出<i>λ</i>=10<sup>-5</sup>为较优值.</p>
                </div>
                <h4 class="anchor-tag" id="131" name="131"><b>1.2.3 B-CNN</b>特征通道的裁剪和网络微调</h4>
                <div class="p1">
                    <p id="132">经过通道级稀疏后, 网络具有众多接近于0的比例因子.裁剪这些比例因子实现特征通道的修剪, 如图4所示.在剪枝操作中引入一个阈值, 对比例因子进行一定比例的裁剪.裁剪后的B-CNN更紧凑, 含有更少的参数和更低的模型复杂度, 从而避免过拟合.此外, 稀疏化B-CNN整体架构为有向非循环图, 只需计算特征提取网络梯度就可实现端到端训练, 梯度计算图如图5所示.</p>
                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904008_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 双线性卷积神经网络梯度计算图" src="Detail/GetImg?filename=images/MSSB201904008_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 双线性卷积神经网络梯度计算图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904008_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Gradient calculation graph of B-CNN</p>

                </div>
                <div class="p1">
                    <p id="134">综上所述, B-CNN网络进行稀疏剪枝的步骤如图6所示.裁剪后的B-CNN识别率有一定程度的下降, 但在训练中对裁剪后的B-CNN进行微调, 可进一步提高分类精度.</p>
                </div>
                <div class="area_img" id="135">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904008_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 双线性卷积神经网络算法改进流程图" src="Detail/GetImg?filename=images/MSSB201904008_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 双线性卷积神经网络算法改进流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904008_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Improvement of B-CNN flowchart</p>

                </div>
                <h3 id="136" name="136" class="anchor-tag">2 实验及结果分析</h3>
                <div class="p1">
                    <p id="137">实验中采用FGVC-aircraft<citation id="177" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、Stanford dogs<citation id="178" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、和Stanford cars<citation id="179" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>3个标准的细粒度图像数据集进行验证.表1中给出3个数据集的具体信息.由于图像中复杂因素的干扰, 如前景较小、遮挡较多、光照较强等, 卷积层会提取许多不重要的特征.本次实验验证稀疏层在不重要的特征通道中的作用和影响.</p>
                </div>
                <div class="area_img" id="138">
                    <p class="img_tit"><b>表1 细粒度数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Fine-grained datasets</p>
                    <p class="img_note"></p>
                    <table id="138" border="1"><tr><td><br />名称</td><td>类别数</td><td>训练集数目</td><td>测试集数目</td></tr><tr><td><br />FGVC-aircraft</td><td>100</td><td>6667</td><td>3333</td></tr><tr><td><br />Stanford dogs</td><td>120</td><td>12000</td><td>8580</td></tr><tr><td><br />Stanford cars</td><td>196</td><td>8144</td><td>8041</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="139">实验基于ubuntu系统, 采用keras深度学习框架和python编程语言, 硬件使用英伟达GTX1080显卡进行训练.输入图像大小固定为448×448, 数据集的预处理工作包括:图像零均值化、随机裁剪和填充放大.实验中B-CNN利用VGG-16预训练模型, 截断在relu5层的称为M-Net (Medium CNN) <citation id="180" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, 即B-CNN[M, M], 截断在relu5_3层的称为D-Net<citation id="181" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 即B-CNN[D, D].稀疏化B-CNN采用2个D-Net, 其中采用L1对比例因子稀疏的称为Sparse B-CNN[D, D] (L1) , 采用L2对比例因子稀疏的称为Sparse B-CNN[D, D] (L2) .</p>
                </div>
                <h4 class="anchor-tag" id="140" name="140"><b>2.1</b> 在<b>FGVC-aircraft</b>数据集上的分类实验</h4>
                <div class="p1">
                    <p id="141">实验步骤分为三步:粗训练、微调训练和剪枝微调.粗训练如图7所示, 训练B-CNN最后分类层50至100个周期, 学习步长为0.9.原B-CNN、Sparse B-CNN[D, D] (L1) 、Sparse B-CNN[D, D] (L2) 在测试集上准确率分别为75.6%、83.3%、81.7%, 相比原B-CNN, 分别提高7.7%, 6.1%.</p>
                </div>
                <div class="p1">
                    <p id="142">微调训练如图8所示, 训练周期为50, 学习步长为0.001.B-CNN, Sparse B-CNN[D, D] (L1) , Sparse B-CNN[D, D] (L2) 准确率分别为82%、88.4%、88.7%, 相比原B-CNN, 分别提高4.4%、4.7%.提高的主要原因是正则化对比例因子的稀疏降低不重要通道对分类的贡献, 而L2能保留更多特征但无法达到L1的稀疏性, 同时进行网络特征通道的筛选, 经过筛选后进行剪枝操作.</p>
                </div>
                <div class="p1">
                    <p id="143">剪枝微调对稀疏化B-CNN设定阈值进行剪枝, 经过50个周期的微调, 最后Sparse B-CNN[D, D] (L1) 准确率为88.7%, Sparse B-CNN[D, D] (L2) 准确率为88.9%.</p>
                </div>
                <div class="p1">
                    <p id="144">随着<i>λ</i>的增大, 更多的比例因子为零或接近于零, 式 (1) 中的稀疏惩罚项对比例因子的稀疏与<i>λ</i>成正相关.一旦<i>λ</i>过大, 会造成过多特征被抑制, 导致准确率降低, 当<i>λ</i>=10<sup>-4</sup>时, 稀疏化的B-CNN分类精度比<i>λ</i>=10<sup>-5</sup>时降低近2%.剪枝微调中不同数据集设定的阈值不同, 在FGVC-aircraft数据集上采用25%的修剪率.</p>
                </div>
                <div class="area_img" id="145">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904008_14500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 第一步粗训练曲线图 (仅训练B-CNN最后一层分类层)" src="Detail/GetImg?filename=images/MSSB201904008_14500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 第一步粗训练曲线图 (仅训练B-CNN最后一层分类层)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904008_14500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Graph of training in the first step (only training classification layer) </p>

                </div>
                <div class="area_img" id="146">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904008_14600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 第二步微调曲线图 (训练B-CNN全部权重和比例因子)" src="Detail/GetImg?filename=images/MSSB201904008_14600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 第二步微调曲线图 (训练B-CNN全部权重和比例因子)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904008_14600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Graph of fine-tuning in the second step (training all weights and scaling factors) </p>

                </div>
                <div class="p1">
                    <p id="148">多种方法在FGVC-aircraft数据集上的准确率对比如表2所示.FV-SIFT (Fish Vector-Scale Invariant Feature Transform) <citation id="182" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>通过提取密集尺度不变特征变换 (Scale Invariant Feature Transform, SIFT) 特征实现, 字典大小为256, 空间金字塔有1层.FV+SIFT (Fish Vector+Scale Invariant Feature Transform) <citation id="183" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>字典大小为1 024, 使用多尺度SIFT特征, 空间金字塔有2层, 相比本文方法, 两者准确率分别降低27.7%、8%.FC-CNN (CNN with Fully-Connected Layers) <citation id="184" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>使用D-Net获得准确率为74.1%, 使用M-Net获得准确率为57.3%, 准确率较低.FV-CNN<citation id="185" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>对CNN特征进行FV (Fish Vector) 编码, 在使用D-Net或M-Net时准确率都比本文方法降低10%以上, 这是因为FV采用混合高斯模型构建码本, 编码后的向量通常不是稀疏的.</p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit"><b>表2 各方法在FGVC-aircraft数据集上的准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Accuracy comparison of different methods on FGVC- aircraft dataset </p>
                    <p class="img_note">%</p>
                    <table id="149" border="1"><tr><td><br />方法</td><td>准确率</td></tr><tr><td><br />FV-SIFT<sup>[6]</sup></td><td>61.0</td></tr><tr><td><br />FV+SIFT<sup>[22]</sup></td><td>80.7</td></tr><tr><td><br />FC-CNN[M]<sup>[6]</sup></td><td>57.3</td></tr><tr><td><br />FC-CNN[D]<sup>[6]</sup></td><td>74.1</td></tr><tr><td><br />FV-CNN[M]<sup>[6]</sup></td><td>70.1</td></tr><tr><td><br />FV-CNN[D]<sup>[6]</sup></td><td>77.6</td></tr><tr><td><br />BaseNet+SegNet<sup>[23]</sup></td><td>83.4</td></tr><tr><td><br />Original B-CNN[D, D]<sup>[6]</sup></td><td>84.1</td></tr><tr><td><br />Original B-CNN[D, D] (本文复现的BCNN) </td><td>82.0</td></tr><tr><td><br />Sparse B-CNN[D, D] (L1) </td><td>88.4</td></tr><tr><td><br />Sparse B-CNN[D, D] (L1) (剪枝后) </td><td>88.7</td></tr><tr><td><br />Sparse B-CNN[D, D] (L2) </td><td>88.7</td></tr><tr><td><br />Sparse B-CNN[D, D] (L2) (剪枝后) </td><td>88.9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="150">BaseNet+SegNet (Base Convolutional Neural Net-work+Segmentation Convolutional Neural Network) <citation id="186" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>利用训练好的基础网络模型 (BaseNet) 生成自上而下注意图, 再用注意图初始化GraphCut算法, 分割关键的目标区域, 提高图像的判别性, 最后对分割图像提取CNN特征实现细粒度分类, 准确率为83.4%, 仍比本文方法降低5.3%.</p>
                </div>
                <h4 class="anchor-tag" id="151" name="151"><b>2.2</b> 在<b>Stanford dogs</b> 数据集上的分类实验</h4>
                <div class="p1">
                    <p id="152">在粗训练中, Sparse B-CNN[D, D] (L1) 、Sparse B-CNN[D, D] (L2) 的准确率分别为79.5%、80.8%, 比原B-CNN (78.2%) 分别提高1.3%、2.6%.在微调工作中, 学习率选择0.000 1, Sparse B-CNN[D, D] (L1) 、Sparse B-CNN[D, D] (L2) 准确率分别为81.3%、81.4%, 比原B-CNN (80.4%) 分别提高0.9%、1.0%.在剪枝微调中, 在Stanford dog数据集上的裁剪率采用5%, 经过微调后Sparse B-CNN[D, D] (L1) 准确率为83.4%, Sparse B-CNN[D, D] (L2) 准确率为83.2%.与FGVC-aircraft数据集相同, <i>λ</i>的加大一样会导致分类精度的降低, 并且在Stanford dogs数据集上裁剪率不宜过高.</p>
                </div>
                <div class="p1">
                    <p id="153">表3列出各方法在Stanford dog数据集上的准确率对比.</p>
                </div>
                <div class="area_img" id="154">
                    <p class="img_tit"><b>表3 各方法在Stanford Dogs数据集上的准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Accuracy comparison of different methods on Stanford Dogs dataset </p>
                    <p class="img_note">%</p>
                    <table id="154" border="1"><tr><td><br />方法</td><td>准确率</td></tr><tr><td><br />NAC (AlexNet) <sup>[24]</sup></td><td>68.6</td></tr><tr><td><br />PDFR (AlexNet) <sup>[25]</sup></td><td>71.9</td></tr><tr><td><br />VGG-16<sup>[7]</sup></td><td>76.7</td></tr><tr><td><br />DVAN<sup>[26]</sup></td><td>81.5</td></tr><tr><td><br />FCAN<sup>[27]</sup></td><td>84.2</td></tr><tr><td><br />Original B-CNN[D, D] (本文复现的BCNN) </td><td>80.4</td></tr><tr><td><br />Sparse B-CNN[D, D] (L1) </td><td>81.3</td></tr><tr><td><br />Sparse B-CNN[D, D] (L1) (剪枝后) </td><td>83.4</td></tr><tr><td><br />Sparse B-CNN[D, D] (L2) </td><td>81.4</td></tr><tr><td><br />Sparse B-CNN[D, D] (L2) (剪枝后) </td><td>83.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="155">NAC (AlexNet) <citation id="187" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation> (Neural Activation Constella-tions Find Parts by Computing Neural Activation Patterns) 利用卷积网络AlexNet提取特征产生的关键点, 并利用关键点提取局部区域信息.具体通过卷积特征的可视化分析寻找响应较强烈的区域作为特征提取点, 但精度只有68.6%, 远低于本文方法.由于特征图的分辨率与原图相差较大, 所以很难对原图特征响应高的区域进行精确定位.PDFR (AlexNet) (Picking Deep Filter Responses Propose to Find Distinctive Filters and Learn Part Detectors) <citation id="188" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>是基于挑选深度Filter Response框架的算法, 准确率比本文方法降低11.5%.单使用一个VGG-16网络取得的分类精度比本文方法降低6.7%, 这是因为VGG-16无法有效提取图像显著区域的特征, 而且VGG-16深度较深, 存在过多的冗余, 容易导致过拟合.DVAN (Diverse Attention Net-work Attends Object from Coarse to Fine by Multiple Region Proposals) <citation id="189" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>为多注意力网络, 通过提高视觉注意力多样性达到提取判别性特征, 但准确率比本文方法降低1.9%, 需要设计特定损失函数对多个位置的特征进行判别, 网络较复杂.FCAN (Fully Convolutional Attention Network Adaptively Selects Multiple Task-Driven Visual Attention by Reinforcement Learning) <citation id="190" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>同样采用视觉注意力机制, 基于强化学习的全卷积网络, 取得84.2%的准确率, 比本文方法提高0.6%, 但训练时间较长, 而稀疏化B-CNN在保证精度的同时, 复杂度更低, 训练时间更短.</p>
                </div>
                <h4 class="anchor-tag" id="156" name="156"><b>2.3</b> 在<b>Stanford cars</b>数据集上的分类实验</h4>
                <div class="p1">
                    <p id="157">在粗训练中, Sparse B-CNN[D, D] (L1) 、Sparse B-CNN[D, D] (L2) 的准确率分别为84.5%、83.4%, 比原B-CNN (79.7%) 分别提高4.8%、3.7%.在微调工作中, 学习率可选择区间为 (0.001, 0.002) , Sparse B-CNN[D, D] (L1) 、Sparse B-CNN[D, D] (L2) 的准确率分别为91.1%、90.5%, Sparse B-CNN[D, D] (L1) 比原B-CNN (90.6%) 提高0.5%, 而Sparse B-CNN[D, D] (L2) 比原B-CNN (90.6%) 降低0.1%.在剪枝微调中, 裁剪率为10%, 剪枝后经过微调, Sparse B-CNN[D, D] (L1) 准确率提高到91.3%, Sparse B-CNN[D, D] (L2) 准确率为91.0%.Stanford cars数据集裁剪率不宜超过20%.</p>
                </div>
                <div class="p1">
                    <p id="158">表4列出各方法在Stanford cars数据集上准确率对比, 其中Train Anno表示使用人工标注框或标注点.</p>
                </div>
                <div class="p1">
                    <p id="159">DVAN<citation id="191" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、FCAN<citation id="192" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>准确率分别为87.1%、89.1%, 分别比本文方法降低4.2%、2.2%.FCAN训练时加入标注框信息后, 准确率可提高至91.3%, 虽然与本文方法准确率相同, 但标注框的引入将约束算法的实用性.单独使用VGG-19<citation id="193" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>仅取得84.9%的准确率, 比本文方法降低6.4%, 原因和VGG-16相同.R-CNN (Region with Convolutional Neural Net-work Features) <citation id="194" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>为物体检测算法, 准确率为88.4%, 比本文方法降低2.9%.R-CNN使用选择性搜索算法, 会产生大量无关的候选区域, 造成计算上的浪费, 在训练时需要标注框信息.PA-CNN (Part Alignment-Based Method Generates Parts by Using Co-segmentation and Alignment) <citation id="195" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>通过对图像的分割和对齐产生局部关键区域, 用于特征提取分类, 准确率为92.8%, 比本文方法提高1.5%, 但也需要标注框信息.</p>
                </div>
                <div class="area_img" id="160">
                    <p class="img_tit"><b>表4 各方法在Stanford Cars数据集上的准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Accuracy comparison of different methods on Stanford Cars dataset </p>
                    <p class="img_note">%</p>
                    <table id="160" border="1"><tr><td><br />方法</td><td>准确率</td></tr><tr><td><br />R-CNN<sup>[28]</sup> (需要标注信息) </td><td>88.4</td></tr><tr><td><br />PA-CNN<sup>[29]</sup> (需要标注信息) </td><td>92.8</td></tr><tr><td><br />FCAN<sup>[27]</sup> (需要标注信息) </td><td>91.3</td></tr><tr><td><br />VGG-19<sup>[7]</sup></td><td>84.9</td></tr><tr><td><br />DVAN<sup>[26]</sup></td><td>87.1</td></tr><tr><td><br />FCAN<sup>[27]</sup></td><td>89.1</td></tr><tr><td><br />Original B-CNN[D, D]<sup>[10]</sup></td><td>90.6</td></tr><tr><td><br />Original B-CNN[D, D] (本文复现的BCNN) </td><td>89.6</td></tr><tr><td><br />Sparse B-CNN[D, D] (L1) </td><td>91.1</td></tr><tr><td><br />Sparse B-CNN[D, D] (L1) (剪枝后) </td><td>91.3</td></tr><tr><td><br />Sparse B-CNN[D, D] (L2) </td><td>90.5</td></tr><tr><td><br />Sparse B-CNN[D, D] (L2) (剪枝后) </td><td>91.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="161" name="161" class="anchor-tag">3 结 束 语</h3>
                <div class="p1">
                    <p id="162">本文提出稀疏化B-CNN, 利用稀疏层中的比例因子与特征通道相连, 通过稀疏正则化调节比例因子的稀疏性, 实现B-CNN中特征通道的稀疏性, 即对贡献低的特征通道值降低至零或接近零, 降低不重要特征通道对最后分类层的影响, 可明显改善细粒度图像分类精度.通过设置全局裁剪率, 裁剪一定比例的特征通道, 使网络忽略对识别结果影响低的特征, 提高关键特征的显著性, 有效解决训练集不是很大时产生过拟合的影响.实验表明, 利用剪枝可进一步提高方法在细粒度图像数据集上的识别准确度, 剪枝后的B-CNN网络的结构不变而复杂度降低.下一步的工作将通过研究区域检测算法, 使算法在只有类别标记的前提下, 有效完成对关键局部区域的定位工作, 获得更具判别性的特征表示.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="198" type="formula" href="images/MSSB201904008_19800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">马力</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="199" type="formula" href="images/MSSB201904008_19900.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">王永雄</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Birdlets: Subordinate categorization using volumetric primi-tives and pose-normalized appearance">

                                <b>[1]</b> FARRELL R, OZA O, ZHANG N, <i>et al</i>.Birdlets:Subordinate Categorization Using Volumetric Primitives and Pose-Normalized Appearance // Proc of the International Conference on Computer Vision.Washington, USA:IEEE, 2011:161-168.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pose Pooling Kernels for Sub-Category Recognition">

                                <b>[2]</b> ZHANG N, FARRELL R, DARRELL T.Pose Pooling Kernels for Sub-category Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2012:3665-3672.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201708002&amp;v=MTI2ODM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emdVcjdNS0NMZlliRzRIOWJNcDQ5RlpvUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 罗建豪, 吴建鑫.基于深度卷积特征的细粒度图像分类研究综述.自动化学报, 2017, 43 (8) :1306-1318. (LUO J H, WU J X.A Survey on Fine-Grained Image Categorization Using Deep Convolutional Features.Acta Automatica Sinica, 2017, 43 (8) :1306-1318.) 
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning">

                                <b>[4]</b> CUI Y, SONG Y, SUN C, <i>et al</i>.Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:4109-4118.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Attention-Based Spatially Recursive Networks for Fine-Grained Visual Recognition">

                                <b>[5]</b> WU L, WANG Y, LI X, <i>et al</i>.Deep Attention-Based Spatially Recursive Networks for Fine-Grained Visual Recognition.IEEE Tran-sactions on Cybernetics, 2019, 49 (5) :1791-1802.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bilinear CNN models for fine-grained visual recognition">

                                <b>[6]</b> LIN T Y, ROYCHOWDHURY A, MAJI S.Bilinear CNN Models for Fine-Grained Visual Recognition // Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:1449-1457.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" >
                                    <b>[7]</b>
                                 SIMONYAN K, ZISSERMAN A.Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL].[2018-12-06].https://arxiv.org/pdf/1409.1556.pdf.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimal brain damage">

                                <b>[8]</b> LECUN Y, DENKER J S, SOLLA S A.Optimal Brain Damage // TOURETZKY D S, ed.Advances in Neural Information Processing Systems 2.San Francisco, USA:Morgan Kaufmann Publishers, 1990:598-605.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving Neural Networks by Preventing Co-adaptation of Feature Detectors">

                                <b>[9]</b> HINTON G E, SRIVASTAVA N, KRIZHEVSKY A, <i>et al</i>.Improving Neural Networks by Preventing Co-adaptation of Feature Detectors[C/OL].[2018-12-06].https://arxiv.org/pdf/1207.0580.pdf.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bagging, boosting, and C4. 5">

                                <b>[10]</b> QUINLAN J R.Bagging, Boosting, and C4.5 // Proc of the 13th National Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 1996, I:725-730.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:accelerating deep network training by reducing internal covariate shift">

                                <b>[11]</b> IOFFE S, SZEGEDY C.Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift[C/OL].[2018-12-06].https://arxiv.org/pdf/1502.03167v3.pdf.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201903003&amp;v=MzAzOTA1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6Z1VyN01MejdTWkxHNEg5ak1ySTlGWjRRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 曹文龙, 芮建武, 李敏.神经网络模型压缩方法综述.计算机应用研究, 2019, 36 (3) :649-656. (CAO W L, BING J W, LI M.Survey on Neural Network Model Compression Methods.Application Research of Computers, 2019, 36 (3) :649-656.) 
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Predicting Parameters in Deep Learning[C/OL]">

                                <b>[13]</b> DENIL M, SHAKIBI B, DINH L, <i>et al</i>.Predicting Parameters in Deep Learning[C/OL].[2018-12-06].https://arxiv.org/pdf/1306.0543.pdf.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep compression:compressing deep neural networks with pruning trained quantization and huffman coding">

                                <b>[14]</b> HAN S, MAO H Z, DALLY W J.Deep Compression:Compre-ssing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding[C/OL].[2018-12-06].https://arxiv.org/pdf/1510.00149.pdf.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Structured Sparsity in Deep Neural Networks[C/OL]">

                                <b>[15]</b> WEN W, WU C P, WANG Y D, <i>et al</i>.Learning Structured Sparsity in Deep Neural Networks[C/OL].[2018-12-06].https://arxiv.org/pdf/1608.03665.pdf.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning efficient convolutional networks through network slimming">

                                <b>[16]</b> LIU Z, LI J G, SHEN Z Q, <i>et al</i>.Learning Efficient Convolutional Networks through Network Slimming // Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2017:2755-2763.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pruning Filters for Efficient ConvNets[C/OL]">

                                <b>[17]</b> LI H, KADAV A, DURDANOVIC I, <i>et al</i>.Pruning Filters for Efficient Convents[C/OL].[2018-12-06].https://arxiv.org/pdf/1608.08710.pdf.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fine-Grained Visual Classification of Aircraft[C/OL]">

                                <b>[18]</b> MAJI S, RAHTU E, KANNALA J, <i>et al</i>.Fine-Grained Visual Classification of Aircraft[C/OL].[2018-12-06].https://arxiv.org/pdf/1306.5151.pdf.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Novel Dataset for Fine-Grained Image Categorization:Stanford Dogs[C/OL]">

                                <b>[19]</b> KHOSLA A, JAYADEVAPRAKASH N, YAO B P, <i>et al</i>.Novel Dataset for Fine-Grained Image Categorization:Stanford Dogs[C/OL].[2018-12-06].http://59.80.44.98/people.csail.mit.edu/khosla/papers/fgvc2011.pdf.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3d object representations for fine-grained categorization">

                                <b>[20]</b> KRAUSE J, STARK M, DENG J, <i>et al</i>.3D Object Representations for Fine-Grained Categorization // Proc of the IEEE International Conference on Computer Vision Workshops.Washington, USA:IEEE, 2013:554-561.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Return of the Devil in the Details:Delving Deep into Convolutional Nets[C/OL]">

                                <b>[21]</b> CHATFIELD K, SIMONYAN K, VEDALDI A, <i>et al</i>.Return of the Devil in the Details:Delving Deep into Convolutional Nets[C/OL].[2018-12-06].https://arxiv.org/pdf/1405.3531.pdf.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700171523&amp;v=MTk0NTU4SDlETXFJOUZaZXdPQ1g0Nm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGc1ZheFk9TmlmT2ZiSw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> GOSSELIN P H, MURRAY N, JÉGOU H, <i>et al</i>.Revisiting the Fisher Vector for Fine-Grained Classification.Pattern Recognition Letters, 2014, 49:92-98.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201609004&amp;v=MzE2MDFyQ1VSTE9lWmVSbkZ5emdVcjdNUHlyZmJMRzRIOWZNcG85RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> 冯语姗, 王子磊.自上而下注意图分割的细粒度图像分类.中国图象图形学报, 2016, 21 (9) :1147-1154. (FENG Y S, WANG Z L.Fine-Grained Image Categorization with Segmentation Based on Top-Down Attention Map.Journal of Image and Graphics, 2016, 21 (9) :1147-1154.) 
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural activation constellations:unsupervised part model discovery with convolutional networks">

                                <b>[24]</b> SIMON M, RODNER E.Neural Activation Constellations:Unsupervised Part Model Discovery with Convolutional Networks // Proc of the IEEE International Conference on Computer Vision.Wa-shington, USA:IEEE, 2015:1143-1151.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Picking deep filter responses for fine-grained image recognition">

                                <b>[25]</b> ZHANG X P, XIONG H K, ZHOU W G, <i>et al</i>.Picking Deep Filter Responses for Fine-Grained Image Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:1134-1142.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Diversified visual attention networks for fine-grained object classification">

                                <b>[26]</b> ZHAO B, WU X, FENG J S, <i>et al</i>.Diversified Visual Attention Networks for Fine-Grained Object Classification.IEEE Transactions on Multimedia, 2017, 19 (6) :1245-1256.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully Convolutional Attention Networks for Fine-Grained Recognition[C/OL]">

                                <b>[27]</b> LIU X, XIA T, WANG J, <i>et al</i>.Fully Convolutional Attention Networks for Fine-Grained Recognition[C/OL].[2018-12-06].https://arxiv.org/pdf/1603.06765.pdf.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[28]</b> GIRSHICK R, DONAHUE J, DARRELL T, <i>et al</i>.Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:580-587.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fine-grained recognition without part annotations">

                                <b>[29]</b> KRAUSE J, JIN H L, YANG J C, <i>et al</i>.Fine-Grained Recognition without Part Annotations // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:5546-5555.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201904008" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201904008&amp;v=MjYwMDdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6Z1VyN01LRDdZYkxHNEg5ak1xNDlGYklRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
