<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131456142530000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201908008%26RESULT%3d1%26SIGN%3dw9IIoevG3O2VHDi3PN%252biOImkA0k%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201908008&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201908008&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201908008&amp;v=MzA0MDVMT2VaZVJuRnkva1VyL05LRDdZYkxHNEg5ak1wNDlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#428" data-title="1 相关知识 ">1 相关知识</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#432" data-title="&lt;b&gt;1.1&lt;/b&gt; 正则化贝叶斯推理"><b>1.1</b> 正则化贝叶斯推理</a></li>
                                                <li><a href="#442" data-title="&lt;b&gt;1.2&lt;/b&gt; 潜在的狄利克雷分配"><b>1.2</b> 潜在的狄利克雷分配</a></li>
                                                <li><a href="#460" data-title="&lt;b&gt;1.3&lt;/b&gt; 分类器"><b>1.3</b> 分类器</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#474" data-title="2 最大熵判别LDA的坐标下降算法 ">2 最大熵判别LDA的坐标下降算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#477" data-title="&lt;b&gt;2.1&lt;/b&gt; 简化的优化问题"><b>2.1</b> 简化的优化问题</a></li>
                                                <li><a href="#497" data-title="&lt;b&gt;2.2&lt;/b&gt; 坐标下降算法"><b>2.2</b> 坐标下降算法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#531" data-title="3 快速采样算法 ">3 快速采样算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#533" data-title="&lt;b&gt;3.1&lt;/b&gt; 吉布斯采样"><b>3.1</b> 吉布斯采样</a></li>
                                                <li><a href="#554" data-title="&lt;b&gt;3.2&lt;/b&gt; 拒绝采样算法"><b>3.2</b> 拒绝采样算法</a></li>
                                                <li><a href="#622" data-title="&lt;b&gt;3.3&lt;/b&gt; 预处理算法"><b>3.3</b> 预处理算法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#629" data-title="4 实验及结果分析 ">4 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#636" data-title="&lt;b&gt;4.1&lt;/b&gt; 收敛速度及训练时间"><b>4.1</b> 收敛速度及训练时间</a></li>
                                                <li><a href="#648" data-title="&lt;b&gt;4.2&lt;/b&gt; 分类器训练轮数的影响"><b>4.2</b> 分类器训练轮数的影响</a></li>
                                                <li><a href="#651" data-title="&lt;b&gt;4.3&lt;/b&gt; 超参数&lt;i&gt;ε&lt;/i&gt;的选择"><b>4.3</b> 超参数<i>ε</i>的选择</a></li>
                                                <li><a href="#658" data-title="&lt;b&gt;4.4&lt;/b&gt; 训练时间随主题数量的变化"><b>4.4</b> 训练时间随主题数量的变化</a></li>
                                                <li><a href="#661" data-title="&lt;b&gt;4.5&lt;/b&gt; 影响时间复杂度的值"><b>4.5</b> 影响时间复杂度的值</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#664" data-title="5 结束语 ">5 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#627" data-title="&lt;b&gt;表1 2种算法的时间复杂度对比&lt;/b&gt;"><b>表1 2种算法的时间复杂度对比</b></a></li>
                                                <li><a href="#632" data-title="&lt;b&gt;表2 实验数据集&lt;/b&gt;"><b>表2 实验数据集</b></a></li>
                                                <li><a href="#635" data-title="&lt;b&gt;表3 各数据集选择的超参数&lt;/b&gt;"><b>表3 各数据集选择的超参数</b></a></li>
                                                <li><a href="#643" data-title="图1 4种算法在3个数据集上的收敛速度对比">图1 4种算法在3个数据集上的收敛速度对比</a></li>
                                                <li><a href="#647" data-title="图2 4种算法在3个数据集上的训练时间对比">图2 4种算法在3个数据集上的训练时间对比</a></li>
                                                <li><a href="#650" data-title="图3 不同&lt;i&gt;i&lt;/i&gt;&lt;sub&gt;SVM&lt;/sub&gt;对训练时间的影响">图3 不同<i>i</i><sub>SVM</sub>对训练时间的影响</a></li>
                                                <li><a href="#656" data-title="图4 ε对拒绝率和稀疏率的影响">图4 ε对拒绝率和稀疏率的影响</a></li>
                                                <li><a href="#657" data-title="图5 ε对采样时间的影响">图5 ε对采样时间的影响</a></li>
                                                <li><a href="#660" data-title="图6 2种算法的训练时间随主题数量的变化情况">图6 2种算法的训练时间随主题数量的变化情况</a></li>
                                                <li><a href="#663" data-title="&lt;b&gt;表4 影响时间复杂度的常数取值&lt;/b&gt;"><b>表4 影响时间复杂度的常数取值</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="354">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                     BLEI D M, NG A Y, JORDAN M I.Latent Dirichlet Allocation.Journal of Machine Learning Research, 2003, 3:993-1022.</a>
                                </li>
                                <li id="356">


                                    <a id="bibliography_2" title=" BLEI D M, LAFFERTY J D.Correlated Topic Models[C/OL].[2019-04-21].http://people.ee.duke.edu/～lcarin/Blei2005CTM.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Correlated Topic Models[C/OL]">
                                        <b>[2]</b>
                                         BLEI D M, LAFFERTY J D.Correlated Topic Models[C/OL].[2019-04-21].http://people.ee.duke.edu/～lcarin/Blei2005CTM.pdf.
                                    </a>
                                </li>
                                <li id="358">


                                    <a id="bibliography_3" title=" BLEI D M, LAFFERTY J D.Dynamic Topic Models // Proc of the 23rd International Conference on Machine Learning.Berlin, Germany:Springer, 2006.DOI:10.1145/1143844.1143859." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dynamic topic models">
                                        <b>[3]</b>
                                         BLEI D M, LAFFERTY J D.Dynamic Topic Models // Proc of the 23rd International Conference on Machine Learning.Berlin, Germany:Springer, 2006.DOI:10.1145/1143844.1143859.
                                    </a>
                                </li>
                                <li id="360">


                                    <a id="bibliography_4" title=" CHEN J F, ZHU J, LU J, &lt;i&gt;et al&lt;/i&gt;.Scalable Training of Hierarchical Topic Models.Proceedings of the VLDB Endowment, 2018, 11 (7) :826-839." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalable Training of Hierarchical Topic Models">
                                        <b>[4]</b>
                                         CHEN J F, ZHU J, LU J, &lt;i&gt;et al&lt;/i&gt;.Scalable Training of Hierarchical Topic Models.Proceedings of the VLDB Endowment, 2018, 11 (7) :826-839.
                                    </a>
                                </li>
                                <li id="362">


                                    <a id="bibliography_5" title=" BLEI D M, GRIFFITHS T L, JORDAN M I, &lt;i&gt;et al&lt;/i&gt;.Hierarchical Topic Models and the Nested Chinese Restaurant Process[C/OL].[2019-04-21].https://people.eecs.berkeley.edu/～jordan/papers/lda-crp.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical Topic Models and the Nested Chinese Restaurant Process[C/OL]">
                                        <b>[5]</b>
                                         BLEI D M, GRIFFITHS T L, JORDAN M I, &lt;i&gt;et al&lt;/i&gt;.Hierarchical Topic Models and the Nested Chinese Restaurant Process[C/OL].[2019-04-21].https://people.eecs.berkeley.edu/～jordan/papers/lda-crp.pdf.
                                    </a>
                                </li>
                                <li id="364">


                                    <a id="bibliography_6" title=" MIAO Y S, YU L, BLUNSOM P.Neural Variational Inference for Text Processing // Proc of the 23rd International Conference on Machine Learning.Berlin, Germany:Springer, 2016:1727-1736." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural variational inference for text processing">
                                        <b>[6]</b>
                                         MIAO Y S, YU L, BLUNSOM P.Neural Variational Inference for Text Processing // Proc of the 23rd International Conference on Machine Learning.Berlin, Germany:Springer, 2016:1727-1736.
                                    </a>
                                </li>
                                <li id="366">


                                    <a id="bibliography_7" title=" WEI X, CROFT W B.LDA-Based Document Models for AD-HOC Retrieval // Proc of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM, 2006:178-185." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LDA-based document models for Ad-hoc retrieval">
                                        <b>[7]</b>
                                         WEI X, CROFT W B.LDA-Based Document Models for AD-HOC Retrieval // Proc of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM, 2006:178-185.
                                    </a>
                                </li>
                                <li id="368">


                                    <a id="bibliography_8" title=" WANG C, BLEI D M.Collaborative Topic Modeling for Recommending Scientific Articles // Proc of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2011:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Collaborative topic modeling for recommending scientific arti-cles">
                                        <b>[8]</b>
                                         WANG C, BLEI D M.Collaborative Topic Modeling for Recommending Scientific Articles // Proc of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2011:448-456.
                                    </a>
                                </li>
                                <li id="370">


                                    <a id="bibliography_9" title=" LIU S X, WANG X T, LIU J F, &lt;i&gt;et al&lt;/i&gt;.TopicPanorama:A Full Picture of Relevant Topics // Proc of the IEEE Conference on Visual Analytics Science and Technology.Washington, USA:IEEE, 2014:183-192." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Topic Panorama:A full picture of relevant topics">
                                        <b>[9]</b>
                                         LIU S X, WANG X T, LIU J F, &lt;i&gt;et al&lt;/i&gt;.TopicPanorama:A Full Picture of Relevant Topics // Proc of the IEEE Conference on Visual Analytics Science and Technology.Washington, USA:IEEE, 2014:183-192.
                                    </a>
                                </li>
                                <li id="372">


                                    <a id="bibliography_10" title=" WANG Y, ZHAO X M, SUN Z L, &lt;i&gt;et al&lt;/i&gt;.Towards Topic Modeling for Big Data[C/OL].[2019-04-21].https://arxiv.org/pdf/1405.4402v1.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards Topic Modeling for Big Data[C/OL]">
                                        <b>[10]</b>
                                         WANG Y, ZHAO X M, SUN Z L, &lt;i&gt;et al&lt;/i&gt;.Towards Topic Modeling for Big Data[C/OL].[2019-04-21].https://arxiv.org/pdf/1405.4402v1.pdf.
                                    </a>
                                </li>
                                <li id="374">


                                    <a id="bibliography_11" title=" DONG Y P, SU H, ZHU J, &lt;i&gt;et al&lt;/i&gt;.Improving Interpretability of Deep Neural Networks with Semantic Information // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:4306-4314." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving Interpretability of Deep Neural Networks with Semantic Information">
                                        <b>[11]</b>
                                         DONG Y P, SU H, ZHU J, &lt;i&gt;et al&lt;/i&gt;.Improving Interpretability of Deep Neural Networks with Semantic Information // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:4306-4314.
                                    </a>
                                </li>
                                <li id="376">


                                    <a id="bibliography_12" title=" MCAULIFFE J D, BLEI D M.Supervised Topic Models // PLATT J C, KOLLER D, SINGER Y, &lt;i&gt;et al&lt;/i&gt;., eds.Advances in Neural Information Processing Systems 20.Cambridge, USA:The MIT Press, 2008:121-128." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised topic models">
                                        <b>[12]</b>
                                         MCAULIFFE J D, BLEI D M.Supervised Topic Models // PLATT J C, KOLLER D, SINGER Y, &lt;i&gt;et al&lt;/i&gt;., eds.Advances in Neural Information Processing Systems 20.Cambridge, USA:The MIT Press, 2008:121-128.
                                    </a>
                                </li>
                                <li id="378">


                                    <a id="bibliography_13" title=" LACOSTE-JULIEN S, SHA F, JORDAN M I.DiscLDA:Discriminative Learning for Dimensionality Reduction and Classification // KOLLER D, SCHUURMANS D, BENGIO Y, &lt;i&gt;et al&lt;/i&gt;., eds.Advances in Neural Information Processing Systems 21.Cambridge, USA:The MIT Press, 2009:897-904." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Disclda:Discriminative learning for dimensionality reduction and classification">
                                        <b>[13]</b>
                                         LACOSTE-JULIEN S, SHA F, JORDAN M I.DiscLDA:Discriminative Learning for Dimensionality Reduction and Classification // KOLLER D, SCHUURMANS D, BENGIO Y, &lt;i&gt;et al&lt;/i&gt;., eds.Advances in Neural Information Processing Systems 21.Cambridge, USA:The MIT Press, 2009:897-904.
                                    </a>
                                </li>
                                <li id="380">


                                    <a id="bibliography_14" title=" ZHU J, AHMED A, XING E P.MedLDA:Maximum Margin Supervised Topic Models.Journal of Machine Learning Research, 2012, 13:2237-2278." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MedLDA: Maximum margin supervised topic models">
                                        <b>[14]</b>
                                         ZHU J, AHMED A, XING E P.MedLDA:Maximum Margin Supervised Topic Models.Journal of Machine Learning Research, 2012, 13:2237-2278.
                                    </a>
                                </li>
                                <li id="382">


                                    <a id="bibliography_15" title=" ZHU J, CHEN N, PERKINS H, &lt;i&gt;et al&lt;/i&gt;.Gibbs Max-Margin Topic Models with Data Augmentation.Journal of Machine Learning Research, 2014, 15:1073-1110." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gibbs Max-margin Topic Models with Data Augmentation">
                                        <b>[15]</b>
                                         ZHU J, CHEN N, PERKINS H, &lt;i&gt;et al&lt;/i&gt;.Gibbs Max-Margin Topic Models with Data Augmentation.Journal of Machine Learning Research, 2014, 15:1073-1110.
                                    </a>
                                </li>
                                <li id="384">


                                    <a id="bibliography_16" title=" YAO L M, MIMNO D, MCCALLUM A.Efficient Methods for To-pic Model Inference on Streaming Document Collections // Proc of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2009:937-946." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient methods for topic model inference on streaming document collections">
                                        <b>[16]</b>
                                         YAO L M, MIMNO D, MCCALLUM A.Efficient Methods for To-pic Model Inference on Streaming Document Collections // Proc of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2009:937-946.
                                    </a>
                                </li>
                                <li id="386">


                                    <a id="bibliography_17" title=" LI A Q, AHMED A, RAVI S, &lt;i&gt;et al&lt;/i&gt;.Reducing the Sampling Complexity of Topic Models // Proc of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2014:891-900." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reducing the Sampling Complexity of Topic Models">
                                        <b>[17]</b>
                                         LI A Q, AHMED A, RAVI S, &lt;i&gt;et al&lt;/i&gt;.Reducing the Sampling Complexity of Topic Models // Proc of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2014:891-900.
                                    </a>
                                </li>
                                <li id="388">


                                    <a id="bibliography_18" title=" YU H F, HSIEH C J, YUN H, &lt;i&gt;et al&lt;/i&gt;.A Scalable Asynchronous Distributed Algorithm for Topic Modeling // Proc of the 24th International Conference on World Wide Web.New York, USA:ACM, 2015:1340-1350." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Scalable Asynchronous Distributed Algorithm for Topic Modeling">
                                        <b>[18]</b>
                                         YU H F, HSIEH C J, YUN H, &lt;i&gt;et al&lt;/i&gt;.A Scalable Asynchronous Distributed Algorithm for Topic Modeling // Proc of the 24th International Conference on World Wide Web.New York, USA:ACM, 2015:1340-1350.
                                    </a>
                                </li>
                                <li id="390">


                                    <a id="bibliography_19" title=" YUAN J H, GAO F, HO Q R, &lt;i&gt;et al&lt;/i&gt;.LightLDA:Big Topic Models on Modest Computer Clusters // Proc of the 24th International Conference on World Wide Web.New York, USA:ACM, 2015:1351-1361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lightlda:Big Topic Models on Modest Computer Clusters">
                                        <b>[19]</b>
                                         YUAN J H, GAO F, HO Q R, &lt;i&gt;et al&lt;/i&gt;.LightLDA:Big Topic Models on Modest Computer Clusters // Proc of the 24th International Conference on World Wide Web.New York, USA:ACM, 2015:1351-1361.
                                    </a>
                                </li>
                                <li id="392">


                                    <a id="bibliography_20" title=" CHEN J F, LI K W, ZHU J, &lt;i&gt;et al&lt;/i&gt;.WarpLDA:A Cache Efficient O (1) Algorithm for Latent Dirichlet Allocation.Proceedings of the VLDB Endowment, 2016, 9 (10) :744-755." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=WarpLDA:a cache efficient O(1)algorithm for latent dirichlet allocation">
                                        <b>[20]</b>
                                         CHEN J F, LI K W, ZHU J, &lt;i&gt;et al&lt;/i&gt;.WarpLDA:A Cache Efficient O (1) Algorithm for Latent Dirichlet Allocation.Proceedings of the VLDB Endowment, 2016, 9 (10) :744-755.
                                    </a>
                                </li>
                                <li id="394">


                                    <a id="bibliography_21" title=" JIANG Q X, ZHU J, SUN M S, &lt;i&gt;et al&lt;/i&gt;.Monte Carlo Methods for Maximum Margin Supervised Topic Models // BENGIO S, WALLACH H M, LAROCHELLE H, &lt;i&gt;et al&lt;/i&gt;., eds.Advances in Neural Information Processing Systems 25.Cambridge, USA:The MIT Press, 2012:1592-1600." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Monte carlo methods for maximum margin supervised topic models">
                                        <b>[21]</b>
                                         JIANG Q X, ZHU J, SUN M S, &lt;i&gt;et al&lt;/i&gt;.Monte Carlo Methods for Maximum Margin Supervised Topic Models // BENGIO S, WALLACH H M, LAROCHELLE H, &lt;i&gt;et al&lt;/i&gt;., eds.Advances in Neural Information Processing Systems 25.Cambridge, USA:The MIT Press, 2012:1592-1600.
                                    </a>
                                </li>
                                <li id="396">


                                    <a id="bibliography_22" title=" ZHU J, CHEN J F, HU W B, &lt;i&gt;et al&lt;/i&gt;.Big Learning with Bayesian Methods.National Science Review, 2017, 4 (4) :627-651." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NASR201704019&amp;v=MzE1NjF5ellmTEc0SDliTXE0OUViWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVXIvTks=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         ZHU J, CHEN J F, HU W B, &lt;i&gt;et al&lt;/i&gt;.Big Learning with Bayesian Methods.National Science Review, 2017, 4 (4) :627-651.
                                    </a>
                                </li>
                                <li id="398">


                                    <a id="bibliography_23" title=" MEI S K, ZHU J, ZHU X J.Robust RegBayes:Selectively Incorporating First-Order Logic Domain Knowledge into Bayesian Models // Proc of the 31st International Conference on Machine Learning.Berlin, Germany:Springer, 2014:253-261." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust RegBayes:Selectively incorporating first-order logic domain knowledge into Bayesian models">
                                        <b>[23]</b>
                                         MEI S K, ZHU J, ZHU X J.Robust RegBayes:Selectively Incorporating First-Order Logic Domain Knowledge into Bayesian Models // Proc of the 31st International Conference on Machine Learning.Berlin, Germany:Springer, 2014:253-261.
                                    </a>
                                </li>
                                <li id="400">


                                    <a id="bibliography_24" title=" KOYEJO O, GHOSH J.Constrained Bayesian Inference for Low Rank Multitask Learning // Proc of the 29th Conference on Uncertainty in Artificial Intelligence.Bellevue, USA:AUAI Press, 2013:341-350." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Constrained Bayesian inference for low rank multitask learning">
                                        <b>[24]</b>
                                         KOYEJO O, GHOSH J.Constrained Bayesian Inference for Low Rank Multitask Learning // Proc of the 29th Conference on Uncertainty in Artificial Intelligence.Bellevue, USA:AUAI Press, 2013:341-350.
                                    </a>
                                </li>
                                <li id="402">


                                    <a id="bibliography_25" title=" GRIFFITHS T L, STEYVERS M.Finding Scientific Topics.Proceedings of the National Academy of Sciences of the United States of America, 2004, 101 (s1) :5228-5235." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Finding scientific topics">
                                        <b>[25]</b>
                                         GRIFFITHS T L, STEYVERS M.Finding Scientific Topics.Proceedings of the National Academy of Sciences of the United States of America, 2004, 101 (s1) :5228-5235.
                                    </a>
                                </li>
                                <li id="404">


                                    <a id="bibliography_26" title=" ZHENG X, YU Y L, XING E P.Linear Time Samplers for Supervised Topic Models Using Compositional Proposals // Proc of the 21st ACM SIGKDD International Conference on Knowledge Disco-very and Data Mining.New York, USA:ACM, 2015:1523-1532." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Linear time samplers for supervised topic models using compositional proposals">
                                        <b>[26]</b>
                                         ZHENG X, YU Y L, XING E P.Linear Time Samplers for Supervised Topic Models Using Compositional Proposals // Proc of the 21st ACM SIGKDD International Conference on Knowledge Disco-very and Data Mining.New York, USA:ACM, 2015:1523-1532.
                                    </a>
                                </li>
                                <li id="406">


                                    <a id="bibliography_27" title=" HSIEH C J, CHANG K W, LIN C J, &lt;i&gt;et al&lt;/i&gt;.A Dual Coordinate De-scent Method for Large-Scale Linear SVM // Proc of the International Conference on Machine Learning.Berlin, Germany:Sprin-ger, 2008:408-415." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A dual coordinate descent method for large-scale linear SVM">
                                        <b>[27]</b>
                                         HSIEH C J, CHANG K W, LIN C J, &lt;i&gt;et al&lt;/i&gt;.A Dual Coordinate De-scent Method for Large-Scale Linear SVM // Proc of the International Conference on Machine Learning.Berlin, Germany:Sprin-ger, 2008:408-415.
                                    </a>
                                </li>
                                <li id="408">


                                    <a id="bibliography_28" title=" SHALEV-SHWARTZ S, ZHANG T.Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization.Journal of Machine Learning Research, 2013, 14:567-599." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stochastic dual coordinate ascent methods for regularized loss">
                                        <b>[28]</b>
                                         SHALEV-SHWARTZ S, ZHANG T.Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization.Journal of Machine Learning Research, 2013, 14:567-599.
                                    </a>
                                </li>
                                <li id="410">


                                    <a id="bibliography_29" title=" LEWIS D D, YANG Y M, ROSE T G, &lt;i&gt;et al&lt;/i&gt;.RCV1:A New Benchmark Collection for Text Categorization Research.Journal of Machine Learning Research, 2004, 5:361-397." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rcv1: A new benchmark collection for text categorization research">
                                        <b>[29]</b>
                                         LEWIS D D, YANG Y M, ROSE T G, &lt;i&gt;et al&lt;/i&gt;.RCV1:A New Benchmark Collection for Text Categorization Research.Journal of Machine Learning Research, 2004, 5:361-397.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(08),736-745 DOI:10.16451/j.cnki.issn1003-6059.201908007            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>最大熵判别主题模型的高效学习算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E9%94%AE%E9%A3%9E&amp;code=31246474&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈键飞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B1%E5%86%9B&amp;code=08189228&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朱军</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E7%B3%BB&amp;code=0187103&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">清华大学计算机科学与技术系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>现有的有监督主题模型训练算法的时间复杂度一般线性于主题数量, 限制了其大规模应用.基于此种情况, 文中提出最大熵判别潜在狄利克雷分配 (MedLDA) 有监督主题模型的高效学习算法.算法为坐标下降算法, 训练分类器的迭代次数少于MedLDA已有的蒙特卡洛算法.算法还利用拒绝采样及高效的预处理技术, 将训练的时间复杂度从线性于主题数量降至亚线性于主题数量.在多个文本数据集上的对比实验表明, 相比原有的蒙特卡洛算法, 文中算法在训练速度上有大幅提升.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%89%E7%9B%91%E7%9D%A3%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">有监督主题模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9D%90%E6%A0%87%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">坐标下降算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%90%89%E5%B8%83%E6%96%AF%E9%87%87%E6%A0%B7%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吉布斯采样算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%8B%92%E7%BB%9D%E9%87%87%E6%A0%B7%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">拒绝采样算法;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *陈键飞, 博士, 主要研究方向为大规模机器学习、概率推理、主题模型.E-mail:chenjian14@mails.tsinghua.edu.cn.;
                                </span>
                                <span>
                                    *朱军, 博士, 教授, 主要研究方向为机器学习.E-mail:dcszj@mail.tsinghua.edu.cn.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-12</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金重点国际合作项目 (No.61620106010);</span>
                                <span>北京市自然科学基金重点专题项目 (No.L172037) 资助;</span>
                    </p>
            </div>
                    <h1><b>Efficient Learning Algorithm for Maximum Entropy Discrimination Topic Models</b></h1>
                    <h2>
                    <span>CHEN Jianfei</span>
                    <span>ZHU Jun</span>
            </h2>
                    <h2>
                    <span>Department of Computer Science and Technology, Tsinghua University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Time complexity of the existing supervised topic model training algorithms is generally linear to the number of topics and therefore their large-scale application is limited. To solve this problem, an efficient learning algorithm for maximum entropy discrimination of latent Dirichlet allocation (MedLDA) supervised subject model is proposed in this paper. The proposed algorithm is based on coordinate descent, and the number of iterations of training classifiers is less than that of the existing Monte Carlo algorithm for MedLDA. The algorithm also makes use of rejection sampling and efficient preprocessing technique to reduce the time complexity of training from linear to sub-linear with respect to the number of topics. The comparison experiments on multiple text corpora show that the proposed algorithm makes a great improvement in training speed compared with the existing Monte Carlo algorithm.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Supervised%20Topic%20Models&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Supervised Topic Models;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Coordinate%20Descent&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Coordinate Descent;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Gibbs%20Sampling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Gibbs Sampling;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Rejection%20Sampling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Rejection Sampling;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    CHEN Jianfei, Ph.D.His research interests include largescale machine learning, probabilistic inference and topic models.;
                                </span>
                                <span>
                                    ZHU Jun, Ph.D., professor.His research interest includes machine learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-12</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No61620106010);</span>
                                <span>Beijing Natural Science Foundation (No.L172037);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="424">主题模型<citation id="666" type="reference"><link href="354" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>是重要的文本特征学习工具.这类模型可以从文本数据集中发现主题, 即语义相关联的词群, 并将文档分解为主题的分布.由于其良好的特征学习效果和可解释性, 主题模型及其变体<citation id="672" type="reference"><link href="356" rel="bibliography" /><link href="358" rel="bibliography" /><link href="360" rel="bibliography" /><link href="362" rel="bibliography" /><link href="364" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>被广泛应用于信息检索<citation id="667" type="reference"><link href="366" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、推荐系统<citation id="668" type="reference"><link href="368" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、数据可视化<citation id="669" type="reference"><link href="370" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、在线广告<citation id="670" type="reference"><link href="372" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、可解释学习<citation id="671" type="reference"><link href="374" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>等领域.</p>
                </div>
                <div class="p1">
                    <p id="425">传统的主题模型是无监督的, 如潜在狄利克雷分配 (Latent Dirichlet Allocation, LDA) <citation id="673" type="reference"><link href="354" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.它们只利用文本本身的信息, 而未利用额外的标签.在实际应用中, 存在着大量有标注文本数据集, 如在线广告中使用的点击通过率数据集<citation id="674" type="reference"><link href="372" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>等.有监督主题模型是主题模型的一个变体.通过联合建模文本内容和标签的概率分布, 有监督主题模型不但能够进行文本分类, 还可以利用监督信息学到更好的主题表示.典型的有监督主题模型包括有监督LDA (Supervised LDA, sLDA) <citation id="675" type="reference"><link href="376" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、判别式LDA (Discriminative LDA, DiscLDA) <citation id="676" type="reference"><link href="378" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、最大熵判别LDA (Maximum Entropy Discrimination LDA, MedLDA) <citation id="677" type="reference"><link href="380" rel="bibliography" /><link href="382" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>等.</p>
                </div>
                <div class="p1">
                    <p id="426">为了使主题模型能够处理海量的文本数据集, 需要更快、时间复杂度更低的训练算法.利用主题模型的稀疏性, 学者们提出许多高效算法, 包括SparseLDA<citation id="678" type="reference"><link href="384" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、AliasLDA<citation id="679" type="reference"><link href="386" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、F+LDA<citation id="680" type="reference"><link href="388" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、LightLDA<citation id="681" type="reference"><link href="390" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、WarpLDA<citation id="682" type="reference"><link href="392" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>等.然而, 由于有监督主题模型使用非共轭的先验, 这些高效算法不能直接应用于有监督主题模型.对于有监督主题模型, 已有的蒙特卡洛算法<citation id="683" type="reference"><link href="394" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>的时间复杂度是线性于主题数量的, 远高于LDA的亚线性、甚至常数级的时间复杂度.有监督主题模型训练算法的高时间复杂度限制其推广到多主题、多类别的数据集上.</p>
                </div>
                <div class="p1">
                    <p id="427">本文针对典型的有监督主题模型MedLDA, 提出高效学习算法.基于正则化贝叶斯学习<citation id="684" type="reference"><link href="396" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>, MedLDA融合最大间隔准则和贝叶斯学习.由于结合两者的优势, 相比sLDA和DiscLDA, MedLDA的分类器只依赖于很少一部分支持向量, 在提升模型鲁棒性的同时使更高效的训练成为可能.本文利用MedLDA这一性质, 降低训练过程中学习分类器和采样话题的时间复杂度, 提升总体的训练效率.具体地, 本文提出MedLDA的坐标下降算法, 减少分类器训练的迭代次数, 并提出拒绝性采样算法, 将采样每个词的时间复杂度从线性于主题数量降至亚线性于主题数量.最后提出利用支持向量稀疏性的预处理算法, 将预处理每篇文档的时间复杂度从线性于类别数量降至亚线性于类别数量.在20NewsGroups、Wiki、RCV1等常用数据集上的实验表明, 本文算法的训练速度是MedLDA现有的蒙特卡洛算法<citation id="685" type="reference"><link href="394" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>的4倍.</p>
                </div>
                <h3 id="428" name="428" class="anchor-tag">1 相关知识</h3>
                <div class="p1">
                    <p id="429">MedLDA是一个有监督主题模型, 同时处理如下2个任务:1) 对文档主题进行建模;2) 利用文档的主题分布预测其标签.模型的输入为<i>D</i>篇文档构成的数据集 (<i>W</i>, <b><i>Y</i></b>) , 其中<i>W</i>={<b><i>w</i></b><sub><i>d</i></sub>}<mathml id="430"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup></mrow></math></mathml>为文档内容, 每篇文档<b><i>w</i></b><sub><i>d</i></sub>={<i>w</i><sub><i>dn</i></sub>}<mathml id="431"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>d</mi></msub></mrow></msubsup></mrow></math></mathml>由<i>N</i><sub><i>d</i></sub>个词构成, <i>w</i><sub><i>dn</i></sub>∈{1, 2, …, <i>V</i>}为其中第<i>n</i>个词在<i>V</i>个词构成的词汇表中的序号.<b><i>Y</i></b>为一个<i>D</i>×<i>L</i>的标签矩阵, 其中<i>y</i><sub><i>dl</i></sub>∈{-1, 1}表示第<i>d</i>篇文档的第<i>l</i>个标签.这里描述一个多任务学习的场景, 即每个文档有<i>L</i>个标签.特别地, 若只需要对文档进行二分类, 则<i>L</i>=1;若需要对文档进行多分类, 则每篇文档<i>d</i>有且仅有一个<i>l</i>, 使得<i>y</i><sub><i>dl</i></sub>=±1.</p>
                </div>
                <h4 class="anchor-tag" id="432" name="432"><b>1.1</b> 正则化贝叶斯推理</h4>
                <div class="p1">
                    <p id="433">MedLDA为正则化贝叶斯推理框架<citation id="686" type="reference"><link href="396" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>的一个具体例子.正则化贝叶斯推理在贝叶斯推理基础上, 引入正则化项, 使模型更灵活.具体地, 令<i>Θ</i>为模型参数、<b>D</b>为数据集, 则给定先验<i>p</i> (<i>Θ</i>) 和似然度<mathml id="434"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold">D</mi><mo stretchy="false">|</mo><mi>Θ</mi><mo stretchy="false">) </mo></mrow></math></mathml>, 由贝叶斯公式, 模型参数的后验应为</p>
                </div>
                <div class="p1">
                    <p id="435" class="code-formula">
                        <mathml id="435"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>Θ</mi><mo stretchy="false">|</mo><mi mathvariant="bold">D</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>Θ</mi><mo stretchy="false">) </mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold">D</mi><mo stretchy="false">|</mo><mi>Θ</mi><mo stretchy="false">) </mo></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold">D</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="436">贝叶斯推理的过程可以使用一个等效的求解变分分布<i>q</i> (<i>Θ</i>) 的变分问题描述:</p>
                </div>
                <div class="p1">
                    <p id="437" class="code-formula">
                        <mathml id="437"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>q</mi><mo stretchy="false"> (</mo><mi>Θ</mi><mo stretchy="false">) </mo></mrow></munder><mspace width="0.25em" /><mtext>Κ</mtext><mtext>L</mtext><mo stretchy="false"> (</mo><mi>q</mi><mo stretchy="false"> (</mo><mi>Θ</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>Θ</mi><mo stretchy="false">|</mo><mi mathvariant="bold">D</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="438">正则化贝叶斯推理在贝叶斯基础上, 引入后验正则化项<i>Ω</i> (<i>q</i> (<i>Θ</i>) ;<b>D</b>) , 并定义如下优化问题:</p>
                </div>
                <div class="p1">
                    <p id="439"><mathml id="440"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>q</mi><mo stretchy="false"> (</mo><mi>Θ</mi><mo stretchy="false">) </mo></mrow></munder><mspace width="0.25em" /><mtext>Κ</mtext><mtext>L</mtext><mo stretchy="false"> (</mo><mi>q</mi><mo stretchy="false"> (</mo><mi>Θ</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>Θ</mi><mo stretchy="false">|</mo><mi mathvariant="bold">D</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>+</mo><mi>c</mi><mi>Ω</mi><mo stretchy="false"> (</mo><mi>q</mi><mo stretchy="false"> (</mo><mi>Θ</mi><mo stretchy="false">) </mo></mrow></math></mathml>;<b>D</b>) .</p>
                </div>
                <div class="p1">
                    <p id="441">后验正则化项的引入大幅加强贝叶斯推理的灵活性, 通过合理选择正则化项, 可以实现基于最大间隔准则限制提升预测性能<citation id="687" type="reference"><link href="380" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、基于逻辑规则约束利用专家知识<citation id="688" type="reference"><link href="398" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、或是限制稀疏性<citation id="689" type="reference"><link href="400" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>等功能.具体到MedLDA, 模型分为两部分:LDA部分由后验分布描述, 用于建模文档的分布<i>p</i> (<b><i>W</i></b>) ;分类器部分使用正则化项实现, 用于根据文档预测<b><i>Y</i></b>.</p>
                </div>
                <h4 class="anchor-tag" id="442" name="442"><b>1.2</b> 潜在的狄利克雷分配</h4>
                <div class="p1">
                    <p id="443">LDA部分学习<i>K</i>个主题, 可以使用一个产生式过程描述如下:</p>
                </div>
                <div class="p1">
                    <p id="444">1) 对每个主题<i>k</i>=1, 2, …, <i>K</i>, 生成</p>
                </div>
                <div class="p1">
                    <p id="445">ϕ<sub><i>k</i></sub>～Dir (<i>β</i>, <i>V</i>) ;</p>
                </div>
                <div class="p1">
                    <p id="446">2) 对每篇文档<i>d</i>=1, 2, …, <i>D</i>, 生成</p>
                </div>
                <div class="p1">
                    <p id="447"><i>θ</i><sub><i>d</i></sub>～Dir (<i>α</i>, <i>K</i>) ;</p>
                </div>
                <div class="p1">
                    <p id="448">3) 对每篇文档<i>d</i>的每个位置<i>n</i>, 生成主题分配<i>z</i><sub><i>dn</i></sub>～Mult (<i>θ</i><sub><i>d</i></sub>) , 生成词<i>w</i><sub><i>dn</i></sub>～Mult (ϕ<sub><i>z</i><sub><i>dn</i></sub></sub>) .</p>
                </div>
                <div class="p1">
                    <p id="449">其中:Dir (<i>α</i>, <i>K</i>) 为<i>K</i>维的对称狄利克雷分布, 集中度参数为<i>α</i>; Mult (<i>θ</i>) 为离散分布.记<b><i>Z</i></b>={<b><i>z</i></b><sub><i>d</i></sub>}<mathml id="450"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup></mrow></math></mathml>, <b><i>z</i></b><sub><i>d</i></sub>={<i>z</i><sub><i>dn</i></sub>}<mathml id="451"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>d</mi></msub></mrow></msubsup></mrow></math></mathml>, <i>θ</i>={<i>θ</i><sub><i>d</i></sub>}<mathml id="452"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup></mrow></math></mathml>, ϕ={ϕ<sub><i>k</i></sub>}<mathml id="453"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></msubsup></mrow></math></mathml>, 该产生式过程定义一个联合分布<mathml id="454"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">Ζ</mi><mo>, </mo><mi mathvariant="bold-italic">θ</mi><mo>, </mo><mtext>ϕ</mtext><mo stretchy="false">|</mo><mi>α</mi><mo>, </mo><mi>β</mi><mo stretchy="false">) </mo></mrow></math></mathml>, 根据共轭性<citation id="690" type="reference"><link href="402" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>, 可以解析地积分掉 (<i>θ</i>, ϕ) , 得到<mathml id="455"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">|</mo><mi>α</mi><mo>, </mo><mi>β</mi><mo stretchy="false">) </mo><mo>.</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="456">为了符号的简洁, 在下文中均省略超参 (<i>α</i>, <i>β</i>) .LDA的训练需要求出隐变量<b><i>Z</i></b>的后验分布<mathml id="457"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">) </mo></mrow></math></mathml>, 这可以理解为如下变分问题:</p>
                </div>
                <div class="p1">
                    <p id="458"><mathml id="459"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>q</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">) </mo></mrow></munder><mspace width="0.25em" /><mtext>Κ</mtext><mtext>L</mtext><mo stretchy="false"> (</mo><mi>q</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></mathml>.      (1) </p>
                </div>
                <h4 class="anchor-tag" id="460" name="460"><b>1.3</b> 分类器</h4>
                <div class="p1">
                    <p id="461">为了分类, MedLDA学习一组分类器.每个分类器接受的输入是文档的主题分布</p>
                </div>
                <div class="p1">
                    <p id="462" class="code-formula"><mathml id="463"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub></mrow></math></mathml>:<mathml id="464"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mi>d</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>d</mi></msub></mrow></munderover><mi mathvariant="bold-italic">e</mi></mstyle><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow></msub></mrow></msub><mo>, </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="465">其中<b><i>e</i></b><sub><i>k</i></sub>为第<i>k</i>维为1、其它维为0的<i>K</i>维单位向量.分类器<i>η</i>对于第<i>l</i>个任务的输出为<mathml id="466"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">η</mi><msubsup><mrow></mrow><mi>l</mi><mtext>Τ</mtext></msubsup><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub></mrow></math></mathml>.MedLDA希望学习分类器的分布<i>q</i> (<i>η</i>) , 使期望分类器<mathml id="467"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>q</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">η</mi><mo stretchy="false">) </mo><mi>q</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">η</mi><msubsup><mrow></mrow><mi>l</mi><mtext>Τ</mtext></msubsup><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">]</mo></mrow></math></mathml>的间隔最大.分类器权重的先验分布为<i>p</i> (<i>η</i>) =<b>N</b> (<b>0</b>, <b><i>I</i></b>) .</p>
                </div>
                <div class="p1">
                    <p id="468">简写<i>q</i><sub><b><i>Z</i></b></sub>=<i>q</i> (<b><i>Z</i></b>) , <i>q</i><sub><i>η</i></sub>=<i>q</i> (<i>η</i>) , 结合LDA部分的损失式 (1) 和最大间隔分类器的损失, MedLDA定义的优化问题:</p>
                </div>
                <div class="area_img" id="469">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201908008_46900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="471">其中<i>ξ</i><sub><i>dl</i></sub>、<i>l</i>分别为最大间隔学习的松弛变量和间隔, </p>
                </div>
                <div class="p1">
                    <p id="472" class="code-formula">
                        <mathml id="472"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold">L</mi><mo stretchy="false"> (</mo><mi>q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">η</mi></msub><mo>, </mo><mi>q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ζ</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ζ</mi></msub></mrow></msub><mo stretchy="false">[</mo><mi>ln</mi><mspace width="0.25em" /><mi>q</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">) </mo><mo>-</mo><mi>ln</mi><mspace width="0.25em" /><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">η</mi></msub></mrow></msub><mo stretchy="false">[</mo><mi>ln</mi><mspace width="0.25em" /><mi>q</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">η</mi><mo stretchy="false">) </mo><mo>-</mo><mi>ln</mi><mspace width="0.25em" /><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">η</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="473">MedLDA已有的算法包括变分推理算法<citation id="691" type="reference"><link href="380" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>和蒙特卡洛算法<citation id="692" type="reference"><link href="394" rel="bibliography" /><link href="404" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">26</a>]</sup></citation>.两者相比, 蒙特卡洛算法因为近似更少, 得到的模型质量优于变分推理算法.但是两种算法的时间复杂度均正比于主题数量, 这限制MedLDA应用于大规模数据集.</p>
                </div>
                <h3 id="474" name="474" class="anchor-tag">2 最大熵判别LDA的坐标下降算法</h3>
                <div class="p1">
                    <p id="476">本节中提出MedLDA的坐标下降算法, 相比原有的蒙特卡洛算法<citation id="693" type="reference"><link href="394" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, 坐标下降算法无需在每次优化分类器时将其优化至收敛, 因此可大幅减少优化分类器的迭代次数.</p>
                </div>
                <h4 class="anchor-tag" id="477" name="477"><b>2.1</b> 简化的优化问题</h4>
                <div class="p1">
                    <p id="478">式 (2) 为一个凸优化问题, 使用拉格朗日乘子法求解.引入拉格朗日乘子<i>μ</i>、<b><i>r</i></b>, 则式 (2) 的拉格朗日函数为</p>
                </div>
                <div class="p1">
                    <p id="479" class="code-formula">
                        <mathml id="479"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false"> (</mo><mi>q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">η</mi></msub><mo>, </mo><mi>q</mi><msub><mrow></mrow><mi>Ζ</mi></msub><mo>, </mo><mi mathvariant="bold-italic">ξ</mi><mo>, </mo><mi mathvariant="bold-italic">μ</mi><mo>, </mo><mi mathvariant="bold-italic">r</mi><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mi mathvariant="bold">L</mi><mo stretchy="false"> (</mo><mi>q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ζ</mi></msub><mo>, </mo><mi>q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">η</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mn>2</mn><mi>c</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi>ξ</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi>r</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi>ξ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi>μ</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi>E</mi><msub><mrow></mrow><mrow><mi>q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">η</mi></msub></mrow></msub><mrow><mo stretchy="false">[</mo><mi mathvariant="bold-italic">η</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">]</mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mi>E</mi><msub><mrow></mrow><mrow><mi>q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ζ</mi></msub></mrow></msub><mo stretchy="false">[</mo><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">]</mo><mo>-</mo><mi>l</mi><mo>+</mo><mi>ξ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mo stretchy="false">) </mo><mo>.</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="480">式 (2) 的最优解应为</p>
                </div>
                <div class="area_img" id="481">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201908008_48100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="483">将上式对<i>q</i><sub><i>η</i></sub>求导, 可得最优解</p>
                </div>
                <div class="p1">
                    <p id="484"><i>q</i><sup>*</sup> (<i>η</i><sub><i>l</i></sub>) =<b>N</b> (<i>κ</i><sub><i>l</i></sub> (<i>μ</i><sub><i>l</i></sub>, <i>q</i><sub><i>Z</i></sub>) , <b><i>I</i></b>) ,      (5) </p>
                </div>
                <div class="p1">
                    <p id="485">其中</p>
                </div>
                <div class="p1">
                    <p id="486" class="code-formula">
                        <mathml id="486"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">κ</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mi>l</mi></msub><mo>, </mo><mi>q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ζ</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi>μ</mi></mstyle><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi>y</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi>E</mi><msub><mrow></mrow><mrow><mi>q</mi><msub><mrow></mrow><mrow><msub><mrow></mrow><mi mathvariant="bold-italic">Ζ</mi></msub></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">]</mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="487">将式 (3) 对<i>ξ</i>求导, 可得</p>
                </div>
                <div class="p1">
                    <p id="488"><i>L</i>′<sub><i>ξ</i><sub><sub><i>dl</i></sub></sub></sub>=2<i>c</i>-<i>μ</i><sub><i>dl</i></sub>-<i>r</i><sub><i>dl</i></sub>=0, </p>
                </div>
                <div class="p1">
                    <p id="489">结合<i>μ</i><sub><i>dl</i></sub>≥0, <i>r</i><sub><i>dl</i></sub>≥0, 可得</p>
                </div>
                <div class="p1">
                    <p id="490">0≤<i>μ</i><sub><i>dl</i></sub>≤2<i>c</i>.      (6) </p>
                </div>
                <div class="p1">
                    <p id="491">将式 (5) 、式 (6) 代入式 (4) , 可得简化后的优化问题:</p>
                </div>
                <div class="area_img" id="492">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201908008_49200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="494">其中</p>
                </div>
                <div class="area_img" id="495">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201908008_49500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <h4 class="anchor-tag" id="497" name="497"><b>2.2</b> 坐标下降算法</h4>
                <div class="p1">
                    <p id="498">式 (7) 对应的优化问题可以使用坐标下降法求解, 从初始解 (<i>q</i><sup> (0) </sup><sub><b><i>Z</i></b></sub>, <i>μ</i><sup> (0) </sup>) 开始, 迭代进行如下操作.</p>
                </div>
                <div class="p1">
                    <p id="499">1) 给定<i>q</i><sup> (<i>t</i>) </sup><sub><b><i>Z</i></b></sub>, <i>μ</i><sup> (<i>t</i>) </sup>, 优化<i>q</i><sup> (<i>t</i>+1) </sup><sub><b><i>Z</i></b></sub>.由式 (8) 求导可得最优的<i>q</i><sub><b><i>Z</i></b></sub>应满足</p>
                </div>
                <div class="p1">
                    <p id="500"><mathml id="501"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ζ</mi></msub><mo>∝</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">) </mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi mathvariant="bold-italic">κ</mi></mstyle><msub><mrow></mrow><mi>l</mi></msub><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">μ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mi>q</mi><msub><mrow></mrow><mi>Ζ</mi></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi>μ</mi></mstyle><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi>y</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>.      (9) </p>
                </div>
                <div class="p1">
                    <p id="502">这不是一个解析解, 因为式 (9) 的左右两边均有<i>q</i><sup> (<i>t</i>) </sup><sub><b><i>Z</i></b></sub>.为了可解性, 本文使用<i>κ</i><sub><i>l</i></sub> (<i>μ</i><sup> (<i>t</i>) </sup>, <i>q</i><sub><b><i>Z</i></b></sub><sup> (<i>t</i>) </sup>) 近似式 (9) 右边的<i>κ</i><sub><i>l</i></sub> (<i>μ</i><sup> (<i>t</i>) </sup>, <i>q</i><sub><b><i>Z</i></b></sub>) .</p>
                </div>
                <div class="p1">
                    <p id="503">2) 给定<i>q</i><sup> (<i>t</i>+1) </sup><sub><b><i>Z</i></b></sub>, 优化<i>μ</i><sup> (<i>t</i>+1) </sup>.由式 (7) 、式 (8) , 优化<i>μ</i>的问题可拆解成<i>L</i>个独立的子问题, 其中优化<i>μ</i><sub><i>l</i></sub>的子问题为</p>
                </div>
                <div class="p1">
                    <p id="504" class="code-formula">
                        <mathml id="504"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mi>l</mi></msub></mrow></munder><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">|</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi>μ</mi></mstyle><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi>y</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi>E</mi><msub><mrow></mrow><mrow><mi>q</mi><msubsup><mrow></mrow><mrow><msub><mrow></mrow><mi mathvariant="bold-italic">Ζ</mi></msub></mrow><mrow><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msup></mrow></msubsup></mrow></msub><mo stretchy="false">[</mo><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">]</mo><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi mathvariant="bold">l</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi>μ</mi></mstyle><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mo>, </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mn>0</mn><mo>≤</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mo>≤</mo><mn>2</mn><mi>c</mi><mo>, </mo><mo>∀</mo><mi>d</mi><mo>, </mo><mi>l</mi><mo>, </mo></mtd></mtr></mtable><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="505">其中期望<i>E</i><sub><i>q</i><sup> (<i>t</i>+1) </sup><sub><sub><b><i>Z</i></b></sub></sub></sub>[<b><i>z</i></b><sub><i>d</i></sub>]可以通过从<i>q</i><mathml id="506"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><msub><mrow></mrow><mi mathvariant="bold-italic">Ζ</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>中采样近似.式 (10) 的优化问题与支持向量机的对偶问题具有相同形式, 可以使用支持向量机的求解算法, 如随机对偶坐标上升法<citation id="694" type="reference"><link href="406" rel="bibliography" /><link href="408" rel="bibliography" /><sup>[<a class="sup">27</a>,<a class="sup">28</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="507">总之, 坐标下降算法训练流程对每轮迭代重复如下步骤:</p>
                </div>
                <div class="p1">
                    <p id="508">1) 训练<b>LDA</b>.按式 (9) 得到<b><i>q</i></b><mathml id="509"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><msub><mrow></mrow><mi mathvariant="bold-italic">Ζ</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, 并通过采样近似期望<mathml id="510"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">E</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">q</mi><msubsup><mrow></mrow><mrow><msub><mrow></mrow><mi mathvariant="bold-italic">Ζ</mi></msub></mrow><mrow><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msup></mrow></msubsup></mrow></msub><mo stretchy="false">[</mo><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">]</mo><mo>.</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="511">2) 训练分类器.给定<mathml id="512"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>q</mi><msubsup><mrow></mrow><mrow><msub><mrow></mrow><mi mathvariant="bold-italic">Ζ</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></msub><mo stretchy="false">[</mo><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">]</mo></mrow></math></mathml>, 按算法1优化<i>μ</i><sup> (<i>t</i>+1) </sup>.</p>
                </div>
                <div class="p1">
                    <p id="513">这些步骤与原有的蒙特卡洛算法<citation id="695" type="reference"><link href="394" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>相似.主要区别如下:在原有的蒙特卡洛算法中, 训练分类器的过程要重复迭代直至<i>μ</i>收敛.这实际上是浪费的, 因为<i>q</i><sub><sub><b><i>Z</i></b></sub></sub>在不断变化, 对某个固定的<i>q</i><sub><sub><b><i>Z</i></b></sub></sub>将<i>μ</i>优化至收敛并无意义.而坐标下降算法解决这个问题, 在坐标下降的框架中, 只要每轮优化<i>μ</i>的迭代对目标函数<i>L</i> (<i>q</i><sub><sub><b><i>Z</i></b></sub></sub>, <i>μ</i>) 有提升即可.故每次仅优化<i>μ</i>固定的轮数, 记这个轮数为<i>i</i><sub>svm</sub>.相比优化至收敛, 大幅降低迭代次数.坐标下降算法的具体实现如算法1所示.</p>
                </div>
                <div class="p1">
                    <p id="514"><b>算法1</b> MedLDA的坐标下降算法</p>
                </div>
                <div class="p1">
                    <p id="515">令<b><i>Z</i></b>←<b><i>Z</i></b><sup> (0) </sup>, <i>μ</i>←<i>μ</i><sup> (0) </sup>, </p>
                </div>
                <div class="p1">
                    <p id="516">对<i>l</i>∈{1, 2, …, <i>L</i>}, 计算<mathml id="517"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">κ</mi><msub><mrow></mrow><mi>l</mi></msub><mo>←</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>d</mi></munder><mi>μ</mi></mstyle><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi>y</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="518">for 每轮迭代 do</p>
                </div>
                <div class="p1">
                    <p id="519">采样<mathml id="520"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ζ</mi><mo>∼</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">) </mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi mathvariant="bold-italic">κ</mi></mstyle><msubsup><mrow></mrow><mi>l</mi><mtext>Τ</mtext></msubsup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi>μ</mi></mstyle><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi>y</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="521">for 每个任务<i>l</i>∈{1, 2, …, <i>L</i>} do</p>
                </div>
                <div class="p1">
                    <p id="522">计算<mathml id="523"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">κ</mi><msub><mrow></mrow><mi>l</mi></msub><mo>←</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>d</mi></munder><mi>μ</mi></mstyle><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi>y</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="524">for 每次迭代<i>i</i>∈{1, 2, …, <i>i</i><sub>svm</sub>} do</p>
                </div>
                <div class="p1">
                    <p id="525">for 每个文档<i>d</i>, 顺序随机 do</p>
                </div>
                <div class="p1">
                    <p id="526" class="code-formula">
                        <mathml id="526"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mo stretchy="false">) </mo><mo>←</mo><mi mathvariant="bold">l</mi><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi mathvariant="bold-italic">κ</mi><msubsup><mrow></mrow><mi>l</mi><mtext>Τ</mtext></msubsup><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>μ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mo>←</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mo>+</mo><mfrac><mrow><mi>g</mi><mo stretchy="false"> (</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mo stretchy="false">) </mo></mrow><mrow><mrow><mo stretchy="false">∥</mo><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi mathvariant="bold-italic">κ</mi><msub><mrow></mrow><mi>l</mi></msub><mo>←</mo><mi mathvariant="bold-italic">κ</mi><msub><mrow></mrow><mi>l</mi></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi>y</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>μ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mo>←</mo><mrow><mi>max</mi></mrow><mo stretchy="false">{</mo><mn>0</mn><mo>, </mo><mrow><mi>min</mi></mrow><mo stretchy="false">{</mo><mn>2</mn><mi>c</mi><mo>, </mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mo stretchy="false">}</mo><mo stretchy="false">}</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi mathvariant="bold-italic">κ</mi><msub><mrow></mrow><mi>l</mi></msub><mo>←</mo><mi mathvariant="bold-italic">κ</mi><msub><mrow></mrow><mi>l</mi></msub><mo>+</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi>y</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="527">end for</p>
                </div>
                <div class="p1">
                    <p id="528">end for</p>
                </div>
                <div class="p1">
                    <p id="529">end for</p>
                </div>
                <div class="p1">
                    <p id="530">end for</p>
                </div>
                <h3 id="531" name="531" class="anchor-tag">3 快速采样算法</h3>
                <div class="p1">
                    <p id="532">本节讨论如何快速采样式 (9) 中的<i>q</i> (<b><i>Z</i></b>) , 使采样每个词的时间复杂度从线性于主题数量降至亚线性.</p>
                </div>
                <h4 class="anchor-tag" id="533" name="533"><b>3.1</b> 吉布斯采样</h4>
                <div class="p1">
                    <p id="534">可以使用吉布斯采样从<i>q</i> (<b><i>Z</i></b>) 中采样, 由狄利克雷分布的共轭性<citation id="696" type="reference"><link href="402" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>, 得</p>
                </div>
                <div class="p1">
                    <p id="535" class="code-formula">
                        <mathml id="535"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">) </mo><mo>∝</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mrow><mfrac><mrow><mi>B</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>d</mi></msub><mo>+</mo><mi mathvariant="bold-italic">α</mi><mo stretchy="false">) </mo></mrow><mrow><mi>B</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">α</mi><mn>1</mn><mo stretchy="false">) </mo></mrow></mfrac></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mfrac><mrow><mi>B</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mi>β</mi><mo stretchy="false">) </mo></mrow><mrow><mi>B</mi><mo stretchy="false"> (</mo><mi>β</mi><mn>1</mn><mo stretchy="false">) </mo></mrow></mfrac></mrow></mstyle><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="536">其中</p>
                </div>
                <div class="p1">
                    <p id="537" class="code-formula">
                        <mathml id="537"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">α</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mtext>Γ</mtext></mstyle><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mtext>Γ</mtext><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="538">为多元贝塔函数, Γ (·) 为伽马函数, <mathml id="539"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>1</mn><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>, </mo><mn>1</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>1</mn><mo stretchy="false">) </mo><mo>, </mo><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>d</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>d</mi></msub></mrow></munderover><mi mathvariant="bold-italic">e</mi></mstyle><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow></msub></mrow></msub></mrow></math></mathml>为文档-主题计数向量, <b><i>e</i></b><sub><i>z</i><sub><i>dn</i></sub></sub>为第1节中定义的单位向量, </p>
                </div>
                <div class="p1">
                    <p id="540" class="code-formula">
                        <mathml id="540"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>d</mi></msub></mrow></munderover><mi mathvariant="bold-italic">Ι</mi></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>z</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow></msub><mo>=</mo><mi>k</mi><mo stretchy="false">) </mo><mi>e</mi><msub><mrow></mrow><mrow><mi>w</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow></msub></mrow></msub></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="541">为主题-单词计数向量, 其中<b><i>I</i></b> (<i>z</i><sub><i>dn</i></sub>=<i>k</i>) 为指示器函数, 在<i>z</i><sub><i>dn</i></sub>=<i>k</i>时为1, 其余时候为0.根据式 (9) 整理得</p>
                </div>
                <div class="p1">
                    <p id="542" class="code-formula">
                        <mathml id="542"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>q</mi><mo stretchy="false"> (</mo><mi>z</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow></msub><mo>=</mo><mi>k</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mrow><mo>⇁</mo><mi>d</mi><mi>n</mi></mrow></msub><mo stretchy="false">) </mo><mo>∝</mo><mi>q</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>n</mi><mi>k</mi></mrow></msub><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="543">其中</p>
                </div>
                <div class="p1">
                    <p id="544" class="code-formula">
                        <mathml id="544"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>q</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>n</mi><mi>k</mi></mrow></msub><mo>=</mo><mo stretchy="false"> (</mo><mi>C</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow><mrow><mo>⇁</mo><mi>d</mi><mi>n</mi></mrow></msubsup><mo>+</mo><mi>α</mi><mo stretchy="false">) </mo><mfrac><mrow><mi>C</mi><msubsup><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>w</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow></msub></mrow><mrow><mo>⇁</mo><mi>d</mi><mi>n</mi></mrow></msubsup><mo>+</mo><mi>β</mi></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>v</mi></munder><mi>C</mi></mstyle><msubsup><mrow></mrow><mrow><mi>k</mi><mi>v</mi></mrow><mrow><mo>⇁</mo><mi>d</mi><mi>n</mi></mrow></msubsup><mo>+</mo><mi>V</mi><mi>β</mi></mrow></mfrac><mi>λ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow></msub><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>λ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mi>Λ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow></msub></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mi>Λ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow></msub></mrow></mfrac><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>Λ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mi>d</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>μ</mi></mstyle><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi>y</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msub><mi>κ</mi><msub><mrow></mrow><mrow><mi>l</mi><mi>k</mi></mrow></msub><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>κ</mi><msub><mrow></mrow><mrow><mi>l</mi><mi>k</mi></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">κ</mi><msub><mrow></mrow><mi>l</mi></msub><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">μ</mi><msubsup><mrow></mrow><mi>l</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi>q</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Ζ</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo></mrow><msub><mrow></mrow><mi>k</mi></msub><mo>, </mo><mspace width="0.25em" /></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="545">⇁<b><i>dn</i></b>为从相应的集合或计数中去掉 (<b><i>d</i>, <i>n</i></b>) 位置的词.已有的蒙特卡洛算法直接根据式 (11) 采样, 时间复杂度正比于主题数.</p>
                </div>
                <div class="p1">
                    <p id="546">下面分析时间复杂度.按式 (5) 计算所有<i>κ</i><sub><b><i>lk</i></b></sub>需要<b><i>O</i> (<i>DK</i></b><sub><b><i>d</i></b></sub><b><i>L</i></b>) 的时间, 其中</p>
                </div>
                <div class="p1">
                    <p id="547" class="code-formula">
                        <mathml id="547"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">d</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi mathvariant="bold-italic">D</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi mathvariant="bold-italic">d</mi><mo>=</mo><mn>1</mn></mrow><mi mathvariant="bold-italic">L</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="548">表示计数向量<b><i>C</i></b><sub><i>d</i></sub>的平均非零元数量.直接计算<i>Λ</i><sub><i>dk</i></sub>需要<i>O</i> (<i>DKL</i>) 的时间, 直接按式 (11) 采样<i>z</i><sub><i>dn</i></sub>需要<mathml id="549"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mi>D</mi><mover accent="true"><mi>Ν</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mi>Κ</mi><mo stretchy="false">) </mo></mrow></math></mathml>的时间, 其中</p>
                </div>
                <div class="p1">
                    <p id="550" class="code-formula">
                        <mathml id="550"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Ν</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>D</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi>Ν</mi></mstyle><msub><mrow></mrow><mi>d</mi></msub></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="551">为平均文档长度.故总时间复杂度为</p>
                </div>
                <div class="p1">
                    <p id="552" class="code-formula">
                        <mathml id="552"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mi>D</mi><mi>Κ</mi><mo stretchy="false"> (</mo><mi>L</mi><mo>+</mo><mover accent="true"><mi>Ν</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="553">正比于主题数量.</p>
                </div>
                <h4 class="anchor-tag" id="554" name="554"><b>3.2</b> 拒绝采样算法</h4>
                <div class="p1">
                    <p id="555">本文设计拒绝采样算法, 使采样<i>z</i><sub><i>dn</i></sub>更高效.记</p>
                </div>
                <div class="p1">
                    <p id="556"><i>λ</i><sub><i>d</i></sub>= (<i>λ</i><sub><i>dk</i></sub>) <mathml id="557"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></msubsup></mrow></math></mathml>, <i>Λ</i><sub><i>d</i></sub>= (<i>Λ</i><sub><i>dk</i></sub>) <mathml id="558"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></msubsup></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="559">注意到<i>λ</i><sub><i>d</i></sub>实际上是对<i>Λ</i><sub><i>d</i></sub>进行softmax变换的结果.若<i>Λ</i><sub><i>d</i></sub>的每维较接近, 则<i>λ</i><sub><i>d</i></sub>的每维应较接近1/<i>K</i>;若<i>Λ</i><sub><i>d</i></sub>的不同维相差很大, 则<i>λ</i><sub><i>dk</i></sub>在<i>Λ</i><sub><i>dk</i></sub>最大的一维会接近1, 而其它维接近0.无论哪种情况, <i>λ</i><sub><i>d</i></sub>的大部分维度应当差不多.拒绝采样算法为<i>λ</i><sub><i>dk</i></sub>设定一个上界</p>
                </div>
                <div class="p1">
                    <p id="560" class="code-formula">
                        <mathml id="560"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>λ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow></msub><mo>≤</mo><mi>ε</mi><mo>+</mo><mover accent="true"><mi>λ</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow></msub><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="561">其中</p>
                </div>
                <div class="p1">
                    <p id="562" class="code-formula">
                        <mathml id="562"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>λ</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow></msub><mo>=</mo><mrow><mi>max</mi></mrow><mo stretchy="false">{</mo><mn>0</mn><mo>, </mo><mi>λ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow></msub><mo>-</mo><mi>ε</mi><mo stretchy="false">}</mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="563">基于上述观察, 在<i>ε</i>选取合适时, <mathml id="564"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>λ</mi><mo>^</mo></mover></math></mathml><sub><i>dk</i></sub>应当非常稀疏.记</p>
                </div>
                <div class="p1">
                    <p id="565" class="code-formula">
                        <mathml id="565"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>ϕ</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>k</mi><mi>v</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>C</mi><msubsup><mrow></mrow><mrow><mi>k</mi><mi>v</mi></mrow><mrow><mo>⇁</mo><mi>d</mi><mi>n</mi></mrow></msubsup><mo>+</mo><mi>β</mi></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>v</mi></munder><mi>C</mi></mstyle><msubsup><mrow></mrow><mrow><mi>k</mi><mi>v</mi></mrow><mrow><mo>⇁</mo><mi>d</mi><mi>n</mi></mrow></msubsup><mo>+</mo><mi>V</mi><mi>β</mi></mrow></mfrac><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="566">可得</p>
                </div>
                <div class="area_img" id="567">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201908008_56700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="569">式 (12) 可以看作3个分布<i>q</i><mathml id="570"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow><mn>1</mn></msubsup></mrow></math></mathml>, <i>q</i><mathml id="571"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow><mn>2</mn></msubsup></mrow></math></mathml>, <i>q</i><mathml id="572"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow><mn>3</mn></msubsup></mrow></math></mathml>的混合, 三者的混合比例分别正比于<mathml id="573"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>q</mi></mstyle><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi><mi>k</mi></mrow><mn>1</mn></msubsup><mo>, </mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>q</mi></mstyle><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi><mi>k</mi></mrow><mn>2</mn></msubsup><mo>, </mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>q</mi></mstyle><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi><mi>k</mi></mrow><mn>3</mn></msubsup></mrow></math></mathml>.因此可随机按照混合比例选择一个分布, 再从对应分布中采样.注意到, 分别由于<i>C</i><mathml id="574"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow><mrow><mo>⇁</mo><mi>d</mi><mi>n</mi></mrow></msubsup></mrow></math></mathml>和<mathml id="575"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>λ</mi><mo>^</mo></mover></math></mathml><sub><i>dk</i></sub>的稀疏性, <i>q</i><mathml id="576"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow><mn>1</mn></msubsup></mrow></math></mathml>和<i>q</i><mathml id="577"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow><mn>2</mn></msubsup></mrow></math></mathml>都是稀疏的.而<i>q</i><mathml id="578"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi><mi>k</mi></mrow><mn>3</mn></msubsup></mrow></math></mathml>∝<mathml id="579"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>ϕ</mi><mo>^</mo></mover></math></mathml><sub><i>k</i>, <i>w</i><sub><i>dn</i></sub></sub>只和<i>w</i><sub><i>dn</i></sub>有关, 和具体的文档<i>d</i>无关, 可以采用和F+LDA<citation id="697" type="reference"><link href="388" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>同样的技巧, 按单词序号<i>w</i><sub><i>dn</i></sub>的顺序, 而非文档序号<i>d</i>的顺序采样每个词, 通过Fenwick树<citation id="698" type="reference"><link href="388" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>在<i>O</i> (ln <i>K</i>) 的时间内实现从<i>q</i><mathml id="580"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow><mn>3</mn></msubsup></mrow></math></mathml>中的采样.最后, 需要以概率</p>
                </div>
                <div class="p1">
                    <p id="581" class="code-formula">
                        <mathml id="581"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mi>q</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>n</mi><mi>k</mi></mrow></msub></mrow><mrow><mi>C</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow><mrow><mo>⇁</mo><mi>d</mi><mi>n</mi></mrow></msubsup><mi>λ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow></msub><mover accent="true"><mi>ϕ</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>k</mi><mi>v</mi></mrow></msub><mo>+</mo><mi>α</mi><mover accent="true"><mi>λ</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow></msub><mover accent="true"><mi>ϕ</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>k</mi><mi>v</mi></mrow></msub><mo>+</mo><mi>α</mi><mi>ε</mi><mover accent="true"><mi>ϕ</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>w</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow></msub></mrow></msub></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="582">拒绝采到的样本.拒绝概率与参数<i>ε</i>的选择有关.若<i>ε</i>=0, 拒绝概率为0, 但是<mathml id="583"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>λ</mi><mo>^</mo></mover></math></mathml><sub><i>dk</i></sub>不是稀疏的, 因此采样的时间复杂度仍是<i>O</i> (<i>K</i>) .若<i>ε</i>很大, 则<mathml id="584"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>λ</mi><mo>^</mo></mover></math></mathml><sub><i>dk</i></sub>很稀疏, 但拒绝概率可能很高.拒绝采样算法的伪代码如算法2所示.</p>
                </div>
                <div class="p1">
                    <p id="585"><b>算法2</b> MedLDA的拒绝采样算法</p>
                </div>
                <div class="p1">
                    <p id="586">for 每个词<i>v</i>∈{1, 2, …, <i>v</i>} do</p>
                </div>
                <div class="p1">
                    <p id="587">为分布<i>q</i><mathml id="588"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow><mn>3</mn></msubsup></mrow></math></mathml>建Fenwick树</p>
                </div>
                <div class="p1">
                    <p id="589">for 每个满足<i>w</i><sub><i>dn</i></sub>=<i>v</i>的词项 do</p>
                </div>
                <div class="p1">
                    <p id="590">更新<i>C</i><sub><i>dz</i><sub><i>dn</i></sub></sub>、<i>C</i><sub><i>z</i><sub><i>dn</i></sub><i>v</i></sub>和Fenwick树</p>
                </div>
                <div class="p1">
                    <p id="591">通过枚举所有<i>C</i><sub><i>dk</i></sub>&gt;0的<i>k</i>, 计算分布<i>q</i><mathml id="592"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow><mn>1</mn></msubsup></mrow></math></mathml>和混合</p>
                </div>
                <div class="p1">
                    <p id="593">比例<mathml id="594"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>q</mi></mstyle><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi><mi>k</mi></mrow><mn>1</mn></msubsup></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="595">通过枚举所有<mathml id="596"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>λ</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>d</mi><mi>k</mi></mrow></msub><mo>&gt;</mo><mn>0</mn></mrow></math></mathml>的<i>k</i>, 计算分布<i>q</i><mathml id="597"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow><mn>2</mn></msubsup></mrow></math></mathml>和混合</p>
                </div>
                <div class="p1">
                    <p id="598">比例<mathml id="599"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>q</mi></mstyle><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi><mi>k</mi></mrow><mn>2</mn></msubsup></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="600">do</p>
                </div>
                <div class="p1">
                    <p id="601">采样<i>u</i>～Mult<mathml id="602"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>q</mi></mstyle><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi><mi>k</mi></mrow><mn>1</mn></msubsup><mo>, </mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>q</mi></mstyle><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi><mi>k</mi></mrow><mn>2</mn></msubsup><mo>, </mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>q</mi></mstyle><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi><mi>k</mi></mrow><mn>3</mn></msubsup><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="603">若<i>u</i>=3, 从Fenwick树中采样主题<i>k</i>, </p>
                </div>
                <div class="p1">
                    <p id="604">否则从<i>q</i><mathml id="605"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow><mi>u</mi></msubsup></mrow></math></mathml>中采样主题<i>k</i></p>
                </div>
                <div class="p1">
                    <p id="606">while 样本<i>k</i>被拒绝</p>
                </div>
                <div class="p1">
                    <p id="607"><i>z</i><sub><i>dn</i></sub>←<i>k</i>, 更新<i>C</i><sub><i>dk</i></sub>、<i>C</i><sub><i>kv</i></sub>和Fenwick树</p>
                </div>
                <div class="p1">
                    <p id="608">end for</p>
                </div>
                <div class="p1">
                    <p id="609">end for</p>
                </div>
                <div class="p1">
                    <p id="610">该算法时间复杂度分析如下.计算<mathml id="611"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>q</mi></mstyle><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi><mi>k</mi></mrow><mn>1</mn></msubsup></mrow></math></mathml>及从<i>q</i><mathml id="612"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow><mn>1</mn></msubsup></mrow></math></mathml>中采样的时间复杂度都是每个词<i>O</i> (<i>K</i><sub><i>d</i></sub>) 的.计算<mathml id="613"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>q</mi></mstyle><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi><mi>k</mi></mrow><mn>2</mn></msubsup></mrow></math></mathml>及从<i>q</i><mathml id="614"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow><mn>2</mn></msubsup></mrow></math></mathml>中采样的时间复杂度是每个词<i>O</i> (<i>K</i><sub><i>λ</i></sub>) 的, 其中</p>
                </div>
                <div class="p1">
                    <p id="615" class="code-formula">
                        <mathml id="615"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><msub><mrow></mrow><mi>λ</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>D</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mo stretchy="false">∥</mo></mstyle><mover accent="true"><mi mathvariant="bold-italic">λ</mi><mo>^</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="616">反映<mathml id="617"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">λ</mi><mo>^</mo></mover></math></mathml><sub><i>d</i></sub>的稀疏性.维护Fenwick树及从<i>q</i><mathml id="618"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>n</mi></mrow><mn>3</mn></msubsup></mrow></math></mathml>中的采样的时间复杂度为<i>O</i> (ln <i>K</i>) .因此, 从<i>q</i> (<b><i>Z</i></b>) 采样的时间复杂度从<mathml id="619"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mi>D</mi><mover accent="true"><mi>Ν</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mi>Κ</mi><mo stretchy="false">) </mo></mrow></math></mathml>降至</p>
                </div>
                <div class="p1">
                    <p id="620" class="code-formula">
                        <mathml id="620"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mi>D</mi><mover accent="true"><mi>Ν</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>Κ</mi><msub><mrow></mrow><mi>d</mi></msub><mo>+</mo><mi>Κ</mi><msub><mrow></mrow><mi>λ</mi></msub><mo>+</mo><mrow><mi>ln</mi></mrow><mspace width="0.25em" /><mi>Κ</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="621">亚线性于主题数量.</p>
                </div>
                <h4 class="anchor-tag" id="622" name="622"><b>3.3</b> 预处理算法</h4>
                <div class="p1">
                    <p id="623">如第3.1节中所述, 直接计算所有<i>Λ</i><sub><i>dk</i></sub>的时间复杂度是<i>O</i> (<i>DKL</i>) .但是, 注意到由于支持向量机的性质, <i>μ</i><sub><i>dl</i></sub>是稀疏的, 故<i>μ</i><sub><i>dl</i></sub>=0的项无需考虑, 时间复杂度可降为<i>O</i> (<i>DKL</i><sub>SV</sub>) , 其中</p>
                </div>
                <div class="p1">
                    <p id="624" class="code-formula">
                        <mathml id="624"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>V</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>D</mi></mfrac><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">μ</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="625">表示平均每个文档有的支持向量数量.</p>
                </div>
                <div class="p1">
                    <p id="626">表1整理蒙特卡洛算法<citation id="699" type="reference"><link href="394" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>和本文的高效算法的时间复杂度.</p>
                </div>
                <div class="area_img" id="627">
                    <p class="img_tit"><b>表1 2种算法的时间复杂度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Time complexity comparison of 2 algorithms</p>
                    <p class="img_note"></p>
                    <table id="627" border="1"><tr><td><br />步骤</td><td>操作</td><td>蒙特卡洛算法</td><td>高效算法</td></tr><tr><td><br />采样</td><td>优化<i>μ</i></td><td><i>O</i> (<i>I</i><sub>svm</sub><i>DK</i><sub><i>d</i></sub><i>L</i>) </td><td><i>O</i> (<i>i</i><sub>svm</sub><i>DK</i><sub><i>d</i></sub><i>L</i>) </td></tr><tr><td><br />分类器</td><td>计算<i>κ</i><br /><br /><br />采样<b><i>Z</i></b></td><td><i>O</i> (<i>DK</i><sub><i>d</i></sub><i>L</i>) <br /><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mi>D</mi><mover accent="true"><mi>Ν</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mi>Κ</mi><mo stretchy="false">) </mo></mrow></math></td><td><i>O</i> (<i>DK</i><sub><i>d</i></sub><i>L</i>) <br /><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mi>D</mi><mover accent="true"><mi>Ν</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>Κ</mi><msub><mrow></mrow><mi>d</mi></msub><mo>+</mo><mi>Κ</mi><msub><mrow></mrow><mi>λ</mi></msub><mo>+</mo><mrow><mi>ln</mi></mrow><mspace width="0.25em" /><mi>Κ</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></td></tr><tr><td><br />预处理</td><td>预处理<i>Λ</i></td><td><i>O</i> (<i>DKL</i>) </td><td><i>O</i> (<i>DKL</i><sub>SV</sub>) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="628">表1中蒙特卡洛算法支持向量机的迭代次数记为<i>I</i><sub>svm</sub>.相比蒙特卡洛算法, 本文的高效算法将优化<i>μ</i>的迭代次数从<i>I</i><sub>svm</sub>降至<i>i</i><sub>svm</sub>, 并将采样的时间复杂度从线性降至亚线性于主题数量, 将预处理的时间复杂度降至亚线性于标签数量.</p>
                </div>
                <h3 id="629" name="629" class="anchor-tag">4 实验及结果分析</h3>
                <div class="p1">
                    <p id="630">在20NewsGroups (http://qwone.com/～jason/20Newsgroups/20news-bydate-matlab.tgz) 、LSHTC2-Wiki-large (简写为Wiki) 、RCV1<citation id="700" type="reference"><link href="410" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>数据集上进行实验.除了20NewsGroups是多分类问题以外, 另外两个数据集都是多任务问题, 数据集的统计信息见表2.20NewsGroups数据集使用原始的训练/测试划分;Wiki、RCV1数据集随机抽取5 000篇文档作为测试集.由于Wiki、RCV1数据集较大, 这里从2个数据集中各随机选取20 000篇训练文档和2 000篇测试文档的子集以方便调参, 记为Wiki子集和RCV1子集.</p>
                </div>
                <div class="area_img" id="632">
                    <p class="img_tit"><b>表2 实验数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Experimental datasets</p>
                    <p class="img_note"></p>
                    <table id="632" border="1"><tr><td><br />名称</td><td>文档<br />数量<i>D</i></td><td>词汇表<br />大小<i>V</i></td><td>平均文档<br />长度<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Ν</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub></mrow></math></td><td>分类<br />任务数<i>L</i></td></tr><tr><td><br />20NewsGroups</td><td>18774</td><td>53485</td><td>115</td><td>20</td></tr><tr><td><br />Wiki子集</td><td>22000</td><td>80442</td><td>61</td><td>20</td></tr><tr><td><br />RCV1子集</td><td>22000</td><td>47168</td><td>129</td><td>103</td></tr><tr><td><br />Wiki</td><td>1105000</td><td>915420</td><td>61</td><td>20</td></tr><tr><td><br />RCV1</td><td>804414</td><td>285483</td><td>122</td><td>103</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="633">在模型评价方面:多分类问题 (20NewsGroups) 使用测试准确率作为判断算法收敛情况的标准;多任务问题使用测试集上宏平均的F1 (macro-averaged F1) 作为判断算法收敛情况的标准.宏平均的F1将每个二分类任务分别计算的F1值取平均得到.为了表述简洁, 将测试准确率和测试集上宏平均的F1统称为“测试结果”.</p>
                </div>
                <div class="p1">
                    <p id="634">在超参方面, 狄利克雷先验<i>α</i>=10/<i>K</i>, <i>β</i>=0.01为手工指定, 而超参<i>K</i>, <i>c</i>, <b>l</b>通过网格搜索得到, 选取的值如表3所示.在MedLDA的蒙特卡洛算法<citation id="701" type="reference"><link href="394" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>中训练支持向量机直到对偶间隙小于0.1, 而本文的高效算法固定SVM训练轮数<i>i</i><sub>SVM</sub>=3, 阈值<i>ε</i>=2/<i>K</i>.</p>
                </div>
                <div class="area_img" id="635">
                    <p class="img_tit"><b>表3 各数据集选择的超参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table3 Selected hyperparameters for different datasets</p>
                    <p class="img_note"></p>
                    <table id="635" border="1"><tr><td><br />数据集</td><td>主题数量<i>K</i></td><td>分类器权重<i>c</i></td><td>间隔大小<i>l</i></td></tr><tr><td><br />20NewsGroups</td><td>200</td><td>3</td><td>10</td></tr><tr><td><br />Wiki</td><td>400</td><td>2.56</td><td>128</td></tr><tr><td><br />RCV1</td><td>800</td><td>1</td><td>0.1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="636" name="636"><b>4.1</b> 收敛速度及训练时间</h4>
                <div class="p1">
                    <p id="637">为了探究本文算法效率, 对比如下4种算法的收敛速度及训练算法中各部分耗时:</p>
                </div>
                <div class="p1">
                    <p id="638">1) 蒙特卡洛算法<citation id="702" type="reference"><link href="394" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="639">2) 拒绝采样算法.将蒙特卡洛采样算法中的采样部分替换成3.2节中的拒绝采样算法.</p>
                </div>
                <div class="p1">
                    <p id="640">3) 坐标下降+拒绝采样.在上一种算法的基础上使用2.2节中的坐标下降算法, 并令<i>i</i><sub>SVM</sub>=3.</p>
                </div>
                <div class="p1">
                    <p id="641">4) 高效算法.本文的完整算法, 在上一种算法的基础上使用3.3节的预处理算法.</p>
                </div>
                <div class="p1">
                    <p id="642">每种算法迭代20轮.图1反映测试结果随时间的变化情况.可以看到, 相比蒙特卡洛算法, 本文的高效算法在相同时间内得到更好的测试结果.</p>
                </div>
                <div class="area_img" id="643">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201908008_64300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 4种算法在3个数据集上的收敛速度对比" src="Detail/GetImg?filename=images/MSSB201908008_64300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 4种算法在3个数据集上的收敛速度对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201908008_64300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Convergence rate comparison of 4 algorithms on 3 datasets</p>

                </div>
                <div class="p1">
                    <p id="644">为了深入研究采用高效算法后不同部分的用时变化, 分别测量表1中总结的采样、分类器、预处理部分的运行时间, 结果如图2所示.对于类别数量较少、文档长度较长的20NewsGroups数据集, 时间占比最大的是采样部分.而对于文档较短的Wiki数据集和类别数量较多的RCV1数据集, 时间占比较大的是分类器部分.</p>
                </div>
                <div class="p1">
                    <p id="645">对比各算法运行时间, 可以发现采样部分的用时在使用拒绝采样算法后显著下降, 而分类器部分的用时在使用坐标下降算法后显著下降, 这与表1中对时间复杂度的分析一致.</p>
                </div>
                <div class="p1">
                    <p id="646">总之, 蒙特卡洛算法比本文的高效算法在20NewsGroups、Wiki和RCV1数据集上每轮迭代的用时分别延至2.4、 4和4.17倍.</p>
                </div>
                <div class="area_img" id="647">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201908008_64700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 4种算法在3个数据集上的训练时间对比" src="Detail/GetImg?filename=images/MSSB201908008_64700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 4种算法在3个数据集上的训练时间对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201908008_64700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Training time comparison of 4 algorithms on 3 datasets</p>

                </div>
                <h4 class="anchor-tag" id="648" name="648"><b>4.2</b> 分类器训练轮数的影响</h4>
                <div class="p1">
                    <p id="649">本节验证坐标下降算法分类器训练轮数<i>i</i><sub>svm</sub>对算法收敛速度的影响.分别设<i>i</i><sub>svm</sub>=1、2、3、4, 以及采用蒙特卡洛算法的策略, 迭代直到对偶间隔小于0.1时停止 (记作max) , 得到测试结果如图3所示.可以看到, <i>i</i><sub>svm</sub>≥2时算法均能收敛到较优结果, 这证明蒙特卡洛算法每次将<i>μ</i>优化至收敛是没有必要的, 可以选择较小的<i>i</i><sub>svm</sub>以提高算法效率.值得注意的是, “max”的测试结果在RCV1子集数据集上不如<i>i</i><sub>svm</sub>=3的好, 这可能是因为过度优化分类器导致过拟合到训练数据造成的.基于这些实验结果, 本文推荐选择整体效果最优的<i>i</i><sub>svm</sub>=3.</p>
                </div>
                <div class="area_img" id="650">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201908008_65000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同iSVM对训练时间的影响" src="Detail/GetImg?filename=images/MSSB201908008_65000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 不同<i>i</i><sub>SVM</sub>对训练时间的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201908008_65000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Effect of different <i>i</i><sub>SVM</sub> on training time</p>

                </div>
                <h4 class="anchor-tag" id="651" name="651"><b>4.3</b> 超参数<i>ε</i>的选择</h4>
                <div class="p1">
                    <p id="652">在拒绝采样算法中, 超参<i>ε</i>表示<mathml id="653"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">λ</mi><mo>^</mo></mover></math></mathml><sub><i>d</i></sub>的稀疏率 (即非零元所占比例) 与采样算法的拒绝率的一个取舍, 若<i>ε</i>=0, 拒绝率为0, 但稀疏率为1, 即<mathml id="654"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">λ</mi><mo>^</mo></mover></math></mathml><sub><i>d</i></sub>是稠密的, 此时算法无法利用稀疏性加速, 时间复杂度仍然与主题数成正比.随着<i>ε</i>的增大, 稀疏率下降, 拒绝率增大.为了提高算法效率, 需要选取适当的<i>ε</i>.</p>
                </div>
                <div class="p1">
                    <p id="655">图4和图5为<i>ε</i>不同时算法的稀疏率、拒绝率及采样时间.实验表明在<i>ε</i>∈[1/<i>K</i>, 2/<i>K</i>]时, 稀疏率和拒绝率都较低, 此时采样时间达到最短.如3.2节中所述, 这是因为大多数<i>Λ</i><sub><i>dk</i></sub>都很接近, 故<i>λ</i><sub><i>dk</i></sub>≈1/<i>K</i>, 在<i>ε</i>刚好大于1/<i>K</i>时能较好处理这种情况, 故此时稀疏率急剧下降, 而拒绝率变化很小.因此本文推荐使用<i>ε</i>=2/<i>K</i>.</p>
                </div>
                <div class="area_img" id="656">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201908008_65600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 ε对拒绝率和稀疏率的影响" src="Detail/GetImg?filename=images/MSSB201908008_65600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 ε对拒绝率和稀疏率的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201908008_65600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Effect of ε on rejection rate and sparsity rate</p>

                </div>
                <div class="area_img" id="657">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201908008_65700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 ε对采样时间的影响" src="Detail/GetImg?filename=images/MSSB201908008_65700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 ε对采样时间的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201908008_65700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Effect of ε on sampling time</p>

                </div>
                <h4 class="anchor-tag" id="658" name="658"><b>4.4</b> 训练时间随主题数量的变化</h4>
                <div class="p1">
                    <p id="659">图6为训练时间随主题数量<i>K</i>的变化情况.与表1中对时间复杂度的分析一致, 蒙特卡洛算法的训练时间基本随主题数量线性增长, 因此无法适用于主题数多的情况.而本文的高效算法时间复杂度随主题数量亚线性变化, 耗时随主题数增长很慢.</p>
                </div>
                <div class="area_img" id="660">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201908008_66000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 2种算法的训练时间随主题数量的变化情况" src="Detail/GetImg?filename=images/MSSB201908008_66000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 2种算法的训练时间随主题数量的变化情况  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201908008_66000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Training time of 2 algorithms varying with the number of topics</p>

                </div>
                <h4 class="anchor-tag" id="661" name="661"><b>4.5</b> 影响时间复杂度的值</h4>
                <div class="p1">
                    <p id="662">表1中使用值如下:文档-主题向量的非零元数量<i>K</i><sub><i>d</i></sub>, <i>λ</i><sub><i>dk</i></sub>的非零元数量<i>K</i><sub><i>λ</i></sub>, 每篇文档平均的支持向量数量<i>L</i><sub>SV</sub>, 类别数<i>L</i>, 蒙特卡洛算法训练支持向量机的迭代轮数<i>I</i><sub>svm</sub>.表4为在3个数据集上的典型值.由表可见, 稀疏性明显降低时间复杂度, 即<i>K</i><sub><i>d</i></sub>≪<i>K</i>, <i>K</i><sub><i>λ</i></sub>≪<i>K</i>, <i>L</i><sub>SV</sub>≪<i>L</i>.这些结果表明本文算法的计算量确实远小于蒙特卡洛算法.</p>
                </div>
                <div class="area_img" id="663">
                    <p class="img_tit"><b>表4 影响时间复杂度的常数取值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Values of constants affecting time complexity</p>
                    <p class="img_note"></p>
                    <table id="663" border="1"><tr><td>数据集</td><td><i>K</i><sub><i>d</i></sub></td><td><i>K</i><sub><i>λ</i></sub></td><td><i>K</i></td><td><i>L</i><sub><i>SV</i></sub></td><td><i>L</i></td><td><i>I</i><sub>svm</sub></td></tr><tr><td><br />20NewsGroups</td><td>11</td><td>2</td><td>200</td><td>1</td><td>20</td><td>18</td></tr><tr><td><br />Wiki</td><td>5</td><td>2</td><td>400</td><td>2</td><td>20</td><td>53</td></tr><tr><td><br />RCV1</td><td>18</td><td>25</td><td>800</td><td>3</td><td>103</td><td>26</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="664" name="664" class="anchor-tag">5 结束语</h3>
                <div class="p1">
                    <p id="665">本文针对有监督主题模型训练时间复杂度较高、正比于主题数量的问题, 提出MedLDA的高效训练算法.算法基于坐标下降的框架, 降低训练分类器的迭代轮数, 并通过拒绝采样和高效的预处理将时间复杂度从线性于主题数量降至亚线性.今后进一步研究方向包括:1) 进一步优化支持向量机训练的时间复杂度, 降至亚线性于任务数量<i>L</i>;2) 每篇文档分别自适应地调整阈值;3) 基于正则化贝叶斯推理框架, 推广至更多模型.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="705" type="formula" href="images/MSSB201908008_70500.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">陈键飞</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="706" type="formula" href="images/MSSB201908008_70600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">朱军</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="354">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                 BLEI D M, NG A Y, JORDAN M I.Latent Dirichlet Allocation.Journal of Machine Learning Research, 2003, 3:993-1022.
                            </a>
                        </p>
                        <p id="356">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Correlated Topic Models[C/OL]">

                                <b>[2]</b> BLEI D M, LAFFERTY J D.Correlated Topic Models[C/OL].[2019-04-21].http://people.ee.duke.edu/～lcarin/Blei2005CTM.pdf.
                            </a>
                        </p>
                        <p id="358">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dynamic topic models">

                                <b>[3]</b> BLEI D M, LAFFERTY J D.Dynamic Topic Models // Proc of the 23rd International Conference on Machine Learning.Berlin, Germany:Springer, 2006.DOI:10.1145/1143844.1143859.
                            </a>
                        </p>
                        <p id="360">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalable Training of Hierarchical Topic Models">

                                <b>[4]</b> CHEN J F, ZHU J, LU J, <i>et al</i>.Scalable Training of Hierarchical Topic Models.Proceedings of the VLDB Endowment, 2018, 11 (7) :826-839.
                            </a>
                        </p>
                        <p id="362">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical Topic Models and the Nested Chinese Restaurant Process[C/OL]">

                                <b>[5]</b> BLEI D M, GRIFFITHS T L, JORDAN M I, <i>et al</i>.Hierarchical Topic Models and the Nested Chinese Restaurant Process[C/OL].[2019-04-21].https://people.eecs.berkeley.edu/～jordan/papers/lda-crp.pdf.
                            </a>
                        </p>
                        <p id="364">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural variational inference for text processing">

                                <b>[6]</b> MIAO Y S, YU L, BLUNSOM P.Neural Variational Inference for Text Processing // Proc of the 23rd International Conference on Machine Learning.Berlin, Germany:Springer, 2016:1727-1736.
                            </a>
                        </p>
                        <p id="366">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LDA-based document models for Ad-hoc retrieval">

                                <b>[7]</b> WEI X, CROFT W B.LDA-Based Document Models for AD-HOC Retrieval // Proc of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM, 2006:178-185.
                            </a>
                        </p>
                        <p id="368">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Collaborative topic modeling for recommending scientific arti-cles">

                                <b>[8]</b> WANG C, BLEI D M.Collaborative Topic Modeling for Recommending Scientific Articles // Proc of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2011:448-456.
                            </a>
                        </p>
                        <p id="370">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Topic Panorama:A full picture of relevant topics">

                                <b>[9]</b> LIU S X, WANG X T, LIU J F, <i>et al</i>.TopicPanorama:A Full Picture of Relevant Topics // Proc of the IEEE Conference on Visual Analytics Science and Technology.Washington, USA:IEEE, 2014:183-192.
                            </a>
                        </p>
                        <p id="372">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards Topic Modeling for Big Data[C/OL]">

                                <b>[10]</b> WANG Y, ZHAO X M, SUN Z L, <i>et al</i>.Towards Topic Modeling for Big Data[C/OL].[2019-04-21].https://arxiv.org/pdf/1405.4402v1.pdf.
                            </a>
                        </p>
                        <p id="374">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving Interpretability of Deep Neural Networks with Semantic Information">

                                <b>[11]</b> DONG Y P, SU H, ZHU J, <i>et al</i>.Improving Interpretability of Deep Neural Networks with Semantic Information // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:4306-4314.
                            </a>
                        </p>
                        <p id="376">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised topic models">

                                <b>[12]</b> MCAULIFFE J D, BLEI D M.Supervised Topic Models // PLATT J C, KOLLER D, SINGER Y, <i>et al</i>., eds.Advances in Neural Information Processing Systems 20.Cambridge, USA:The MIT Press, 2008:121-128.
                            </a>
                        </p>
                        <p id="378">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Disclda:Discriminative learning for dimensionality reduction and classification">

                                <b>[13]</b> LACOSTE-JULIEN S, SHA F, JORDAN M I.DiscLDA:Discriminative Learning for Dimensionality Reduction and Classification // KOLLER D, SCHUURMANS D, BENGIO Y, <i>et al</i>., eds.Advances in Neural Information Processing Systems 21.Cambridge, USA:The MIT Press, 2009:897-904.
                            </a>
                        </p>
                        <p id="380">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MedLDA: Maximum margin supervised topic models">

                                <b>[14]</b> ZHU J, AHMED A, XING E P.MedLDA:Maximum Margin Supervised Topic Models.Journal of Machine Learning Research, 2012, 13:2237-2278.
                            </a>
                        </p>
                        <p id="382">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gibbs Max-margin Topic Models with Data Augmentation">

                                <b>[15]</b> ZHU J, CHEN N, PERKINS H, <i>et al</i>.Gibbs Max-Margin Topic Models with Data Augmentation.Journal of Machine Learning Research, 2014, 15:1073-1110.
                            </a>
                        </p>
                        <p id="384">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient methods for topic model inference on streaming document collections">

                                <b>[16]</b> YAO L M, MIMNO D, MCCALLUM A.Efficient Methods for To-pic Model Inference on Streaming Document Collections // Proc of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2009:937-946.
                            </a>
                        </p>
                        <p id="386">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reducing the Sampling Complexity of Topic Models">

                                <b>[17]</b> LI A Q, AHMED A, RAVI S, <i>et al</i>.Reducing the Sampling Complexity of Topic Models // Proc of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2014:891-900.
                            </a>
                        </p>
                        <p id="388">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Scalable Asynchronous Distributed Algorithm for Topic Modeling">

                                <b>[18]</b> YU H F, HSIEH C J, YUN H, <i>et al</i>.A Scalable Asynchronous Distributed Algorithm for Topic Modeling // Proc of the 24th International Conference on World Wide Web.New York, USA:ACM, 2015:1340-1350.
                            </a>
                        </p>
                        <p id="390">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lightlda:Big Topic Models on Modest Computer Clusters">

                                <b>[19]</b> YUAN J H, GAO F, HO Q R, <i>et al</i>.LightLDA:Big Topic Models on Modest Computer Clusters // Proc of the 24th International Conference on World Wide Web.New York, USA:ACM, 2015:1351-1361.
                            </a>
                        </p>
                        <p id="392">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=WarpLDA:a cache efficient O(1)algorithm for latent dirichlet allocation">

                                <b>[20]</b> CHEN J F, LI K W, ZHU J, <i>et al</i>.WarpLDA:A Cache Efficient O (1) Algorithm for Latent Dirichlet Allocation.Proceedings of the VLDB Endowment, 2016, 9 (10) :744-755.
                            </a>
                        </p>
                        <p id="394">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Monte carlo methods for maximum margin supervised topic models">

                                <b>[21]</b> JIANG Q X, ZHU J, SUN M S, <i>et al</i>.Monte Carlo Methods for Maximum Margin Supervised Topic Models // BENGIO S, WALLACH H M, LAROCHELLE H, <i>et al</i>., eds.Advances in Neural Information Processing Systems 25.Cambridge, USA:The MIT Press, 2012:1592-1600.
                            </a>
                        </p>
                        <p id="396">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NASR201704019&amp;v=MDIwOTgvTkt5ellmTEc0SDliTXE0OUViWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVXI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> ZHU J, CHEN J F, HU W B, <i>et al</i>.Big Learning with Bayesian Methods.National Science Review, 2017, 4 (4) :627-651.
                            </a>
                        </p>
                        <p id="398">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust RegBayes:Selectively incorporating first-order logic domain knowledge into Bayesian models">

                                <b>[23]</b> MEI S K, ZHU J, ZHU X J.Robust RegBayes:Selectively Incorporating First-Order Logic Domain Knowledge into Bayesian Models // Proc of the 31st International Conference on Machine Learning.Berlin, Germany:Springer, 2014:253-261.
                            </a>
                        </p>
                        <p id="400">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Constrained Bayesian inference for low rank multitask learning">

                                <b>[24]</b> KOYEJO O, GHOSH J.Constrained Bayesian Inference for Low Rank Multitask Learning // Proc of the 29th Conference on Uncertainty in Artificial Intelligence.Bellevue, USA:AUAI Press, 2013:341-350.
                            </a>
                        </p>
                        <p id="402">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Finding scientific topics">

                                <b>[25]</b> GRIFFITHS T L, STEYVERS M.Finding Scientific Topics.Proceedings of the National Academy of Sciences of the United States of America, 2004, 101 (s1) :5228-5235.
                            </a>
                        </p>
                        <p id="404">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Linear time samplers for supervised topic models using compositional proposals">

                                <b>[26]</b> ZHENG X, YU Y L, XING E P.Linear Time Samplers for Supervised Topic Models Using Compositional Proposals // Proc of the 21st ACM SIGKDD International Conference on Knowledge Disco-very and Data Mining.New York, USA:ACM, 2015:1523-1532.
                            </a>
                        </p>
                        <p id="406">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A dual coordinate descent method for large-scale linear SVM">

                                <b>[27]</b> HSIEH C J, CHANG K W, LIN C J, <i>et al</i>.A Dual Coordinate De-scent Method for Large-Scale Linear SVM // Proc of the International Conference on Machine Learning.Berlin, Germany:Sprin-ger, 2008:408-415.
                            </a>
                        </p>
                        <p id="408">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stochastic dual coordinate ascent methods for regularized loss">

                                <b>[28]</b> SHALEV-SHWARTZ S, ZHANG T.Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization.Journal of Machine Learning Research, 2013, 14:567-599.
                            </a>
                        </p>
                        <p id="410">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rcv1: A new benchmark collection for text categorization research">

                                <b>[29]</b> LEWIS D D, YANG Y M, ROSE T G, <i>et al</i>.RCV1:A New Benchmark Collection for Text Categorization Research.Journal of Machine Learning Research, 2004, 5:361-397.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201908008" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201908008&amp;v=MzA0MDVMT2VaZVJuRnkva1VyL05LRDdZYkxHNEg5ak1wNDlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
