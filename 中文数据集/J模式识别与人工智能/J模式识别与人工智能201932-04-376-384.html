<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131445900967500%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dMSSB201904013%26RESULT%3d1%26SIGN%3dc2zuA9VDroX2TeyiVdBMuBqVhGI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201904013&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201904013&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201904013&amp;v=MDM1MTR6cXFCdEdGckNVUkxPZVplUm5GeXpnVWIvSktEN1liTEc0SDlqTXE0OUVaNFFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#23" data-title="1 多视图半监督行为识别方法框架 ">1 多视图半监督行为识别方法框架</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#31" data-title="2 人体动作特征视图表示 ">2 人体动作特征视图表示</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#32" data-title="&lt;b&gt;2.1 GRB&lt;/b&gt;模态下的傅立叶描述子"><b>2.1 GRB</b>模态下的傅立叶描述子</a></li>
                                                <li><a href="#48" data-title="&lt;b&gt;2.2 Depth&lt;/b&gt;模态下的时空兴趣点"><b>2.2 Depth</b>模态下的时空兴趣点</a></li>
                                                <li><a href="#55" data-title="&lt;b&gt;2.3 Joints&lt;/b&gt;模态下的关节点投影分布特征"><b>2.3 Joints</b>模态下的关节点投影分布特征</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="3 多视图半监督学习模型 ">3 多视图半监督学习模型</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#126" data-title="4 实验及结果分析 ">4 实验及结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#153" data-title="5 结 束 语 ">5 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#30" data-title="图1 本文方法系统框架图">图1 本文方法系统框架图</a></li>
                                                <li><a href="#40" data-title="图2 基于曲率函数的傅立叶描述子">图2 基于曲率函数的傅立叶描述子</a></li>
                                                <li><a href="#57" data-title="图3 人体关节点在3个投影面的投影">图3 人体关节点在3个投影面的投影</a></li>
                                                <li><a href="#173" data-title="图4 KIAD中的行为">图4 KIAD中的行为</a></li>
                                                <li><a href="#174" data-title="图5 2种方法在KIAD数据集上的行为识别混淆矩阵">图5 2种方法在KIAD数据集上的行为识别混淆矩阵</a></li>
                                                <li><a href="#175" data-title="图6 2种方法在G3D数据集上的行为识别混淆矩阵">图6 2种方法在G3D数据集上的行为识别混淆矩阵</a></li>
                                                <li><a href="#141" data-title="图7 2种方法在CAD60数据集上的行为识别混淆矩阵">图7 2种方法在CAD60数据集上的行为识别混淆矩阵</a></li>
                                                <li><a href="#143" data-title="&lt;b&gt;表1 2种方法在3个数据集上的实验结果&lt;/b&gt;"><b>表1 2种方法在3个数据集上的实验结果</b></a></li>
                                                <li><a href="#176" data-title="图8 3个数据集上无标记样本个数对实验结果的影响">图8 3个数据集上无标记样本个数对实验结果的影响</a></li>
                                                <li><a href="#176" data-title="图8 3个数据集上无标记样本个数对实验结果的影响">图8 3个数据集上无标记样本个数对实验结果的影响</a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;表2 2种方法在3个数据集上的正确率对比&lt;/b&gt;"><b>表2 2种方法在3个数据集上的正确率对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" L&#220; M Q, CHEN L, CHEN T M, &lt;i&gt;et al&lt;/i&gt;.Bi-view Semi-supervised Learning Based Semantic Human Activity Recognition Using Acce-lerometers.IEEE Transactions on Mobile Computing, 2018, 17 (9) :1991-2001." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bi-view Semi-supervised Learning Based Semantic Human Activity Recognition Using Acce-lerometers">
                                        <b>[1]</b>
                                         L&#220; M Q, CHEN L, CHEN T M, &lt;i&gt;et al&lt;/i&gt;.Bi-view Semi-supervised Learning Based Semantic Human Activity Recognition Using Acce-lerometers.IEEE Transactions on Mobile Computing, 2018, 17 (9) :1991-2001.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" YUAN H J.A Semi-supervised Human Action Recognition Algorithm Based on Skeleton Feature.Journal of Information Hiding and Multimedia Signal Processing, 2015, 6 (1) :175-182." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Semi-supervised Human Action Recognition Algorithm Based on Skeleton Feature">
                                        <b>[2]</b>
                                         YUAN H J.A Semi-supervised Human Action Recognition Algorithm Based on Skeleton Feature.Journal of Information Hiding and Multimedia Signal Processing, 2015, 6 (1) :175-182.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" LIU W F, LIU H L, TAO D P, &lt;i&gt;et al&lt;/i&gt;.Multiview Hessian Regula-rized Logistic Regression for Action Recognition.Signal Processing, 2015, 110:101-107." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700154695&amp;v=MTIyMDVIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRnNXYWhNPU5pZk9mYks4SDlETXFJOUZaZTRMQ25VOG9CTVQ2VDRQUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         LIU W F, LIU H L, TAO D P, &lt;i&gt;et al&lt;/i&gt;.Multiview Hessian Regula-rized Logistic Regression for Action Recognition.Signal Processing, 2015, 110:101-107.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" WANG S, MA Z G, YANG Y, &lt;i&gt;et al&lt;/i&gt;.Semi-supervised Multiple Feature Analysis for Action Recognition.IEEE Transactions on Multimedia, 2014, 16 (2) :289-298." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised Multiple Feature Analysis for Action Recognition">
                                        <b>[4]</b>
                                         WANG S, MA Z G, YANG Y, &lt;i&gt;et al&lt;/i&gt;.Semi-supervised Multiple Feature Analysis for Action Recognition.IEEE Transactions on Multimedia, 2014, 16 (2) :289-298.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" ZHANG T Z, LIU S, XU C S, &lt;i&gt;et al&lt;/i&gt;.Boosted Multi-class Semi-supervised Learning for Human Action Recognition.Pattern Recogni-tion, 2011, 44 (10/11) :2334-2342." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738134&amp;v=MDcyMjdIRFg4OW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGc1dhaE09TmlmT2ZiSzdIdEROcVk5RlkrZw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         ZHANG T Z, LIU S, XU C S, &lt;i&gt;et al&lt;/i&gt;.Boosted Multi-class Semi-supervised Learning for Human Action Recognition.Pattern Recogni-tion, 2011, 44 (10/11) :2334-2342.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" LAPTEV I, LINDEBERG T.Space-Time Interest Points // Proc of the 9th IEEE International Conference on Computer Vision.Wa-shington, USA:IEEE, 2003:432-439." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Space-time interest points">
                                        <b>[6]</b>
                                         LAPTEV I, LINDEBERG T.Space-Time Interest Points // Proc of the 9th IEEE International Conference on Computer Vision.Wa-shington, USA:IEEE, 2003:432-439.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" >
                                        <b>[7]</b>
                                     HARRIS C, STEPHENS M.A Combined Corner and Edge Detector // Proc of the 4th Alvey Vision Conference.New York, USA:ACM, 1988, 15:147-151.</a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" DOLLAR P, RABAUD V, COTTRELL G, &lt;i&gt;et al&lt;/i&gt;.Behavior Recognition via Sparse Spatio-Temporal Features // Proc of the 2nd Joint IEEE International Workshop on Visual Surveillance and Perfor-mance Evaluation of Tracking and Surveillance.Washington, USA:IEEE, 2005:65-72." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Behavior recognition via sparse spatio-temporal features">
                                        <b>[8]</b>
                                         DOLLAR P, RABAUD V, COTTRELL G, &lt;i&gt;et al&lt;/i&gt;.Behavior Recognition via Sparse Spatio-Temporal Features // Proc of the 2nd Joint IEEE International Workshop on Visual Surveillance and Perfor-mance Evaluation of Tracking and Surveillance.Washington, USA:IEEE, 2005:65-72.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(04),376-384 DOI:10.16451/j.cnki.issn1003-6059.201904011            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于多视图半监督学习的人体行为识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%94%90%E8%B6%85&amp;code=23340525&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">唐超</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%96%87%E5%89%91&amp;code=08402641&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王文剑</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%99%93%E5%B3%B0&amp;code=24406280&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王晓峰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E7%90%9B&amp;code=41750880&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张琛</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B9%E4%B9%90&amp;code=21749967&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邹乐</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%90%88%E8%82%A5%E5%AD%A6%E9%99%A2%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E7%B3%BB&amp;code=0130127&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">合肥学院计算机科学与技术系</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E8%A5%BF%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0176514&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山西大学计算机与信息技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>由于人的行为在本质上的复杂性, 单一行为特征视图缺乏全面分析人类行为的能力.文中提出基于多视图半监督学习的人体行为识别方法.首先, 提出3种不同模态视图数据, 用于表征人体动作, 即基于RGB模态数据的傅立叶描述子特征视图、基于深度模态数据的时空兴趣点特征视图和基于关节模态数据的关节点投影分布特征视图.然后, 使用多视图半监督学习框架建模, 充分利用不同视图提供的互补信息, 确保基于少量标记和大量未标记数据半监督学习取得更好的分类精度.最后, 利用分类器级融合技术并结合3种视图的预测能力, 同时有效解决未标记样本置信度评估问题.在公开的人体行为识别数据集上实验表明, 采用多个动作特征视图融合的特征表示方法的判别力优于单个动作特征视图, 取得有效的人体行为识别性能.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E4%BD%93%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人体行为识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E8%A7%86%E5%9B%BE%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多视图学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">半监督学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A8%E4%BD%9C%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">动作特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Kinect%E4%BC%A0%E6%84%9F%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Kinect传感器;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *唐超 (通讯作者) , 博士, 副教授, 主要研究方向为机器学习、计算机视觉.E-mail:tangchao77@sina.com.;
                                </span>
                                <span>
                                    王文剑, 博士, 教授, 主要研究方向为机器学习、计算智能.E-mail:wjwang@sxu.edu.cn.;
                                </span>
                                <span>
                                    王晓峰, 博士, 教授, 主要研究方向为图像处理、计算智能.E-mail:xfwang@iim.ac.cn.;
                                </span>
                                <span>
                                    张琛, 博士, 讲师, 主要研究方向为机器学习、计算智能.E-mail:zhangchen0304@163.com.;
                                </span>
                                <span>
                                    邹乐, 博士研究生, 副教授, 主要研究方向为图像处理、计算智能.E-mail:zoule1983@163.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-25</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.61673249, 61672204, 61806068);</span>
                                <span>安徽省自然科学基金面上项目 (No.1908085MF184);</span>
                                <span>安徽高校自然科学研究重点项目 (No.KJ2018A0556, KJ2018A0555);</span>
                                <span>安徽高校优秀拔尖人才培育基金项目 (No.gxfx2017099, gxyq ZD2017076);</span>
                                <span>合肥学院人才科研基金项目 (No.16-17RC19) ;合肥学院教学研究重点项目 (No.018hfjyxm09) 资助;</span>
                    </p>
            </div>
                    <h1><b>Human Action Recognition Based on Multi-view Semi-supervised Learning</b></h1>
                    <h2>
                    <span>TANG Chao</span>
                    <span>WANG Wenjian</span>
                    <span>WANG Xiaofeng</span>
                    <span>ZHANG Chen</span>
                    <span>ZOU Le</span>
            </h2>
                    <h2>
                    <span>Department of Computer Science and Technology, Hefei University</span>
                    <span>School of Computer and Information Science, Shanxi University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Since human action is complicated in nature, single action feature view lacks the ability of comprehensively profiling human action. A method for human action recognition based on multi-view semi-supervised learning is proposed in this paper. Firstly, a method based on three different modal views is proposed to represent human action, namely Fourier descriptor feature view based on RGB modal data, spatial and temporal interest point feature view based on depth modal data and joints projection distribution feature view based on joints modal data. Secondly, multi-view semi-supervised learning framework is utilized for modeling. The complementary information provided by different views is utilized to ensure better classification accuracy with a small amount of labeled data and a large amount of unlabeled data. The classifier-level fusion technology is employed to combine the predictive ability of three views, and the problem of confidence evaluation of unlabeled samples is effectively solved. The experimental results on the open human behavior recognition dataset show that the discriminant power of the feature representation method based on the fusion of multiple action feature views is better than that of single action feature view, and the effective human action recognition performance is achieved.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Human%20Action%20Recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Human Action Recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multi-view%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multi-view Learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Semi-supervised%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Semi-supervised Learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Action%20Feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Action Feature;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Kinect%20Sensor&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Kinect Sensor;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    TANG Chao ( Corresponding author ) , Ph.D., associate professor. His research interests include machine learning and computer vision.;
                                </span>
                                <span>
                                    WANG Wenjian, Ph. D., professor. Her research interests include machine learning and computing intelligence.;
                                </span>
                                <span>
                                    WANG Xiaofeng, Ph. D., professor. His research interests include image processing and computing intelligence.;
                                </span>
                                <span>
                                    ZHANG Chen, Ph. D., lecturer. Her research interests include machine learning and computing intelligence.;
                                </span>
                                <span>
                                    ZOU Le, Ph. D. candidate, associate professor. His research interests include image processing and computing intelligence.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-25</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.61673249, 61672204, 61806068);</span>
                                <span>Natural Science Foundation of Anhui Province (No.1908085MF184);</span>
                                <span>Key Scientific Research Foundation of Education Department of Anhui Province (No.KJ2018A0556, KJ2018A0555);</span>
                                <span>Excellent Talents Training Funded Project of Universities of Anhui Province (No.gxfx2017099, gxyq ZD2017076);</span>
                                <span>Scientific Research Fund Project of Talents of Hefei University (No.16-17RC19) ;Key Teaching and Research Project of Hefei University (No.2018hfjyxm09);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="19">人体行为识别研究是自动检测和分析来自传感器 (如RGB相机、深度相机、可穿戴惯性传感器或其它模态传感器) 的各种人类行为.人体行为识别的研究在过去十年中取得重大进展, 并在各种学科中受到越来越多的关注.行为识别是普适计算领域中重要的研究内容, 具有广阔的应用前景, 在视频监控与监测、体育运动训练、电子娱乐和人机交互等方面具有广阔的应用前景.</p>
                </div>
                <div class="p1">
                    <p id="20">尽管基于视觉的人体行为识别技术取得不断进步, 但识别性能仍面临各种挑战.1) 由于存在杂乱的背景、相机运动和遮挡等情况, 很难从视频中完全捕捉动作的时空结构.2) 动作视频中有大量冗余数据.如果建模涉及到视频的所有数据, 冗余信息可能会降低识别算法的精度;如果只涉及特定的部分, 可能会丢弃一些具有代表性的信息.3) 现有的人体行为识别算法大多是基于一个单一的识别特征以构建模型.然而, 单一的人体行为特征很难较好地模拟复杂的行为.4) 为了训练一个良好的行为模型, 需要大量的标记数据, 以便有足够的训练样本以实现泛化能力.然而, 标记视频成本很高, 而未标记的视频可以较易从公共摄像机获取.在这种情况下, 如何利用大量的未标记数据提高整个系统的性能是一个至关重要的问题.半监督学习 (Semi-supervised Learning, SSL) 是解决这一问题的有效方法.</p>
                </div>
                <div class="p1">
                    <p id="21">目前, 将半监督学习应用于人体行为识别领域的研究还较少, 人体行为识别方法大多基于单一视图表征人体行为.由于行为本质上的复杂性, 单个视图缺乏对人体行为进行全面剖析的能力.Lü等<citation id="155" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出基于加速度计的双视图半监督学习框架, 进行人体行为识别.相比基于单视图的人体行为表示方法, 基于双视图的人体行为表示方法更有效, 能够获得具有竞争力的人体行为识别性能.Yuan等<citation id="156" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出基于骨架特征的半监督人体动作识别算法:首先计算累积骨架图像和骨架历史图像作为人体动作特征表示;然后, 通过约束半监督<i>K</i>均值聚类算法预测未标记动作的标签, 同时生成平均累积和历史骨架图像作为每个类别动作的模型;最后, 利用最近邻方法, 根据特征图像与预先建立的模板之间的相关系数对观测到的动作进行分类.人体行为往往具有多种模态表达形式和不同的表征形式, Liu等<citation id="157" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出基于多视图Hessian正则逻辑回归 (Multiview Hessian Regularized Logistic Regression, MHLR) 的人体行为识别方法, 结合多个Hessian正则化, 可以充分利用局部几何特征, 还可以处理具有多个表示的多视图实例.Wang等<citation id="158" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出基于多视觉特征的半监督人体行为识别方法, 同时从少量带标签的视频中学习多个特征, 并自动利用带标签和无标签数据之间的数据分布以提高识别性能.Zhang等<citation id="159" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出改进的多类半监督学习算法, 发掘非标记数据中的有用信息.</p>
                </div>
                <div class="p1">
                    <p id="22">本文提出基于多视图半监督学习算法 (Multi-view Semi-supervised Learning, MVSL) 的行为识别方法.为了最大限度地发挥方法效用, 只依赖于Kinect传感器提供的视图数据.提出3种不同的视图数据:骨架关节点视图数据、RGB彩色图像视图数据和深度图像视图数据.基于这3种视图的方法可以充分利用视图的互补表达能力以综合表征人体动作.提出基于半监督学习框架, 采用半监督学习算法解决标签样本不足的问题, 并利用分类器级融合技术结合3种视图的预测能力, 同时有效解决未标记样本置信度评估问题.在自建和公共的人体行为数据集上的实验表明本文算法取得的识别率高于监督学习算法.</p>
                </div>
                <h3 id="23" name="23" class="anchor-tag">1 多视图半监督行为识别方法框架</h3>
                <div class="p1">
                    <p id="24">在人体行为识别问题中, 所有的行为都可以作为未标记数据, 标记数据的获取要求用户标注每类行为所属的类别, 如果使用监督学习方法, 需要用户标记上千个行为作为样本, 才能使训练的学习器具有较好的泛化性能, 而几乎没有用户愿意花如此多的时间标记样本.在只有少量的用户标记样本和大量的未标记样本的情况下, 使用半监督学习方法训练行为识别模型可能是一个较好的选择.本文给出的系统框架包括2个模块:视图学习模块和半监督学习模块.如图1所示, 主要步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="25">1) 通过Kinect传感器, 捕获同一动作的RGB图像、深度图像和骨架图像.然后提取运动人体感兴趣区域, 为计算局部特征和全局特征做好准备.</p>
                </div>
                <div class="p1">
                    <p id="26">2) 在RGB图像上提取基于傅立叶描述子的全局特征视图, 在深度图像上提取基于时空兴趣点的局部特征视图, 同时在骨架图像中提取关节点投影分布特征视图.</p>
                </div>
                <div class="p1">
                    <p id="27">3) 将获得3种特征视图数据分割为两部分数据, 包括少量的有标记样本数据和大量未标记样本数据, 供半监督学习模块使用.</p>
                </div>
                <div class="p1">
                    <p id="28">4) 在每类特征视图上学习得到一个基分类器, 然后通过基于不一致性的多视图半监督学习方法进行协同训练.</p>
                </div>
                <div class="p1">
                    <p id="29">5) 通过分类器集成策略, 将训练得到的各个分类器结果进行集成决策输出.</p>
                </div>
                <div class="area_img" id="30">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904013_030.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文方法系统框架图" src="Detail/GetImg?filename=images/MSSB201904013_030.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文方法系统框架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904013_030.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Architecture of the proposed method</p>

                </div>
                <h3 id="31" name="31" class="anchor-tag">2 人体动作特征视图表示</h3>
                <h4 class="anchor-tag" id="32" name="32"><b>2.1 GRB</b>模态下的傅立叶描述子</h4>
                <div class="p1">
                    <p id="33">傅立叶描述子 (Fourier Descriptor, FD) 是基于曲线的形状描述符.FD的优势在于可以实现良好的形状表示和规范化, 具有紧凑性, 有效实现计算.同时FD具有位置、旋转和尺度不变性, 对某些局部噪声不敏感.FD可以有效计算感知形状特性, 低频对应于平均形状, 高频描述形状细节.</p>
                </div>
                <div class="p1">
                    <p id="34">针对RGB模态下的图像数据, 本文采用基于曲率函数 (Curvature Function) 的傅立叶描述子表征人体行为.</p>
                </div>
                <div class="p1">
                    <p id="35">RGB模态下傅立叶描述子的主要思想是通过固定数量的人体轮廓采样边界点 (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) , <i>i</i>=1, 2, …, <i>N</i>描述轮廓, 这些点采用等距离采样, 具体如图2所示.</p>
                </div>
                <div class="p1">
                    <p id="36">轮廓线上某一点的曲率定义为轮廓切向角度相对于弧长的变化率.曲率函数</p>
                </div>
                <div class="p1">
                    <p id="37" class="code-formula">
                        <mathml id="37"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mi>d</mi><mrow><mi>d</mi><mi>s</mi></mrow></mfrac><mi>θ</mi><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="38">式中, <i>θ</i> (<i>s</i>) 为轮廓线的切向角度, </p>
                </div>
                <div class="p1">
                    <p id="39" class="code-formula">
                        <mathml id="39"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>θ</mi><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>arctan</mi></mrow><mo stretchy="false"> (</mo><mfrac><mrow><msup><mi>y</mi><mo>′</mo></msup><msub><mrow></mrow><mi>s</mi></msub></mrow><mrow><msup><mi>x</mi><mo>′</mo></msup><msub><mrow></mrow><mi>s</mi></msub></mrow></mfrac><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><msup><mi>y</mi><mo>′</mo></msup><msub><mrow></mrow><mi>s</mi></msub><mo>=</mo><mfrac><mrow><mi>d</mi><mi>y</mi></mrow><mrow><mi>d</mi><mi>s</mi></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><msup><mi>x</mi><mo>′</mo></msup><msub><mrow></mrow><mi>s</mi></msub><mo>=</mo><mfrac><mrow><mi>d</mi><mi>x</mi></mrow><mrow><mi>d</mi><mi>s</mi></mrow></mfrac><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="40">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904013_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于曲率函数的傅立叶描述子" src="Detail/GetImg?filename=images/MSSB201904013_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基于曲率函数的傅立叶描述子  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904013_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Fourier descriptor based on curvature function</p>

                </div>
                <div class="p1">
                    <p id="41">经过离散傅立叶变换 (Discrete Fourier Trans-form, DFT) , 空域中的曲率函数可以利用频域中的傅立叶变换系数描述.这种变换的结果称为傅立叶系数, 使用 (<i>f</i><sub>0</sub>, <i>f</i><sub>1</sub>, …, <i>f</i><sub><i>N</i>-1</sub>) 表示.因为<i>f</i><sub>0</sub>总是为0, <i>f</i><sub>1</sub>总是为1, 所以有<i>N</i>-2个可用系数.直流分量<i>f</i><sub>1</sub>仅指示形状位置, 则傅立叶描述子</p>
                </div>
                <div class="p1">
                    <p id="42"><mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mi>D</mi><mo>=</mo><mo stretchy="false"> (</mo><mfrac><mrow><mi>f</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mrow><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mo>, </mo><mfrac><mrow><mi>f</mi><msub><mrow></mrow><mn>3</mn></msub></mrow><mrow><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mo>, </mo><mo>⋯</mo><mo>, </mo><mfrac><mrow><mi>f</mi><msub><mrow></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mrow><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mo stretchy="false">) </mo></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="44">得到曲率函数形状描述子后需要进一步对其归一化处理.采用对数函数转换法对其进行归一化, 即</p>
                </div>
                <div class="p1">
                    <p id="45" class="code-formula">
                        <mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msup><mi>D</mi><mo>′</mo></msup><mo>=</mo><mfrac><mrow><mi>lg</mi><mrow><mo>|</mo><mrow><mfrac><mrow><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac></mrow><mo>|</mo></mrow></mrow><mrow><mi>lg</mi><mspace width="0.25em" /><mi>max</mi><mo stretchy="false"> (</mo><mrow><mo>|</mo><mrow><mfrac><mrow><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac></mrow><mo>|</mo></mrow><mo stretchy="false">) </mo></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><mi>i</mi><mo>=</mo><mn>2</mn><mo>, </mo><mn>3</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Ν</mi><mo>-</mo><mn>2</mn><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="46">通常, 在实际应用中只选取傅立叶变换后能量集中的低频部分作为行为描述特征, 本文实验取前面30个低频分量作为特征描述向量, 可以得到基于曲率函数的傅里叶描述子 (Curvature Function-Based FD, CFFD) , 因此RGB模态下特征视图可以表示为</p>
                </div>
                <div class="p1">
                    <p id="47" class="code-formula">
                        <mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>R</mi><mi>G</mi><mi>B</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mrow><mo stretchy="false">[</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>R</mi><mi>G</mi><mi>B</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false"> (</mo><mi>R</mi><mi>G</mi><mi>B</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>Ν</mi><mrow><mo stretchy="false"> (</mo><mi>R</mi><mi>G</mi><mi>B</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">]</mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>, </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>R</mi><mi>G</mi><mi>B</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mo stretchy="false">[</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>, </mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>3</mn><mn>0</mn></mrow></msub><mo stretchy="false">]</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="48" name="48"><b>2.2 Depth</b>模态下的时空兴趣点</h4>
                <div class="p1">
                    <p id="49">Laptev等<citation id="160" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出时空兴趣点 (Space-Time Interest Points) , 它是由空域兴趣点 (Interest Points in Spatial Domain) <citation id="161" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>扩展到时间域得到.由于Laptev等采用高斯滤波器进行滤波, 导致检测的兴趣点个数较少.Dollar等<citation id="162" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>采用Gabor滤波器进行滤波, 检测的兴趣点的数目会随着时间和空间的局部邻域尺寸的改变而改变.检测的兴趣点个数相对多且稳定.</p>
                </div>
                <div class="p1">
                    <p id="50">本文采用Dollar等提出的兴趣点检测方法对Depth模态下的视频图像序列进行兴趣点检测, 可以获得人体运动局部特征, 它对运动主体的表观变化、视角改变、遮挡的情况具有良好的鲁棒性.</p>
                </div>
                <div class="p1">
                    <p id="51">假设<i>P</i><sub><i>t</i></sub>={<i>p</i><sub>1</sub>, <i>p</i><sub>2</sub>, …, <i>p</i><sub><i>n</i></sub>}为深度图像序列<i>D</i> (<i>x</i>, <i>y</i>, <i>t</i>) 某一帧在某一时刻对应的时空兴趣点集, <i>n</i>为采集到的时空兴趣点个数, 每个兴趣点可由<i>p</i><sub><i>i</i></sub> (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) 表示.本文采用人体中心点作为极点<i>O</i>.这样兴趣点在图像坐标系下的坐标就可以转换为以人体中心点<i>O</i>为极点的极坐标表示形式, 即</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><msqrt><mrow><mrow><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mi>arctan</mi></mrow><mo stretchy="false"> (</mo><mfrac><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>c</mi></msub></mrow><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>c</mi></msub></mrow></mfrac><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="53">其中 (<i>x</i><sub><i>c</i></sub>, <i>y</i><sub><i>c</i></sub>) 、 (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) 分别是人体中心点和兴趣点在图像直角坐标系下的坐标.经过归一化处理后, 可以得到基于深度图像的时空兴趣点特征 (Depth Image-Based Space Time Interest Point, DISTIP) .因此Depth模态下特征视图可以表示为</p>
                </div>
                <div class="p1">
                    <p id="54" class="code-formula">
                        <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>D</mi><mi>e</mi><mi>p</mi><mi>t</mi><mi>h</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mo stretchy="false">[</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>D</mi><mi>e</mi><mi>p</mi><mi>t</mi><mi>h</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false"> (</mo><mi>D</mi><mi>e</mi><mi>p</mi><mi>t</mi><mi>h</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>Ν</mi><mrow><mo stretchy="false"> (</mo><mi>D</mi><mi>e</mi><mi>p</mi><mi>t</mi><mi>h</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">]</mo><mo>, </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>D</mi><mi>e</mi><mi>p</mi><mi>t</mi><mi>h</mi></mrow></msubsup><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false"> (</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mo stretchy="false"> (</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>2</mn></mrow></msub><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>2</mn></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mo stretchy="false"> (</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="55" name="55"><b>2.3 Joints</b>模态下的关节点投影分布特征</h4>
                <div class="p1">
                    <p id="56">随着微软Kinect传感器的推出, 获取人体三维骨架信息变得更加方便.Kinect能够采集人体的20个关节点的三维坐标信息, 利用这些关节点的坐标信息, 可以找到对人体姿态良好的特征表示, 如图3所示.</p>
                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904013_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 人体关节点在3个投影面的投影" src="Detail/GetImg?filename=images/MSSB201904013_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 人体关节点在3个投影面的投影  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904013_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Projection of joints points of human body</p>

                </div>
                <div class="p1">
                    <p id="58">基于Joints模态数据, 本文提出关节点投影空间分布特征以表征人体动作.首先, 将采集得到每帧的三维骨架数据在3个平面 (<i>XOY</i>平面、<i>YOZ</i>平面、<i>XOZ</i>平面) 进行正交投影, 得到单帧三维人体骨架关节数据的投影点的位置在不同投影平面上的分布.然后, 分别对3个投影平面上关节点采用极坐标方式表示:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><msqrt><mrow><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup><mo>+</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt><mo>, </mo><mspace width="0.25em" /><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>arctan</mi><mo stretchy="false"> (</mo><mfrac><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>2</mn><mn>0</mn><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">最后, 再将三个投影平面上的投影点极坐标顺序进行拼接, 作为这一帧的特征向量.</p>
                </div>
                <div class="p1">
                    <p id="61">得到关节点投影极坐标后, 需要进一步对其进行归一化处理.为了保存数据落在[0, 1]内, 采用极小极大法规一化处理, 可以得到具有平移变换、尺度变换和旋转变换不变性的Joints模态下关节点分布特征 (Joints Point Projection Distribution Feature, JPPDF) .因此Joints模态下的特征视图可以表示为</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>J</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>s</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mrow><mo stretchy="false">[</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>J</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>s</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false"> (</mo><mi>J</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>s</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>Ν</mi><mrow><mo stretchy="false"> (</mo><mi>J</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>s</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">]</mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>, </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>J</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mo stretchy="false">[</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>, </mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>2</mn><mn>0</mn></mrow></msub><mo stretchy="false">]</mo><mo>.</mo></mtd></mtr><mtr><mtd></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <h3 id="63" name="63" class="anchor-tag">3 多视图半监督学习模型</h3>
                <div class="p1">
                    <p id="64">目前的人体行为识别算法通常依赖于长时间的视频数据, 需要在动作发生一段时间后才能给出判断.科学研究表明人类可以从单幅图像中获取图像中高层语义信息, 可以从单幅图像中判断发生的动作类别.本文采用基于单帧的快速识别算法, 具有较高的计算效率.</p>
                </div>
                <div class="p1">
                    <p id="65">半监督学习或从标记和未标记数据中学习引起广泛关注.对于许多实际应用来说, 获取标签信息费时费力, 但较易收集未标记的样本.在这种情况下, 结合有限的标记数据与未标记数据, 有助于学习得到有效的函数表达.</p>
                </div>
                <div class="p1">
                    <p id="66">多视图学习 (Multi-view Learning) 是一个从多个不同特征集表示的数据中进行机器学习的问题.而多视图半监督学习具有视图协议 (View Agree-ments) 性质, 即通过要求不同视图学习得到的函数具有相似的输出, 可以减小假设空间的大小, 获得更好的泛化性能.</p>
                </div>
                <div class="p1">
                    <p id="67">通过Kinect传感器, 可以得到同一场景下不同模态图像数据, 并在此基础上学习得到人体行为数据集的3个特征视图<b><i>X</i></b><sup> (<i>view</i>) </sup>:<b><i>X</i></b><sup> (<i>RGB</i>) </sup>、<b><i>X</i></b><sup> (<i>Depth</i>) </sup>和<b><i>X</i></b><sup> (<i>Joints</i>) </sup>.每个给定特征视图<b><i>X</i></b><sup> (<i>view</i>) </sup>由有标记数据集</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>, </mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi>y</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>l</mi><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>∈</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msup></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">和未标记数据集</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>U</mi><mo>=</mo><mo stretchy="false">{</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mn>2</mn></mrow><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mi>u</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">}</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">组成, 其中</p>
                </div>
                <div class="p1">
                    <p id="72"><i>view</i>∈{<i>RGB</i>, <i>Depth</i>, <i>Joints</i>}, </p>
                </div>
                <div class="p1">
                    <p id="73"><i>l</i>为有标记样本个数, <i>u</i>为未记记样本个数.因此, 可在每个特征视图<b><i>X</i></b><sup> (<i>view</i>) </sup>上, 分别使用初始有标记数据<i>L</i>学习得到一个初始分类器<i>h</i><sup> (<i>view</i>) </sup>←<i>SVM</i> (<i>L</i>) </p>
                </div>
                <div class="p1">
                    <p id="74">然后使用这些分类器<i>h</i><sup> (<i>view</i>) </sup>分别对每个未标记数据<b><i>x</i></b>′∈<i>U</i>进行类别预测计算<i>h</i><sup> (<i>view</i>) </sup> (<b><i>x</i></b>′) , 这里分类器采用支持向量机 (SVM) , 它的输出值为概率值.如果行为类别数为<i>M</i>, 则有</p>
                </div>
                <div class="p1">
                    <p id="75">{<i>p</i><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, <i>p</i><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, …, <i>p</i><mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Μ</mi><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>}←<i>h</i><sup> (<i>view</i>) </sup> (<b><i>x</i></b>′) .</p>
                </div>
                <div class="p1">
                    <p id="79">对于未标记数据<b><i>x</i></b>′的动作类别预测概率值, 取3个视图上的平均值:</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>h</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mn>3</mn></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">|</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">|</mo></mrow></munder><mi>h</mi></mstyle><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">{</mo><mfrac><mn>1</mn><mn>3</mn></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">|</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">|</mo></mrow></munder><mi>p</mi></mstyle><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mn>1</mn><mn>3</mn></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">|</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">|</mo></mrow></munder><mi>p</mi></mstyle><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mfrac><mn>1</mn><mn>3</mn></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">|</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">|</mo></mrow></munder><mi>p</mi></mstyle><msubsup><mrow></mrow><mi>Μ</mi><mrow><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">则未标记数据<b><i>x</i></b>′最后预测类别为平均概率值最大的对应动作类</p>
                </div>
                <div class="p1">
                    <p id="82"><mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>←</mo><mrow><mi>arg</mi></mrow><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mn>1</mn><mo>&lt;</mo><mi>m</mi><mo>&lt;</mo><mi>Μ</mi></mrow></munder><mspace width="0.25em" /><mi>h</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="84">预测该类别的置信度</p>
                </div>
                <div class="p1">
                    <p id="85"><mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>o</mi><mi>n</mi><mi>f</mi><mi>i</mi><mi>d</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>←</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mn>1</mn><mo>&lt;</mo><mi>m</mi><mo>&lt;</mo><mi>Μ</mi></mrow></munder><mi>h</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="87">在完成未标记样本的预测与置信度评估后, 经过筛选后再将这些样本加入到有标记样本训练集中, 以此增大训练样本集.再重新训练模型, 经过迭代后, 显著提升模型的泛化能力.具体过程如算法所示.</p>
                </div>
                <div class="p1">
                    <p id="88"><b>算法</b> 多视图半监督学习算法 (MVSL) </p>
                </div>
                <div class="p1">
                    <p id="89"><b>输入</b> 初始有标记训练集<i>L</i>, 无标记训练集<i>U</i>, </p>
                </div>
                <div class="p1">
                    <p id="90">迭代次数<i>T</i>, 动作类别数<i>M</i>, </p>
                </div>
                <div class="p1">
                    <p id="91">基学习器算法SVM, 核函数<i>Kernel</i> (·) , </p>
                </div>
                <div class="p1">
                    <p id="92">核参数<i>para</i></p>
                </div>
                <div class="p1">
                    <p id="93"><b>输出</b> 各个视图上的分类器<i>h</i><sup> (<i>T</i>) (<i>view</i>) </sup></p>
                </div>
                <div class="p1">
                    <p id="94">训练部分:</p>
                </div>
                <div class="p1">
                    <p id="95">for <i>view</i>=<i>RGB</i>, <i>Depth</i>, <i>Joints</i></p>
                </div>
                <div class="p1">
                    <p id="96">构建初始学习器集合</p>
                </div>
                <div class="p1">
                    <p id="97"><i>h</i><sup> (0) (<i>RGB</i>) </sup> (<i>x</i>) ←<i>SVM</i> (<i>L</i><sup> (0) (<i>RGB</i>) </sup>, <i>Kernel</i>, <i>para</i>) </p>
                </div>
                <div class="p1">
                    <p id="98"><i>h</i><sup> (0) (<i>Depth</i>) </sup> (<i>x</i>) ←<i>SVM</i> (<i>L</i><sup> (0) (<i>Depth</i>) </sup>, <i>Kernel</i>, <i>para</i>) </p>
                </div>
                <div class="p1">
                    <p id="99"><i>h</i><sup> (0) (<i>Joints</i>) </sup> (<i>x</i>) ←<i>SVM</i> (<i>L</i><sup> (0) (<i>Joints</i>) </sup>, <i>Kernel</i>, <i>para</i>) </p>
                </div>
                <div class="p1">
                    <p id="100">end for</p>
                </div>
                <div class="p1">
                    <p id="101">for <i>t</i>=0, 1, …, <i>T</i>-1</p>
                </div>
                <div class="p1">
                    <p id="102">从无标记训练集<i>U</i>中随机抽取一个大小为<i>s</i>的子集<i>U</i>′</p>
                </div>
                <div class="p1">
                    <p id="103">for 每个<b><i>x</i></b>′<sub><i>i</i></sub>∈<i>U</i>′ do</p>
                </div>
                <div class="p1">
                    <p id="104">使用<i>h</i><sup> (<i>t</i>) (<i>view</i>) </sup> (<b><i>x</i></b>′<sub><i>i</i></sub>) 预测类别</p>
                </div>
                <div class="p1">
                    <p id="105"><i>h</i><sup> (<i>t</i>) (<i>view</i>) </sup> (<b><i>x</i></b>′<sub><i>i</i></sub>) ={<i>p</i><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, <i>p</i><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, …, <i>p</i><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Μ</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>}</p>
                </div>
                <div class="p1">
                    <p id="109">集成3个视图上的预测结果</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>h</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mn>3</mn></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">|</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">|</mo></mrow></munder><mi>h</mi></mstyle><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mo stretchy="false">{</mo><mfrac><mn>1</mn><mn>3</mn></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">|</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">|</mo></mrow></munder><mi>p</mi></mstyle><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mfrac><mn>1</mn><mn>3</mn></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">|</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">|</mo></mrow></munder><mi>p</mi></mstyle><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mfrac><mn>1</mn><mn>3</mn></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">|</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">|</mo></mrow></munder><mi>p</mi></mstyle><msubsup><mrow></mrow><mi>Μ</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">}</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">计算数据点<b><i>x</i></b>′<sub><i>i</i></sub>的类别</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo stretchy="false"> (</mo><msup><mi>x</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>←</mo><mrow><mi>arg</mi></mrow><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mn>1</mn><mo>&lt;</mo><mi>m</mi><mo>&lt;</mo><mi>Μ</mi></mrow></munder><mspace width="0.25em" /><mi>h</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">end for</p>
                </div>
                <div class="p1">
                    <p id="114">挑取置信度最高的前<i>top</i>个<b><i>x</i></b>′<sub><i>i</i></sub>和它的预测标记<i>y</i>′<sub><i>i</i></sub>保存在训练集<i>L</i><sup><i>t</i></sup>, 同时从<i>U</i>′删除<b><i>x</i></b>′<sub><i>i</i></sub></p>
                </div>
                <div class="p1">
                    <p id="115">更新有标记训练集<i>L</i><sup><i>t</i>+1</sup>←<i>L</i><sup><i>t</i></sup></p>
                </div>
                <div class="p1">
                    <p id="116">在更新训练集上重新训练学习器</p>
                </div>
                <div class="p1">
                    <p id="117"><i>h</i><sup> (<i>t</i>+1) (<i>RGB</i>) </sup> (<b><i>x</i></b>) ←<i>SVM</i> (<i>L</i><sup> (<i>t</i>+1) (<i>RGB</i>) </sup>, <i>Kernel</i>, <i>para</i>) </p>
                </div>
                <div class="p1">
                    <p id="118"><i>h</i><sup> (<i>t</i>+1) (<i>Depth</i>) </sup> (<b><i>x</i></b>) ←<i>SVM</i> (<i>L</i><sup> (<i>t</i>+1) (<i>Depth</i>) </sup>, <i>Kernel</i>, <i>para</i>) </p>
                </div>
                <div class="p1">
                    <p id="119"><i>h</i><sup> (<i>t</i>+1) (<i>Joints</i>) </sup> (<b><i>x</i></b>) ←<i>SVM</i> (<i>L</i><sup> (<i>t</i>+1) (<i>Joints</i>) </sup>, <i>Kernel</i>, <i>para</i>) </p>
                </div>
                <div class="p1">
                    <p id="120">从<i>U</i>中随机挑选<i>top</i>个样本更新子集<i>U</i>′</p>
                </div>
                <div class="p1">
                    <p id="121">end for</p>
                </div>
                <div class="p1">
                    <p id="122">通过多视图半监督学习方法得到<i>h</i><sup> (<i>T</i>) (<i>view</i>) </sup>后, 可以通过集成这三个视图的预测结果, 取各个视图上的平均值作为最后的输出结果</p>
                </div>
                <div class="p1">
                    <p id="123"><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>h</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mn>3</mn></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi></mrow></munder><mi>h</mi></mstyle><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>Τ</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>v</mi><mi>i</mi><mi>e</mi><mi>w</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="125">多视图半监督学习算法通过在不同的特征视图上训练各个分类器, 使每个成员分类模型具有不同的特征偏置效应, 并通过协同训练过程, 从整体上提升各个成员分类器的泛化能力.最后通过集成各个成员分类器的分类结果达到提升预测精度的效果.</p>
                </div>
                <h3 id="126" name="126" class="anchor-tag">4 实验及结果分析</h3>
                <div class="p1">
                    <p id="127">在Kinect室内行为数据集 (Kinect Indoor Action Dataset, KIAD) 、G3D行为数据集和CAD60 (Cornell Activity Dataset 60) 行为数据集上测试本文方法.</p>
                </div>
                <div class="p1">
                    <p id="128">大多数公开的人类行为识别基准数据库都是由强度照相机拍摄的视频数据.本文利用Kinect传感器拍摄人体动作视频图像数据集KIAD, 提供3种不同模态的图像信息:GRB模态图像、Depth图像和人体骨架关节信息图像.KIAD人体动作数据集包括10种不同行为:行走、坐着、侧身行走、跑步、拾起、跳跃、手摇、拍手、击拳和弯下.上述行为由9人执行完成.图4为Kinect人体行为数据库中10种行为的视频片段示例.G3D行为数据集 (http://dipersec.king.ac.uk/G3D/G3D.html) 包含20种人体动作, 每类的动作分别由10个人完成.CAD60行为数据集 (http://pr.cs.cornell.edu/humanactivities/data.php) 包含12个行为, 由4个人在5种不同的环境下完成.3个行为数据集中同种动作包含3种不同模态的图像信息:RGB图像、深度图像和骨架关节点图像.</p>
                </div>
                <div class="area_img" id="173">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904013_17300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 KIAD中的行为" src="Detail/GetImg?filename=images/MSSB201904013_17300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 KIAD中的行为  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904013_17300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Actions from KIAD</p>

                </div>
                <div class="p1">
                    <p id="133">实验对比监督学习方法 (本文实验中采用SVM学习模型) 和使用本文方法的识别率, 同时对比本文方法与其它经典方法.</p>
                </div>
                <div class="p1">
                    <p id="134">实验前需要对每个数据集进行有效划分, 本次实验中训练集和测试集中样本比例为4∶1, 进一步划分训练集样本, 取有标记样本和无标记样本的比例为1∶4.为了保持各个动作类在样本集中的比例均衡性, 划分后的样本集中各动作样本的比例与原数据集中比例保持相同.图5～图7给出SVM与MVSL的行为识别混淆矩阵结果.混淆矩阵对角线上的元素数值表示相应动作被正确分类的比例, 不在对角线上的 (<i>i</i>, <i>j</i>) 元素表示第<i>i</i>类行为被分类为第<i>j</i>类行为的比例.</p>
                </div>
                <div class="p1">
                    <p id="135">由图5可以看到:在KIAD数据集上, 本文方法识别率更高.由此表明:MVSL可以有效利用大量未标记样本提升学习系统的泛化能力, 达到较好的识别效果.</p>
                </div>
                <div class="p1">
                    <p id="136">图6和图7分别给出MVSL和SVM在G3D、CAD60数据集上的实验结果, 从中可以发现, MVSL都取得优于SVM的识别率.</p>
                </div>
                <div class="area_img" id="174">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904013_17400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 2种方法在KIAD数据集上的行为识别混淆矩阵" src="Detail/GetImg?filename=images/MSSB201904013_17400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 2种方法在KIAD数据集上的行为识别混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904013_17400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Confusion matrix of action recognition of 2 methods on KIAD dataset</p>

                </div>
                <div class="area_img" id="175">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904013_17500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 2种方法在G3D数据集上的行为识别混淆矩阵" src="Detail/GetImg?filename=images/MSSB201904013_17500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 2种方法在G3D数据集上的行为识别混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904013_17500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Confusion matrix of action recognition of 2 methods on G3D dataset</p>

                </div>
                <div class="area_img" id="141">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904013_14100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 2种方法在CAD60数据集上的行为识别混淆矩阵" src="Detail/GetImg?filename=images/MSSB201904013_14100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 2种方法在CAD60数据集上的行为识别混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904013_14100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Confusion matrix of action recognition of 2 methods on CAD60 dataset</p>

                </div>
                <div class="p1">
                    <p id="142">表1为SVM与MVSL在3个数据集上的精确率、召回率和F值对比结果.从表中可以看到, 在不同的数据集上, MVSL都取得较好的学习较果.</p>
                </div>
                <div class="area_img" id="143">
                    <p class="img_tit"><b>表1 2种方法在3个数据集上的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Experimental results of 2 methods on 3 datasets</p>
                    <p class="img_note"></p>
                    <table id="143" border="1"><tr><td rowspan="2"><br />数据集</td><td colspan="2"><br />精确率</td><td colspan="2"><br />召回率</td><td colspan="2"><br />F值</td></tr><tr><td><br />SVM</td><td>MVSL</td><td><br />SVM</td><td>MVSL</td><td><br />SVM</td><td>MVSL</td></tr><tr><td>KIAD</td><td>0.87</td><td>0.96</td><td>0.87</td><td>0.95</td><td>0.87</td><td>0.95</td></tr><tr><td><br />G3D</td><td>0.83</td><td>0.90</td><td>0.83</td><td>0.90</td><td>0.83</td><td>0.90</td></tr><tr><td><br />CAD60</td><td>0.84</td><td>0.91</td><td>0.84</td><td>0.91</td><td>0.84</td><td>0.91</td></tr><tr><td><br />平均值</td><td>0.85</td><td>0.92</td><td>0.85</td><td>0.92</td><td>0.85</td><td>0.92</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="144">图8给出3个数据集上未标记训练样本个数对学习器泛化精度的影响.从图中可以看到, 在初始有标记训练样本为100, 700, 1300的情况下, 伴随着无标记训练样本的增加, 学习器的精度均得到提升.整体上看, 当有标记样本为100时, 学习器的精度提升最大.因此对于多视图半监督学习来说, 选择合适的有标记样本和未标记样本的比例很重要.</p>
                </div>
                <div class="area_img" id="176">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904013_17600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 3个数据集上无标记样本个数对实验结果的影响" src="Detail/GetImg?filename=images/MSSB201904013_17600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 3个数据集上无标记样本个数对实验结果的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904013_17600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Effect of unlabeled sample number on experimental results on 3 datasets</p>

                </div>
                <div class="area_img" id="176">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904013_17601.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 3个数据集上无标记样本个数对实验结果的影响" src="Detail/GetImg?filename=images/MSSB201904013_17601.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 3个数据集上无标记样本个数对实验结果的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904013_17601.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Effect of unlabeled sample number on experimental results on 3 datasets</p>

                </div>
                <div class="p1">
                    <p id="151">表2给出MVSL与其它半监督学习方法的实验对比情况.自训练1 (Self-Training 1, ST1) 为基于SVM的自训练 (Self-Training) 半监督学习算法, 首先在有标记数据集上训练初始SVM分类器, 使用训练所得的SVM分类器预测未标记数据, 预测得出的预测类别概率越大, 表示分类取得的置信度越高, 将置信度较高的未标记样本和其类别标记一起加入到已标记训练集中, 重新训练分类器, 经过迭代训练达到停机条件.自训练2 (Self-Training 2, ST2) 是自训练的半监督学习算法, 采用随机森林 (Random Forest, RF) 作为学习器.FAKE为协同训练算法, 采用随机方法分割数据特征, 产生两个人工分割特征视图, 应用标准的协同训练 (Co-training) 方法进行训练.从表2可以看到, MVSL在3个数据集上的正确率都优于其它方法.</p>
                </div>
                <div class="area_img" id="152">
                    <p class="img_tit"><b>表2 2种方法在3个数据集上的正确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Accuracy comparison of 2 methods on 3 datasets</p>
                    <p class="img_note"></p>
                    <table id="152" border="1"><tr><td>数据集</td><td>SL</td><td>ST1</td><td>ST2</td><td>FAKE</td><td>MVSL</td></tr><tr><td><br />KIAD</td><td>0.60</td><td>0.89</td><td>0.90</td><td>0.91</td><td>0.95</td></tr><tr><td><br />G3D</td><td>0.46</td><td>0.87</td><td>0.86</td><td>0.89</td><td>0.92</td></tr><tr><td><br />CAD60</td><td>0.78</td><td>0.90</td><td>0.89</td><td>0.87</td><td>0.94</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="153" name="153" class="anchor-tag">5 结 束 语</h3>
                <div class="p1">
                    <p id="154">本文提出基于多视图半监督学习的人体行为识别方法.利用Kinect提供的3种不同模态图像数据构造不同的行为特征视图, 然后采用多视图半监督学习算法进行协同训练, 充分挖掘各个行为特征视图之间有用的信息和大量未标记样本的潜在信息, 增强学习器的泛化能力.实验表明, 本文方法在准确度和效率方面优于许多已有的半监督学习方法, 因此, 本文方法可行且高效.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="168" type="formula" href="images/MSSB201904013_16800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">唐超</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="169" type="formula" href="images/MSSB201904013_16900.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">王文剑</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="170" type="formula" href="images/MSSB201904013_17000.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">王晓峰</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="171" type="formula" href="images/MSSB201904013_17100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">张琛</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="172" type="formula" href="images/MSSB201904013_17200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">邹乐</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bi-view Semi-supervised Learning Based Semantic Human Activity Recognition Using Acce-lerometers">

                                <b>[1]</b> LÜ M Q, CHEN L, CHEN T M, <i>et al</i>.Bi-view Semi-supervised Learning Based Semantic Human Activity Recognition Using Acce-lerometers.IEEE Transactions on Mobile Computing, 2018, 17 (9) :1991-2001.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Semi-supervised Human Action Recognition Algorithm Based on Skeleton Feature">

                                <b>[2]</b> YUAN H J.A Semi-supervised Human Action Recognition Algorithm Based on Skeleton Feature.Journal of Information Hiding and Multimedia Signal Processing, 2015, 6 (1) :175-182.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700154695&amp;v=MTE4NTNuVThvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRnNXYWhNPU5pZk9mYks4SDlETXFJOUZaZTRMQw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> LIU W F, LIU H L, TAO D P, <i>et al</i>.Multiview Hessian Regula-rized Logistic Regression for Action Recognition.Signal Processing, 2015, 110:101-107.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised Multiple Feature Analysis for Action Recognition">

                                <b>[4]</b> WANG S, MA Z G, YANG Y, <i>et al</i>.Semi-supervised Multiple Feature Analysis for Action Recognition.IEEE Transactions on Multimedia, 2014, 16 (2) :289-298.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738134&amp;v=MTc0MzA4OW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGc1dhaE09TmlmT2ZiSzdIdEROcVk5RlkrZ0hEWA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> ZHANG T Z, LIU S, XU C S, <i>et al</i>.Boosted Multi-class Semi-supervised Learning for Human Action Recognition.Pattern Recogni-tion, 2011, 44 (10/11) :2334-2342.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Space-time interest points">

                                <b>[6]</b> LAPTEV I, LINDEBERG T.Space-Time Interest Points // Proc of the 9th IEEE International Conference on Computer Vision.Wa-shington, USA:IEEE, 2003:432-439.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" >
                                    <b>[7]</b>
                                 HARRIS C, STEPHENS M.A Combined Corner and Edge Detector // Proc of the 4th Alvey Vision Conference.New York, USA:ACM, 1988, 15:147-151.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Behavior recognition via sparse spatio-temporal features">

                                <b>[8]</b> DOLLAR P, RABAUD V, COTTRELL G, <i>et al</i>.Behavior Recognition via Sparse Spatio-Temporal Features // Proc of the 2nd Joint IEEE International Workshop on Visual Surveillance and Perfor-mance Evaluation of Tracking and Surveillance.Washington, USA:IEEE, 2005:65-72.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201904013" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201904013&amp;v=MDM1MTR6cXFCdEdGckNVUkxPZVplUm5GeXpnVWIvSktEN1liTEc0SDlqTXE0OUVaNFFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
