<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131440955342500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201903001%26RESULT%3d1%26SIGN%3dnOU5%252bTsyprrhhnlHHZWNUAhwwBM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201903001&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201903001&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201903001&amp;v=MDA0NjhaZVJuRnl6blZydk1LRDdZYkxHNEg5ak1ySTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#105" data-title="1 YOLO实时目标检测算法 ">1 YOLO实时目标检测算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#124" data-title="2 基于改进YOLO的水下鱼类目标检测算法 ">2 基于改进YOLO的水下鱼类目标检测算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#133" data-title="2.1 Underwater-YOLO网络结构设计">2.1 Underwater-YOLO网络结构设计</a></li>
                                                <li><a href="#151" data-title="2.2 Underwater-YOLO的迁移学习">2.2 Underwater-YOLO的迁移学习</a></li>
                                                <li><a href="#164" data-title="2.3 基于CLAHE的水下图像增强预处理">2.3 基于CLAHE的水下图像增强预处理</a></li>
                                                <li><a href="#186" data-title="2.4 基于结构相似度的视频帧选择性前向推理计算">2.4 基于结构相似度的视频帧选择性前向推理计算</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#200" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#203" data-title="3.1 Underwater-YOLO检测结果">3.1 Underwater-YOLO检测结果</a></li>
                                                <li><a href="#212" data-title="3.2 训练策略对神经网络性能的影响">3.2 训练策略对神经网络性能的影响</a></li>
                                                <li><a href="#221" data-title="3.3 选择性前向推理对目标检测速度的影响">3.3 选择性前向推理对目标检测速度的影响</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#228" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#108" data-title="图1 YOLO网络结构Fig.1 Network structure of YOLO">图1 YOLO网络结构Fig.1 Network structure of YOLO</a></li>
                                                <li><a href="#111" data-title="图2 YOLO输出矩阵信息编码Fig.2 Coding mode of output matrix information of YOLO">图2 YOLO输出矩阵信息编码Fig.2 Coding mode of output matrix ......</a></li>
                                                <li><a href="#131" data-title="图3 本文算法总体框架Fig.3 Framework of the proposed method.">图3 本文算法总体框架Fig.3 Framework of the proposed method.</a></li>
                                                <li><a href="#136" data-title="图4 Underwater-YOLO的网络结构Fig.4 Network structure of Underwater-YOLO">图4 Underwater-YOLO的网络结构Fig.4 Network structure of ......</a></li>
                                                <li><a href="#139" data-title="图5 Underwater-YOLO的特征流程图Fig.5 Feature flow chart of underwater-YOLO">图5 Underwater-YOLO的特征流程图Fig.5 Feature flow chart o......</a></li>
                                                <li><a href="#142" data-title="图6 Underwater-YOLO预测矩阵编码方式Fig.6Encoding mode of prediction matrix information of Underwater-YOLO">图6 Underwater-YOLO预测矩阵编码方式Fig.6Encoding mode of pr......</a></li>
                                                <li><a href="#144" data-title="图7 锚点框与边界框的位置关系Fig.7Positional relationship between anchor box and bounding box">图7 锚点框与边界框的位置关系Fig.7Positional relationship betwee......</a></li>
                                                <li><a href="#156" data-title="图8 Underwater-YOLO的知识迁移过程Fig.8 Knowledge transferring of Underwater-YOLO">图8 Underwater-YOLO的知识迁移过程Fig.8 Knowledge transferr......</a></li>
                                                <li><a href="#198" data-title="图9 视频帧选择性前向推理计算过程Fig.9 Forward inference computation based on key frame selection">图9 视频帧选择性前向推理计算过程Fig.9 Forward inference computati......</a></li>
                                                <li><a href="#207" data-title="表1 水下目标检测实验结果Table 1 Experimental results of underwater object detection">表1 水下目标检测实验结果Table 1 Experimental results of under......</a></li>
                                                <li><a href="#209" data-title="图1 0 2种网络结构检测结果对比Fig.10 Comparison of detection results of 2 networks">图1 0 2种网络结构检测结果对比Fig.10 Comparison of detection re......</a></li>
                                                <li><a href="#214" data-title="表2 Underwater-YOLO使用不同训练策略的网络性能对比Table 2 Performance comparison of Underwater-YOLO using different training strategies">表2 Underwater-YOLO使用不同训练策略的网络性能对比Table 2 Performan......</a></li>
                                                <li><a href="#218" data-title="图1 1 2种训练策略检测结果对比Fig.11 Comparison of detection results using different training strategies">图1 1 2种训练策略检测结果对比Fig.11 Comparison of detection re......</a></li>
                                                <li><a href="#225" data-title="表3 在GTX1080ti GPU平台上视频目标检测处理速度Table 3 Speed of video object detection on GTX1080ti GPU platform">表3 在GTX1080ti GPU平台上视频目标检测处理速度Table 3 Speed of vid......</a></li>
                                                <li><a href="#226" data-title="表4 在Intel i5CPU平台上视频目标检测处理速度Table 4 Speed of video object detection on Intel i5 CPU platform">表4 在Intel i5CPU平台上视频目标检测处理速度Table 4 Speed of video......</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="51">


                                    <a id="bibliography_1" title="QIAO X, BAO J H, ZENG L H, et al.An Automatic Active Contour Method for Sea Cucumber Segmentation in Natural Underwater Environments.Computers and Electronics in Agriculture, 2017, 135:134-142." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1EB412C753E00679AC6EE0E85B6F7B46&amp;v=MDQyNDVmeENZZWg2REh3L3lCOWltVGtJUFgrWHBCZEhmOFNUTjc2WkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHdMcTZ4S3c9TmlmT2ZiTE5iTlhOcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        QIAO X, BAO J H, ZENG L H, et al.An Automatic Active Contour Method for Sea Cucumber Segmentation in Natural Underwater Environments.Computers and Electronics in Agriculture, 2017, 135:134-142.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_2" title="LI Q Z, ZHANG Y, ZANG F N.Fast Multicamera Video Stitching for Underwater Wide Field-of-View Observation.Journal of Electronic Imaging, 2014, 23 (2) :367-368." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJEG&amp;filename=SJEG5F9C44723FF5F7DDE3141B10FC9E8F19&amp;v=MjEzNDROaWZPYWJiT0Y2TElxNGhIWjUxNUNRbyt1MkptNlQ1NVNRM2pyR1JHY01lY003dVdDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3THE2eEt3PQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        LI Q Z, ZHANG Y, ZANG F N.Fast Multicamera Video Stitching for Underwater Wide Field-of-View Observation.Journal of Electronic Imaging, 2014, 23 (2) :367-368.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_3" title="BONIN-FONT F, OLIVER G, WIRTH S, et al.Visual Sensing for Autonomous Underwater Exploration and Intervention Tasks.Ocean Engineering, 2015, 93 (1) :25-44." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122200114518&amp;v=MTgxMjBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZ3UmJoWT1OaWZPZmJLOUg5UE9yWTlGWmVvTENYMHhvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        BONIN-FONT F, OLIVER G, WIRTH S, et al.Visual Sensing for Autonomous Underwater Exploration and Intervention Tasks.Ocean Engineering, 2015, 93 (1) :25-44.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_4" title="MAHMOOD A, BENNAMOUN M, AN S J, et al.Deep Image Representations for Coral Image Classification.IEEE Journal of Oceanic Engineering, 2019, 44 (1) :121-131." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Image Representations for Coral Image Classification">
                                        <b>[4]</b>
                                        MAHMOOD A, BENNAMOUN M, AN S J, et al.Deep Image Representations for Coral Image Classification.IEEE Journal of Oceanic Engineering, 2019, 44 (1) :121-131.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_5" title="GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[5]</b>
                                        GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:580-587.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_6" title="GIRSHICK R.Fast R-CNN//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast r-cnn">
                                        <b>[6]</b>
                                        GIRSHICK R.Fast R-CNN//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:1440-1448.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_7" title="REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks.IEEETransactions on Pattern Analysis and Machine Intelligence, 2017, 6 (1) :1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">
                                        <b>[7]</b>
                                        REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks.IEEETransactions on Pattern Analysis and Machine Intelligence, 2017, 6 (1) :1137-1149.
                                    </a>
                                </li>
                                <li id="65">


                                    <a id="bibliography_8" title="REDMON J, DIVVALA S, GIRSHICK R, et al.You Only Look Once:Unified, Real-Time Object Detection//Proc of the IEEEConference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:779-788." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You only lookonce:unified,real-time object detection">
                                        <b>[8]</b>
                                        REDMON J, DIVVALA S, GIRSHICK R, et al.You Only Look Once:Unified, Real-Time Object Detection//Proc of the IEEEConference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:779-788.
                                    </a>
                                </li>
                                <li id="67">


                                    <a id="bibliography_9" title="REDMON J, FARHADI A.YOLO9000:Better, Faster, Stronger//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:6517-6525." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better,Faster,Stronger">
                                        <b>[9]</b>
                                        REDMON J, FARHADI A.YOLO9000:Better, Faster, Stronger//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:6517-6525.
                                    </a>
                                </li>
                                <li id="69">


                                    <a id="bibliography_10" title="REDMON J, FARHADI A.Yolov3:An Incremental Improvement[C/OL].[2018-10-11].https://arxiv.org/pdf/1804.02767.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLOv3:An Incremental Improvement[C/OL]">
                                        <b>[10]</b>
                                        REDMON J, FARHADI A.Yolov3:An Incremental Improvement[C/OL].[2018-10-11].https://arxiv.org/pdf/1804.02767.pdf.
                                    </a>
                                </li>
                                <li id="71">


                                    <a id="bibliography_11" title="LIN T Y, MAIRE M, BELONGIE S, et al.Microsoft COCO:Common Objects in Context//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2014:740-755." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Microsoft COCO:common objects in context">
                                        <b>[11]</b>
                                        LIN T Y, MAIRE M, BELONGIE S, et al.Microsoft COCO:Common Objects in Context//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2014:740-755.
                                    </a>
                                </li>
                                <li id="73">


                                    <a id="bibliography_12" title="LIU W, ANGUELOV D, ERHAN D, et al.SSD:Single Shot Multibox Detector//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ssd:Single shot multibox detector">
                                        <b>[12]</b>
                                        LIU W, ANGUELOV D, ERHAN D, et al.SSD:Single Shot Multibox Detector//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:21-37.
                                    </a>
                                </li>
                                <li id="75">


                                    <a id="bibliography_13" title="HSIAO Y H, CHEN C C, LIN S I, et al.Real-World Underwater Fish Recognition and Identification, Using Sparse Representation.Ecological Informatics, 2014, 23:13-21." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600536416&amp;v=MjQxNjg2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGd1JiaFk9TmlmT2ZiSzhIdERNcVk5RlllZ0pDSDAvb0JNVA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        HSIAO Y H, CHEN C C, LIN S I, et al.Real-World Underwater Fish Recognition and Identification, Using Sparse Representation.Ecological Informatics, 2014, 23:13-21.
                                    </a>
                                </li>
                                <li id="77">


                                    <a id="bibliography_14" title="CUTTER G, STIERHOFF K, ZENG J M.Automated Detection of Rockfish in Unconstrained Underwater Videos Using Haar Cascades and a New Image Dataset:Labeled Fishes in the Wild//Proc of the IEEE Winter Applications and Computer Vision Workshops.Washington, USA:IEEE, 2015:57-62." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automated detection of rockfish in unconstrained underwater videos using Haar cascades and a new image dataset:Labeled fishes in the wild">
                                        <b>[14]</b>
                                        CUTTER G, STIERHOFF K, ZENG J M.Automated Detection of Rockfish in Unconstrained Underwater Videos Using Haar Cascades and a New Image Dataset:Labeled Fishes in the Wild//Proc of the IEEE Winter Applications and Computer Vision Workshops.Washington, USA:IEEE, 2015:57-62.
                                    </a>
                                </li>
                                <li id="79">


                                    <a id="bibliography_15" title="SEESE N, MYERS A, SMITH K, et al.Adaptive Foreground Extraction for Deep Fish Classification//Proc of the 2nd Workshop on Computer Vision for Analysis of Underwater Imagery.Washington, USA:IEEE, 2016:19-24." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive Foreground Extraction for Deep Fish Classification">
                                        <b>[15]</b>
                                        SEESE N, MYERS A, SMITH K, et al.Adaptive Foreground Extraction for Deep Fish Classification//Proc of the 2nd Workshop on Computer Vision for Analysis of Underwater Imagery.Washington, USA:IEEE, 2016:19-24.
                                    </a>
                                </li>
                                <li id="81">


                                    <a id="bibliography_16" title="LI X, TANG Y H, GAO T W.Deep But Lightweight Neural Networks for Fish Detection//Proc of the OCEANS 2017.Washington, USA:IEEE, 2017.DOI:10.1109/OCEANSE.2017.8084961." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep But Lightweight Neural Networks for Fish Detection">
                                        <b>[16]</b>
                                        LI X, TANG Y H, GAO T W.Deep But Lightweight Neural Networks for Fish Detection//Proc of the OCEANS 2017.Washington, USA:IEEE, 2017.DOI:10.1109/OCEANSE.2017.8084961.
                                    </a>
                                </li>
                                <li id="83">


                                    <a id="bibliography_17" title="SUNG M, YU S C, GIRDHAR Y.Vision Based Real-Time Fish Detection Using Convolutional Neural Network//Proc of the OCEANS 2017.Washington, USA:IEEE, 2017.DOI:10.1109/OCEANSE.2017.8084889." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vision Based Real-Time Fish Detection Using Convolutional Neural Network">
                                        <b>[17]</b>
                                        SUNG M, YU S C, GIRDHAR Y.Vision Based Real-Time Fish Detection Using Convolutional Neural Network//Proc of the OCEANS 2017.Washington, USA:IEEE, 2017.DOI:10.1109/OCEANSE.2017.8084889.
                                    </a>
                                </li>
                                <li id="85">


                                    <a id="bibliography_18" title="LONG J, SHELHAMER E, DARRELL T.Fully Convolutional Networks for Semantic Segmentation//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[18]</b>
                                        LONG J, SHELHAMER E, DARRELL T.Fully Convolutional Networks for Semantic Segmentation//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:3431-3440.
                                    </a>
                                </li>
                                <li id="87">


                                    <a id="bibliography_19" title="PAN S J, YANG Q.A Survey on Transfer Learning.IEEE Transactions on Knowledge and Data Engineering, 2010, 22 (10) :1345-1359." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Survey on Transfer Learning">
                                        <b>[19]</b>
                                        PAN S J, YANG Q.A Survey on Transfer Learning.IEEE Transactions on Knowledge and Data Engineering, 2010, 22 (10) :1345-1359.
                                    </a>
                                </li>
                                <li id="89">


                                    <a id="bibliography_20" title="YOSINSKI J, CLUNE J, BENGIO Y, et al.How Transferable Are Features in Deep Neural Networks?//Proc of the 27th International Conference on Neural Information Processing Systems.Cambridge, USA:The MIT Press, 2014:3320-3328." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=How Transferable are Features in Deep Neural Networks?">
                                        <b>[20]</b>
                                        YOSINSKI J, CLUNE J, BENGIO Y, et al.How Transferable Are Features in Deep Neural Networks?//Proc of the 27th International Conference on Neural Information Processing Systems.Cambridge, USA:The MIT Press, 2014:3320-3328.
                                    </a>
                                </li>
                                <li id="91">


                                    <a id="bibliography_21" title="RUSSAKOVSKY O, DENG J, SU H, et al.Imagenet Large Scale Visual Recognition Challenge.International Journal of Computer Vision, 2015, 115 (3) :211-252." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet:Large scale visual recognition challenge">
                                        <b>[21]</b>
                                        RUSSAKOVSKY O, DENG J, SU H, et al.Imagenet Large Scale Visual Recognition Challenge.International Journal of Computer Vision, 2015, 115 (3) :211-252.
                                    </a>
                                </li>
                                <li id="93">


                                    <a id="bibliography_22" title="LI X, SHANG M, HAO J, et al.Accelerating Fish Detection and Recognition by Sharing CNNs with Objectness Learning//Proc of the OCEANS 2016.Washington, USA:IEEE, 2016.DOI:10.1109/OCEANSAP.2016.7485476." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accelerating Fish Detection and Recognition by Sharing CNNs with Objectness Learning">
                                        <b>[22]</b>
                                        LI X, SHANG M, HAO J, et al.Accelerating Fish Detection and Recognition by Sharing CNNs with Objectness Learning//Proc of the OCEANS 2016.Washington, USA:IEEE, 2016.DOI:10.1109/OCEANSAP.2016.7485476.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(03),193-203 DOI:10.16451/j.cnki.issn1003-6059.201903001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于改进YOLO和迁移学习的水下鱼类目标实时检测</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%BA%86%E5%BF%A0&amp;code=10326182&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李庆忠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%AE%9C%E5%85%B5&amp;code=41420904&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李宜兵</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%89%9B%E7%82%AF&amp;code=21863653&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">牛炯</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E6%B5%B7%E6%B4%8B%E5%A4%A7%E5%AD%A6%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0108748&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国海洋大学工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了实现非限制环境中水下机器人基于视频图像的水下鱼类目标快速检测, 提出基于改进YOLO和迁移学习的水下鱼类目标实时检测算法.针对YOLO网络的不足, 设计适合水下机器人嵌入式系统计算能力的精简YOLO网络 (Underwater-YOLO) .利用迁移学习方法训练Underwater-YOLO网络, 克服海底鱼类已知样本集有限的限制.利用基于限制对比度自适应直方图均衡化的水下图像增强预处理算法, 克服水下图像的降质问题.利用基于帧间图像结构相似度的选择性网络前向计算策略, 提高视频帧检测速率.实验表明, 文中算法能实现在非限制环境下海底鱼类目标的实时检测.相比YOLO, 文中算法对海底鱼类小目标和重叠目标具有更好的检测性能.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%B1%BC%E7%B1%BB%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">鱼类目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%95%E7%BA%A7%E5%BC%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%20(YOLO)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">单级式目标检测算法 (YOLO) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">迁移学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李庆忠, 博士, 教授, 主要研究方向为图像处理、信号处理、模式识别.E-mail:liqingzhong@ouc.edu.cn.;
                                </span>
                                <span>
                                    *李宜兵 (通讯作者) , 硕士研究生, 主要研究方向为智能信息处理、模式识别.E-mail:liy-ibing@stu.ouc.edu.cn.;
                                </span>
                                <span>
                                    牛炯, 博士研究生, 工程师, 主要研究方向为高频雷达信号处理、高频雷达海洋环境监测技术.E-mail:459258810@qq.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-14</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目 (No.2017YFC1405202);</span>
                                <span>国家自然科学基金项目 (No.61132005);</span>
                                <span>海洋公益性行业科研专项 (No.201605002);</span>
                    </p>
            </div>
                    <h1>Real-Time Detection of Underwater Fish Based on Improved YOLO and Transfer Learning</h1>
                    <h2>
                    <span>LI Qingzhong</span>
                    <span>LI Yibing</span>
                    <span>NIU Jiong</span>
            </h2>
                    <h2>
                    <span>College of Engineering, Ocean University of China</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To fast detect underwater fish in unrestricted underwater environment based on underwater video collected by underwater robots, a real-time detection algorithm for underwater fish based on improved you only look once ( YOLO) and transfer learning is proposed. Firstly, an underwater-YOLO for the embedding computer system of underwater robots is designed to overcome the shortcomings of traditional YOLO. Then, transfer learning strategy is employed to train the underwater-YOLO network and alleviate the limitation of known underwater fish samples. A preprocessing algorithm based on contrast limited adaptive histogram equalization is proposed to overcome the problem of underwater image degradation. Finally, a video frame selection method for foreground computation of underwater-YOLO based on structure similarity between inter-frames is proposed to increase the detection frame rate. The experimental results show that the proposed algorithm achieves the goal of real-time detection of underwater fish in unconstrained underwater environment. Compared with the traditional YOLO, the proposed underwater-YOLO generates better detection performance in complex scenes with small fish and overlapped fishes.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Fish%20Target%20Detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Fish Target Detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=You%20Only%20Look%20Once%20(YOLO)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">You Only Look Once (YOLO) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Transfer%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Transfer Learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LI Qingzhong, Ph. D., professor. His research interests include image processing, signal processing and pattern recognition.;
                                </span>
                                <span>
                                    LI Yibing ( Corresponding author) , master student. His research interests include intelligent information processing and pattern recognition.;
                                </span>
                                <span>
                                    NIU Jiong, Ph.D. candidate, engineer. His research interests include high frequency radar signal processing and high frequency radar marine environment monitoring technology.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-11-14</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Key R&amp;D Plan of China (No.2017YFC1405202);</span>
                                <span>National Natural Science Foundation of China (No.61132005);</span>
                                <span>National Marine Technology Program for Public Welfare of China (No.201605002);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="95">深海探测与作业技术是海洋技术研究的重要领域之一.水下机器人 (有缆水下机器人 (Remote Operated Vehicles, ROVs) /无缆水下机器人 (Autonomous Underwater Vehicles, AUVs) 是目前最先进的深海探测与作业装备.要实现深海探测与作业, 水下机器人必须能快速识别理解海底环境, 并对感兴趣的目标进行准确识别与定位<citation id="230" type="reference"><link href="51" rel="bibliography" /><link href="53" rel="bibliography" /><link href="55" rel="bibliography" /><link href="57" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>.因此, 利用装有水下摄像机的AUVs对水下感兴趣目标进行实时搜寻、监测具有重要研究价值和应用前景.本文目标就是研究海底非限制环境下基于视频图像的鱼类目标的快速检测算法, 为海底鱼类等生物资源的监测、保护及可持续开发提供技术支持.</p>
                </div>
                <div class="p1">
                    <p id="96">在地面非限制环境下, 感兴趣目标的检测算法发展迅速, 在检测精度和实时性方面已接近实用性.其主要原因是基于深度卷积神经网络 (Deep Convolutional Neural Network, DCNN) 的目标检测算法可有效克服非限制环境目标检测的困难和瓶颈.其主流方法可分为如下两类.</p>
                </div>
                <div class="p1">
                    <p id="97">1) 基于区域卷积神经网络 (Region Convolutional Neural Network, R-CNN) 深度学习的目标检测方法.Girshick等<citation id="231" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出R-CNN, 由于R-CNN在所有候选区域中重复进行特征提取, 影响目标检测的效率, 所以快速区域神经网络 (Fast R-CNN) <citation id="232" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>采用直接在特征图像上产生候选区域的方式, 避免反复进行特征提取, 提高目标检测效率.超快区域神经网络 (Faster R-CNN) <citation id="233" type="reference"><link href="63" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>直接使用候选区域生成网络 (Region Proposal Network, RPN) 产生召回率更高的候选区域, 平均准确率更高, 处理速度更快.</p>
                </div>
                <div class="p1">
                    <p id="98">2) 基于单级式目标检测算法 (You Only Look Once, YOLO) 深度学习目标检测算法.为了克服R-CNN的目标检测算法实时性差的问题, Redmon等<citation id="234" type="reference"><link href="65" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出YOLO, 摒弃候选区域生成的中间步骤, 将目标区域预测和目标类别预测整合在单个DCNN模型中.在YOLO基础上Redmon等<citation id="235" type="reference"><link href="67" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出YOLOv2 (YOLO Version 2, YOLOv2) , 采用诸多改进策略, 提高平均准确率和运行速度.在YOLO基础上, Redmon等<citation id="236" type="reference"><link href="69" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>又提出YOLOv3算法 (YOLO Version 3, YOLOv3) , 在特征提取阶段, 采用浅层次特征和深层次特征的融合, 提取更具鉴别性的深层特征.对于Microsoft COCO数据集<citation id="237" type="reference"><link href="71" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 准确率与SSD算法 (Single Shot MultiBox De-tector, SSD) <citation id="238" type="reference"><link href="73" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>相当, 处理速度为SSD的3倍.</p>
                </div>
                <div class="p1">
                    <p id="99">虽然地面目标的实时检测算法已较完善, 但在水下非限制环境中, 水下感兴趣目标的实时检测仍面临许多挑战, 其主要原因是水下介质、水下光照条件、水下海底环境等都要比地面机器人面临的环境复杂.对于基于水下视觉的鱼类目标检测, 目前代表性的研究方法主要分为如下3类.</p>
                </div>
                <div class="p1">
                    <p id="100">1) 基于水下图像底层特征的鱼类目标检测算法.Hsiao等<citation id="239" type="reference"><link href="75" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出真实水下非限制环境中鱼类目标的检测识别算法, 通过提取水下图像的各种底层特征, 并利用稀疏表达技术设计分类器进行鱼类目标的辨识, 识别率可达81.8%, 但实时性较差.Cutter等<citation id="240" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>利用Haar-like特征和多个级联分类器实现海底非限制环境下岩石鱼类目标的检测, 检测率可达89%, 但对于鱼类非正常姿态、背景复杂、光照条件恶劣、噪声过大、分辨率较低等非限制环境的检测结果并不理想, 误检率较高.</p>
                </div>
                <div class="p1">
                    <p id="101">2) 基于R-CNN深度学习的鱼类目标检测算法.Seese等<citation id="241" type="reference"><link href="79" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出鱼类目标检测分类算法, 首先利用融合技术估计背景图像并实现鱼类前景区域的分割, 然后对于分割的鱼类前景区域, 再利用DCNN自动提取特征与分类.Li等<citation id="242" type="reference"><link href="81" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>改进Faster R-CNN的结构, 提出适合水下鱼类目标检测的轻型R-CNN, 虽然检测率可达89.95%, 但实时性仍欠佳, 仅为11帧/秒.</p>
                </div>
                <div class="p1">
                    <p id="102">3) 基于YOLO深度学习的鱼类目标检测算法.Sung等<citation id="243" type="reference"><link href="83" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>把一般地面目标实时检测的YOLO网络结构模型移植到水下鱼类目标的检测, 利用特定的水下视频作为训练、测试数据集, 取得较好的分类精度和实时性, 但缺乏考虑水下机器人的实际硬件限制和水下光照环境的复杂多变性.</p>
                </div>
                <div class="p1">
                    <p id="103">通过上述分析可知, 目前对水下非限制环境中基于视觉的鱼类目标的检测主要存在如下三方面问题.1) 由于光在水体介质中存在严重的衰减和散射效应, 造成水下图像质量下降, 为水下鱼类目标的特征学习、识别理解造成严重障碍.2) 由于海底鱼类目标大多具有未知性, 鱼类目标在自动识别学习时, 已知标签的样本数量非常有限, 传统的训练学习方法不再适用.3) 水下机器人研制采用嵌入式计算机系统, 相比一般台式计算机, 硬件运算能力具有较大的限制性.因此, 基于YOLO等深度学习的地面目标实时检测算法不再适合水下机器人的嵌入式系统.</p>
                </div>
                <div class="p1">
                    <p id="104">为了克服上述问题, 本文提出基于改进YOLO和迁移学习的水下鱼类目标实时检测算法.针对YOLO网络结构进行改进, 提出适合水下机器人嵌入式系统的精简YOLO结构 (Underwater-YOLO) , 可以胜任水下鱼类目标的实时检测要求.针对网络训练时样本不足的问题, 采用迁移学习的方法训练Underwater-YOLO网络.针对水下海底图像散射模糊、对比度较低、光照不均问题, 提出基于限制对比度自适应直方图均衡化 (Contrast Limited Adaptive Histogram Equalization, CLAHE) 的水下图像增强预处理算法.此外, 提出基于图像结构相似度 (Structural Similarity, SSIM) 的视频帧选择性网络前向计算策略, 有效提高检测速率.</p>
                </div>
                <h3 id="105" name="105" class="anchor-tag">1 YOLO实时目标检测算法</h3>
                <div class="p1">
                    <p id="106">YOLO目标检测算法将目标检测问题转化为一个回归问题, 仅使用一个深度卷积神经网络模型进行目标检测, 能以较高的准确率实现快速目标检测与识别.</p>
                </div>
                <div class="p1">
                    <p id="107">YOLO的网络结构如图1所示.由图1可知, YOLO网络结构主要包括两部分.1) 特征提取.利用一个具有20个卷积层的基础网络提取输入图像的特征, 可从大小为448×448×3的彩色图像中提取14×14×1 024维特征.2) 回归预测.使用随机初始化权值的4层卷积进一步运算特征, 产生维度为7×7×1024维包含特征, 然后使用4 096个尺寸为7×7的卷积核将特征矩阵进行二维卷积操作, 产生4 096×1维全连接特征, 通过全连接层产生539×1维输出特征, 将输出特征重排为7×7×11维, 作为回归的输出矩阵.</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_10800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 YOLO网络结构Fig.1 Network structure of YOLO" src="Detail/GetImg?filename=images/MSSB201903001_10800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 YOLO网络结构Fig.1 Network structure of YOLO  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_10800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="109">回归输出矩阵包含目标边界框的尺寸和坐标、边界框内包含目标的概率及目标所属类别.这些信息的编码方式如图2所示.</p>
                </div>
                <div class="p1">
                    <p id="110">图2为输出矩阵第1、2维度取1时, 第3维度的编码信息, 即一个图像块的编码信息.目标检测结果以边界框选中目标的方式呈现, 其中, x、y为边界框中心点相对于网格左上方的偏移量, w、h为边界框的宽度和高度.C为边界框内包含目标的置信度, 编码结果给出两个边界框的预测值, 根据置信度C选择合理的边界框.P (c) 为被检测目标属于某一类别的概率, 为目标检测结果确定类别.</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_11100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 YOLO输出矩阵信息编码Fig.2 Coding mode of output matrix information of YOLO" src="Detail/GetImg?filename=images/MSSB201903001_11100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 YOLO输出矩阵信息编码Fig.2 Coding mode of output matrix information of YOLO  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_11100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="112">将训练样本的标注信息处理成为输出矩阵的编码顺序, 可以建立理想输出与实际输出的损失函数.</p>
                </div>
                <div class="p1">
                    <p id="113">坐标预测的损失函数如下:</p>
                </div>
                <div class="area_img" id="114">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_11400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="115">预测网格框包含目标时的损失函数:</p>
                </div>
                <div class="area_img" id="116">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_11600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="117">预测网格框不包含目标时的目标函数:</p>
                </div>
                <div class="area_img" id="118">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_11800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="119">预测目标类别的损失函数:</p>
                </div>
                <div class="area_img" id="120">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_12000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="121">综合部分的损失函数求得总损失函数:</p>
                </div>
                <div class="area_img" id="122">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_12200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="123">以损失函数作为卷积神经网络的优化准则, 利用随机梯度下降法进行优化, 即可完成YOLO网络的训练.</p>
                </div>
                <h3 id="124" name="124" class="anchor-tag">2 基于改进YOLO的水下鱼类目标检测算法</h3>
                <div class="p1">
                    <p id="126">YOLO目标检测算法不能直接应用于水下图像鱼类目标的检测, 主要原因体现在如下3方面.</p>
                </div>
                <div class="p1">
                    <p id="127">1) 在结构上, YOLO网络结构过于复杂, 前向推理计算时需要较高的计算能力.而水下机器人搭载的嵌入式处理系统计算能力有限, 所以无法在嵌入式系统上运行YOLO.此外, 由于YOLO仅在单一尺度下进行目标检测, 难以检测位置重叠、体态较小的鱼类目标.</p>
                </div>
                <div class="p1">
                    <p id="128">2) 海底鱼类样本相对匮乏, 已知标签的样本数量非常有限, 传统训练方法无法有效充分训练泛化性能较高的深度神经网络, 必须根据水下图像的特点提出能使用小样本充分训练网络的方法.</p>
                </div>
                <div class="p1">
                    <p id="129">3) 由于海底视频图像呈现对比度较低、纹理细节模糊及光照不均匀等现象, 所以YOLO无法适应海底观测视频图像.</p>
                </div>
                <div class="p1">
                    <p id="130">为了克服上述问题, 本文提出基于改进YOLO和迁移学习的鱼类目标实时检测算法, 算法总体框架如图3所示.</p>
                </div>
                <div class="area_img" id="131">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_13100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 本文算法总体框架Fig.3 Framework of the proposed method." src="Detail/GetImg?filename=images/MSSB201903001_13100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 本文算法总体框架Fig.3 Framework of the proposed method.  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_13100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="132">本文算法主要包括如下4个模块:UnderwaterYOLO网络结构设计、Underwater-YOLO网络的迁移学习、基于CLAHE的水下图像增强预处理、基于SSIM的视频帧选择性前向推理计算.下面具体介绍各模块的实现过程.</p>
                </div>
                <h4 class="anchor-tag" id="133" name="133">2.1 Underwater-YOLO网络结构设计</h4>
                <div class="p1">
                    <p id="134">为了既能适合水下机器人的嵌入式系统的计算能力, 又能实现对水下鱼类小目标、重叠目标的准确检测, 本文设计适合水下机器人进行鱼类目标检测的网络结构 (Underwater-YOLO) , 如图4所示.</p>
                </div>
                <div class="p1">
                    <p id="135">由图4可知, Underwater-YOLO主要包括特征提取和预测回归两部分.特征提取部分包括7个卷积层, 每个卷积层框图中的数据分别表示卷积核数量、卷积核尺寸、卷积步长 (省略者默认步长为1) Underwater-YOLO的特征提取部分仅使用7层卷积, 并以步长为2的卷积运算进行下采样, 如此设计的目的是为了减少水下机器人嵌入式系统进行目标检测时的运算量.而在回归预测部分, 本文舍弃全连接网络结构, 整个网络使用全卷积神经网络 (Fully Convolutional Network, FCN) 的结构, 克服在回归过程中将空间特征转化为二维特征丢失空间信息的弊端<citation id="244" type="reference"><link href="85" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>.</p>
                </div>
                <div class="area_img" id="136">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_13600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Underwater-YOLO的网络结构Fig.4 Network structure of Underwater-YOLO" src="Detail/GetImg?filename=images/MSSB201903001_13600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 Underwater-YOLO的网络结构Fig.4 Network structure of Underwater-YOLO  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_13600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="137">为了实现水下鱼类小目标的检测, 在回归预测部分采用特征拼接实现多尺度预测方式.对于深层特征, 由于下采样使特征尺寸减小, 导致越深层次的特征尺寸越小, 因此深层特征中小目标的信息会因为下采样而丢失.为了解决仅使用深层特征进行目标检测导致微小目标被忽略的缺陷, 采用多尺度的检测方式, 即联合浅层特征和深层特征, 提高小目标的检测率.图像特征在Underwater-YOLO网络结构中的传递流程如图5所示, 该过程可直观理解为多尺度回归.图中每个子框中的数据分别表示为 (特征宽度, 特征高度, 特征通道数) .</p>
                </div>
                <div class="p1">
                    <p id="138">从图5可看出, 输入图像在Underwater-YOLO的特征提取部分逐层提炼, 形成尺寸小、维数大的深层抽象特征.深层特征在回归预测部分继续进行卷积操作, 最终产生粗尺度预测矩阵.由于特征提取部分伴随下采样, 一些特征的细节信息丢失.在预测回归阶段, 将深层特征进行上采样, 使之与浅层特征尺寸匹配, 然后拼接不同层的尺寸, 生成融合特征.使用融合特征继续进行回归预测, 最终产生细尺度预测矩阵.</p>
                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_13900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 Underwater-YOLO的特征流程图Fig.5 Feature flow chart of underwater-YOLO" src="Detail/GetImg?filename=images/MSSB201903001_13900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 Underwater-YOLO的特征流程图Fig.5 Feature flow chart of underwater-YOLO  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_13900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="140">Underwater-YOLO的输入为3个通道的彩色图像, 通过Underwater-YOLO网络后产生13×13×12和26×26×12两个尺度的预测矩阵.其中13×13×12的预测矩阵编码将图像划分为13×13个网格, 每个网格内包含鱼类目标的置信度及边界框的坐标值.而26×26×12的预测矩阵编码将图像划分为26×26个网格, 每个网格内包含鱼类目标的置信度和边界框的坐标信息.</p>
                </div>
                <div class="p1">
                    <p id="141">对于重叠目标的检测问题, YOLO在图像的每个子块中最多只能检测到一个目标, 若在一个子块中同时出现多个目标, 会造成重叠目标的漏检.为此, 对输出的预测结果矩阵进行重新编码.借鉴Faster R-CNN中锚点的思想, 为一个图像子块设置多个锚点, 每个锚点可编码目标的边界框的坐标值、存在目标的置信度及类别.由于锚点被初始化为不同尺寸, 发生重叠的目标可以被不同的锚点捕获.预测矩阵中信息的编码方式如图6所示.</p>
                </div>
                <div class="area_img" id="142">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 Underwater-YOLO预测矩阵编码方式Fig.6Encoding mode of prediction matrix information of Underwater-YOLO" src="Detail/GetImg?filename=images/MSSB201903001_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 Underwater-YOLO预测矩阵编码方式Fig.6Encoding mode of prediction matrix information of Underwater-YOLO  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="143">2个预测矩阵的第3维长度一致, 编码相同的信息.t<sub>x</sub>、t<sub>y</sub>为边界框偏离图像子块左上顶点的位置信息, t<sub>w</sub>、t<sub>h</sub>为确定边界框宽度和高度的参数.上述4个参数可解决边界框在图像中的定位问题.C为边界框中是否包含目标的置信度, P (C) 为边界框内目标的类别判断, 判断目标属于鱼类的概率.上述所有参数可使用一个锚点进行描述, 通过引入锚点, 增加目标检测的预设候选区域, 有利于解决重叠目标的漏检问题.图像中图像块、锚点与边界框分布及尺寸关系如图7所示.</p>
                </div>
                <div class="area_img" id="144">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_14400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 锚点框与边界框的位置关系Fig.7Positional relationship between anchor box and bounding box" src="Detail/GetImg?filename=images/MSSB201903001_14400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 锚点框与边界框的位置关系Fig.7Positional relationship between anchor box and bounding box  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_14400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="145">图7中c<sub>x</sub>、c<sub>y</sub>为图像子块对于图像左上角的偏移量, σ (t<sub>x</sub>) 、σ (t<sub>y</sub>) 为边界框中心点相对于图像子块的偏移量按照图像子块的尺寸归一化的结果, p<sub>w</sub>、p<sub>h</sub>为锚点框的宽度和高度, (x, y) 为边界框的中心点坐标, w、h为边界框的宽度和高度.由x、y、w、h可确定边界框在图中的位置, 可通过预测矩阵中的偏移参数计算求得:</p>
                </div>
                <div class="area_img" id="146">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_14600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="147">锚点框的初始尺寸与指数函数相乘求得边界框的尺寸, 当为锚点指定不同的初始尺寸时会获得不同尺寸的边界框.因此, 可通过设置多个不同尺寸的锚点框的方式改善目标检测的图像子块中检测多个目标的效果.</p>
                </div>
                <div class="p1">
                    <p id="148">通过使用多尺度预测与多锚点的编码方式, 一幅图像能产生更多的待检测区域, 在该网络结构中共产生</p>
                </div>
                <div class="area_img" id="149">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_14900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="150">检测区域, 包含4种不同尺寸的锚点框.通过这一改进能极大增加小目标检测的召回率, 并且能检测重叠的目标.</p>
                </div>
                <h4 class="anchor-tag" id="151" name="151">2.2 Underwater-YOLO的迁移学习</h4>
                <div class="p1">
                    <p id="152">为了训练Underwater-YOLO, 通过采集水下鱼类图像, 以人工标注的方式建立水下鱼类目标检测数据集.数据集共有1 500幅ROV在海底环境下的实拍图像, 其中500幅图像挑选自labeled fishes in the wild数据集<citation id="245" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 1 000幅图像来源于实拍水下视频中的关键帧.将图像数据集样本分为训练集和验证集两部分, 训练集共有1 000个样本, 验证集共有500个样本.</p>
                </div>
                <div class="p1">
                    <p id="153">由于已标注的水下鱼类检测数据较少, 直接用于训练Underwater-YOLO难以获得具有泛化性的训练结果, 特别是特征提取部分的卷积层需要充分训练, 才能从图像提取出具有鉴别性的特征.为此, 本文采用迁移学习<citation id="246" type="reference"><link href="87" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>的方法训练卷积神经网络, Yosinski等<citation id="247" type="reference"><link href="89" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>论证在迁移学习中特征迁移的有效性.在迁移学习中, 将本文标注的1 000幅水下图像样本集作为目标域, 将ImageNet分类数据集<citation id="248" type="reference"><link href="91" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>作为源域, 使用微调 (Finetune) 的方式迁移学习通过分类网络训练的模型.</p>
                </div>
                <div class="p1">
                    <p id="154">本文的迁移学习分为两部分:分类网络训练;将分类网络的知识迁移到检测模型, 在此基础上进行Underwater-YOLO的训练.</p>
                </div>
                <div class="p1">
                    <p id="155">对于分类网络部分, 首先根据Underwater-YOLO网络结构建立分类网络结构.如图8所示, 以Underwater-YOLO特征提取部分的卷积层为基础, 通过添加一层卷积核数量为1 000的13×13的卷积层, 将特征矩阵转化为1 000×1维特征向量, 然后再加一层softmax层将特征向量转化为概率输出.</p>
                </div>
                <div class="area_img" id="156">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_15600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 Underwater-YOLO的知识迁移过程Fig.8 Knowledge transferring of Underwater-YOLO" src="Detail/GetImg?filename=images/MSSB201903001_15600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 Underwater-YOLO的知识迁移过程Fig.8 Knowledge transferring of Underwater-YOLO  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_15600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="157">建立分类网络后, 使用ImageNet数据集进行训练.网络训练超参数选择如下:批次大小 (Batch Size) 为64, 输入图像高度为416, 输入图像宽度为416, 动量为0.9, 权值衰减为0.000 1, 最大迭代次数为100 000.学习率调整策略如下:第1～10 000次迭代设置为0.1, 第10 000～80 000次迭代设置为0.01, 第80 000～100 000次迭代设置为0.001, 微调, 由此减小权值在极小值附近的震荡.</p>
                </div>
                <div class="p1">
                    <p id="158">训练完分类网络后, 可将分类网络训练的知识迁移到Underwater-YOLO特征提取部分, 迁移过程如图8所示.</p>
                </div>
                <div class="p1">
                    <p id="159">使用ImageNet数据集训练完分类网络后, 卷积核的权值经过充分训练, 能提取泛化特征, 这些卷积核就可作为分类网络学习到的知识.将这些知识转移到Underwater-YOLO的特征提取部分, 设置特征提取部分的学习率为0, 锁定特征提取部分的卷积核.回归预测部分的卷积核通过随机初始化的方式赋值, 然后使用水下图像样本集训练回归预测部分的卷积核.</p>
                </div>
                <div class="p1">
                    <p id="160">由于水下图像训练集数量较少, 本文提出结合限制对比度自适应直方图均衡化 (CLAHE) <citation id="249" type="reference"><link href="91" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>的迁移学习训练方法.在训练回归检测网络阶段, 同时使用数据扩增的手段, 使小样本训练集发挥最大效果.采用的数据扩增手段包括:1) 随机旋转图像.将原始图像随机旋转-15°～15°.2) 调整曝光度和饱和度.将图像从RGB颜色空间转化到HSV空间, 对曝光度和饱和度随机调整至原先的1/1.5～1.5倍.3) 随机调整亮度分量, 使亮度增加或减少原先的-0.1～0.1倍.4) 随机调节限制对比度自适应直方图均衡化的裁剪阈值, 获得不同直方图分布规律的训练样本, 提升训练结果的泛化性.</p>
                </div>
                <div class="p1">
                    <p id="161">此外, 由于锚点框的尺寸也会影响目标检测的效果, 若能为锚点框初始化合理的尺寸, 则能有效提高目标检测的指标.本文使用聚类方法, 以训练样本中已标注的边界框的尺寸为参考, 确定锚点框的尺寸.选择k个聚类中心, 以边界框和聚类尺寸的IOU指标确定距离函数:</p>
                </div>
                <div class="area_img" id="162">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_16200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="163">以该距离函数作为优化准则进行聚类, 针对水下鱼类目标检测数据集, 获得与训练样本中边界框重合率最大的锚点尺寸.按照聚类结果初始化锚点尺寸, 提高预测边界框与实际目标的重合率.</p>
                </div>
                <h4 class="anchor-tag" id="164" name="164">2.3 基于CLAHE的水下图像增强预处理</h4>
                <div class="p1">
                    <p id="165">对于水下图像, 由于光线的严重衰减, 图像整体对比度较低, 噪声较大, 存在光照不均匀的现象.为此, 本文采用限制对比度直方图均衡化 (CLAHE) 方法进行水下图像的增强处理.</p>
                </div>
                <div class="p1">
                    <p id="166">限制对比度自适应直方图均衡化图像增强预处理算法步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="167">算法CLAHE</p>
                </div>
                <div class="p1">
                    <p id="168">step 1将输入图像从RGB空间转换到HSV颜色空间.</p>
                </div>
                <div class="p1">
                    <p id="169">step 2将亮度分量V图像划分为k个大小为m×n的子块, 每个子块相互连续, 互不重叠.</p>
                </div>
                <div class="p1">
                    <p id="170">step 3计算子区域中每个灰度级可以平均分配到的像素个数</p>
                </div>
                <div class="area_img" id="171">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_17100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="172">其中, N<sub>gray</sub>为子区域中灰度级的数量, N<sub>CR-x</sub>为子区域x轴方向的像素数, N<sub>CR-y</sub>为y轴方向的像素数.限制每个灰度级包含的像素数不超过平均值N<sub>ave</sub>的N<sub>clip</sub>倍, 则实际剪切极限值</p>
                </div>
                <div class="area_img" id="173">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_17300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="174">其中, N<sub>clip</sub>为截取限制系数, 含义是限制子区域每个灰度级包含的像素数不超过平均像素数的N<sub>clip</sub>倍.</p>
                </div>
                <div class="p1">
                    <p id="175">step 4对每个子块, 按照裁剪极限值N<sub>CL</sub>剪切灰度直方图, 多余的像素数量重新分配到各个灰度级中, 设已被剪切的像素总数为N<sub>sum＿clip</sub>, 可得每个灰度级均分的剪切像素数N<sub>add</sub>, </p>
                </div>
                <div class="area_img" id="176">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_17600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="177">重新分配的过程如下:</p>
                </div>
                <div class="area_img" id="178">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_17800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="179">其中H (i) 为原始区域中第i个灰度级的像素数.</p>
                </div>
                <div class="p1">
                    <p id="180">step 5重复step 3的分配过程, 剩余的像素数为N<sub>sum＿clip</sub>≤N<sub>gray</sub>, 被分配像素步长值</p>
                </div>
                <div class="area_img" id="181">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_18100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="182">平均分配到每个灰度级.</p>
                </div>
                <div class="p1">
                    <p id="183">step 6对每个子块剪切后的灰度直方图进行直方图均衡化.</p>
                </div>
                <div class="p1">
                    <p id="184">step 7使用双线性插值的方法去除图像分块处理后产生的块状效应.</p>
                </div>
                <div class="p1">
                    <p id="185">step 8将增强预处理后的V分量图像, 和原先的H、S分量一起转换到RGB颜色空间, 得到增强后水下图像.</p>
                </div>
                <h4 class="anchor-tag" id="186" name="186">2.4 基于结构相似度的视频帧选择性前向推理计算</h4>
                <div class="p1">
                    <p id="188">在利用水下机器人视觉系统进行水下鱼类目标检测时, 若利用Underwater-YOLO网络对视频的每帧都进行前向计算检测, 由于帧速一般为30帧/秒, 则水下机器人的嵌入式系统的计算能力难以承受.考虑到视频相邻帧间存在大量信息冗余, 本文提出基于帧间图像结构相似度 (SSIM) 的视频帧选择方法, 即只对关键帧进行Underwater-YOLO网络的前向检测计算, 相似的多余帧不需要重复进行前向检测计算.</p>
                </div>
                <div class="p1">
                    <p id="189">SSIM为一种基于结构信息衡量图像相似程度的评价准则.结构相似度将图像的相似度建模为亮度 (l) 、对比度 (c) 和结构 (s) 的组合指标.令x、y表示相邻的帧间图像, 两者之间的亮度相似指标使用均值 (μ<sub>x</sub>, μ<sub>y</sub>) 进行估计, 对比度相似指标使用标准差 (σ<sub>x</sub>, σ<sub>y</sub>) 进行估计, 结构相似指标使用协方差 (σ<sub>xy</sub>) 进行估计.</p>
                </div>
                <div class="p1">
                    <p id="190">SSIM可使用上述3个指标的组合, 定义如下:</p>
                </div>
                <div class="area_img" id="191">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_19100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="192">其中</p>
                </div>
                <div class="area_img" id="193">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_19300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="194">α≥0, β≥0, γ≥0, 分别用于调节亮度、对比度、结构相似度的权重.当取α=β=λ=1, C<sub>3</sub>=C<sub>2</sub>/2, 可得</p>
                </div>
                <div class="area_img" id="195">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903001_19500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="196">通过计算视频相邻帧之间的结构相似度, 可判断视频内容的变化幅度, 当画面几乎不发生变化时, 相邻帧保持较高的结构相似度, 即SSIM (x, y) ≈1, 此时视频内容基本固定, 不需要进行深度学习的前向推理.当画面变化累积到一定程度, 视频帧的结构相似度变小, 此时需要进行深度学习前向推理, 更新目标检测的结果.</p>
                </div>
                <div class="p1">
                    <p id="197">视频帧选择性前向推理计算过程如图9所示.</p>
                </div>
                <div class="area_img" id="198">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_19800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 视频帧选择性前向推理计算过程Fig.9 Forward inference computation based on key frame selection" src="Detail/GetImg?filename=images/MSSB201903001_19800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 视频帧选择性前向推理计算过程Fig.9 Forward inference computation based on key frame selection  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_19800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="199">以视频的第一帧图像进行卷积神经网络的前向运算并作为参考帧, 与视频的下一帧进行对比, 计算两帧之间的结构相似度.设定结构相似度阈值, 若两帧结构相似度大于阈值, 不进行前向推理, 直接输出图像并附加前一帧的检测结果.若两帧的结构相似度小于等于规定的阈值, 对当前帧进行UnderwaterYOLO网络的前向推理, 并将当前帧赋值为新的参考帧.</p>
                </div>
                <h3 id="200" name="200" class="anchor-tag">3 实验及结果分析</h3>
                <div class="p1">
                    <p id="201">为了验证Underwater-YOLO网络的有效性及基于Underwater-YOLO网络进行水下鱼类目标检测的可行性, 将YOLO网络和Underwater-YOLO应用于水下鱼类检测.实验采用的数据集为手工标注的ROV实拍水下图像.服务器的配置参数如下:intel i5CPU、64 GB内存、GTX1080ti GPU (11 GB显存) 、Ubuntu16.4操作系统.</p>
                </div>
                <div class="p1">
                    <p id="202">实验主要分为3部分:1) 测试Underwater-YOLO与YOLO基础网络进行鱼类目标检测的性能, 验证Underwater-YOLO网络结构的有效性.2) 使用结合限制对比度自适应直方图均衡化的迁移学习方法及锚点维度聚类的方法训练深度神经网络, 讨论本文提出的训练策略的有效性.3) 使用本文提出的基于SSIM的选择性前向推理的方法进行视频目标检测, 验证选择性前向推理方法在实时鱼类检测过程中的有效性.</p>
                </div>
                <h4 class="anchor-tag" id="203" name="203">3.1 Underwater-YOLO检测结果</h4>
                <div class="p1">
                    <p id="204">本部分实验旨在验证Underwater-YOLO网络结构的有效性, 因此在训练阶段仅采用迁移学习加数据扩增的策略以相同的超参数训练YOLO、YOLOv2、YOLOv3及Underwater-YOLO网络.将500幅测试图像输入训练好的网络进行鱼类目标的位置回归.当神经网络预测的目标边界框与手工标注的边界框交并化 (Intersection Over Union, IOU) 大于等于0.5时, 认为成功检测目标;否则, 目标漏检.选择准确率 (P) 、召回率 (R) 、平均交并比 (Mean Intersection over Union, m IOU) 、每秒检测帧数 (Frames per Second, FPS) 作为评价准则.</p>
                </div>
                <div class="p1">
                    <p id="205">各种目标检测算法在水下目标检测的性能如表1所示.</p>
                </div>
                <div class="p1">
                    <p id="206">文献<citation id="250" type="reference">[<a class="sup">14</a>]</citation>为基于Haar-like特征的Adaboost水下图像鱼类检测方法, 使用相似的鱼类检测数据集, 检测准确率为66%, 对于复杂的水下环境有较差的鲁棒性, 因此使用传统方式的鱼类检测准确率受到限制.相比Adaboost目标检测方法, 基于YOLO的目标检测算法具有较好的性能指标.使用最基本的YOLO网络结构, 目标检测的准确率比Adaboost目标检测方法提高10%.而本文建立的UnderwaterYOLO在准确率和召回率上都高于YOLO基础网络, 与YOLOv2相当.由于YOLOv3具有深层的网络结构, 使用多种改进策略, 目标检测的准确率和召回率偏高.但Underwater-YOLO仅使用7层卷积神经网络进行特征提取, 大幅降低目标检测的运算量, 检测速度可达122帧/秒, 远高于YOLO、YOLOv2、YOLOv3.</p>
                </div>
                <div class="area_img" id="207">
                                            <p class="img_tit">
                                                表1 水下目标检测实验结果Table 1 Experimental results of underwater object detection
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_20700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201903001_20700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_20700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 水下目标检测实验结果Table 1 Experimental results of underwater object detection" src="Detail/GetImg?filename=images/MSSB201903001_20700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="208">使用基本的YOLO网络和Underwater-YOLO进行水下图像测试, 典型结果对比如图10所示.</p>
                </div>
                <div class="area_img" id="209">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_20900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 0 2种网络结构检测结果对比Fig.10 Comparison of detection results of 2 networks" src="Detail/GetImg?filename=images/MSSB201903001_20900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 0 2种网络结构检测结果对比Fig.10 Comparison of detection results of 2 networks  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_20900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="210">图10 (a) 为YOLO网络使用迁移学习的策略进行训练后检测鱼类的结果, (b) 为Underwater-YOLO网络经过相同的训练过程后的检测结果.对比两者可以看出, 使用YOLO网络结构进行鱼类目标检测时, 仅能检测较明显的目标, 漏检体态较小、光线较暗处的目标.而使用Underwater-YOLO网络进行目标检测时, 能够检测一些不明显的鱼类, 大幅提升目标检测的召回率.</p>
                </div>
                <div class="p1">
                    <p id="211">实验结果表明, 在水下鱼类检测中, UnderwaterYOLO能以较少的卷积层实现目标检测, 不仅具有较好的准确率和召回率, 还大幅提升目标检测的运算速度.网络中的特征融合、多尺度检测及使用锚点框编码输出信息可以有效提升网络性能.</p>
                </div>
                <h4 class="anchor-tag" id="212" name="212">3.2 训练策略对神经网络性能的影响</h4>
                <div class="p1">
                    <p id="213">在训练Underwater-YOLO过程中本文采用多种训练策略 (A+B+C) , 本节实验验证各种训练策略对目标检测性能指标的影响.对Underwater-YOLO使用不同的训练策略进行训练, 通过测试集可获得不同训练策略下目标检测性能指标, 如表2所示.表2中, A表示迁移学习, B表示基于CLAHE的图像预处理, C表示使用维度聚类方法确定锚点的初始尺寸.</p>
                </div>
                <div class="area_img" id="214">
                                            <p class="img_tit">
                                                表2 Underwater-YOLO使用不同训练策略的网络性能对比Table 2 Performance comparison of Underwater-YOLO using different training strategies
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_21400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201903001_21400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_21400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 Underwater-YOLO使用不同训练策略的网络性能对比Table 2 Performance comparison of Underwater-YOLO using different training strategies" src="Detail/GetImg?filename=images/MSSB201903001_21400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="215">由表2可得, 直接使用水下图像训练集进行网络训练, 获得的结果缺乏泛化性, 当使用测试集进行测试时, 准确率和召回率很低.策略A充分训练卷积神经网络部分的卷积核, 在测试过程中准确率得到明显提升.策略B能克服光照不均匀情况下暗区目标难以检测的缺陷, 召回率明显提升.策略C使预测边界框更接近目标位置, m IOU得到明显提升.而策略A+B为本文提出的结合CLAHE的迁移学习方法, 由表可知, 准确率和召回率都得到提升, 高于单独使用一种策略.综上所述, 综合使用A+B+C训练策略, 准确率可达到93%, 召回率可达到84%, m IOU可达到0.72.相比直接进行目标检测, 准确率提升14%, 召回率提升18%, m IOU提升14%.</p>
                </div>
                <div class="p1">
                    <p id="216">由实验结果可知, 通过使用多种策略的组合, 提升最大的性能指标是召回率.目标检测中的召回率是指被正确检测的鱼类目标占图像中所有应被检测出鱼类的比率.若直接训练Underwater-YOLO而不使用本文的改进措施, 召回率仅有66%, 这意味着有大量的目标被漏检.而引入本文的改进策略, 召回率提升至84%.</p>
                </div>
                <div class="p1">
                    <p id="217">图11为使用普通迁移学习训练UnderwaterYOLO与使用本文训练策略进行网络训练的典型检测结果对比.</p>
                </div>
                <div class="area_img" id="218">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_21800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 1 2种训练策略检测结果对比Fig.11 Comparison of detection results using different training strategies" src="Detail/GetImg?filename=images/MSSB201903001_21800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 1 2种训练策略检测结果对比Fig.11 Comparison of detection results using different training strategies  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_21800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="219">图11 (a) 为Underwater-YOLO网络使用普通迁移学习策略训练后的检测结果, (b) 为UnderwaterYOLO网络使用本文的结合CLAHE的迁移学习的策略及使用锚点框维度聚类方法进行训练的检测结果.由检测结果可看出, 两者都具有检测重叠目标的能力, 这是由于网络结构中使用锚点的编码方式与多尺度预测的结果.但 (a) 中仍存在目标漏检的情况, 漏检的目标使用椭圆边框标注.上方未被检测的原因是鱼类目标体态特殊, 目标较小;下方未被检测的原因是目标发生严重重叠, 不易观察.使用本文的训练策略, 由 (b) 的结果可看出, 本文方法可克服在鱼类体态不正常和严重重叠状态下目标漏检的问题, 正如表2所示, 由于漏检率下降从而有效提升本文方法的召回率.</p>
                </div>
                <div class="p1">
                    <p id="220">综上所述, 迁移学习可提升目标检测的准确率, 基于CLAHE的图像预处理可提升目标检测的召回率, 基于维度聚类的锚点框初始化可提升目标检测的m IOU及召回率, 综合各种策略可发挥其优势, 提高整体性能指标.</p>
                </div>
                <h4 class="anchor-tag" id="221" name="221">3.3 选择性前向推理对目标检测速度的影响</h4>
                <div class="p1">
                    <p id="222">为了验证基于SSIM的选择性前向推理视频加速检测的有效性, 在GTX1080ti GPU上统计视频目标检测的平均速度, 结果如表3所示.</p>
                </div>
                <div class="p1">
                    <p id="223">由表3可知, 选择性前向推理的视频加速算法在GPU平台上检测速度有所提升, 但提升幅度并不理想, 主要原因是在GPU上运行卷积神经网络单次前向推理的耗时较少, 并且引入SSIM的计算过程仍由CPU完成, 检测速度难以得到很大提升.</p>
                </div>
                <div class="p1">
                    <p id="224">然而水下机器人难以配备高能耗的GPU, 主要运算平台为嵌入式处理器.由于电脑CPU与嵌入式处理器的性能相当, 本文统计在Intel i5CPU平台上选择性前向推理视频检测的处理速度, 平均处理速度如表4所示.</p>
                </div>
                <div class="area_img" id="225">
                                            <p class="img_tit">
                                                表3 在GTX1080ti GPU平台上视频目标检测处理速度Table 3 Speed of video object detection on GTX1080ti GPU platform
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_22500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201903001_22500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">帧/秒</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_22500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 在GTX1080ti GPU平台上视频目标检测处理速度Table 3 Speed of video object detection on GTX1080ti GPU platform" src="Detail/GetImg?filename=images/MSSB201903001_22500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="226">
                                            <p class="img_tit">
                                                表4 在Intel i5CPU平台上视频目标检测处理速度Table 4 Speed of video object detection on Intel i5 CPU platform
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903001_22600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201903001_22600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">帧/秒</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903001_22600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表4 在Intel i5CPU平台上视频目标检测处理速度Table 4 Speed of video object detection on Intel i5 CPU platform" src="Detail/GetImg?filename=images/MSSB201903001_22600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="227">由表4可得, 在CPU平台上选择性前向推理过程可显著提升目标检测的速度, 对于YOLOv3网络, 由于网络结构较复杂, 进行全帧运算仅能达到0.2帧/秒.综上所述, 通过使用选择性前向推理的视频检测方法可有效减少深度神经网络的推理次数, 提高目标检测效率.Underwater-YOLO网络通过使用选择性前向推理视频加速方法, 可将目标检测的速度提升到28帧/秒, 实现在CPU平台上的实时检测.</p>
                </div>
                <h3 id="228" name="228" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="229">本文利用深度学习的方法, 提出基于改进YOLO与迁移学习的水下鱼类目标实时检测算法.构建Underwater-YOLO网络结构, 使用特征融合的方法进行多尺度的目标检测, 可检测出位置重叠和体态较小的鱼类目标.在算法的实时性方面, 通过减少网络结构的卷积层数与卷积核的数量, 并且通过计算帧间结构相似度, 选择性地更新深度网络的前向推理, 实现在水下机器人嵌入式系统上的水下鱼类目标实时检测.提出的迁移学习方法能够获得训练出具有较强泛化性能的网络模型.提出的基于限制对比度自适应直方图均衡化预处理算法可以去除水下图像的散射模糊现象, 并且克服光照不均匀问题.实验表明, 在水下视频测试集上, 鱼类目标检测速度在CPU平台上达28帧/秒, 准确率达93%, 召回率达84%.深海生物资源的识别与统计是一个迫切需要解决的问题, 通过应用本文的水下鱼类目标实时检测算法, 可以使水下机器人更好地服务于深海生物资源的探索与开发.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="2" type="formula" href="images/MSSB201903001_00200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">李庆忠</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="4" type="formula" href="images/MSSB201903001_00400.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">李宜兵</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="6" type="formula" href="images/MSSB201903001_00600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">牛炯</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="51">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1EB412C753E00679AC6EE0E85B6F7B46&amp;v=MDQ3MTdCOWltVGtJUFgrWHBCZEhmOFNUTjc2WkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHdMcTZ4S3c9TmlmT2ZiTE5iTlhOcmZ4Q1llaDZESHcveQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>QIAO X, BAO J H, ZENG L H, et al.An Automatic Active Contour Method for Sea Cucumber Segmentation in Natural Underwater Environments.Computers and Electronics in Agriculture, 2017, 135:134-142.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJEG&amp;filename=SJEG5F9C44723FF5F7DDE3141B10FC9E8F19&amp;v=MjE2MDJmQ3BiUTM1TkZod0xxNnhLdz1OaWZPYWJiT0Y2TElxNGhIWjUxNUNRbyt1MkptNlQ1NVNRM2pyR1JHY01lY003dVdDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>LI Q Z, ZHANG Y, ZANG F N.Fast Multicamera Video Stitching for Underwater Wide Field-of-View Observation.Journal of Electronic Imaging, 2014, 23 (2) :367-368.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122200114518&amp;v=MTg1MzNSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGd1JiaFk9TmlmT2ZiSzlIOVBPclk5Rlplb0xDWDB4b0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>BONIN-FONT F, OLIVER G, WIRTH S, et al.Visual Sensing for Autonomous Underwater Exploration and Intervention Tasks.Ocean Engineering, 2015, 93 (1) :25-44.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Image Representations for Coral Image Classification">

                                <b>[4]</b>MAHMOOD A, BENNAMOUN M, AN S J, et al.Deep Image Representations for Coral Image Classification.IEEE Journal of Oceanic Engineering, 2019, 44 (1) :121-131.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[5]</b>GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:580-587.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast r-cnn">

                                <b>[6]</b>GIRSHICK R.Fast R-CNN//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:1440-1448.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">

                                <b>[7]</b>REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks.IEEETransactions on Pattern Analysis and Machine Intelligence, 2017, 6 (1) :1137-1149.
                            </a>
                        </p>
                        <p id="65">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You only lookonce:unified,real-time object detection">

                                <b>[8]</b>REDMON J, DIVVALA S, GIRSHICK R, et al.You Only Look Once:Unified, Real-Time Object Detection//Proc of the IEEEConference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:779-788.
                            </a>
                        </p>
                        <p id="67">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better,Faster,Stronger">

                                <b>[9]</b>REDMON J, FARHADI A.YOLO9000:Better, Faster, Stronger//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:6517-6525.
                            </a>
                        </p>
                        <p id="69">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLOv3:An Incremental Improvement[C/OL]">

                                <b>[10]</b>REDMON J, FARHADI A.Yolov3:An Incremental Improvement[C/OL].[2018-10-11].https://arxiv.org/pdf/1804.02767.pdf.
                            </a>
                        </p>
                        <p id="71">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Microsoft COCO:common objects in context">

                                <b>[11]</b>LIN T Y, MAIRE M, BELONGIE S, et al.Microsoft COCO:Common Objects in Context//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2014:740-755.
                            </a>
                        </p>
                        <p id="73">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ssd:Single shot multibox detector">

                                <b>[12]</b>LIU W, ANGUELOV D, ERHAN D, et al.SSD:Single Shot Multibox Detector//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:21-37.
                            </a>
                        </p>
                        <p id="75">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600536416&amp;v=MDA3NDBKRndSYmhZPU5pZk9mYks4SHRETXFZOUZZZWdKQ0gwL29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>HSIAO Y H, CHEN C C, LIN S I, et al.Real-World Underwater Fish Recognition and Identification, Using Sparse Representation.Ecological Informatics, 2014, 23:13-21.
                            </a>
                        </p>
                        <p id="77">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automated detection of rockfish in unconstrained underwater videos using Haar cascades and a new image dataset:Labeled fishes in the wild">

                                <b>[14]</b>CUTTER G, STIERHOFF K, ZENG J M.Automated Detection of Rockfish in Unconstrained Underwater Videos Using Haar Cascades and a New Image Dataset:Labeled Fishes in the Wild//Proc of the IEEE Winter Applications and Computer Vision Workshops.Washington, USA:IEEE, 2015:57-62.
                            </a>
                        </p>
                        <p id="79">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive Foreground Extraction for Deep Fish Classification">

                                <b>[15]</b>SEESE N, MYERS A, SMITH K, et al.Adaptive Foreground Extraction for Deep Fish Classification//Proc of the 2nd Workshop on Computer Vision for Analysis of Underwater Imagery.Washington, USA:IEEE, 2016:19-24.
                            </a>
                        </p>
                        <p id="81">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep But Lightweight Neural Networks for Fish Detection">

                                <b>[16]</b>LI X, TANG Y H, GAO T W.Deep But Lightweight Neural Networks for Fish Detection//Proc of the OCEANS 2017.Washington, USA:IEEE, 2017.DOI:10.1109/OCEANSE.2017.8084961.
                            </a>
                        </p>
                        <p id="83">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vision Based Real-Time Fish Detection Using Convolutional Neural Network">

                                <b>[17]</b>SUNG M, YU S C, GIRDHAR Y.Vision Based Real-Time Fish Detection Using Convolutional Neural Network//Proc of the OCEANS 2017.Washington, USA:IEEE, 2017.DOI:10.1109/OCEANSE.2017.8084889.
                            </a>
                        </p>
                        <p id="85">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[18]</b>LONG J, SHELHAMER E, DARRELL T.Fully Convolutional Networks for Semantic Segmentation//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:3431-3440.
                            </a>
                        </p>
                        <p id="87">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Survey on Transfer Learning">

                                <b>[19]</b>PAN S J, YANG Q.A Survey on Transfer Learning.IEEE Transactions on Knowledge and Data Engineering, 2010, 22 (10) :1345-1359.
                            </a>
                        </p>
                        <p id="89">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=How Transferable are Features in Deep Neural Networks?">

                                <b>[20]</b>YOSINSKI J, CLUNE J, BENGIO Y, et al.How Transferable Are Features in Deep Neural Networks?//Proc of the 27th International Conference on Neural Information Processing Systems.Cambridge, USA:The MIT Press, 2014:3320-3328.
                            </a>
                        </p>
                        <p id="91">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet:Large scale visual recognition challenge">

                                <b>[21]</b>RUSSAKOVSKY O, DENG J, SU H, et al.Imagenet Large Scale Visual Recognition Challenge.International Journal of Computer Vision, 2015, 115 (3) :211-252.
                            </a>
                        </p>
                        <p id="93">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accelerating Fish Detection and Recognition by Sharing CNNs with Objectness Learning">

                                <b>[22]</b>LI X, SHANG M, HAO J, et al.Accelerating Fish Detection and Recognition by Sharing CNNs with Objectness Learning//Proc of the OCEANS 2016.Washington, USA:IEEE, 2016.DOI:10.1109/OCEANSAP.2016.7485476.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201903001" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201903001&amp;v=MDA0NjhaZVJuRnl6blZydk1LRDdZYkxHNEg5ak1ySTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
