<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131460956905000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dMSSB201910002%26RESULT%3d1%26SIGN%3dTRSAIDB6fbRYnnd9ItlqsP56Mtc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201910002&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201910002&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201910002&amp;v=MTUzNDBGckNVUkxPZVplUm5GeS9rV3IvTUtEN1liTEc0SDlqTnI0OUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#270" data-title="1 3D多尺度特征融合残差网络的高光谱图像分类方法 ">1 3D多尺度特征融合残差网络的高光谱图像分类方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#271" data-title="&lt;b&gt;1.1 3D&lt;/b&gt;卷积算法"><b>1.1 3D</b>卷积算法</a></li>
                                                <li><a href="#277" data-title="&lt;b&gt;1.2&lt;/b&gt; 多尺度网络"><b>1.2</b> 多尺度网络</a></li>
                                                <li><a href="#280" data-title="&lt;b&gt;1.3 3D&lt;/b&gt;多尺度特征融合残差网络"><b>1.3 3D</b>多尺度特征融合残差网络</a></li>
                                                <li><a href="#311" data-title="&lt;b&gt;1.4 HSI&lt;/b&gt;的总深度学习框架"><b>1.4 HSI</b>的总深度学习框架</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#317" data-title="2 实验及结果分析 ">2 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#318" data-title="&lt;b&gt;2.1&lt;/b&gt; 实验数据集"><b>2.1</b> 实验数据集</a></li>
                                                <li><a href="#322" data-title="&lt;b&gt;2.2&lt;/b&gt; 网络设置"><b>2.2</b> 网络设置</a></li>
                                                <li><a href="#331" data-title="&lt;b&gt;2.3&lt;/b&gt; 各方法的实验结果对比"><b>2.3</b> 各方法的实验结果对比</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#351" data-title="3 结 束 语 ">3 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#273" data-title="图1 3D卷积运算示意图">图1 3D卷积运算示意图</a></li>
                                                <li><a href="#285" data-title="图2 3D多尺度特征融合残差网络流程图">图2 3D多尺度特征融合残差网络流程图</a></li>
                                                <li><a href="#299" data-title="图3 光谱多尺度特征融合残差块">图3 光谱多尺度特征融合残差块</a></li>
                                                <li><a href="#301" data-title="图4 空间多尺度特征融合残差块">图4 空间多尺度特征融合残差块</a></li>
                                                <li><a href="#313" data-title="图5 MFFRN的高光谱图像分类总流程图">图5 MFFRN的高光谱图像分类总流程图</a></li>
                                                <li><a href="#325" data-title="图6 不同数目的光谱-空间残差块的精度对比">图6 不同数目的光谱-空间残差块的精度对比</a></li>
                                                <li><a href="#395" data-title="图7 不同训练集下的精度对比">图7 不同训练集下的精度对比</a></li>
                                                <li><a href="#330" data-title="&lt;b&gt;表1 不同空间尺寸下的精度对比&lt;/b&gt;"><b>表1 不同空间尺寸下的精度对比</b></a></li>
                                                <li><a href="#334" data-title="&lt;b&gt;表2 各方法在IN数据集上的测试精度对比&lt;/b&gt;"><b>表2 各方法在IN数据集上的测试精度对比</b></a></li>
                                                <li><a href="#335" data-title="&lt;b&gt;表3 各方法在KSC数据集上的测试精度对比&lt;/b&gt;"><b>表3 各方法在KSC数据集上的测试精度对比</b></a></li>
                                                <li><a href="#336" data-title="&lt;b&gt;表4 各方法在UP数据集上的测试精度对比&lt;/b&gt;"><b>表4 各方法在UP数据集上的测试精度对比</b></a></li>
                                                <li><a href="#396" data-title="图8 各方法在IN数据集上的视觉图对比">图8 各方法在IN数据集上的视觉图对比</a></li>
                                                <li><a href="#396" data-title="图8 各方法在IN数据集上的视觉图对比">图8 各方法在IN数据集上的视觉图对比</a></li>
                                                <li><a href="#397" data-title="图9 各方法在KSC数据集上的视觉图对比">图9 各方法在KSC数据集上的视觉图对比</a></li>
                                                <li><a href="#398" data-title="图10 各方法在UP数据集上的视觉图对比">图10 各方法在UP数据集上的视觉图对比</a></li>
                                                <li><a href="#398" data-title="图10 各方法在UP数据集上的视觉图对比">图10 各方法在UP数据集上的视觉图对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="399">


                                    <a id="bibliography_1" title="LANDGREBE D.Hyperspectral Image Data Analysis.IEEE Signal Processing Magazine,2002,19(1):17-28．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hyperspectral image data analysis">
                                        <b>[1]</b>
                                        LANDGREBE D.Hyperspectral Image Data Analysis.IEEE Signal Processing Magazine,2002,19(1):17-28．
                                    </a>
                                </li>
                                <li id="401">


                                    <a id="bibliography_2" title="BIOUCAS-DIAS J M,PLZAZ A,CAMPS-VALLS G,et al.Hyperspectral Remote Sensing Data Analysis and Future Challenges.IEEEGeoscience and Remote Sensing Magazine,2013,1(2):6-36．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hyperspectral remote sensingdata analysis and future challenges">
                                        <b>[2]</b>
                                        BIOUCAS-DIAS J M,PLZAZ A,CAMPS-VALLS G,et al.Hyperspectral Remote Sensing Data Analysis and Future Challenges.IEEEGeoscience and Remote Sensing Magazine,2013,1(2):6-36．
                                    </a>
                                </li>
                                <li id="403">


                                    <a id="bibliography_3" title="ZHANG X,SUN Y L,SHANG K,et al.Crop Classification Based on Feature Band Set Construction and Object-Oriented Approach Using Hyperspectral Images.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2016,9(9):4117-4128．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Crop classification based on feature band set construction and object-oriented approach using hyperspectral images">
                                        <b>[3]</b>
                                        ZHANG X,SUN Y L,SHANG K,et al.Crop Classification Based on Feature Band Set Construction and Object-Oriented Approach Using Hyperspectral Images.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2016,9(9):4117-4128．
                                    </a>
                                </li>
                                <li id="405">


                                    <a id="bibliography_4" title="UZKENT B,RANGNEKAR A,HOFFMAN M J.Aerial Vehicle Tracking by Adaptive Fusion of Hyperspectral Likelihood Maps//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,1998:233-242．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Aerial Vehicle Tracking by Adaptive Fusion of Hyperspectral Likelihood Maps">
                                        <b>[4]</b>
                                        UZKENT B,RANGNEKAR A,HOFFMAN M J.Aerial Vehicle Tracking by Adaptive Fusion of Hyperspectral Likelihood Maps//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,1998:233-242．
                                    </a>
                                </li>
                                <li id="407">


                                    <a id="bibliography_5" title="CARTER G A,LUCAS K L,BLOSSOM G A,et al.Remote Sensing and Mapping of Tamarisk Along the Colorado River,USA:AComparative Use of Summer-Acquired Hyperion,Thematic Mapper and Quickbird Data.Remote Sensing,2009,1(3):318-329．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Remote Sensing and Mapping of Tamarisk along the Colorado River, USA: A Comparative Use of Summer-Acquired Hyperion, Thematic Mapper and QuickBird Data">
                                        <b>[5]</b>
                                        CARTER G A,LUCAS K L,BLOSSOM G A,et al.Remote Sensing and Mapping of Tamarisk Along the Colorado River,USA:AComparative Use of Summer-Acquired Hyperion,Thematic Mapper and Quickbird Data.Remote Sensing,2009,1(3):318-329．
                                    </a>
                                </li>
                                <li id="409">


                                    <a id="bibliography_6" title="ATIYA A.Learning with Kernels:Support Vector Machines,Regularization,Optimization,and Beyond.IEEE Transactions on Neural Networks,2005,16(3).DOI:10.1109/TNN.2005.848998．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=BOOK REVIEWS - Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond">
                                        <b>[6]</b>
                                        ATIYA A.Learning with Kernels:Support Vector Machines,Regularization,Optimization,and Beyond.IEEE Transactions on Neural Networks,2005,16(3).DOI:10.1109/TNN.2005.848998．
                                    </a>
                                </li>
                                <li id="411">


                                    <a id="bibliography_7" title="曹扬，赵慧洁，黄四牛，等．基于高效置信传播的改进马尔可夫随机场高光谱数据分类算法．模式识别与人工智能，2014,27(3):248-255.(CAO Y,ZHAO H J,HUANG S N,et al.An Improved Markov Random Field Classification Approach for Hyperspectral Data Based on Efficient Belief Propagation.Pattern Recognition and Artificial Intelligence,2014,27(3):248-255．)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201403009&amp;v=MDIxMzVxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tXci9QS0Q3WWJMRzRIOVhNckk5RmJZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        曹扬，赵慧洁，黄四牛，等．基于高效置信传播的改进马尔可夫随机场高光谱数据分类算法．模式识别与人工智能，2014,27(3):248-255.(CAO Y,ZHAO H J,HUANG S N,et al.An Improved Markov Random Field Classification Approach for Hyperspectral Data Based on Efficient Belief Propagation.Pattern Recognition and Artificial Intelligence,2014,27(3):248-255．)
                                    </a>
                                </li>
                                <li id="413">


                                    <a id="bibliography_8" title="孙立新，高文．基于粗糙集的遥感优化分类波段选择．模式识别与人工智能，2000,13(2):181-186.(SUN L X,GAO W.Selecting the Optimal Classification Bands Based on Rough Sets.Pattern Recognition and Artificial Intelligence,2000,13(2):181-186．)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB200002011&amp;v=MjQwNjVMRzRIdEhNclk5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tXci9QS0Q3WWI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        孙立新，高文．基于粗糙集的遥感优化分类波段选择．模式识别与人工智能，2000,13(2):181-186.(SUN L X,GAO W.Selecting the Optimal Classification Bands Based on Rough Sets.Pattern Recognition and Artificial Intelligence,2000,13(2):181-186．)
                                    </a>
                                </li>
                                <li id="415">


                                    <a id="bibliography_9" title="CHEN Y,NASRABADI N M,TRAN T D.Hyperspectral Image Classification Using Dictionary-Based Sparse Representation.IEEETransactions on Geoscience and Remote Sensing,2011,49(10):3973-3985．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hyperspectral Image Classification Using Dictionary-Based Sparse Representation">
                                        <b>[9]</b>
                                        CHEN Y,NASRABADI N M,TRAN T D.Hyperspectral Image Classification Using Dictionary-Based Sparse Representation.IEEETransactions on Geoscience and Remote Sensing,2011,49(10):3973-3985．
                                    </a>
                                </li>
                                <li id="417">


                                    <a id="bibliography_10" title="LIU J J,WU Z B,WEI Z H,et al.Spatial-Spectral Kernel Sparse Representation for Hyperspectral Image Classification.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2013,6(6):2462-2471．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial-spectral kernel sparse representation for hyperspectral image classification">
                                        <b>[10]</b>
                                        LIU J J,WU Z B,WEI Z H,et al.Spatial-Spectral Kernel Sparse Representation for Hyperspectral Image Classification.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2013,6(6):2462-2471．
                                    </a>
                                </li>
                                <li id="419">


                                    <a id="bibliography_11" title="YUAN H L.Robust Patch-Based Sparse Representation for Hyperspectral Image Classification.International Journal of Wavelets Multiresolution and Information Processing,2017,15(3):1351-1363．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust patch-based sparse representation for hyperspectral image classification">
                                        <b>[11]</b>
                                        YUAN H L.Robust Patch-Based Sparse Representation for Hyperspectral Image Classification.International Journal of Wavelets Multiresolution and Information Processing,2017,15(3):1351-1363．
                                    </a>
                                </li>
                                <li id="421">


                                    <a id="bibliography_12" title="HER N/ANDEZ-ESPINOSA C,FERNNDEZ-REDONDO M,TORRES-SOSPEDRA J.Some Experiments with Ensembles of Neural Networks for Classification of Hyperspectral Images//Proc of the International Symposium on Neural Networks.Berlin,Germany:Springer,2004:912-917．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Some Experiments with Ensembles of Neural Networks for Classification of Hyperspectral Images">
                                        <b>[12]</b>
                                        HER N/ANDEZ-ESPINOSA C,FERNNDEZ-REDONDO M,TORRES-SOSPEDRA J.Some Experiments with Ensembles of Neural Networks for Classification of Hyperspectral Images//Proc of the International Symposium on Neural Networks.Berlin,Germany:Springer,2004:912-917．
                                    </a>
                                </li>
                                <li id="423">


                                    <a id="bibliography_13" title="SLAVKOVIKJ V,VERSTOCKT S,DE NEVE W,et al.Hyperspectral Image Classification with Convolutional Neural Networks//Proc of the 23rd ACM International Conference on Multimedia.New York,USA:ACM,2015:1159-1162．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hyperspectral image classification with convolutional neural networks">
                                        <b>[13]</b>
                                        SLAVKOVIKJ V,VERSTOCKT S,DE NEVE W,et al.Hyperspectral Image Classification with Convolutional Neural Networks//Proc of the 23rd ACM International Conference on Multimedia.New York,USA:ACM,2015:1159-1162．
                                    </a>
                                </li>
                                <li id="425">


                                    <a id="bibliography_14" title="HU W,HUANG Y Y,WEI L,et al.Deep Convolutional Neural Networks for Hyperspectral Image Classification.Journal of Sensors,2015.DOI:10.1155/2015/258619．" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJHD&amp;filename=SJHD15081800000389&amp;v=MDk1MjVySzlIdG5OcDQ5RlpPc1BEM1F3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSjE4ZGFoVT1OaWZEYQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        HU W,HUANG Y Y,WEI L,et al.Deep Convolutional Neural Networks for Hyperspectral Image Classification.Journal of Sensors,2015.DOI:10.1155/2015/258619．
                                    </a>
                                </li>
                                <li id="427">


                                    <a id="bibliography_15" title="ZHU J,FANG L Y,GHAMISI P.Deformable Convolutional Neural Networks for Hyperspectral Image Classification.IEEE Geoscience and Remote Sensing Letters,2018,15(8):1254-1258．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deformable convolutional neural networks for hyperspectral image classifica-tion">
                                        <b>[15]</b>
                                        ZHU J,FANG L Y,GHAMISI P.Deformable Convolutional Neural Networks for Hyperspectral Image Classification.IEEE Geoscience and Remote Sensing Letters,2018,15(8):1254-1258．
                                    </a>
                                </li>
                                <li id="429">


                                    <a id="bibliography_16" title="YANG X F,YE Y M,LI X T,et al.Hyperspectral Image Classification with Deep Learning Models.IEEE Transactions on Geoscience and Remote Sensing,2018,56(9):5408-5423．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hyperspectral Image Classification With Deep Learning Models">
                                        <b>[16]</b>
                                        YANG X F,YE Y M,LI X T,et al.Hyperspectral Image Classification with Deep Learning Models.IEEE Transactions on Geoscience and Remote Sensing,2018,56(9):5408-5423．
                                    </a>
                                </li>
                                <li id="431">


                                    <a id="bibliography_17" title="XU Y H,ZHANG L P,DU B,et al.Spectral-Spatial Unified Networks for Hyperspectral Image Classification.IEEE Transactions on Geoscience and Remote Sensing,2018,56(10):5893-5909．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spectralspatial unified networks for hyperspectral image classification">
                                        <b>[17]</b>
                                        XU Y H,ZHANG L P,DU B,et al.Spectral-Spatial Unified Networks for Hyperspectral Image Classification.IEEE Transactions on Geoscience and Remote Sensing,2018,56(10):5893-5909．
                                    </a>
                                </li>
                                <li id="433">


                                    <a id="bibliography_18" title="HOCHREITER S,SCHMIDHUBER J.Long Short-Term Memory.Neural Computation,1997,9(8):1735-1780．" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTk1MDJqbVVMYklKMThkYWhVPU5pZkpaYks5SHRqTXFvOUZaT29MRFhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                        HOCHREITER S,SCHMIDHUBER J.Long Short-Term Memory.Neural Computation,1997,9(8):1735-1780．
                                    </a>
                                </li>
                                <li id="435">


                                    <a id="bibliography_19" title="ZHONG Z,LI J,LUO Z M,et al.Spectral-Spatial Residual Network for Hyperspectral Image Classification:A 3-D Deep Learning Framework.IEEE Transactions on Geoscience and Remote Sensing,2018,56(2):847-858．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spectral-spatial residual network for hyperspectral image classification:a 3-D deep learning framework">
                                        <b>[19]</b>
                                        ZHONG Z,LI J,LUO Z M,et al.Spectral-Spatial Residual Network for Hyperspectral Image Classification:A 3-D Deep Learning Framework.IEEE Transactions on Geoscience and Remote Sensing,2018,56(2):847-858．
                                    </a>
                                </li>
                                <li id="437">


                                    <a id="bibliography_20" title="LUO Y N,ZOU J,YAO C F,et al.HSI-CNN:A Novel Convolution Neural Network for Hyperspectral Image//Proc of the International Conference on Audio Language and Image Processing.Berlin,Germany:Springer,2018:464-469．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=HSI-CNN:A Novel Convolution Neural Network for Hyperspectral Image">
                                        <b>[20]</b>
                                        LUO Y N,ZOU J,YAO C F,et al.HSI-CNN:A Novel Convolution Neural Network for Hyperspectral Image//Proc of the International Conference on Audio Language and Image Processing.Berlin,Germany:Springer,2018:464-469．
                                    </a>
                                </li>
                                <li id="439">


                                    <a id="bibliography_21" title="SHAMSOLMOALI P,ZAREAPOOR M,YANG J.Convolutional Neural Network in Network(CNNiN):Hyperspectral Image Classification and Dimensionality Reduction.IET Image Processing,2019,13(2):246-253．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Network in Network (CNNiN):Hyperspectral Image Classification and Dimensionality Reduction">
                                        <b>[21]</b>
                                        SHAMSOLMOALI P,ZAREAPOOR M,YANG J.Convolutional Neural Network in Network(CNNiN):Hyperspectral Image Classification and Dimensionality Reduction.IET Image Processing,2019,13(2):246-253．
                                    </a>
                                </li>
                                <li id="441">


                                    <a id="bibliography_22" title="HE M Y,LI B,CHEN H H.Multi-scale 3D Deep Convolutional Neural Network for Hyperspectral Image Classification//Proc of the IEEE International Conference on Image Processing.Washington,USA:IEEE,2017:3904-3908．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-scale 3D deep convolutional neural network for hyperspectral image classification">
                                        <b>[22]</b>
                                        HE M Y,LI B,CHEN H H.Multi-scale 3D Deep Convolutional Neural Network for Hyperspectral Image Classification//Proc of the IEEE International Conference on Image Processing.Washington,USA:IEEE,2017:3904-3908．
                                    </a>
                                </li>
                                <li id="443">


                                    <a id="bibliography_23" title="LI J C,FANG F M,MEI K F,et al.Multi-scale Residual Network for Image Super-Resolution//Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2018:527-542．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-scale residual network for image super-resolution">
                                        <b>[23]</b>
                                        LI J C,FANG F M,MEI K F,et al.Multi-scale Residual Network for Image Super-Resolution//Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2018:527-542．
                                    </a>
                                </li>
                                <li id="445">


                                    <a id="bibliography_24" title="GONG Z Q,ZHONG P,YU Y,et al.A CNN with Multi-scale Convolution and Diversified Metric for Hyperspectral Image Classification.IEEE Transactions on Geoscience and Remote Sensing,2019,57(6):3599-3618．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A CNN with Multi-scale Convolution and Diversified Metric for Hyperspectral Image Classification">
                                        <b>[24]</b>
                                        GONG Z Q,ZHONG P,YU Y,et al.A CNN with Multi-scale Convolution and Diversified Metric for Hyperspectral Image Classification.IEEE Transactions on Geoscience and Remote Sensing,2019,57(6):3599-3618．
                                    </a>
                                </li>
                                <li id="447">


                                    <a id="bibliography_25" title="XIONG Z T,YUAN Y,WANG Z.AI-NET:Attention Inception Neural Networks for Hyperspectral Image Classification//Proc of the IEEE International Geoscience and Remote Sensing Symposium.Washington,USA:IEEE,2018:2647-2650．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=AI-NET:Attention Inception Neural Networks for Hyperspectral Image Classification">
                                        <b>[25]</b>
                                        XIONG Z T,YUAN Y,WANG Z.AI-NET:Attention Inception Neural Networks for Hyperspectral Image Classification//Proc of the IEEE International Geoscience and Remote Sensing Symposium.Washington,USA:IEEE,2018:2647-2650．
                                    </a>
                                </li>
                                <li id="449">


                                    <a id="bibliography_26" title="LI Y,ZHANG H K,SHEN Q.Spectral-Spatial Classification of Hyperspectral Imagery with 3D Convolutional Neural Network.Remote Sensing,2017,9(1):67-88．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spectral–spatial classification of hyperspectral imagery with 3D convolutional neural network">
                                        <b>[26]</b>
                                        LI Y,ZHANG H K,SHEN Q.Spectral-Spatial Classification of Hyperspectral Imagery with 3D Convolutional Neural Network.Remote Sensing,2017,9(1):67-88．
                                    </a>
                                </li>
                                <li id="451">


                                    <a id="bibliography_27" title="KUO B C,HO H H,LI C H,et al.A Kernel-Based Feature Selection Method for SVM with RBF Kernel for Hyperspectral Image Classification.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2014,7(1):317-326．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A kernel-based feature selection method for SVM with RBF kernel for hyperspectral image classification">
                                        <b>[27]</b>
                                        KUO B C,HO H H,LI C H,et al.A Kernel-Based Feature Selection Method for SVM with RBF Kernel for Hyperspectral Image Classification.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2014,7(1):317-326．
                                    </a>
                                </li>
                                <li id="453">


                                    <a id="bibliography_28" title="LUO H W,TANG Y Y,WANG Y L,et al.Hyperspectral Image Classification Based on Spectral Spatial One-Dimensional Manifold Embedding.IEEE Transactions on Geoscience and Remote Sensing,2016,54(9):5319-5340．" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hyperspectral Image Classification Based on Spectral Spatial One-Dimensional Manifold Embedding">
                                        <b>[28]</b>
                                        LUO H W,TANG Y Y,WANG Y L,et al.Hyperspectral Image Classification Based on Spectral Spatial One-Dimensional Manifold Embedding.IEEE Transactions on Geoscience and Remote Sensing,2016,54(9):5319-5340．
                                    </a>
                                </li>
                                <li id="455">


                                    <a id="bibliography_29" title="WANG Q,GAO J Y,YUAN Y.A Joint Convolutional Neural Networks and Context Transfer for Street Scenes Labeling.IEEE Transactions on Intelligent Transportation Systems,2018,19(5):1457-1470." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Joint Convolutional Neural Networks and Context Transfer for Street Scenes Labeling">
                                        <b>[29]</b>
                                        WANG Q,GAO J Y,YUAN Y.A Joint Convolutional Neural Networks and Context Transfer for Street Scenes Labeling.IEEE Transactions on Intelligent Transportation Systems,2018,19(5):1457-1470.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(10),882-891 DOI:10.16451/j.cnki.issn1003-6059.201910002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于3D多尺度特征融合残差网络的高光谱图像分类</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E6%96%87%E6%85%A7&amp;code=43237362&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭文慧</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%B9%E9%A3%9E%E9%BE%99&amp;code=35333575&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曹飞龙</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E8%AE%A1%E9%87%8F%E5%A4%A7%E5%AD%A6%E7%90%86%E5%AD%A6%E9%99%A2%E5%BA%94%E7%94%A8%E6%95%B0%E5%AD%A6%E7%B3%BB&amp;code=1699595&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国计量大学理学院应用数学系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>深度学习中用于训练的高光谱图像(HSI)数据十分有限,因此较深的网络不利于空谱特征的提取.为了缓解该问题,文中提出3D多尺度特征融合残差网络,利用深度学习和多尺度特征融合的方式对光谱-空间特征进行有序的学习.首先对3D-HSI数据进行自适应降维,将降维后的图像作为网络输入.然后,通过多尺度特征融合残差块依次提取光谱-空间特征,融合不同尺度的特征,通过特征共享增强信息流,获得更丰富的特征.最后以端到端的方式训练网络.在相关数据集上的测试表明,文中网络具有良好的分类性能.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度特征融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E5%85%89%E8%B0%B1%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高光谱图像分类;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    郭文慧,硕士研究生．主要研究方向为深度学习、图像处理等．E-mail:whguo3823269@163.com.&lt;image id="391" type="formula" href="images/MSSB201910002_39100.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    *曹飞龙(通讯作者),博士,教授．主要研究方向为深度学习、图像处理等．E-mail:feilongcao@gmail.com.&lt;image id="392" type="formula" href="images/MSSB201910002_39200.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(No.61672477)资助;</span>
                    </p>
            </div>
                    <h1><b>Hyperspectral Image Classification Based on 3D Multi-scale Feature Fusion Residual Network</b></h1>
                    <h2>
                    <span>GUO Wenhui</span>
                    <span>CAO Feilong</span>
            </h2>
                    <h2>
                    <span>Department of Applied Mathematics, College of Sciences,China Jiliang University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Hyperspectral image(HSI) data used for training in deep learning are insufficient, and therefore deeper network is unfavorable for spectral-spatial feature extraction. To solve this problem, a 3 D multi-scale feature fusion residual network is proposed. Spectral-spatial features are learned by deep learning and multi-scale feature fusion. Firstly, the dimension of 3 D-HSI data is adaptively reduced, and the images after dimensionality reduction are used as the input of the network. Secondly, spectral-spatial features are extracted successively through multi-scale feature fusion residual blocks and features of different scales are fused. The information flow is enhanced through sharing features and richer features are obtained. Finally, the network is trained end-to-end and tested on corresponding datasets. Experimental results show the satisfactory classification performance of the proposed network.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multi-scale%20Feature%20Fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multi-scale Feature Fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Feature%20Extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Feature Extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Hyperspectral%20Image%20Classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Hyperspectral Image Classification;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    GUO Wenhui, master student. Her research interests include deep learning and image processing.;
                                </span>
                                <span>
                                    CAO Feilong (Corresponding author), Ph.D.,professor. His research interests include deep learning and image processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-20</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China(No.61672477);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="263">高光谱图像(Hyperspectral Image, HSI)含有丰富的光谱信息<citation id="457" type="reference"><link href="399" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,在环境<citation id="458" type="reference"><link href="401" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、农作物<citation id="459" type="reference"><link href="403" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、地表<citation id="460" type="reference"><link href="405" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>和物理应用<citation id="461" type="reference"><link href="407" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等领域发挥重要作用,并且HSI的分类可促进高光谱遥感的应用,因此有必要对HSI进行精细分类.迄今为止,HSI的分类研究已经成为图像分类的研究热点之一,并取得大量的研究成果.</p>
                </div>
                <div class="p1">
                    <p id="264">传统的分类算法有支持向量机(SVM)<citation id="464" type="reference"><link href="409" rel="bibliography" /><link href="411" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>、遗传算法<citation id="462" type="reference"><link href="413" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、稀疏表示<citation id="465" type="reference"><link href="415" rel="bibliography" /><link href="417" rel="bibliography" /><link href="419" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>、人工神经网络<citation id="463" type="reference"><link href="421" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等.随着HSI采集设备与技术的发展和对HSI分类精度的要求,这些分类算法已不能满足HSI的分类要求.</p>
                </div>
                <div class="p1">
                    <p id="265">研究者们开始转向使用深度学习的方法分类HSI图像.Slavkovikj等<citation id="466" type="reference"><link href="423" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出基于卷积神经网络(Convolu-tional Neural Networks, CNN)的HSI分类特征学习方法,能直接从高光谱输入数据中学习结构特征.Hu等<citation id="467" type="reference"><link href="425" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>采用深度卷积神经网络直接在光谱域中分类高光谱图像.Zhu等<citation id="468" type="reference"><link href="427" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出基于可变形CNN的分类方法,根据HSI复杂的空间环境自适应调整卷积采样位置及它的大小和形状.Yang等<citation id="469" type="reference"><link href="429" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>利用空间上下文信息和多尺度的光谱特征信息,提高HSI的分类性能.</p>
                </div>
                <div class="p1">
                    <p id="266">相比传统的分类算法,上述算法可以提高分类精度,但是由于高光谱图像数据具有波段多、波段间高相关性、数据冗余及空间特征和光谱特征合为一体等特征,空谱特征未得到有效学习.</p>
                </div>
                <div class="p1">
                    <p id="267">为了充分利用有判别性的空间和光谱特征,Xu等<citation id="470" type="reference"><link href="431" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出具有端对端结构的光谱-空间统一网络,使用长短期记忆网络<citation id="471" type="reference"><link href="433" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>学习光谱特征,CNN学习空间特征,再融合学习特征.Zhong等<citation id="472" type="reference"><link href="435" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>利用端对端结构的光谱-空间残差网络连续学习空谱特征.Luo等<citation id="473" type="reference"><link href="437" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>根据HSI数据特征提出HSI卷积神经网络.Shamsolmoali等<citation id="474" type="reference"><link href="439" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提出利用空间信息和光谱信息从原始HSI中产生高层次特征的网络,既可以学习到有判别性的光谱特征,又可以学习到有价值的空间特征.</p>
                </div>
                <div class="p1">
                    <p id="268">虽然光谱-空间特征都通过CNN获得,但HSI具有复杂层次分布,训练数据有限,CNN模型的卷积核尺度单一,不利于HSI的分类.</p>
                </div>
                <div class="p1">
                    <p id="269"><citation id="486" type="reference"><link href="441" rel="bibliography" /><link href="443" rel="bibliography" /><link href="445" rel="bibliography" /><link href="447" rel="bibliography" />文献[22]～文献[25]</citation>通过设置不同尺度的卷积核构造多尺度网络,用于提取特征,通过不同尺度的卷积运算,捕获更丰富的特征.受多尺度网络的启发,本文提出3D多尺度特征融合残差网络(3D Multi-scale Feature Fusion Residual Network, MFFRN)的高光谱图像分类方法,主要研究多尺度特征融合状态对高光谱图像(Hyperspectral Image, HSI)分类的影响,并通过各支路间的特征共享增强信息流.此外,为了充分提取光谱-空间特征,选择尺寸合适的卷积核顺次提取光谱特征和空间特征.整个网络的分类性能良好.</p>
                </div>
                <h3 id="270" name="270" class="anchor-tag">1 3D多尺度特征融合残差网络的高光谱图像分类方法</h3>
                <h4 class="anchor-tag" id="271" name="271"><b>1.1 3D</b>卷积算法</h4>
                <div class="p1">
                    <p id="272">HSI是具有丰富光谱-空间特征的3D数据,根据这一特征,采用3D卷积运算<citation id="477" type="reference"><link href="449" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>提取光谱空间特征.图1给出3D卷积运算示意图.</p>
                </div>
                <div class="area_img" id="273">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910002_273.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 3D卷积运算示意图" src="Detail/GetImg?filename=images/MSSB201910002_273.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 3D卷积运算示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910002_273.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Illustration of 3D convolutional operation</p>

                </div>
                <div class="p1">
                    <p id="274">由图1可知,输入层为3D图像,维度由空间维度和光谱维度组成,因此卷积核在输入3D图像的两个维度上均进行卷积运算,卷积一次仅得到3D图像中的一个体素点,输入图像整体卷积后得到一个新的3D特征图:</p>
                </div>
                <div class="p1">
                    <p id="275" class="code-formula">
                        <mathml id="275"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>X</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mrow><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi></mrow></msubsup><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>m</mi></munder><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>q</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Q</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>R</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mn>1</mn></mrow></munderover><mi>W</mi></mstyle></mrow></mstyle></mrow></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>m</mi></mrow><mrow><mi>p</mi><mo>,</mo><mi>q</mi><mo>,</mo><mi>r</mi></mrow></msubsup><mi>X</mi><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>,</mo><mi>m</mi></mrow><mrow><mi>x</mi><mo>+</mo><mi>p</mi><mo>,</mo><mi>y</mi><mo>+</mo><mi>q</mi><mo>,</mo><mi>z</mi><mo>+</mo><mi>r</mi></mrow></msubsup><mo>+</mo><mi>b</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="276">其中,<i>X</i><mathml id="353"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mrow><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi></mrow></msubsup></mrow></math></mathml>为第<i>i</i>层的第<i>j</i>个特征图在(<i>x</i>,<i>y</i>,<i>z</i>)位置的输出值,<i>m</i>为第<i>i</i>-1层连接到当前特征图上的特征图的集合,<i>W</i><mathml id="354"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>m</mi></mrow><mrow><mi>p</mi><mo>,</mo><mi>q</mi><mo>,</mo><mi>r</mi></mrow></msubsup></mrow></math></mathml>为3D卷积核在第<i>m</i>个特征图的(<i>p</i>,<i>q</i>,<i>r</i>)位置的权值,<i>b</i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>为偏置,<i>σ</i>(·)为激活函数,<i>P</i><sub><i>i</i></sub>、<i>Q</i><sub><i>i</i></sub>、<i>R</i><sub><i>i</i></sub>分别为卷积核的长度、宽度和高度.</p>
                </div>
                <h4 class="anchor-tag" id="277" name="277"><b>1.2</b> 多尺度网络</h4>
                <div class="p1">
                    <p id="278">HSI具有数据量较大的特点,但用于训练的数据有限.为了缓解因训练样本量小导致分类性能较弱的问题,在HSI分类中引入多尺度网络.</p>
                </div>
                <div class="p1">
                    <p id="279">CNN的任何一个参数都可能对生成的特征图产生影响.固定尺度的卷积核学到的特征通常是特定的,不利于特征的学习.因此使用多尺度网络,通过学习不同尺度下的信息,提取更多有鉴别性的特征,促进小样本数据的特征提取.通过多尺度网络分类HSI可以缓解因训练样本有限而导致的精度降低的问题.</p>
                </div>
                <h4 class="anchor-tag" id="280" name="280"><b>1.3 3D</b>多尺度特征融合残差网络</h4>
                <div class="p1">
                    <p id="281">HSI不是普通的二维图像,而是三维数据,包括一维的光谱数据和二维的空间数据.尽管HSI含有丰富的光谱信息,但是波段较多,相邻波段间相关性较高,存在特征冗余及空间特征信息与光谱特征信息融合为一体.</p>
                </div>
                <div class="p1">
                    <p id="282">由于光谱信息和空间信息对HSI的分类都有重要作用,在特征提取时,应考虑光谱维和空间维的特征.研究者们通过使用3D卷积核对HSI提取特征<citation id="478" type="reference"><link href="429" rel="bibliography" /><link href="435" rel="bibliography" /><link href="441" rel="bibliography" /><link href="449" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">19</a>,<a class="sup">22</a>,<a class="sup">26</a>]</sup></citation>,虽然提高分类精度,但未充分提取有鉴别性的光谱特征或空间特征.</p>
                </div>
                <div class="p1">
                    <p id="283">本文为了更好地预测地物的类标签,提出3D多尺度特征融合残差网络(MFFRN),通过设计尺寸为1×1×5和1×1×7的卷积核着重提取光谱特征,3×3×1和5×5×1的卷积核着重提取空间特征.在整个网络中,通过连续地提取光谱特征和空间特征,可有效利用更多有鉴别性的光谱-空间特征.多尺度网络的应用 (即增加网络宽度)可缓解训练样本有限的问题.此外,本文将特征融合模块嵌入多尺度网络中,通过共享不同尺度间的特征信息,增强网络的信息流,有利于空谱特征的提取,改善分类精度.</p>
                </div>
                <div class="p1">
                    <p id="284">整个网络的模型如图2所示.</p>
                </div>
                <div class="area_img" id="285">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910002_285.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 3D多尺度特征融合残差网络流程图" src="Detail/GetImg?filename=images/MSSB201910002_285.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 3D多尺度特征融合残差网络流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910002_285.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Flowchart of 3D multi-scale feature fusion residual network</p>

                </div>
                <div class="p1">
                    <p id="286">由图2可知,本文网络是一个端对端的训练网络.训练过程的输入中使用步长为1×1×2的24个1×1×7的卷积核对原图像进行卷积运算,达到降维目的,将降维后的3D特征图作为光谱特征提取的输入.</p>
                </div>
                <div class="p1">
                    <p id="287">在光谱特征提取时,首先使用24个尺寸为1×1×5和1×1×7卷积核学习特征:</p>
                </div>
                <div class="area_img" id="288">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201910002_28800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="290">其中,<i>O</i><sub><i>n</i></sub><sub>-1</sub>为多尺度特征融合残差块的输入特征图,“*”为卷积运算,<i>w</i>为卷积核的权值,<i>b</i>为卷积核的偏置,<i>w</i>和<i>b</i>中上指标为卷积层数,下指标为卷积核尺寸,<i>σ</i>(·)为修正线性单元(Rectified Linear Unit, ReLU)激活函数.</p>
                </div>
                <div class="p1">
                    <p id="291">提取两个尺度下的浅层光谱特征,融合每个尺度下学习的24个特征图(即48个特征图),然后再使用24个不同尺度的光谱卷积核进行卷积运算:</p>
                </div>
                <div class="area_img" id="292">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201910002_29200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="294">其中,[<i>M</i><sub>1</sub>,<i>N</i><sub>1</sub>]、[<i>M</i><sub>2</sub>,<i>N</i><sub>2</sub>]为光谱特征融合运算.其余变量含义同式(1).</p>
                </div>
                <div class="p1">
                    <p id="295">通过共享光谱特征,两个支路中流入更多的信息,有利于光谱特征的提取.最后融合学习到的光谱特征,通过1×1×1的卷积运算后与光谱特征的原输入逐元素相加:</p>
                </div>
                <div class="area_img" id="393">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201910002_39300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="394">其中变量含义同式(1)和式(2).</p>
                </div>
                <div class="p1">
                    <p id="298">光谱特征提取图如图3所示.</p>
                </div>
                <div class="area_img" id="299">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910002_299.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 光谱多尺度特征融合残差块" src="Detail/GetImg?filename=images/MSSB201910002_299.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 光谱多尺度特征融合残差块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910002_299.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Spectral multi-scale feature fusion residual block</p>

                </div>
                <div class="p1">
                    <p id="300">在光谱特征提取之后,将学习到的有判别性的光谱特征图(9×9×1,128)变形为(9×9×128,1)的特征柱,并作为空间特征提取的输入.在空间特征提取时,首先使用24个3×3×128的空间卷积核,减少输入特征图的空间尺寸,并提取浅层的空间特征,将输出的24个7×7×1的特征图作为空间多尺度特征融合块的输入.空间特征提取过程与光谱特征提取过程相似,如图4所示.</p>
                </div>
                <div class="area_img" id="301">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910002_301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 空间多尺度特征融合残差块" src="Detail/GetImg?filename=images/MSSB201910002_301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 空间多尺度特征融合残差块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910002_301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Spatial multi-scale feature fusion residual block</p>

                </div>
                <div class="p1">
                    <p id="302">首先分别使用24个3×3×1和24个5×5×1的卷积核对输入图像进行卷积运算:</p>
                </div>
                <div class="p1">
                    <p id="303" class="code-formula">
                        <mathml id="303"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>w</mi><msubsup><mrow></mrow><mrow><mn>3</mn><mo>×</mo><mn>3</mn><mo>×</mo><mn>1</mn></mrow><mn>1</mn></msubsup><mo>*</mo><mi>Η</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mrow><mn>3</mn><mo>×</mo><mn>3</mn><mo>×</mo><mn>1</mn></mrow><mn>1</mn></msubsup><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mi>Q</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>w</mi><msubsup><mrow></mrow><mrow><mn>5</mn><mo>×</mo><mn>5</mn><mo>×</mo><mn>1</mn></mrow><mn>1</mn></msubsup><mo>*</mo><mi>Η</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mrow><mn>5</mn><mo>×</mo><mn>5</mn><mo>×</mo><mn>1</mn></mrow><mn>1</mn></msubsup><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="304">其中<i>H</i><sub><i>n</i></sub><sub>-1</sub>为多尺度特征融合残差块的空间特征输入图.提取两个尺度下的空间特征,融合这两个尺度下的48个特征图,再次使用24个3×3×1和24个5×5×1的卷积核进行卷积运算:</p>
                </div>
                <div class="p1">
                    <p id="305" class="code-formula">
                        <mathml id="305"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>w</mi><msubsup><mrow></mrow><mrow><mn>3</mn><mo>×</mo><mn>3</mn><mo>×</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo>*</mo><mo stretchy="false">[</mo><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>Q</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">]</mo><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mrow><mn>3</mn><mo>×</mo><mn>3</mn><mo>×</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mi>Q</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>w</mi><msubsup><mrow></mrow><mrow><mn>5</mn><mo>×</mo><mn>5</mn><mo>×</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo>*</mo><mo stretchy="false">[</mo><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>Q</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">]</mo><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mrow><mn>5</mn><mo>×</mo><mn>5</mn><mo>×</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="306">其中,[<i>P</i><sub>1</sub>,<i>Q</i><sub>1</sub>]、[<i>P</i><sub>2</sub>,<i>Q</i><sub>2</sub>]为空间特征融合运算.</p>
                </div>
                <div class="p1">
                    <p id="307">通过融合两个尺度下的特征,增强两个支路的信息流,将学习到的特征融合并通过1×1×1的卷积运算后与空间特征的原输入逐元素相加:</p>
                </div>
                <div class="p1">
                    <p id="308"><i>H</i><sub><i>n</i></sub>=<i>H</i><sub><i>n</i></sub><sub>-1</sub>+<i>w</i><mathml id="357"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mo>×</mo><mn>1</mn><mo>×</mo><mn>1</mn></mrow><mn>3</mn></msubsup></mrow></math></mathml>*[<i>P</i><sub>2</sub>,<i>Q</i><sub>2</sub>]+<i>b</i><mathml id="358"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mo>×</mo><mn>1</mn><mo>×</mo><mn>1</mn></mrow><mn>3</mn></msubsup></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="309">整个结构通过多尺度网络连续提取光谱特征和空间特征,使提取到的光谱-空间特征更充分、更具有鉴别性.将不同尺度下提取的特征融合,通过特征共享,增强网络的信息流,有利于光谱-空间上下文的特征提取.</p>
                </div>
                <div class="p1">
                    <p id="310">顺次提取光谱维和空间维的特征,通过平均池化层将(5×5×1,24)的3D特征图转化为(1×1×1,24)的特征向量,再采用全连接层和softmax层分类数据集.在每个卷积层后使用归一化(Batch Nor-malization, BN)<citation id="479" type="reference"><link href="447" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>处理,进一步提高网络的分类精度.</p>
                </div>
                <h4 class="anchor-tag" id="311" name="311"><b>1.4 HSI</b>的总深度学习框架</h4>
                <div class="p1">
                    <p id="312">为了能选择分类效果优、泛化能力佳的模型,在MFFRN框架中,将Indian Pines(IN),Kennedy Space Center(KSC)及University of Pavia(UP)这3个数据集分别划分成三组:训练集、验证集和测试集.训练集<i>Z</i><sub>1</sub>及其相应的标签<i>Y</i><sub>1</sub>用于更新网络的参数,验证集<i>Z</i><sub>2</sub>及其相应的标签<i>Y</i><sub>2</sub>用于监测在训练阶段生成的中间模型,测试集<i>Z</i><sub>3</sub>用于评估最优训练网络.HSI分类的总流程图如图5所示.</p>
                </div>
                <div class="area_img" id="313">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910002_313.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 MFFRN的高光谱图像分类总流程图" src="Detail/GetImg?filename=images/MSSB201910002_313.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 MFFRN的高光谱图像分类总流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910002_313.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Flowchart of HSI classification based on MFFRN</p>

                </div>
                <div class="p1">
                    <p id="314">在图5中,10%(或20%)的训练集样本输入网络中得到初始模型,通过10%验证集更新中间模型,多次更新迭代后,保留最优模型.最后将剩余80%(或70%)的测试集通过最优模型预测样本的类标签.在迭代更新过程中,真实的类标签<i>Y</i><sub>3</sub>={<i>y</i><sub>1</sub>,<i>y</i><sub>2</sub>,…,<i>y</i><sub><i>L</i></sub>}与预测的类标签<mathml id="359"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>Y</mi></mstyle><mo>︿</mo></mover><msub><mrow></mrow><mn>3</mn></msub><mo>=</mo><mo stretchy="false">{</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">}</mo></mrow></math></mathml>间的差异使用交叉熵损失函数估计:</p>
                </div>
                <div class="p1">
                    <p id="315" class="code-formula">
                        <mathml id="315"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>L</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mrow><mi>log</mi></mrow><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mrow><mi>log</mi></mrow><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="316">其中,<i>L</i>为每个数据集总的类标签数,<i>y</i><sub><i>i</i></sub>为样本真实类标签,<mathml id="360"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>y</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>为样本预测类标签.整个网络按照图5的流程迭代更新得到最终的预测结果.</p>
                </div>
                <h3 id="317" name="317" class="anchor-tag">2 实验及结果分析</h3>
                <h4 class="anchor-tag" id="318" name="318"><b>2.1</b> 实验数据集</h4>
                <div class="p1">
                    <p id="319">本文使用IN、KSC、UP数据集验证模型.IN数据集共有16类地物,图像大小为145×145,波段数为220个,AVIRIS传感器在印第安纳西北部采集,由于毁灭20个光谱带,只保留200个光谱带,所以变为145×145×200的3D数据.KSC数据集中共有13类地物,AVIRIS传感器于1996年在佛罗里达州采集.图像大小为512×614,波段数为176,3D数据形状为512×614×176.UP数据集共有9类地物,在2001年由意大利北部的反射式光学系统成像光谱仪采集,图像大小为610×340,波段数为103,形状为610×340×103.</p>
                </div>
                <div class="p1">
                    <p id="320">采用如下指标评价模型的性能:总精度(Over-all Accuracy, OA)、平均精度(Average Accuracy, AA),Kappa系数.OA为分类正确的像素点与总的像素点的比值,AA为指定类别像素点的分类精度的平均值.Kappa系数为被评价的分类方法相对完全随机分类产生的错误减少的比例值.</p>
                </div>
                <div class="p1">
                    <p id="321">本文实验使用Python3.5 Keras框架在NVIDIA Quadro K2200(GPU)下完成.为了实验结果的一般性,每个数据集重复进行5次实验,取平均值作为最终的分类精度.</p>
                </div>
                <h4 class="anchor-tag" id="322" name="322"><b>2.2</b> 网络设置</h4>
                <div class="p1">
                    <p id="323">为了设置更好的网络结构,通过实验对比,确定更优的光谱空间多尺度特征融合残差块数目、空间输入尺寸及输入训练集的百分比.</p>
                </div>
                <div class="p1">
                    <p id="324">图6为光谱残差块和空间残差块的数目对分类精度的影响,图中模坐标“<i>x</i>+<i>y</i>”的形式表示<i>x</i>个光谱残差块和<i>y</i>个空间残差块.以KSC数据集为例,不同的块数组合展示不同的OA、AA和Kappa系数.由图可见,残差块组合数为2+2时精度达到最大,所以本文的网络结构采用这一组合.</p>
                </div>
                <div class="area_img" id="325">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910002_325.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同数目的光谱-空间残差块的精度对比" src="Detail/GetImg?filename=images/MSSB201910002_325.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同数目的光谱-空间残差块的精度对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910002_325.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Accuracy comparison of different numbers of spectral-spatial residual blocks</p>

                </div>
                <div class="p1">
                    <p id="327">图7为3个数据集在不同百分比的训练集下的精度.由(a)、(b)可清晰看出,当训练集在5%～10%之间时,精度明显上升.当百分比达到20%后精度增加缓慢,所以对于IN、KSC数据集,选择20%训练样本训练网络.由(c)可知,训练集在5%～7.5%之间时,精度上升明显.当训练集达到10%后精度变化缓慢,基本保持不变,所以对于UP数据集,选择10%的训练样本训练网络.</p>
                </div>
                <div class="p1">
                    <p id="328">表1为不同空间尺寸下的精度对比.由表可知,随着补丁块尺寸的增加,OA、AA和Kappa系数也随之增大,当尺寸增加到9×9时,精度增加缓慢或停止增加.尽管空间尺寸为11×11时,IN、UP数据集的OA和AA达到最大,但是当空间尺寸为9×9时,OA、AA和Kappa系数近似于空间尺寸为11×11时的精度.综合考虑,选择空间尺寸为9×9的输入尺寸训练网络.</p>
                </div>
                <div class="area_img" id="395">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910002_39500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同训练集下的精度对比" src="Detail/GetImg?filename=images/MSSB201910002_39500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同训练集下的精度对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910002_39500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Accuracy comparison of different training sets</p>

                </div>
                <div class="area_img" id="330">
                    <p class="img_tit"><b>表1 不同空间尺寸下的精度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Precision comparison under different space dimensions</p>
                    <p class="img_note"></p>
                    <table id="330" border="1"><tr><td rowspan="2"><br />空间尺寸</td><td colspan="3"><br />IN数据集</td><td colspan="3"><br />KSC数据集</td><td colspan="3"><br />UP数据集</td></tr><tr><td><br />OA/%</td><td>AA/%</td><td>Kappa/%</td><td><br />OA/%</td><td>AA/%</td><td>Kappa/%</td><td><br />OA/%</td><td>AA/%</td><td>Kappa/%</td></tr><tr><td>3×3</td><td>92.92</td><td>94.29</td><td>91.91</td><td>96.65</td><td>95.33</td><td>96.27</td><td>97.70</td><td>97.10</td><td>96.94</td></tr><tr><td><br />7×7</td><td>99.66</td><td>99.66</td><td>99.62</td><td>99.94</td><td><b>99.95</b></td><td>99.93</td><td>99.89</td><td>99.87</td><td>99.86</td></tr><tr><td><br />9×9</td><td>99.70</td><td>99.69</td><td><b>99.67</b></td><td><b>99.95</b></td><td>99.92</td><td><b>99.94</b></td><td><b>99.91</b></td><td>99.88</td><td><b>99.88</b></td></tr><tr><td><br /><b>11×11</b></td><td><b>99.71</b></td><td><b>99.70</b></td><td>99.66</td><td>99.94</td><td>99.91</td><td><b>99.94</b></td><td><b>99.91</b></td><td><b>99.92</b></td><td><b>99.88</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="331" name="331"><b>2.3</b> 各方法的实验结果对比</h4>
                <div class="p1">
                    <p id="332">对比方法如下:基于径向基函数的支持向量机分类方法(SVM Radial Basis Function, SVM-RBF)<citation id="480" type="reference"><link href="451" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>、一维卷积神经网络(One-Dimensional CNN, 1D-CNN)<citation id="481" type="reference"><link href="453" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>、关注开端神经网络(Attention Inception Neural Networks, AI-NET)<citation id="482" type="reference"><link href="447" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、二维卷积神经网络(Two-Dimensional CNN, 2D-CNN)<citation id="483" type="reference"><link href="455" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>、三维卷积神经网络(Three-Dimensional CNN, 3D-CNN)<citation id="484" type="reference"><link href="449" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、光谱空间残差网络(Spectral-Spatial Residual Network, SSRN)<citation id="485" type="reference"><link href="435" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation><sup></sup>.</p>
                </div>
                <div class="p1">
                    <p id="333">表2～表4为各方法在3个数据集上的测试精度对比.</p>
                </div>
                <div class="area_img" id="334">
                    <p class="img_tit"><b>表2 各方法在IN数据集上的测试精度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Testing accuracy comparison of different methods on IN dataset</p>
                    <p class="img_note"></p>
                    <table id="334" border="1"><tr><td>地物类别</td><td>SVM-RBF</td><td>1D-CNN</td><td>AI-NET</td><td>2D-CNN</td><td>3D-CNN</td><td>SSRN</td><td>MFFRN</td></tr><tr><td><br />1</td><td>75.00</td><td>66.66</td><td>96.97</td><td><b>100</b></td><td><b>100</b></td><td>97.82</td><td><b>100</b></td></tr><tr><td><br />2</td><td>78.63</td><td>91.68</td><td>97.06</td><td>98.18</td><td>98.90</td><td>99.17</td><td><b>99.70</b></td></tr><tr><td><br />3</td><td>64.41</td><td>75.00</td><td>99.12</td><td>99.11</td><td>99.65</td><td>99.53</td><td><b>100</b></td></tr><tr><td><br />4</td><td>59.09</td><td>67.04</td><td>95.65</td><td>98.24</td><td>98.75</td><td>97.79</td><td><b>99.42</b></td></tr><tr><td><br />5</td><td>86.98</td><td>86.09</td><td><b>100</b></td><td>97.98</td><td>99.56</td><td>99.24</td><td><b>100</b></td></tr><tr><td><br />6</td><td>96.91</td><td>97.43</td><td>98.42</td><td>99.80</td><td>99.80</td><td>99.51</td><td><b>100</b></td></tr><tr><td><br />7</td><td>77.72</td><td>81.81</td><td><b>100</b></td><td><b>100</b></td><td>94.73</td><td>98.70</td><td><b>100</b></td></tr><tr><td><br />8</td><td>97.38</td><td>99.47</td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td>99.85</td><td><b>100</b></td></tr><tr><td><br />9</td><td>18.75</td><td>56.25</td><td><b>100</b></td><td><b>100</b></td><td>99.55</td><td>98.50</td><td><b>100</b></td></tr><tr><td><br />10</td><td>77.94</td><td>87.41</td><td>96.01</td><td>96.18</td><td>98.83</td><td>98.74</td><td><b>100</b></td></tr><tr><td><br />11</td><td>87.92</td><td>86.22</td><td>99.40</td><td>97.91</td><td>96.52</td><td>99.30</td><td><b>99.77</b></td></tr><tr><td><br />12</td><td>73.21</td><td>87.50</td><td>88.45</td><td>98.58</td><td><b>100</b></td><td>98.43</td><td>99.05</td></tr><tr><td><br />13</td><td>97.56</td><td>96.34</td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td></tr><tr><td><br />14</td><td>97.72</td><td>98.81</td><td>98.66</td><td><b>100</b></td><td>99.77</td><td>99.31</td><td>99.33</td></tr><tr><td><br />15</td><td>50.94</td><td>55.18</td><td>97.12</td><td>97.83</td><td><b>100</b></td><td>99.20</td><td>99.29</td></tr><tr><td><br />16</td><td>75.67</td><td>89.18</td><td>97.05</td><td>96.92</td><td>98.57</td><td>97.82</td><td><b>100</b></td></tr><tr><td><br />OA/%</td><td>83.67±0.40</td><td>88.34±0.56</td><td>96.97±0.56</td><td>98.04±0.50</td><td>99.12±0.11</td><td>99.22±0.14</td><td><b>99.70</b>±<b>0.10</b></td></tr><tr><td><br />AA/%</td><td>75.96±0.53</td><td>82.63±0.61</td><td>97.58±0.92</td><td>97.86±0.37</td><td>99.08±0.09</td><td>99.05±0.17</td><td><b>99.69</b>±<b>0.16</b></td></tr><tr><td><br />Kappa/%</td><td>81.21±0.48</td><td>86.63±0.83</td><td>96.54±0.56</td><td>97.77±0.56</td><td>99.07±0.24</td><td>99.11±0.22</td><td><b>99.67</b>±<b>0.11</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="335">
                    <p class="img_tit"><b>表3 各方法在KSC数据集上的测试精度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Testing accuracy comparison of different methods on KSC dataset</p>
                    <p class="img_note"></p>
                    <table id="335" border="1"><tr><td>地物类别</td><td>SVM-RBF</td><td>1D-CNN</td><td>AI-NET</td><td>2D-CNN</td><td>3D-CNN</td><td>SSRN</td><td>MFFRN</td></tr><tr><td><br />1</td><td>95.49</td><td>98.31</td><td>99.43</td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td></tr><tr><td><br />2</td><td>87.58</td><td>86.08</td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td>98.77</td><td><b>100</b></td></tr><tr><td><br />3</td><td>78.43</td><td>83.33</td><td>95.65</td><td>93.88</td><td>96.35</td><td><b>100</b></td><td><b>100</b></td></tr><tr><td><br />4</td><td>51.65</td><td>78.10</td><td>89.66</td><td>90.22</td><td>97.64</td><td>96.07</td><td><b>100</b></td></tr><tr><td><br />5</td><td>43.75</td><td>54.68</td><td>93.94</td><td>97.22</td><td>93.96</td><td>96.61</td><td><b>100</b></td></tr><tr><td><br />6</td><td>30.65</td><td>33.33</td><td>87.43</td><td>99.33</td><td>99.37</td><td><b>100</b></td><td><b>100</b></td></tr><tr><td><br />7</td><td>88.88</td><td>96.42</td><td>94.03</td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td></tr><tr><td><br />8</td><td>74.89</td><td>90.60</td><td>99.34</td><td><b>99.34</b></td><td><b>100</b></td><td><b>100</b></td><td>99.34</td></tr><tr><td><br />9</td><td>87.82</td><td>89.42</td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td></tr><tr><td><br />10</td><td>90.90</td><td>96.28</td><td>98.95</td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td></tr><tr><td><br />11</td><td>98.00</td><td>98.80</td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td></tr><tr><td><br />12</td><td>83.38</td><td>90.79</td><td>98.91</td><td>99.41</td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td></tr><tr><td><br />13</td><td>99.60</td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td></tr><tr><td><br />OA/%</td><td>84.09±0.63</td><td>89.56±0.76</td><td>98.22±0.57</td><td>98.56±0.70</td><td>99.44±0.34</td><td>99.64±0.22</td><td><b>99.95</b>±<b>0.03</b></td></tr><tr><td><br />AA/%</td><td>77.77±0.85</td><td>84.32±1.04</td><td>96.82±0.66</td><td>97.55±0.86</td><td>99.31±0.42</td><td>99.34±0.32</td><td><b>99.92</b>±<b>0.04</b></td></tr><tr><td><br />Kappa/%</td><td>82.33±0.71</td><td>88.37±0.99</td><td>98.02±0.63</td><td>98.39±0.78</td><td>99.36±0.47</td><td>99.60±0.25</td><td><b>99.94</b>±<b>0.03</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="336">
                    <p class="img_tit"><b>表4 各方法在UP数据集上的测试精度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Testing accuracy comparison of different methods on UP dataset</p>
                    <p class="img_note"></p>
                    <table id="336" border="1"><tr><td>地物类别</td><td>SVM-RBF</td><td>1D-CNN</td><td>AI-NET</td><td>2D-CNN</td><td>3D-CNN</td><td>SSRN</td><td>MFFRN</td></tr><tr><td><br />1</td><td>92.71</td><td>94.63</td><td>96.35</td><td>98.71</td><td>99.43</td><td><b>100</b></td><td>99.92</td></tr><tr><td><br />2</td><td>98.42</td><td>98.59</td><td>99.54</td><td>99.84</td><td>99.87</td><td><b>99.99</b></td><td>99.97</td></tr><tr><td><br />3</td><td>73.90</td><td>73.90</td><td>99.29</td><td>98.52</td><td>99.07</td><td>99.10</td><td><b>99.65</b></td></tr><tr><td><br />4</td><td>93.53</td><td>96.64</td><td>99.14</td><td>99.51</td><td>99.54</td><td>99.25</td><td><b>99.71</b></td></tr><tr><td><br />5</td><td>99.29</td><td>99.84</td><td>99.90</td><td><b>100</b></td><td><b>100</b></td><td>99.81</td><td><b>100</b></td></tr><tr><td><br />6</td><td>86.37</td><td>87.90</td><td>99.70</td><td>99.65</td><td>99.47</td><td><b>100</b></td><td>99.98</td></tr><tr><td><br />7</td><td>87.49</td><td>89.54</td><td>96.46</td><td>97.33</td><td>99.62</td><td>98.25</td><td><b>99.81</b></td></tr><tr><td><br />8</td><td>92.10</td><td>92.99</td><td>92.83</td><td>98.45</td><td>98.51</td><td>99.66</td><td><b>99.89</b></td></tr><tr><td><br />9</td><td>99.44</td><td>99.55</td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td><td><b>100</b></td></tr><tr><td><br />OA/%</td><td>93.60±0.09</td><td>94.68±0.53</td><td>98.47±0.59</td><td>99.43±0.10</td><td>99.62±0.09</td><td>99.80±0.04</td><td><b>99.91</b>±<b>0.03</b></td></tr><tr><td><br />AA/%</td><td>91.47±0.06</td><td>92.90±0.34</td><td>98.23±0.59</td><td>99.26±0.12</td><td>99.55±0.13</td><td>99.56±0.04</td><td><b>99.88</b>±<b>0.04</b></td></tr><tr><td><br />Kappa/%</td><td>91.62±0.14</td><td>93.05±0.49</td><td>98.01±0.39</td><td>99.25±0.13</td><td>99.50±0.15</td><td>99.74±0.06</td><td><b>99.88</b>±<b>0.03</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="337">由表2～表4可知,本文方法可达到最优精度,3个数据集的OA分别为99.70%、99.95% 和99.91%,比SSRN的OA分别高出0.48%、0.31%和0.11%.</p>
                </div>
                <div class="p1">
                    <p id="338">图8～图10分别为不同方法在3个数据集上的视觉图.由图可清晰看到,SVM-RBF、1D-CNN视觉效果图噪声明显,分类模糊.AI-NET、2D-CNN和3D-CNN对地物分类相对清晰,但仍有些许噪声.SSRN对地物的分类较清晰.MFFRN的分类可视化图最清晰,对地物分类的结果最接近真实地物标签.</p>
                </div>
                <div class="area_img" id="396">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910002_39600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 各方法在IN数据集上的视觉图对比" src="Detail/GetImg?filename=images/MSSB201910002_39600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 各方法在IN数据集上的视觉图对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910002_39600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Experimental results comparison of different methods on IN dataset</p>

                </div>
                <div class="area_img" id="396">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910002_39601.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 各方法在IN数据集上的视觉图对比" src="Detail/GetImg?filename=images/MSSB201910002_39601.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 各方法在IN数据集上的视觉图对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910002_39601.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Experimental results comparison of different methods on IN dataset</p>

                </div>
                <div class="area_img" id="397">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910002_39700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 各方法在KSC数据集上的视觉图对比" src="Detail/GetImg?filename=images/MSSB201910002_39700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 各方法在KSC数据集上的视觉图对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910002_39700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Experimental results comparison of different methods on KSC dataset</p>

                </div>
                <div class="area_img" id="398">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910002_39800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 各方法在UP数据集上的视觉图对比" src="Detail/GetImg?filename=images/MSSB201910002_39800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 各方法在UP数据集上的视觉图对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910002_39800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 Experimental results comparison of different methods on UP dataset</p>

                </div>
                <div class="area_img" id="398">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910002_39801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 各方法在UP数据集上的视觉图对比" src="Detail/GetImg?filename=images/MSSB201910002_39801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 各方法在UP数据集上的视觉图对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910002_39801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 Experimental results comparison of different methods on UP dataset</p>

                </div>
                <h3 id="351" name="351" class="anchor-tag">3 结 束 语</h3>
                <div class="p1">
                    <p id="352">为了改善高光谱图像的分类性能,本文提出端对端的3D多尺度特征融合残差网络,采用自适应方法连续提取光谱空间特征.考虑到高光谱图像波段较多、数据间存在冗余及训练样本有限等特征,在不加深网络的情况下通过使用多尺度的方法提取更丰富的特征.融合不同尺度下的特征,使不同支路的特征共享,增强信息流,从而探测不同尺度下更丰富的图像特征.为了更好地收敛网络,进一步提高分类性能,在多尺度特征融合块中引入残差连接.实验表明,本文方法在高光谱图像分类上性能良好,在视觉效果上也表现出一定优势.今后会根据HSI的特点尝试结合多尺度网络及数据增强、通道-空间关注等方法,构造更优的网络结构.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="399">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hyperspectral image data analysis">

                                <b>[1]</b>LANDGREBE D.Hyperspectral Image Data Analysis.IEEE Signal Processing Magazine,2002,19(1):17-28．
                            </a>
                        </p>
                        <p id="401">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hyperspectral remote sensingdata analysis and future challenges">

                                <b>[2]</b>BIOUCAS-DIAS J M,PLZAZ A,CAMPS-VALLS G,et al.Hyperspectral Remote Sensing Data Analysis and Future Challenges.IEEEGeoscience and Remote Sensing Magazine,2013,1(2):6-36．
                            </a>
                        </p>
                        <p id="403">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Crop classification based on feature band set construction and object-oriented approach using hyperspectral images">

                                <b>[3]</b>ZHANG X,SUN Y L,SHANG K,et al.Crop Classification Based on Feature Band Set Construction and Object-Oriented Approach Using Hyperspectral Images.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2016,9(9):4117-4128．
                            </a>
                        </p>
                        <p id="405">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Aerial Vehicle Tracking by Adaptive Fusion of Hyperspectral Likelihood Maps">

                                <b>[4]</b>UZKENT B,RANGNEKAR A,HOFFMAN M J.Aerial Vehicle Tracking by Adaptive Fusion of Hyperspectral Likelihood Maps//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,1998:233-242．
                            </a>
                        </p>
                        <p id="407">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Remote Sensing and Mapping of Tamarisk along the Colorado River, USA: A Comparative Use of Summer-Acquired Hyperion, Thematic Mapper and QuickBird Data">

                                <b>[5]</b>CARTER G A,LUCAS K L,BLOSSOM G A,et al.Remote Sensing and Mapping of Tamarisk Along the Colorado River,USA:AComparative Use of Summer-Acquired Hyperion,Thematic Mapper and Quickbird Data.Remote Sensing,2009,1(3):318-329．
                            </a>
                        </p>
                        <p id="409">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=BOOK REVIEWS - Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond">

                                <b>[6]</b>ATIYA A.Learning with Kernels:Support Vector Machines,Regularization,Optimization,and Beyond.IEEE Transactions on Neural Networks,2005,16(3).DOI:10.1109/TNN.2005.848998．
                            </a>
                        </p>
                        <p id="411">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201403009&amp;v=MDg2OTFuRnkva1dyL1BLRDdZYkxHNEg5WE1ySTlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>曹扬，赵慧洁，黄四牛，等．基于高效置信传播的改进马尔可夫随机场高光谱数据分类算法．模式识别与人工智能，2014,27(3):248-255.(CAO Y,ZHAO H J,HUANG S N,et al.An Improved Markov Random Field Classification Approach for Hyperspectral Data Based on Efficient Belief Propagation.Pattern Recognition and Artificial Intelligence,2014,27(3):248-255．)
                            </a>
                        </p>
                        <p id="413">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB200002011&amp;v=MTYxOTgvUEtEN1liTEc0SHRITXJZOUVaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rV3I=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>孙立新，高文．基于粗糙集的遥感优化分类波段选择．模式识别与人工智能，2000,13(2):181-186.(SUN L X,GAO W.Selecting the Optimal Classification Bands Based on Rough Sets.Pattern Recognition and Artificial Intelligence,2000,13(2):181-186．)
                            </a>
                        </p>
                        <p id="415">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hyperspectral Image Classification Using Dictionary-Based Sparse Representation">

                                <b>[9]</b>CHEN Y,NASRABADI N M,TRAN T D.Hyperspectral Image Classification Using Dictionary-Based Sparse Representation.IEEETransactions on Geoscience and Remote Sensing,2011,49(10):3973-3985．
                            </a>
                        </p>
                        <p id="417">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial-spectral kernel sparse representation for hyperspectral image classification">

                                <b>[10]</b>LIU J J,WU Z B,WEI Z H,et al.Spatial-Spectral Kernel Sparse Representation for Hyperspectral Image Classification.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2013,6(6):2462-2471．
                            </a>
                        </p>
                        <p id="419">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust patch-based sparse representation for hyperspectral image classification">

                                <b>[11]</b>YUAN H L.Robust Patch-Based Sparse Representation for Hyperspectral Image Classification.International Journal of Wavelets Multiresolution and Information Processing,2017,15(3):1351-1363．
                            </a>
                        </p>
                        <p id="421">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Some Experiments with Ensembles of Neural Networks for Classification of Hyperspectral Images">

                                <b>[12]</b>HER N/ANDEZ-ESPINOSA C,FERNNDEZ-REDONDO M,TORRES-SOSPEDRA J.Some Experiments with Ensembles of Neural Networks for Classification of Hyperspectral Images//Proc of the International Symposium on Neural Networks.Berlin,Germany:Springer,2004:912-917．
                            </a>
                        </p>
                        <p id="423">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hyperspectral image classification with convolutional neural networks">

                                <b>[13]</b>SLAVKOVIKJ V,VERSTOCKT S,DE NEVE W,et al.Hyperspectral Image Classification with Convolutional Neural Networks//Proc of the 23rd ACM International Conference on Multimedia.New York,USA:ACM,2015:1159-1162．
                            </a>
                        </p>
                        <p id="425">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJHD&amp;filename=SJHD15081800000389&amp;v=MjAzMTV1SHlqbVVMYklKMThkYWhVPU5pZkRhcks5SHRuTnA0OUZaT3NQRDNRd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>HU W,HUANG Y Y,WEI L,et al.Deep Convolutional Neural Networks for Hyperspectral Image Classification.Journal of Sensors,2015.DOI:10.1155/2015/258619．
                            </a>
                        </p>
                        <p id="427">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deformable convolutional neural networks for hyperspectral image classifica-tion">

                                <b>[15]</b>ZHU J,FANG L Y,GHAMISI P.Deformable Convolutional Neural Networks for Hyperspectral Image Classification.IEEE Geoscience and Remote Sensing Letters,2018,15(8):1254-1258．
                            </a>
                        </p>
                        <p id="429">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hyperspectral Image Classification With Deep Learning Models">

                                <b>[16]</b>YANG X F,YE Y M,LI X T,et al.Hyperspectral Image Classification with Deep Learning Models.IEEE Transactions on Geoscience and Remote Sensing,2018,56(9):5408-5423．
                            </a>
                        </p>
                        <p id="431">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spectralspatial unified networks for hyperspectral image classification">

                                <b>[17]</b>XU Y H,ZHANG L P,DU B,et al.Spectral-Spatial Unified Networks for Hyperspectral Image Classification.IEEE Transactions on Geoscience and Remote Sensing,2018,56(10):5893-5909．
                            </a>
                        </p>
                        <p id="433">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MjMzMjVpZkpaYks5SHRqTXFvOUZaT29MRFhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUoxOGRhaFU9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b>HOCHREITER S,SCHMIDHUBER J.Long Short-Term Memory.Neural Computation,1997,9(8):1735-1780．
                            </a>
                        </p>
                        <p id="435">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spectral-spatial residual network for hyperspectral image classification:a 3-D deep learning framework">

                                <b>[19]</b>ZHONG Z,LI J,LUO Z M,et al.Spectral-Spatial Residual Network for Hyperspectral Image Classification:A 3-D Deep Learning Framework.IEEE Transactions on Geoscience and Remote Sensing,2018,56(2):847-858．
                            </a>
                        </p>
                        <p id="437">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=HSI-CNN:A Novel Convolution Neural Network for Hyperspectral Image">

                                <b>[20]</b>LUO Y N,ZOU J,YAO C F,et al.HSI-CNN:A Novel Convolution Neural Network for Hyperspectral Image//Proc of the International Conference on Audio Language and Image Processing.Berlin,Germany:Springer,2018:464-469．
                            </a>
                        </p>
                        <p id="439">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Network in Network (CNNiN):Hyperspectral Image Classification and Dimensionality Reduction">

                                <b>[21]</b>SHAMSOLMOALI P,ZAREAPOOR M,YANG J.Convolutional Neural Network in Network(CNNiN):Hyperspectral Image Classification and Dimensionality Reduction.IET Image Processing,2019,13(2):246-253．
                            </a>
                        </p>
                        <p id="441">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-scale 3D deep convolutional neural network for hyperspectral image classification">

                                <b>[22]</b>HE M Y,LI B,CHEN H H.Multi-scale 3D Deep Convolutional Neural Network for Hyperspectral Image Classification//Proc of the IEEE International Conference on Image Processing.Washington,USA:IEEE,2017:3904-3908．
                            </a>
                        </p>
                        <p id="443">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-scale residual network for image super-resolution">

                                <b>[23]</b>LI J C,FANG F M,MEI K F,et al.Multi-scale Residual Network for Image Super-Resolution//Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2018:527-542．
                            </a>
                        </p>
                        <p id="445">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A CNN with Multi-scale Convolution and Diversified Metric for Hyperspectral Image Classification">

                                <b>[24]</b>GONG Z Q,ZHONG P,YU Y,et al.A CNN with Multi-scale Convolution and Diversified Metric for Hyperspectral Image Classification.IEEE Transactions on Geoscience and Remote Sensing,2019,57(6):3599-3618．
                            </a>
                        </p>
                        <p id="447">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=AI-NET:Attention Inception Neural Networks for Hyperspectral Image Classification">

                                <b>[25]</b>XIONG Z T,YUAN Y,WANG Z.AI-NET:Attention Inception Neural Networks for Hyperspectral Image Classification//Proc of the IEEE International Geoscience and Remote Sensing Symposium.Washington,USA:IEEE,2018:2647-2650．
                            </a>
                        </p>
                        <p id="449">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spectral–spatial classification of hyperspectral imagery with 3D convolutional neural network">

                                <b>[26]</b>LI Y,ZHANG H K,SHEN Q.Spectral-Spatial Classification of Hyperspectral Imagery with 3D Convolutional Neural Network.Remote Sensing,2017,9(1):67-88．
                            </a>
                        </p>
                        <p id="451">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A kernel-based feature selection method for SVM with RBF kernel for hyperspectral image classification">

                                <b>[27]</b>KUO B C,HO H H,LI C H,et al.A Kernel-Based Feature Selection Method for SVM with RBF Kernel for Hyperspectral Image Classification.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2014,7(1):317-326．
                            </a>
                        </p>
                        <p id="453">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hyperspectral Image Classification Based on Spectral Spatial One-Dimensional Manifold Embedding">

                                <b>[28]</b>LUO H W,TANG Y Y,WANG Y L,et al.Hyperspectral Image Classification Based on Spectral Spatial One-Dimensional Manifold Embedding.IEEE Transactions on Geoscience and Remote Sensing,2016,54(9):5319-5340．
                            </a>
                        </p>
                        <p id="455">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Joint Convolutional Neural Networks and Context Transfer for Street Scenes Labeling">

                                <b>[29]</b>WANG Q,GAO J Y,YUAN Y.A Joint Convolutional Neural Networks and Context Transfer for Street Scenes Labeling.IEEE Transactions on Intelligent Transportation Systems,2018,19(5):1457-1470.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201910002" />
        <input id="dpi" type="hidden" value="96" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201910002&amp;v=MTUzNDBGckNVUkxPZVplUm5GeS9rV3IvTUtEN1liTEc0SDlqTnI0OUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
