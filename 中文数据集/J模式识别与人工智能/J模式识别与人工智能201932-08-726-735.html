<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131456110030000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201908007%26RESULT%3d1%26SIGN%3d7jYT5p8T527yudQx0yR7jbl5qXs%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201908007&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201908007&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201908007&amp;v=MDAzMThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1VyL0lLRDdZYkxHNEg5ak1wNDlGWTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#69" data-title="1 邻域粗糙集 ">1 邻域粗糙集</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#94" data-title="2 高维类不平衡数据在线特征选择算法 ">2 高维类不平衡数据在线特征选择算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#95" data-title="&lt;b&gt;2.1&lt;/b&gt; 问题描述"><b>2.1</b> 问题描述</a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;2.2&lt;/b&gt; 三种在线评估方法"><b>2.2</b> 三种在线评估方法</a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;2.3&lt;/b&gt; 算法相关定理及其证明"><b>2.3</b> 算法相关定理及其证明</a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;2.4&lt;/b&gt; 类不平衡数据依赖度的计算方法"><b>2.4</b> 类不平衡数据依赖度的计算方法</a></li>
                                                <li><a href="#168" data-title="&lt;b&gt;2.5&lt;/b&gt; 算法步骤"><b>2.5</b> 算法步骤</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#192" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#193" data-title="&lt;b&gt;3.1&lt;/b&gt; 实验数据及评价指标"><b>3.1</b> 实验数据及评价指标</a></li>
                                                <li><a href="#200" data-title="&lt;b&gt;3.2&lt;/b&gt; 与在线特征选择算法的对比"><b>3.2</b> 与在线特征选择算法的对比</a></li>
                                                <li><a href="#214" data-title="&lt;b&gt;3.3&lt;/b&gt; 参数&lt;b&gt;&lt;i&gt;n&lt;/i&gt;&lt;/b&gt;的影响"><b>3.3</b> 参数<b><i>n</i></b>的影响</a></li>
                                                <li><a href="#230" data-title="&lt;b&gt;3.4&lt;/b&gt; 特征流顺序的影响"><b>3.4</b> 特征流顺序的影响</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#246" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#153" data-title="图1 不平衡数据的样本邻域示意图">图1 不平衡数据的样本邻域示意图</a></li>
                                                <li><a href="#196" data-title="&lt;b&gt;表1 实验数据集&lt;/b&gt;"><b>表1 实验数据集</b></a></li>
                                                <li><a href="#204" data-title="&lt;b&gt;表2 5种算法在KNN分类器上的G-mean对比&lt;/b&gt;"><b>表2 5种算法在KNN分类器上的G-mean对比</b></a></li>
                                                <li><a href="#205" data-title="&lt;b&gt;表3 5种算法在SVM分类器上的G-mean对比&lt;/b&gt;"><b>表3 5种算法在SVM分类器上的G-mean对比</b></a></li>
                                                <li><a href="#206" data-title="&lt;b&gt;表4 5种算法在CART分类器上的G-mean对比&lt;/b&gt;"><b>表4 5种算法在CART分类器上的G-mean对比</b></a></li>
                                                <li><a href="#208" data-title="&lt;b&gt;表5 5种算法在7个数据集上的运行时间&lt;/b&gt;"><b>表5 5种算法在7个数据集上的运行时间</b></a></li>
                                                <li><a href="#209" data-title="&lt;b&gt;表6 5种算法在7个数据集上选择特征子集的个数&lt;/b&gt;"><b>表6 5种算法在7个数据集上选择特征子集的个数</b></a></li>
                                                <li><a href="#224" data-title="图2 不同的&lt;i&gt;n&lt;/i&gt;在3个基分类器上的G-mean值">图2 不同的<i>n</i>在3个基分类器上的G-mean值</a></li>
                                                <li><a href="#227" data-title="图3 在4个数据集上不同&lt;i&gt;n&lt;/i&gt;值的运行时间">图3 在4个数据集上不同<i>n</i>值的运行时间</a></li>
                                                <li><a href="#237" data-title="图4 不同特征流下本文算法的G-mean值">图4 不同特征流下本文算法的G-mean值</a></li>
                                                <li><a href="#237" data-title="图4 不同特征流下本文算法的G-mean值">图4 不同特征流下本文算法的G-mean值</a></li>
                                                <li><a href="#240" data-title="图5 不同特征流下本文算法的运行时间">图5 不同特征流下本文算法的运行时间</a></li>
                                                <li><a href="#242" data-title="图6 不同特征流下本文算法的选择特征个数">图6 不同特征流下本文算法的选择特征个数</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" WANG M, LI H, TAO D C, &lt;i&gt;et al&lt;/i&gt;.Multimodal Graph-Based Reranking for Web Image Search.IEEE Transactions on Image Processing, 2012, 21 (11) :4649-4661." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multimodal Graph-Based Reranking for Web Image Search">
                                        <b>[1]</b>
                                         WANG M, LI H, TAO D C, &lt;i&gt;et al&lt;/i&gt;.Multimodal Graph-Based Reranking for Web Image Search.IEEE Transactions on Image Processing, 2012, 21 (11) :4649-4661.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" WANG J L, ZHAO P L, HOI S C H, &lt;i&gt;et al&lt;/i&gt;.Online Feature Selection and Its Applications.IEEE Transactions on Knowledge and Data Engineering, 2014, 26 (3) :698-710." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online feature selection and its applications">
                                        <b>[2]</b>
                                         WANG J L, ZHAO P L, HOI S C H, &lt;i&gt;et al&lt;/i&gt;.Online Feature Selection and Its Applications.IEEE Transactions on Knowledge and Data Engineering, 2014, 26 (3) :698-710.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" ROBNIK-SIKONJA M, KNONNENKO I.Theoretical and Empirical Analysis of ReliefF and RReliefF.Machine Learning, 2003, 53 (1/2) :23-69." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340690&amp;v=MDAxMDhSN3FlYnVkdEZDSGxWTDdKSUY0PU5qN0Jhck80SHRITnJJdEZZdUlQWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         ROBNIK-SIKONJA M, KNONNENKO I.Theoretical and Empirical Analysis of ReliefF and RReliefF.Machine Learning, 2003, 53 (1/2) :23-69.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" HOI S C H, WANG J L, ZHAO P L, &lt;i&gt;et al&lt;/i&gt;.Online Feature Selection for Mining Big Data // Proc of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining:Algorithms, Systems, Programming Models and Applications.New York, USA:ACM, 2012:93-100." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online feature selection for mining big data">
                                        <b>[4]</b>
                                         HOI S C H, WANG J L, ZHAO P L, &lt;i&gt;et al&lt;/i&gt;.Online Feature Selection for Mining Big Data // Proc of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining:Algorithms, Systems, Programming Models and Applications.New York, USA:ACM, 2012:93-100.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" LI K W, YU M X, LIU L, &lt;i&gt;et al&lt;/i&gt;.Feature Selection Method Based on Weighted Mutual Information for Imbalanced Data.International Journal of Software Engineering and Knowledge Engineering, 2018, 28 (8) :1177-1194." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature Selection Method Based on Weighted Mutual Information for Imbalanced Data">
                                        <b>[5]</b>
                                         LI K W, YU M X, LIU L, &lt;i&gt;et al&lt;/i&gt;.Feature Selection Method Based on Weighted Mutual Information for Imbalanced Data.International Journal of Software Engineering and Knowledge Engineering, 2018, 28 (8) :1177-1194.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" DING W, STEPINSKI T F, MU Y, &lt;i&gt;et al&lt;/i&gt;.Subkilometer Crater Discovery with Boosting and Transfer Learning.ACM Transactions on Intelligent Systems and Technology, 2011, 2 (4) .DOI:10.1145/1989734.1989743." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000002781&amp;v=MjE4MjlpZklZN0s3SHRqTnI0OUZaT3NOQzNRNG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUoxOFZhaEk9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         DING W, STEPINSKI T F, MU Y, &lt;i&gt;et al&lt;/i&gt;.Subkilometer Crater Discovery with Boosting and Transfer Learning.ACM Transactions on Intelligent Systems and Technology, 2011, 2 (4) .DOI:10.1145/1989734.1989743.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" 王晨曦, 林耀进, 唐莉, 等.基于信息粒化的多标记特征选择算法.模式识别与人工智能, 2018, 31 (2) :123-131. (WANG C X, LIN Y J, TAN L, &lt;i&gt;et al&lt;/i&gt;.Multi-label Feature Selection Based on Information Granulation.Pattern Recognition and Artificial Intelligence, 2018, 31 (2) :123-131.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201802003&amp;v=MzEyMzA1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1VyL0lLRDdZYkxHNEg5bk1yWTlGWjRRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         王晨曦, 林耀进, 唐莉, 等.基于信息粒化的多标记特征选择算法.模式识别与人工智能, 2018, 31 (2) :123-131. (WANG C X, LIN Y J, TAN L, &lt;i&gt;et al&lt;/i&gt;.Multi-label Feature Selection Based on Information Granulation.Pattern Recognition and Artificial Intelligence, 2018, 31 (2) :123-131.) 
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" WU G, CHANG E Y.Class-Boundary Alignment for Imbalanced Dataset Learning // Proc of the Workshop on Learning from Imba-lanced Datasets.New York, USA:ACM, 2003:49-56." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Class-boundary alignment for imbalanced dataset learning">
                                        <b>[8]</b>
                                         WU G, CHANG E Y.Class-Boundary Alignment for Imbalanced Dataset Learning // Proc of the Workshop on Learning from Imba-lanced Datasets.New York, USA:ACM, 2003:49-56.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" 刘景华, 林梦雷, 王晨曦, 等.基于局部子空间的多标记特征选择算法.模式识别与人工智能, 2016, 29 (3) :240-251. (LIN J H, LIN M L, WANG C X, &lt;i&gt;et al&lt;/i&gt;.Multi-label Feature Selection Algorithm Based on Local Subspace.Pattern Recognition and Artificial Intelligence, 2016, 29 (3) :240-251.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201603006&amp;v=MDQ3OTFuRnkva1VyL0lLRDdZYkxHNEg5Zk1ySTlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         刘景华, 林梦雷, 王晨曦, 等.基于局部子空间的多标记特征选择算法.模式识别与人工智能, 2016, 29 (3) :240-251. (LIN J H, LIN M L, WANG C X, &lt;i&gt;et al&lt;/i&gt;.Multi-label Feature Selection Algorithm Based on Local Subspace.Pattern Recognition and Artificial Intelligence, 2016, 29 (3) :240-251.) 
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" LIANG J Y, WANG F, DANG C Y, &lt;i&gt;et al&lt;/i&gt;.An Efficient Rough Feature Selection Algorithm with a Multi-granulation View.International Journal of Approximate Reasoning, 2012, 53 (6) :912-926." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100142920&amp;v=MDg2NTk1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSjE4VmFoST1OaWZPZmJLN0h0RE9ybzlGWmU4TkJYNA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         LIANG J Y, WANG F, DANG C Y, &lt;i&gt;et al&lt;/i&gt;.An Efficient Rough Feature Selection Algorithm with a Multi-granulation View.International Journal of Approximate Reasoning, 2012, 53 (6) :912-926.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" GU Q Q, LI Z H, HAN J W.Generalized Fisher Score for Feature Selection // Proc of the 27th Conference on Uncertainty in Artificial Intelligence.Berlin, Germany:Springer, 2011:266-273." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalized Fisher Score for Feature Selection">
                                        <b>[11]</b>
                                         GU Q Q, LI Z H, HAN J W.Generalized Fisher Score for Feature Selection // Proc of the 27th Conference on Uncertainty in Artificial Intelligence.Berlin, Germany:Springer, 2011:266-273.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" PENG H C, LONG F H, DING C.Feature Selection Based on Mutual Information:Criteria of Max-Dependence, Max-Relevance, and Min-Redundancy.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (8) :1226-1238." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy">
                                        <b>[12]</b>
                                         PENG H C, LONG F H, DING C.Feature Selection Based on Mutual Information:Criteria of Max-Dependence, Max-Relevance, and Min-Redundancy.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (8) :1226-1238.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" TIBSHIRANI R.Regression Shrinkage and Selection via the Lasso.IEEE Transactions on Pattern Analysis and Machine Intelligence, 1996, 58 (1) :267-288." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14120603924782&amp;v=MjQyNzlZZXJLOEg5UE1xWTlHYmVrTEMzUTdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKMThWYWhJPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         TIBSHIRANI R.Regression Shrinkage and Selection via the Lasso.IEEE Transactions on Pattern Analysis and Machine Intelligence, 1996, 58 (1) :267-288.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" CHAWLA N V, BOWYER K W, HALL L O, &lt;i&gt;et al&lt;/i&gt;.SMOTE:Synthetic Minority Over-Sampling Technique.Journal of Artificial Intelligence Research, 2002, 16 (1) :321-357." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SMOTE: synthetic minority over-sampling technique">
                                        <b>[14]</b>
                                         CHAWLA N V, BOWYER K W, HALL L O, &lt;i&gt;et al&lt;/i&gt;.SMOTE:Synthetic Minority Over-Sampling Technique.Journal of Artificial Intelligence Research, 2002, 16 (1) :321-357.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" GUYON I, ELISSEEFF A.An Introduction to Variable and Feature Selection.Journal of Machine Learning Research, 2003, 3:1157-1182." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An introduction to variable and feature selection">
                                        <b>[15]</b>
                                         GUYON I, ELISSEEFF A.An Introduction to Variable and Feature Selection.Journal of Machine Learning Research, 2003, 3:1157-1182.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" LI H G, WU X D, LI Z, &lt;i&gt;et al&lt;/i&gt;.Group Feature Selection with Streaming Features // Proc of the 13th IEEE International Confe-rence on Data Mining.Washington, USA:IEEE, 2013:1109-1114." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Group feature selection with streaming features">
                                        <b>[16]</b>
                                         LI H G, WU X D, LI Z, &lt;i&gt;et al&lt;/i&gt;.Group Feature Selection with Streaming Features // Proc of the 13th IEEE International Confe-rence on Data Mining.Washington, USA:IEEE, 2013:1109-1114.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" WANG J, WANG M, LI P P, &lt;i&gt;et al&lt;/i&gt;.Online Feature Selection with Group Structure Analysis.IEEE Transactions on Knowledge and Data Engineering, 2015, 27 (11) :3029-3041." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online feature selection with group structure analysis">
                                        <b>[17]</b>
                                         WANG J, WANG M, LI P P, &lt;i&gt;et al&lt;/i&gt;.Online Feature Selection with Group Structure Analysis.IEEE Transactions on Knowledge and Data Engineering, 2015, 27 (11) :3029-3041.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" ZHOU J, FOSTER D P, STINE R A, &lt;i&gt;et al&lt;/i&gt;.Streamwise Feature Selection.Journal of Machine Learning Research, 2006, 3:1861-1885." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Streamwise feature selection">
                                        <b>[18]</b>
                                         ZHOU J, FOSTER D P, STINE R A, &lt;i&gt;et al&lt;/i&gt;.Streamwise Feature Selection.Journal of Machine Learning Research, 2006, 3:1861-1885.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" ZHOU P, HU X G, LI P P, &lt;i&gt;et al&lt;/i&gt;.Online Feature Selection for High-Dimensional Class-Imbalanced Data.Knowledge-Based Systems, 2017, 136:187-199." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES2A666E6BBFF3ED20CE60BD34E3A1CD86&amp;v=MTc5Njl6bDlPZ3ZocUdjMkNMUG5NYktaQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZodzdtK3dLZz1OaWZPZmJISkdOZksyb2szRnAxNUR3bE56Ulpnbg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         ZHOU P, HU X G, LI P P, &lt;i&gt;et al&lt;/i&gt;.Online Feature Selection for High-Dimensional Class-Imbalanced Data.Knowledge-Based Systems, 2017, 136:187-199.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" WU X D, YU K, DING W, &lt;i&gt;et al&lt;/i&gt;.Online Feature Selection with Streaming Features.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (5) :1178-1192." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online feature selection with streaming features">
                                        <b>[20]</b>
                                         WU X D, YU K, DING W, &lt;i&gt;et al&lt;/i&gt;.Online Feature Selection with Streaming Features.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (5) :1178-1192.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" YU K, WU X D, DING W, &lt;i&gt;et al&lt;/i&gt;.Scalable and Accurate Online Feature Selection for Big Data.ACM Transactions on Knowledge Discovery from Data, 2016, 11 (2) .DOI:10.1145/2976744." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM390FFF33858E8979717A5E1CF83FE812&amp;v=MjM3MTNmT0dRbGZDcGJRMzVORmh3N20rd0tnPU5pZklZN0N4SHFlNjJZeEdiTzRIZVhRd3lCOFU2emdNVFFyajMyUTllc1RoVGJ1ZENPTnZGU2lXV3I3SklGcG1hQnVIWQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         YU K, WU X D, DING W, &lt;i&gt;et al&lt;/i&gt;.Scalable and Accurate Online Feature Selection for Big Data.ACM Transactions on Knowledge Discovery from Data, 2016, 11 (2) .DOI:10.1145/2976744.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" LIU J H, LIN Y J, LI Y W, &lt;i&gt;et al&lt;/i&gt;.Online Multi-label Streaming Feature Selection Based on Neighborhood Rough Set.Pattern Re-cognition, 2018, 84:273-287." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC0F4FC48B60B9DCF6FF0C3ACEA015564&amp;v=MzE5NDNQZm5WTnZHQVZuRWw5TzN5VDMyZEVlYk9SUUx5YkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHc3bSt3S2c9TmlmT2ZjQzRhTlc2M0l0TkZ1MA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         LIU J H, LIN Y J, LI Y W, &lt;i&gt;et al&lt;/i&gt;.Online Multi-label Streaming Feature Selection Based on Neighborhood Rough Set.Pattern Re-cognition, 2018, 84:273-287.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" LIN Y J, HU Q H, ZHANG J, &lt;i&gt;et al&lt;/i&gt;.Multi-label Feature Selection with Streaming Labels.Information Sciences, 2016, 372:256-275." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES799FAE56C9C023967815EEB722549A65&amp;v=MjIzMDMzZkxhZE5MeWFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3N20rd0tnPU5pZk9mYlN4RjZlOTJvcERGK0o4REg0NnhoQVU0ajU0UFFxUXF4QQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         LIN Y J, HU Q H, ZHANG J, &lt;i&gt;et al&lt;/i&gt;.Multi-label Feature Selection with Streaming Labels.Information Sciences, 2016, 372:256-275.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" 胡清华, 于达仁, 谢宗霞.基于邻域粒化和粗糙逼近的数值属性约简.软件学报, 2008, 19 (3) :640-649. (HU Q H, YU D R, XIE Z X.Numerical Attribute Reduction Based on Neighborhood Granulation and Rough Approximation.Journal of Software, 2008, 19 (3) :640-649.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200803018&amp;v=MTAyODVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVXIvSU55ZlRiTEc0SHRuTXJJOUViSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                         胡清华, 于达仁, 谢宗霞.基于邻域粒化和粗糙逼近的数值属性约简.软件学报, 2008, 19 (3) :640-649. (HU Q H, YU D R, XIE Z X.Numerical Attribute Reduction Based on Neighborhood Granulation and Rough Approximation.Journal of Software, 2008, 19 (3) :640-649.) 
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(08),726-735 DOI:10.16451/j.cnki.issn1003-6059.201908006            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于邻域粗糙集的高维类不平衡数据在线流特征选择</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E7%A5%A5%E7%84%B0&amp;code=42707031&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈祥焰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9E%97%E8%80%80%E8%BF%9B&amp;code=33194400&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">林耀进</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%99%A8%E6%9B%A6&amp;code=37906622&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王晨曦</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%97%BD%E5%8D%97%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=1698703&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">闽南师范大学计算机学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%99%BA%E8%83%BD%E5%BA%94%E7%94%A8%E7%A6%8F%E5%BB%BA%E7%9C%81%E9%AB%98%E7%AD%89%E5%AD%A6%E6%A0%A1%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据科学与智能应用福建省高等学校重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在许多实际应用中, 数据经常呈现高维不平衡特征, 特征还根据需求在不同时间段动态生成.基于此种情况, 文中提出基于邻域粗糙集的高维类不平衡数据的在线流特征选择算法.算法设计基于小类重要度的粗糙依赖度计算公式, 同时, 提出在线相关性分析、在线冗余度分析、在线重要度分析三种策略, 用于选择在大类和小类之间具有高可分离性的特征.在7个高维类不平衡数据集上的实验表明, 文中算法可以有效选择一个较好的特征子集, 性能较优.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%A8%E7%BA%BF%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">在线特征选择;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E7%BB%B4%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高维不平衡数据;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%82%BB%E5%9F%9F%E7%B2%97%E7%B3%99%E9%9B%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邻域粗糙集;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%B2%97%E7%B3%99%E4%BE%9D%E8%B5%96%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">粗糙依赖度;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    陈祥焰, 硕士研究生, 主要研究方向为数据挖掘.E-mail:1053214451@qq.com.;
                                </span>
                                <span>
                                    *林耀进, 博士, 教授, 主要研究方向为数据挖掘、机器学习.E-mail:zzlinyaojin@163.com.;
                                </span>
                                <span>
                                    王晨曦, 硕士, 讲师, 主要研究方向为数据挖掘.E-mail:wangcx5@sina.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-01</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.61672272);</span>
                                <span>福建省自然科学基金项目 (No.2018J01548, 2018J01547);</span>
                                <span>福建省教育厅科技项目 (No.JT180318) 资助;</span>
                    </p>
            </div>
                    <h1><b>Online Streaming Feature Selection for High-Dimensional and Class-Imbalanced Data Based on Neighborhood Rough Set</b></h1>
                    <h2>
                    <span>CHEN Xiangyan</span>
                    <span>LIN Yaojin</span>
                    <span>WANG Chenxi</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Engineering, Minnan Normal University</span>
                    <span>Key Laboratory of Data Science and Intelligence Application, Fujian Province University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In many real world applications, data is dynamically generated at different time periods in addition to high-dimensional imbalanced features. An high-dimensional class-imbalanced online feature selection algorithm based on neighborhood rough set is proposed. The algorithm design is based on rough dependency calculation formula of small class significance. Meanwhile, three evaluation criteria of online relevance analysis, online redundancy analysis and online significance analysis, are presented to select features with high separability between majority and minority classes. Experimental results on seven high-dimensional and class-imbalanced datasets show that the proposed method can effectively select a better feature subset with better performance.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Online%20Feature%20Selection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Online Feature Selection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=High-Dimensional%20and%20Class-Imbalance%20Data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">High-Dimensional and Class-Imbalance Data;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Neighborhood%20Rough%20Set&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Neighborhood Rough Set;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Rough%20Dependence&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Rough Dependence;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    CHEN Xiangyan, master student.His research interests include data mining.;
                                </span>
                                <span>
                                    LIN Yaojin, Ph.D., professor.His research interests include data mining and machine learning.;
                                </span>
                                <span>
                                    WANG Chenxi, master, lecturer.Her research interests include data mining.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-01</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.61672272);</span>
                                <span>Natural Science Foundation of Fujian Province (No.2018J01548, 2018J01547);</span>
                                <span>Technology Project of Education Department of Fujian Province (No.JT180318);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="64">在图像分类<citation id="248" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、疾病诊断<citation id="249" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、多媒体信息检索<citation id="250" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等实际分类任务中, 数据经常呈现特征空间超高维、类别分布不平衡的特点, 产生高维不平衡数据分类建模问题<citation id="252" type="reference"><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>.同时, 特征空间的高维性带来数据存储容量较大、分类时间较长及过拟合等问题.因此, 如何准确识别高维不平衡数据中少数类或小类显得更有实际价值.面向高维不平衡数据建立有效的分类模型具有重要意义<citation id="251" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="65">在高维不平衡数据分类学习中, 学者们从特征空间降维、样本类别均衡化, 或同时利用特征空间降维和样本类别均衡化提升分类学习性能<citation id="262" type="reference"><link href="11" rel="bibliography" /><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>.在特征空间降维方面, 一种是将高维特征空间中的样本通过映射或变换的方式转换至低维空间, 达到降维效果<citation id="253" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>.另一种是通过删除不相关或冗余的特征获取尽可能小的强相关、低冗余的特征子集<citation id="254" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 如基于广义F值的特征选择算法 (Generalized Fisher Score for Feature Selection, Fisher Score) <citation id="255" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 最大相关最小冗余算法 (Minimal Redundancy and Maximal Relevance, mRMR) <citation id="256" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>和最小化绝对收缩和选择算子 (Least Absolute Shrinkage and Selection Operator, LASSO) <citation id="257" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>等.在样本类别均衡化方面, 主要包含数据层面和算法层面, 数据层面主要利用各种重采样技术, 如合成少数过采样技术 (Synthetic Minority Oversampling Technique, SMOTE) <citation id="258" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>.算法层面包括调整变量类的成本、概率估计及决策阈值等<citation id="259" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>.同时利用特征空间降维技术和样本类别均衡化提取特征子集的代表性算法包括流环境下的组特征选择算法 (Group Feature Selection with Streaming Fea-tures, GFSSF) <citation id="260" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>和在线组特征选择算法 (Online Feature Selection with Group Structure Analysis, OGFS) <citation id="261" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="66">在现实应用领域的许多高维不平衡数据的收集过程中, 特征并非固定静态, 而是动态未知, 即特征根据需求在不同时间段动态生成, 并且最终产生多少个特征是未知的<citation id="263" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>.因此, 在对高维不平衡数据进行特征选择前, 特征空间信息未知甚至特征空间大小无限, 特征以动态流的方式逐个到达.如何在流特征环境中进行高维不平衡数据的特征选择已逐渐引起研究者们的关注<citation id="264" type="reference"><link href="9" rel="bibliography" /><link href="19" rel="bibliography" /><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">9</a>,<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="67">目前, 流特征选择主要可分为单标记流特征选择和多标记流特征选择.单标记流特征选择算法中较有代表性的算法包括流特征选择 (Streamwise Feature Selection) (如α-investing) <citation id="265" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>, 高维类不平衡数据在线特征选择算法 (Online Feature Selection for High-Dimensional Class-Imbalanced Data, K-OFSD) <citation id="266" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>和在线流特征选择 (Online Streaming Feature Selection, OSFS) <sup></sup><citation id="267" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>等.此外, 为了应对大数据背景下超高维数据带来的挑战, Yu等<citation id="268" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提出可扩展和准确的在线特征选择算法 (Scalable and Accurate Online Feature Selection Approach, SAOLA) .另外, 在多标记学习中, 多标记流特征选择算法中代表性的算法包括基于邻域粗糙集的多标记流特征选择算法 (Online Multi-label Streaming Fea-ture Selection Based on Neighborhood Rough Set, OMNRS) <citation id="269" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>和流标记下的多标记特征选择算法 (Multi-label Feature Selection with Streaming Labels, MLFSL) <citation id="270" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="68">上述关于批量特征选择或流特征选择算法基本都是针对正常类分布数据.传统的批量特征选择算法已无法满足现实应用中时间复杂性的要求, 因为实际应用中特征大都随时间逐个流入, 并且整个特征空间未知<citation id="271" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>.同时, 现有的在线流特征选择算法并未考虑类不平衡问题.基于此种情况, 本文改进邻域粗糙集依赖度计算方法, 处理高维不平衡数据, 同时, 根据在线相关性分析、在线冗余度分析及在线重要度分析准则, 提出基于邻域粗糙集的高维类不平衡数据在线流特征选择算法 (OSFS for High-Dimensional Class-Imbalanced Data Based on Neigh-borhood Rough Set, OFS) .OFS通过在线分析条件特征与条件特征间或条件特征与决策属性间的依赖度、冗余度和重要度, 在线处理逐个流进的特征, 选择在大类和小类之间具有高可分离性的特征子集.在7个高维小样本数据集上的实验表明, 相比现有的在线特征选择算法, OFS性能更优.</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag">1 邻域粗糙集</h3>
                <div class="p1">
                    <p id="70"><b>定义1</b><citation id="272" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation> 给定一个非空的有限样本集合</p>
                </div>
                <div class="p1">
                    <p id="71"><i>U</i>={<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>n</i></sub>}, </p>
                </div>
                <div class="p1">
                    <p id="72"><i>C</i>={<i>a</i><sub>1</sub>, <i>a</i><sub>2</sub>, …, <i>a</i><sub><i>m</i></sub>}作为描述<i>U</i>的实数型特征集合, 如果<i>C</i>能生成该论域<i>U</i>上的一簇邻域关系, <i>D</i>为决策属性, 称<i>NDS</i>=〈<i>U</i>, <i>C</i>, <i>D</i>〉为邻域决策系统.</p>
                </div>
                <div class="p1">
                    <p id="73"><b>定义2</b><citation id="273" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation> 设<i>U</i>为非空集合, 对∀<i>x</i><sub><i>i</i></sub>∈<i>U</i>, <i>x</i><sub><i>j</i></sub>∈<i>U</i>, <i>x</i><sub><i>k</i></sub>∈<i>U</i>都存在唯一确定的实函数<i>Δ</i>与之对应, 并且<i>Δ</i>满足</p>
                </div>
                <div class="p1">
                    <p id="74">1) <i>Δ</i> (<i>x</i><sub><i>i</i></sub>, <i>x</i><sub><i>j</i></sub>) ≥0当且仅当<i>x</i><sub><i>i</i></sub>=<i>x</i><sub><i>j</i></sub>, <i>Δ</i> (<i>x</i><sub><i>i</i></sub>, <i>x</i><sub><i>j</i></sub>) =0;</p>
                </div>
                <div class="p1">
                    <p id="75">2) <i>Δ</i> (<i>x</i><sub><i>i</i></sub>, <i>x</i><sub><i>j</i></sub>) =<i>Δ</i> (<i>x</i><sub><i>j</i></sub>, <i>x</i><sub><i>i</i></sub>) ;</p>
                </div>
                <div class="p1">
                    <p id="76">3) <i>Δ</i> (<i>x</i><sub><i>i</i></sub>, <i>x</i><sub><i>k</i></sub>) ≤<i>Δ</i> (<i>x</i><sub><i>i</i></sub>, <i>x</i><sub><i>j</i></sub>) +<i>Δ</i> (<i>x</i><sub><i>j</i></sub>, <i>x</i><sub><i>k</i></sub>) .</p>
                </div>
                <div class="p1">
                    <p id="77"><b>定义3</b><citation id="274" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation> 设〈<i>U</i>, <i>Δ</i>〉为非空度量空间, <i>x</i>∈<i>U</i>, <i>δ</i>≥0, 则称点集</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">{</mo><mi>y</mi><mo stretchy="false">|</mo><mi>Δ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>≤</mo><mi>δ</mi><mo>, </mo><mi>y</mi><mo>∈</mo><mi>U</mi><mo stretchy="false">}</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">是以<i>x</i>为中心、<i>δ</i>为半径的闭球, 又称为<i>x</i>的邻域.如果<i>Δ</i>为欧氏距离, 则样本<i>x</i>的邻域则是以<i>x</i>为中心、<i>δ</i>为半径的超球体.</p>
                </div>
                <div class="p1">
                    <p id="80"><b>定义4</b><citation id="275" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>  给定<i>NDS</i>=〈<i>U</i>, <i>C</i>, <i>D</i>〉, <i>D</i>将<i>U</i>划分为<i>N</i>个等价类:<i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>, …, <i>X</i><sub><i>N</i></sub>, <i>B</i>⊆<i>C</i>生成<i>U</i>上的邻域关系<i>R</i><sub><i>B</i></sub>, 则决策<i>D</i>关于<i>B</i>的邻域下近似和邻域上近似分别为</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></munder><mi>D</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∪</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><munder accentunder="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></munder></mrow></mstyle><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mspace width="0.25em" /><mover accent="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></mover><mi>D</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∪</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mover accent="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></mover></mrow></mstyle><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">其中</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><munder accentunder="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></munder><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mo stretchy="false">{</mo><mi>x</mi><mo stretchy="false">|</mo><mi>δ</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>⊆</mo><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>x</mi><mo>∈</mo><mi>U</mi><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr><mtr><mtd><mover accent="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></mover><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mo stretchy="false">{</mo><mi>x</mi><mo stretchy="false">|</mo><mi>δ</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∩</mo><mi>X</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>≠</mo><mo>∅</mo><mo>, </mo><mi>x</mi><mo>∈</mo><mi>U</mi><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">邻域决策系统的正域和负域分别定义为</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mi>Ο</mi><mi>S</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo>=</mo><munder accentunder="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></munder><mi>D</mi><mo>, </mo></mtd></mtr><mtr><mtd><mi>Ν</mi><mi>E</mi><mi>G</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo>=</mo><mi>U</mi><mo>-</mo><mover accent="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></mover><mi>D</mi><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86"><b>定义5</b><citation id="276" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation> 给定<i>NDS</i>=〈<i>U</i>, <i>C</i>, <i>D</i>〉, 决策属性<i>D</i>对于条件属性子集<i>B</i>⊆<i>C</i>的依赖度为</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>C</mi><mi>a</mi><mi>r</mi><mi>d</mi><mo stretchy="false"> (</mo><munder accentunder="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></munder><mi>D</mi><mo stretchy="false">) </mo></mrow><mrow><mi>C</mi><mi>a</mi><mi>r</mi><mi>d</mi><mo stretchy="false"> (</mo><mi>U</mi><mo stretchy="false">) </mo></mrow></mfrac><mspace width="0.25em" /><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">由上式可知依赖度是单调的, 若</p>
                </div>
                <div class="p1">
                    <p id="89"><i>B</i><sub>1</sub>⊆<i>B</i><sub>2</sub>⊆…⊆<i>C</i>, </p>
                </div>
                <div class="p1">
                    <p id="90">则有</p>
                </div>
                <div class="p1">
                    <p id="91"><i>γ</i><sub><i>B</i>1</sub> (<i>D</i>) ≤<i>γ</i><sub><i>B</i>2</sub> (<i>D</i>) ≤…≤<i>γ</i><sub><i>C</i></sub> (<i>D</i>) .</p>
                </div>
                <div class="p1">
                    <p id="92"><b>定义6</b><citation id="277" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation> 给定<i>NDS</i>=〈<i>U</i>, <i>C</i>, <i>D</i>〉, <i>B</i>⊆<i>C</i>, 对∀<i>a</i>∉<i>B</i>, 属性<i>a</i>的重要度定义如下:</p>
                </div>
                <div class="p1">
                    <p id="93"><i>sig</i> (<i>a</i>, <i>B</i>, <i>D</i>) =<i>γ</i><sub><i>B</i>∪{<i>a</i>}</sub> (<i>D</i>) -<i>γ</i><sub><i>B</i></sub> (<i>D</i>) .</p>
                </div>
                <h3 id="94" name="94" class="anchor-tag">2 高维类不平衡数据在线特征选择算法</h3>
                <h4 class="anchor-tag" id="95" name="95"><b>2.1</b> 问题描述</h4>
                <div class="p1">
                    <p id="96">给定<i>NDST</i>= (<b><i>U</i></b>, <b><i>C</i></b>∪<b><i>D</i></b>, <b><i>I</i></b>, <i>t</i>) 表示一个不平衡数据在线流特征选择框架, 其中, <b><i>U</i></b>为一个非空有限集合, <b><i>C</i></b>为条件特征子集, <b><i>D</i></b>为决策属性.</p>
                </div>
                <div class="p1">
                    <p id="97"><b><i>U</i></b>=[<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>n</i></sub>]<sup>T</sup>∈<b>R</b><sup><i>n</i>×<i>d</i></sup></p>
                </div>
                <div class="p1">
                    <p id="98">由<i>d</i>维特征空间</p>
                </div>
                <div class="p1">
                    <p id="99"><b><i>F</i></b>=[<i>f</i><sub>1</sub>, <i>f</i><sub>2</sub>, …, <i>f</i><sub><i>d</i></sub>]<sup>T</sup>∈<b>R</b><sup><i>d</i></sup></p>
                </div>
                <div class="p1">
                    <p id="100">上的<i>n</i>个样本组成.</p>
                </div>
                <div class="p1">
                    <p id="101"><b><i>D</i></b>=[<i>k</i><sub>1</sub>, <i>k</i><sub>2</sub>, …, <i>k</i><sub><i>r</i></sub>]<sup>T</sup>∈<b>R</b><sup><i>r</i>×1</sup></p>
                </div>
                <div class="p1">
                    <p id="102">表示样本集中存在<i>r</i>个类别属性.</p>
                </div>
                <div class="p1">
                    <p id="103"><b><i>I</i></b>=[<i>I</i><sub>small</sub>, <i>I</i><sub>large</sub>]<sup>T</sup>∈<i>R</i>.</p>
                </div>
                <div class="p1">
                    <p id="104">其中, <i>R</i>表示不平衡邻域关系, <i>I</i><sub>large</sub>表示不平衡数据中的大类, <i>I</i><sub>small</sub>表示小类, 大类样本的数量远多于小类样本的数量.于是, 定义一个高维不平衡数据在线流特征选择问题如下:给定论域<b><i>U</i></b>, 决策属性<b><i>D</i></b>, 在时间戳<i>t</i>时刻, 流进的特征为<i>f</i><sub><i>t</i></sub>, 已选特征子集<b><i>S</i></b>⊆<b><i>C</i></b>, 从当前已选特征集<b><i>S</i></b>∪<i>f</i><sub><i>t</i></sub>中选出一个在大类和小类中具有高可分离性的特征子集.</p>
                </div>
                <h4 class="anchor-tag" id="105" name="105"><b>2.2</b> 三种在线评估方法</h4>
                <div class="p1">
                    <p id="106">由定义5可知依赖度函数是单调函数, 依赖度反映论域中能确切分类样本的比例, 正域越大, 条件特征子集对决策属性的依赖度越大, 说明该特征对决策属性相关性越强, 并且对决策属性是重要的.为了选择在大类和小类之间有高可分离性的特征, 得到最终的特征子集具有强相关性和低冗余性, 本节基于邻域粗糙集模型介绍3种在线评估准则.</p>
                </div>
                <div class="p1">
                    <p id="107">1) 在线相关性分析.在时间戳<i>t</i>时刻, 记新到达的特征<i>f</i><sub><i>t</i></sub>对决策属性<i>D</i>的依赖度为<i>γ</i><sub><i>f</i><sub><i>t</i></sub></sub> (<i>D</i>) , 若<i>γ</i><sub><i>f</i><sub><i>t</i></sub></sub> (<i>D</i>) &lt;<i>β</i>, 表示新到达的特征与决策属性的依赖度小于<i>β</i>, 认为该特征与决策之间有弱相关性.</p>
                </div>
                <div class="p1">
                    <p id="108">2) 在线冗余度分析.给定条件特征集合</p>
                </div>
                <div class="p1">
                    <p id="109"><i>C</i>={<i>a</i><sub>1</sub>, <i>a</i><sub>2</sub>, …, <i>a</i><sub><i>m</i></sub>}</p>
                </div>
                <div class="p1">
                    <p id="110">和决策特征集<i>D</i>, 假定在时间戳<i>t</i>时刻, 新到达的特征为<i>f</i><sub><i>t</i></sub>, 在时间戳<i>t</i>-1时刻, 已选的特征子集为<i>S</i><sub><i>t</i>-1</sub>⊆<i>C</i>, 若</p>
                </div>
                <div class="p1">
                    <p id="111"><i>γ</i><sub><i>f</i><sub><i>t</i></sub></sub> (<i>D</i>) -<i>γ</i><sub><i>S</i><sub><i>t</i>-1</sub></sub> (<i>D</i>) &gt;0, </p>
                </div>
                <div class="p1">
                    <p id="112">表示新到达特征<i>f</i><sub><i>t</i></sub>的依赖度大于当前已选特征子集的依赖度, 根据依赖度函数的单调性, 将时间戳<i>t</i>时刻已选择特征子集替换为{<i>f</i><sub><i>t</i></sub>}.</p>
                </div>
                <div class="p1">
                    <p id="113">3) 在线重要度分析.给定条件特征集合</p>
                </div>
                <div class="p1">
                    <p id="114"><i>C</i>={<i>a</i><sub>1</sub>, <i>a</i><sub>2</sub>, …, <i>a</i><sub><i>m</i></sub>}</p>
                </div>
                <div class="p1">
                    <p id="115">和决策特征集<i>D</i>, 对于在线流特征选择问题, 在时间戳<i>t</i>时刻, 对于新到达的特征<i>f</i><sub><i>t</i></sub>, 定义<i>f</i><sub><i>t</i></sub>的重要度为</p>
                </div>
                <div class="p1">
                    <p id="116"><i>SIG</i><sub><i>C</i></sub> (<i>f</i><sub><i>t</i></sub>, <i>D</i>) =<i>γ</i><sub><i>C</i>∪{<i>f</i><sub><i>t</i></sub>}</sub> (<i>D</i>) -<i>γ</i><sub><i>C</i></sub> (<i>D</i>) .</p>
                </div>
                <div class="p1">
                    <p id="117">基于在线重要度的准则, 若新到达的特征<i>f</i><sub><i>t</i></sub>的重要度大于0, 则将<i>f</i><sub><i>t</i></sub>并入到已选的特征子集中.</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118"><b>2.3</b> 算法相关定理及其证明</h4>
                <div class="p1">
                    <p id="119">高维类不平衡数据在线特征选择问题在于找到一个能在大类和小类之间具有高可区分性的特征子集.</p>
                </div>
                <div class="p1">
                    <p id="120"><b>定理1</b> 假设在时间戳<i>t</i>-1时刻, 已选特征集合为<i>S</i><sub><i>t</i>-1</sub>, </p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula">
                        <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">|</mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>f</mi><msup><mspace width="0.25em" /><mo>′</mo></msup><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></munder><mi>γ</mi></mstyle><msub><mrow></mrow><mrow><mi>f</mi><msup><mtext> </mtext><mo>′</mo></msup></mrow></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mo>∀</mo><mi>γ</mi><msub><mrow></mrow><mrow><mi>f</mi><msup><mspace width="0.25em" /><mo>′</mo></msup><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></msub><mo>&gt;</mo><mi>β</mi><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="122">在时间戳<i>t</i>时刻, 流入一个新特征<i>f</i><sub><i>t</i></sub>, 如果<i>γ</i><sub><i>f</i><sub><i>t</i></sub></sub> (<i>D</i>) &lt;<i>β</i>, 同时将<i>f</i><sub><i>t</i></sub>并入集合<i>S</i><sub><i>t</i>-1</sub>中, 那么<i>R</i><sub><i>t</i></sub>&lt;<i>R</i><sub><i>t</i>-1</sub>.</p>
                </div>
                <div class="p1">
                    <p id="123"><b>证明</b> 假设</p>
                </div>
                <div class="p1">
                    <p id="124" class="code-formula">
                        <mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">|</mo><mo>=</mo><mi>n</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>, </mo><mspace width="0.25em" /><mi>R</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="125">易知</p>
                </div>
                <div class="p1">
                    <p id="126" class="code-formula">
                        <mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>f</mi><msup><mspace width="0.25em" /><mo>′</mo></msup><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></munder><mi>γ</mi></mstyle><msub><mrow></mrow><mrow><mi>f</mi><msup><mspace width="0.25em" /><mo>′</mo></msup></mrow></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo>=</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mi>n</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="127">对∀<i>γ</i><sub><i>f</i> ′∈<i>S</i><sub><i>t</i>-1</sub></sub>&gt;<i>β</i>, 有<i>r</i><sub><i>t</i>-1</sub>&gt;<i>β</i>.如果将新流入的特征<i>f</i><sub><i>t</i></sub>并到集合<i>S</i><sub><i>t</i>-1</sub>中, 则</p>
                </div>
                <div class="p1">
                    <p id="128" class="code-formula">
                        <mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mstyle displaystyle="true"><mo>∪</mo><mi>f</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo></mtd></mtr><mtr><mtd><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><msup><mi>f</mi><mo>′</mo></msup><mo>∈</mo><mi>S</mi><mi>t</mi></mrow></munder><mi>γ</mi></mstyle><msub><mrow></mrow><mrow><mi>f</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mn>1</mn></mrow></mfrac><mo stretchy="false"> (</mo><mi>n</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mrow></mrow><mrow><mi>f</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mn>1</mn></mrow></mfrac><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mn>1</mn></mrow></mfrac><mi>γ</mi><msub><mrow></mrow><mrow><mi>f</mi><msup><mspace width="0.25em" /><mo>′</mo></msup></mrow></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mn>1</mn></mrow></mfrac><mo stretchy="false"> (</mo><mi>γ</mi><msub><mrow></mrow><mrow><mi>f</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo>-</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mo>∵</mo><mi>γ</mi><msub><mrow></mrow><mrow><mi>f</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mi>β</mi><mo>&lt;</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>, </mo></mtd></mtr><mtr><mtd><mo>∴</mo><mi>γ</mi><msub><mrow></mrow><mrow><mi>f</mi><mi>t</mi></mrow></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo>-</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>&lt;</mo><mn>0</mn><mo>, </mo></mtd></mtr><mtr><mtd><mo>∴</mo><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo>&lt;</mo><mi>R</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="129">所以, 根据在线相关性分析准则, 对于<i>f</i><sub><i>t</i></sub>, 如果<i>γ</i><sub><i>f</i><sub><i>t</i></sub></sub> (<i>D</i>) &lt;<i>β</i>, 并入当前特征子集中, 整个集合相对于决策属性<i>D</i>的相关性变弱, 基于条件特征相对于决策属性强相关性的考虑, 若<i>γ</i><sub><i>f</i><sub><i>t</i></sub></sub> (<i>D</i>) &lt;<i>β</i>, 该特征将被丢弃.</p>
                </div>
                <div class="p1">
                    <p id="130"><b>定理2</b> 假设有条件特征子集<i>S</i>, <i>D</i>为决策属性集, 在任意时间戳<i>t</i>, 流进一个新的特征<i>f</i><sub><i>t</i></sub>, 则有</p>
                </div>
                <div class="p1">
                    <p id="131"><i>γ</i> (<i>S</i>∪<i>f</i><sub><i>t</i></sub>, <i>D</i>) ≥<i>γ</i> (<i>S</i>, <i>D</i>) .</p>
                </div>
                <div class="p1">
                    <p id="132"><b>证明</b> ∵<i>S</i>⊆<i>S</i>∪<i>f</i><sub><i>t</i></sub>, 由定义5易知依赖度是单调的, </p>
                </div>
                <div class="p1">
                    <p id="133">∴<i>γ</i><sub><i>S</i>∪<i>f</i><sub><i>t</i></sub></sub> (<i>D</i>) ≥<i>γ</i><sub><i>S</i></sub> (<i>D</i>) .</p>
                </div>
                <div class="p1">
                    <p id="134"><b>定理3</b> 假设在时间戳<i>t</i>-1时刻, 已选择特征子集为<i>S</i><sub><i>t</i>-1</sub>, 在时间戳<i>t</i>时刻, 新流入的特征为<i>f</i><sub><i>t</i></sub>, 如果<i>γ</i><sub><i>ft</i></sub>&gt;<i>γ</i><sub><i>S</i><sub><i>t</i>-1</sub></sub>, 那么<i>γ</i><sub><i>ft</i></sub>&gt;<i>γ</i><sub>∀<i>f</i><sub><i>t</i></sub>∈<i>S</i><sub><i>t</i>-1</sub></sub>.</p>
                </div>
                <div class="p1">
                    <p id="135"><b>证明</b> 假设</p>
                </div>
                <div class="p1">
                    <p id="136"><i>S</i><sub><i>t</i>-1</sub>={<i>f</i><sub>1</sub>, <i>f</i><sub>2</sub>, …, <i>f</i><sub><i>n</i></sub>}, </p>
                </div>
                <div class="p1">
                    <p id="137">同时将<i>γ</i> (<i>S</i><sub><i>t</i>-1</sub>, <i>D</i>) 记为<i>γ</i><sub><i>S</i><sub><i>t</i>-1</sub></sub>.由定理2可知, </p>
                </div>
                <div class="p1">
                    <p id="138"><i>γ</i><sub><i>S</i><sub><i>t</i>-1</sub></sub>≥<i>γ</i><sub>{<i>S</i><sub><i>t</i>-1</sub>-<i>f</i><sub><i>n</i></sub>}</sub>≥<i>γ</i><sub>{<i>S</i><sub><i>t</i>-1</sub>-<i>f</i><sub><i>n</i></sub>-<i>f</i><sub><i>n</i>-1</sub>}</sub>≥…≥<i>γ</i><sub><i>f</i><sub>1</sub></sub>, </p>
                </div>
                <div class="p1">
                    <p id="139">∴<i>γ</i><sub><i>S</i><sub><i>t</i>-1</sub></sub>≥<i>γ</i><sub>∀<i>f</i> ′∈<i>S</i><sub><i>t</i>-1</sub></sub>, </p>
                </div>
                <div class="p1">
                    <p id="140">∵<i>γ</i><sub><i>f</i><sub><i>t</i></sub></sub>&gt;<i>γ</i><sub><i>S</i><sub><i>t</i>-1</sub></sub>, </p>
                </div>
                <div class="p1">
                    <p id="141">∴<i>γ</i><sub><i>f</i> ′</sub>&gt;<i>γ</i><sub>∀<i>f</i> ′∈<i>S</i><sub><i>t</i>-1</sub></sub>.</p>
                </div>
                <div class="p1">
                    <p id="142">如果在时间戳<i>t</i>时刻, 新流入的特征<i>f</i><sub><i>t</i></sub>的依赖度大于当前已选特征子集<i>S</i><sub><i>t</i>-1</sub>的依赖度, 即</p>
                </div>
                <div class="p1">
                    <p id="143"><i>γ</i><sub><i>f</i><sub><i>t</i></sub></sub> (<i>D</i>) -<i>γ</i><sub><i>S</i><sub><i>t</i>-1</sub></sub> (<i>D</i>) &gt;0, </p>
                </div>
                <div class="p1">
                    <p id="144">由定理3易知<i>γ</i><sub><i>f</i><sub><i>t</i></sub></sub>&gt;<i>γ</i><sub><i>S</i><sub><i>t</i>-1</sub></sub>.根据在线冗余度分析准则, 将当前已选特征子集使用单个特征<i>f</i><sub><i>t</i></sub>代替, 即</p>
                </div>
                <div class="p1">
                    <p id="145"><i>S</i><sub><i>t</i></sub>={<i>f</i><sub><i>t</i></sub>}.</p>
                </div>
                <div class="p1">
                    <p id="146">如果在时间戳<i>t</i>时刻, 存在当前已选特征子集<i>S</i><sub><i>t</i>-1</sub>, 新流入的特征<i>f</i><sub><i>t</i></sub>依赖度<i>γ</i><sub><i>f</i><sub><i>t</i></sub></sub> (<i>D</i>) &gt;<i>β</i>, 单个特征<i>f</i><sub><i>t</i></sub>的依赖度不满足在线冗余度分析准则, 即</p>
                </div>
                <div class="p1">
                    <p id="147"><i>γ</i><sub><i>f</i><sub><i>t</i></sub></sub> (<i>D</i>) ≤<i>γ</i><sub><i>S</i><sub><i>t</i>-1</sub></sub> (<i>D</i>) .</p>
                </div>
                <div class="p1">
                    <p id="148">因此, 需利用在线重要度分析准则决定是否将<i>f</i><sub><i>t</i></sub>并入特征子集<i>S</i><sub><i>t</i>-1</sub>中.基于在线重要度分析准则, 特征<i>f</i><sub><i>t</i></sub>的重要度定义为</p>
                </div>
                <div class="p1">
                    <p id="149"><i>sig</i> (<i>f</i><sub><i>t</i></sub>) =<i>γ</i><sub><i>S</i><sub><i>t</i>-1</sub>∪{<i>f</i><sub><i>t</i></sub>}</sub> (<i>D</i>) -<i>γ</i><sub><i>S</i><sub><i>t</i>-1</sub></sub> (<i>D</i>) , </p>
                </div>
                <div class="p1">
                    <p id="150">当且仅当<i>sig</i> (<i>f</i><sub><i>t</i></sub>) &gt;0, <i>f</i><sub><i>t</i></sub>并入当前已选特征子集<i>S</i><sub><i>t</i>-1</sub>中.通过在线重要度分析, 最终选择的特征子集对决策属性具有最大的重要度.</p>
                </div>
                <h4 class="anchor-tag" id="151" name="151"><b>2.4</b> 类不平衡数据依赖度的计算方法</h4>
                <div class="p1">
                    <p id="152">在不平衡数据中, 小类样本的邻域很难保证类别一致, 利用邻域粗糙集方法计算目标决策类的下近似会遗漏小类样本.如图1所示, <i>x</i><sub>1</sub>表示大类样本, <i>x</i><sub>2</sub>表示小类样本, 在不平衡数据集中, 小类样本数量所占比例非常小.为了区分大类样本与小类样本, OFS根据欧氏距离的度量取前<i>N</i>个最近的固定个数邻域逼近下近似.</p>
                </div>
                <div class="area_img" id="153">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201908007_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 不平衡数据的样本邻域示意图" src="Detail/GetImg?filename=images/MSSB201908007_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 不平衡数据的样本邻域示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201908007_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Illustration of sample neighborhood with imbalanced data</p>

                </div>
                <div class="p1">
                    <p id="155">从图1可知, 利用传统的邻域粗糙集的依赖度计算方法, 会偏向大类样本, 几乎忽略小类样本.因此本文提出不平衡数据的依赖度计算方法, 如定义7所示.</p>
                </div>
                <div class="p1">
                    <p id="156"><b>定义7</b> 给定特征子集<i>S</i>, 样本集合为<i>U</i>, 不平衡邻域关系为<i>R</i>, 样本<i>x</i><sub><i>i</i></sub>在不平衡邻域关系上前<i>N</i>个近邻集合为<i>S</i><sub><i>R</i></sub> (<i>x</i><sub><i>i</i></sub>) , 定义决策属性<i>D</i>对特征子集<i>S</i>的依赖度</p>
                </div>
                <div class="p1">
                    <p id="157" class="code-formula">
                        <mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>e</mi><mi>p</mi><msub><mrow></mrow><mi>S</mi></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>C</mi><mi>a</mi><mi>r</mi><mi>d</mi><mo stretchy="false"> (</mo><mi>U</mi><mo stretchy="false">) </mo></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false">|</mo><mi>U</mi><mtext> </mtext><mo stretchy="false">|</mo></mrow></munderover><mi>C</mi></mstyle><mi>a</mi><mi>r</mi><mi>d</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="158">其中, 在<i>S</i><sub><i>R</i></sub> (<i>x</i><sub><i>i</i></sub>) 中与<i>x</i><sub><i>i</i></sub>有相同类别标签的样本数为<i>Sum</i><sub><i>P</i></sub>, <i>S</i><sub><i>R</i></sub> (<i>x</i><sub><i>i</i></sub>) 中所有样本数量为<i>Sum</i><sub><i>N</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="159">1) 若<i>x</i><sub><i>i</i></sub>属于<i>I</i><sub>large</sub>且<i>S</i><sub><i>R</i></sub> (<i>x</i><sub><i>i</i></sub>) 中所有样本都属于<i>I</i><sub>large</sub>, 则<i>Card</i><sub><i>x</i><sub><i>i</i></sub></sub>=1.</p>
                </div>
                <div class="p1">
                    <p id="160">2) 若<i>x</i><sub><i>i</i></sub>属于<i>I</i><sub>large</sub>且<i>S</i><sub><i>R</i></sub> (<i>x</i><sub><i>i</i></sub>) 中样本的类别分布不一致, 存在</p>
                </div>
                <div class="p1">
                    <p id="161" class="code-formula">
                        <mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>u</mi><mi>m</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo>-</mo><mi>S</mi><mi>u</mi><mi>m</mi><msub><mrow></mrow><mi>Ρ</mi></msub><mo>≥</mo><mfrac><mrow><mi>S</mi><mi>u</mi><mi>m</mi><msub><mrow></mrow><mi>Ν</mi></msub></mrow><mi>n</mi></mfrac><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="162"><i>n</i>为参数, 则</p>
                </div>
                <div class="p1">
                    <p id="163" class="code-formula">
                        <mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>a</mi><mi>r</mi><mi>d</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>=</mo><mfrac><mrow><mi>S</mi><mi>u</mi><mi>m</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mrow><mi>S</mi><mi>u</mi><mi>m</mi><msub><mrow></mrow><mi>Ν</mi></msub></mrow></mfrac><mo>;</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="164">否则</p>
                </div>
                <div class="p1">
                    <p id="165"><i>Card</i><sub><i>x</i><sub><i>i</i></sub></sub>=0.</p>
                </div>
                <div class="p1">
                    <p id="166">3) 若<i>x</i><sub><i>i</i></sub>属于<i>I</i><sub>small</sub>, 则</p>
                </div>
                <div class="p1">
                    <p id="167" class="code-formula">
                        <mathml id="167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>a</mi><mi>r</mi><mi>d</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>=</mo><mfrac><mrow><mi>S</mi><mi>u</mi><mi>m</mi><msub><mrow></mrow><mi>Ρ</mi></msub></mrow><mrow><mi>S</mi><mi>u</mi><mi>m</mi><msub><mrow></mrow><mi>Ν</mi></msub></mrow></mfrac><mspace width="0.25em" /><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="168" name="168"><b>2.5</b> 算法步骤</h4>
                <div class="p1">
                    <p id="169">本文算法步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="170"><b>算法</b> OFS</p>
                </div>
                <div class="p1">
                    <p id="171"><b>输入</b><i>NDS</i>=〈<i>U</i>, <i>C</i>, <i>D</i>〉, </p>
                </div>
                <div class="p1">
                    <p id="172">在<i>t</i>时刻, 选择的特征子集<i>S</i><sub><i>t</i></sub>, </p>
                </div>
                <div class="p1">
                    <p id="173">单个特征和决策类之间的依赖度阈值<i>β</i>, </p>
                </div>
                <div class="p1">
                    <p id="174">控制<i>S</i><sub><i>R</i></sub> (<i>x</i><sub><i>i</i></sub>) 中大类与小类的比例<i>n</i></p>
                </div>
                <div class="p1">
                    <p id="175"><b>输出</b> 已选的特征子集<i>S</i></p>
                </div>
                <div class="p1">
                    <p id="176">1.初始化特征子集<i>S</i>=∅;</p>
                </div>
                <div class="p1">
                    <p id="177">2.在<i>t</i>时刻流进新特征<i>f</i><sub><i>t</i></sub>;</p>
                </div>
                <div class="p1">
                    <p id="178">3.计算<i>f</i><sub><i>t</i></sub>与决策类之间的依赖度<i>Dep</i><sub><i>I</i></sub> (<i>f</i><sub><i>t</i></sub>) ;</p>
                </div>
                <div class="p1">
                    <p id="179">4.如果<i>Dep</i><sub><i>I</i></sub> (<i>f</i><sub><i>t</i></sub>) &lt;<i>β</i>, 删除<i>f</i><sub><i>t</i></sub>并执行14;</p>
                </div>
                <div class="p1">
                    <p id="180">5.计算<i>t</i>-1时刻, 特征子集的依赖度<i>Dep</i><sub><i>I</i></sub> (<i>S</i><sub><i>t</i>-1</sub>) ;</p>
                </div>
                <div class="p1">
                    <p id="181">6.IF <i>Dep</i><sub><i>I</i></sub> (<i>f</i><sub><i>t</i></sub>) &gt;<i>Dep</i><sub><i>I</i></sub> (<i>S</i><sub><i>t</i>-1</sub>) </p>
                </div>
                <div class="p1">
                    <p id="182">7. <i>S</i><sub><i>t</i></sub>←{<i>f</i><sub><i>t</i></sub>};</p>
                </div>
                <div class="p1">
                    <p id="183">8.END IF</p>
                </div>
                <div class="p1">
                    <p id="184">9.IF <i>Dep</i><sub><i>I</i></sub> (<i>S</i><sub><i>t</i>-1</sub>∪<i>f</i><sub><i>t</i></sub>) &gt;<i>Dep</i><sub><i>I</i></sub> (<i>S</i><sub><i>t</i>-1</sub>) </p>
                </div>
                <div class="p1">
                    <p id="185">10. <i>S</i><sub><i>t</i></sub>←<i>S</i><sub><i>t</i>-1</sub>∪{<i>f</i><sub><i>t</i></sub>};</p>
                </div>
                <div class="p1">
                    <p id="186">11.ELSE</p>
                </div>
                <div class="p1">
                    <p id="187">12. return <i>S</i><sub><i>t</i></sub>←<i>S</i><sub><i>t</i>-1</sub>;</p>
                </div>
                <div class="p1">
                    <p id="188">13.END IF</p>
                </div>
                <div class="p1">
                    <p id="189">14.直到没有新特征流进, 返回值<i>S</i>.</p>
                </div>
                <div class="p1">
                    <p id="190">算法主要有3个主要的关键步骤:步骤3计算单个特征<i>f</i><sub><i>t</i></sub>的依赖度, 步骤6计算单个特征<i>f</i><sub><i>t</i></sub>依赖度与当前特征子集<i>S</i><sub><i>t</i>-1</sub>依赖度的关系, 步骤9计算<i>S</i><sub><i>t</i>-1</sub>∪<i>f</i><sub><i>t</i></sub>与<i>S</i><sub><i>t</i>-1</sub>之间依赖度的关系.假设当前数据集有<i>I</i>个特征、<i>S</i>个实例, OFS的时间复杂度为</p>
                </div>
                <div class="p1">
                    <p id="191" class="code-formula">
                        <mathml id="191"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mo stretchy="false">|</mo><mi>Ι</mi><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>S</mi><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <h3 id="192" name="192" class="anchor-tag">3 实验及结果分析</h3>
                <h4 class="anchor-tag" id="193" name="193"><b>3.1</b> 实验数据及评价指标</h4>
                <div class="p1">
                    <p id="194">表1列举7个高维DNA微阵列数据集, 为了适应类不平衡数据分类的要求, 将所有数据集分成两类 (大类/小类) .</p>
                </div>
                <div class="p1">
                    <p id="195">实验中采用分类回归树 (Classification and Re-gression Tree, CART) 、<i>K</i>-近邻 (<i>K</i>-Nearest Neighbor, KNN) 和支持向量机 (Support Vector Machine, SVM) 这3个基分类器评估选定的特征子集.另外, 随机选择1/2样本进行训练, 其余样本进行测试.训练数据集和测试数据集具有相同的类不平衡比率, 所有实验结果在10次运行中取平均值.实验平台统一采用Matlab R2013b.</p>
                </div>
                <div class="area_img" id="196">
                    <p class="img_tit"><b>表1 实验数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Experimental datasets</p>
                    <p class="img_note"></p>
                    <table id="196" border="1"><tr><td>名称</td><td>样本数</td><td>特征数</td><td>类别数</td><td>小类</td><td>大类</td><td>不平衡率</td></tr><tr><td><br />LUNG</td><td>181</td><td>12533</td><td>2</td><td>31</td><td>150</td><td>4.84</td></tr><tr><td><br />LUNG2</td><td>203</td><td>3312</td><td>5</td><td>17</td><td>186</td><td>10.94</td></tr><tr><td><br />DLBCL</td><td>77</td><td>6285</td><td>2</td><td>19</td><td>58</td><td>3.05</td></tr><tr><td><br />LYM</td><td>62</td><td>4026</td><td>3</td><td>9</td><td>53</td><td>5.89</td></tr><tr><td><br />GLIOMA</td><td>50</td><td>4434</td><td>4</td><td>7</td><td>43</td><td>6.14</td></tr><tr><td><br />SRBCT</td><td>83</td><td>2308</td><td>4</td><td>11</td><td>72</td><td>6.55</td></tr><tr><td><br />CAR</td><td>174</td><td>9182</td><td>11</td><td>7</td><td>167</td><td>23.85</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="197">本次实验采用G-mean作为主要的分类性能评价指标:</p>
                </div>
                <div class="p1">
                    <p id="198" class="code-formula">
                        <mathml id="198"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mo>-</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>=</mo><msqrt><mrow><mo stretchy="false"> (</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mfrac><mrow><mi>Τ</mi><mi>Ν</mi></mrow><mrow><mi>Τ</mi><mi>Ν</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mo stretchy="false">) </mo></mrow></msqrt><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="199">其中, <i>TP</i>表示真正例, <i>TN</i>表示真负例, <i>FP</i>表示假正例, <i>FN</i>表示假负例.</p>
                </div>
                <h4 class="anchor-tag" id="200" name="200"><b>3.2</b> 与在线特征选择算法的对比</h4>
                <div class="p1">
                    <p id="201">为了进一步验证本文算法的有效性, 在运行时间、G-mean和选择特征的数量上进行对比.对比算法包括K-OFSD、<i>α</i>-investing、OSFS和SAOLA.</p>
                </div>
                <div class="p1">
                    <p id="202">OFS中固定邻域个数<i>N</i>=7, 单个特征与决策类之间的阈值<i>β</i>=0.5.OSFS和SAOLA中显著性水平<i>α</i>=0.01, <i>α</i>-investing中显著性水平参数<i>α</i>设置与文献<citation id="278" type="reference">[<a class="sup">18</a>]</citation>相同.K-OFSD中<i>K</i>的设置与文献<citation id="279" type="reference">[<a class="sup">19</a>]</citation>相同.</p>
                </div>
                <div class="p1">
                    <p id="203">表2～表4分别描述5种算法在KNN (<i>K</i>=1) 、SVM和CART分类器上的G-mean结果.表5和表6分别为5种算法的运行时间和选择特征子集的个数.</p>
                </div>
                <div class="area_img" id="204">
                    <p class="img_tit"><b>表2 5种算法在KNN分类器上的G-mean对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 G-mean comparison of 5 algorithms using KNN as base classifier</p>
                    <p class="img_note"></p>
                    <table id="204" border="1"><tr><td>数据集</td><td>OFS</td><td>K-OFSD</td><td><i>α</i>-<br />investing</td><td>SAOLA</td><td>OSFS</td></tr><tr><td><br />LUNG</td><td><b>0.9885</b></td><td>0.6675</td><td>0.5661</td><td>0.9680</td><td>0.9405</td></tr><tr><td><br />SRBCT</td><td><b>1.0000</b></td><td><b>1.0000</b></td><td>0.5222</td><td>0.9095</td><td>0.8855</td></tr><tr><td><br />DLBCL</td><td><b>0.9304</b></td><td>0.8359</td><td>0.6935</td><td>0.9127</td><td>0.8573</td></tr><tr><td><br />LUNG2</td><td><b>0.9973</b></td><td>0.9343</td><td>0.9727</td><td>0.6730</td><td>0.8265</td></tr><tr><td><br />LYM</td><td><b>1.0000</b></td><td><b>1.0000</b></td><td>0.9428</td><td><b>1.0000</b></td><td>0.9205</td></tr><tr><td><br />GLIOMA</td><td>0.9401</td><td><b>0.9850</b></td><td>0.6820</td><td>0.8212</td><td>0.6722</td></tr><tr><td><br />CAR</td><td><b>1.0000</b></td><td><b>1.0000</b></td><td>0.8452</td><td>0.8426</td><td>0.9482</td></tr><tr><td><br />平均值</td><td><b>0.9795</b></td><td>0.9175</td><td>0.7464</td><td>0.8753</td><td>0.8644</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="205">
                    <p class="img_tit"><b>表3 5种算法在SVM分类器上的G-mean对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 G-mean comparison of 5 algorithms using SVM as base classifier</p>
                    <p class="img_note"></p>
                    <table id="205" border="1"><tr><td>数据集</td><td>OFS</td><td>K-OFSD</td><td><i>α</i>-<br />investing</td><td>SAOLA</td><td>OSFS</td></tr><tr><td><br />LUNG</td><td><b>1.0000</b></td><td>0.8340</td><td>0.9607</td><td><b>1.0000</b></td><td>0.9590</td></tr><tr><td><br />SRBCT</td><td><b>1.0000</b></td><td><b>1.0000</b></td><td>0.0000</td><td>0.8130</td><td>0.6695</td></tr><tr><td><br />DLBCL</td><td>0.9295</td><td>0.7878</td><td>0.7477</td><td><b>0.9733</b></td><td>0.9097</td></tr><tr><td><br />LUNG2</td><td><b>0.9368</b></td><td><b>0.9368</b></td><td>0.8219</td><td>0.7113</td><td>0.8173</td></tr><tr><td><br />LYM</td><td>0.9879</td><td><b>1.0000</b></td><td>0.9873</td><td>0.9570</td><td>0.9342</td></tr><tr><td><br />GLIOMA</td><td>0.8111</td><td><b>0.9806</b></td><td>0.6314</td><td>0.6208</td><td>0.6314</td></tr><tr><td><br />CAR</td><td><b>0.9871</b></td><td>0.9258</td><td>0.9175</td><td>0.6547</td><td>0.9598</td></tr><tr><td><br />平均值</td><td><b>0.9491</b></td><td>0.9263</td><td>0.7256</td><td>0.8182</td><td>0.8401</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="206">
                    <p class="img_tit"><b>表4 5种算法在CART分类器上的G-mean对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 G-mean comparison of 5 algorithms using CART as base classifier</p>
                    <p class="img_note"></p>
                    <table id="206" border="1"><tr><td>数据集</td><td>OFS</td><td>SAOLA</td><td><i>α</i>-<br />investing</td><td>K-OFSD</td><td>OSFS</td></tr><tr><td><br />LUNG</td><td>0.8036</td><td><b>0.8621</b></td><td>0.7776</td><td>0.8304</td><td>0.8216</td></tr><tr><td><br />SRBCT</td><td>0.7385</td><td>0.6816</td><td><b>0.8469</b></td><td>0.7385</td><td>0.6504</td></tr><tr><td><br />DLBCL</td><td><b>0.7757</b></td><td>0.7668</td><td>0.5636</td><td>0.5964</td><td>0.7573</td></tr><tr><td><br />LUNG2</td><td><b>0.9544</b></td><td>0.5263</td><td>0.6692</td><td><b>0.9544</b></td><td>0.8079</td></tr><tr><td><br />LYM</td><td><b>0.9905</b></td><td>0.9806</td><td>0.8566</td><td>0.7689</td><td>0.6659</td></tr><tr><td><br />GLIOMA</td><td><b>0.8588</b></td><td>0.4539</td><td>0.7733</td><td>0.8353</td><td>0.5533</td></tr><tr><td><br />CAR</td><td>0.8542</td><td>0.7399</td><td>0.9147</td><td><b>0.9258</b></td><td>0.9253</td></tr><tr><td><br />平均值</td><td><b>0.8377</b></td><td>0.7159</td><td>0.7717</td><td>0.8071</td><td>0.7402</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="207">从表1～表4可知, 在使用KNN分类器时, OFS 在7个高维不平衡数据集中的6个上G-mean值均优于K-OFSD.使用SVM或CART分类器时, 5个数据集上的G-mean值较好, 在所有分类器上的平均G-mean值优于K-OFSD.在运行时间上, OFS和K-OFSD在每个数据集上的运行时间相当, 平均运行时间也都相当.</p>
                </div>
                <div class="area_img" id="208">
                    <p class="img_tit"><b>表5 5种算法在7个数据集上的运行时间</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Running time of 5 algorithms on 7 datasets s</p>
                    <p class="img_note"></p>
                    <table id="208" border="1"><tr><td>数据集</td><td>OFS</td><td>K-OFSD</td><td><i>α</i>-<br />investing</td><td>SAOLA</td><td>OSFS</td></tr><tr><td><br />LUNG</td><td>13.928</td><td>13.2501</td><td><b>2.3902</b></td><td>5.3219</td><td>30.8765</td></tr><tr><td><br />SRBCT</td><td>1.9168</td><td>1.3753</td><td><b>0.1771</b></td><td>0.7574</td><td>1.9618</td></tr><tr><td><br />DLBCL</td><td>4.9612</td><td>4.0559</td><td><b>0.4968</b></td><td>1.6028</td><td>3.4468</td></tr><tr><td><br />LUNG2</td><td>14.871</td><td>13.3827</td><td><b>0.4759</b></td><td>2.2141</td><td>5.896</td></tr><tr><td><br />LYM</td><td>2.0807</td><td>1.7301</td><td><b>0.3716</b></td><td>1.3842</td><td>4.5608</td></tr><tr><td><br />GLIOMA</td><td>2.0132</td><td>1.6507</td><td><b>0.3048</b></td><td>3.0393</td><td>3.8901</td></tr><tr><td><br />CAR</td><td>29.725</td><td>29.7912</td><td><b>1.1007</b></td><td>3.9389</td><td>12.9053</td></tr><tr><td><br />平均值</td><td>9.9281</td><td>9.3194</td><td><b>0.7596</b></td><td>2.6084</td><td>9.0768</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="209">
                    <p class="img_tit"><b>表6 5种算法在7个数据集上选择特征子集的个数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 6 Number of selected feature subsets of 5 algorithms on 7 datasets</p>
                    <p class="img_note"></p>
                    <table id="209" border="1"><tr><td>数据集</td><td>OFS</td><td>K-OFSD</td><td><i>α</i>-<br />investing</td><td>SAOLA</td><td>OSFS</td></tr><tr><td><br />LUNG</td><td>20</td><td><b>2</b></td><td>60</td><td>42</td><td>4</td></tr><tr><td><br />SRBCT</td><td>10</td><td>4</td><td>26</td><td>17</td><td><b>3</b></td></tr><tr><td><br />DLBCL</td><td>11</td><td>10</td><td>8</td><td>20</td><td><b>3</b></td></tr><tr><td><br />LUNG2</td><td>37</td><td>31</td><td>45</td><td>30</td><td><b>8</b></td></tr><tr><td><br />LYM</td><td>5</td><td>21</td><td>27</td><td>19</td><td><b>2</b></td></tr><tr><td><br />GLIOMA</td><td>8</td><td>7</td><td>4</td><td>17</td><td><b>2</b></td></tr><tr><td><br />CAR</td><td>20</td><td><b>3</b></td><td>25</td><td>34</td><td>5</td></tr><tr><td><br />平均值</td><td>16.143</td><td>11.1429</td><td>27.8571</td><td>25.5714</td><td><b>3.8571</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="210">由表<b>5</b>可知, <i>α</i>-investing几乎在每个数据集上都具有最短的运行时间, 尽管在运行时间上<i>α</i>-investing具有较大优势, 但由表<b>2</b>和表<b>3</b>可见, <i>α</i>-investing在KNN和SVM分类器上的G-mean值都不理想, 尤其在SRBCT、GLIOMA数据集上, 只在少数两个数据集上具有较高的预测精度.由表<b>4</b>可知, 在CART分类器上, OFS在所有数据集上的G-mean值几乎都优于<i>α</i>-investing.表<b>6</b>中<i>α</i>-investing在大部分数据集上选择的特征个数多于OFS.所以在整个实验中, OFS在很多方面的性能均优于<i>α</i>-investing.</p>
                </div>
                <div class="p1">
                    <p id="211">在使用KNN分类器时, OFS在<b>7</b>个数据集中的<b>6</b>个上G-mean值优于SAOLA, 在LYM数据集上具有相同的精度.使用SVM分类器时, OFS在<b>7</b>个数据集中的<b>5</b>个上具有较高的G-mean值, 在LUNG数据集上, G-mean值相同.由表<b>4</b>可知, 除LUNG数据集, OFS在其它数据集上的G-mean值优于SAOLA.观察表<b>5</b>和表<b>6</b>可知, 虽然SAOLA运行时间更短, 但OFS选择更少的特征.上述情况说明在线冗余性分析时, OFS筛选强相关、低冗余的特征.</p>
                </div>
                <div class="p1">
                    <p id="212">观察表<b>2</b>和表<b>3</b>发现, 虽然在KNN和SVM分类器上, OSFS均得到很高的预测精度, 但基于两个分类器, OFS在每个数据集上的G-mean值都优于OSFS.同时, 在CART分类器上, OFS在<b>6</b>个数据集上的G-mean值都高于OSFS.<b>2</b>种算法的运行时间相当.在表<b>6</b>中, OSFS在每个数据集上选择最少的特征, 在LYM、GLIOMA数据集上仅选择<b>2</b>个特征, 说明OSFS在处理较稀疏的数据集时可能丢失有些重要的信息.而OFS能较好处理离散数据.</p>
                </div>
                <div class="p1">
                    <p id="213">由上述分析易知, 基于KNN、SVM和CART分类器, OFS在<b>7</b>个高维类不平衡数据集上具有最高的G-mean值, 尽管<i>α</i>-investing、SAOLA具有较快的运行时间, 在选择的特征上也不是最少, 但在处理不平衡数据问题时明显优于其他<b>4</b>种对比算法.</p>
                </div>
                <h4 class="anchor-tag" id="214" name="214"><b>3.3</b> 参数<b><i>n</i></b>的影响</h4>
                <div class="p1">
                    <p id="215">本节主要讨论定义7中参数<i>n</i>对OFS处理类不平衡数据结果的影响.<i>n</i>的主要作用是决定对象样本邻域内大类样本中存在的小类样本的比例.根据类不平衡数据依赖度计算方法, 下面分析<i>n</i>=1, 2, 3, 4对实验结果的影响.</p>
                </div>
                <div class="p1">
                    <p id="216">为了减小样本稀疏性对实验带来的影响, 实验选择DLBCL、GLIOMA、LUNG2、CAR数据集讨论不同<i>n</i>值对实验的影响, 图2为分类器不同时<i>n</i>的G-mean结果.</p>
                </div>
                <div class="p1">
                    <p id="217">从图2 (a) 可看到, 使用KNN作为基分类器时, 在DLBCL、GLIOMA、CAR数据集上的实验结果都随<i>n</i>值的增大呈上升趋势.虽然实验结果在LUNG2数据集上随<i>n</i>的变化有所波动, 但在<i>n</i>=4时具有较好的实验结果.</p>
                </div>
                <div class="p1">
                    <p id="218">从图2 (b) 可看到, 使用SVM作为基分类器时, 随着<i>n</i>从小到大变化, G-mean值虽然在4个数据集上均有波动, 但是当<i>n</i>取值越大时在4个数据集上的精度波动越小, 说明<i>n</i>取值越大越能更好适应不平衡数据集.由计算易知, 当<i>n</i>=4时, 在4个数据集上的G-mean最佳.</p>
                </div>
                <div class="p1">
                    <p id="219">从图2 (c) 可看到, 使用CART作为基分类器时, <i>n</i>取值不同对实验结果的影响较大, 特别是当<i>n</i>=2, 3时, 实验结果显示G-mean值的波动较大且在有的数据集上G-mean值相对较小.当<i>n</i>取值变大, 实验结果会趋于平稳, 极差相对较小.同样, 实验结果显示<i>n</i>=4时, 在4个数据集上有最好的G-mean.</p>
                </div>
                <div class="area_img" id="224">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201908007_22400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同的n在3个基分类器上的G-mean值" src="Detail/GetImg?filename=images/MSSB201908007_22400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 不同的<i>n</i>在3个基分类器上的G-mean值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201908007_22400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 G-mean predictive accuracy variation with different <i>n</i> using 3 base classifiers</p>

                </div>
                <div class="p1">
                    <p id="226">在4个数据集上, 根据不同<i>n</i>的取值, 得到10次运行时间的结果, 图3为10次运行时间的平均值.由图可见, <i>n</i>对运行时间几乎无影响.</p>
                </div>
                <div class="area_img" id="227">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201908007_227.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 在4个数据集上不同n值的运行时间" src="Detail/GetImg?filename=images/MSSB201908007_227.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 在4个数据集上不同<i>n</i>值的运行时间  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201908007_227.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Running time variation with different values of <i>n</i> on 4 datasets</p>

                </div>
                <div class="p1">
                    <p id="229">由上述实验分析可知, <i>n</i>的不同取值对实验结果影响较大, 当<i>n</i>取值较大时在3个基分类器上都会获得较好的实验结果, 参数越大, 预测精度越稳定.所以在实验中, 选择参数<i>n</i>=4作为全局最优值.</p>
                </div>
                <h4 class="anchor-tag" id="230" name="230"><b>3.4</b> 特征流顺序的影响</h4>
                <div class="p1">
                    <p id="231">本节讨论不同特征流的顺序对OFS的影响, 实验设置3种不同的特征流顺序, 分别是original、inverse和random.</p>
                </div>
                <div class="p1">
                    <p id="232">图4分别给出3种不同特征流顺序下OFS在7个数据集上的实验结果.</p>
                </div>
                <div class="area_img" id="237">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201908007_23700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同特征流下本文算法的G-mean值" src="Detail/GetImg?filename=images/MSSB201908007_23700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同特征流下本文算法的G-mean值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201908007_23700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 G-mean values of the proposed algorithm on different feature stream orders</p>
                                <p class="img_note"> (c) CART</p>

                </div>
                <div class="area_img" id="237">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201908007_23701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同特征流下本文算法的G-mean值" src="Detail/GetImg?filename=images/MSSB201908007_23701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同特征流下本文算法的G-mean值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201908007_23701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 G-mean values of the proposed algorithm on different feature stream orders</p>
                                <p class="img_note"> (c) CART</p>

                </div>
                <div class="p1">
                    <p id="239">图5和图6分别为在7个数据集上的运行时间和所选特征的平均数量.在上述实验中, 选择KNN、SVM和CART作为基本分类器.</p>
                </div>
                <div class="area_img" id="240">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201908007_240.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同特征流下本文算法的运行时间" src="Detail/GetImg?filename=images/MSSB201908007_240.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同特征流下本文算法的运行时间  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201908007_240.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Running time variation with different feature stream orders</p>

                </div>
                <div class="area_img" id="242">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201908007_242.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同特征流下本文算法的选择特征个数" src="Detail/GetImg?filename=images/MSSB201908007_242.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同特征流下本文算法的选择特征个数  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201908007_242.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Number of selected features of the proposed algorithm on different feature stream orders</p>

                </div>
                <div class="p1">
                    <p id="244">由图4～图6结果可以看出, 随着特征流顺序不同, 3个基分类器上的G-mean值、运行时间及所选特征个数都只有较小的波动.在KNN分类器上的波动最小, 在SVM分类器上有的数据集可能会有所波动, 主要原因是SVM分类器具有较强的鲁棒性, G-mean值基本随所选特征个数的增加而有所提高, OFS选择的特征数量相对较少, 所以才导致SVM分类器上预测精度有所波动.在CART分类器上的预测精度的波动均在可接受的范围内.</p>
                </div>
                <div class="p1">
                    <p id="245">综上所述, 本文算法基于不同的特征流顺序在3种分类器上都可以得到较好效果, 不同的特征流顺序对OFS基本无影响, OFS能较好地适应各种不同的特征流顺序.</p>
                </div>
                <h3 id="246" name="246" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="247">本文研究高维不平衡数据在线流特征选择问题, 如何处理小类的样本对在线特征选择算法的影响是关键.基于此种情况, 本文提出基于邻域粗糙集的高维类不平衡数据在线流特征选择算法 (OFS) .在计算单个特征与决策属性依赖度时减弱小类的一致性约束, 在特征选择的过程中, 加强小类的区分能力.最终选择的特征子集能够在大类和小类中具有较高的可分离性.在7个高维类不平衡数据集上的实验显示, 在总体上OFS性能较好.本文仅考虑二分类不平衡数据的在线流特征选择问题.今后将进一步讨论多类不平衡数据的在线流特征选择方法.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="283" type="formula" href="images/MSSB201908007_28300.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">陈祥焰</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="284" type="formula" href="images/MSSB201908007_28400.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">林耀进</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="285" type="formula" href="images/MSSB201908007_28500.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">王晨曦</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multimodal Graph-Based Reranking for Web Image Search">

                                <b>[1]</b> WANG M, LI H, TAO D C, <i>et al</i>.Multimodal Graph-Based Reranking for Web Image Search.IEEE Transactions on Image Processing, 2012, 21 (11) :4649-4661.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online feature selection and its applications">

                                <b>[2]</b> WANG J L, ZHAO P L, HOI S C H, <i>et al</i>.Online Feature Selection and Its Applications.IEEE Transactions on Knowledge and Data Engineering, 2014, 26 (3) :698-710.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340690&amp;v=MDkwNzhIbFZMN0pJRjQ9Tmo3QmFyTzRIdEhOckl0Rll1SVBZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZD&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> ROBNIK-SIKONJA M, KNONNENKO I.Theoretical and Empirical Analysis of ReliefF and RReliefF.Machine Learning, 2003, 53 (1/2) :23-69.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online feature selection for mining big data">

                                <b>[4]</b> HOI S C H, WANG J L, ZHAO P L, <i>et al</i>.Online Feature Selection for Mining Big Data // Proc of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining:Algorithms, Systems, Programming Models and Applications.New York, USA:ACM, 2012:93-100.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature Selection Method Based on Weighted Mutual Information for Imbalanced Data">

                                <b>[5]</b> LI K W, YU M X, LIU L, <i>et al</i>.Feature Selection Method Based on Weighted Mutual Information for Imbalanced Data.International Journal of Software Engineering and Knowledge Engineering, 2018, 28 (8) :1177-1194.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000002781&amp;v=MzEyNzdoST1OaWZJWTdLN0h0ak5yNDlGWk9zTkMzUTRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKMThWYQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> DING W, STEPINSKI T F, MU Y, <i>et al</i>.Subkilometer Crater Discovery with Boosting and Transfer Learning.ACM Transactions on Intelligent Systems and Technology, 2011, 2 (4) .DOI:10.1145/1989734.1989743.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201802003&amp;v=MDgzMTlHRnJDVVJMT2VaZVJuRnkva1VyL0lLRDdZYkxHNEg5bk1yWTlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 王晨曦, 林耀进, 唐莉, 等.基于信息粒化的多标记特征选择算法.模式识别与人工智能, 2018, 31 (2) :123-131. (WANG C X, LIN Y J, TAN L, <i>et al</i>.Multi-label Feature Selection Based on Information Granulation.Pattern Recognition and Artificial Intelligence, 2018, 31 (2) :123-131.) 
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Class-boundary alignment for imbalanced dataset learning">

                                <b>[8]</b> WU G, CHANG E Y.Class-Boundary Alignment for Imbalanced Dataset Learning // Proc of the Workshop on Learning from Imba-lanced Datasets.New York, USA:ACM, 2003:49-56.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201603006&amp;v=MjQ5ODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVXIvSUtEN1liTEc0SDlmTXJJOUZZb1FLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 刘景华, 林梦雷, 王晨曦, 等.基于局部子空间的多标记特征选择算法.模式识别与人工智能, 2016, 29 (3) :240-251. (LIN J H, LIN M L, WANG C X, <i>et al</i>.Multi-label Feature Selection Algorithm Based on Local Subspace.Pattern Recognition and Artificial Intelligence, 2016, 29 (3) :240-251.) 
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100142920&amp;v=MTAyMDJmT2ZiSzdIdERPcm85RlplOE5CWDQ1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSjE4VmFoST1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> LIANG J Y, WANG F, DANG C Y, <i>et al</i>.An Efficient Rough Feature Selection Algorithm with a Multi-granulation View.International Journal of Approximate Reasoning, 2012, 53 (6) :912-926.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalized Fisher Score for Feature Selection">

                                <b>[11]</b> GU Q Q, LI Z H, HAN J W.Generalized Fisher Score for Feature Selection // Proc of the 27th Conference on Uncertainty in Artificial Intelligence.Berlin, Germany:Springer, 2011:266-273.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy">

                                <b>[12]</b> PENG H C, LONG F H, DING C.Feature Selection Based on Mutual Information:Criteria of Max-Dependence, Max-Relevance, and Min-Redundancy.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (8) :1226-1238.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14120603924782&amp;v=MDE4OTcxOFZhaEk9TmlmWWVySzhIOVBNcVk5R2Jla0xDM1E3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> TIBSHIRANI R.Regression Shrinkage and Selection via the Lasso.IEEE Transactions on Pattern Analysis and Machine Intelligence, 1996, 58 (1) :267-288.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SMOTE: synthetic minority over-sampling technique">

                                <b>[14]</b> CHAWLA N V, BOWYER K W, HALL L O, <i>et al</i>.SMOTE:Synthetic Minority Over-Sampling Technique.Journal of Artificial Intelligence Research, 2002, 16 (1) :321-357.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An introduction to variable and feature selection">

                                <b>[15]</b> GUYON I, ELISSEEFF A.An Introduction to Variable and Feature Selection.Journal of Machine Learning Research, 2003, 3:1157-1182.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Group feature selection with streaming features">

                                <b>[16]</b> LI H G, WU X D, LI Z, <i>et al</i>.Group Feature Selection with Streaming Features // Proc of the 13th IEEE International Confe-rence on Data Mining.Washington, USA:IEEE, 2013:1109-1114.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online feature selection with group structure analysis">

                                <b>[17]</b> WANG J, WANG M, LI P P, <i>et al</i>.Online Feature Selection with Group Structure Analysis.IEEE Transactions on Knowledge and Data Engineering, 2015, 27 (11) :3029-3041.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Streamwise feature selection">

                                <b>[18]</b> ZHOU J, FOSTER D P, STINE R A, <i>et al</i>.Streamwise Feature Selection.Journal of Machine Learning Research, 2006, 3:1861-1885.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES2A666E6BBFF3ED20CE60BD34E3A1CD86&amp;v=MDk0MzROdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZodzdtK3dLZz1OaWZPZmJISkdOZksyb2szRnAxNUR3bE56UlpnbnpsOU9ndmhxR2MyQ0xQbk1iS1pDTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> ZHOU P, HU X G, LI P P, <i>et al</i>.Online Feature Selection for High-Dimensional Class-Imbalanced Data.Knowledge-Based Systems, 2017, 136:187-199.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online feature selection with streaming features">

                                <b>[20]</b> WU X D, YU K, DING W, <i>et al</i>.Online Feature Selection with Streaming Features.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (5) :1178-1192.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM390FFF33858E8979717A5E1CF83FE812&amp;v=MjQyMTl3eUI4VTZ6Z01UUXJqMzJROWVzVGhUYnVkQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZodzdtK3dLZz1OaWZJWTdDeEhxZTYyWXhHYk80SGVYUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> YU K, WU X D, DING W, <i>et al</i>.Scalable and Accurate Online Feature Selection for Big Data.ACM Transactions on Knowledge Discovery from Data, 2016, 11 (2) .DOI:10.1145/2976744.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC0F4FC48B60B9DCF6FF0C3ACEA015564&amp;v=MjY1NzNiUTM1TkZodzdtK3dLZz1OaWZPZmNDNGFOVzYzSXRORnUwUGZuVk52R0FWbkVsOU8zeVQzMmRFZWJPUlFMeWJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> LIU J H, LIN Y J, LI Y W, <i>et al</i>.Online Multi-label Streaming Feature Selection Based on Neighborhood Rough Set.Pattern Re-cognition, 2018, 84:273-287.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES799FAE56C9C023967815EEB722549A65&amp;v=MDg5MDJPR1FsZkNwYlEzNU5GaHc3bSt3S2c9TmlmT2ZiU3hGNmU5Mm9wREYrSjhESDQ2eGhBVTRqNTRQUXFRcXhBM2ZMYWROTHlhQ09OdkZTaVdXcjdKSUZwbWFCdUhZZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> LIN Y J, HU Q H, ZHANG J, <i>et al</i>.Multi-label Feature Selection with Streaming Labels.Information Sciences, 2016, 372:256-275.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200803018&amp;v=MTY0Nzh0R0ZyQ1VSTE9lWmVSbkZ5L2tVci9JTnlmVGJMRzRIdG5Nckk5RWJJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b> 胡清华, 于达仁, 谢宗霞.基于邻域粒化和粗糙逼近的数值属性约简.软件学报, 2008, 19 (3) :640-649. (HU Q H, YU D R, XIE Z X.Numerical Attribute Reduction Based on Neighborhood Granulation and Rough Approximation.Journal of Software, 2008, 19 (3) :640-649.) 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201908007" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201908007&amp;v=MDAzMThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1VyL0lLRDdZYkxHNEg5ak1wNDlGWTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
