<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131461174248750%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dMSSB201910011%26RESULT%3d1%26SIGN%3dohl3KRRI1zCYJsQynnaV1nJ3CB0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201910011&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201910011&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201910011&amp;v=MDU2NDZHNEg5ak5yNDlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1dyM09LRDdZYkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#52" data-title="1 标记分布学习 ">1 标记分布学习</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="2 集成学习 ">2 集成学习</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="&lt;b&gt;2.1&lt;/b&gt; 集成学习理论"><b>2.1</b> 集成学习理论</a></li>
                                                <li><a href="#66" data-title="&lt;b&gt;2.2&lt;/b&gt; 基分类器选择"><b>2.2</b> 基分类器选择</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#96" data-title="3 基于标记分布的异态集成学习算法 ">3 基于标记分布的异态集成学习算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#127" data-title="4 实验及结果分析 ">4 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#128" data-title="&lt;b&gt;4.1&lt;/b&gt; 实验数据集和评价指标"><b>4.1</b> 实验数据集和评价指标</a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;4.2&lt;/b&gt; 异态集成算法实验结果"><b>4.2</b> 异态集成算法实验结果</a></li>
                                                <li><a href="#139" data-title="&lt;b&gt;4.3&lt;/b&gt; 对比算法实验结果"><b>4.3</b> 对比算法实验结果</a></li>
                                                <li><a href="#155" data-title="&lt;b&gt;4.4&lt;/b&gt; 稳定性分析"><b>4.4</b> 稳定性分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#164" data-title="5 结 束 语 ">5 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#101" data-title="图1 HELA-LDL流程图">图1 HELA-LDL流程图</a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;表1 实验数据集&lt;/b&gt;"><b>表1 实验数据集</b></a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;表2 评价指标&lt;/b&gt;"><b>表2 评价指标</b></a></li>
                                                <li><a href="#135" data-title="&lt;b&gt;表3 两个分类器组合在s-JAFFE数据集上的测试结果&lt;/b&gt;"><b>表3 两个分类器组合在s-JAFFE数据集上的测试结果</b></a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;表4 三个分类器组合在s-JAFFE数据集上的测试结果&lt;/b&gt;"><b>表4 三个分类器组合在s-JAFFE数据集上的测试结果</b></a></li>
                                                <li><a href="#148" data-title="&lt;b&gt;表5 六种算法在Chebyshev距离上的测试结果&lt;/b&gt;"><b>表5 六种算法在Chebyshev距离上的测试结果</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;表6 六种算法在Clark距离上的测试结果&lt;/b&gt;"><b>表6 六种算法在Clark距离上的测试结果</b></a></li>
                                                <li><a href="#150" data-title="&lt;b&gt;表7 六种算法在Canberra标准上的测试结果&lt;/b&gt;"><b>表7 六种算法在Canberra标准上的测试结果</b></a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;表8 六种算法在Kullback-Leibler散度上的测试结果&lt;/b&gt;"><b>表8 六种算法在Kullback-Leibler散度上的测试结果</b></a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;表9 六种算法在Cosine相关性上的测试结果&lt;/b&gt;"><b>表9 六种算法在Cosine相关性上的测试结果</b></a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;表10 六种算法在Intersection相似性上的测试结果&lt;/b&gt;"><b>表10 六种算法在Intersection相似性上的测试结果</b></a></li>
                                                <li><a href="#154" data-title="&lt;b&gt;表11 六种算法在各数据集上耗时对比&lt;/b&gt;"><b>表11 六种算法在各数据集上耗时对比</b></a></li>
                                                <li><a href="#199" data-title="图2 六种算法在6个评价指标上的稳定性指数值">图2 六种算法在6个评价指标上的稳定性指数值</a></li>
                                                <li><a href="#199" data-title="图2 六种算法在6个评价指标上的稳定性指数值">图2 六种算法在6个评价指标上的稳定性指数值</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="200">


                                    <a id="bibliography_1" title=" ZHANG M L,ZHOU Z H.A Review on Multi-label Learning Algorithms.IEEE Transactions on Knowledge and Data Engineering,2014,26(8):1819-1837." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A review on multi-label learning algorithms">
                                        <b>[1]</b>
                                         ZHANG M L,ZHOU Z H.A Review on Multi-label Learning Algorithms.IEEE Transactions on Knowledge and Data Engineering,2014,26(8):1819-1837.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_2" title=" 耿新,徐宁,邵瑞枫.面向标记分布学习的标记增强.计算机研究与发展,2017,54(6):1171-1184.(GENG X,XU N,SHAO R F.Label Enhancement for Label Distribution Learning.Journal of Computer Research and Development,2017,54(6):1171-1184.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201706004&amp;v=MjI2MzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rV3IzT0x5dlNkTEc0SDliTXFZOUZZSVE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         耿新,徐宁,邵瑞枫.面向标记分布学习的标记增强.计算机研究与发展,2017,54(6):1171-1184.(GENG X,XU N,SHAO R F.Label Enhancement for Label Distribution Learning.Journal of Computer Research and Development,2017,54(6):1171-1184.)
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_3" title=" GENG X.Label Distribution Learning.IEEE Transactions on Know-ledge and Data Engineering,2016,28(7):1734-1748." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Label Distribution Learning&amp;quot;">
                                        <b>[3]</b>
                                         GENG X.Label Distribution Learning.IEEE Transactions on Know-ledge and Data Engineering,2016,28(7):1734-1748.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_4" >
                                        <b>[4]</b>
                                     GENG X,YIN C,ZHOU Z H.Facial Age Estimation by Learning from Label Distributions.IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(10):2401-2412.</a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_5" title=" HE Z Z,LI X,ZHANG Z F,et al.Data-Dependent Label Distribution Learning for Age Estimation.IEEE Transactions on Image Processing,2017,26(8):3846-3858." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data-dependent label distribution learning for age estimation">
                                        <b>[5]</b>
                                         HE Z Z,LI X,ZHANG Z F,et al.Data-Dependent Label Distribution Learning for Age Estimation.IEEE Transactions on Image Processing,2017,26(8):3846-3858.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_6" title=" ZHANG Z X,WANG M,GENG X.Crowd Counting in Public Vi-deo Surveillance by Label Distribution Learning.Neurocomputing,2015,166:151-163." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES84F83F3FDFD2CC6FF08C9A84EDB4B9BF&amp;v=MDI5MzEzNU5GaHc3bTJ3cTQ9TmlmT2ZidThhTm5QMll3ekVKMTdEZzlLeVdCbDZqY09RUTdxcUdkQkM3Ym1UTWpwQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         ZHANG Z X,WANG M,GENG X.Crowd Counting in Public Vi-deo Surveillance by Label Distribution Learning.Neurocomputing,2015,166:151-163.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_7" title=" GAO B B,XING C,XIE C W,et al.Deep Label Distribution Learning with Label Ambiguity.IEEE Transactions on Image Processing,2017,26(6):2825-2838." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Label Distribution Learning with Label Ambiguity">
                                        <b>[7]</b>
                                         GAO B B,XING C,XIE C W,et al.Deep Label Distribution Learning with Label Ambiguity.IEEE Transactions on Image Processing,2017,26(6):2825-2838.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_8" title=" ZHOU Y,XUE H,GENG X.Emotion Distribution Recognition from Facial Expressions // Proc of the 23rd ACM Conference on Multimedia.New York,USA:ACM,2015:1247-1250." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Emotion Distribution Recognition from Facial Expressions">
                                        <b>[8]</b>
                                         ZHOU Y,XUE H,GENG X.Emotion Distribution Recognition from Facial Expressions // Proc of the 23rd ACM Conference on Multimedia.New York,USA:ACM,2015:1247-1250.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_9" title=" 赵权,耿新.标记分布学习中目标函数的选择.计算机科学与探索,2017,11(5):708-719.(ZHAO Q,GENG X.Selection of Target Function in Label Distribution Learning.Journal of Frontiers of Computer Science and Technology,2017,11(5):708-719.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXTS201705004&amp;v=MjI1NjBMalhmZmJHNEg5Yk1xbzlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1dyM08=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         赵权,耿新.标记分布学习中目标函数的选择.计算机科学与探索,2017,11(5):708-719.(ZHAO Q,GENG X.Selection of Target Function in Label Distribution Learning.Journal of Frontiers of Computer Science and Technology,2017,11(5):708-719.)
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_10" title=" 崔颖,徐凯,陆忠军,等.主动学习策略融合算法在高光谱图像分类中的应用.通信学报,2018,39(4):91-99.(CUI Y,XU K,LU Z J,et al.Combination Strategy of Active Learning for Hyperspectral Images Classification.Journal on Communications,2018,39(4):91-99.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB201804010&amp;v=MDExMTZxQnRHRnJDVVJMT2VaZVJuRnkva1dyM09NVFhUYkxHNEg5bk1xNDlFWklRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         崔颖,徐凯,陆忠军,等.主动学习策略融合算法在高光谱图像分类中的应用.通信学报,2018,39(4):91-99.(CUI Y,XU K,LU Z J,et al.Combination Strategy of Active Learning for Hyperspectral Images Classification.Journal on Communications,2018,39(4):91-99.)
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_11" title=" TSOUMAKAS G,DIMOU A,SPYROMITROS E,et al.Correlation-Based Pruning of Stacked Binary Relevance Models for Multi-label Learning // Proc of the 1st International Workshop on Lear-ning from Multi-label Data.Berlin,Germany:Springer,2009:101-116." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Correlation-Based Pruning of Stacked Binary Relevance Models for Multi-label Learning">
                                        <b>[11]</b>
                                         TSOUMAKAS G,DIMOU A,SPYROMITROS E,et al.Correlation-Based Pruning of Stacked Binary Relevance Models for Multi-label Learning // Proc of the 1st International Workshop on Lear-ning from Multi-label Data.Berlin,Germany:Springer,2009:101-116.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_12" title=" 张笑铭,王志君,梁利平.一种适用于卷积神经网络的Stacking算法.计算机工程,2018,44(4):243-247.(ZHANG X M,WANG Z J,LIANG L P.A Stacking Algorithm for Convolution Neural Network.Computer Engineering,2018,44(4):243-247.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201804039&amp;v=MTYwMTgzT0x6N0JiYkc0SDluTXE0OUdiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rV3I=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         张笑铭,王志君,梁利平.一种适用于卷积神经网络的Stacking算法.计算机工程,2018,44(4):243-247.(ZHANG X M,WANG Z J,LIANG L P.A Stacking Algorithm for Convolution Neural Network.Computer Engineering,2018,44(4):243-247.)
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_13" title=" 周星,丁立新,万润泽,等.分类器集成算法研究.武汉大学学报(理学版),2015,61(6):503-508.(ZHOU X,DING L X,WAN R Z,et al.Research on Classifier Ensemble Algorithms.Journal of Wuhan University(Natural Science Edition),2015,61(6):503-508.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WHDY201506001&amp;v=MTI4MzJGeS9rV3IzT01pWFBkN0c0SDlUTXFZOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         周星,丁立新,万润泽,等.分类器集成算法研究.武汉大学学报(理学版),2015,61(6):503-508.(ZHOU X,DING L X,WAN R Z,et al.Research on Classifier Ensemble Algorithms.Journal of Wuhan University(Natural Science Edition),2015,61(6):503-508.)
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_14" title=" 李巧,周双娥,杨晶.模型融合在用户续购行为分析中的应用.小型微型计算机系统,2017,38(10):2231-2235.(LI Q,ZHOU S E,YANG J.Application of Model Blending in User Renewal Behavior Analysis.Journal of Chinese Computer Systems,2017,38(10):2231-2235.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201710012&amp;v=MDUxNDVMT2VaZVJuRnkva1dyM09QVFhjZHJHNEg5Yk5yNDlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         李巧,周双娥,杨晶.模型融合在用户续购行为分析中的应用.小型微型计算机系统,2017,38(10):2231-2235.(LI Q,ZHOU S E,YANG J.Application of Model Blending in User Renewal Behavior Analysis.Journal of Chinese Computer Systems,2017,38(10):2231-2235.)
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_15" title=" 傅艺绮,董威,尹良泽,等.基于组合机器学习算法的软件缺陷预测模型.计算机研究与发展,2017,54(3):633-641.(FU Y Q,DONG W,YIN L Z,et al.Software Defect Prediction Model Based on the Combination of Machine Learning Algorithms.Journal of Computer Research and Development,2017,54(3):633-641.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201703018&amp;v=MDM3NjNVUkxPZVplUm5GeS9rV3IzT0x5dlNkTEc0SDliTXJJOUViSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         傅艺绮,董威,尹良泽,等.基于组合机器学习算法的软件缺陷预测模型.计算机研究与发展,2017,54(3):633-641.(FU Y Q,DONG W,YIN L Z,et al.Software Defect Prediction Model Based on the Combination of Machine Learning Algorithms.Journal of Computer Research and Development,2017,54(3):633-641.)
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_16" title=" WOLPERT D H.Stacked Generalization.Neural Networks,1992,5(2):241-259." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked generalization">
                                        <b>[16]</b>
                                         WOLPERT D H.Stacked Generalization.Neural Networks,1992,5(2):241-259.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_17" title=" LIN Y J,LI Y W,WANG C X,et al.Attribute Reduction for Multi-label Learning with Fuzzy Rough Set.Knowledge-Based Systems,2018,152:51-61." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES22A4CF21D5805EE3B00914C749BDE4F4&amp;v=MzEyMzFoUWN5YkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHc3bTJ3cTQ9TmlmT2ZiRzZiOVcvMlkxRUVPNEhESGxNdWhWaDZqOTBTWHVScXhZOEM4Yg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         LIN Y J,LI Y W,WANG C X,et al.Attribute Reduction for Multi-label Learning with Fuzzy Rough Set.Knowledge-Based Systems,2018,152:51-61.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(10),945-954 DOI:10.16451/j.cnki.issn1003-6059.201910009            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于标记分布学习的异态集成学习算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E4%B8%80%E5%AE%BE&amp;code=35637665&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王一宾</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%94%B0%E6%96%87%E6%B3%89&amp;code=40673507&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">田文泉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A8%8B%E7%8E%89%E8%83%9C&amp;code=35637663&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">程玉胜</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%AE%89%E5%BA%86%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=1699539&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安庆师范大学计算机与信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%AE%89%E5%BA%86%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E5%AE%89%E5%BE%BD%E7%9C%81%E9%AB%98%E6%A0%A1%E6%99%BA%E8%83%BD%E6%84%9F%E7%9F%A5%E4%B8%8E%E8%AE%A1%E7%AE%97%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安庆师范大学安徽省高校智能感知与计算重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了提高预测的准确性,文中结合机器学习中堆积(Stacking)集成框架,组合多个分类器对标记分布进行学习,提出基于标记分布学习的异态集成学习算法(HELA-LDL).算法构造两层模型框架,通过第一层结构将样本数据采用组合方式进行异态集成学习,融合各分类器的学习结果,将融合结果输入到第二层分类器,预测结果是带有置信度的标记分布.在专用数据集上的对比实验表明,HELA-LDL可以发挥各种算法在不同场景下的性能较优,稳定性分析进一步说明算法的有效性.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%87%E8%AE%B0%E5%88%86%E5%B8%83%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">标记分布学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%82%E6%80%81%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">异态集成学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%9E%E5%BD%92%E6%8B%9F%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">回归拟合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A0%86%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">堆积;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王一宾,硕士,教授,主要研究方向为多标记学习、机器学习、软件安全．E-mail:wangyb07@mail.ustc.edu.cn.&lt;image id="195" type="formula" href="images/MSSB201910011_19500.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    田文泉,硕士研究生,主要研究方向为机器学习、大数据、数据统计．E-mail:tianwq16@126.com.&lt;image id="196" type="formula" href="images/MSSB201910011_19600.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    *程玉胜(通讯作者),博士,教授,主要研究方向为大数据、粗糙集、特征选择的机器学习．E-mail:Chengyshaq@163.com.&lt;image id="197" type="formula" href="images/MSSB201910011_19700.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>安徽省高校重点科研项目(No.KJ2017A352);</span>
                                <span>安徽省高校重点实验室基金项目(No.ACAIM160102)资助;</span>
                    </p>
            </div>
                    <h1><b>Heterogeneous Ensemble Learning Algorithm Based on Label Distribution Learning</b></h1>
                    <h2>
                    <span>WANG Yibin</span>
                    <span>TIAN Wenquan</span>
                    <span>CHENG Yusheng</span>
            </h2>
                    <h2>
                    <span>School of Computer and Information,Anqing Normal University</span>
                    <span>University Key Laboratory of Intelligent Perception and Computing of Anhui Province,Anqing Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To improve prediction accuracy, a stacking integration framework in machine learning is employed to learn label distribution through multiple classifiers, and a heterogeneous ensemble learning algorithm based on label distribution learning(HELA-LDL) is proposed. A two-layer model framework is constructed, and the sample data are combined through the first layer structure to integrate the learning results of each classifier. Finally, the fusion results are input to the second layer classifier as the original feature, and the labels are predicted to be a label distribution with confidence. Comparative experiments on specialized datasets show that HELA-LDL is superior to other algorithms in various scenes. The stability analysis further illustrates the effectiveness of HELA-LDL.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Label%20Distribution%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Label Distribution Learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Heterogeneous%20Ensemble%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Heterogeneous Ensemble Learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Regression%20Fitting&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Regression Fitting;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Stacking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Stacking;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Yibin,master,professor. His research interests include multi-label learning, machine learning and software security.;
                                </span>
                                <span>
                                    TIAN Wenquan,master student. His research interests include machine learning,big data and data statistics.;
                                </span>
                                <span>
                                    CHENG Yusheng(Corresponding author), Ph. D.,professor. His research interests include big data,rough sets and machine learning for feature selection.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-28</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by Natural Science Research Funds of Education Department of Anhui Province(No.KJ2017A352);</span>
                                <span>Foundation Project of Key Laboratories of Anhui Province(No.ACAIM160102);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="47">在标记学习中,一个示例可能具有多个标记<citation id="234" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,并且不同标记对于同一个实例的描述程度并不都相同,有一定的差别.针对标记存在主次等问题,耿新等<citation id="235" type="reference"><link href="202" rel="bibliography" /><link href="204" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>提出标记分布学习算法,使用类似概率的形式对传统的-1和1标记进行描述,对示例的标记分布情况进行准确直观表示.</p>
                </div>
                <div class="p1">
                    <p id="48">标记分布学习在各领域发挥广泛作用<citation id="237" type="reference"><link href="206" rel="bibliography" /><link href="208" rel="bibliography" /><link href="210" rel="bibliography" /><link href="212" rel="bibliography" /><link href="214" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>,目前学者们已经提出问题转化贝叶斯(Problem Transformation Bayes, PT-Bayes)、问题转化支持向量机(Problem Transformation Support Vector Machine, PT-SVM)、算法适应<i>K</i>近邻(Algorithm Adaptation <i>K</i> Nearest Neighbors, AA-KNN)、算法适应反向传播(Algorithm Adaptation Backpropagation, AA-BP)等应用算法.同时针对标记分布学习提出改进迭代尺度专用算法(Specialized Algorithms Improved Iterative Scaling, SA-IIS),利用迭代求解直接解决标记分布问题<citation id="236" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>.上述具有代表性的5种学习算法都是通过单个模型完成标记学习.由于算法处理策略的不同,在不同数据集上预测精度存在较大差异.</p>
                </div>
                <div class="p1">
                    <p id="49">为了提高准确率,现有机器学习大都通过选择合适的分类器以获得较优效果,但不同的分类器由于方法各异导致分类效果不尽相同.如何利用各分类器在不同评价指标上的各种优势,保持分类器多样性、提高算法精度及泛化能力是研究重点.</p>
                </div>
                <div class="p1">
                    <p id="50">学者们考虑到不同算法的各种优势,研究基分类器融合的方法,组合多个分类器以获得较优性能.崔颖等<citation id="238" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>基于集成学习引入分类器加权组合思想,提高分类器泛化能力.Tsoumakas等<citation id="239" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出多标记的堆积(Stacking)分类方法,研究对基分类器的融合方法,提高模型整体性能.张笑铭等<citation id="240" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出适用于卷积神经网络的Stacking算法,将多个卷积神经网络作为基分类器,提高分类精度.周星等<citation id="241" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>以三种常用的集成方法,从多角度研究分类器算法.李巧等<citation id="242" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>根据集成学习的思想,从模型融合角度,研究提高准确率和鲁棒性的方法.傅艺绮等<citation id="243" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出组合机器学习算法,有效提高软件缺陷预测性能.但是,标记分布的标记值是连续型变量,目前还无相关的异态集成研究算法.</p>
                </div>
                <div class="p1">
                    <p id="51">基于此种情况,本文组合多个分类器,重点研究在不同分类器异态集成下提高标记分布学习准确率的方法,提出基于标记分布的异态集成学习算法(Heterogeneous Ensemble Learning Algorithm Based on Label Distribution Learning, HELA-LDL).首先结合Stacking思想构造两层模型框架,选择<i>K</i>近邻(<i>K</i> Nearest Neighbors, KNN)、线性回归(Linear Regre-ssion, LR)、随机森林(Random Forest, RF)和梯度提升树(Gradient Boosting Decision Tree, GBDT)作为第一层学习器,RF作为第二层分类器.然后将样本数据集分为训练集和测试集,通过第一层多种学习算法在相同训练集上进行训练预测,预测结果作为中间特征融合输出.最后将中间特征输入到第二层分类器,得出最终结果.采用不同分类器组合的方式可以提高算法整体的精确度,预测结果为类概率形式,使输出数据是带有置信度的预测值,符合标记分布范式.为了验证算法,先对比异态集成算法与组成它的基分类器,再通过专用化数据集与多种实验算法进行对比,结果表明,本文算法在多个标记分布评价指标上均取得较优效果.</p>
                </div>
                <h3 id="52" name="52" class="anchor-tag">1 标记分布学习</h3>
                <div class="p1">
                    <p id="53">标记分布学习是以概率的形式将传统标记-1和1进行转化的学习范式.对于一个样本标记集合,集合中的每个分量表示标记对样本示例描述的程度大小.</p>
                </div>
                <div class="p1">
                    <p id="54">对于一个示例集合<i>x</i>,标记集合<i>y</i>,描述度<i>d</i><mathml id="167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>x</mi><mi>y</mi></msubsup></mrow></math></mathml>表示例<i>x</i>中标记分量<i>y</i>占所有示例标记的比重,值介于0和1之间.标记分布学习通过训练集<i>S</i>建立<mathml id="168"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math></mathml>函数模型,<i>d</i><mathml id="169"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>x</mi><mi>y</mi></msubsup></mrow></math></mathml>满足的条件和概率分布一致,即</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msubsup><mrow></mrow><mi>x</mi><mi>y</mi></msubsup><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo><mo>,</mo><mspace width="0.25em" /><mstyle displaystyle="true"><munder><mo>∑</mo><mi>y</mi></munder><mi>d</mi></mstyle><msubsup><mrow></mrow><mi>x</mi><mi>y</mi></msubsup><mo>=</mo><mn>1</mn><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56">标记分布学习可以完成标记数值的相互转化.相比传统学习模型,泛化程度更高,在标记学习中应用更广泛.</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag">2 集成学习</h3>
                <h4 class="anchor-tag" id="58" name="58"><b>2.1</b> 集成学习理论</h4>
                <div class="p1">
                    <p id="59">集成学习最初来自于组合预测理论,通过合理组合多个基分类器,集成为一个整体学习系统,达到提高模型预测精度的目的.异态集成学习综合各种优点,具有比单分类器更高的精确性、鲁棒性及泛化性能.常见的集成算法有平均法、投票法、学习法等.</p>
                </div>
                <div class="p1">
                    <p id="60">在二分类研究中,假设有<i>N</i>个相互独立的基分类器,每个分类器<i>C</i><sub><i>m</i></sub>的错误率为<i>ε</i>,<i>m</i>为第<i>m</i>个分类器,则分类问题与真实函数<i>f</i>之间的关系表示为</p>
                </div>
                <div class="p1">
                    <p id="61"><i>p</i>(<i>C</i><sub><i>m</i></sub>(<i>x</i>)≠<i>f</i>(<i>x</i>))=<i>ε</i>.</p>
                </div>
                <div class="p1">
                    <p id="62">如果采用投票法集成多个模型,集成模型的错误率为</p>
                </div>
                <div class="area_img" id="63">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201910011_06300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="65">当每个独立的分类器错误率<i>ε</i>都小于0.5时,集成分类器的错误率随着<i>N</i>的增大而降低,最终趋于0.</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66"><b>2.2</b> 基分类器选择</h4>
                <div class="p1">
                    <p id="67">为了提高算法模型的精确度,基分类器的选择具有2个原则:1)分类器具有多样性,保证异态集成分类器之间存在一定的差异性;2)保证分类器的准确性,分类效果优于随机预测.</p>
                </div>
                <div class="p1">
                    <p id="68">标记分布是一种类似概率分布的形式,对标记分布的预测可以采用回归函数拟合概率分布模型.集成学习可以通过对数据集的共同学习建立逼近样本真实分布的模型,因此本文采用KNN、RF、LR和GBDT分类器进行异态集成学习.通过4种不同策略在原始数据集的共同学习,将原始特征转化为高级特征.四种分类器算法的说明如下.</p>
                </div>
                <div class="p1">
                    <p id="69">KNN为基于示例之间距离的学习,在标记分布学习中,数据标签是连续的,可以使用基于邻居的回归,即对于示例<i>x</i>,最近<i>k</i>个邻居的平均标记分布作为分布的预测值.</p>
                </div>
                <div class="p1">
                    <p id="70">2个示例<i><b>x</b></i><sub><i>i</i></sub>=(<i>x</i><sub><i>i</i></sub><sub>1</sub>,<i>x</i><sub><i>i</i></sub><sub>2</sub>,…,<i>x</i><sub><i>in</i></sub>),<i><b>x</b></i><sub><i>j</i></sub>=(<i>x</i><sub><i>j</i></sub><sub>1</sub>,<i>x</i><sub><i>j</i></sub><sub>2</sub>,…,<i>x</i><sub><i>jn</i></sub>)的欧几里得距离为</p>
                </div>
                <div class="p1">
                    <p id="71"><mathml id="170"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mroot><mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow></msub><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mtext> </mtext></mroot></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="72">计算示例<i><b>x</b></i>邻居的平均标记分布数值:</p>
                </div>
                <div class="p1">
                    <p id="73"><mathml id="171"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>k</mi></mfrac><mstyle displaystyle="true"><mo>∑</mo><mi>d</mi></mstyle><msubsup><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msubsup><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>c</mi></mrow></math></mathml>,</p>
                </div>
                <div class="p1">
                    <p id="74">其中<i>c</i>为标记数目.</p>
                </div>
                <div class="p1">
                    <p id="75">LR为分析各变量之间关系的分析方法,函数关系如下.在标记分布学习中,利用特征及标记之间的相关性,建立线性回归模型:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>y</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ω</mi><mo>,</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>ω</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>ω</mi><msub><mrow></mrow><mn>1</mn></msub><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>ω</mi><msub><mrow></mrow><mi>n</mi></msub><mi>x</mi><msub><mrow></mrow><mi>n</mi></msub><mo>,</mo></mtd></mtr><mtr><mtd><mi>y</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ω</mi><mo>,</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold-italic">ω</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">x</mi><mo>=</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><mo>,</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">X</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>x</mi><msub><mrow></mrow><mi>n</mi></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>,</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">W</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mn>0</mn></msub></mtd></mtr><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mn>1</mn></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mi>n</mi></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">线性回归求解系数为<i>ω</i>=(<i>ω</i><sub>1</sub>,<i>ω</i><sub>2</sub>,…,<i>ω</i><sub><i>n</i></sub>)<sup>T</sup>的线性模型,就是解决<mathml id="172"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>min</mi></mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">ω</mi><mo>-</mo><mi mathvariant="bold-italic">y</mi></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>问题.求解得</p>
                </div>
                <div class="p1">
                    <p id="78"><i>ω</i>=(<i><b>x</b></i><sup>T</sup><i><b>x</b></i>)<sup>-1</sup><i><b>x</b></i><sup>T</sup><i><b>y</b></i>.</p>
                </div>
                <div class="p1">
                    <p id="79">在标记分布中设数据集<i>S</i>={(<i><b>x</b></i><sub><i>i</i></sub>,<i><b>D</b></i><sub><i>i</i></sub>)}<mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>,其中<i><b>x</b></i><sub><i>i</i></sub>为<i>n</i>维特征向量,<i><b>D</b></i><sub><i>i</i></sub>为分布式标记,通过已知分布求解<i>ω</i>,预测输出标记分布,则标记分布的最小化目标函数:</p>
                </div>
                <div class="p1">
                    <p id="80"><mathml id="174"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>i</mi><mi>Ν</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="81">LR用于建立分析特征与标记之间关系,针对标记分布的特点,LR具有较快的速度、较高的准确度和鲁棒性.</p>
                </div>
                <div class="p1">
                    <p id="82">RF在处理标记分布数据时,首先采用随机可重复取样的方式,抽取子样本.然后在子样本数据中,随机抽取部分特征建立决策树模型.假设随机森林中有<i>M</i>棵决策树,每棵树学习得到的模型为<i>f</i><sub><i>m</i></sub>,则在标记分布学习中,给定样本数据集<i>S</i>={(<i><b>x</b></i><sub><i>i</i></sub>,<i><b>D</b></i><sub><i>i</i></sub>)}<mathml id="175"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>,对于未知样本<i><b>x</b></i>′的预测模型:</p>
                </div>
                <div class="p1">
                    <p id="83"><mathml id="176"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo>=</mo><mfrac><mn>1</mn><mi>Μ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>f</mi></mstyle><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="84">随着学习器数目的增加,RF可以取得较小的泛化误差,产生较高准确度的分类器,并且能够处理多分类问题,在处理标记分布数据集,尤其当特征数目较大时具有较优结果.</p>
                </div>
                <div class="p1">
                    <p id="85">GBDT由多个分类器经过迭代形成强分类器的组合决策树模型,对所有决策树的输出进行累加,在当前模型基础上拟合回归树.</p>
                </div>
                <div class="p1">
                    <p id="86">设含有<i>N</i>个样本的数据集<i>S</i>={(<i><b>x</b></i><sub><i>i</i></sub>,<i><b>D</b></i><sub><i>i</i></sub>)}<mathml id="177"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>,其中<i><b>x</b></i><sub><i>i</i></sub>为<i>n</i>维特征向量,<i><b>D</b></i><sub><i>i</i></sub>为与示例<i><b>x</b></i><sub><i>i</i></sub>的分布式标记,损失函数<i>L</i>(<i><b>y</b></i>,<i>F</i>(<i><b>x</b></i>))可微,决策树的个数为<i>M</i>.</p>
                </div>
                <div class="p1">
                    <p id="87">使用常量<i>γ</i><sub>0</sub>初始化模型:</p>
                </div>
                <div class="p1">
                    <p id="88"><mathml id="178"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>arg</mi></mrow><mspace width="0.25em" /><mrow><mi>min</mi></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>L</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">γ</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></math></mathml>,</p>
                </div>
                <div class="p1">
                    <p id="89">对于<i>m</i>=1,2,…,<i>M</i>,残差</p>
                </div>
                <div class="p1">
                    <p id="90"><mathml id="179"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>m</mi></mrow></msub><mo>=</mo><mo>-</mo><mrow><mo stretchy="false">[</mo><mfrac><mrow><mo>∂</mo><mi>L</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo></mrow><msub><mrow></mrow><mrow><mi>F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo></mrow></msub><mo>,</mo><mspace width="0.25em" /><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>n</mi></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="91">根据训练集构建弱分类器<i><b>h</b></i><sub><i><b>m</b></i></sub>(<i><b>x</b></i>),计算乘子</p>
                </div>
                <div class="p1">
                    <p id="92"><mathml id="180"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">γ</mi><msub><mrow></mrow><mi>m</mi></msub><mo>=</mo><mrow><mi>arg</mi></mrow><mspace width="0.25em" /><mrow><mi>min</mi></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>L</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>γ</mi><mi>h</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="93">引入学习率<i>υ</i>(0&lt;<i>υ</i>&lt;1)提高算法的泛化能力,修改后的更新规则为</p>
                </div>
                <div class="p1">
                    <p id="94"><i>F</i><sub><i>m</i></sub>(<i><b>x</b></i>)=<i>F</i><sub><i>m</i></sub><sub>-1</sub>(<i><b>x</b></i>)+<i>υγ</i><sub><i>m</i></sub><i>h</i><sub><i>m</i></sub>(<i><b>x</b></i>).</p>
                </div>
                <div class="p1">
                    <p id="95">GBDT将弱分类器组合成强分类器,沿着梯度下降的方向构建新模型,不断更新,使模型最终趋于稳定.在处理标记分布学习时,可以有效处理数据中连续的特征,并充分分析考虑特征间的相互影响,最终输出预测结果.</p>
                </div>
                <h3 id="96" name="96" class="anchor-tag">3 基于标记分布的异态集成学习算法</h3>
                <div class="p1">
                    <p id="98">在标记学习中,不同算法在各评价指标下具有不同优势,构建一个多分类器系统模型能够适应更广泛的场景.异态集成算法就是充分利用不同算法的优势,形成一个强大的框架.</p>
                </div>
                <div class="p1">
                    <p id="99">利用集成技术构造标记学习模型能够改善模型的预测精度和泛化能力,本文利用Stacking思想构建标记分布学习模型.学习框架利用不同的基分类器组合学习,提高模型预测精度.</p>
                </div>
                <div class="p1">
                    <p id="100">分类器集成的构建主要有2部分:生成基分类器和合并结果.一方面保证异态集成学习分类器的多样性,选择KNN、LR、RF、GBDT分类器方法训练样本,获得一组基分类器,另一方面组合信息的表示.由于标记分布具有类似概率的特点,将多个分类器的结果融合训练,使输出的标记为类概率形式,可得出更符合示例描述程度的分布,结果更准确.具体流程图如图1所示.</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910011_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 HELA-LDL流程图" src="Detail/GetImg?filename=images/MSSB201910011_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 HELA-LDL流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910011_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Flow chart of HELA-LDL</p>

                </div>
                <div class="p1">
                    <p id="102">算法分为两层分类器模型结构:第一层结构中各分类器对数据进行学习,各算法共同学习的结果是原始数据经过基分类器学习的高级特征,决定最终实验预测结果的精度;第二层结构学习高级特征,预测输出分布.</p>
                </div>
                <div class="p1">
                    <p id="103">对于第一层模型,首先将原始标记分布数据集分为训练集和测试集,对训练集随机分成五份,一份作为测试部分,其余四份为训练部分.然后使用KNN、LR、RF和GBDT分类器在相同数据集上进行训练,生成多个基分类器.</p>
                </div>
                <div class="p1">
                    <p id="104">使用基分类器模型对测试部分进行预测,生成数据集traindata 1,完成第一份数据的预测工作.同时预测测试集生成testdata 1.重复5次分别生成traindata和对应的testdata.合并traindata数据,得出训练集经过特征学习后的融合数据,记为融合训练集.将testdata数据集合并取平均,得出测试集经过特征学习后的融合数据,记为融合测试集.第一层结构的特征学习工作完成.</p>
                </div>
                <div class="p1">
                    <p id="105">第一层输出作为中间特征输入到第二层结构,第二层结构选择RF.融合训练集并建立随机森林回归模型,预测时将随机森林中各个树取平均值即为最终的预测标记值.</p>
                </div>
                <div class="p1">
                    <p id="106">Stacking可以保证集成后模型整体性能提高.采用类概率分布作为新数据的属性,使输出数据具有置信度的预测值.异态集成学习能综合各分类器的优点,使模型更符合标记分布范式.</p>
                </div>
                <div class="p1">
                    <p id="107">基于标记分布的异态集成学习算法(HELA-LDL)步骤如下.</p>
                </div>
                <div class="area_img" id="198">
                                <img alt="" src="Detail/GetImg?filename=images/MSSB201910011_19800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="127" name="127" class="anchor-tag">4 实验及结果分析</h3>
                <h4 class="anchor-tag" id="128" name="128"><b>4.1</b> 实验数据集和评价指标</h4>
                <div class="p1">
                    <p id="129">Yeast数据集系列是关于酿酒酵母菌不同时间点基因表达水平的记录.s-JAFFE数据集和s-BU_3DFE数据集为不同表情数据.Natural scene数据集为自然场景图像.Movie数据集为关于电影的5个评分级别.</p>
                </div>
                <div class="area_img" id="130">
                    <p class="img_tit"><b>表1 实验数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Experimental datasets</p>
                    <p class="img_note"></p>
                    <table id="130" border="1"><tr><td><br />名称</td><td>样本数</td><td>特征数</td><td>标记数</td></tr><tr><td><br />Yeast-alpha</td><td>2465</td><td>24</td><td>18</td></tr><tr><td><br />Yeast-elu</td><td>2465</td><td>24</td><td>14</td></tr><tr><td><br />Yeast-diau</td><td>2465</td><td>24</td><td>7</td></tr><tr><td><br />Yeast-heat</td><td>2465</td><td>24</td><td>6</td></tr><tr><td><br />Yeast-spo</td><td>2465</td><td>24</td><td>6</td></tr><tr><td><br />Yeast-cold</td><td>2465</td><td>24</td><td>4</td></tr><tr><td><br />Yeast-dtt</td><td>2465</td><td>24</td><td>4</td></tr><tr><td><br />Yeast-spo5</td><td>2465</td><td>24</td><td>3</td></tr><tr><td><br />s-JAFFE</td><td>213</td><td>243</td><td>6</td></tr><tr><td><br />s-BU_3DFE</td><td>2500</td><td>243</td><td>6</td></tr><tr><td><br />Natural Scene</td><td>2000</td><td>294</td><td>9</td></tr><tr><td><br />Movie</td><td>7755</td><td>1869</td><td>5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="131">标记分布算法预测结果是概率分布形式,对预测结果可以使用距离指标和相似度指标进行度量.选择6种典型评价指标,分别是距离指标(Chebyshev、Clark、Canberra、Kullback-Leibler)及相似度指标(Cosine、Intersection).预测分布与真实分布的距离越小、相似度越大,说明算法性能越优.表2中箭头↑表示表中数据越大越好,↓表示表中数据越小越好.</p>
                </div>
                <div class="area_img" id="132">
                    <p class="img_tit"><b>表2 评价指标</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Evaluation indexes</p>
                    <p class="img_note"></p>
                    <table id="132" border="1"><tr><td><br />评价指标</td><td>计算公式</td></tr><tr><td><br />Chebyshev↓</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>i</mi><mi>s</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><mo stretchy="false">)</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>j</mi></munder><mo stretchy="false">|</mo><mi>d</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>︿</mo></mover></mrow><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo></mrow></math></td></tr><tr><td><br /><i>Clark</i>↓</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>i</mi><mi>s</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mroot><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mfrac><mrow><mo stretchy="false">(</mo><mi>d</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>︿</mo></mover></mrow><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mo stretchy="false">(</mo><mi>d</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>︿</mo></mover></mrow><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></mstyle></mrow><mtext> </mtext></mroot></mrow></math></td></tr><tr><td><br /><i>Canberra</i>↓</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>i</mi><mi>s</mi><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mfrac><mrow><mo stretchy="false">|</mo><mi>d</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>︿</mo></mover></mrow><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo></mrow><mrow><mi>d</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>︿</mo></mover></mrow><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow></mstyle></mrow></math></td></tr><tr><td><br /><i>Kullback</i>-<i>Leibler</i>↓</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>i</mi><mi>s</mi><msub><mrow></mrow><mn>4</mn></msub><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>d</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mrow><mi>ln</mi></mrow><mo stretchy="false">(</mo><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>︿</mo></mover></mrow><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mo stretchy="false">)</mo></mrow></math></td></tr><tr><td><br /><i>Cosine</i>↑</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>S</mi><mi>i</mi><mi>m</mi></mrow><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>d</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>︿</mo></mover></mrow><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mroot><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>d</mi></mstyle><msubsup><mrow></mrow><mi>j</mi><mn>2</mn></msubsup></mrow><mtext> </mtext></mroot><mroot><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>︿</mo></mover></mrow></mstyle><msub><mrow></mrow><mi>j</mi></msub><msup><mrow></mrow><mn>2</mn></msup></mrow><mtext> </mtext></mroot></mrow></mfrac></mrow></math></td></tr><tr><td><br /><i>Intersection</i>↑</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>i</mi><mi>m</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mi>min</mi></mrow></mstyle><mo stretchy="false">(</mo><mi>d</mi><msub><mrow></mrow><mi>j</mi></msub><mo>,</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>d</mi></mstyle><mo>︿</mo></mover></mrow><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></math></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="133" name="133"><b>4.2</b> 异态集成算法实验结果</h4>
                <div class="p1">
                    <p id="134">表3为在s-JAFFE数据集上采用两个分类器进行组合的实验结果对比.表4为在s-JAFFE数据集上采用三个分类器进行组合的实验结果对比.采用不同分类器的组合算法在各评价指标上最优值使用黑体数字表示.</p>
                </div>
                <div class="area_img" id="135">
                    <p class="img_tit"><b>表3 两个分类器组合在s-JAFFE数据集上的测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Test results of 2 classifiers on s-JAFFE dataset</p>
                    <p class="img_note"></p>
                    <table id="135" border="1"><tr><td>评价指标</td><td>RF+LR</td><td>RF+GBDT</td><td>RF+KNN</td><td>LR+GBDT</td><td>LR+KNN</td><td>GBDT+KNN</td></tr><tr><td><br />Chebyshev↓</td><td>0.1349</td><td>0.1211</td><td>0.1200</td><td><b>0.1124</b></td><td>0.1217</td><td>0.1204</td></tr><tr><td><br />Clark↓</td><td>0.4856</td><td>0.4473</td><td>0.4247</td><td>0.4112</td><td>0.4304</td><td><b>0.3968</b></td></tr><tr><td><br />Canberra↓</td><td>0.9353</td><td>0.8580</td><td>0.8262</td><td>0.8137</td><td>0.8602</td><td><b>0.7996</b></td></tr><tr><td><br />Cosine↑</td><td>0.9408</td><td>0.9383</td><td>0.9414</td><td><b>0.9569</b></td><td>0.9422</td><td>0.9537</td></tr><tr><td><br />Intersection↑</td><td>0.8607</td><td>0.8538</td><td>0.8376</td><td>0.8578</td><td>0.8548</td><td><b>0.8649</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="136">
                    <p class="img_tit"><b>表4 三个分类器组合在s-JAFFE数据集上的测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Test results of 3 classifiers on s-JAFFE dataset</p>
                    <p class="img_note"></p>
                    <table id="136" border="1"><tr><td><br />评价指标</td><td>RF+LR+GBDT</td><td>RF+LR+KNN</td><td>RF+GBDT+KNN</td><td>KNN+LR+GBDT</td></tr><tr><td><br />Chebyshev↓</td><td><b>0.1061</b></td><td>0.1203</td><td>0.1122</td><td>0.1119</td></tr><tr><td><br />Clark↓</td><td>0.4119</td><td>0.4135</td><td>0.4111</td><td><b>0.4065</b></td></tr><tr><td><br />Canberra↓</td><td><b>0.7847</b></td><td>0.8108</td><td>0.8153</td><td>0.7908</td></tr><tr><td><br />Cosine↑</td><td>0.9471</td><td>0.9388</td><td>0.9486</td><td><b>0.9498</b></td></tr><tr><td><br />Intersection↑</td><td>0.8638</td><td>0.8570</td><td><b>0.8782</b></td><td>0.8675</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="137">通过表3和表4的分析对比可知,相比两个分类器组合,采用三个分类器组合的方式在各指标上具有一定优势.由表3和表4可知,在Chebyshev、Canberra和Cosine指标上,以LR和GBDT组合的分类器实验效果较好.</p>
                </div>
                <div class="p1">
                    <p id="138">在Clark和Intersection指标上采用GBDT和KNN进行组合,效果较好.实验表明在s-JAFFE数据集上,加入GBDT的组合算法可以有效提高模型的预测精度,GBDT与不同算法的组合可以在各评价指标上发挥良好效果.</p>
                </div>
                <h4 class="anchor-tag" id="139" name="139"><b>4.3</b> 对比算法实验结果</h4>
                <div class="p1">
                    <p id="140">在12个数据集上对比5种算法,其中SA-IIS迭代次数设置为50.在每个数据集上采用十折交叉验证方式,记录各算法在不同评价指标上的平均值和标准差,并以统计形式给出排序.实验结果如表5～表10所示,黑体数字为最优值.</p>
                </div>
                <div class="p1">
                    <p id="141">分析表5～表10的实验结果,结论如下.</p>
                </div>
                <div class="p1">
                    <p id="142">本文算法在6种评价指标上排名优于其它算法,各算法排名如下:</p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mtext>Η</mtext><mtext>E</mtext><mtext>L</mtext><mtext>A</mtext><mo>-</mo><mtext>L</mtext><mtext>D</mtext><mtext>L</mtext><mspace width="0.25em" /><mo>&gt;</mo><mspace width="0.25em" /><mtext>A</mtext><mtext>A</mtext><mo>-</mo><mtext>Κ</mtext><mtext>Ν</mtext><mtext>Ν</mtext><mo>&gt;</mo><mtext>S</mtext><mtext>A</mtext><mo>-</mo><mtext>Ι</mtext><mtext>Ι</mtext><mtext>S</mtext><mo>&gt;</mo><mtext>Ρ</mtext><mtext>Τ</mtext><mo>-</mo><mtext>S</mtext><mtext>V</mtext><mtext>Μ</mtext><mo>&gt;</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>A</mtext><mtext>A</mtext><mo>-</mo><mtext>B</mtext><mtext>Ρ</mtext><mo>&gt;</mo><mtext>Ρ</mtext><mtext>Τ</mtext><mo>-</mo><mtext>B</mtext><mtext>a</mtext><mtext>y</mtext><mtext>e</mtext><mtext>s</mtext><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144">SA-IIS在各评价指标上性能接近AA-KNN.HELA-LDL在Yeast数据集上数值优于各对比算法,说明算法针对标记分布数据集较有效.在Natural Scene数据集,HELA-LDL在Clark、Canberra指标上性能略低于AA-KNN,其它4个指标均最优,综合排名仍为第一.</p>
                </div>
                <div class="p1">
                    <p id="145">在s-JAFFE、s-BU_3DFE数据集上,相比对比算法,HELA-LDL在精度上有明显提高.在Movie数据集上,样本及特征数目较多,HELA-LDL预测结果仍然最好.</p>
                </div>
                <div class="p1">
                    <p id="146">对比现有多个算法,HELA-LDL在标记分布框架下具有较优效果,说明HELA-LDL的有效性.异态集成学习能够融合各模型优势,有利于标记分布学习.</p>
                </div>
                <div class="p1">
                    <p id="147">表11为6种算法的耗时对比.采用集成学习能够提高整体精度,但是随着基分类器数目的增多,算法整体耗时增大.通过对比发现HELA-LDL耗时高于PT-Bayes和AA-KNN,但仍在合理接受范围,其耗时优于专用算法.</p>
                </div>
                <div class="area_img" id="148">
                    <p class="img_tit"><b>表5 六种算法在Chebyshev距离上的测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Experimental results of 6 algorithms measured by Chebyshev distance</p>
                    <p class="img_note"></p>
                    <table id="148" border="1"><tr><td>数据集</td><td>PT-Bayes</td><td>PT-SVM</td><td>AA-BP</td><td>AA-KNN</td><td>SA-IIS</td><td>HELA-LDL</td></tr><tr><td><br />Yeast-alpha</td><td>0.0996±0.0051</td><td>0.0139±0.0004</td><td>0.0367±0.0013</td><td>0.0146±0.0004</td><td>0.0170±0.0005</td><td><b>0.0131</b>±<b>0.0004</b></td></tr><tr><td><br />Yeast-elu</td><td>0.1121±0.0061</td><td>0.0171±0.0003</td><td>0.0389±0.0018</td><td>0.0176±0.0004</td><td>0.0203±0.0004</td><td><b>0.0161</b>±<b>0.0003</b></td></tr><tr><td><br />Yeast-diau</td><td>0.1577±0.0069</td><td>0.0443±0.0034</td><td>0.0501±0.0020</td><td>0.0391±0.0013</td><td>0.0412±0.0009</td><td><b>0.0354</b>±<b>0.0012</b></td></tr><tr><td><br />Yeast-heat</td><td>0.1741±0.0105</td><td>0.0441±0.0011</td><td>0.0530±0.0027</td><td>0.0447±0.0017</td><td>0.0466±0.0011</td><td><b>0.0401</b>±<b>0.0016</b></td></tr><tr><td><br />Yeast-spo</td><td>0.1754±0.0090</td><td>0.0633±0.0032</td><td>0.0671±0.0035</td><td>0.0629±0.0027</td><td>0.0613±0.0015</td><td><b>0.0567</b>±<b>0.0012</b></td></tr><tr><td><br />Yeast-cold</td><td>0.1831±0.0133</td><td>0.0571±0.0033</td><td>0.0591±0.0026</td><td>0.0550±0.0017</td><td>0.0566±0.0016</td><td><b>0.0503</b>±<b>0.0013</b></td></tr><tr><td><br />Yeast-dtt</td><td>0.1818±0.0134</td><td>0.0380±0.0016</td><td>0.0433±0.0015</td><td>0.0397±0.0017</td><td>0.0436±0.0013</td><td><b>0.0346</b>±<b>0.0019</b></td></tr><tr><td><br />Yeast-spo5</td><td>0.2013±0.0133</td><td>0.0925±0.0034</td><td>0.0942±0.0035</td><td>0.0970±0.0054</td><td>0.0950±0.0023</td><td><b>0.0888</b>±<b>0.0034</b></td></tr><tr><td><br />Natural Scene</td><td>0.4053±0.0250</td><td>0.4273±0.0328</td><td>0.3944±0.0172</td><td>0.3142±0.0131</td><td>0.3557±0.0139</td><td><b>0.2673</b>±<b>0.0131</b></td></tr><tr><td><br />s-JAFFE</td><td>0.1232±0.0083</td><td>0.1224±0.0080</td><td>0.1391±0.0129</td><td>0.0988±0.0139</td><td>0.1160±0.0141</td><td><b>0.0891</b>±<b>0.0085</b></td></tr><tr><td><br />s-BU_3DFE</td><td>0.1386±0.0037</td><td>0.1415±0.0058</td><td>0.1429±0.0064</td><td>0.1276±0.0035</td><td>0.1344±0.0050</td><td><b>0.1010</b>±<b>0.0030</b></td></tr><tr><td><br />Movie</td><td>0.2014±0.0028</td><td>0.2292±0.0191</td><td>0.1391±0.0034</td><td>0.1240±0.0026</td><td>0.1473±0.0020</td><td><b>0.1131</b>±<b>0.0020</b></td></tr><tr><td><br />算法排位</td><td>1.42</td><td>3.42</td><td>2.33</td><td>4.33</td><td>3.5</td><td><b>6</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit"><b>表6 六种算法在Clark距离上的测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 6 Experimental results of 6 algorithms measured by Clark distance</p>
                    <p class="img_note"></p>
                    <table id="149" border="1"><tr><td>数据集</td><td>PT-Bayes</td><td>PT-SVM</td><td>AA-BP</td><td>AA-KNN</td><td>SA-IIS</td><td>HELA-LDL</td></tr><tr><td><br />Yeast-alpha</td><td>1.1729±0.0395</td><td>0.2217±0.0062</td><td>0.7349±0.0311</td><td>0.2305±0.0041</td><td>0.2614±0.0071</td><td><b>0.2052</b>±<b>0.0062</b></td></tr><tr><td><br />Yeast-elu</td><td>1.0343±0.0496</td><td>0.2115±0.0043</td><td>0.5449±0.0279</td><td>0.2171±0.0057</td><td>0.2404±0.0035</td><td><b>0.1972</b>±<b>0.0031</b></td></tr><tr><td><br />Yeast-diau</td><td>0.7531±0.0333</td><td>0.2371±0.0141</td><td>0.2753±0.0104</td><td>0.2110±0.0062</td><td>0.2215±0.0052</td><td><b>0.1927</b>±<b>0.0062</b></td></tr><tr><td><br />Yeast-heat</td><td>0.6838±0.0386</td><td>0.1902±0.0047</td><td>0.2321±0.0132</td><td>0.1939±0.0070</td><td>0.2010±0.0047</td><td><b>0.1743</b>±<b>0.0070</b></td></tr><tr><td><br />Yeast-spo</td><td>0.6843±0.0302</td><td>0.2675±0.0123</td><td>0.2907±0.0164</td><td>0.2668±0.0103</td><td>0.2624±0.0074</td><td><b>0.2431</b>±<b>0.0048</b></td></tr><tr><td><br />Yeast-cold</td><td>0.4954±0.0351</td><td>0.1542±0.0085</td><td>0.1609±0.0069</td><td>0.1497±0.0047</td><td>0.1532±0.0049</td><td><b>0.1370</b>±<b>0.0041</b></td></tr><tr><td><br />Yeast-dtt</td><td>0.4986±0.0358</td><td>0.1034±0.0044</td><td>0.1183±0.0042</td><td>0.1075±0.0051</td><td>0.1172±0.0039</td><td><b>0.0939</b>±<b>0.0057</b></td></tr><tr><td><br />Yeast-spo5</td><td>0.4180±0.0278</td><td>0.1864±0.0080</td><td>0.1895±0.0073</td><td>0.1958±0.0113</td><td>0.1914±0.0051</td><td><b>0.1799</b>±<b>0.0081</b></td></tr><tr><td><br />Natural Scene</td><td>2.5206±0.0275</td><td>2.5662±0.0330</td><td>2.4994±0.0212</td><td><b>1.9384</b>±<b>0.0377</b></td><td>2.4715±0.0169</td><td>1.9736±0.0182</td></tr><tr><td><br />s-JAFFE</td><td>0.4397±0.0152</td><td>0.4470±0.0161</td><td>0.5229±0.0487</td><td>0.3485±0.0306</td><td>0.4155±0.0256</td><td><b>0.3321</b>±<b>0.0208</b></td></tr><tr><td><br />s-BU_3DFE</td><td>0.4125±0.0070</td><td>0.4236±0.0151</td><td>0.4654±0.0199</td><td>0.4031±0.0087</td><td>0.4138±0.0072</td><td><b>0.3234</b>±<b>0.0086</b></td></tr><tr><td><br />Movie</td><td>0.8065±0.0086</td><td>0.8323±0.0800</td><td>0.6405±0.0165</td><td>0.5488±0.0102</td><td>0.5824±0.0076</td><td><b>0.5095</b>±<b>0.0067</b></td></tr><tr><td><br />算法排位</td><td>1.58</td><td>3.33</td><td>2.42</td><td>4.17</td><td>3.58</td><td>5.92</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="150">
                    <p class="img_tit"><b>表7 六种算法在Canberra标准上的测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 7 Experimental results of 6 algorithms measured by Canberra metric</p>
                    <p class="img_note"></p>
                    <table id="150" border="1"><tr><td>数据集</td><td>PT-Bayes</td><td>PT-SVM</td><td>AA-BP</td><td>AA-KNN</td><td>SA-IIS</td><td>HELA-LDL</td></tr><tr><td><br />Yeast-alpha</td><td>4.1937±0.1492</td><td>0.7251±0.0205</td><td>2.4208±0.0956</td><td>0.7532±0.0140</td><td>0.8618±0.0232</td><td><b>0.6679</b>±<b>0.0205</b></td></tr><tr><td><br />Yeast-elu</td><td>3.2778±0.1712</td><td>0.6242±0.0134</td><td>1.5983±0.0830</td><td>0.6419±0.0179</td><td>0.7125±0.0093</td><td><b>0.5779</b>±<b>0.0097</b></td></tr><tr><td><br />Yeast-diau</td><td>1.7086±0.0840</td><td>0.5096±0.0333</td><td>0.5944±0.0207</td><td>0.4539±0.0151</td><td>0.4787±0.0101</td><td><b>0.4128</b>±<b>0.0126</b></td></tr><tr><td><br />Yeast-heat</td><td>1.4440±0.0827</td><td>0.3798±0.0110</td><td>0.4671±0.0236</td><td>0.3903±0.0135</td><td>0.4042±0.0098</td><td><b>0.3465</b>±<b>0.0124</b></td></tr><tr><td><br />Yeast-spo</td><td>1.4431±0.0689</td><td>0.5504±0.0253</td><td>0.5951±0.0337</td><td>0.5490±0.0220</td><td>0.5394±0.0159</td><td><b>0.4983</b>±<b>0.0101</b></td></tr><tr><td><br />Yeast-cold</td><td>0.8669±0.0620</td><td>0.2660±0.0150</td><td>0.2773±0.0123</td><td>0.2593±0.0072</td><td>0.2649±0.0085</td><td><b>0.2357</b>±<b>0.0076</b></td></tr><tr><td><br />Yeast-dtt</td><td>0.8719±0.0649</td><td>0.1779±0.0083</td><td>0.2041±0.0076</td><td>0.1847±0.0081</td><td>0.2027±0.0072</td><td><b>0.1613</b>±<b>0.0058</b></td></tr><tr><td><br />Yeast-spo5</td><td>0.6488±0.0450</td><td>0.2862±0.0113</td><td>0.2911±0.0112</td><td>0.3003±0.0170</td><td>0.2939±0.0077</td><td><b>0.2759</b>±<b>0.0116</b></td></tr><tr><td><br />Natural Scene</td><td>7.1395±0.1106</td><td>7.2686±0.1413</td><td>6.9622±0.0905</td><td><b>4.6736</b>±<b>0.1066</b></td><td>6.8127±0.0721</td><td>4.8120±0.0656</td></tr><tr><td><br />s-JAFFE</td><td>0.9225±0.0391</td><td>0.9335±0.0314</td><td>1.0716±0.1133</td><td>0.7142±0.0640</td><td>0.8620±0.0580</td><td><b>0.6715</b>±<b>0.0409</b></td></tr><tr><td><br />s-BU_3DFE</td><td>0.9025±0.0160</td><td>0.9154±0.0315</td><td>0.9823±0.0385</td><td>0.8315±0.0193</td><td>0.8969±0.0180</td><td><b>0.6698</b>±<b>0.0166</b></td></tr><tr><td><br />Movie</td><td>1.5635±0.0188</td><td>1.6277±0.1939</td><td>1.2238±0.0305</td><td>1.0553±0.021</td><td>1.1199±0.0164</td><td><b>0.9720</b>±<b>0.0153</b></td></tr><tr><td><br />算法排位</td><td>1.5</td><td>3.42</td><td>2.16</td><td>4.42</td><td>3.58</td><td><b>5.92</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="151">
                    <p class="img_tit"><b>表8 六种算法在Kullback-Leibler散度上的测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 8 Experimental results of 6 algorithms measured by Kullback-Leibler divergence</p>
                    <p class="img_note"></p>
                    <table id="151" border="1"><tr><td>数据集</td><td>PT-Bayes</td><td>PT-SVM</td><td>AA-BP</td><td>AA-KNN</td><td>SA-IIS</td><td>HELA-LDL</td></tr><tr><td><br />Yeast-alpha</td><td>0.2776±0.0238</td><td>0.0060±0.0003</td><td>0.0872±0.9475</td><td>0.0065±0.0002</td><td>0.0085±0.0004</td><td><b>0.0055</b>±<b>0.0005</b></td></tr><tr><td><br />Yeast-elu</td><td>0.2825±0.0328</td><td>0.0069±0.0003</td><td>0.0608±0.0092</td><td>0.0073±0.0004</td><td>0.0091±0.0003</td><td><b>0.0061</b>±<b>0.0009</b></td></tr><tr><td><br />Yeast-diau</td><td>0.2698±0.0276</td><td>0.0177±0.0019</td><td>0.0264±0.0024</td><td>0.0149±0.0009</td><td>0.0157±0.0006</td><td><b>0.0124</b>±<b>0.0013</b></td></tr><tr><td><br />Yeast-heat</td><td>0.2684±0.0345</td><td>0.0138±0.0006</td><td>0.0216±0.0029</td><td>0.0144±0.0010</td><td>0.0152±0.0007</td><td><b>0.0115</b>±<b>0.0010</b></td></tr><tr><td><br />Yeast-spo</td><td>0.2788±0.0398</td><td>0.0292±0.0028</td><td>0.0332±0.0039</td><td>0.0291±0.0023</td><td>0.0268±0.0015</td><td><b>0.0244</b>±<b>0.0012</b></td></tr><tr><td><br />Yeast-cold</td><td>0.2174±0.0356</td><td>0.0147±0.0013</td><td>0.0162±0.0016</td><td>0.0139±0.0011</td><td>0.0146±0.0010</td><td><b>0.0117</b>±<b>0.0013</b></td></tr><tr><td><br />Yeast-dtt</td><td>0.2264±0.0393</td><td>0.0068±0.0007</td><td>0.0089±0.0006</td><td>0.0074±0.0008</td><td>0.0080±0.0006</td><td><b>0.0058</b>±<b>0.0017</b></td></tr><tr><td><br />Yeast-spo5</td><td>0.2066±0.0427</td><td>0.0299±0.0023</td><td>0.0312±0.0023</td><td>0.0347±0.0039</td><td>0.0314±0.0015</td><td><b>0.0280</b>±<b>0.0020</b></td></tr><tr><td><br />Natural Scene</td><td>2.2634±0.4388</td><td>1.5258±0.2236</td><td>1.1608±0.0725</td><td>1.1154±0.0596</td><td>0.9396±0.0272</td><td><b>0.7923</b>±<b>0.0540</b></td></tr><tr><td><br />s-JAFFE</td><td>0.0763±0.0063</td><td>0.0790±0.0058</td><td>0.1173±0.0245</td><td>0.0538±0.0107</td><td>0.0693±0.0116</td><td><b>0.0464</b>±<b>0.0078</b></td></tr><tr><td><br />s-BU_3DFE</td><td>0.0849±0.0029</td><td>0.0909±0.0048</td><td>0.1023±0.0096</td><td>0.0818±0.0039</td><td>0.0819±0.0040</td><td><b>0.0528</b>±<b>0.0028</b></td></tr><tr><td><br />Movie</td><td>0.7297±0.0590</td><td>0.2938±0.0697</td><td>0.1664±0.0107</td><td>0.1177±0.0051</td><td>0.1317±0.0046</td><td><b>0.0955</b>±<b>0.0040</b></td></tr><tr><td><br />算法排位</td><td>1.33</td><td>3.58</td><td>2.42</td><td>3.92</td><td>3.75</td><td><b>6</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="152">
                    <p class="img_tit"><b>表9 六种算法在Cosine相关性上的测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 9 Experimental results of 6 algorithms measured by Cosine coefficient</p>
                    <p class="img_note"></p>
                    <table id="152" border="1"><tr><td>数据集</td><td>PT-Bayes</td><td>PT-SVM</td><td>AA-BP</td><td>AA-KNN</td><td>SA-IIS</td><td>HELA-LDL</td></tr><tr><td><br />Yeast-alpha</td><td>0.8485±0.0070</td><td>0.9941±0.0003</td><td>0.9475±0.0032</td><td>0.9936±0.0002</td><td>0.9914±0.0004</td><td><b>0.9948</b>±<b>0.0003</b></td></tr><tr><td><br />Yeast-elu</td><td>0.8531±0.0092</td><td>0.9934±0.0002</td><td>0.9597±0.0039</td><td>0.9929±0.0004</td><td>0.9910±0.0003</td><td><b>0.9941</b>±<b>0.0002</b></td></tr><tr><td><br />Yeast-diau</td><td>0.8638±0.0070</td><td>0.9836±0.0018</td><td>0.9773±0.0017</td><td>0.9863±0.0009</td><td>0.9853±0.0005</td><td><b>0.9887</b>±<b>0.0007</b></td></tr><tr><td><br />Yeast-heat</td><td>0.8668±0.0103</td><td>0.9868±0.0006</td><td>0.9803±0.0020</td><td>0.9863±0.0009</td><td>0.9854±0.0006</td><td><b>0.9891</b>±<b>0.0008</b></td></tr><tr><td><br />Yeast-spo</td><td>0.8611±0.0093</td><td>0.9725±0.0027</td><td>0.9695±0.0032</td><td>0.9727±0.0022</td><td>0.9747±0.0013</td><td><b>0.9779</b>±<b>0.0006</b></td></tr><tr><td><br />Yeast-cold</td><td>0.8936±0.0095</td><td>0.9860±0.0013</td><td>0.9848±0.0014</td><td>0.9868±0.0008</td><td>0.9861±0.0008</td><td><b>0.9889</b>±<b>0.0007</b></td></tr><tr><td><br />Yeast-dtt</td><td>0.8950±0.0098</td><td>0.9935±0.0005</td><td>0.9916±0.0005</td><td>0.9929±0.0006</td><td>0.9915±0.0005</td><td><b>0.9945</b>±<b>0.0007</b></td></tr><tr><td><br />Yeast-spo5</td><td>0.8980±0.0106</td><td>0.9735±0.0018</td><td>0.9723±0.0018</td><td>0.9694±0.0033</td><td>0.9721±0.0012</td><td><b>0.9757</b>±<b>0.0017</b></td></tr><tr><td><br />Natural Scene</td><td>0.5589±0.0133</td><td>0.4567±0.0610</td><td>0.5925±0.0196</td><td>0.7041±0.0129</td><td>0.6659±0.0107</td><td><b>0.7736</b>±<b>0.0146</b></td></tr><tr><td><br />s-JAFFE</td><td>0.9281±0.0063</td><td>0.9258±0.0056</td><td>0.8991±0.0189</td><td>0.9484±0.0109</td><td>0.9347±0.0112</td><td><b>0.9560</b>±<b>0.0077</b></td></tr><tr><td><br />s-BU_3DFE</td><td>0.9179±0.0028</td><td>0.9128±0.0045</td><td>0.9037±0.0069</td><td>0.9202±0.0035</td><td>0.9203±0.0036</td><td><b>0.9473</b>±<b>0.0029</b></td></tr><tr><td><br />Movie</td><td>0.8495±0.0022</td><td>0.7810±0.0495</td><td>0.9028±0.0045</td><td>0.9224±0.0032</td><td>0.9081±0.0030</td><td><b>0.9369</b>±<b>0.0025</b></td></tr><tr><td><br />算法排位</td><td>1.5</td><td>3.33</td><td>2.25</td><td>4.25</td><td>3.67</td><td><b>6</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="153">
                    <p class="img_tit"><b>表10 六种算法在Intersection相似性上的测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 10 Experimental results of 6 algorithms measured by Intersection similarity</p>
                    <p class="img_note"></p>
                    <table id="153" border="1"><tr><td>数据集</td><td>PT-Bayes</td><td>PT-SVM</td><td>AA-BP</td><td>AA-KNN</td><td>SA-IIS</td><td>HELA-LDL</td></tr><tr><td><br />Yeast-alpha</td><td>0.7725±0.0078</td><td>0.9600±0.0011</td><td>0.8740±0.0043</td><td>0.9584±0.0008</td><td>0.9518±0.0013</td><td><b>0.9630</b>±<b>0.0011</b></td></tr><tr><td><br />Yeast-elu</td><td>0.7727±0.0109</td><td>0.9559±0.0009</td><td>0.8917±0.0052</td><td>0.9547±0.0013</td><td>0.9491±0.0007</td><td><b>0.9593</b>±<b>0.0010</b></td></tr><tr><td><br />Yeast-diau</td><td>0.7690±0.0108</td><td>0.9289±0.0048</td><td>0.9177±0.0028</td><td>0.9370±0.0022</td><td>0.9331±0.0013</td><td><b>0.9428</b>±<b>0.0018</b></td></tr><tr><td><br />Yeast-heat</td><td>0.7708±0.0122</td><td>0.9376±0.0019</td><td>0.9235±0.0035</td><td>0.9359±0.0022</td><td>0.9332±0.0016</td><td><b>0.9433</b>±<b>0.0020</b></td></tr><tr><td><br />Yeast-spo</td><td>0.7691±0.0108</td><td>0.9089±0.0043</td><td>0.9022±0.0053</td><td>0.9095±0.0037</td><td>0.9109±0.0026</td><td><b>0.9178</b>±<b>0.0014</b></td></tr><tr><td><br />Yeast-cold</td><td>0.7965±0.0139</td><td>0.9341±0.0038</td><td>0.9317±0.0031</td><td>0.9360±0.0016</td><td>0.9344±0.0020</td><td><b>0.9420</b>±<b>0.0018</b></td></tr><tr><td><br />Yeast-dtt</td><td>0.7964±0.0147</td><td>0.9561±0.0021</td><td>0.9496±0.0019</td><td>0.9544±0.0018</td><td>0.9496±0.0018</td><td><b>0.9601</b>±<b>0.0022</b></td></tr><tr><td><br />Yeast-spo5</td><td>0.7987±0.0133</td><td>0.9075±0.0034</td><td>0.9058±0.0035</td><td>0.9030±0.0054</td><td>0.9050±0.0023</td><td><b>0.9110</b>±<b>0.0033</b></td></tr><tr><td><br />Natural Scene</td><td>0.3500±0.0129</td><td>0.3483±0.0394</td><td>0.4278±0.0160</td><td>0.5571±0.0125</td><td>0.4631±0.0102</td><td><b>0.6181</b>±<b>0.0134</b></td></tr><tr><td><br />s-JAFFE</td><td>0.8430±0.0077</td><td>0.8410±0.0059</td><td>0.8181±0.0195</td><td>0.8763±0.0133</td><td>0.8536±0.0119</td><td><b>0.8860</b>±<b>0.0070</b></td></tr><tr><td><br />s-BU_3DFE</td><td>0.8388±0.0032</td><td>0.8356±0.0058</td><td>0.8236±0.0067</td><td>0.8479±0.0038</td><td>0.8394±0.0037</td><td><b>0.8791</b>±<b>0.0032</b></td></tr><tr><td><br />Movie</td><td>0.7226±0.0028</td><td>0.6870±0.0440</td><td>0.7976±0.0051</td><td>0.8224±0.0038</td><td>0.8032±0.0033</td><td><b>0.8397</b>±<b>0.0030</b></td></tr><tr><td><br />算法排位</td><td>1.5</td><td>3.33</td><td>2.25</td><td>4.33</td><td>3.59</td><td><b>6</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="154">
                    <p class="img_tit"><b>表11 六种算法在各数据集上耗时对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 11 Time consuming comparison of 6 algorithms on different datasets s</p>
                    <p class="img_note"></p>
                    <table id="154" border="1"><tr><td>数据集</td><td>PT-Bayes</td><td>PT-SVM</td><td>AA-KNN</td><td>AA-BP</td><td>SA-IIS</td><td>HELA-LDL</td></tr><tr><td><br />Yeast-alpha</td><td>11.91</td><td>864.87</td><td>16.86</td><td>740.25</td><td>220.17</td><td>164.07</td></tr><tr><td><br />Yeast-elu</td><td>11.77</td><td>515.76</td><td>17.29</td><td>720.92</td><td>184.77</td><td>124.88</td></tr><tr><td><br />Yeast-diau</td><td>11.71</td><td>105.26</td><td>17.09</td><td>877.36</td><td>99.25</td><td>57.45</td></tr><tr><td><br />Yeast-heat</td><td>10.96</td><td>74.69</td><td>17.19</td><td>941.35</td><td>91.91</td><td>51.64</td></tr><tr><td><br />Yeast-spo</td><td>11.05</td><td>74.13</td><td>17.01</td><td>652.81</td><td>57.26</td><td>41.64</td></tr><tr><td><br />Yeast-cold</td><td>10.76</td><td>302.17</td><td>16.96</td><td>598.26</td><td>59.82</td><td>27.46</td></tr><tr><td><br />Yeast-dtt</td><td>10.61</td><td>30.01</td><td>17.05</td><td>600.34</td><td>52.26</td><td>28.29</td></tr><tr><td><br />Yeast-spo5</td><td>10.61</td><td>15.74</td><td>17.20</td><td>998.17</td><td>34.47</td><td>20.52</td></tr><tr><td><br />s-JAFFE</td><td>1.29</td><td>74.81</td><td>1.04</td><td>156.97</td><td>544.60</td><td>20.47</td></tr><tr><td><br />s-BU_3DFE</td><td>13.05</td><td>824.04</td><td>23.63</td><td>893.96</td><td>430.11</td><td>383.61</td></tr><tr><td><br />Movie</td><td>146.26</td><td>1312.61</td><td>775.14</td><td>8081.5</td><td>4698.3</td><td>221.24</td></tr><tr><td><br />平均消耗</td><td>22.73</td><td>381.28</td><td>85.13</td><td>1387.45</td><td>588.44</td><td>103.75</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="155" name="155"><b>4.4</b> 稳定性分析</h4>
                <div class="p1">
                    <p id="156">为了验证标记分布中不同学习算法的稳定性,本文利用蜘蛛网图<citation id="244" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>对各算法进行表示.将不同评价指标的结果标准化在[0.1,0.5]内作为通用标准.通过归一化后的值表示稳定指数.图2显示12个数据集上6种算法的稳定性.</p>
                </div>
                <div class="area_img" id="199">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910011_19900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 六种算法在6个评价指标上的稳定性指数值" src="Detail/GetImg?filename=images/MSSB201910011_19900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 六种算法在6个评价指标上的稳定性指数值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910011_19900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Values of stability index of 6 algorithms with 6 evaluation metrics</p>

                </div>
                <div class="area_img" id="199">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910011_19901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 六种算法在6个评价指标上的稳定性指数值" src="Detail/GetImg?filename=images/MSSB201910011_19901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 六种算法在6个评价指标上的稳定性指数值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910011_19901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Values of stability index of 6 algorithms with 6 evaluation metrics</p>

                </div>
                <div class="p1">
                    <p id="162">由图2(a)可知,在Chebyshev评价指标上,HELA-LDL在12个数据集上平均精度的稳定指数值是0.5,获得相当稳定的效果.由(b)、(c)可知,在Clark、Canberra评价指标上,HELA-LDL在自然场景数据集上性能略低于AA-KNN,仍稳定在[0.4,0.5]内,在其它数据集上稳定性指标均优于其它算法.由(d)可知,在Kullback-Leibler评价指标上,HELA-LDL在12个数据集上平均精度的稳定指数值均为0.5.由(e)、(f)可知,在Cosine、Intersection评价指标上,HELA-LDL在12个数据集上平均精度的稳定指数值均为0.5,在各数据集上表现较稳定,稳定性指标均较优.</p>
                </div>
                <div class="p1">
                    <p id="163">图2表明,相比现有多种算法,HELA-LDL具有更好的稳定性,虽然在个别数据集上略低于AA-KNN,但是稳定指数值波动不大,整体较优,说明HELA-LDL适合于标记分布学习,更有利于提高整体性能.</p>
                </div>
                <h3 id="164" name="164" class="anchor-tag">5 结 束 语</h3>
                <div class="p1">
                    <p id="165">在标记分布学习中,针对单个算法对预测模型性能产生影响的问题,本文提出基于标记分布的异态集成学习算法(HELA-LDL).首先使用KNN、LR、RF、GBDT对原始数据特征进行学习预测,对结果融合输出作为中间特征,再将新特征学习通过RF构建标记分布预测模型.在专用数据集上对比现有算法,结果表明,HELA-LDL在不同规模的标记分布数据集上均能产生良好效果,尤其在人脸表情等真实数据集上均取得较优精度.对算法的稳定性分析进一步说明算法的有效性.</p>
                </div>
                <div class="p1">
                    <p id="166">由于基分类器学习的结果对预测模型的影响较大,针对分布数据集选择准确度更高的学习算法可以有效提高模型整体的预测性能,下一步将研究组合更有效的学习算法.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="200">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A review on multi-label learning algorithms">

                                <b>[1]</b> ZHANG M L,ZHOU Z H.A Review on Multi-label Learning Algorithms.IEEE Transactions on Knowledge and Data Engineering,2014,26(8):1819-1837.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201706004&amp;v=MTI0ODBMeXZTZExHNEg5Yk1xWTlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1dyM08=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 耿新,徐宁,邵瑞枫.面向标记分布学习的标记增强.计算机研究与发展,2017,54(6):1171-1184.(GENG X,XU N,SHAO R F.Label Enhancement for Label Distribution Learning.Journal of Computer Research and Development,2017,54(6):1171-1184.)
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Label Distribution Learning&amp;quot;">

                                <b>[3]</b> GENG X.Label Distribution Learning.IEEE Transactions on Know-ledge and Data Engineering,2016,28(7):1734-1748.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_4" >
                                    <b>[4]</b>
                                 GENG X,YIN C,ZHOU Z H.Facial Age Estimation by Learning from Label Distributions.IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(10):2401-2412.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data-dependent label distribution learning for age estimation">

                                <b>[5]</b> HE Z Z,LI X,ZHANG Z F,et al.Data-Dependent Label Distribution Learning for Age Estimation.IEEE Transactions on Image Processing,2017,26(8):3846-3858.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES84F83F3FDFD2CC6FF08C9A84EDB4B9BF&amp;v=MTc2MDVRN3FxR2RCQzdibVRNanBDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3N20yd3E0PU5pZk9mYnU4YU5uUDJZd3pFSjE3RGc5S3lXQmw2amNPUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> ZHANG Z X,WANG M,GENG X.Crowd Counting in Public Vi-deo Surveillance by Label Distribution Learning.Neurocomputing,2015,166:151-163.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Label Distribution Learning with Label Ambiguity">

                                <b>[7]</b> GAO B B,XING C,XIE C W,et al.Deep Label Distribution Learning with Label Ambiguity.IEEE Transactions on Image Processing,2017,26(6):2825-2838.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Emotion Distribution Recognition from Facial Expressions">

                                <b>[8]</b> ZHOU Y,XUE H,GENG X.Emotion Distribution Recognition from Facial Expressions // Proc of the 23rd ACM Conference on Multimedia.New York,USA:ACM,2015:1247-1250.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXTS201705004&amp;v=MTQ2NzE0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rV3IzT0xqWGZmYkc0SDliTXFvOUZZSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 赵权,耿新.标记分布学习中目标函数的选择.计算机科学与探索,2017,11(5):708-719.(ZHAO Q,GENG X.Selection of Target Function in Label Distribution Learning.Journal of Frontiers of Computer Science and Technology,2017,11(5):708-719.)
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB201804010&amp;v=MzIxNTBSbkZ5L2tXcjNPTVRYVGJMRzRIOW5NcTQ5RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 崔颖,徐凯,陆忠军,等.主动学习策略融合算法在高光谱图像分类中的应用.通信学报,2018,39(4):91-99.(CUI Y,XU K,LU Z J,et al.Combination Strategy of Active Learning for Hyperspectral Images Classification.Journal on Communications,2018,39(4):91-99.)
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Correlation-Based Pruning of Stacked Binary Relevance Models for Multi-label Learning">

                                <b>[11]</b> TSOUMAKAS G,DIMOU A,SPYROMITROS E,et al.Correlation-Based Pruning of Stacked Binary Relevance Models for Multi-label Learning // Proc of the 1st International Workshop on Lear-ning from Multi-label Data.Berlin,Germany:Springer,2009:101-116.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201804039&amp;v=Mjg2NzVrV3IzT0x6N0JiYkc0SDluTXE0OUdiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 张笑铭,王志君,梁利平.一种适用于卷积神经网络的Stacking算法.计算机工程,2018,44(4):243-247.(ZHANG X M,WANG Z J,LIANG L P.A Stacking Algorithm for Convolution Neural Network.Computer Engineering,2018,44(4):243-247.)
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WHDY201506001&amp;v=MTEyOTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rV3IzT01pWFBkN0c0SDlUTXFZOUZaWVE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 周星,丁立新,万润泽,等.分类器集成算法研究.武汉大学学报(理学版),2015,61(6):503-508.(ZHOU X,DING L X,WAN R Z,et al.Research on Classifier Ensemble Algorithms.Journal of Wuhan University(Natural Science Edition),2015,61(6):503-508.)
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201710012&amp;v=MDI1NTVxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tXcjNPUFRYY2RyRzRIOWJOcjQ5RVpvUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 李巧,周双娥,杨晶.模型融合在用户续购行为分析中的应用.小型微型计算机系统,2017,38(10):2231-2235.(LI Q,ZHOU S E,YANG J.Application of Model Blending in User Renewal Behavior Analysis.Journal of Chinese Computer Systems,2017,38(10):2231-2235.)
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201703018&amp;v=MDUyNDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1dyM09MeXZTZExHNEg5Yk1ySTlFYklRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 傅艺绮,董威,尹良泽,等.基于组合机器学习算法的软件缺陷预测模型.计算机研究与发展,2017,54(3):633-641.(FU Y Q,DONG W,YIN L Z,et al.Software Defect Prediction Model Based on the Combination of Machine Learning Algorithms.Journal of Computer Research and Development,2017,54(3):633-641.)
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked generalization">

                                <b>[16]</b> WOLPERT D H.Stacked Generalization.Neural Networks,1992,5(2):241-259.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES22A4CF21D5805EE3B00914C749BDE4F4&amp;v=MTQ1MzU9TmlmT2ZiRzZiOVcvMlkxRUVPNEhESGxNdWhWaDZqOTBTWHVScXhZOEM4YmhRY3liQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZodzdtMndxNA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> LIN Y J,LI Y W,WANG C X,et al.Attribute Reduction for Multi-label Learning with Fuzzy Rough Set.Knowledge-Based Systems,2018,152:51-61.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201910011" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201910011&amp;v=MDU2NDZHNEg5ak5yNDlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1dyM09LRDdZYkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
