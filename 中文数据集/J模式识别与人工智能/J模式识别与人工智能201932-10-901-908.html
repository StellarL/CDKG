<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131461005967500%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dMSSB201910004%26RESULT%3d1%26SIGN%3d49ywlz5%252bwROY5VSBIeT5Rul0eBc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201910004&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201910004&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201910004&amp;v=MjQ3NzFuRnkva1dyekpLRDdZYkxHNEg5ak5yNDlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#240" data-title="1 系统框架 ">1 系统框架</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#246" data-title="2 特征提取 ">2 特征提取</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#247" data-title="&lt;b&gt;2.1 RGB-HOG&lt;/b&gt;特征"><b>2.1 RGB-HOG</b>特征</a></li>
                                                <li><a href="#266" data-title="&lt;b&gt;2.2 D-STIP&lt;/b&gt;特征"><b>2.2 D-STIP</b>特征</a></li>
                                                <li><a href="#275" data-title="&lt;b&gt;2.3 J-PF&lt;/b&gt;特征"><b>2.3 J-PF</b>特征</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#283" data-title="3 人体行为识别方法 ">3 人体行为识别方法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#297" data-title="4 实验及结果分析 ">4 实验及结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#321" data-title="5 结 束 语 ">5 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#245" data-title="图1 本文方法系统框架图">图1 本文方法系统框架图</a></li>
                                                <li><a href="#249" data-title="图2 HOG主要流程图">图2 HOG主要流程图</a></li>
                                                <li><a href="#282" data-title="图3 骨骼关节点图">图3 骨骼关节点图</a></li>
                                                <li><a href="#360" data-title="图4 G3D数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵">图4 G3D数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵</a></li>
                                                <li><a href="#360" data-title="图4 G3D数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵">图4 G3D数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵</a></li>
                                                <li><a href="#361" data-title="图5 CAD-60数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵">图5 CAD-60数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵</a></li>
                                                <li><a href="#361" data-title="图5 CAD-60数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵">图5 CAD-60数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵</a></li>
                                                <li><a href="#317" data-title="&lt;b&gt;表1 使用单模态特征和多模态特征下本文方法的识别率&lt;/b&gt;"><b>表1 使用单模态特征和多模态特征下本文方法的识别率</b></a></li>
                                                <li><a href="#319" data-title="&lt;b&gt;表2 各方法的识别率对比&lt;/b&gt;"><b>表2 各方法的识别率对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="362">


                                    <a id="bibliography_1" title=" CHEN C,JAFARI R,KEHTARNAVAZ N.A Survey of Depth and Inertial Sensor Fusion for Human Action Recognition.Multimedia Tools and Applications,2017,76(3):4405-4425." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A survey of depth and inertial sensor fusion for human action recognition">
                                        <b>[1]</b>
                                         CHEN C,JAFARI R,KEHTARNAVAZ N.A Survey of Depth and Inertial Sensor Fusion for Human Action Recognition.Multimedia Tools and Applications,2017,76(3):4405-4425.
                                    </a>
                                </li>
                                <li id="364">


                                    <a id="bibliography_2" title=" CAI Z Y,HAN J G,LIU L,et al.RGB-D Datasets Using Microsoft Kinect or Similar Sensors:A Survey.Multimedia Tools and Applications,2017,76(3):4313-4355." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=RGB-D datasets using microsoft kinect or similar sen-sors:a survey">
                                        <b>[2]</b>
                                         CAI Z Y,HAN J G,LIU L,et al.RGB-D Datasets Using Microsoft Kinect or Similar Sensors:A Survey.Multimedia Tools and Applications,2017,76(3):4313-4355.
                                    </a>
                                </li>
                                <li id="366">


                                    <a id="bibliography_3" title=" ZHANG J,LI W Q,OGUNBONA P O,et al.RGB-D-Based Action Recognition Datasets:A Survey.Pattern Recognition,2016,60:86-105." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6B4657B45365879DB85FFCC162CFAC90&amp;v=MTU1MjFCdUhZZk9HUWxmQ3BiUTM1TkZodzdtMnc2az1OaWZPZmJYS0d0ZkpxUDFCWWVnSkNYUSt4bUpoNGpvTFBneVJyUlEzQ3NUbE5yT2ZDT052RlNpV1dyN0pJRnBtYQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         ZHANG J,LI W Q,OGUNBONA P O,et al.RGB-D-Based Action Recognition Datasets:A Survey.Pattern Recognition,2016,60:86-105.
                                    </a>
                                </li>
                                <li id="368">


                                    <a id="bibliography_4" title=" ZHANG B C,YANG Y,CHEN C,et al.Action Recognition Using 3D Histograms of Texture and a Multi-class Boosting Classifier.IEEE Transactions on Image Processing,2017,26(10):4648-4660." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Action Recognition Using 3D Histograms of Texture and A Multi-Class Boosting Classifier">
                                        <b>[4]</b>
                                         ZHANG B C,YANG Y,CHEN C,et al.Action Recognition Using 3D Histograms of Texture and a Multi-class Boosting Classifier.IEEE Transactions on Image Processing,2017,26(10):4648-4660.
                                    </a>
                                </li>
                                <li id="370">


                                    <a id="bibliography_5" title=" CHEN C,LIU M Y,LIU H,et al.Multi-temporal Depth Motion Maps-Based Local Binary Patterns for 3-D Human Action Recognition.IEEE Access,2017,5:22590-22604." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-temporal depth motion mapsbased local binary patterns for 3-D human action recognition">
                                        <b>[5]</b>
                                         CHEN C,LIU M Y,LIU H,et al.Multi-temporal Depth Motion Maps-Based Local Binary Patterns for 3-D Human Action Recognition.IEEE Access,2017,5:22590-22604.
                                    </a>
                                </li>
                                <li id="372">


                                    <a id="bibliography_6" title=" CHEN C,LIU K,KEHTARNAVAZ N.Real-Time Human Action Recognition Based on Depth Motion Maps.Journal of Real-Time Image Processing,2016,12(1):155-163." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time human action recognition based on depth motion maps">
                                        <b>[6]</b>
                                         CHEN C,LIU K,KEHTARNAVAZ N.Real-Time Human Action Recognition Based on Depth Motion Maps.Journal of Real-Time Image Processing,2016,12(1):155-163.
                                    </a>
                                </li>
                                <li id="374">


                                    <a id="bibliography_7" title=" LI W Q,ZHANG Z Y,LIU Z C.Action Recognition Based on a Bag of 3D Points // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2010:9-14." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Action recognition based on a bag of 3Dpoints">
                                        <b>[7]</b>
                                         LI W Q,ZHANG Z Y,LIU Z C.Action Recognition Based on a Bag of 3D Points // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2010:9-14.
                                    </a>
                                </li>
                                <li id="376">


                                    <a id="bibliography_8" title=" YANG X D,ZHANG C Y,TIAN Y L.Recognizing Actions Using Depth Motion Maps-Based Histograms of Oriented Gradients // Proc of the 20th ACM International Conference on Multimedia.New York,USA:ACM,2012 :1057-1060." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recognizing actions using depth motion maps-based histograms of oriented gradients">
                                        <b>[8]</b>
                                         YANG X D,ZHANG C Y,TIAN Y L.Recognizing Actions Using Depth Motion Maps-Based Histograms of Oriented Gradients // Proc of the 20th ACM International Conference on Multimedia.New York,USA:ACM,2012 :1057-1060.
                                    </a>
                                </li>
                                <li id="378">


                                    <a id="bibliography_9" title=" BULBUL M F,JIANG Y S,MA J W.Human Action Recognition Based on DMMs,HOGs and Contourlet Transform // Proc of the IEEE International Conference on Multimedia Big Data.Washington,USA:IEEE,2015:389-394." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human Action Recognition Based on DMMs,HOGs and Contourlet Transform">
                                        <b>[9]</b>
                                         BULBUL M F,JIANG Y S,MA J W.Human Action Recognition Based on DMMs,HOGs and Contourlet Transform // Proc of the IEEE International Conference on Multimedia Big Data.Washington,USA:IEEE,2015:389-394.
                                    </a>
                                </li>
                                <li id="380">


                                    <a id="bibliography_10" title=" YANG X D,TIAN Y L.Super Normal Vector for Activity Recognition Using Depth Sequences // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2014:804-811." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Super normal vector for activity recognition using depth sequences">
                                        <b>[10]</b>
                                         YANG X D,TIAN Y L.Super Normal Vector for Activity Recognition Using Depth Sequences // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2014:804-811.
                                    </a>
                                </li>
                                <li id="382">


                                    <a id="bibliography_11" title=" SLAMA R,WANNOUS H,DAOUDI M.Grassmannian Representation of Motion Depth for 3D Human Gesture and Action Recognition // Proc of the 22nd International Conference on Pattern Recognition.Washington,USA:IEEE,2014:3499-3504." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Grassmannian Representation of Motion Depth for 3D Human Gesture and Action Recognition">
                                        <b>[11]</b>
                                         SLAMA R,WANNOUS H,DAOUDI M.Grassmannian Representation of Motion Depth for 3D Human Gesture and Action Recognition // Proc of the 22nd International Conference on Pattern Recognition.Washington,USA:IEEE,2014:3499-3504.
                                    </a>
                                </li>
                                <li id="384">


                                    <a id="bibliography_12" title=" OREIFEJ O,LIU Z C.HON4D:Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2013 :716-723." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hon4d:Histogram of oriented 4d normals for activity recognition from depth sequences">
                                        <b>[12]</b>
                                         OREIFEJ O,LIU Z C.HON4D:Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2013 :716-723.
                                    </a>
                                </li>
                                <li id="386">


                                    <a id="bibliography_13" title=" JIA C C,FU Y.Low-Rank Tensor Subspace Learning for RGB-D Action Recognition.IEEE Transactions on Image Processing,2016,25(10):4641-4652." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Low-rank tensor subspace learning for RGB-D action recognition">
                                        <b>[13]</b>
                                         JIA C C,FU Y.Low-Rank Tensor Subspace Learning for RGB-D Action Recognition.IEEE Transactions on Image Processing,2016,25(10):4641-4652.
                                    </a>
                                </li>
                                <li id="388">


                                    <a id="bibliography_14" title=" LEIGHTLEY D,MCPHEE J S,YAP M H.Automated Analysis and Quantification of Human Mobility Using a Depth Sensor.IEEE Journal of Biomedical and Health Informatics,2017,21(4):939-948." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automated Analysis and Quantification of Human Mobility Using a Depth Sensor">
                                        <b>[14]</b>
                                         LEIGHTLEY D,MCPHEE J S,YAP M H.Automated Analysis and Quantification of Human Mobility Using a Depth Sensor.IEEE Journal of Biomedical and Health Informatics,2017,21(4):939-948.
                                    </a>
                                </li>
                                <li id="390">


                                    <a id="bibliography_15" title=" KEROLA T,INOUE N,SHINODA K.Cross-View Human Action Recognition from Depth Maps Using Spectral Graph Sequences.Computer Vision and Image Understanding,2017,154:108-126." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC6324117676DC7279978FCE5BDAA120A&amp;v=MDU2NDlXQkJDTU9WUjdydUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHc3bTJ3Nms9TmlmT2ZjQytIZFBJcm81Q1l1d0plQTgrelJFYTR6aDFQZ3lYcQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         KEROLA T,INOUE N,SHINODA K.Cross-View Human Action Recognition from Depth Maps Using Spectral Graph Sequences.Computer Vision and Image Understanding,2017,154:108-126.
                                    </a>
                                </li>
                                <li id="392">


                                    <a id="bibliography_16" title=" DALAL N,TRIGGS B.Histograms of Oriented Gradients for Human Detection // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2005,I:886-893." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for hu- man detection">
                                        <b>[16]</b>
                                         DALAL N,TRIGGS B.Histograms of Oriented Gradients for Human Detection // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2005,I:886-893.
                                    </a>
                                </li>
                                <li id="394">


                                    <a id="bibliography_17" title=" LAPTEV I,LINDEBERG T.Space-Time Interest Points // Proc of the 9th IEEE International Conference on Computer Vision.Wa-shington,USA:IEEE,2003,I:432-439." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Space-time interest points">
                                        <b>[17]</b>
                                         LAPTEV I,LINDEBERG T.Space-Time Interest Points // Proc of the 9th IEEE International Conference on Computer Vision.Wa-shington,USA:IEEE,2003,I:432-439.
                                    </a>
                                </li>
                                <li id="396">


                                    <a id="bibliography_18" title=" HARRIS C,STEPHENS M,HARRIS C J,et al.A Combined Corner and Edge Detector // Proc of the 4th Alvey Vision Confe-rence.Berlin,Germany:Springer,1988:147-151." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A combined corner and edge detector">
                                        <b>[18]</b>
                                         HARRIS C,STEPHENS M,HARRIS C J,et al.A Combined Corner and Edge Detector // Proc of the 4th Alvey Vision Confe-rence.Berlin,Germany:Springer,1988:147-151.
                                    </a>
                                </li>
                                <li id="398">


                                    <a id="bibliography_19" title=" JOHANSSON G.Visual Perception of Biological Motion and a Model for Its Analysis.Perception &amp;amp; Psychophysics,1973,14(2):201-211." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual perception of biological motion and a model for its analysis">
                                        <b>[19]</b>
                                         JOHANSSON G.Visual Perception of Biological Motion and a Model for Its Analysis.Perception &amp;amp; Psychophysics,1973,14(2):201-211.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(10),901-908 DOI:10.16451/j.cnki.issn1003-6059.201910004            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于RGB-D图像特征的人体行为识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%94%90%E8%B6%85&amp;code=23340525&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">唐超</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%96%87%E5%89%91&amp;code=08402641&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王文剑</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E7%90%9B&amp;code=41750880&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张琛</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BD%AD%E5%8D%8E&amp;code=39548078&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">彭华</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E4%BC%9F&amp;code=33910130&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李伟</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%90%88%E8%82%A5%E5%AD%A6%E9%99%A2%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E9%99%A2&amp;code=0130127&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">合肥学院人工智能与大数据学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E8%A5%BF%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0176514&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山西大学计算机与信息技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%BB%8D%E5%85%B4%E6%96%87%E7%90%86%E5%AD%A6%E9%99%A2%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%B3%BB&amp;code=0083433&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">绍兴文理学院计算机科学与工程系</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8E%A6%E9%97%A8%E7%90%86%E5%B7%A5%E5%AD%A6%E9%99%A2%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0041540&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">厦门理工学院计算机与信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对现有的多模态特征融合方法不能有效度量不同特征的贡献度的问题,文中提出基于RGB-深度(RGB-D)图像特征的人体动作识别方法.首先获取基于RGB模态信息的方向梯度直方图特征、基于深度图像模态信息的时空兴趣点特征和基于关节模态信息的人体关节点位置特征,分别表征人体动作.采用不同距离度量公式的最近邻分类器对这3种不同模态特征表示的预测样本进行集成决策分类.在公开数据集上的实验表明,文中方法具有简单、快速,高效的特点.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E4%BD%93%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人体动作识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=RGB-%E6%B7%B1%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">RGB-深度;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%AD%A6%E4%B9%A0%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多学习器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A8%A1%E6%80%81%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多模态特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最近邻分类器;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *唐超(通讯作者),博士,副教授,主要研究方向为机器学习、计算机视觉．E-mail:tangchao77@sina.com.&lt;image id="354" type="formula" href="images/MSSB201910004_35400.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    王文剑,博士,教授．主要研究方向为机器学习、计算智能．E-mail:wjwang@sxu.edu.cn.&lt;image id="355" type="formula" href="images/MSSB201910004_35500.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    张琛,博士,讲师,主要研究方向为机器学习、计算智能．E-mail:zhangchen0304@163.com.&lt;image id="356" type="formula" href="images/MSSB201910004_35600.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    彭华,博士,讲师,主要研究方向为仿脑智能系统、人机交互、机器学习．E-mail:6195340@qq.com.&lt;image id="357" type="formula" href="images/MSSB201910004_35700.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    李伟,博士,副教授,主要研究方向为人工智能、计算机图形学、人机交互．E-mail:liweipla@sina.com.&lt;image id="358" type="formula" href="images/MSSB201910004_35800.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-06-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(No.61673249,61806068,61662025);</span>
                                <span>安徽高校优秀拔尖人才培育项目(No.gxfx2017099);</span>
                                <span>福建省出国留学奖学金项目;</span>
                                <span>厦门市科技规划指导项目(No.3502Z20179038);</span>
                                <span>合肥学院教学研究重点项目(No.018hfjyxm09)资助;</span>
                    </p>
            </div>
                    <h1><b>Human Action Recognition Using RGB-D Image Features</b></h1>
                    <h2>
                    <span>TANG Chao</span>
                    <span>WANG Wenjian</span>
                    <span>ZHANG Chen</span>
                    <span>PENG Hua</span>
                    <span>LI Wei</span>
            </h2>
                    <h2>
                    <span>School of Artificial Intelligence and Big Data,Hefei University</span>
                    <span>School of Computer and Information Technology,Shanxi University</span>
                    <span>Department of Computer Science and Engineering,Shaoxing University</span>
                    <span>School of Computer and Information Engineering, Xiamen University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Since the existing multi-modal feature fusion methods cannot measure the contribution of different features effectively, a human action recognition method based on RGB-depth image features is proposed. Firstly, the histogram of oriented gradient feature based on RGB modal information, the space-time interest points feature based on depth modal information, and the joints relative position feature based on joints modal information are acquired to express human actions, respectively. Then, nearest neighbor classifiers with different distance measurement formulas are utilized to classify prediction samples expressed by the three modal features. The experimental results on public datasets show that the proposed method is simple, fast and efficient.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Human%20Action%20Recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Human Action Recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=RGB-Depth(RGB-D)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">RGB-Depth(RGB-D);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multiple%20Learner&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multiple Learner;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multimodal%20Feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multimodal Feature;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Nearest%20Neighbor%20Classifier&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Nearest Neighbor Classifier;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    TANG Chao(Corresponding author), Ph.D.,associate professor. His research interests include machine learning and computer vision.;
                                </span>
                                <span>
                                    WANG Wenjian,Ph. D.,professor. Her research interests include machine learning and computer intelligence.;
                                </span>
                                <span>
                                    ZHANG Chen,Ph. D.,lecturer. Her research interests include machine learning and computer intelligence.;
                                </span>
                                <span>
                                    PENG Hua,Ph.D.,lecturer. His research interests include brain-like intelligent systems,human-robot interaction and machine learning.;
                                </span>
                                <span>
                                    LI Wei,Ph.D.,associate professor. His research interests include artificial Intelligence, computer graphics and human compu-ter interaction.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-06-15</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China(No.61673249,61806068,61662025);</span>
                                <span>Excellent Talents Training Project of Universities of Anhui Province(No.gxfx2017099);</span>
                                <span>Scholarship for Studying Abroad Program of Fujian;</span>
                                <span>Science and Technology Planning Guidance Project of Xiamen(No.3502Z20179038);</span>
                                <span>Key Teaching and Research Project of Hefei University(No.2018 hfjyxm09);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="234">人体行为识别是人工智能重要研究领域.随着图像传感器技术的不断发展,如RGB-深度图像(RGB-Depth, RGB-D)等传感器能提供连续的多模态数据流,该方法具有广泛应用,包括人机交互、智能监控、智能老人看护和体育运动分析等领域.</p>
                </div>
                <div class="p1">
                    <p id="235">近些年来,基于RGB视频图像信息的计算机视觉研究越来越丰富.然而,RGB图像通常只提供场景中物体的表观信息.当场景中前景与背景有部分相似纹理或颜色相近时,仅依靠有限的RGB信息很难处理这类问题.此外,RGB图像描述的对象外观对常见变化(如光照变化)无鲁棒性,这严重阻碍基于RGB的视觉算法在实际应用环境中的使用.</p>
                </div>
                <div class="p1">
                    <p id="236">早期基于视觉的方法使用RGB相机,最近已出现Microsoft Kinect这样的RGB-D传感器,价格更低,功能更多.首先,RGB-D传感器提供的深度图像对可见光的影响不变,因为深度信息是使用红外线获取.因此,可以使用相对简单的方法在深度图上分割人体形状.然而,在一幅RGB彩色图像上,必须考虑由可见光引起的变化.其次,由于深度图像不能像RGB颜色那样提供丰富的隐私信息,因此使用深度图可较好地保护被监视人员的隐私.特别是室内监控系统,隐私是主要考虑的问题.目前有许多研究者们开展基于RGB-D图像数据信息的人体行为识别工作<citation id="400" type="reference"><link href="362" rel="bibliography" /><link href="364" rel="bibliography" /><link href="366" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="237">深度图像存储传感器和场景中物体之间的欧几里得距离,较容易从杂乱的背景中提取运动人体.在<citation id="415" type="reference"><link href="368" rel="bibliography" /><link href="370" rel="bibliography" /><link href="372" rel="bibliography" /><link href="374" rel="bibliography" />文献[4]～文献[7]</citation>中将三维深度信息投影到3个二维正交平面(前视图、侧视图和俯视图)上再进行特征提取.Li等<citation id="403" type="reference"><link href="374" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>从上述正交平面的投影中提取人体轮廓的三维代表点,以模拟姿势进行识别.Yang等<citation id="404" type="reference"><link href="376" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>使用两个连续帧之间的阈值叠加深度图生成深度运动图(Depth Motion Maps, DMMs),根据DMMs计算方向梯度直方图(Histogram of Oriented Gradient, HOG),描述整个序列中的人类运动.Bulbul等<citation id="405" type="reference"><link href="378" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>采用多尺度、多方向分析的轮廓线变换改进DMMs,增强DMMs的形状特征.</p>
                </div>
                <div class="p1">
                    <p id="238">上述方法的一个局限性是未考虑三维点的邻域信息,可能会丢弃有用的信息.在深度图像序列中一组三维点计算的表面法向量可用于描述形状和运动信息<citation id="410" type="reference"><link href="380" rel="bibliography" /><link href="382" rel="bibliography" /><link href="384" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>.Oreifej等<citation id="406" type="reference"><link href="384" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>将深度序列划分为多个时空单元,计算定向4D方向直方图(Histogram of Oriented 4D, HON4D),可描述表面的法方向分布,用于动作识别.Jia等<citation id="407" type="reference"><link href="386" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>从每个动作视频中提取定向4D方向直方图,用于在张量子空间中构建张量表示,以保存判别和局部信息.Leightley等<citation id="408" type="reference"><link href="388" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>使用微软Kinect深度传感器,提出系统框架,可以自动识别和评估人类运动性损伤.Kerola等<citation id="409" type="reference"><link href="390" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出基于Kinect的具有视角不变性的人体行为识别方法,使用谱图小波变换(Spectral Graph Wavelet Transform, SGWT)技术在深度序列图上构建两种不同行为特征描述子,同时采用时间金字塔池化方案(Temporal Pyramid Pooling Scheme)消除不同动作序列长度不一致的问题.</p>
                </div>
                <div class="p1">
                    <p id="239">针对现有多模态特征融合方法不能有效度量不同特征的贡献度的问题,本文提出基于RGB-D图像特征的人体行为识别方法,充分利用RGB-D传感器提供的多模态信息提取有效人体动作特征,采用泛化能力较好的多个<i>K</i>近邻(<i>K</i> Nearest Neighbor, KNN)分类器集成决策分类.本文提出3种不同模态信息的人体动作特征:基于RGB图像信息的方向梯度直方图特征(RGB-Based HOG, RGB-HOG),具有良好几何尺度不变性;基于深度图像(Depth Image)的时空兴趣点特征(Depth-Based Space-Time Interest Points, D-STIP),保持人体动作的动态性特征,具有局部不变性;基于关节图(Joints Image)的位置特征(Joints-Based Position Feature, J-PF),具有良好动作空间结构描述能力.在G3D和CAD-60公开的RGB-D人体行为数据集上的实验表明,本文方法具有较高的识别率.</p>
                </div>
                <h3 id="240" name="240" class="anchor-tag">1 系统框架</h3>
                <div class="p1">
                    <p id="241">为了提高识别系统的实用性与鲁棒性,本文利用RGB-D传感器提供的不同图像数据分别提取3种不同的行为特征,采用多个<i>K</i>近邻分类器集成进行识别,系统流程如图1所示.方法步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="242">1)由RGB-D传感器获得同步的RGB、Depth和Joints图像.</p>
                </div>
                <div class="p1">
                    <p id="243">2)对输入的RGB图像进行灰度化处理,然后提取方向梯度直方图特征(RGB-HOG),对输入的Depth深度图像计算时空兴趣点特征(D-STIP),在获得三维骨架图像上提取位置特征(J-PF).</p>
                </div>
                <div class="p1">
                    <p id="244">3)采用基于3种不同距离度量公式的最近邻分类器<i>KNN</i><sub><i>i</i></sub>(<i>i</i>=1,2,3)分别对上述3种不同特征表达下的同一动作进行分类,每个<i>KNN</i><sub><i>i</i></sub>(<i>i</i>=1,2,3)给出最相似的<i>top</i>_<i>k</i><sub><i>i</i></sub>(<i>i</i>=1,2,3)个动作类别预测.然后采用基于权重的统计方法计算{<i>top</i>_<i>k</i><sub>1</sub>,<i>top</i>_<i>k</i><sub>2</sub>,<i>top</i>_<i>k</i><sub>3</sub>}集合最置信的类别作为最后的预测结果.</p>
                </div>
                <div class="area_img" id="245">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910004_245.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文方法系统框架图" src="Detail/GetImg?filename=images/MSSB201910004_245.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文方法系统框架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910004_245.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Architecture of the proposed method</p>

                </div>
                <h3 id="246" name="246" class="anchor-tag">2 特征提取</h3>
                <h4 class="anchor-tag" id="247" name="247"><b>2.1 RGB-HOG</b>特征</h4>
                <div class="p1">
                    <p id="248">HOG特征<citation id="411" type="reference"><link href="392" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>已成功应用于行人检测及许多与图像特征相关的问题,具有计算简单、高效的特点.HOG是在图像空域上计算得到,对几何和光照形变具有不变性特点.因此可从RGB图上提取HOG特征作为行为描述子.HOG使用图像梯度和边缘方向密度描述图像的局部信息,具体流程如图2所示.</p>
                </div>
                <div class="area_img" id="249">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910004_249.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 HOG主要流程图" src="Detail/GetImg?filename=images/MSSB201910004_249.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 HOG主要流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910004_249.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Main steps of HOG algorithm</p>

                </div>
                <div class="p1">
                    <p id="250">HOG步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="251"><b>算法</b> HOG</p>
                </div>
                <div class="p1">
                    <p id="252"><b>输入</b> 图像<i>f</i>(<i>x</i>,<i>y</i>)</p>
                </div>
                <div class="p1">
                    <p id="253"><b>输出</b> RGB-HOG</p>
                </div>
                <div class="p1">
                    <p id="254">step 1 图像灰度化</p>
                </div>
                <div class="p1">
                    <p id="255"><i>f</i>(<i>x</i>,<i>y</i>)←<i>rgb</i>2<i>gray</i>(<i>f</i>(<i>x</i>,<i>y</i>)).</p>
                </div>
                <div class="p1">
                    <p id="256">step 2 为了减少局部阴影和光照的影响,采用Gamma压缩对图像进行噪声抑制:</p>
                </div>
                <div class="p1">
                    <p id="257"><i>I</i>(<i>x</i>,<i>y</i>)←<i>GammaCorrection</i>(<i>f</i>(<i>x</i>,<i>y</i>)).</p>
                </div>
                <div class="p1">
                    <p id="258">step 3 计算每个像素的梯度,包括大小与方向:</p>
                </div>
                <div class="p1">
                    <p id="259" class="code-formula">
                        <mathml id="259"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>Ι</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mn>1</mn><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>-</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>-</mo><mn>1</mn><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mi>Ι</mi><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>-</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mo stretchy="false">|</mo><mo>∇</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mo>=</mo><msqrt><mrow><mi>Ι</mi><msubsup><mrow></mrow><mi>x</mi><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>Ι</mi><msubsup><mrow></mrow><mi>y</mi><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></msqrt><mo>,</mo></mtd></mtr><mtr><mtd><mi>Φ</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>a</mtext><mtext>r</mtext><mtext>c</mtext><mspace width="0.25em" /><mi>tan</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mfrac><mrow><mi>Ι</mi><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mrow><mi>Ι</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">)</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="260">step 4 将梯度图像∇<i>I</i>(<i>x</i>,<i>y</i>)划分为<i>N</i>个细胞(Cell),采用9个Bin直方图统计每个细胞内的梯度方向信息,即细胞内的每个像素的梯度方向属于0～360°的9个区间之一:</p>
                </div>
                <div class="p1">
                    <p id="261" class="code-formula">
                        <mathml id="261"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>C</mi><mi>e</mi><mi>l</mi><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>Ν</mi><mo stretchy="false">)</mo><mo>←</mo><mo>∇</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>.</mo></mtd></mtr><mtr><mtd><mi>C</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>h</mi><mi>o</mi><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>Ν</mi><mo stretchy="false">)</mo><mo>←</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mi>B</mi><mi>i</mi><mi>n</mi><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false">(</mo><mi>C</mi><mi>e</mi><mi>l</mi><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>Ν</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="262">step 5 将若干个细胞组合成一个Block,串联每个Block内Cell的梯度方向直方图,形成Block直方图特征:</p>
                </div>
                <div class="p1">
                    <p id="263" class="code-formula">
                        <mathml id="263"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>B</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>k</mi><mi>h</mi><mi>o</mi><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>Μ</mi><mo stretchy="false">)</mo><mo>←</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mi>B</mi><mi>i</mi><mi>n</mi><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false">(</mo><mi>B</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>k</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>Μ</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="264">step 6</p>
                </div>
                <div class="p1">
                    <p id="265"><i>HOG</i>←{<i>Blockhog</i><sub>1</sub>,<i>Blockhog</i><sub>2</sub>,…,<i>Blockhog</i><sub><i>M</i></sub>}.</p>
                </div>
                <h4 class="anchor-tag" id="266" name="266"><b>2.2 D-STIP</b>特征</h4>
                <div class="p1">
                    <p id="267">Laptev等<citation id="412" type="reference"><link href="394" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>引入局部时空兴趣点(Local Space-time Interest Points, STIP)作为运动特征表征人体行为.STIP特征对应于在空间和时间上具有显著局部变化的点.STIP提供一种描述运动事件的简洁表示形式,又避免背景分割和动运跟踪等相关的预处理工作.Laptev等通过将Harris角点<citation id="413" type="reference"><link href="396" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>检测器扩展至时空域检测时空兴趣点,找出在视频序列中在空间和时间上具有显著变化的点,即<i>p</i>(<i>x</i>,<i>y</i>,<i>t</i>)点,使时空兴趣点响应值<i>H</i>取得局部最大值:</p>
                </div>
                <div class="p1">
                    <p id="269"><i>H</i>(<i>p</i>;<i>Σ</i>)=det(<i>μ</i>(<i>p</i>;<i>Σ</i>))+<i>k</i>trace<sup>3</sup>(<i>μ</i>(<i>p</i>;<i>Σ</i>)),</p>
                </div>
                <div class="p1">
                    <p id="270">其中,det(·)为矩阵的行列式,trace(·)为矩阵的直迹, <i>μ</i>为二阶矩矩阵</p>
                </div>
                <div class="p1">
                    <p id="273"><image id="359" type="formula" href="images/MSSB201910004_35900.jpg" display="inline" placement="inline"><alt></alt></image><image id="359" type="formula" href="images/MSSB201910004_35901.jpg" display="inline" placement="inline"><alt></alt></image>为高斯核函数,<i>σ</i><mathml id="325"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>d</mi><mn>2</mn></msubsup></mrow></math></mathml>为空间尺度因子,<i>τ</i><mathml id="326"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>d</mi><mn>2</mn></msubsup></mrow></math></mathml>为时间尺度因子,<i>D</i>(·)为深度视频图像序列.</p>
                </div>
                <div class="p1">
                    <p id="274">本文在深度图上提取时空兴趣点,并给每个时空兴趣点划出一个包围块,计算该包围块的梯度直方图与光流直方图作为特征,由此得到基于深度图的时空兴趣点特征(D-STIP).该特征计算方法简单,具有鲁棒性.</p>
                </div>
                <h4 class="anchor-tag" id="275" name="275"><b>2.3 J-PF</b>特征</h4>
                <div class="p1">
                    <p id="276">Johansson<citation id="414" type="reference"><link href="398" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>研究显示人体三维关节位置有助于识别行为.现有大多数基于骨骼的动作识别方法都将人体骨骼视为一组点集或一组连接的刚性段.这些方法未考虑不同身体部位之间复杂的结构,但这些结构可以反映人体动作的空间变化.为了充分考虑不同关节之间的相对空间结构,本文提出关节相对位置特征以描述人体行为.</p>
                </div>
                <div class="p1">
                    <p id="277">假设人体由<i>N</i>个关节表示,图3显示一个具有20个关节的骨骼示例.设</p>
                </div>
                <div class="p1">
                    <p id="278"><i>J</i><sup><i>i</i></sup>(<i>t</i>)=(<i>J</i><mathml id="327"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>x</mi><mi>i</mi></msubsup></mrow></math></mathml>(<i>t</i>),<i>J</i><mathml id="328"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>y</mi><mi>i</mi></msubsup></mrow></math></mathml>(<i>t</i>),<i>J</i><mathml id="329"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>z</mi><mi>i</mi></msubsup></mrow></math></mathml>(<i>t</i>)), 1≤<i>i</i>≤<i>N</i>,</p>
                </div>
                <div class="p1">
                    <p id="279">为第<i>i</i>个关节点的三维坐标.本文采用极坐标的形式描述关节点三维相对位置信息:</p>
                </div>
                <div class="p1">
                    <p id="280" class="code-formula">
                        <mathml id="280"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><msqrt><mrow><mrow><mo stretchy="false">(</mo><mi>J</mi><msubsup><mrow></mrow><mi>y</mi><mi>i</mi></msubsup><mo>-</mo><mi>J</mi><msubsup><mrow></mrow><mi>y</mi><mi>c</mi></msubsup><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo stretchy="false">(</mo><mi>J</mi><msubsup><mrow></mrow><mi>x</mi><mi>i</mi></msubsup><mo>-</mo><mi>J</mi><msubsup><mrow></mrow><mi>x</mi><mi>c</mi></msubsup><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mo>,</mo></mtd></mtr><mtr><mtd><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mtext>a</mtext><mtext>r</mtext><mtext>c</mtext><mspace width="0.25em" /><mi>tan</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>J</mi><msubsup><mrow></mrow><mi>y</mi><mi>i</mi></msubsup><mo>-</mo><mi>J</mi><msubsup><mrow></mrow><mi>y</mi><mi>c</mi></msubsup></mrow><mrow><mi>J</mi><msubsup><mrow></mrow><mi>x</mi><mi>i</mi></msubsup><mo>-</mo><mi>J</mi><msubsup><mrow></mrow><mi>x</mi><mi>c</mi></msubsup></mrow></mfrac><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mn>1</mn><mn>9</mn><mo>,</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="281">其中,(<i>J</i><mathml id="330"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>x</mi><mi>c</mi></msubsup></mrow></math></mathml>,<i>J</i><mathml id="331"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>y</mi><mi>c</mi></msubsup></mrow></math></mathml>)和(<i>J</i><mathml id="332"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>x</mi><mi>i</mi></msubsup></mrow></math></mathml>,<i>J</i><mathml id="333"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>y</mi><mi>i</mi></msubsup></mrow></math></mathml>)分别为人体躯干chest关节点和其余关节点在直角坐标系下的坐标.从而得到具有平移变换、尺度变换和旋转变换不变性的基于Joints的位置特征(J-PF).</p>
                </div>
                <div class="area_img" id="282">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910004_282.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 骨骼关节点图" src="Detail/GetImg?filename=images/MSSB201910004_282.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 骨骼关节点图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910004_282.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Sketch map of skeletal joint point</p>

                </div>
                <h3 id="283" name="283" class="anchor-tag">3 人体行为识别方法</h3>
                <div class="p1">
                    <p id="284">KNN学习是一种统计分类器,属于惰性学习.针对行为识别问题,从训练集中搜索与给定新动作距离最近的<i>K</i>个动作,根据这<i>K</i>个动作所属的类别判定新动作所属的动作类别.本文提出基于多模态特征训练集的多学习器集成识别方法,可以更有效地识别新动作类别,充分利用不同学习器的偏置效应,提升学习的泛化能力.具体算法步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="285">1)分别使用不同的模态信息描述训练动作特征向量集:</p>
                </div>
                <div class="p1">
                    <p id="286" class="code-formula">
                        <mathml id="286"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>Τ</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi><msub><mrow></mrow><mrow><mtext>R</mtext><mtext>G</mtext><mtext>B</mtext><mo>-</mo><mtext>Η</mtext><mtext>Ο</mtext><mtext>G</mtext></mrow></msub><mo>=</mo><mo stretchy="false">{</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">}</mo><mo>,</mo></mtd></mtr><mtr><mtd><mi>Τ</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi><msub><mrow></mrow><mrow><mtext>D</mtext><mo>-</mo><mtext>S</mtext><mtext>Τ</mtext><mtext>Ι</mtext><mtext>Ρ</mtext></mrow></msub><mo>=</mo><mo stretchy="false">{</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">}</mo><mo>,</mo></mtd></mtr><mtr><mtd><mi>Τ</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi><msub><mrow></mrow><mrow><mtext>J</mtext><mo>-</mo><mtext>Ρ</mtext><mtext>F</mtext></mrow></msub><mo>=</mo><mo stretchy="false">{</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="287">2)确定待预测动作<i>Θ</i>的3种不同模态描述下的向量表示:</p>
                </div>
                <div class="p1">
                    <p id="288" class="code-formula">
                        <mathml id="288"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mo stretchy="false">(</mo><mi>R</mi><mi>G</mi><mi>B</mi><mo>-</mo><mi>Η</mi><mi>Ο</mi><mi>G</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>R</mi><mi>G</mi><mi>B</mi><mo>-</mo><mi>Η</mi><mi>Ο</mi><mi>G</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>R</mi><mi>G</mi><mi>B</mi><mo>-</mo><mi>Η</mi><mi>Ο</mi><mi>G</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mo stretchy="false">(</mo><mi>D</mi><mo>-</mo><mi>S</mi><mi>Τ</mi><mi>Ι</mi><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>D</mi><mo>-</mo><mi>S</mi><mi>Τ</mi><mi>Ι</mi><mi>Ρ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>D</mi><mo>-</mo><mi>S</mi><mi>Τ</mi><mi>Ι</mi><mi>Ρ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mo stretchy="false">(</mo><mi>J</mi><mo>-</mo><mi>Ρ</mi><mi>F</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>J</mi><mo>-</mo><mi>Ρ</mi><mi>F</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>J</mi><mo>-</mo><mi>Ρ</mi><mi>F</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="289">其中,<i><b>x</b></i><sup>*</sup>、<i><b>y</b></i><sup>*</sup>和<i><b>z</b></i><sup>*</sup>为待预测动作<i>Θ</i>的3种不同的特征向量表示形式.</p>
                </div>
                <div class="p1">
                    <p id="290">3)在3种不同的训练动作集中分别采用不同的距离度量公式选出待识别动作最相似的<i>top</i>_<i>k</i><sub>1</sub>,<i>top</i>_<i>k</i><sub>2</sub>和<i>top</i>_<i>k</i><sub>3</sub>个动作,不同模态下的特征相似度计算公式分别如下:</p>
                </div>
                <div class="p1">
                    <p id="291" class="code-formula">
                        <mathml id="291"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mrow><mi>S</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><msub><mrow></mrow><mrow><mtext>R</mtext><mtext>G</mtext><mtext>B</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup><mo>,</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">[</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mrow></mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mo>,</mo></mtd></mtr><mtr><mtd><mrow><mi>S</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><msub><mrow></mrow><mrow><mtext>D</mtext><mtext>e</mtext><mtext>p</mtext><mtext>t</mtext><mtext>h</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mo>*</mo></msup><mo>,</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false">|</mo></mstyle><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo stretchy="false">|</mo><mo>,</mo></mtd></mtr><mtr><mtd><mrow><mi>S</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><msub><mrow></mrow><mrow><mtext>J</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mo>*</mo></msup><mo>,</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mo>*</mo></msup><mo>-</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><msup><mo stretchy="false">)</mo><mo>′</mo></msup><mi>V</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mo>*</mo></msup><mo>-</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="292">其中<i>V</i><sup>-1</sup>为<i><b>z</b></i><sup>*</sup>和<i><b>z</b></i>所在数据集的协方差函数.</p>
                </div>
                <div class="p1">
                    <p id="293">4)在与待预测动作的<i>top</i>_<i>k</i><sub>1</sub>+<i>top</i>_<i>k</i><sub>2</sub>+<i>top</i>_<i>k</i><sub>3</sub>个最相似动作中,依次计算每类的权重:</p>
                </div>
                <div class="p1">
                    <p id="294" class="code-formula">
                        <mathml id="294"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false">(</mo><mi>Θ</mi><mo>,</mo><mi>C</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>Κ</mi><mi>Ν</mi><mi>Ν</mi></mrow></munder><mrow><mi>S</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow></mstyle><msub><mrow></mrow><mrow><mtext>R</mtext><mtext>G</mtext><mtext>B</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup><mo>,</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mi>A</mi><mi>t</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>b</mi><mi>u</mi><mi>t</mi><mi>e</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>,</mo><mi>C</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>Κ</mi><mi>Ν</mi><mi>Ν</mi></mrow></munder><mrow><mi>S</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow></mstyle><msub><mrow></mrow><mrow><mtext>D</mtext><mtext>e</mtext><mtext>p</mtext><mtext>t</mtext><mtext>h</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mo>*</mo></msup><mo>,</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mi>A</mi><mi>t</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>b</mi><mi>u</mi><mi>t</mi><mi>e</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>j</mi></msub><mo>,</mo><mi>C</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>Κ</mi><mi>Ν</mi><mi>Ν</mi></mrow></munder><mrow><mi>S</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow></mstyle><msub><mrow></mrow><mrow><mtext>J</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mo>*</mo></msup><mo>,</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mi>A</mi><mi>t</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>b</mi><mi>u</mi><mi>t</mi><mi>e</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo>,</mo><mi>C</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="295">其中,<i><b>x</b></i><sup>*</sup>、<i><b>y</b></i><sup>*</sup>和<i><b>z</b></i><sup>*</sup>为动作<i>Θ</i>不同的模态表示下的特征向量,<i>Similarity</i>(·)为相似度计算函数,<i>Attribute</i>(·)为类别属性函数.如果<i><b>x</b></i><sup>*</sup>属于类<i>C</i><sub><i>j</i></sub>,那么函数<i>Attribute</i>(·)值为1,否则为0.</p>
                </div>
                <div class="p1">
                    <p id="296">5)对比类的权重,将待预测动作分到权重最大的类别.</p>
                </div>
                <h3 id="297" name="297" class="anchor-tag">4 实验及结果分析</h3>
                <div class="p1">
                    <p id="298">实验采用交叉验证的方法(Cross-Validation)训练识别模型和测试性能.同时采用精确率(Precision)、召回率(Recall)和F值(F-measure)衡量算法的效果:</p>
                </div>
                <div class="p1">
                    <p id="299" class="code-formula">
                        <mathml id="299"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mo>,</mo></mtd></mtr><mtr><mtd><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mo>,</mo></mtd></mtr><mtr><mtd><mi>F</mi><mo>-</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>s</mi><mi>u</mi><mi>r</mi><mi>e</mi><mo>=</mo><mfrac><mrow><mn>2</mn><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>⋅</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><mrow><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="300">对于二分类问题来说:TP对应被分类模型正确预测的正样本数;FP对应被分类模型错误预测为正类的负样本数;FN对应被分类模型错误预测为负类的正样本数.这些公式可以推广到多分类问题中.</p>
                </div>
                <div class="p1">
                    <p id="301">G3D数据集(http://dipersec.king.ac.uk/G3D/</p>
                </div>
                <div class="p1">
                    <p id="302">G3D.html)包含一系列使用RGB-D传感器Micro-soft Kinect捕获的游戏操作动作,记录同步的RGB、深度和骨架数据.</p>
                </div>
                <div class="p1">
                    <p id="303">CAD-60数据集(http://pr.cs.cornell.edu/humanactivities/data.php)由5个动作执行人在5个不同场景下完成的动作组成,活动使用Microsoft Kinect传感器记录.每个视频都带有RGB图像、深度图像和跟踪骨架.</p>
                </div>
                <div class="p1">
                    <p id="304">实验1中图4以混淆矩阵的方式展示在G3D数据集上使用单一模态信息特征和多模态信息特征下的识别率.由图中可看出,当采用多模态信息特征时,20个动作类别的识别准确率普遍高于单一模态特征下的识别率,同时在defend,throw bowling ball,aim and fire gun,wave,flap和clap动作上取得100%的正确率.</p>
                </div>
                <div class="area_img" id="360">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910004_36000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 G3D数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵" src="Detail/GetImg?filename=images/MSSB201910004_36000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 G3D数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910004_36000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Confusion matrix of action recognition using single modal feature and multimodal feature on G3D datasets</p>

                </div>
                <div class="area_img" id="360">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910004_36001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 G3D数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵" src="Detail/GetImg?filename=images/MSSB201910004_36001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 G3D数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910004_36001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Confusion matrix of action recognition using single modal feature and multimodal feature on G3D datasets</p>

                </div>
                <div class="p1">
                    <p id="312">图5以混淆矩阵的方式展示在CAD-60数据集上使用单一模态信息特征和多模态信息特征下的识别率.通过对比可发现,本文方法在CAD-60数据集上取得不错的识别率,达到94%,同时在drink water, stirring, relaxing on couch 和writing on white-board动作上取得100%的正确率.</p>
                </div>
                <div class="area_img" id="361">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910004_36100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 CAD-60数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵" src="Detail/GetImg?filename=images/MSSB201910004_36100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 CAD-60数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910004_36100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Confusion matrix of action recognition using single modal feature and multimodal feature on CAD-60 datasets</p>

                </div>
                <div class="area_img" id="361">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910004_36101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 CAD-60数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵" src="Detail/GetImg?filename=images/MSSB201910004_36101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 CAD-60数据集上使用单一模态特征和多模态特征下的识别结果混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910004_36101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Confusion matrix of action recognition using single modal feature and multimodal feature on CAD-60 datasets</p>

                </div>
                <div class="p1">
                    <p id="316">采用精确率、召回率和F值评估使用单一模态信息特征和多模态信息特征情况下的行为识别率如表1所示.由表可看出,采用多模态信息特征下本文方法的行为识别率高于采用单一模态信息特征表示下的行为识别率.</p>
                </div>
                <div class="area_img" id="317">
                                            <p class="img_tit">
                                                <b>表1 使用单模态特征和多模态特征下本文方法的识别率</b>
                                                    <br />
                                                Table 1 Recognition accuracy using single modal feature and multimodal feature 
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910004_31700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201910004_31700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910004_31700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 使用单模态特征和多模态特征下本文方法的识别率" src="Detail/GetImg?filename=images/MSSB201910004_31700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="318">在实验2中,对比方法如下:Boosting、Bagging、支持向量机(SVM)和人工神经网络(Artificial Neural Network, ANN).表2为各方法识别率对比.</p>
                </div>
                <div class="area_img" id="319">
                                            <p class="img_tit">
                                                <b>表2 各方法的识别率对比</b>
                                                    <br />
                                                Table 2 Recognition rate comparison of different methods 
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910004_31900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201910004_31900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910004_31900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 各方法的识别率对比" src="Detail/GetImg?filename=images/MSSB201910004_31900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="320">由表2可知,本文方法取得最高的识别率(94%).同时,本文方法整体性能也优于对比方法,达到87%的平均正确率.</p>
                </div>
                <h3 id="321" name="321" class="anchor-tag">5 结 束 语</h3>
                <div class="p1">
                    <p id="322">本文提出基于RGB-D图像特征的人体动作识别方法,利用RGB-D传感器获得3种不同的模态信息,分别提取RGB-HOG特征、D-STIP特征和J-PF特征,并采用多学习器集成的学习策略,充分利用不同学习器的偏置效应.在标准公开的数据集上取得较优的识别率,同时实时性、鲁棒性较好.行为识别是一个充满挑战性的开放性课题,虽然本文方法在公开数据集上取得较优的实验结果,但是仍有许多问题需要深入研究.为了获得泛化能力较强的分类模型,通常需要大量的标记训练视频样本,这需要大量的人工标记劳力,给建模带来实际的困难.如何利用大量触手可及的未标记视频样本提升学习系统的性能是一个值得研究的方向.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="362">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A survey of depth and inertial sensor fusion for human action recognition">

                                <b>[1]</b> CHEN C,JAFARI R,KEHTARNAVAZ N.A Survey of Depth and Inertial Sensor Fusion for Human Action Recognition.Multimedia Tools and Applications,2017,76(3):4405-4425.
                            </a>
                        </p>
                        <p id="364">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=RGB-D datasets using microsoft kinect or similar sen-sors:a survey">

                                <b>[2]</b> CAI Z Y,HAN J G,LIU L,et al.RGB-D Datasets Using Microsoft Kinect or Similar Sensors:A Survey.Multimedia Tools and Applications,2017,76(3):4313-4355.
                            </a>
                        </p>
                        <p id="366">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6B4657B45365879DB85FFCC162CFAC90&amp;v=MTc5NDBOaWZPZmJYS0d0ZkpxUDFCWWVnSkNYUSt4bUpoNGpvTFBneVJyUlEzQ3NUbE5yT2ZDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3N20ydzZrPQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> ZHANG J,LI W Q,OGUNBONA P O,et al.RGB-D-Based Action Recognition Datasets:A Survey.Pattern Recognition,2016,60:86-105.
                            </a>
                        </p>
                        <p id="368">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Action Recognition Using 3D Histograms of Texture and A Multi-Class Boosting Classifier">

                                <b>[4]</b> ZHANG B C,YANG Y,CHEN C,et al.Action Recognition Using 3D Histograms of Texture and a Multi-class Boosting Classifier.IEEE Transactions on Image Processing,2017,26(10):4648-4660.
                            </a>
                        </p>
                        <p id="370">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-temporal depth motion mapsbased local binary patterns for 3-D human action recognition">

                                <b>[5]</b> CHEN C,LIU M Y,LIU H,et al.Multi-temporal Depth Motion Maps-Based Local Binary Patterns for 3-D Human Action Recognition.IEEE Access,2017,5:22590-22604.
                            </a>
                        </p>
                        <p id="372">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time human action recognition based on depth motion maps">

                                <b>[6]</b> CHEN C,LIU K,KEHTARNAVAZ N.Real-Time Human Action Recognition Based on Depth Motion Maps.Journal of Real-Time Image Processing,2016,12(1):155-163.
                            </a>
                        </p>
                        <p id="374">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Action recognition based on a bag of 3Dpoints">

                                <b>[7]</b> LI W Q,ZHANG Z Y,LIU Z C.Action Recognition Based on a Bag of 3D Points // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2010:9-14.
                            </a>
                        </p>
                        <p id="376">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recognizing actions using depth motion maps-based histograms of oriented gradients">

                                <b>[8]</b> YANG X D,ZHANG C Y,TIAN Y L.Recognizing Actions Using Depth Motion Maps-Based Histograms of Oriented Gradients // Proc of the 20th ACM International Conference on Multimedia.New York,USA:ACM,2012 :1057-1060.
                            </a>
                        </p>
                        <p id="378">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human Action Recognition Based on DMMs,HOGs and Contourlet Transform">

                                <b>[9]</b> BULBUL M F,JIANG Y S,MA J W.Human Action Recognition Based on DMMs,HOGs and Contourlet Transform // Proc of the IEEE International Conference on Multimedia Big Data.Washington,USA:IEEE,2015:389-394.
                            </a>
                        </p>
                        <p id="380">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Super normal vector for activity recognition using depth sequences">

                                <b>[10]</b> YANG X D,TIAN Y L.Super Normal Vector for Activity Recognition Using Depth Sequences // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2014:804-811.
                            </a>
                        </p>
                        <p id="382">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Grassmannian Representation of Motion Depth for 3D Human Gesture and Action Recognition">

                                <b>[11]</b> SLAMA R,WANNOUS H,DAOUDI M.Grassmannian Representation of Motion Depth for 3D Human Gesture and Action Recognition // Proc of the 22nd International Conference on Pattern Recognition.Washington,USA:IEEE,2014:3499-3504.
                            </a>
                        </p>
                        <p id="384">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hon4d:Histogram of oriented 4d normals for activity recognition from depth sequences">

                                <b>[12]</b> OREIFEJ O,LIU Z C.HON4D:Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2013 :716-723.
                            </a>
                        </p>
                        <p id="386">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Low-rank tensor subspace learning for RGB-D action recognition">

                                <b>[13]</b> JIA C C,FU Y.Low-Rank Tensor Subspace Learning for RGB-D Action Recognition.IEEE Transactions on Image Processing,2016,25(10):4641-4652.
                            </a>
                        </p>
                        <p id="388">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automated Analysis and Quantification of Human Mobility Using a Depth Sensor">

                                <b>[14]</b> LEIGHTLEY D,MCPHEE J S,YAP M H.Automated Analysis and Quantification of Human Mobility Using a Depth Sensor.IEEE Journal of Biomedical and Health Informatics,2017,21(4):939-948.
                            </a>
                        </p>
                        <p id="390">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC6324117676DC7279978FCE5BDAA120A&amp;v=MTQwMjNCQ01PVlI3cnVDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3N20ydzZrPU5pZk9mY0MrSGRQSXJvNUNZdXdKZUE4K3pSRWE0emgxUGd5WHFXQg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> KEROLA T,INOUE N,SHINODA K.Cross-View Human Action Recognition from Depth Maps Using Spectral Graph Sequences.Computer Vision and Image Understanding,2017,154:108-126.
                            </a>
                        </p>
                        <p id="392">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for hu- man detection">

                                <b>[16]</b> DALAL N,TRIGGS B.Histograms of Oriented Gradients for Human Detection // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2005,I:886-893.
                            </a>
                        </p>
                        <p id="394">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Space-time interest points">

                                <b>[17]</b> LAPTEV I,LINDEBERG T.Space-Time Interest Points // Proc of the 9th IEEE International Conference on Computer Vision.Wa-shington,USA:IEEE,2003,I:432-439.
                            </a>
                        </p>
                        <p id="396">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A combined corner and edge detector">

                                <b>[18]</b> HARRIS C,STEPHENS M,HARRIS C J,et al.A Combined Corner and Edge Detector // Proc of the 4th Alvey Vision Confe-rence.Berlin,Germany:Springer,1988:147-151.
                            </a>
                        </p>
                        <p id="398">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual perception of biological motion and a model for its analysis">

                                <b>[19]</b> JOHANSSON G.Visual Perception of Biological Motion and a Model for Its Analysis.Perception &amp; Psychophysics,1973,14(2):201-211.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201910004" />
        <input id="dpi" type="hidden" value="96" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201910004&amp;v=MjQ3NzFuRnkva1dyekpLRDdZYkxHNEg5ak5yNDlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
