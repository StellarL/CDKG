<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131448694405000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201905012%26RESULT%3d1%26SIGN%3dgTSfCB8wyJ9o3S3oG3Q5UeOHLlU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201905012&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201905012&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201905012&amp;v=MTkwMzZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpnVmJyQUtEN1liTEc0SDlqTXFvOUU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#71" data-title="1 面向交通场景的语义分割模型 ">1 面向交通场景的语义分割模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#76" data-title="&lt;b&gt;1.1 密集连接模块与空洞卷积&lt;/b&gt;"><b>1.1 密集连接模块与空洞卷积</b></a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;1.2 全卷积化&lt;/b&gt;&lt;b&gt;DenseNet&lt;/b&gt;&lt;b&gt;121网络&lt;/b&gt;"><b>1.2 全卷积化</b><b>DenseNet</b><b>121网络</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;1.3 多尺度特征融合模块&lt;/b&gt;"><b>1.3 多尺度特征融合模块</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="2  实验及结果分析 ">2  实验及结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#160" data-title="3 结 束 语 ">3 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="图1 基于DenseNet的多尺度语义分割模型">图1 基于DenseNet的多尺度语义分割模型</a></li>
                                                <li><a href="#83" data-title="图2 带有空洞卷积的Dense模块">图2 带有空洞卷积的Dense模块</a></li>
                                                <li><a href="#87" data-title="图3 过渡层">图3 过渡层</a></li>
                                                <li><a href="#92" data-title="图4 多尺度特征融合模块">图4 多尺度特征融合模块</a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;表1 2种模型的分割精度&lt;/b&gt;"><b>表1 2种模型的分割精度</b></a></li>
                                                <li><a href="#115" data-title="图5 2种模型的收敛情况">图5 2种模型的收敛情况</a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;表2 3种全卷积化编码器对分割模型的影响&lt;/b&gt;"><b>表2 3种全卷积化编码器对分割模型的影响</b></a></li>
                                                <li><a href="#139" data-title="图6 4种池化尺度下的预测结果可视化">图6 4种池化尺度下的预测结果可视化</a></li>
                                                <li><a href="#139" data-title="图6 4种池化尺度下的预测结果可视化">图6 4种池化尺度下的预测结果可视化</a></li>
                                                <li><a href="#139" data-title="图6 4种池化尺度下的预测结果可视化">图6 4种池化尺度下的预测结果可视化</a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;表3 5种方法在19类物体上的分割精度&lt;/b&gt;"><b>表3 5种方法在19类物体上的分割精度</b></a></li>
                                                <li><a href="#144" data-title="&lt;b&gt;表4 4种方法的参数量及存储空间&lt;/b&gt;"><b>表4 4种方法的参数量及存储空间</b></a></li>
                                                <li><a href="#147" data-title="&lt;b&gt;表5 3种方法的综合性能&lt;/b&gt;"><b>表5 3种方法的综合性能</b></a></li>
                                                <li><a href="#148" data-title="图7 3种模型的收敛情况">图7 3种模型的收敛情况</a></li>
                                                <li><a href="#203" data-title="图8 3种方法预测结果可视化">图8 3种方法预测结果可视化</a></li>
                                                <li><a href="#158" data-title="图9 3种方法在19类物体上的预测精度">图9 3种方法在19类物体上的预测精度</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" GARCIA-GARCIA A, ORTS-ESCOLANO S, OPREA S, &lt;i&gt;et al&lt;/i&gt;.A Review on Deep Learning Techniques Applied to Semantic Segmentation[C/OL].[2018-08-15].https://arxiv.org/pdf/1704.06857.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Review on Deep Learning Techniques Applied to Semantic Segmentation[C/OL]">
                                        <b>[1]</b>
                                         GARCIA-GARCIA A, ORTS-ESCOLANO S, OPREA S, &lt;i&gt;et al&lt;/i&gt;.A Review on Deep Learning Techniques Applied to Semantic Segmentation[C/OL].[2018-08-15].https://arxiv.org/pdf/1704.06857.pdf.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 张新明, 祝晓斌, 蔡强, 等.图像语义分割深度学习模型综述.高技术通讯, 2017, 27 (9) :808-815. (ZHANG X M, ZHU X B, CAI Q, &lt;i&gt;et al&lt;/i&gt;.Survey of the Deep Learning Models for Image Semantic Segmentation.Chinese High Technology Letters, 2017, 27 (9) :808-815.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GJSX2017Z1005&amp;v=MDU2NjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emdWYnJBSWlmWWRyRzRIOWFtcm85RllZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         张新明, 祝晓斌, 蔡强, 等.图像语义分割深度学习模型综述.高技术通讯, 2017, 27 (9) :808-815. (ZHANG X M, ZHU X B, CAI Q, &lt;i&gt;et al&lt;/i&gt;.Survey of the Deep Learning Models for Image Semantic Segmentation.Chinese High Technology Letters, 2017, 27 (9) :808-815.) 
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 姜枫, 顾庆, 郝慧珍, 等.基于内容的图像分割方法综述.软件学报, 2017, 28 (1) :160-183. (JIANG F, GU Q, HAO H Z, &lt;i&gt;et a&lt;/i&gt;l.Survey on Content-Based Image Segmentation Methods.Journal of Software, 2017, 28 (1) :160-183.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201701008&amp;v=MDM1NDBOeWZUYkxHNEg5Yk1ybzlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6Z1ZickE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         姜枫, 顾庆, 郝慧珍, 等.基于内容的图像分割方法综述.软件学报, 2017, 28 (1) :160-183. (JIANG F, GU Q, HAO H Z, &lt;i&gt;et a&lt;/i&gt;l.Survey on Content-Based Image Segmentation Methods.Journal of Software, 2017, 28 (1) :160-183.) 
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" ZHAO H S, QI X J, SHEN X Y, &lt;i&gt;et al&lt;/i&gt;.ICNet for Real Time Semantic Segmentation on High-Resolution Images[C/OL].[2018-08-15].https://arxiv.org/pdf/1704.08545.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ICNet for Real Time Semantic Segmentation on High-Resolution Images[C/OL]">
                                        <b>[4]</b>
                                         ZHAO H S, QI X J, SHEN X Y, &lt;i&gt;et al&lt;/i&gt;.ICNet for Real Time Semantic Segmentation on High-Resolution Images[C/OL].[2018-08-15].https://arxiv.org/pdf/1704.08545.pdf.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 李琳辉, 钱波, 连静, 等.基于卷积神经网络的交通场景语义分割方法研究.通信学报, 2018, 39 (4) :123-130. (LI L H, QIAN B, LIAN J, &lt;i&gt;et al&lt;/i&gt;.Study on Traffic Scene Semantic Segmentation Method Based on Convolutional Neural Network.Journal on Communications, 2018, 39 (4) :123-130.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB201804013&amp;v=MTU1OTBSbkZ5emdWYnJBTVRYVGJMRzRIOW5NcTQ5RVo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         李琳辉, 钱波, 连静, 等.基于卷积神经网络的交通场景语义分割方法研究.通信学报, 2018, 39 (4) :123-130. (LI L H, QIAN B, LIAN J, &lt;i&gt;et al&lt;/i&gt;.Study on Traffic Scene Semantic Segmentation Method Based on Convolutional Neural Network.Journal on Communications, 2018, 39 (4) :123-130.) 
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" LIU C, YUEN J, TORRALBA A.Sift Flow:Dense Corresponden-ce across Scenes and Its Applications // HASSNER T, LIU C, eds.Dense Image Correspondences for Computer Vision.Berlin, Germany:Springer, 2011:15-49." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sift Flow:Dense Corresponden-ce across Scenes and Its Applications">
                                        <b>[6]</b>
                                         LIU C, YUEN J, TORRALBA A.Sift Flow:Dense Corresponden-ce across Scenes and Its Applications // HASSNER T, LIU C, eds.Dense Image Correspondences for Computer Vision.Berlin, Germany:Springer, 2011:15-49.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" FARABET C, COUPRIE C, NAJMAN L, &lt;i&gt;et al&lt;/i&gt;.Learning Hierarchical Features for Scene Labeling.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (8) :1915-1929." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning hierarchical features for scene labeling">
                                        <b>[7]</b>
                                         FARABET C, COUPRIE C, NAJMAN L, &lt;i&gt;et al&lt;/i&gt;.Learning Hierarchical Features for Scene Labeling.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (8) :1915-1929.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" LONG J, SHELHAMER E, DARRELL T.Fully Convolutional Networks for Semantic Segmentation[C/OL].[2018-08-15].https://arxiv.org/pdf/1411.4038.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully Convolutional Networks for Semantic Segmentation[C/OL]">
                                        <b>[8]</b>
                                         LONG J, SHELHAMER E, DARRELL T.Fully Convolutional Networks for Semantic Segmentation[C/OL].[2018-08-15].https://arxiv.org/pdf/1411.4038.pdf.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet Classification with Deep Convolutional Neural Networks.Communications of the ACM, 2017, 60 (6) :84-90." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMD71033EF8EE149F894A1F173A0F662B0&amp;v=MTQyMjVyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3TDI1eGFBPU5pZklZOGUvSDlIUHJQb3piSjU2RFhnd3VSNGE3azU4UG43bHIyTTFEN1NTUjhpZkNPTnZGU2lXVw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet Classification with Deep Convolutional Neural Networks.Communications of the ACM, 2017, 60 (6) :84-90.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" SIMONYAN K, ZISSERMAN A.Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL].[2018-08-15].https://arxiv.org/pdf/1409.1556.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL]">
                                        <b>[10]</b>
                                         SIMONYAN K, ZISSERMAN A.Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL].[2018-08-15].https://arxiv.org/pdf/1409.1556.pdf.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" SZEGEDY C, LIU W, JIA Y Q, &lt;i&gt;et al&lt;/i&gt;.Going Deeper with Convolutions // Proc of the 28th IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[11]</b>
                                         SZEGEDY C, LIU W, JIA Y Q, &lt;i&gt;et al&lt;/i&gt;.Going Deeper with Convolutions // Proc of the 28th IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:1-9.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" HE K M, ZHANG X Y, REN S Q, &lt;i&gt;et al&lt;/i&gt;.Deep Residual Learning for Image Recognition // Proc of the 28th IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[12]</b>
                                         HE K M, ZHANG X Y, REN S Q, &lt;i&gt;et al&lt;/i&gt;.Deep Residual Learning for Image Recognition // Proc of the 28th IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:770-778.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" BADRINARAYANAN V, HANDA A, CIPOLLA R.SegNet:A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling.IEEE Transaction on Pattern Analysis and Machine Intelligence, 2015, 39 (12) :2481-2495." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SegNet:A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling">
                                        <b>[13]</b>
                                         BADRINARAYANAN V, HANDA A, CIPOLLA R.SegNet:A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling.IEEE Transaction on Pattern Analysis and Machine Intelligence, 2015, 39 (12) :2481-2495.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" RONNEBERGER O, FISCHER P, BROX T.U-Net:Convolutional Networks for Biomedical Image Segmentation // Proc of the International Conference on Medical Image Computing and Computer-Assisted Intervention.Berlin, Germany:Springer, 2015:234-241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">
                                        <b>[14]</b>
                                         RONNEBERGER O, FISCHER P, BROX T.U-Net:Convolutional Networks for Biomedical Image Segmentation // Proc of the International Conference on Medical Image Computing and Computer-Assisted Intervention.Berlin, Germany:Springer, 2015:234-241.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" BANSAL A, CHEN X L, RUSSELL B, &lt;i&gt;et al&lt;/i&gt;.PixelNet:Representation of the Pixels, by the Pixels, and for the Pixels[C/OL].[2018-08-15].https://arxiv.org/pdf/1702.06506.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PixelNet:Representation of the Pixels,by the Pixels,and for the Pixels[C/OL]">
                                        <b>[15]</b>
                                         BANSAL A, CHEN X L, RUSSELL B, &lt;i&gt;et al&lt;/i&gt;.PixelNet:Representation of the Pixels, by the Pixels, and for the Pixels[C/OL].[2018-08-15].https://arxiv.org/pdf/1702.06506.pdf.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" CHEN L C, PAPANDREOU G, KOKKINOS I, &lt;i&gt;et al&lt;/i&gt;.Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs[C/OL].[2018-08-15].https://arxiv.org/pdf/1412.7062.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs[C/OL]">
                                        <b>[16]</b>
                                         CHEN L C, PAPANDREOU G, KOKKINOS I, &lt;i&gt;et al&lt;/i&gt;.Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs[C/OL].[2018-08-15].https://arxiv.org/pdf/1412.7062.pdf.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" CHEN L C, PAPANDREOU G, KOKKINOS I, &lt;i&gt;et al&lt;/i&gt;.DeepLab:Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (4) :834-848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Lab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs">
                                        <b>[17]</b>
                                         CHEN L C, PAPANDREOU G, KOKKINOS I, &lt;i&gt;et al&lt;/i&gt;.DeepLab:Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (4) :834-848.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" ZHAO H S, SHI J P, QI X J, &lt;i&gt;et al&lt;/i&gt;.Pyramid Scene Parsing Network // Proc of the IEEE Conference on Computer Vision and Pa-ttern Recognition.Washington, USA:IEEE, 2017:6230-6239." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pyramid scene parsing network">
                                        <b>[18]</b>
                                         ZHAO H S, SHI J P, QI X J, &lt;i&gt;et al&lt;/i&gt;.Pyramid Scene Parsing Network // Proc of the IEEE Conference on Computer Vision and Pa-ttern Recognition.Washington, USA:IEEE, 2017:6230-6239.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" YU F, KOLTUN V.Multi-scale Context Aggregation by Dilated Convolutions[C/OL].[2018-08-15].https://arxiv.org/pdf/1511.07122.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-scale context aggregation by dilated convolutions">
                                        <b>[19]</b>
                                         YU F, KOLTUN V.Multi-scale Context Aggregation by Dilated Convolutions[C/OL].[2018-08-15].https://arxiv.org/pdf/1511.07122.pdf.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" YU F, KOLTUN V, FUNKHOUSER T.Dilated Residual Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:636-644." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dilated residual networks">
                                        <b>[20]</b>
                                         YU F, KOLTUN V, FUNKHOUSER T.Dilated Residual Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:636-644.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" CHEN L C, PAPANDREOU G, SCHROFF F, &lt;i&gt;et al&lt;/i&gt;.Rethinking Atrous Convolution for Semantic Image Segmentation[C/OL].[2018-08-15].https://arxiv.org/pdf/1706.05587.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rethinking Atrous Convolution for Semantic Image Segmentation[C/OL]">
                                        <b>[21]</b>
                                         CHEN L C, PAPANDREOU G, SCHROFF F, &lt;i&gt;et al&lt;/i&gt;.Rethinking Atrous Convolution for Semantic Image Segmentation[C/OL].[2018-08-15].https://arxiv.org/pdf/1706.05587.pdf.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" XIE C W, ZHOU H Y, WU J X.Vortex Pooling:Improving Con-text Representation in Semantic Segmentation[C/OL].[2018-08-15].https://arxiv.org/pdf/1804.06242v2.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vortex Pooling:Improving Con-text Representation in Semantic Segmentation[C/OL]">
                                        <b>[22]</b>
                                         XIE C W, ZHOU H Y, WU J X.Vortex Pooling:Improving Con-text Representation in Semantic Segmentation[C/OL].[2018-08-15].https://arxiv.org/pdf/1804.06242v2.pdf.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" HUANG G, LIU Z, VAN DER MAATEN L, &lt;i&gt;et al&lt;/i&gt;.Densely Connected Convolutional Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:2261-2269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Densely connected convolutional networks">
                                        <b>[23]</b>
                                         HUANG G, LIU Z, VAN DER MAATEN L, &lt;i&gt;et al&lt;/i&gt;.Densely Connected Convolutional Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:2261-2269.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" >
                                        <b>[24]</b>
                                     DENG J, DONG W, SOCHER R, &lt;i&gt;et al&lt;/i&gt;.ImageNet:A Large-Scale Hierarchical Image Database // Proc of the 22th IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2009:248-255.</a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" BENENSON R, FRANKE U, ROTH S, &lt;i&gt;et al&lt;/i&gt;.The Cityscapes Da-taset for Semantic Urban Scene Understanding // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:3213-3223." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;The Cityscapes Dataset for Semantic Urban Scene Understanding,&amp;quot;">
                                        <b>[25]</b>
                                         BENENSON R, FRANKE U, ROTH S, &lt;i&gt;et al&lt;/i&gt;.The Cityscapes Da-taset for Semantic Urban Scene Understanding // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:3213-3223.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(05),472-480 DOI:10.16451/j.cnki.issn1003-6059.201905010            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于DenseNet的复杂交通场景语义分割方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%92%8B%E6%96%8C&amp;code=07418215&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蒋斌</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B6%82%E6%96%87%E8%BD%A9&amp;code=41999553&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">涂文轩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E8%B6%85&amp;code=07427897&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨超</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E8%99%B9%E9%9B%A8&amp;code=41999554&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘虹雨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E5%AD%90%E9%BE%99&amp;code=27121019&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵子龙</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B9%96%E5%8D%97%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0060047&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">湖南大学信息科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对交通场景语义分割方法存在参数量较大、计算效率较低、精度不足等问题, 文中提出基于全卷积化DenseNet的多尺度端到端语义分割模型.首先, 构建一种含混合空洞卷积的密集连接模块, 同时沿通道维度级联各模块, 用于提取图像特征.然后, 采集多尺度视觉信息并以此作为监督信号回传至原通道中.最后, 通过双线性插值法获得预测输出.在CityScapes数据集上的测试实验表明, 文中方法对复杂交通场景的解析能力较强, 预测精度和分割效率较高.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%A4%E9%80%9A%E5%9C%BA%E6%99%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">交通场景;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像语义分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空洞卷积;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度特征融合;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *蒋斌 (通讯作者) , 博士, 副教授, 主要研究方向为大数据技术、计算机视觉、机器学习.E-mail:jiangbin@hnu.edu.cn.;
                                </span>
                                <span>
                                    涂文轩, 硕士研究生, 主要研究方向为计算机视觉、机器学习.E-mail:twx@hnu.edu.cn.;
                                </span>
                                <span>
                                    杨超, 博士, 副教授, 主要研究方向为大数据技术、社会网络计算、智能信息处理.E-mail:yangchaoedu@hnu.edu.cn.;
                                </span>
                                <span>
                                    刘虹雨, 硕士研究生, 主要研究方向为计算机视觉、机器学习.E-mail:www884886@126.com.;
                                </span>
                                <span>
                                    赵子龙, 硕士研究生, 主要研究方向为计算机视觉、机器学习.E-mail:zerahhah@gmail.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金青年科学基金项目 (No.61702176);</span>
                                <span>湖南省自然科学基金项目 (No.2017JJ3038) 资助;</span>
                    </p>
            </div>
                    <h1><b>Semantic Segmentation Method for Complex Traffic Scene Based on DenseNet</b></h1>
                    <h2>
                    <span>JIANG Bin</span>
                    <span>TU Wenxuan</span>
                    <span>YANG Chao</span>
                    <span>LIU Hongyu</span>
                    <span>ZHAO Zilong</span>
            </h2>
                    <h2>
                    <span>College of Computer Science and Electronic Engineering, Hunan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>An end-to-end multi-scale semantic segmentation model based on fully convolutional DenseNet is proposed, aiming at the problems of traditional semantic segmentation methods for street scene, such as the large number of parameters and low computational efficiency and precision. Firstly, convolution layers embedded with hybrid dilation convolution are stacked to establish a dense module, and then the modules are cascaded along channel dimension to extract features. Next, multi-scale visual information regarded as supervised signals are transferred back to original channels. Finally, the prediction results are obtained by bilinear interpolation method. Experimental results on Cityscapes dataset demonstrate that the proposed method achieves an efficient segmentation and performs a better accuracy for street scene parsing.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Traffic%20Scene&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Traffic Scene;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Image%20Semantic%20Segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Image Semantic Segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Dilated%20Convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Dilated Convolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multi-scale%20Feature%20Fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multi-scale Feature Fusion;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    JIANG Bin ( Corresponding author) , Ph. D. , associate professor. His research interests include big data technology, computer vision and machine learning.;
                                </span>
                                <span>
                                    TU Wenxuan, master student. His research interests include computer vision and machine learning.;
                                </span>
                                <span>
                                    YANG Chao, Ph. D. , associate professor. Her research interests include big data technology, social network computing and intelligent information processing.;
                                </span>
                                <span>
                                    LIU Hongyu, master student. His research interests include computer vision and machine learning.;
                                </span>
                                <span>
                                    ZHAO Zilong, master student. His research interests include computer vision and machine learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-15</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by Young Scientists Fund of National Natural Science Foundation of China (No.61702176);</span>
                                <span>Natural Science Foundation of Hunan Province (No.2017JJ3038);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="64">随着计算机视觉技术的发展, 计算机可以像人类一样对室内外场景实现更清晰和完整的建模与认知.图像分割技术作为视觉理解的基石性研究, 在实际生活中应用广泛<citation id="163" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>.在无人驾驶领域, 通过将整个场景以像素精度进行语义层面的划分, 从像素折算到物理距离, 计算机可实现对场景的完整建模<citation id="164" type="reference"><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="65">在基于卷积神经网络 (Convolutional Neural Network, CNN) 的分割方法中, Liu等<citation id="165" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、Farabet等<citation id="166" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>将像素周围的一个图像块作为CNN的输入, 用于训练和预测, 然而像素块大小限制感知区域的范围, 导致模型性能受到限制.全卷积网络 (Fully Convolutional Networks, FCN) 改进上述问题, 将图像分割由一个孤立的领域转向为从粗略到精细的自然推理过程<citation id="167" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>.当前具有代表性的深度神经网络包括AlexNet<citation id="168" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、VGGNet<citation id="169" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、GoogLeNet<citation id="170" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和残差网络 (Residual Networks, ResNet) <citation id="171" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等, 这些图像分类模型展现出高效的特征表达能力, 将传统的图像分割任务引申为图像语义分割任务.</p>
                </div>
                <div class="p1">
                    <p id="66">在国内外面向图像语义分割的研究工作中, Long等<citation id="172" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出基于全卷积网络的语义分割方法, 将VGG网络的全连接层改为卷积层, 利用转置卷积进行上采样以实现像素级别端到端的训练.SegNet<citation id="173" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>为一种基于编码-解码的对称结构, FCN仅复制编码器特征, SegNet将最大池化指数转移至解码器中, 改善分割效率.同样, U-net通过跳跃连接融合解码信息与对应通道的编码信息, 恢复特征图至原分辨率大小<citation id="174" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>.由于编码-解码结构未充分考虑像素与像素之间的关联度, 因此缺乏语义的空间一致性.</p>
                </div>
                <div class="p1">
                    <p id="67">为了有效处理场景中各目标之间的关系并提升模型对上下文的推理能力, 学者们研究与探讨聚合多尺度视觉信息.在自然场景解析的任务中, Bansal等<citation id="175" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>探索图像的低层次、中层次及高层次语义信息, 从多维特征图上挖掘不同的内容, 提出基于VGG16的PixelNet网络.针对更复杂的交通环境, Chen等相继提出DeepLab v1<citation id="176" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>和DeepLab v2<citation id="177" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 在空间维度上实现金字塔型的空洞池化 (Atrous Spatial Pyramid Pooling, ASPP) 操作, 以全连接条件随机场 (Condition Random Field, CRF) 的后处理方式实现像素之间的结构化预测.为了引入足够的上下文信息及不同感受野下的全局信息, 金字塔场景解析网络 (Pyramid Scene Parsing Network, PSP-Net) <citation id="178" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>通过金字塔池化模块 (Pyramid Pooling Module, PPM) 聚合不同区域的特征, 并引入辅助损失函数优化模型的性能.</p>
                </div>
                <div class="p1">
                    <p id="68">有关研究表明, 深度神经网络中的池化层扩大感受野, 有助于实现目标分类.但在下采样过程中, 频繁的卷积和池化操作不利于像素的精准定位<citation id="179" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>.针对上述问题, Yu等<citation id="180" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>向卷积层引入一个称为“扩张率 (Dilation Rate, DR) ”的参数, 定义卷积核处理数据时各权值的间距.在相同的权值计算量下, 空洞卷积提供更大的感受野.</p>
                </div>
                <div class="p1">
                    <p id="69">空洞残差网络 (Dilated Residual Networks, D-RN) <citation id="181" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>采用空洞卷积替换下采样中的池化层, 在放大原网络感受野的同时减少图像空间分辨率的损失.为了进一步提升语义分割的精度, Chen等<citation id="182" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>改进ASPP模块, 提出基于多尺度空洞卷积的级联或并行架构.受空洞卷积的启发, Xie等<citation id="183" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>有效利用上下文信息构建多孔空间金字塔, 充分利用先验知识理解不同的场景.</p>
                </div>
                <div class="p1">
                    <p id="70">上述工作都为后续图像语义分割技术的研究提供借鉴意义, 但仍有许多问题亟待解决.当前大多数主流的深度学习语义分割框架都遵循VGG网络或恒等映射 (Identity Mapping, IM) 的方式以构建编码器.相比前者, 密集连接卷积网络 (Densely Connect-ed Convolutional Networks, DenseNet) <citation id="184" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>在一定程度上规避信息冗余、计算低效等问题.本文充分考虑模型效率在实际应用中的强烈需求, 结合多种扩张率的空洞卷积与密集连接模块 (Dense Block, DB) , 构建全卷积化DenseNet121网络.相比VGG、ResNet基准网络, DenseNet121增强编码器的表征能力, 降低语义分割模型的参数量与存储空间.因此本文进而提出面向复杂交通场景端到端的高效语义分割方法, 探究不同尺度空间下的池化层对分割精度和模型存储的影响.在不损失精度的前提下, 增强类内的语义一致性, 改善模型的综合性能.在CityScapes数据集上的实验表明, 相比当前主流的语义分割方法, 本文方法对语义分割精度、模型存储开销和参数计算效率等均有明显改善.</p>
                </div>
                <h3 id="71" name="71" class="anchor-tag">1 面向交通场景的语义分割模型</h3>
                <div class="p1">
                    <p id="72">本文提出的语义分割模型如图1所示.</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905012_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于DenseNet的多尺度语义分割模型" src="Detail/GetImg?filename=images/MSSB201905012_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基于DenseNet的多尺度语义分割模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905012_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Multi-scale semantic segmentation model based on DenseNet</p>

                </div>
                <div class="p1">
                    <p id="75">首先分析密集连接网络与空洞卷积的优势, 介绍嵌入混合空洞卷积的密集连接模块.然后, 阐述过渡层的作用并改进, 同时级联初始化层、密集连接模块与过渡层, 提出使用全卷积化DenseNet121作为特征提取器.最后, 探讨FCN的不足, 通过构建多尺度特征融合模块聚合上下文信息, 改善模型的分割性能.</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76"><b>1.1 密集连接模块与空洞卷积</b></h4>
                <div class="p1">
                    <p id="77">在下采样过程中, 池化层造成特征图丢失许多细节信息, 不利于预测不同类别像素的定位.然而, 去除池化结构却降低上层卷积核的感受野.为了平衡两者之间的关系, 受已有工作的启发<citation id="185" type="reference"><link href="39" rel="bibliography" /><link href="41" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>, 本文在图像编码过程中借鉴空洞卷积的思路, 设计嵌入混合空洞卷积的密集连接模块.</p>
                </div>
                <div class="p1">
                    <p id="78">在深度卷积神经网络中, 如果有<i>L</i>层网络, 会存在<i>L</i>个连接:<i>x</i><sub><i>l</i></sub>=<i>H</i><sub><i>l</i></sub> (<i>x</i><sub><i>l</i>-1</sub>) .<i>x</i><sub><i>l</i></sub>通过应用非线性变换<i>H</i><sub><i>l</i></sub>计算对上一层<i>x</i>的输出.<i>H</i><sub><i>l</i></sub>通常定义为一个带有线性整流函数 (Rectified Linear Unit, ReLU) 的卷积操作.</p>
                </div>
                <div class="p1">
                    <p id="79">在恒等映射的思想上进一步延伸, Huang等<citation id="186" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>提出密集连接网络DenseNet<citation id="187" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>.对于网络中的每层, 在它之前所有层生成的特征图都会作为该层的输入, 特征图输出以前馈方式向后进行传导, 整个网络共存在<i>L</i> (<i>L</i>+1) /2个连接.第<i>L</i>层的输出定义为</p>
                </div>
                <div class="p1">
                    <p id="80"><i>x</i><sub><i>l</i></sub>=<i>H</i><sub><i>l</i></sub> ([<i>x</i><sub><i>l</i>-1</sub>, <i>x</i><sub><i>l</i>-2</sub>, <i>x</i><sub><i>l</i>-3</sub>, …, <i>x</i><sub>0</sub>]) , </p>
                </div>
                <div class="p1">
                    <p id="81">其中, […]表示连接操作, <i>H</i><sub><i>l</i></sub>表示由正则化层 (Batch Normalization, BN) 、ReLU层、卷积层和随机失活 (Dropout) 层组成的模块, 这种沿通道维度串联的方式增强特征在网络中的传输.每层输出<i>k</i>个特征映射图呈线性增长, 定义<i>k</i>为增长率, 用于控制网络的宽度, 一般设置为较小的值 (如<i>k</i>=12) .如图2所示, 从输入图像或上一层的输出开始, <i>x</i><sub>0</sub>为输入图像, 第一层创建<i>k</i>个特征映射图, 并与输入<i>x</i><sub>0</sub>相连接.第二层再创建<i>k</i>个特征映射图, 在通道维度上, 融合新的特征映射图与上一层输出的特征映射图.以此类推, Dense模块的最终输出是前4层所有4 × <i>k</i>个特征映射图.</p>
                </div>
                <div class="p1">
                    <p id="82">为了维持卷积层对多类别目标的感受野, 文献<citation id="188" type="reference">[<a class="sup">18</a>]</citation>和文献<citation id="189" type="reference">[<a class="sup">21</a>]</citation>在编码器中应用不同的空洞卷积策略.然而, 相同或等比例的扩张率会丢失大量信息, 无法减小“网格”效应.若扩张率过大, 采集的信息关联度较低, 破坏类内的语义一致性<citation id="190" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>.基于此种情况, 在最后2个Dense模块上依次交替嵌入扩张率为1、2、3、5的空洞卷积替换常规卷积层.该模块不仅有助于提高计算效率, 促进信息在网络上的流动, 而且在保持感受野大小不变的情况下减少冗余的存储资源, 有助于编码器的构建.</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905012_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 带有空洞卷积的Dense模块" src="Detail/GetImg?filename=images/MSSB201905012_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 带有空洞卷积的Dense模块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905012_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Dense module with dilated convolution</p>

                </div>
                <h4 class="anchor-tag" id="84" name="84"><b>1.2 全卷积化</b><b>DenseNet</b><b>121网络</b></h4>
                <div class="p1">
                    <p id="85">FCN与深度卷积神经网络的区别在于把CNN最后的全连接层转换成卷积层, 解决像素级别的图像分割问题.然而, 大多数语义分割方法的基础网络由VGG、ResNet等改进而来, 冗余的参数消耗大量的计算资源, 不利于模型的推理与训练.</p>
                </div>
                <div class="p1">
                    <p id="86">本文延续FCN的建模思路, 加载在ImageNet数据集<citation id="191" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>上预训练好的DenseNet121网络.该网络由初始化层、密集连接层、过渡层 (Transition Layer, TL) (如图3) 与全连接层构成.</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905012_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 过渡层" src="Detail/GetImg?filename=images/MSSB201905012_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 过渡层  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905012_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Transition layer</p>

                </div>
                <div class="p1">
                    <p id="88">首先, 移除DenseNet121最后的全连接层, 全卷积化剩余的网络结构.然后, 原网络中过渡层由正则化层、ReLU层、1×1卷积层和1×1均值池化层组成, 用于有效降低每个Dense模块的输出通道数量.为了减少空间信息的丢失并增强模型的抗过拟合能力, 本文对原DenseNet网络中的过渡层进行优化, 在卷积层后添加Dropout层并去除阶段2、阶段3和阶段4的池化结构, 如图3所示.最后, 级联初始化层、含有空洞卷积的密集连接模块及优化后的过渡层, 作为初始编码器.</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89"><b>1.3 多尺度特征融合模块</b></h4>
                <div class="p1">
                    <p id="90">相比VGG、ResNet, 本文方法的编码器重复利用已有特征并持续探索新的特征, 提高参数的复用效率.原始图像经过编码器获得原输入1/8大小 (Output Stride, OS) 的特征图.在下采样过程中, 随着网络层数的加深, 深层特征图的抽象能力加强, 能更好地定位全局目标.浅层特征图包含更多的空间信息, 更擅长细节描述.然而在FCN中, 基于跳跃结构 (Skip Architecture, SA) 的上采样方式未充分考虑像素之间的关系, 对图像的细节信息不敏感, 导致预测结果较模糊和平滑.</p>
                </div>
                <div class="p1">
                    <p id="91">为了克服FCN的局限性, 提升各特征图对像素的判别能力.如图4所示, 本文采用4种不同尺度的全局均值池化层采集图像信息, 池化尺度分别为1×1, 2×2, 3×3, 6×6, 从不同维度依次覆盖图像的全局特征、局部特征和边缘特征.若该模块共有<i>C</i>个池化尺度, 在每个尺度后使用1×1的卷积层依次将原通道数降为1/<i>C</i>, 通过双线性插值层获得<i>C</i>个多尺度特征映射图.然后, 将融合后的多尺度特征作为监督信息回传到各通道中, 与原特征图上的每个像素对应相乘, 自适应地学习各特征图之间的相互关系, 增强有用信息, 抑制无效信息, 提高模型对像素的判别能力.最后, 在通道维度上结合多尺度特征融合信息与加权后的原特征图信息, 经过第2个双线性插值层获得最终预测输出.</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905012_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 多尺度特征融合模块" src="Detail/GetImg?filename=images/MSSB201905012_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 多尺度特征融合模块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905012_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Mutil-scale feature fusion module</p>

                </div>
                <h3 id="93" name="93" class="anchor-tag">2  实验及结果分析</h3>
                <div class="p1">
                    <p id="94">为了评估本文方法的性能, 使用具有代表性的交通场景CityScapes数据集<citation id="192" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>.该数据集包含带有精确语义标注的5 000幅道路交通图像, 训练集2 975幅, 验证集500幅, 测试集1 525幅.每幅图像原始大小为1 024×2 048, 包括含背景类在内的天空、道路、火车、汽车、行人、树木、路灯等20个类别.测试集的标签未公开.本文进行多组对比实验, 仅使用验证集评估各个模型.</p>
                </div>
                <div class="p1">
                    <p id="95">本文实验使用深度学习框架Pytorch3.0, 网络的训练、验证与测试均在Pytorch环境下完成.编程语言为python3.5, 基于Window10操作系统.实验的硬件环境为Intel (R) Core (TM) i7-8700k处理器, 主频为3.70 GHz, 内存为32 GB, NVIDIA 1080TI 显卡.该配置是目前深度学习计算的主流配置.</p>
                </div>
                <div class="p1">
                    <p id="96">数据预处理及模型训练细节如下.图像输入大小统一裁剪为512×1 024, 通过随机翻转、角度在-10°到10°之间随机旋转、随机高斯滤波对数据进行增强处理.初始学习率设置为0.001, 动量设置为0.9, 权重衰减参数设置为0.000 5, Dropout统一设置为0.5.使用随机梯度下降法 (Stochastic Gradient Descent, SGD) 优化损失函数.在训练中, 每次输入模型的批量大小 (Batch Size, BS) 设置为2, 执行100个Epoch, 每30个Epoch将学习率按0.5倍缩减, 共迭代150 000次.随着迭代次数的增加, 若在验证集上误差呈现上升趋势, 终止训练 (Early Stopping, ES) 并将权重作为模型的最终参数.</p>
                </div>
                <div class="p1">
                    <p id="97">本文使用的数据集共有20个类别 (从<i>L</i><sub>0</sub>到<i>L</i><sub>19</sub>, 其中包含一个背景类) .使用<i>p</i><sub><i>ij</i></sub>表示本属于类<i>i</i>但被预测为类<i>j</i>的像素数量, 即<i>p</i><sub><i>ii</i></sub>表示真正的数量, 而<i>p</i><sub><i>ij</i></sub>表示假正, <i>p</i><sub><i>ji</i></sub>表示假负.衡量语义分割模型性能的常用评价指标有如下4种:像素精度 (Pixel Accuracy, PA) 、均像素精度 (Mean PA, MPA) 、均交并比 (MeanIntersection over Union, MIoU) 和频权交并比 (Fre-quency Weighted Intersection over Union, FWIoU) .具体公式如下.</p>
                </div>
                <div class="p1">
                    <p id="100">1) 像素精度 (PA) :</p>
                </div>
                <div class="p1">
                    <p id="101"><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mi>A</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="103">2) 均像素精度 (MPA) :</p>
                </div>
                <div class="p1">
                    <p id="104"><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>Ρ</mi><mi>A</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mrow><mo>) </mo></mrow></mrow></mstyle></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="106">3) 均交并比 (MIoU) :</p>
                </div>
                <div class="p1">
                    <p id="107"><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>Ι</mi><mi>o</mi><mi>U</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mrow><mfrac><mrow><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo>-</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow></mfrac></mrow></mstyle></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="109">4) 频权交并比 (FWIoU) :</p>
                </div>
                <div class="p1">
                    <p id="110"><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mi>Μ</mi><mi>Ι</mi><mi>o</mi><mi>U</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo>-</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow></mfrac></mrow><mo>) </mo></mrow></mrow></mstyle></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="112">通过对比实验验证本文方法的有效性.实验共分为四组:第一组实验从收敛速度与分割精度两方面展现预训练机制对本文方法的影响;第二组实验验证全卷积化DenseNet121网络的有效性;第三组实验对比分析多尺度融合模块中不同池化尺度及数量在分割精度与存储空间上的表现;第四组实验从模型的参数量、存储空间及平均推理时间等多方面将本文方法与全卷积网络 (Fully Convolutional Net-work, FCN) 、SegNet进行对比分析.由于MIoU和MPA直观简洁、代表性较强而成为最常用的度量指标, 每组实验均采用MIoU与MPA作为对模型性能的评判标准.</p>
                </div>
                <div class="p1">
                    <p id="113">表1和图5分别给出预训练与非预训练模型的分割精度和收敛情况.由表1、图5可知, 预训练及非预训练模型在相同实验条件下迭代100个Epoch.相比重新训练整个网络, 本文方法在预训练模型上的收敛速度更快, 在训练集上显著提高MIoU与MPA.因此, 为了减少大量的训练时间, 提高算法的运行效率, 以下实验均基于预训练模型进行对比分析.</p>
                </div>
                <div class="area_img" id="114">
                    <p class="img_tit"><b>表1 2种模型的分割精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Segmentation precision of 2 models</p>
                    <p class="img_note"></p>
                    <table id="114" border="1"><tr><td><br />模型</td><td>MIoU</td><td>MPA</td></tr><tr><td><br />未预训练模型</td><td>55.17</td><td>67.11</td></tr><tr><td><br />预训练模型</td><td><b>70.41</b></td><td><b>81.17</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905012_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 2种模型的收敛情况" src="Detail/GetImg?filename=images/MSSB201905012_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 2种模型的收敛情况  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905012_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Convergence of 2 models</p>

                </div>
                <div class="p1">
                    <p id="116">表2在FCN的基础上横向对比3种全卷积化编码器的性能.相比前两种方法, 得益于基础网络DenseNet的优势, FCN-Dense121在保证精度的前提下, 大幅减少分割模型的参数量和存储空间.</p>
                </div>
                <div class="area_img" id="117">
                    <p class="img_tit"><b>表2 3种全卷积化编码器对分割模型的影响</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Influence of 3 fully convolutional encoders on segmentation model</p>
                    <p class="img_note"></p>
                    <table id="117" border="1"><tr><td><br />模型</td><td>参数量<br /> (×10<sup>7</sup>) </td><td>存储空间<br />/MB</td><td>MPA</td><td>MIoU</td></tr><tr><td><br />FCN-VGG16</td><td>13.43</td><td>524.9</td><td>68.71</td><td>57.47</td></tr><tr><td><br />FCN-Res101</td><td>4.27</td><td>167.3</td><td>75.18</td><td>62.43</td></tr><tr><td><br />FCN-Dense121</td><td><b>0.71</b></td><td><b>28.2</b></td><td>74.23</td><td>62.09</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="118">为了进一步提升分割精度, 本文探讨融合多尺度特征对模型综合性能的影响.图6和表3分别给出4种池化尺度在验证集上的可视化结果与预测精度.图6 (a) 为原始图像, (b) 为原始图像对应的真实标签并作为对比基准评定各方法的效果. (d) 为池化尺度 (2×2) 的预测结果, 相比池化尺度 (1×1) , 整体的语义分割精度略有提升, 但仍然存在相对较大的噪声输出. (e) 为池化尺度 (3×3) 的预测结果, 相比 (c) 、 (d) , 预测结果中的噪声信息相对较小, 较好改善类间的区分度与分割边界, 精度分别提升约2.90%、1.95%, 但是仍丢失许多细节信息, 对小物体的分割能力偏弱.在适当增加参数量与存储量的条件下, 池化尺度 (6×6) 综合考虑更细微的局部特征. (f) 更接近 (b) 中的真实标签, 适当增强对易混淆物体的识别能力与分割边界的定位能力, 在验证集上的预测分割精度达到69.22%.原因在于不同尺度空间存在相同的关键点, 通过融合不同感受野的特征通道, 在各尺度下对客观物体具有更丰富的描述, 在一定程度上表现出更好的物体识别能力.</p>
                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905012_13900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 4种池化尺度下的预测结果可视化" src="Detail/GetImg?filename=images/MSSB201905012_13900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 4种池化尺度下的预测结果可视化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905012_13900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Visualization of prediction results at 4 pooling scales</p>

                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905012_13901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 4种池化尺度下的预测结果可视化" src="Detail/GetImg?filename=images/MSSB201905012_13901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 4种池化尺度下的预测结果可视化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905012_13901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Visualization of prediction results at 4 pooling scales</p>

                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905012_13902.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 4种池化尺度下的预测结果可视化" src="Detail/GetImg?filename=images/MSSB201905012_13902.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 4种池化尺度下的预测结果可视化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905012_13902.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Visualization of prediction results at 4 pooling scales</p>

                </div>
                <div class="area_img" id="142">
                    <p class="img_tit"><b>表3 5种方法在19类物体上的分割精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"> Table 3 Segmentation accuracy of 5 methods on 19 objects</p>
                    <p class="img_note"></p>
                    <table id="142" border="1"><tr><td rowspan="2"><br />物体</td><td colspan="4"><br />池化尺度</td><td rowspan="2">本文方法</td></tr><tr><td><br />1×1</td><td>2×2</td><td>3×3</td><td>6×6</td></tr><tr><td><br />马路</td><td>93.50</td><td>94.05</td><td>93.90</td><td>95.04</td><td><b>97.67</b></td></tr><tr><td>人行道</td><td>67.47</td><td>69.91</td><td>69.68</td><td>74.17</td><td><b>81.53</b></td></tr><tr><td><br />建筑</td><td>88.59</td><td>87.92</td><td>88.24</td><td>89.18</td><td><b>89.97</b></td></tr><tr><td><br />墙</td><td>43.65</td><td>38.32</td><td><b>54.93</b></td><td>49.59</td><td>43.19</td></tr><tr><td><br />围栏</td><td>42.50</td><td>43.02</td><td>43.57</td><td>50.98</td><td><b>51.46</b></td></tr><tr><td><br />杆子</td><td>40.80</td><td>41.55</td><td>41.09</td><td>47.43</td><td><b>49.53</b></td></tr><tr><td><br />信号灯</td><td>50.20</td><td>53.28</td><td>51.85</td><td><b>59.55</b></td><td>58.40</td></tr><tr><td><br />交通标识</td><td>64.92</td><td>60.60</td><td>61.81</td><td>67.66</td><td><b>68.01</b></td></tr><tr><td><br />植物</td><td>89.24</td><td>89.21</td><td>88.62</td><td>90.26</td><td><b>90.32</b></td></tr><tr><td><br />地面</td><td>49.88</td><td>51.26</td><td>50.81</td><td>56.25</td><td><b>59.78</b></td></tr><tr><td><br />天空</td><td>92.23</td><td>92.01</td><td>91.54</td><td><b>92.46</b></td><td>92.44</td></tr><tr><td><br />行人</td><td>70.11</td><td>68.26</td><td>67.75</td><td>72.11</td><td><b>73.46</b></td></tr><tr><td><br />骑手</td><td>49.84</td><td>43.59</td><td>46.61</td><td>48.65</td><td><b>53.26</b></td></tr><tr><td><br />汽车</td><td>91.25</td><td>91.44</td><td>91.07</td><td>91.69</td><td><b>92.12</b></td></tr><tr><td><br />卡车</td><td>52.03</td><td>64.05</td><td>66.24</td><td><b>68.20</b></td><td>66.00</td></tr><tr><td><br />巴士</td><td>67.46</td><td>73.75</td><td>72.79</td><td><b>79.16</b></td><td>76.65</td></tr><tr><td><br />火车</td><td>38.72</td><td>49.60</td><td>66.35</td><td>63.92</td><td><b>66.50</b></td></tr><tr><td><br />摩托车</td><td>50.14</td><td>48.17</td><td>51.58</td><td>51.04</td><td><b>58.00</b></td></tr><tr><td><br />自行车</td><td>64.07</td><td>65.80</td><td>64.27</td><td>67.76</td><td><b>69.55</b></td></tr><tr><td><br />MIoU</td><td>63.56</td><td>64.51</td><td>66.46</td><td>69.22</td><td><b>70.41</b></td></tr><tr><td><br />MPA</td><td>74.62</td><td>74.93</td><td>77.45</td><td>80.40</td><td><b>81.17</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="143">表<b>4</b>给出上述<b>4</b>种方法与本文方法的其它评价指标.Den<b>121</b>-SPP-<b>1</b>只关注全局信息, 参数计算量和模型占用存储最少.在CityScapes数据集中, 对围栏、灯杆与信号灯等细小物体的分割能力较弱, 引入的噪声信息较多.在此基础上, Den<b>121</b>-SPP-<b>2</b>、Den<b>121</b>-SPP-<b>3</b>、Den<b>121</b>-SPP-<b>4</b>从不同的视觉角度考虑更多的局部信息, 增强模型对类间的区分能力.随着池化规模的增加, 消耗的存储资源与计算资源也随之增多, 但整体的平均预测精度逐步上升.为了均衡两者之间的关系, 本文将多尺度融合特征作为监督信息加权到原特征图中, 为后者提供一个较强的一致性约束, 从而把高分辨率小尺度的精确性与低分辨率大尺度的细节性统一.实验表明, 本文方法在不增加额外计算及存储负担的前提下, 提升大部分物体的分割精度, MIoU值比方法<b>1</b>～方法<b>4</b>分别提高<b>6.85%、5.90%、3.95</b>%和<b>1.19</b>%, 如表<b>4</b>所示.</p>
                </div>
                <div class="area_img" id="144">
                    <p class="img_tit"><b>表4 4种方法的参数量及存储空间</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"> Table 4 Number of parameters and storage space of 4 methods</p>
                    <p class="img_note"></p>
                    <table id="144" border="1"><tr><td><br />方法</td><td>池化尺度</td><td>监督信息</td><td>参数量<br /> (×10<sup>7</sup>) </td><td>存储空间<br />/MB</td></tr><tr><td><br />Den121-SPP-1</td><td>1</td><td>×</td><td>1.02</td><td>40.1</td></tr><tr><td><br />Den121-SPP-2</td><td>1, 2</td><td>×</td><td>1.10</td><td>43.4</td></tr><tr><td><br />Den121-SPP-3</td><td>1, 2, 3</td><td>×</td><td>1.19</td><td>46.8</td></tr><tr><td><br />Den121-SPP-4</td><td>1, 2, 3, 6</td><td>×</td><td>1.27</td><td>50.1</td></tr><tr><td><br />本文方法</td><td>1, 2, 3, 6</td><td>√</td><td>1.27</td><td>50.1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="145">上述实验表明, 在相同训练条件下, 特征图在各尺度空间上的响应不同.单尺度表示无法满足模型对图像处理和解析的需求, 而多尺度表示类似于人类视觉系统的图像分析方法.通过融合低阶、中阶和高阶特征图, 加强特征之间的监督作用, 可减少不同区域之间的信息损失, 优化分割对象的类间区分性和类内一致性.</p>
                </div>
                <div class="p1">
                    <p id="146">将本文方法与主流的语义分割模型 (<i>FCN</i>-8<i>s</i>, <i>SegNet</i>) 进行纵向对比分析, 结果如表5和图7所示.由表5和图7可知, 综合考虑参数量、存储空间、平均推理时间、分割精度及收敛情况这5方面因素, 3种方法的综合性能从高到低依次为本文方法、<i>SegNet</i>、<i>FCN</i>-8<i>s</i>.<i>FCN</i>-8<i>s</i>的分割精度最低, 参数量及模型规模最大, 其中绝大部分参数来自全卷积化的<i>VGG</i>网络, 对单幅图像的平均推理时间最长.<i>SegNet</i>和<i>FCN</i>-8<i>s</i>最大的不同在于上采样过程, <i>SegNet</i>的<i>MIoU</i>、<i>MPA</i>值比<i>FCN</i>-8<i>s</i>分别提高约6.00%、6.60%.<i>SegNet</i>的参数量和存储空间仅为<i>FCN</i>-8<i>s</i>的1/6, 对预测像素的定位相对准确.在模型存储和计算效率上, <i>SegNet</i>比<i>FCN</i>-8<i>s</i>更高效.相比<i>FCN</i>-8<i>s</i>、<i>SegNet</i>, 本文方法的<i>MIoU</i>、<i>MPA</i>分别达到70.41%、81.17%, 在适当提升分割精度的同时, 模型的平均推理时间最快, 参数量与占用存储最少, 各项指标均优于其它方法.</p>
                </div>
                <div class="area_img" id="147">
                    <p class="img_tit"><b>表5 3种方法的综合性能</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Comprehensive performance of 3 methods</p>
                    <p class="img_note"></p>
                    <table id="147" border="1"><tr><td><br />指标</td><td>FCN-8s</td><td>SegNet</td><td>本文方法</td></tr><tr><td><br />参数量 (×10<sup>7</sup>) </td><td>13.43</td><td>2.84</td><td>1.27</td></tr><tr><td><br />存储空间/MB</td><td>524.9</td><td>111.2</td><td>50.1</td></tr><tr><td><br />平均推理时间/s</td><td>0.089</td><td>0.072</td><td>0.031</td></tr><tr><td><br />MIoU</td><td>57.47</td><td>63.47</td><td>70.41</td></tr><tr><td><br />MPA</td><td>68.71</td><td>75.31</td><td>81.17</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="148">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905012_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 3种模型的收敛情况" src="Detail/GetImg?filename=images/MSSB201905012_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 3种模型的收敛情况  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905012_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Convergence of 3 models</p>

                </div>
                <div class="p1">
                    <p id="149">图8分别给出FCN-8s、SegNet与本文方法的可视化结果. (b) 中预测结果最粗糙, 忽略灯杆、交通信号灯、骑手等大量小物体.因为FCN-8s中频繁的转置卷积层和跳跃连接不能恢复底层丰富的边缘特征, 直接经过反卷积操作无法保持定位信息.对于 (c) 、 (d) , FCN-8s网络仅复制编码器特征, 而SegNet网络存储特征映射最大值索引, 这在一定程度上保护原有结构并改善边界划分效果, 因此SegNet在像素定位方面相对稳定.图9为3种方法的预测精度.由图可知, SegNet在小物体方面与本文方法的分割精度相当, 但是对于易混淆物体, 无法正确区分汽车、卡车与火车.因为在恢复特征图的过程中, 由于输入上采样层的特征信息过于稀疏, 仅针对单个像素进行分类, 未充分考虑像素之间的关系, 对近似物体或多类别物体的识别不够敏感, 导致像素分类误差较大.相比FCN-8s、SegNet, 本文方法能较好改善上述问题.通过结合密集连接模块与多尺度信息, 充分考虑图像内各物体之间的联系, 适当扩大感受野并增强物体识别能力.无论是单类别物体、多类别物体还是近似物体, 本文方法均能保持相对稳定的预测结果.</p>
                </div>
                <div class="area_img" id="203">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905012_20300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 3种方法预测结果可视化" src="Detail/GetImg?filename=images/MSSB201905012_20300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 3种方法预测结果可视化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905012_20300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Visualization of prediction results of 3 methods</p>

                </div>
                <div class="area_img" id="158">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905012_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 3种方法在19类物体上的预测精度" src="Detail/GetImg?filename=images/MSSB201905012_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 3种方法在19类物体上的预测精度  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905012_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Prediction accuracy of 3 methods on 19 objects</p>

                </div>
                <div class="p1">
                    <p id="159">综上所述, 当前大多数语义分割方法在嵌入式系统中很难部署有限的硬件资源, 而减小模型规模可显著节省带宽, 降低存储开销.本文方法无需任何后处理操作, 在确保分割精度的前提下减少大量的冗余参数与存储空间, 提高深度语义分割网络的计算效率和推理速度.</p>
                </div>
                <h3 id="160" name="160" class="anchor-tag">3 结 束 语</h3>
                <div class="p1">
                    <p id="161">本文提出面向复杂交通场景的语义分割方法, 兼顾算法精度和模型效率两方面的要求.在FCN模型的基础上, 采用混合空洞卷积与密集连接模块提升编码器对图像的编码效率与表征能力, 并通过多尺度特征融合模块充分挖掘上下文信息以提升分割精度.在CityScapes数据集上的实验表明, 相比FC-N8s、SegNet, 本文方法的综合性能最优.</p>
                </div>
                <div class="p1">
                    <p id="162">今后将进一步研究如何提升语义分割模型的泛化能力与实时性, 包括对解码方式的改进与占用资源的调节, 在分割精度、计算复杂度、运行效率及泛化能力之间寻求一个平衡, 使其在智能交通领域具有更好的表现.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="194" type="formula" href="images/MSSB201905012_19400.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">蒋斌</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="196" type="formula" href="images/MSSB201905012_19600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">涂文轩</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="198" type="formula" href="images/MSSB201905012_19800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">杨超</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="200" type="formula" href="images/MSSB201905012_20000.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">刘虹雨</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="202" type="formula" href="images/MSSB201905012_20200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">赵子龙</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Review on Deep Learning Techniques Applied to Semantic Segmentation[C/OL]">

                                <b>[1]</b> GARCIA-GARCIA A, ORTS-ESCOLANO S, OPREA S, <i>et al</i>.A Review on Deep Learning Techniques Applied to Semantic Segmentation[C/OL].[2018-08-15].https://arxiv.org/pdf/1704.06857.pdf.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GJSX2017Z1005&amp;v=MDQyNTR6Z1ZickFJaWZZZHJHNEg5YW1ybzlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 张新明, 祝晓斌, 蔡强, 等.图像语义分割深度学习模型综述.高技术通讯, 2017, 27 (9) :808-815. (ZHANG X M, ZHU X B, CAI Q, <i>et al</i>.Survey of the Deep Learning Models for Image Semantic Segmentation.Chinese High Technology Letters, 2017, 27 (9) :808-815.) 
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201701008&amp;v=MTg0OTBiTXJvOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpnVmJyQU55ZlRiTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 姜枫, 顾庆, 郝慧珍, 等.基于内容的图像分割方法综述.软件学报, 2017, 28 (1) :160-183. (JIANG F, GU Q, HAO H Z, <i>et a</i>l.Survey on Content-Based Image Segmentation Methods.Journal of Software, 2017, 28 (1) :160-183.) 
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ICNet for Real Time Semantic Segmentation on High-Resolution Images[C/OL]">

                                <b>[4]</b> ZHAO H S, QI X J, SHEN X Y, <i>et al</i>.ICNet for Real Time Semantic Segmentation on High-Resolution Images[C/OL].[2018-08-15].https://arxiv.org/pdf/1704.08545.pdf.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB201804013&amp;v=MTYyMDRiTEc0SDluTXE0OUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpnVmJyQU1UWFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 李琳辉, 钱波, 连静, 等.基于卷积神经网络的交通场景语义分割方法研究.通信学报, 2018, 39 (4) :123-130. (LI L H, QIAN B, LIAN J, <i>et al</i>.Study on Traffic Scene Semantic Segmentation Method Based on Convolutional Neural Network.Journal on Communications, 2018, 39 (4) :123-130.) 
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sift Flow:Dense Corresponden-ce across Scenes and Its Applications">

                                <b>[6]</b> LIU C, YUEN J, TORRALBA A.Sift Flow:Dense Corresponden-ce across Scenes and Its Applications // HASSNER T, LIU C, eds.Dense Image Correspondences for Computer Vision.Berlin, Germany:Springer, 2011:15-49.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning hierarchical features for scene labeling">

                                <b>[7]</b> FARABET C, COUPRIE C, NAJMAN L, <i>et al</i>.Learning Hierarchical Features for Scene Labeling.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (8) :1915-1929.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully Convolutional Networks for Semantic Segmentation[C/OL]">

                                <b>[8]</b> LONG J, SHELHAMER E, DARRELL T.Fully Convolutional Networks for Semantic Segmentation[C/OL].[2018-08-15].https://arxiv.org/pdf/1411.4038.pdf.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMD71033EF8EE149F894A1F173A0F662B0&amp;v=MDM1NDRDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3TDI1eGFBPU5pZklZOGUvSDlIUHJQb3piSjU2RFhnd3VSNGE3azU4UG43bHIyTTFEN1NTUjhpZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet Classification with Deep Convolutional Neural Networks.Communications of the ACM, 2017, 60 (6) :84-90.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL]">

                                <b>[10]</b> SIMONYAN K, ZISSERMAN A.Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL].[2018-08-15].https://arxiv.org/pdf/1409.1556.pdf.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[11]</b> SZEGEDY C, LIU W, JIA Y Q, <i>et al</i>.Going Deeper with Convolutions // Proc of the 28th IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:1-9.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[12]</b> HE K M, ZHANG X Y, REN S Q, <i>et al</i>.Deep Residual Learning for Image Recognition // Proc of the 28th IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:770-778.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SegNet:A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling">

                                <b>[13]</b> BADRINARAYANAN V, HANDA A, CIPOLLA R.SegNet:A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling.IEEE Transaction on Pattern Analysis and Machine Intelligence, 2015, 39 (12) :2481-2495.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">

                                <b>[14]</b> RONNEBERGER O, FISCHER P, BROX T.U-Net:Convolutional Networks for Biomedical Image Segmentation // Proc of the International Conference on Medical Image Computing and Computer-Assisted Intervention.Berlin, Germany:Springer, 2015:234-241.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PixelNet:Representation of the Pixels,by the Pixels,and for the Pixels[C/OL]">

                                <b>[15]</b> BANSAL A, CHEN X L, RUSSELL B, <i>et al</i>.PixelNet:Representation of the Pixels, by the Pixels, and for the Pixels[C/OL].[2018-08-15].https://arxiv.org/pdf/1702.06506.pdf.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs[C/OL]">

                                <b>[16]</b> CHEN L C, PAPANDREOU G, KOKKINOS I, <i>et al</i>.Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs[C/OL].[2018-08-15].https://arxiv.org/pdf/1412.7062.pdf.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Lab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs">

                                <b>[17]</b> CHEN L C, PAPANDREOU G, KOKKINOS I, <i>et al</i>.DeepLab:Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (4) :834-848.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pyramid scene parsing network">

                                <b>[18]</b> ZHAO H S, SHI J P, QI X J, <i>et al</i>.Pyramid Scene Parsing Network // Proc of the IEEE Conference on Computer Vision and Pa-ttern Recognition.Washington, USA:IEEE, 2017:6230-6239.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-scale context aggregation by dilated convolutions">

                                <b>[19]</b> YU F, KOLTUN V.Multi-scale Context Aggregation by Dilated Convolutions[C/OL].[2018-08-15].https://arxiv.org/pdf/1511.07122.pdf.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dilated residual networks">

                                <b>[20]</b> YU F, KOLTUN V, FUNKHOUSER T.Dilated Residual Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:636-644.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rethinking Atrous Convolution for Semantic Image Segmentation[C/OL]">

                                <b>[21]</b> CHEN L C, PAPANDREOU G, SCHROFF F, <i>et al</i>.Rethinking Atrous Convolution for Semantic Image Segmentation[C/OL].[2018-08-15].https://arxiv.org/pdf/1706.05587.pdf.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vortex Pooling:Improving Con-text Representation in Semantic Segmentation[C/OL]">

                                <b>[22]</b> XIE C W, ZHOU H Y, WU J X.Vortex Pooling:Improving Con-text Representation in Semantic Segmentation[C/OL].[2018-08-15].https://arxiv.org/pdf/1804.06242v2.pdf.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Densely connected convolutional networks">

                                <b>[23]</b> HUANG G, LIU Z, VAN DER MAATEN L, <i>et al</i>.Densely Connected Convolutional Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:2261-2269.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" >
                                    <b>[24]</b>
                                 DENG J, DONG W, SOCHER R, <i>et al</i>.ImageNet:A Large-Scale Hierarchical Image Database // Proc of the 22th IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2009:248-255.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;The Cityscapes Dataset for Semantic Urban Scene Understanding,&amp;quot;">

                                <b>[25]</b> BENENSON R, FRANKE U, ROTH S, <i>et al</i>.The Cityscapes Da-taset for Semantic Urban Scene Understanding // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:3213-3223.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201905012" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201905012&amp;v=MTkwMzZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpnVmJyQUtEN1liTEc0SDlqTXFvOUU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
