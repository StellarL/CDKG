<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131439459873750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201902007%26RESULT%3d1%26SIGN%3dvTsqXxceyvo0S5invM5wlZRjsH8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201902007&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201902007&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201902007&amp;v=MDc2MTN5em5VTHpQS0Q3WWJMRzRIOWpNclk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#95" data-title="1 密度峰值聚类算法 ">1 密度峰值聚类算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#96" data-title="&lt;b&gt;1.1&lt;/b&gt; 算法介绍"><b>1.1</b> 算法介绍</a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;1.2&lt;/b&gt; 聚类中心选择分析"><b>1.2</b> 聚类中心选择分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#122" data-title="2 正序迭代选择策略 ">2 正序迭代选择策略</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#172" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#173" data-title="&lt;b&gt;3.1&lt;/b&gt; 实验环境与实验数据"><b>3.1</b> 实验环境与实验数据</a></li>
                                                <li><a href="#180" data-title="&lt;b&gt;3.2&lt;/b&gt; 聚类数量分析"><b>3.2</b> 聚类数量分析</a></li>
                                                <li><a href="#201" data-title="&lt;b&gt;3.3&lt;/b&gt; 聚类结果分析"><b>3.3</b> 聚类结果分析</a></li>
                                                <li><a href="#224" data-title="&lt;b&gt;3.4&lt;/b&gt; 时间复杂度分析"><b>3.4</b> 时间复杂度分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#230" data-title="4 结 束 语 ">4 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#118" data-title="图1 样本点的分布图与决策图">图1 样本点的分布图与决策图</a></li>
                                                <li><a href="#141" data-title="图2 不同形式的决策图">图2 不同形式的决策图</a></li>
                                                <li><a href="#159" data-title="图3 具有不同聚类中心的聚类效果图">图3 具有不同聚类中心的聚类效果图</a></li>
                                                <li><a href="#160" data-title="图4 拐点图">图4 拐点图</a></li>
                                                <li><a href="#176" data-title="&lt;b&gt;表1&lt;/b&gt;&lt;b&gt;样本数据集&lt;/b&gt;"><b>表1</b><b>样本数据集</b></a></li>
                                                <li><a href="#179" data-title="&lt;b&gt;表2&lt;/b&gt;&lt;b&gt;不同算法在不同数据集上的参数取值&lt;/b&gt;"><b>表2</b><b>不同算法在不同数据集上的参数取值</b></a></li>
                                                <li><a href="#283" data-title="图5 本文算法在各数据集的拐点图">图5 本文算法在各数据集的拐点图</a></li>
                                                <li><a href="#197" data-title="图6 本文算法在各数据集上的聚类结果">图6 本文算法在各数据集上的聚类结果</a></li>
                                                <li><a href="#200" data-title="&lt;b&gt;表3&lt;/b&gt;&lt;b&gt;本文算法在各数据集上的最佳聚类结果&lt;/b&gt;"><b>表3</b><b>本文算法在各数据集上的最佳聚类结果</b></a></li>
                                                <li><a href="#217" data-title="&lt;b&gt;表4&lt;/b&gt;&lt;b&gt;7种算法在不同数据集上的平均纯度对比&lt;/b&gt;"><b>表4</b><b>7种算法在不同数据集上的平均纯度对比</b></a></li>
                                                <li><a href="#220" data-title="&lt;b&gt;表5&lt;/b&gt;&lt;b&gt;7种算法在不同数据集上的召回率对比&lt;/b&gt;"><b>表5</b><b>7种算法在不同数据集上的召回率对比</b></a></li>
                                                <li><a href="#223" data-title="&lt;b&gt;表6&lt;/b&gt;&lt;b&gt;7种算法在不同数据集上的F-measure值对比&lt;/b&gt;"><b>表6</b><b>7种算法在不同数据集上的F-measure值对比</b></a></li>
                                                <li><a href="#227" data-title="图7 5种算法在各数据集上运行时间对比">图7 5种算法在各数据集上运行时间对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="14">


                                    <a id="bibliography_1" title=" CHEN C L P, ZHANG C Y. Data-Intensive Applications, Challenges, Techniques and Technologies: A Survey on Big Data. Information Sciences, 2014, 275: 314-347." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300029345&amp;v=MjI2NjREM2c4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZ3WGFSVT1OaWZPZmJLOEh0TE9ySTlGWk9rRw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         CHEN C L P, ZHANG C Y. Data-Intensive Applications, Challenges, Techniques and Technologies: A Survey on Big Data. Information Sciences, 2014, 275: 314-347.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_2" title=" GAN W S, LIN J C W, CHAO H C, &lt;i&gt;et al&lt;/i&gt;. Data Mining in Distributed Environment: A Survey. Data Mining and Knowledge Discovery, 2017, 7: e1216." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data Mining in Distributed Environment: A Survey">
                                        <b>[2]</b>
                                         GAN W S, LIN J C W, CHAO H C, &lt;i&gt;et al&lt;/i&gt;. Data Mining in Distributed Environment: A Survey. Data Mining and Knowledge Discovery, 2017, 7: e1216.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_3" title=" XU D K, TIAN Y J. A Comprehensive Survey of Clustering Algorithms. Annals of Data Science, 2015, 2 (2) : 165-193." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Comprehensive Survey of Clustering Algorithms">
                                        <b>[3]</b>
                                         XU D K, TIAN Y J. A Comprehensive Survey of Clustering Algorithms. Annals of Data Science, 2015, 2 (2) : 165-193.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_4" title=" JAIN A K, DUBES R C. Algorithms for Clustering Data. New York, USA: Prentice Hall, 1988." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Algorithms for Clustering Data">
                                        <b>[4]</b>
                                         JAIN A K, DUBES R C. Algorithms for Clustering Data. New York, USA: Prentice Hall, 1988.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_5" title=" 王万良.人工智能及其应用.第3版.高等教育出版社, 2016. (WANG W L. Artificial Intelligence and Application. 3rd Edition. Beijing, China: Higher Education Press, 2016) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787040442557000&amp;v=MjUzNDd6R2JPOEh0WElyWXBBWStzUERCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2c1U3dktJMTBUWEZx&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         王万良.人工智能及其应用.第3版.高等教育出版社, 2016. (WANG W L. Artificial Intelligence and Application. 3rd Edition. Beijing, China: Higher Education Press, 2016) 
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_6" title=" ZHANG Y M, LIU M D, LIU Q W. An Energy-Balanced Clustering Protocol Based on an Improved CFSFDP Algorithm for Wireless Sensor Networks. Sensors, 2018, 18 (3) . DOI: 10.3390/s18030881." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Energy-Balanced Clustering Protocol Based on an Improved CFSFDP Algorithm for Wireless Sensor Networks">
                                        <b>[6]</b>
                                         ZHANG Y M, LIU M D, LIU Q W. An Energy-Balanced Clustering Protocol Based on an Improved CFSFDP Algorithm for Wireless Sensor Networks. Sensors, 2018, 18 (3) . DOI: 10.3390/s18030881.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_7" title=" ALTMAN N, KRZYWINSKI M. Points of Significance: Clustering. Nature Methods, 2017, 14 (6) : 545-546." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering">
                                        <b>[7]</b>
                                         ALTMAN N, KRZYWINSKI M. Points of Significance: Clustering. Nature Methods, 2017, 14 (6) : 545-546.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_8" title=" QIN B Y, LI Z, LUO Z H, &lt;i&gt;et al&lt;/i&gt;. Terahertz Time-Domain Spectroscopy Combined with PCA-CFSFDP Applied for Pesticide Detection. Optical &amp;amp; Quantum Electronics, 2017, 49 (7) . DOI: 10.1007/s11082-017-1080-x." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Terahertz time-domain spectroscopy combined with PCA-CFSFDP applied for pesticide detection">
                                        <b>[8]</b>
                                         QIN B Y, LI Z, LUO Z H, &lt;i&gt;et al&lt;/i&gt;. Terahertz Time-Domain Spectroscopy Combined with PCA-CFSFDP Applied for Pesticide Detection. Optical &amp;amp; Quantum Electronics, 2017, 49 (7) . DOI: 10.1007/s11082-017-1080-x.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_9" title=" 郑建炜, 路程, 秦梦洁, 等.联合特征选择和光滑表示的子空间聚类算法.模式识别与人工智能, 2018, 31 (5) : 409-418. (ZHENG J W, LU C, QIN M J, &lt;i&gt;et al&lt;/i&gt;. Subspace Clustering via Joint Feature Selection and Smooth Representation. Pattern Recognition and Artificial Intelligence, 2018, 31 (5) : 409-418.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201805003&amp;v=MTI3ODVMRzRIOW5NcW85Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5em5VTHpQS0Q3WWI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         郑建炜, 路程, 秦梦洁, 等.联合特征选择和光滑表示的子空间聚类算法.模式识别与人工智能, 2018, 31 (5) : 409-418. (ZHENG J W, LU C, QIN M J, &lt;i&gt;et al&lt;/i&gt;. Subspace Clustering via Joint Feature Selection and Smooth Representation. Pattern Recognition and Artificial Intelligence, 2018, 31 (5) : 409-418.) 
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_10" title=" 逯瑞强, 马福民, 张腾飞.基于区间2-型模糊度量的粗糙&lt;i&gt;K&lt;/i&gt;-means聚类算法.模式识别与人工智能, 2018, 31 (3) : 265-274. (LU R Q, MA F M, ZHANG T F. Interval Type-2 Fuzzy Measure Based Rough &lt;i&gt;K&lt;/i&gt;-means Clustering. Pattern Recognition and Artificial Intelligence, 2018, 31 (3) : 265-274.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201803008&amp;v=MjM4NjZPZVplUm5GeXpuVUx6UEtEN1liTEc0SDluTXJJOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         逯瑞强, 马福民, 张腾飞.基于区间2-型模糊度量的粗糙&lt;i&gt;K&lt;/i&gt;-means聚类算法.模式识别与人工智能, 2018, 31 (3) : 265-274. (LU R Q, MA F M, ZHANG T F. Interval Type-2 Fuzzy Measure Based Rough &lt;i&gt;K&lt;/i&gt;-means Clustering. Pattern Recognition and Artificial Intelligence, 2018, 31 (3) : 265-274.) 
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_11" title=" 雷小锋, 谢昆青, 林帆, 等.一种基于&lt;i&gt;K&lt;/i&gt;-means局部最优性的高效聚类算法.软件学报, 2008, 19 (7) : 1683-1692. (LEI X F, XIE K Q, LIN F, &lt;i&gt;et al&lt;/i&gt;. An Efficient Clustering Algorithm Based on Local Optimality of &lt;i&gt;K&lt;/i&gt;-means. Journal of Software, 2008, 19 (7) : 1683-1692.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200807014&amp;v=MTA2NDVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpuVUx6UE55ZlRiTEc0SHRuTXFJOUVZSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         雷小锋, 谢昆青, 林帆, 等.一种基于&lt;i&gt;K&lt;/i&gt;-means局部最优性的高效聚类算法.软件学报, 2008, 19 (7) : 1683-1692. (LEI X F, XIE K Q, LIN F, &lt;i&gt;et al&lt;/i&gt;. An Efficient Clustering Algorithm Based on Local Optimality of &lt;i&gt;K&lt;/i&gt;-means. Journal of Software, 2008, 19 (7) : 1683-1692.) 
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_12" title=" ZHANG T, RAMAKRISHNAN R, LIVNY M. BIRCH: A New Data Clustering Algorithm and Its Applications. Data Mining and Knowledge Discovery, 1997, 1 (2) : 141-182." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002157106&amp;v=MjIxNThwQ1plc0pZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDSGxWNzNMSTFrPU5qN0Jhck80SHRIT3Jv&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         ZHANG T, RAMAKRISHNAN R, LIVNY M. BIRCH: A New Data Clustering Algorithm and Its Applications. Data Mining and Knowledge Discovery, 1997, 1 (2) : 141-182.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_13" title=" GUHA S, RASTOGI R, SHIM K. CURE: An Efficient Clustering Algorithm for Large Database. Information Systems, 2001, 26 (1) : 35-58." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100892160&amp;v=MjU0NjNQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGd1hhUlU9TmlmT2ZiSzdIdERPcm85RmJPSU5EWG81b0JNVDZUNA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         GUHA S, RASTOGI R, SHIM K. CURE: An Efficient Clustering Algorithm for Large Database. Information Systems, 2001, 26 (1) : 35-58.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     RODRIGUEZ A, LAIO A. Clustering by Fast Search and Find of Density Peaks. Science, 2014, 344 (6191) : 1492-1496.</a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_15" title=" ESTER M, KRIEGEL H P, XU X. A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise // Proc of the International Conference on Knowledge Discovery and Data Mining. Palo Alto, USA: AAAI Press, 1996: 226-231." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise">
                                        <b>[15]</b>
                                         ESTER M, KRIEGEL H P, XU X. A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise // Proc of the International Conference on Knowledge Discovery and Data Mining. Palo Alto, USA: AAAI Press, 1996: 226-231.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_16" title=" XIE J Y, GAO H C, XIE W X, &lt;i&gt;et al&lt;/i&gt;. Robust Clustering by Detecting Density Peaks and Assigning Points Based on Fuzzy Weighted K-nearest Neighbors. Information Sciences, 2016, 354: 19-40." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust clustering by detecting density peaks and assigning points based on fuzzy weighted K-nearest neighbors">
                                        <b>[16]</b>
                                         XIE J Y, GAO H C, XIE W X, &lt;i&gt;et al&lt;/i&gt;. Robust Clustering by Detecting Density Peaks and Assigning Points Based on Fuzzy Weighted K-nearest Neighbors. Information Sciences, 2016, 354: 19-40.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_17" title=" MEHMOOD R, BIE R, JIAO L B, &lt;i&gt;et al&lt;/i&gt;. Adaptive Cutoff Distance: Clustering by Fast Search and Find of Density Peaks. Journal of Intelligent and Fuzzy Systems, 2016, 31 (5) : 2619-2628." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive cutoff distance:Clustering by fast search and find of density peaks">
                                        <b>[17]</b>
                                         MEHMOOD R, BIE R, JIAO L B, &lt;i&gt;et al&lt;/i&gt;. Adaptive Cutoff Distance: Clustering by Fast Search and Find of Density Peaks. Journal of Intelligent and Fuzzy Systems, 2016, 31 (5) : 2619-2628.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_18" title=" WANG W, YANG J, MUNTZ R R. STING: A Statistical Information Grid Approach to Spatial Data Mining // Proc of the International Conference on Very Large Data Bases. San Francisco, USA: Morgan Kaufmann Publisher, 1997: 186-195." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=STING: A statistical information grid approach to spatial data mining">
                                        <b>[18]</b>
                                         WANG W, YANG J, MUNTZ R R. STING: A Statistical Information Grid Approach to Spatial Data Mining // Proc of the International Conference on Very Large Data Bases. San Francisco, USA: Morgan Kaufmann Publisher, 1997: 186-195.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_19" title=" AGRAWAL R, GEHRKE J, GUNOPULOS D, &lt;i&gt;et al&lt;/i&gt;. Automatic Subspace Clustering of High Dimensional Data for Data Mining Applications. ACM SIGMOD Record, 1998, 27 (2) : 94-105." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000068877&amp;v=MjU0NThNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZ3WGFSVT1OaWZJWTdLN0h0ak5yNDlGWk8wSEJIcytvQg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         AGRAWAL R, GEHRKE J, GUNOPULOS D, &lt;i&gt;et al&lt;/i&gt;. Automatic Subspace Clustering of High Dimensional Data for Data Mining Applications. ACM SIGMOD Record, 1998, 27 (2) : 94-105.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_20" title=" 朱杰, 陈黎飞.核密度估计的聚类算法.模式识别与人工智能, 2017, 30 (5) : 439-447. (ZHU J, CHEN L F. Clustering Algorithm with Kernel Density Estimation. Pattern Recognition and Artificial Intelligence, 2017, 30 (5) : 439-447.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201705006&amp;v=MDIyNTE0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpuVUx6UEtEN1liTEc0SDliTXFvOUZZb1FLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         朱杰, 陈黎飞.核密度估计的聚类算法.模式识别与人工智能, 2017, 30 (5) : 439-447. (ZHU J, CHEN L F. Clustering Algorithm with Kernel Density Estimation. Pattern Recognition and Artificial Intelligence, 2017, 30 (5) : 439-447.) 
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_21" title=" HAN J W, KAMBER M, PEI J. Data Mining: Concepts and Techniques. New York, USA: Elsevier, 2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data Mining: Concepts and Techniques">
                                        <b>[21]</b>
                                         HAN J W, KAMBER M, PEI J. Data Mining: Concepts and Techniques. New York, USA: Elsevier, 2011.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_22" title=" BIE R F, MEHMOOD R, RUAN S S, &lt;i&gt;et al&lt;/i&gt;. Adaptive Fuzzy Clustering by Fast Search and Find of Density Peaks. Personal and Ubiquitous Computing, 2016, 20 (5) : 785-793." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive Fuzzy Clustering by Fast Search and Find of Density Peaks">
                                        <b>[22]</b>
                                         BIE R F, MEHMOOD R, RUAN S S, &lt;i&gt;et al&lt;/i&gt;. Adaptive Fuzzy Clustering by Fast Search and Find of Density Peaks. Personal and Ubiquitous Computing, 2016, 20 (5) : 785-793.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_23" title=" MEHMOOD R, BIE R F, DAWOOD H, &lt;i&gt;et al&lt;/i&gt;. Fuzzy Clustering by Fast Search and Find of Density Peaks // Proc of the International Conference on Identification, Information, and Knowledge in the Internet of Things. Washington, USA: IEEE, 2015: 258-261." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fuzzy Clustering by Fast Search and Find of Density Peaks">
                                        <b>[23]</b>
                                         MEHMOOD R, BIE R F, DAWOOD H, &lt;i&gt;et al&lt;/i&gt;. Fuzzy Clustering by Fast Search and Find of Density Peaks // Proc of the International Conference on Identification, Information, and Knowledge in the Internet of Things. Washington, USA: IEEE, 2015: 258-261.
                                    </a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_24" title=" WANG J L, ZHANG Y, LAN X. Automatic Cluster Number Selection by Finding Density Peaks // Proc of the 2nd IEEE Internatio-nal Conference on Computer and Communications. Washington, USA: IEEE, 2016: 13-18." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic Cluster Number Selection by Finding Density Peaks">
                                        <b>[24]</b>
                                         WANG J L, ZHANG Y, LAN X. Automatic Cluster Number Selection by Finding Density Peaks // Proc of the 2nd IEEE Internatio-nal Conference on Computer and Communications. Washington, USA: IEEE, 2016: 13-18.
                                    </a>
                                </li>
                                <li id="62">


                                    <a id="bibliography_25" title=" DING J J, CHEN Z T, HE X X, &lt;i&gt;et al&lt;/i&gt;. Clustering by Finding Density Peaks Based on Chebyshev&#39;s Inequality // Proc of the 35th Chinese Control Conference. Washington, USA: IEEE, 2016: 7169-7172." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering by Finding Density Peaks Based on Chebyshev&amp;#39;&amp;#39;s Inequality">
                                        <b>[25]</b>
                                         DING J J, CHEN Z T, HE X X, &lt;i&gt;et al&lt;/i&gt;. Clustering by Finding Density Peaks Based on Chebyshev&#39;s Inequality // Proc of the 35th Chinese Control Conference. Washington, USA: IEEE, 2016: 7169-7172.
                                    </a>
                                </li>
                                <li id="64">


                                    <a id="bibliography_26" title=" XU X H, JU Y S, LIANG Y L, &lt;i&gt;et al&lt;/i&gt;. Manifold Density Peaks Clustering Algorithm // Proc of the 3rd International Conference on Advanced Cloud and Big Data. Washington, USA: IEEE, 2015: 311-318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Manifold density peaks clustering algorithm">
                                        <b>[26]</b>
                                         XU X H, JU Y S, LIANG Y L, &lt;i&gt;et al&lt;/i&gt;. Manifold Density Peaks Clustering Algorithm // Proc of the 3rd International Conference on Advanced Cloud and Big Data. Washington, USA: IEEE, 2015: 311-318.
                                    </a>
                                </li>
                                <li id="66">


                                    <a id="bibliography_27" title=" ZHOU R, ZHANG S, CHEN C, &lt;i&gt;et al&lt;/i&gt;. A Distance and Density-Based Clustering Algorithm Using Automatic Peak Detection // Proc of the IEEE International Conference on Smart Cloud. Wa-shington, USA: IEEE, 2016: 176-183." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A distance and density-based clustering algorithm using automatic peak detection">
                                        <b>[27]</b>
                                         ZHOU R, ZHANG S, CHEN C, &lt;i&gt;et al&lt;/i&gt;. A Distance and Density-Based Clustering Algorithm Using Automatic Peak Detection // Proc of the IEEE International Conference on Smart Cloud. Wa-shington, USA: IEEE, 2016: 176-183.
                                    </a>
                                </li>
                                <li id="68">


                                    <a id="bibliography_28" title=" 淦文燕, 刘冲.一种改进的搜索密度峰值的聚类算法.智能系统学报, 2017, 12 (2) : 229-236. (GAN W Y, LIU C. An Improved Clustering Algorithm that Searches and Finds Density Peaks. CAAI Transactions on Intelligent Systems, 2017, 12 (2) : 229-236) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZNXT201702013&amp;v=MDczMTVuVUx6UFB5UFRlckc0SDliTXJZOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                         淦文燕, 刘冲.一种改进的搜索密度峰值的聚类算法.智能系统学报, 2017, 12 (2) : 229-236. (GAN W Y, LIU C. An Improved Clustering Algorithm that Searches and Finds Density Peaks. CAAI Transactions on Intelligent Systems, 2017, 12 (2) : 229-236) .
                                    </a>
                                </li>
                                <li id="70">


                                    <a id="bibliography_29" title=" 贾培灵, 樊建聪, 彭延军.一种基于簇边界的密度峰值点快速搜索聚类算法.南京大学学报 (自然科学) , 2017, 53 (2) : 368-377. (JIA P L, FAN J C, PENG Y J. An Improved Clustering Algorithm by Fast Search and Find of Density Peaks Based on Boundary Samples. Journal of Nanjing University (Natural Sciences) , 2017, 53 (2) : 368-377.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJDZ201702019&amp;v=MTUwMjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpuVUx6UEt5ZlBkTEc0SDliTXJZOUViWVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[29]</b>
                                         贾培灵, 樊建聪, 彭延军.一种基于簇边界的密度峰值点快速搜索聚类算法.南京大学学报 (自然科学) , 2017, 53 (2) : 368-377. (JIA P L, FAN J C, PENG Y J. An Improved Clustering Algorithm by Fast Search and Find of Density Peaks Based on Boundary Samples. Journal of Nanjing University (Natural Sciences) , 2017, 53 (2) : 368-377.) 
                                    </a>
                                </li>
                                <li id="72">


                                    <a id="bibliography_30" title=" RAGHAVAN V V, DEOGUN J S, SEVER H. Introduction to Data Mining. New York, USA: John Wiley &amp;amp; Sons, 1998." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Introduction to Data Mining">
                                        <b>[30]</b>
                                         RAGHAVAN V V, DEOGUN J S, SEVER H. Introduction to Data Mining. New York, USA: John Wiley &amp;amp; Sons, 1998.
                                    </a>
                                </li>
                                <li id="74">


                                    <a id="bibliography_31" title=" GIONIS A, MANNILA H, TSAPARAS P. Clustering Aggregation. ACM Transactions on Knowledge Discovery from Data, 2007, 1 (1) . DOI: 10.1145/1217299.1217303." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000093142&amp;v=MjU5MDlpZklZN0s3SHRqTnI0OUZaT0lNRFhnN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGd1hhUlU9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[31]</b>
                                         GIONIS A, MANNILA H, TSAPARAS P. Clustering Aggregation. ACM Transactions on Knowledge Discovery from Data, 2007, 1 (1) . DOI: 10.1145/1217299.1217303.
                                    </a>
                                </li>
                                <li id="76">


                                    <a id="bibliography_32" title=" CHANG H, YEUNG D Y. Robust Path-Based Spectral Clustering. Pattern Recognition, 2008, 41 (1) : 191-203." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739343&amp;v=MjU0ODBaZVp1SHlqbVVMYklKRndYYVJVPU5pZk9mYks3SHRETnFZOUZZK2dHRDNnNm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[32]</b>
                                         CHANG H, YEUNG D Y. Robust Path-Based Spectral Clustering. Pattern Recognition, 2008, 41 (1) : 191-203.
                                    </a>
                                </li>
                                <li id="78">


                                    <a id="bibliography_33" title=" PAL N R, PAL K, KELLER J M, &lt;i&gt;et al&lt;/i&gt;. A Possibilistic Fuzzy &lt;i&gt;c&lt;/i&gt;-means Clustering Algorithm. IEEE Transactions on Fuzzy Systems, 2005, 13 (4) : 517-530." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Possibilistic Fuzzy c-Means Clustering Algorithm">
                                        <b>[33]</b>
                                         PAL N R, PAL K, KELLER J M, &lt;i&gt;et al&lt;/i&gt;. A Possibilistic Fuzzy &lt;i&gt;c&lt;/i&gt;-means Clustering Algorithm. IEEE Transactions on Fuzzy Systems, 2005, 13 (4) : 517-530.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(02),151-160 DOI:10.16451/j.cnki.issn1003-6059.201902007            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于正序迭代选择策略的聚类中心自动选择方法</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E4%B8%87%E8%89%AF&amp;code=10288021&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王万良</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%95%E9%97%AF&amp;code=40739995&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吕闯</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E7%87%95%E4%BC%9F&amp;code=09406708&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵燕伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E6%A5%A0&amp;code=33860330&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高楠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E5%B0%8F%E6%B6%B5&amp;code=38564684&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨小涵</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%85%86%E5%A8%9F&amp;code=31045425&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张兆娟</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%99%E6%B1%9F%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0198836&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">浙江工业大学计算机科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对密度峰值聚类算法的决策函数不能自动有效地确定聚类中心的问题, 提出自动确定聚类中心的密度峰值聚类算法.首先, 通过归一化处理, 使决策函数中的两个变量分布均匀.然后, 在确定聚类中心时, 提出正序迭代选择策略, 即根据聚类核心点数目的变化趋势搜索拐点, 并以拐点之前的点作为聚类中心, 完成聚类.最后, 在UCI数据集上验证文中算法的性能, 算法在未提高时间复杂度的情况下, 可以对任意分布形状的数据集进行聚类, 具有较好的适应性和聚类效果.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%9A%E7%B1%BB%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">聚类中心;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%86%B3%E7%AD%96%E5%87%BD%E6%95%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">决策函数;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AD%A3%E5%BA%8F%E8%BF%AD%E4%BB%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">正序迭代;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AF%86%E5%BA%A6%E5%B3%B0%E5%80%BC%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">密度峰值聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据挖掘;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *王万良, 博士, 教授, 主要研究方向为深度学习、人工智能、大数据.E-mail:wwl@zjut.edu.cn.
;
                                </span>
                                <span>
                                    吕闯, 硕士研究生, 主要研究方向为大数据、数据挖掘.E-mail:lvchuang29@163.com.
;
                                </span>
                                <span>
                                    赵燕伟, 博士, 教授, 主要研究方向为智能设计、智能控制.E-mail:zyw@zjut.edu.cn.
;
                                </span>
                                <span>
                                    高楠, 博士, 讲师, 主要研究方向为数据挖掘、最优化分析、生物信息学等.E-mail:gao nan@zjut.edu.cn.
;
                                </span>
                                <span>
                                    杨小涵, 硕士研究生, 主要研究方向为大数据、深度学习.E-mail:58482769@qq.com.
;
                                </span>
                                <span>
                                    张兆娟, 博士研究生, 主要研究方向为大数据分析、数据驱动的优化、深度学习.E-mail:zjzhang@zjut.edu.cn.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-13</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.61572438, 61702456, 61873240) 资助;</span>
                    </p>
            </div>
                    <h1>Automatic Selection Method of Cluster Center Based on</h1>
                    <h2>
                    <span>WANG Wanliang</span>
                    <span>Chuang</span>
                    <span>ZHAO Yanwei</span>
                    <span>GAO Nan</span>
                    <span>YANG Xiaohan</span>
                    <span>ZHANG Zhaojuan</span>
            </h2>
                    <h2>
                    <span>College of Computer Science and Technology, Zhejiang University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The decision function of density peak clustering algorithm cannot determine the clustering center automatically and effectively. Therefore, a density peak clustering algorithm, automatically clustering by fast search and find of density peaks (AUTO-CFSFDP) , is proposed. Firstly, the normalization process is carried out to make the uneven distribution of variables in the decision function become uniform. Secondly, the selection strategy based on positive-sequence iteration is presented to search elbow point according to the variation trend of the number of cluster core points in the process of determining the cluster center. A set of points before the elbow point is used as the cluster centers to complete clustering. Finally, the performance of AUTO-CFSFDP is evaluated on UCI datasets. AUTO-CFSFDP can cluster the datasets of arbitrary distributions without extra time consumption. The adaptability and clustering results are improved effectively.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Cluster%20Center&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Cluster Center;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Decision%20Function&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Decision Function;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Positive%20Sequence%20Iterative&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Positive Sequence Iterative;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Density%20Peak%20Clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Density Peak Clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Data%20Mining&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Data Mining;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Wanliang, Ph. D., professor. His research interests include deep learning, artificial intelligence and big data.
;
                                </span>
                                <span>
                                    LÜ Chuang, master student. His research interests include big data and data mining.
;
                                </span>
                                <span>
                                    ZHAO Yanwei, Ph.D., professor. Her research interests include intelligent design and intelligent control.
;
                                </span>
                                <span>
                                    GAO Nan, Ph. D., lecturer. Her research interests include data mining, optimization analysis and bioinformatics.
;
                                </span>
                                <span>
                                    YANG Xiaohan, master student. Her research interests include big data and deep learning.
;
                                </span>
                                <span>
                                    ZHANG Zhaojuan, Ph.D. candidate. Her research interests include big data analysis, data-driven optimization and deep learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-08-13</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.61572438, 61702456, 61873240);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="89">伴随着大数据<citation id="286" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>概念的提出, 数据挖掘技术在技术和应用方面又拥有新的平台和环境<citation id="287" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 但同时也面临一定挑战.聚类分析<citation id="284" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>是数据挖掘领域中重要的研究课题, 它属于机器学习中无监督的数据分析方法.聚类<citation id="285" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>可以发现无标签对象之间的潜在关</p>
                </div>
                <div class="p1">
                    <p id="90">系, 按照某一方法将相关关系相近的对象进行分簇, 保证簇间具有较高的相异度和簇内具有较高的相似度.</p>
                </div>
                <div class="p1">
                    <p id="91">聚类分析技术已广泛应用于多个领域, 包括人工智能<citation id="293" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、Web搜索<citation id="294" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、生物学<citation id="291" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、安全<citation id="292" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、数据挖掘<citation id="296" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>等.作为数据分析的基本组成部分, 聚类具有重要作用, 现阶段不仅用于创建多种聚类分析工具, 还产生多种不同的聚类算法.目前主要的聚类算法可以划分为如下5类:划分方法<citation id="295" type="reference"><link href="32" rel="bibliography" /><link href="34" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>、层次方法<citation id="288" type="reference"><link href="36" rel="bibliography" /><link href="38" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>、基于密度的方法<citation id="297" type="reference"><link href="40" rel="bibliography" /><link href="42" rel="bibliography" /><link href="44" rel="bibliography" /><link href="46" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>、基于网格的方法<citation id="290" type="reference"><link href="48" rel="bibliography" /><link href="50" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>及基于模型的方法<citation id="289" type="reference"><link href="52" rel="bibliography" /><link href="54" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="92">2014年Rodriguez等<citation id="299" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出密度峰值聚类算法 (Clustering by Fast Search and Find of Density Peaks, CFSFDP) , 具有密度峰值聚类普遍的优点, 克服基于距离的算法只能发现“类圆形”聚类的缺点, 发现任意形状的簇, 对噪声数据不敏感<citation id="298" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>.但是, CFSFDP需要通过决策图并辅以主观经验才能完成对聚类中心的选择, 导致在选择聚类中心时存在误差, 效率较低.</p>
                </div>
                <div class="p1">
                    <p id="93">为了解决CFSFDP在聚类过程中主观因素影响决策聚类中心选择的问题, 学者们进行一系列的研究.Mehmood等<citation id="303" type="reference"><link href="58" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>提出Fuzzy-CFSFDP, 使用模糊规则自适应选择聚类中心.Wang等<citation id="302" type="reference"><link href="60" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>提出有效的自动确定聚类中心方法, 无需使用聚类结果优化聚类数量.Ding等<citation id="300" type="reference"><link href="62" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>利用切比雪夫不等式方法自动确定聚类中心的数量.Xu等<citation id="301" type="reference"><link href="64" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>提出流行密度峰值聚类方法, 改善基本的密度峰值聚类, 从某种程度上解决手工拖动矩形以识别簇中心的不方便性.Zhou等<citation id="305" type="reference"><link href="66" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>提出改进的Canopy算法, 自动确定聚类中心的阈值.淦文燕等<citation id="306" type="reference"><link href="68" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>引入密度估计熵自适应优化算法参数, 提出改进的自动搜索密度峰值聚类算法.贾培灵等<citation id="304" type="reference"><link href="70" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>提出基于簇边界划分的密度峰值点聚类算法 (Partition-Based Clustering by Fast Search and Find of Density Peaks Algorithm, B-DPC) , 采用二次聚类的方法, 达到较好的聚类效果.虽然国内外都在自动确定聚类中心的研究方面取得一定成果, 但是对比这些算法发现, 都存在追加更多的参数、计算复杂度较高等不足.</p>
                </div>
                <div class="p1">
                    <p id="94">鉴于此种情况, 本文提出自动确定聚类中心的密度峰值聚类算法 (Automatically Clustering by Fast Search and Find of Density Peaks, AUTO-CFSFDP) .针对决策函数中变量分布不均匀的情形, 进行归一化处理, 使决策函数中的两个变量分布均匀.在确定聚类中心时, 提出正序迭代选择策略, 即根据聚类核心点数目的变化趋势搜索拐点, 并以拐点之前的点作为聚类中心, 完成聚类, 解决密度峰值聚类算法不能自动确定聚类中心的问题.在UCI数据集上验证文中算法的性能, 在未提高时间复杂度的同时, 可以对任意分布形状的数据集进行聚类, 具有优良的适应性和聚类效果.</p>
                </div>
                <h3 id="95" name="95" class="anchor-tag">1 密度峰值聚类算法</h3>
                <h4 class="anchor-tag" id="96" name="96"><b>1.1</b> 算法介绍</h4>
                <div class="p1">
                    <p id="97">CFSFDP<citation id="307" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>的核心思想主要是对聚类中心点的刻画, 聚类中心必然密度较大, 即聚类中心点被密度均不超过它的邻居所包围.聚类中心与其它密度更大的数据点之间的距离相对更大.</p>
                </div>
                <div class="p1">
                    <p id="98">考虑待聚类的数据集<i>S</i>={<i>x</i><sub><i>i</i></sub>}, 数据样本集映射至二维空间的分布如图1 (a) 所示, 然后分别计算上述两点定义的局部密度<i>ρ</i><sub><i>i</i></sub>和距离<i>δ</i><sub><i>i</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="99">局部密度<citation id="308" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation><sup></sup></p>
                </div>
                <div class="p1">
                    <p id="100"><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>X</mi></mstyle><mo stretchy="false"> (</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>.      (1) </p>
                </div>
                <div class="p1">
                    <p id="102">其中:<i>ρ</i><sub><i>i</i></sub>表示数据点<i>i</i>的局部密度;<i>d</i><sub><i>ij</i></sub>表示数据点<i>i</i>与数据点<i>j</i>之间的距离, 这里采用欧氏距离计算;<i>d</i><sub><i>c</i></sub>表示截断距离, 计算方法是对任意数据点<i>i</i>与<i>j</i>的所有<i>d</i><sub><i>ij</i></sub>进行升序排列, 取某一比例位置的距离<i>d</i><sub><i>ij</i></sub>作为截断距离<i>d</i><sub><i>c</i></sub>的值, 即<i>d</i><sub><i>c</i></sub>=<i>d</i><sub><i>f</i><sub> (<i>M</i><sub><i>t</i></sub>) </sub></sub>, <i>M</i>表示所有<i>d</i><sub><i>ij</i></sub>的个数, <i>f</i> (<i>M</i><sub><i>t</i></sub>) 表示对<i>M</i><sub><i>t</i></sub>进行四舍五入后得到的整数值, <i>t</i>为比例常数.函数</p>
                </div>
                <div class="p1">
                    <p id="103" class="code-formula">
                        <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>X</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>, </mo></mtd><mtd columnalign="left"><mi>x</mi><mo>&lt;</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>, </mo></mtd><mtd columnalign="left"><mi>x</mi><mo>≥</mo><mn>0</mn></mtd></mtr></mtable></mrow></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="104">距离<citation id="309" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation><sup></sup></p>
                </div>
                <div class="p1">
                    <p id="105"><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>j</mi><mo>∶</mo><mi>ρ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>&gt;</mo><mi>ρ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mo stretchy="false"> (</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>.      (2) </p>
                </div>
                <div class="p1">
                    <p id="107">距离反映的是对数据点<i>i</i>, 局部密度值比之大 (<i>ρ</i><sub><i>j</i></sub>&gt;<i>ρ</i><sub><i>i</i></sub>) , 且距离<i>d</i><sub><i>ij</i></sub>最小的数据点<i>j</i>之间的距离.其中密度最大值点取<i>δ</i><sub><i>i</i></sub>=max<sub><i>j</i></sub> (<i>d</i><sub><i>ij</i></sub>) .</p>
                </div>
                <div class="p1">
                    <p id="108">图1 (a) 为一个样例数据集的映射二维图, 已知结果分为两类, 红色圈标注一类, 蓝色圈标注另一类, 黑色点为噪音点. (b) 为该数据集的决策图, 正如CFSFDP描述的聚类中心点的两点特性, 在决策图上分别有较大的局部密度<i>ρ</i><sub><i>i</i></sub>和距离<i>δ</i><sub><i>i</i></sub>, 因此选择同时具备这两个特性的1号数据点和10号数据点作为聚类的中心点, 这也正是CFSFDP的核心思想.</p>
                </div>
                <div class="p1">
                    <p id="109">CFSFDP步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="110"><b>算法1</b> CFSFDP</p>
                </div>
                <div class="p1">
                    <p id="111">step 1 计算数据样本点<i>i</i>和<i>j</i>之间的距离<i>d</i><sub><i>ij</i></sub>, 求出截断距离<i>d</i><sub><i>c</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="112">step 2 分别按照式 (1) 和式 (2) , 求出局部密度<i>ρ</i><sub><i>i</i></sub>和距离<i>δ</i><sub><i>i</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="113">step 3 根据<i>ρ</i><sub><i>i</i></sub>和<i>δ</i><sub><i>i</i></sub>画出聚类中心点的决策图, 选取数据集合适的聚类中心.</p>
                </div>
                <div class="p1">
                    <p id="114">step 4 将剩余数据点按照聚类中心、聚类边界点密度阈值、数据标号等分类到各个聚类中.</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902007_11800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 样本点的分布图与决策图" src="Detail/GetImg?filename=images/MSSB201902007_11800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 样本点的分布图与决策图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902007_11800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Distribution map and decision map of sample points</p>

                </div>
                <h4 class="anchor-tag" id="119" name="119"><b>1.2</b> 聚类中心选择分析</h4>
                <div class="p1">
                    <p id="120">针对聚类中心的选择这一步骤, CFSFDP通过对数据样本点的局部密度<i>ρ</i><sub><i>i</i></sub>和距离<i>δ</i><sub><i>i</i></sub>得出的决策图如图1 (b) 所示.CFSFDP在选择聚类中心时, 人为选择聚类中心的个数, 在决策图上, 取合适的临界值<i>ρ</i>和<i>δ</i>, 把<i>ρ</i><sub><i>i</i></sub>和<i>δ</i><sub><i>i</i></sub>分别大于临界值<i>ρ</i>和<i>δ</i>的数据点作为聚类中心点, 因为这些点必然满足在数据集中本身具有较大的密度值以及与其它密度较大值的数据点有相对较远的距离的要求.</p>
                </div>
                <div class="p1">
                    <p id="121">CFSFDP选取聚类中心时, 思想简单, 只需选择局部密度和距离同时相对较大的点.但是, 大部分数据集的决策图往往并不如图1所示那样简单明了, 即聚类中心与非中心点具有较大差异.因此, 在聚类过程中, CFSFDP不可避免地有主观因素参与其中, 在确定聚类中心时, 受主观因素的影响, 需要不断尝试选择聚类中心, 而且聚类结果也取决于经验积累, 这将花费极大的时间和精力.为了克服这一缺陷, 本文提出正序迭代选择策略.</p>
                </div>
                <h3 id="122" name="122" class="anchor-tag">2 正序迭代选择策略</h3>
                <div class="p1">
                    <p id="123">多种数据集的决策图中并不能直观反映聚类中心的数目, 因此, 针对决策图中数据点分布不清晰、人为主观选择聚类中心的难度较大、依靠经验积累选择聚类中心的问题, 学者们提出一些方法, 如模糊规则方法<citation id="310" type="reference"><link href="58" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、切比雪夫不等式方法<citation id="311" type="reference"><link href="62" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、熵自适应方法<citation id="312" type="reference"><link href="68" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>等.这些方法是在牺牲计算量和融入其它算法思想的情况下达到自动寻找聚类中心的目的.为了避免计算复杂度增加, 本文提出正序迭代选择策略, 完成聚类中心的自动选择.</p>
                </div>
                <div class="p1">
                    <p id="124">为了有效处理决策函数中变量分布不均匀、能够自动选择数据集聚类中心的问题, 先给出正序迭代选择策略的相关定义.</p>
                </div>
                <div class="p1">
                    <p id="125"><b>定义1</b> 局部密度<citation id="313" type="reference"><link href="64" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation><sup></sup></p>
                </div>
                <div class="p1">
                    <p id="126"><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mrow><mi>exp</mi></mrow></mstyle></mrow></math></mathml><mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>-</mo><mrow><mo stretchy="false"> (</mo><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mrow><mi>d</mi><msub><mrow></mrow><mi>c</mi></msub></mrow></mfrac><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>,      (3) </p>
                </div>
                <div class="p1">
                    <p id="129">其中, d<sub>ij</sub>为数据点x<sub>i</sub>与数据点x<sub>j</sub>相互间的欧氏距离, 用于表示数据点之间的相似度.</p>
                </div>
                <div class="p1">
                    <p id="130">为了避免因为式 (1) 导致的不同数据点可能会有相同的局部密度值的情况发生, 这里采用高斯内核分布计算方法, 式 (1) 的计算结果为数量, 精确度为1, 当截断距离取值较小时, 不同数据点截断距离半径范围内的数据点数有可能会相同, 导致不同数据点的局部密度值可能相等.而采用高斯内核式 (3) 的方法, 计算结果的精确度可以很高 (实验部分取10<sup>-3</sup>) , 极大排除之前所述情况的发生, 可以最大程度地保证不同的数据点拥有不同的局部密度 (重合数据点的局部密度除外) .式 (3) 中数据点i的局部密度ρ<sub>i</sub>同样可以反映距离比截断距离更近的点数<citation id="314" type="reference"><link href="64" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="131">对于距离, 本文采用文献<citation id="315" type="reference">[<a class="sup">14</a>]</citation>的距离计算方式.距离反映的是相对数据点i, 局部密度值比之大 (ρ<sub>j</sub>&gt;ρ<sub>i</sub>) , 且距离d<sub>ij</sub>最小的数据点j之间的距离, 其中密度最大值点取</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>j</mi></munder><mo stretchy="false"> (</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">) </mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133"><b>定义2</b> 归一化决策函数</p>
                </div>
                <div class="p1">
                    <p id="134"><mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>ρ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>ρ</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>ρ</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mi>δ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>δ</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>δ</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow></mfrac></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="136">为了寻找最佳聚类中心数, 首先定义一个对局部密度和距离度量函数, 并且为了避免局部密度和距离的分布不均而导致的决策函数受单一变量影响较大的情况发生, 对决策函数进行归一化处理, 对每个变量进行单位缩放, 这里称为决策函数γ.</p>
                </div>
                <div class="p1">
                    <p id="137">图2 (<i>a</i>) 为<i>Example</i>数据集<citation id="316" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>中无决策函数的决策图, 显然, 很难从图上选择符合要求的聚类中心.对决策函数进行降序排列, 重新画出决策图, 如 (<i>b</i>) 所示, 经放大后取其前20个数据点.由聚类中心的2个特性可知, 聚类中心点必然为排序靠前的若干样本点, 因为这些点与其它点的决策函数值具有明显差异.但是, 仅依靠这一幅图, 很难准确确定此数据集有多少个聚类中心点, 因此引入定义3 (聚类样本的拐点) .</p>
                </div>
                <div class="area_img" id="141">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902007_14100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同形式的决策图" src="Detail/GetImg?filename=images/MSSB201902007_14100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 不同形式的决策图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902007_14100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Decision graphs of different types</p>

                </div>
                <div class="p1">
                    <p id="142"><b>定义3</b> 拐点</p>
                </div>
                <div class="p1">
                    <p id="143"><mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ν</mi><mo>=</mo><mfrac><mrow><mi>Δ</mi><mi>m</mi></mrow><mi>m</mi></mfrac></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="145">其中, <i>m</i>为完成聚类后已经归属到各个聚类中心的样本总数, <i>Δm</i>=<i>m</i><sub><i>c</i></sub>-<i>m</i><sub><i>c</i>+1</sub>为聚类样本数的增量, <i>c</i>为聚类中心个数, <i>m</i><sub><i>c</i></sub>为当聚类中心数为<i>c</i>时的<i>m</i>值.</p>
                </div>
                <div class="p1">
                    <p id="146">拐点方法 (Elbow Method) 为聚类分析中的方法, 旨在帮助找到数据集中簇的适当数量<citation id="317" type="reference"><link href="72" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>.该方法通常计算簇内误方差 (Sum of Squares Due to Error, SSE) 作为目标函数划分簇, 不同的簇数有不同的SSE值, 根据SSE的变化规律选择最佳聚类中心数.</p>
                </div>
                <div class="p1">
                    <p id="147">受此种方法的启发, 本文采用的拐点方法是变化幅值<i>N</i>随着聚类中心数<i>c</i>的变化趋势以划分簇.图3为不同聚类中心的聚类效果图.由图3可知, 当未达到最佳聚类中心数5之前, 虽然聚类中心数<i>c</i>在变化, 但是聚类核心点数<i>m</i>变化较小, 即<i>Δm</i>较小, 变化幅值<i>N</i>对应图4中聚类中心数5之前的曲线.当聚类中心数超过最佳聚类中心数5时, 即聚类中心数为6 (<i>c</i>=6) 时, 聚类效果如图3 (d) 所示, 这时聚类核心点数<i>m</i>变化较大, 即<i>Δm</i>较大.与此对应, 如图4所示, 当聚类中心为点6时, 变化幅值<i>N</i>达到最大值, 此时已经跳过最佳聚类中心的数目, 因此最佳聚类中心数取<i>c</i>-1.</p>
                </div>
                <div class="p1">
                    <p id="148">如图3所示, 采用Example数据集<citation id="318" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>进行实验时, 分别选择3、4、5、6个不同的聚类中心数.从结果图上可以看出, 当聚类中心数出现最佳个数之前, 聚类的核心点总数变化很小, 而当出现第6个聚类中心时, 核心点数急剧减少.同时, 第6个的变化幅值<i>N</i>取到最大值, 考虑到数据集的最佳聚类中心个数为5, 因此, 提出变化幅值<i>c</i>-1为数据集中最佳聚类中心数, 计算和统计变化幅值<i>N</i>的变化规律.正如图4所示, 当聚类中心个数为6时<i>N</i>变化最大, 所以, 最佳聚类中心数为5.本文从变化幅值<i>N</i>的趋势验证方法的准确性.</p>
                </div>
                <div class="p1">
                    <p id="149">由图4可以看出, 在选择聚类中心数量时, 变化幅值总是随着聚类中心数量的变化而变化, 因为每确定一个聚类中心数, 聚类结果中的聚类样本数都在不断变化, 当聚类中心个数取6时, 变化幅值最大, 此点即为拐点, 即当聚类中心数大于最佳聚类中心数时, 新出现的聚类中心必然会对已经聚类的数据簇进行分割, 导致原聚类样本向噪声点转变.</p>
                </div>
                <div class="area_img" id="159">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902007_15900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 具有不同聚类中心的聚类效果图" src="Detail/GetImg?filename=images/MSSB201902007_15900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 具有不同聚类中心的聚类效果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902007_15900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Clustering result graphs with different cluster centers</p>

                </div>
                <div class="area_img" id="160">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902007_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 拐点图" src="Detail/GetImg?filename=images/MSSB201902007_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 拐点图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902007_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Elbow map</p>

                </div>
                <div class="p1">
                    <p id="161">总之, 本文算法步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="162"><b>算法2</b> AUTO-CFSFDP</p>
                </div>
                <div class="p1">
                    <p id="163">step 1 从数据文件中读取数据矩阵, 行表示数据样本数, 列表示样本的维度.</p>
                </div>
                <div class="p1">
                    <p id="164">step 2 计算任意两个数据点的距离<i>d</i><sub><i>ij</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="165">step 3 计算截断距离<i>d</i><sub><i>c</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="166">step 4 计算各个数据点的局部密度<i>ρ</i><sub><i>i</i></sub>、距离<i>δ</i><sub><i>i</i></sub>和决策函数<i>γ</i>.</p>
                </div>
                <div class="p1">
                    <p id="167">step 5 排序<i>γ</i>, 记录下标序号.</p>
                </div>
                <div class="p1">
                    <p id="168">step 6 取聚类中心数<i>c</i>=1, 对样本集进行聚类, 并计算<i>N</i><sub><i>c</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="169">step 7 更新聚类个数, <i>c</i>=<i>c</i>+1, 进行聚类, 并计算<i>N</i><sub><i>c</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="170">step 8 循环step 6、step 7, 直至<i>N</i><sub><i>c</i></sub>出现最大峰值.</p>
                </div>
                <div class="p1">
                    <p id="171">step 9 确定最佳聚类中心个数<i>c</i>=<i>c</i>-1, 最后对非中心点数据样本进行归类划分.</p>
                </div>
                <h3 id="172" name="172" class="anchor-tag">3 实验及结果分析</h3>
                <h4 class="anchor-tag" id="173" name="173"><b>3.1</b> 实验环境与实验数据</h4>
                <div class="p1">
                    <p id="174">实验中应用的操作系统是Windows 10, 集成开发环境为Matlab 2014a.硬件条件为:CPU Inter (R) Core (TM) i7-7700, 主频3.6 GHz, 内存8 GB.</p>
                </div>
                <div class="p1">
                    <p id="175">为了验证本文算法的有效性, 针对UCI公共的数据集及人工数据集进行验证, 数据集的具体信息如表1所示.其中, D1为人工数据集, 数据集的特点是簇间密度相差较大.Example数据集为文献<citation id="319" type="reference">[<a class="sup">14</a>]</citation>讨论使用的数据集, 数据样本较大, 分布特征明显.Aggregation、Spiral数据集为Shape set数据集.这4个数据集在聚类效果图上都可以明显验证本文算法的可视化效果.</p>
                </div>
                <div class="area_img" id="176">
                    <p class="img_tit"><b>表1</b><b>样本数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table1 Sample dataset</p>
                    <p class="img_note"></p>
                    <table id="176" border="1"><tr><td><br />数据集</td><td>实例数</td><td>维度</td><td>类数</td><td>来源</td></tr><tr><td><br />Iris</td><td>150</td><td>4</td><td>3</td><td>UCI</td></tr><tr><td><br />Pima Indian <br />Diabetes (Pima) </td><td>768</td><td>8</td><td>2</td><td>UCI</td></tr><tr><td><br />Seed</td><td>210</td><td>7</td><td>3</td><td>UCI</td></tr><tr><td><br />Aggregation</td><td>788</td><td>2</td><td>7</td><td>Shape set<sup>[31]</sup></td></tr><tr><td><br />Spiral</td><td>314</td><td>2</td><td>3</td><td>Shape set<sup>[32]</sup></td></tr><tr><td><br />D1</td><td>313</td><td>2</td><td>3</td><td>人工数据集</td></tr><tr><td><br />Example</td><td>2000</td><td>-</td><td>5</td><td>文献[14]</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="177">采用多种算法进行实验对比, 对比算法包括DBSCAN (Density-Based Spatial Clustering of Appli-cations with Noise) , <i>K</i>-means, GDP (Geodesic Density Peaks Clustering) , B-DPC, CFSFDP, 模糊<i>C</i>均值 (Fuzzy c-means, FCM) <citation id="321" type="reference"><link href="78" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>.不同算法在各个数据集上选取参数如表2所示.DBSCAN的<i>Minpts</i>、<i>Eps</i>分别在确定正确聚类个数的前提下, 寻找效果较好的值.<i>K</i>-means的<i>K</i>为正确的聚类个数, <i>T</i>为设定的迭代次数.CFSFDP与AUTO-CFSFDP均采用相同的截断距离.B-DPC<citation id="320" type="reference"><link href="70" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>、GDP<citation id="322" type="reference"><link href="64" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>为基于CFSFDP的改进算法.</p>
                </div>
                <div class="area_img" id="179">
                    <p class="img_tit"><b>表2</b><b>不同算法在不同数据集上的参数取值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Parameter values of different algorithms on different datasets</p>
                    <p class="img_note"></p>
                    <table id="179" border="1"><tr><td rowspan="2">算法</td><td rowspan="2">参数</td><td><br />数据集</td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td></tr><tr><td><br />Iris</td><td>Pima</td><td>Seed</td><td>Aggregation</td><td>Spiral</td></tr><tr><td>DBSCAN</td><td><i>Minpts<br />Eps</i></td><td>4<br />0.5</td><td>7<br />22</td><td>7<br />0.8</td><td>15<br />2</td><td>5<br />3</td></tr><tr><td><br /><i>K</i>-means</td><td><i>K<br />T</i></td><td>3<br />50</td><td>2<br />50</td><td>3<br />50</td><td>7<br />50</td><td>3<br />50</td></tr><tr><td><br />GDP</td><td><i>d</i><sub><i>c</i></sub>/%</td><td>2</td><td>0.4</td><td>2</td><td>2</td><td>3</td></tr><tr><td><br />B-DPC</td><td><i>d</i><sub><i>c</i></sub>/%</td><td>2</td><td>0.4</td><td>2</td><td>2</td><td>3</td></tr><tr><td><br />CFSFDP</td><td><i>d</i><sub><i>c</i></sub>/%</td><td>2</td><td>0.4</td><td>2</td><td>2</td><td>3</td></tr><tr><td><br />AUTO-<br />CFSFDP</td><td><i>d</i><sub><i>c</i></sub>/%</td><td>2</td><td>0.4</td><td>2</td><td>2</td><td>3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="180" name="180"><b>3.2</b> 聚类数量分析</h4>
                <div class="p1">
                    <p id="181">本文算法在聚类中心数量递增的同时, 变化幅值也随之发生改变, 因为不同的聚类中心个数和位置对样本数据点有不同分类, 但在最佳聚类结果时, 会对数据集有最充分地聚类, 超过数据集的最佳聚类中心, 聚类分配数据点会有明显改变.</p>
                </div>
                <div class="p1">
                    <p id="182">图5为本文算法在不同数据集上的拐点图.如图所示, Example数据集的最佳聚类中心个数为5 (具体图参见图4) , D1数据集的最佳聚类中心个数为3, Aggregation数据集的最佳聚类中心个数为7, Spiral数据集的最佳聚类中心个数为3.</p>
                </div>
                <div class="area_img" id="283">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902007_28300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 本文算法在各数据集的拐点图" src="Detail/GetImg?filename=images/MSSB201902007_28300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 本文算法在各数据集的拐点图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902007_28300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Elbow maps of the proposed algorithm on different datasets</p>

                </div>
                <div class="p1">
                    <p id="189">图6为本文算法在各个数据集上的聚类结果图, 分别对应4个数据集上的各个可视化聚类结果.从图4～图6可以看出, 本文算法在这些数据集上都取得良好的聚类效果, 从而验证本文算法的有效性.</p>
                </div>
                <div class="p1">
                    <p id="190">AUTO-CFSFDP的核心是正序迭代选择策略, 即自动选择聚类中心策略, 通过选择排序后的决策函数靠前的样本点作为聚类中心, 借助聚类核心点数的变化幅值拐点判断最佳聚类中心个数及位置, 完成聚类.因此, 能否确定数据集的聚类中心个数就是对AUTO-CFSFDP最好的验证.</p>
                </div>
                <div class="area_img" id="197">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902007_19700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 本文算法在各数据集上的聚类结果" src="Detail/GetImg?filename=images/MSSB201902007_19700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 本文算法在各数据集上的聚类结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902007_19700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Clustering results of the proposed algorithm on different datasets</p>

                </div>
                <div class="p1">
                    <p id="199">表3为AUTO-CFSFDP聚类之后, 取得的最佳聚类中心数, 对比原聚类中心数可以看出, 本文算法在确定聚类中心数方面都有正确的聚类结果, 从而验证本文算法的准确性.</p>
                </div>
                <div class="area_img" id="200">
                    <p class="img_tit"><b>表3</b><b>本文算法在各数据集上的最佳聚类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Clustering results of the proposed algorithm on different datasets</p>
                    <p class="img_note"></p>
                    <table id="200" border="1"><tr><td><br />数据集名称</td><td>类别数</td><td>聚类结果</td></tr><tr><td><br />Iris</td><td>3</td><td>3</td></tr><tr><td><br />Pima</td><td>2</td><td>2</td></tr><tr><td><br />Seed</td><td>3</td><td>3</td></tr><tr><td><br />Aggregation</td><td>7</td><td>7</td></tr><tr><td><br />Spiral</td><td>3</td><td>3</td></tr><tr><td><br />D1</td><td>3</td><td>3</td></tr><tr><td><br />Example</td><td>5</td><td>5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="201" name="201"><b>3.3</b> 聚类结果分析</h4>
                <div class="p1">
                    <p id="202">不失一般性, 在验证AUTO-CFSFDP的准确性之后, 再与其它算法进行性能对比.</p>
                </div>
                <div class="p1">
                    <p id="203">表1中Iris、Pima、Seed、Aggregation、Spiral数据集均来自于UCI公开数据集, 是专门检测聚类算法性能的常用数据集, 这5个数据集样本点都有准确的分类, 因此可以通过聚类结果和标准分类以精确计算本文算法的评价结果.具体评价指标如下.</p>
                </div>
                <h4 class="anchor-tag" id="204" name="204">1) 平均纯度 (Purity) </h4>
                <div class="p1">
                    <p id="205"><mathml id="206"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mfrac><mrow><mo stretchy="false">|</mo><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mrow><mstyle displaystyle="true"><mo>∩</mo><mrow></mrow></mstyle></mrow><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo></mrow><mrow><mi>Κ</mi><mo stretchy="false">|</mo><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></mfrac></mrow></mstyle></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="207">其中, <i>P</i><sub><i>i</i></sub>为数据集中某一类已知的样本总数, <i>C</i><sub><i>i</i></sub>为聚类结果对应某一类的样本总数, <i>K</i>为样本数据集的聚类数.</p>
                </div>
                <h4 class="anchor-tag" id="208" name="208">2) 召回率 (Recall) </h4>
                <div class="p1">
                    <p id="209"><mathml id="210"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mfrac><mrow><mo stretchy="false">|</mo><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mstyle displaystyle="true"><mo>∩</mo><mi>C</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo></mrow><mrow><mi>Κ</mi><mo stretchy="false">|</mo><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></mfrac></mrow></mstyle></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="211">其中, <i>P</i><sub><i>i</i></sub>为数据集某一类已知的样本总数, <i>C</i><sub><i>i</i></sub>为算法聚类结果对应某一类的样本总数, <i>K</i>为样本数据集的聚类数.</p>
                </div>
                <h4 class="anchor-tag" id="212" name="212">3) F-measure</h4>
                <div class="p1">
                    <p id="213"><mathml id="214"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>β</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo>⋅</mo><mi>Ρ</mi><mo>⋅</mo><mi>R</mi></mrow><mrow><mi>β</mi><msup><mrow></mrow><mn>2</mn></msup><mo>⋅</mo><mi>Ρ</mi><mo>+</mo><mi>R</mi></mrow></mfrac></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="215">其中, <i>P</i>为平均纯度值, <i>R</i>为召回率, <i>β</i>=1.</p>
                </div>
                <div class="p1">
                    <p id="216">平均纯度为衡量聚类算法的常用指标之一, 其值等于各类与真实类的交集与各个类数总数的比值之和, 反映聚类算法结果中正确的聚类比值.平均纯度值越大, 表示聚类算法的效果就越好.各算法在UCI数据集上聚类的平均纯度结果对比如表4所示.由表可以看出, 本文算法可以自动确定聚类中心.相比DBSCAN、<i>K</i>-means、CFSFDP, 在不同的数据集上, 本文算法都有相对更高的纯度值, 说明聚类效果更好.</p>
                </div>
                <div class="area_img" id="217">
                    <p class="img_tit"><b>表4</b><b>7种算法在不同数据集上的平均纯度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Purity comparison of 7 algorithms on different datasets</p>
                    <p class="img_note"></p>
                    <table id="217" border="1"><tr><td rowspan="2"><br />算法</td><td><br />数据集</td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td></tr><tr><td><br />Iris</td><td>Pima</td><td>Seed</td><td>Aggregation</td><td>Spiral</td></tr><tr><td><br />DBSCAN</td><td>0.491</td><td>0.622</td><td>0.485</td><td>0.988</td><td>1</td></tr><tr><td><br /><i>K</i>-means</td><td>0.898</td><td>0.614</td><td>0.900</td><td>0.768</td><td>0.333</td></tr><tr><td><br />CFSFDP</td><td>0.927</td><td>0.506</td><td>0.836</td><td>1</td><td>1</td></tr><tr><td><br />FCM</td><td>0.828</td><td>0.66</td><td>0.729</td><td>0.633</td><td>0.339</td></tr><tr><td><br />GDP</td><td>0.591</td><td>0.771</td><td>0.684</td><td>1</td><td>1</td></tr><tr><td><br />B-DPC</td><td>0.906</td><td>0.478</td><td>0.904</td><td>0.999</td><td>1</td></tr><tr><td><br />AUTO-CFSFDP</td><td>0.927</td><td>0.517</td><td>0.937</td><td>1</td><td>1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="218">召回率为衡量聚类算法性能的重要指标之一, 其值是聚类算法的聚类结果和有标签的数据集的对应标签集的交集与标签集的平均比值, 召回率的值从某种程度上反映聚类算法的性能好坏, 召回率越大, 说明聚类算法对数据集的聚类效果越好, 对正确的标签数据集的利用率越高.</p>
                </div>
                <div class="p1">
                    <p id="219">表5为各算法在UCI数据集上的召回率对比.由表所示, AUTO-CFSFDP在自动确定聚类中心的同时, 也能保证召回率具有较好结果.相比其它常用聚类算法, 在Iris、Spiral数据集上, 本文算法具有最好的召回率, 说明聚类效果最好.虽然在其它3个数据集上没有表现最高的召回率, 但是聚类效果也接近最佳成绩.</p>
                </div>
                <div class="area_img" id="220">
                    <p class="img_tit"><b>表5</b><b>7种算法在不同数据集上的召回率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Recall comparison of 7 algorithms on different datasets</p>
                    <p class="img_note"></p>
                    <table id="220" border="1"><tr><td rowspan="2"><br />算法</td><td><br />数据集</td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td></tr><tr><td><br />Iris</td><td>Pima</td><td>Seed</td><td>Aggregation</td><td>Spiral</td></tr><tr><td><br />DBSCAN</td><td>0.62</td><td>0.416</td><td>0.4</td><td>0.993</td><td>1</td></tr><tr><td><br /><i>K</i>-means</td><td>0.887</td><td>0.586</td><td>0.895</td><td>0.774</td><td>0.333</td></tr><tr><td><br />CFSFDP</td><td>0.907</td><td>0.507</td><td>0.590</td><td>0.884</td><td>1</td></tr><tr><td><br />FCM</td><td>0.850</td><td>0.744</td><td>0.838</td><td>0.883</td><td>0.554</td></tr><tr><td><br />GDP</td><td>0.724</td><td>0.426</td><td>0.572</td><td>1</td><td>1</td></tr><tr><td><br />B-DPC</td><td>0.900</td><td>0.464</td><td>0.904</td><td>0.999</td><td>0.998</td></tr><tr><td><br />AUTO-CFSFDP</td><td>0.907</td><td>0.518</td><td>0.79</td><td>0.922</td><td>1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="221">F-measure值为衡量聚类算法效果的最常用的指标之一, 其值等于平均纯度和召回率的加权调和平均.F-measure值越大, 反映聚类算法的分类效果越好.</p>
                </div>
                <div class="p1">
                    <p id="222">表6为各算法在各个数据集上的F-measure值对比.由表可以看出, 在不同数据集上, 本文算法都具有不错的聚类效果.在Iris、Spiral数据集上, 具有最好的聚类效果.在其它数据集上, 也有近似最好的聚类结果.</p>
                </div>
                <div class="area_img" id="223">
                    <p class="img_tit"><b>表6</b><b>7种算法在不同数据集上的F-measure值对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 6 F-measure comparison of 7 algorithms on different datasets</p>
                    <p class="img_note"></p>
                    <table id="223" border="1"><tr><td rowspan="2"><br />算法</td><td><br />数据集</td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td></tr><tr><td><br />Iris</td><td>Pima</td><td>Seed</td><td>Aggregation</td><td>Spiral</td></tr><tr><td><br />DBSCAN</td><td>0.548</td><td>0.499</td><td>0.438</td><td>0.990</td><td>1</td></tr><tr><td><br /><i>K</i>-means</td><td>0.892</td><td>0.600</td><td>0.897</td><td>0.771</td><td>0.333</td></tr><tr><td><br />CFSFDP</td><td>0.917</td><td>0.506</td><td>0.692</td><td>0.934</td><td>1</td></tr><tr><td><br />FCM</td><td>0.839</td><td>0.699</td><td>0.778</td><td>0.737</td><td>0.421</td></tr><tr><td><br />GDP</td><td>0.651</td><td>0.563</td><td>0.623</td><td>1</td><td>1</td></tr><tr><td><br />B-DPC</td><td>0.903</td><td>0.471</td><td>0.904</td><td>0.999</td><td>1</td></tr><tr><td><br />AUTO-CFSFDP</td><td>0.917</td><td>0.517</td><td>0.857</td><td>0.959</td><td>1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="224" name="224"><b>3.4</b> 时间复杂度分析</h4>
                <div class="p1">
                    <p id="225">设实验数据集的样本数为<i>n</i>, 根据CFSFDP的步骤描述可知, CFSFDP计算样本任意两点的相似度时间复杂度为<i>O</i> (<i>n</i><sup>2</sup>) , 计算局部密度<i>ρ</i>的时间复杂度为<i>O</i> (<i>n</i><sup>2</sup>) , 计算距离<i>δ</i>的时间复杂度为<i>O</i> (<i>n</i><sup>2</sup>) , 因此, CFSFDP的总时间复杂度为<i>O</i> (<i>n</i><sup>2</sup>) .AUTO- CFSFDP计算样本任意两点的相似度时间复杂度为<i>O</i> (<i>n</i><sup>2</sup>) , 计算局部密度<i>ρ</i>的时间复杂度为<i>O</i> (<i>n</i><sup>2</sup>) , 计算距离<i>δ</i>的时间复杂度为<i>O</i> (<i>n</i><sup>2</sup>) , 遍历聚类中心个数及分配非中心点的时间复杂度为<i>O</i> (<i>n</i><sup>2</sup>) , 因此, AUTO-CFSFDP的总时间复杂度为<i>O</i> (<i>n</i><sup>2</sup>) .综上所述, AUTO-CFSFDP最终的计算复杂度与CFSFDP一样, 都为<i>O</i> (<i>n</i><sup>2</sup>) .</p>
                </div>
                <div class="p1">
                    <p id="226">图7为5种算法的运行时间对比, CFSFDP是在已知聚类中心个数的前提下进行.</p>
                </div>
                <div class="area_img" id="227">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902007_227.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 5种算法在各数据集上运行时间对比" src="Detail/GetImg?filename=images/MSSB201902007_227.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 5种算法在各数据集上运行时间对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902007_227.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Comparison of running time of 5 algorithms on different datasets</p>

                </div>
                <div class="p1">
                    <p id="229">如图7所示, AUTO-CFSFDP运行时间少于其它算法, 在保证聚类准确性的前提下, 提高算法的运行效率.综上所述, AUTO-CFSFDP在保证聚类准确性和计算复杂度不变的情况下, 相比DBSCAN、<i>K</i>-means、CFSFDP, 在聚类平均纯度、召回率、F-measure评价标准上, 都具有更好的聚类效果.</p>
                </div>
                <h3 id="230" name="230" class="anchor-tag">4 结 束 语</h3>
                <div class="p1">
                    <p id="231">本文提出自动确定聚类中心的密度峰值聚类算法, 解决CFSFDP不能自动确定聚类中心、聚类过程受主观因素和经验影响、可能发生聚类效果不理想的问题.针对CFSFDP的决策函数2个变量分布不均匀的问题, 提出归一化决策函数, 避免聚类中心选择时, 某个制约因子的单位过大, 导致聚类中心选择出现误差问题的发生.而且, 针对CFSFDP在选择聚类中心时严重依赖经验的问题, 提出正序迭代选择策略, 可以有效避免主观因素导致的聚类结果不准确问题的发生.实验验证本文算法的可行性和有效性.如何降低算法的时间复杂度和参数<i>d</i><sub><i>c</i></sub>的选择性问题是今后研究工作的重点.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="14">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300029345&amp;v=MDI0NDJ3WGFSVT1OaWZPZmJLOEh0TE9ySTlGWk9rR0QzZzhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> CHEN C L P, ZHANG C Y. Data-Intensive Applications, Challenges, Techniques and Technologies: A Survey on Big Data. Information Sciences, 2014, 275: 314-347.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data Mining in Distributed Environment: A Survey">

                                <b>[2]</b> GAN W S, LIN J C W, CHAO H C, <i>et al</i>. Data Mining in Distributed Environment: A Survey. Data Mining and Knowledge Discovery, 2017, 7: e1216.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Comprehensive Survey of Clustering Algorithms">

                                <b>[3]</b> XU D K, TIAN Y J. A Comprehensive Survey of Clustering Algorithms. Annals of Data Science, 2015, 2 (2) : 165-193.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Algorithms for Clustering Data">

                                <b>[4]</b> JAIN A K, DUBES R C. Algorithms for Clustering Data. New York, USA: Prentice Hall, 1988.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787040442557000&amp;v=MDkzNDd6R2JPOEh0WElyWXBBWStzUERCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2c1U3dktJMTBUWEZx&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 王万良.人工智能及其应用.第3版.高等教育出版社, 2016. (WANG W L. Artificial Intelligence and Application. 3rd Edition. Beijing, China: Higher Education Press, 2016) 
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Energy-Balanced Clustering Protocol Based on an Improved CFSFDP Algorithm for Wireless Sensor Networks">

                                <b>[6]</b> ZHANG Y M, LIU M D, LIU Q W. An Energy-Balanced Clustering Protocol Based on an Improved CFSFDP Algorithm for Wireless Sensor Networks. Sensors, 2018, 18 (3) . DOI: 10.3390/s18030881.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering">

                                <b>[7]</b> ALTMAN N, KRZYWINSKI M. Points of Significance: Clustering. Nature Methods, 2017, 14 (6) : 545-546.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Terahertz time-domain spectroscopy combined with PCA-CFSFDP applied for pesticide detection">

                                <b>[8]</b> QIN B Y, LI Z, LUO Z H, <i>et al</i>. Terahertz Time-Domain Spectroscopy Combined with PCA-CFSFDP Applied for Pesticide Detection. Optical &amp; Quantum Electronics, 2017, 49 (7) . DOI: 10.1007/s11082-017-1080-x.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201805003&amp;v=MDczNzN5em5VTHpQS0Q3WWJMRzRIOW5NcW85Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 郑建炜, 路程, 秦梦洁, 等.联合特征选择和光滑表示的子空间聚类算法.模式识别与人工智能, 2018, 31 (5) : 409-418. (ZHENG J W, LU C, QIN M J, <i>et al</i>. Subspace Clustering via Joint Feature Selection and Smooth Representation. Pattern Recognition and Artificial Intelligence, 2018, 31 (5) : 409-418.) 
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201803008&amp;v=MTgzNjk5bk1ySTlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6blVMelBLRDdZYkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 逯瑞强, 马福民, 张腾飞.基于区间2-型模糊度量的粗糙<i>K</i>-means聚类算法.模式识别与人工智能, 2018, 31 (3) : 265-274. (LU R Q, MA F M, ZHANG T F. Interval Type-2 Fuzzy Measure Based Rough <i>K</i>-means Clustering. Pattern Recognition and Artificial Intelligence, 2018, 31 (3) : 265-274.) 
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200807014&amp;v=MzIwMzR6blVMelBOeWZUYkxHNEh0bk1xSTlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 雷小锋, 谢昆青, 林帆, 等.一种基于<i>K</i>-means局部最优性的高效聚类算法.软件学报, 2008, 19 (7) : 1683-1692. (LEI X F, XIE K Q, LIN F, <i>et al</i>. An Efficient Clustering Algorithm Based on Local Optimality of <i>K</i>-means. Journal of Software, 2008, 19 (7) : 1683-1692.) 
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002157106&amp;v=MDk1MjN0RkNIbFY3M0xJMWs9Tmo3QmFyTzRIdEhPcm9wQ1plc0pZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVk&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> ZHANG T, RAMAKRISHNAN R, LIVNY M. BIRCH: A New Data Clustering Algorithm and Its Applications. Data Mining and Knowledge Discovery, 1997, 1 (2) : 141-182.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100892160&amp;v=MjEzOTBkR2VycVFUTW53WmVadUh5am1VTGJJSkZ3WGFSVT1OaWZPZmJLN0h0RE9ybzlGYk9JTkRYbzVvQk1UNlQ0UFFIL2lyUg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> GUHA S, RASTOGI R, SHIM K. CURE: An Efficient Clustering Algorithm for Large Database. Information Systems, 2001, 26 (1) : 35-58.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 RODRIGUEZ A, LAIO A. Clustering by Fast Search and Find of Density Peaks. Science, 2014, 344 (6191) : 1492-1496.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise">

                                <b>[15]</b> ESTER M, KRIEGEL H P, XU X. A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise // Proc of the International Conference on Knowledge Discovery and Data Mining. Palo Alto, USA: AAAI Press, 1996: 226-231.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust clustering by detecting density peaks and assigning points based on fuzzy weighted K-nearest neighbors">

                                <b>[16]</b> XIE J Y, GAO H C, XIE W X, <i>et al</i>. Robust Clustering by Detecting Density Peaks and Assigning Points Based on Fuzzy Weighted K-nearest Neighbors. Information Sciences, 2016, 354: 19-40.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive cutoff distance:Clustering by fast search and find of density peaks">

                                <b>[17]</b> MEHMOOD R, BIE R, JIAO L B, <i>et al</i>. Adaptive Cutoff Distance: Clustering by Fast Search and Find of Density Peaks. Journal of Intelligent and Fuzzy Systems, 2016, 31 (5) : 2619-2628.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=STING: A statistical information grid approach to spatial data mining">

                                <b>[18]</b> WANG W, YANG J, MUNTZ R R. STING: A Statistical Information Grid Approach to Spatial Data Mining // Proc of the International Conference on Very Large Data Bases. San Francisco, USA: Morgan Kaufmann Publisher, 1997: 186-195.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000068877&amp;v=MDU5NTdycVFUTW53WmVadUh5am1VTGJJSkZ3WGFSVT1OaWZJWTdLN0h0ak5yNDlGWk8wSEJIcytvQk1UNlQ0UFFIL2lyUmRHZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> AGRAWAL R, GEHRKE J, GUNOPULOS D, <i>et al</i>. Automatic Subspace Clustering of High Dimensional Data for Data Mining Applications. ACM SIGMOD Record, 1998, 27 (2) : 94-105.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201705006&amp;v=MjI2NTE0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpuVUx6UEtEN1liTEc0SDliTXFvOUZZb1FLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> 朱杰, 陈黎飞.核密度估计的聚类算法.模式识别与人工智能, 2017, 30 (5) : 439-447. (ZHU J, CHEN L F. Clustering Algorithm with Kernel Density Estimation. Pattern Recognition and Artificial Intelligence, 2017, 30 (5) : 439-447.) 
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data Mining: Concepts and Techniques">

                                <b>[21]</b> HAN J W, KAMBER M, PEI J. Data Mining: Concepts and Techniques. New York, USA: Elsevier, 2011.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive Fuzzy Clustering by Fast Search and Find of Density Peaks">

                                <b>[22]</b> BIE R F, MEHMOOD R, RUAN S S, <i>et al</i>. Adaptive Fuzzy Clustering by Fast Search and Find of Density Peaks. Personal and Ubiquitous Computing, 2016, 20 (5) : 785-793.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fuzzy Clustering by Fast Search and Find of Density Peaks">

                                <b>[23]</b> MEHMOOD R, BIE R F, DAWOOD H, <i>et al</i>. Fuzzy Clustering by Fast Search and Find of Density Peaks // Proc of the International Conference on Identification, Information, and Knowledge in the Internet of Things. Washington, USA: IEEE, 2015: 258-261.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic Cluster Number Selection by Finding Density Peaks">

                                <b>[24]</b> WANG J L, ZHANG Y, LAN X. Automatic Cluster Number Selection by Finding Density Peaks // Proc of the 2nd IEEE Internatio-nal Conference on Computer and Communications. Washington, USA: IEEE, 2016: 13-18.
                            </a>
                        </p>
                        <p id="62">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering by Finding Density Peaks Based on Chebyshev&amp;#39;&amp;#39;s Inequality">

                                <b>[25]</b> DING J J, CHEN Z T, HE X X, <i>et al</i>. Clustering by Finding Density Peaks Based on Chebyshev's Inequality // Proc of the 35th Chinese Control Conference. Washington, USA: IEEE, 2016: 7169-7172.
                            </a>
                        </p>
                        <p id="64">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Manifold density peaks clustering algorithm">

                                <b>[26]</b> XU X H, JU Y S, LIANG Y L, <i>et al</i>. Manifold Density Peaks Clustering Algorithm // Proc of the 3rd International Conference on Advanced Cloud and Big Data. Washington, USA: IEEE, 2015: 311-318.
                            </a>
                        </p>
                        <p id="66">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A distance and density-based clustering algorithm using automatic peak detection">

                                <b>[27]</b> ZHOU R, ZHANG S, CHEN C, <i>et al</i>. A Distance and Density-Based Clustering Algorithm Using Automatic Peak Detection // Proc of the IEEE International Conference on Smart Cloud. Wa-shington, USA: IEEE, 2016: 176-183.
                            </a>
                        </p>
                        <p id="68">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZNXT201702013&amp;v=MTc4NDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5em5VTHpQUHlQVGVyRzRIOWJNclk5RVo0UUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b> 淦文燕, 刘冲.一种改进的搜索密度峰值的聚类算法.智能系统学报, 2017, 12 (2) : 229-236. (GAN W Y, LIU C. An Improved Clustering Algorithm that Searches and Finds Density Peaks. CAAI Transactions on Intelligent Systems, 2017, 12 (2) : 229-236) .
                            </a>
                        </p>
                        <p id="70">
                            <a id="bibliography_29" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJDZ201702019&amp;v=MTA2NzBSbkZ5em5VTHpQS3lmUGRMRzRIOWJNclk5RWJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[29]</b> 贾培灵, 樊建聪, 彭延军.一种基于簇边界的密度峰值点快速搜索聚类算法.南京大学学报 (自然科学) , 2017, 53 (2) : 368-377. (JIA P L, FAN J C, PENG Y J. An Improved Clustering Algorithm by Fast Search and Find of Density Peaks Based on Boundary Samples. Journal of Nanjing University (Natural Sciences) , 2017, 53 (2) : 368-377.) 
                            </a>
                        </p>
                        <p id="72">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Introduction to Data Mining">

                                <b>[30]</b> RAGHAVAN V V, DEOGUN J S, SEVER H. Introduction to Data Mining. New York, USA: John Wiley &amp; Sons, 1998.
                            </a>
                        </p>
                        <p id="74">
                            <a id="bibliography_31" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000093142&amp;v=MDAwNzBmSVk3SzdIdGpOcjQ5RlpPSU1EWGc3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZ3WGFSVT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[31]</b> GIONIS A, MANNILA H, TSAPARAS P. Clustering Aggregation. ACM Transactions on Knowledge Discovery from Data, 2007, 1 (1) . DOI: 10.1145/1217299.1217303.
                            </a>
                        </p>
                        <p id="76">
                            <a id="bibliography_32" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739343&amp;v=MDkzNjZadUh5am1VTGJJSkZ3WGFSVT1OaWZPZmJLN0h0RE5xWTlGWStnR0QzZzZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[32]</b> CHANG H, YEUNG D Y. Robust Path-Based Spectral Clustering. Pattern Recognition, 2008, 41 (1) : 191-203.
                            </a>
                        </p>
                        <p id="78">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Possibilistic Fuzzy c-Means Clustering Algorithm">

                                <b>[33]</b> PAL N R, PAL K, KELLER J M, <i>et al</i>. A Possibilistic Fuzzy <i>c</i>-means Clustering Algorithm. IEEE Transactions on Fuzzy Systems, 2005, 13 (4) : 517-530.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201902007" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201902007&amp;v=MDc2MTN5em5VTHpQS0Q3WWJMRzRIOWpNclk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
