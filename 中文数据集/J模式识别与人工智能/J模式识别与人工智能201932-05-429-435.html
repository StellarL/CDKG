<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131448573623750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201905006%26RESULT%3d1%26SIGN%3dnVG1wGm%252bmFPui%252bLpg8QF%252fcwiV8g%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201905006&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201905006&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201905006&amp;v=MjM0NzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpnVmIzQUtEN1liTEc0SDlqTXFvOUZZb1E=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#73" data-title="1 基于密集网络及误差相似插值的遥感图像时空融合算法 ">1 基于密集网络及误差相似插值的遥感图像时空融合算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#80" data-title="&lt;b&gt;1.1 基于多输入的密集连接网络&lt;/b&gt;"><b>1.1 基于多输入的密集连接网络</b></a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;1.2 基于误差相似的插值重建&lt;/b&gt;"><b>1.2 基于误差相似的插值重建</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#129" data-title="2 实验及结果分析 ">2 实验及结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#160" data-title="3 结 束 语 ">3 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#75" data-title="图1 Landsat和MODIS遥感影像时空融合问题">图1 Landsat和MODIS遥感影像时空融合问题</a></li>
                                                <li><a href="#78" data-title="图2 融合算法的整体处理流程图">图2 融合算法的整体处理流程图</a></li>
                                                <li><a href="#82" data-title="图3 多输入密集连接神经网络">图3 多输入密集连接神经网络</a></li>
                                                <li><a href="#89" data-title="图4 密集连接块">图4 密集连接块</a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;表1 Landsat卫星和MODIS卫星的光谱范围&lt;/b&gt;"><b>表1 Landsat卫星和MODIS卫星的光谱范围</b></a></li>
                                                <li><a href="#138" data-title="图5 近红外-红色-绿色三波段组合的MODIS影像和Landsat 影像">图5 近红外-红色-绿色三波段组合的MODIS影像和Landsat 影像</a></li>
                                                <li><a href="#150" data-title="&lt;b&gt;表2 4种算法融合效果在不同波段的定量对比&lt;/b&gt;"><b>表2 4种算法融合效果在不同波段的定量对比</b></a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;表3 4种算法融合效果的综合定量对比&lt;/b&gt;"><b>表3 4种算法融合效果的综合定量对比</b></a></li>
                                                <li><a href="#158" data-title="图6 4种算法对真实图像的预测结果">图6 4种算法对真实图像的预测结果</a></li>
                                                <li><a href="#158" data-title="图6 4种算法对真实图像的预测结果">图6 4种算法对真实图像的预测结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" GAO F, MASEK J, SCHWALLER M, &lt;i&gt;et al&lt;/i&gt;.On the Blending of the Landsat and MODIS Surface Reflectance:Predicting Daily Landsat Surface Reflectance.IEEE Transactions on Geoscience and Remote Sensing, 2006, 44 (8) :2207-2218." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the blending of the landsat and MODIS surface reflectance: Predicting daily landsat surface reflectance">
                                        <b>[1]</b>
                                         GAO F, MASEK J, SCHWALLER M, &lt;i&gt;et al&lt;/i&gt;.On the Blending of the Landsat and MODIS Surface Reflectance:Predicting Daily Landsat Surface Reflectance.IEEE Transactions on Geoscience and Remote Sensing, 2006, 44 (8) :2207-2218.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" ZHU X L, CHEN J, GAO F, &lt;i&gt;et al&lt;/i&gt;.An Enhanced Spatial and Temporal Adaptive Reflectance Fusion Model for Complex Heterogeneous Regions.Remote Sensing of Environment, 2010, 114 (11) :2610-2623." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600662630&amp;v=MjYxMjJETnFZOUZZdTBOQ244NW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGc1NieE09TmlmT2ZiSzdIdA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         ZHU X L, CHEN J, GAO F, &lt;i&gt;et al&lt;/i&gt;.An Enhanced Spatial and Temporal Adaptive Reflectance Fusion Model for Complex Heterogeneous Regions.Remote Sensing of Environment, 2010, 114 (11) :2610-2623.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" HUANG B, SONG H H.Spatiotemporal Reflectance Fusion via Spa-rse Representation.IEEE Transactions on Geoscience and Remote Sensing, 2012, 50 (10) :3707-3716." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatiotemporal reflectance fusion via sparse representation">
                                        <b>[3]</b>
                                         HUANG B, SONG H H.Spatiotemporal Reflectance Fusion via Spa-rse Representation.IEEE Transactions on Geoscience and Remote Sensing, 2012, 50 (10) :3707-3716.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" WU B, HUANG B, ZHANG L P.An Error-Bound-Regularized Spa-rse Coding for Spatiotemporal Reflectance Fusion.IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (12) :6791-6803." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An error-bound-regularized sparse coding for spatiotemporal reflectance fusion">
                                        <b>[4]</b>
                                         WU B, HUANG B, ZHANG L P.An Error-Bound-Regularized Spa-rse Coding for Spatiotemporal Reflectance Fusion.IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (12) :6791-6803.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" KRIZHEVSKY A, SUTKEVER I, HINTON G E.ImageNet Classification with Deep Convolutional Neural Networks.Communications of the ACM, 2017, 60 (6) :84-90." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMD71033EF8EE149F894A1F173A0F662B0&amp;v=MjUzMjN3dVI0YTdrNThQbjdscjJNMUQ3U1NSOGlmQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0wyNXhhaz1OaWZJWThlL0g5SFByUG96Yko1NkRYZw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         KRIZHEVSKY A, SUTKEVER I, HINTON G E.ImageNet Classification with Deep Convolutional Neural Networks.Communications of the ACM, 2017, 60 (6) :84-90.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" HE K M, ZHANG X Y, REN S Q, &lt;i&gt;et al&lt;/i&gt;.Deep Residual Learning for Image Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[6]</b>
                                         HE K M, ZHANG X Y, REN S Q, &lt;i&gt;et al&lt;/i&gt;.Deep Residual Learning for Image Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:770-778.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" GIRSHICK R, DONAHUE J, DARRELL T, &lt;i&gt;et al&lt;/i&gt;.Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[7]</b>
                                         GIRSHICK R, DONAHUE J, DARRELL T, &lt;i&gt;et al&lt;/i&gt;.Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:580-587.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" LONG J, SHELHAMER E, DARRELL T.Fully Convolutional Networks for Semantic Segmentation.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[8]</b>
                                         LONG J, SHELHAMER E, DARRELL T.Fully Convolutional Networks for Semantic Segmentation.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" LIN M, CHEN Q, YAN S C.Network in Network[C/OL].[2019-01-15].https://arxiv.org/pdf/1312.4400.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Network in Network[C/OL]">
                                        <b>[9]</b>
                                         LIN M, CHEN Q, YAN S C.Network in Network[C/OL].[2019-01-15].https://arxiv.org/pdf/1312.4400.pdf.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" SZEGEDY C, LIU W, JIA Y Q, &lt;i&gt;et al&lt;/i&gt;.Going Deeper with Convolutions // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015.DOI:10.1109/CVPR.2015.7298594." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going Deeper with Convolutions">
                                        <b>[10]</b>
                                         SZEGEDY C, LIU W, JIA Y Q, &lt;i&gt;et al&lt;/i&gt;.Going Deeper with Convolutions // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015.DOI:10.1109/CVPR.2015.7298594.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     DONG C, LOY C C, HE K M, &lt;i&gt;et al&lt;/i&gt;.Image Super-Resolution Using Deep Convolutional Networks.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (2) :295-307.</a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" DONG C, LOY C C, TANG X O.Accelerating the Super-Resolution Convolutional Neural Network[C/OL].[2019-01-15].https://arxiv.org/pdf/1608.00367.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accelerating the Super-Resolution Convolutional Neural Network[C/OL]">
                                        <b>[12]</b>
                                         DONG C, LOY C C, TANG X O.Accelerating the Super-Resolution Convolutional Neural Network[C/OL].[2019-01-15].https://arxiv.org/pdf/1608.00367.pdf.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" KIM J, LEE J K, LEE K M.Accurate Image Super-Resolution Using Very Deep Convolutional Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:1646-1654." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate image super-resolution using very deep convolutional networks">
                                        <b>[13]</b>
                                         KIM J, LEE J K, LEE K M.Accurate Image Super-Resolution Using Very Deep Convolutional Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:1646-1654.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" KIM J, LEE J K, LEE K M.Deeply-Recursive Convolutional Network for Image Super-Resolution // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:1637-1645." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deeply-recursive convolutional network for image super-resolution">
                                        <b>[14]</b>
                                         KIM J, LEE J K, LEE K M.Deeply-Recursive Convolutional Network for Image Super-Resolution // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:1637-1645.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" YANG W H, FENG J S, YANG J C, &lt;i&gt;et al&lt;/i&gt;.Deep Edge Guided Recurrent Residual Learning for Image Super-Resolution.IEEE Transactions on Image Processing, 2017, 26 (12) :5895-5907." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep edge guided recurrent residual learning for image super-resolution">
                                        <b>[15]</b>
                                         YANG W H, FENG J S, YANG J C, &lt;i&gt;et al&lt;/i&gt;.Deep Edge Guided Recurrent Residual Learning for Image Super-Resolution.IEEE Transactions on Image Processing, 2017, 26 (12) :5895-5907.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" ZENG K, YU J, WANG R X, &lt;i&gt;et al&lt;/i&gt;.Coupled Deep Autoencoder for Single Image Super-Resolution.IEEE Transactions on Cybernetics, 2017, 47 (1) :27-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coupled Deep Autoencoder for Single Image Super-Resolution">
                                        <b>[16]</b>
                                         ZENG K, YU J, WANG R X, &lt;i&gt;et al&lt;/i&gt;.Coupled Deep Autoencoder for Single Image Super-Resolution.IEEE Transactions on Cybernetics, 2017, 47 (1) :27-37.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" YANG J C, WANG Z W, LIN Z, &lt;i&gt;et al&lt;/i&gt;.Coupled Dictionary Trai-ning for Image Super-Resolution.IEEE Transactions on Image Processing, 2012, 21 (8) :3467-3478." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coupled Dictionary Training for Image Super-Resolution">
                                        <b>[17]</b>
                                         YANG J C, WANG Z W, LIN Z, &lt;i&gt;et al&lt;/i&gt;.Coupled Dictionary Trai-ning for Image Super-Resolution.IEEE Transactions on Image Processing, 2012, 21 (8) :3467-3478.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" ZEYDE R, ELAD M, PROTTER M.On Single Image Scale-up Using Sparse-Representations // Proc of the International Confe-rence on Curves and Surfaces.Berlin, Germany:Springer, 2010:711-730." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On single image scale-up using sparse-representations">
                                        <b>[18]</b>
                                         ZEYDE R, ELAD M, PROTTER M.On Single Image Scale-up Using Sparse-Representations // Proc of the International Confe-rence on Curves and Surfaces.Berlin, Germany:Springer, 2010:711-730.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" ROMANO Y, PROTTER M, ELAD M.Single Image Interpolation via Adaptive Nonlocal Sparsity-Based Modeling.IEEE Transactions on Image Processing, 2014, 23 (7) :3085-3098." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single image interpolation via adaptive non-local sparsity-based modeling">
                                        <b>[19]</b>
                                         ROMANO Y, PROTTER M, ELAD M.Single Image Interpolation via Adaptive Nonlocal Sparsity-Based Modeling.IEEE Transactions on Image Processing, 2014, 23 (7) :3085-3098.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" DONG W S, ZHANG L, SHI G M, &lt;i&gt;et al&lt;/i&gt;.Nonlocally Centralized Sparse Representation for Image Restoration.IEEE Transactions on Image Processing, 2013, 22 (4) :1620-1630." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Nonlocally Centralized Sparse Representation for Image Restoration">
                                        <b>[20]</b>
                                         DONG W S, ZHANG L, SHI G M, &lt;i&gt;et al&lt;/i&gt;.Nonlocally Centralized Sparse Representation for Image Restoration.IEEE Transactions on Image Processing, 2013, 22 (4) :1620-1630.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" SONG H H, LIU Q S, WANG G J, &lt;i&gt;et al&lt;/i&gt;.Spatiotemporal Satellite Image Fusion Using Deep Convolutional Neural Networks.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2018, 11 (3) :821-829." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatiotemporal Satellite Image Fusion Using Deep Convolutional Neural Networks">
                                        <b>[21]</b>
                                         SONG H H, LIU Q S, WANG G J, &lt;i&gt;et al&lt;/i&gt;.Spatiotemporal Satellite Image Fusion Using Deep Convolutional Neural Networks.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2018, 11 (3) :821-829.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" ZEILER M D, KRISHNAN D, TAYLOR G W, &lt;i&gt;et al&lt;/i&gt;.Deconvolutional Networks // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2010:2528-2535." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deconvolutional networks">
                                        <b>[22]</b>
                                         ZEILER M D, KRISHNAN D, TAYLOR G W, &lt;i&gt;et al&lt;/i&gt;.Deconvolutional Networks // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2010:2528-2535.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" CLEVERT D, UNTERTHINER T, HOCHREITER S.Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) [C/OL].[2019-01-15].https://arxiv.org/pdf/1511.07289.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) [C/OL]">
                                        <b>[23]</b>
                                         CLEVERT D, UNTERTHINER T, HOCHREITER S.Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) [C/OL].[2019-01-15].https://arxiv.org/pdf/1511.07289.pdf.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" KINGMA D P, BA J L.Adam:A Method for Stochastic Optimization[C/OL].[2019-01-15].https://arxiv.org/pdf/1412.6980.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:A Method for Stochastic Optimization[C/OL]">
                                        <b>[24]</b>
                                         KINGMA D P, BA J L.Adam:A Method for Stochastic Optimization[C/OL].[2019-01-15].https://arxiv.org/pdf/1412.6980.pdf.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" GLOROT X, BENGIO Y.Understanding the Difficulty of Training Deep Feedforward Neural Networks // Proc of the International Conference on Artificial Intelligence and Statistics.New York, USA:ACM, 2010:249-256." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding the difficulty of training deep feedforward neural networks">
                                        <b>[25]</b>
                                         GLOROT X, BENGIO Y.Understanding the Difficulty of Training Deep Feedforward Neural Networks // Proc of the International Conference on Artificial Intelligence and Statistics.New York, USA:ACM, 2010:249-256.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_26" title=" GEVAERT C, GARCIA-HARO F.A Comparison of StarFM and an Unmixing-Based Algorithm for Landsat and MODIS Data Fusion.Remote Sensing of Environment, 2015, 156:34-44." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300249404&amp;v=MTU1MzQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRnNTYnhNPU5pZk9mYks5SDlQT3JJOUZadThHQ0h3OW9CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         GEVAERT C, GARCIA-HARO F.A Comparison of StarFM and an Unmixing-Based Algorithm for Landsat and MODIS Data Fusion.Remote Sensing of Environment, 2015, 156:34-44.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_27" title=" ALPARONE L, BARONTI S, GARZELLI A, &lt;i&gt;et al&lt;/i&gt;.A Global Qua-lity Measurement of Pan-Sharpened Multispectral Imagery.IEEE Geoscience and Remote Sensing Letters, 2004, 1 (4) :313-317." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A global quality measurement of pan-sharpened multispectral imagery">
                                        <b>[27]</b>
                                         ALPARONE L, BARONTI S, GARZELLI A, &lt;i&gt;et al&lt;/i&gt;.A Global Qua-lity Measurement of Pan-Sharpened Multispectral Imagery.IEEE Geoscience and Remote Sensing Letters, 2004, 1 (4) :313-317.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(05),429-435 DOI:10.16451/j.cnki.issn1003-6059.201905005            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于多输入密集连接神经网络的遥感图像时空融合算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9A%E5%87%AF%E6%97%8B&amp;code=41999548&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姚凯旋</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%B9%E9%A3%9E%E9%BE%99&amp;code=35333575&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曹飞龙</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E8%AE%A1%E9%87%8F%E5%A4%A7%E5%AD%A6%E7%90%86%E5%AD%A6%E9%99%A2%E5%BA%94%E7%94%A8%E6%95%B0%E5%AD%A6%E7%B3%BB&amp;code=1699595&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国计量大学理学院应用数学系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了解决地表反射率遥感卫星Landsat和MODIS影像的时空融合问题, 文中提出基于多输入密集连接网络的遥感图像时空融合算法.首先提出多输入的密集连接网络, 学习包含连续时刻间差异信息的过渡遥感影像.基于差异相似假设, 融合网络学习得到的2幅过渡影像与已知的2幅高空间分辨率影像, 得到最终的预测影像.对Landsat遥感影像和MODIS遥感影像的融合实验表明, 文中算法在各项定量指标中均较优, 最终的预测图像也可表明, 文中算法对噪声具有较好的鲁棒性, 能较好地恢复细节信息.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%A5%E6%84%9F%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遥感图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%B6%E7%A9%BA%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">时空融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AF%86%E9%9B%86%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">密集神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    姚凯旋, 硕士研究生, 主要研究方向为深度学习、图像处理等.E-mail:yaokx2@gmail.com.;
                                </span>
                                <span>
                                    *曹飞龙 (通讯作者) , 博士, 教授, 主要研究方向为深度学习、图像处理等.E-mail:feilongcao@gmail.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-05</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.61672477) 资助;</span>
                    </p>
            </div>
                    <h1><b>Spatial-Temporal Fusion Algorithm for Remote Sensing Images Based on Multi-input Dense Connected Neural Network</b></h1>
                    <h2>
                    <span>YAO Kaixuan</span>
                    <span>CAO Feilong</span>
            </h2>
                    <h2>
                    <span>Department of Applied Mathematics, College of Sciences, China Jiliang University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To solve the spatial-temporal fusion problem of images of surface reflectivity remote sensing satellites Landsat and MODIS, a spatial-temporal fusion algorithm for remote sensing images based on multi-input dense connected neural network is proposed. Firstly, a multi-input dense connected neural network is put forward to study the remote sensing images containing the difference information between continuous moments. Then, two transition images learned from the network are fused with the two known high spatial resolution images based on the difference similarity hypothesis to obtain the final predicted images. According to the fusion experiment of Landsat remote sensing images and MODIS remote sensing images, the proposed algorithm produces promising results in each quantitative index. The final predicted image by the proposed algorithm is more robust to noise with better recovered detail information.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Remote%20Sensing%20Image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Remote Sensing Image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Spatial-Temporal%20Fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Spatial-Temporal Fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Dense%20Neural%20Network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Dense Neural Network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YAO Kaixuan, master student. His research interests include deep learning and image processing.;
                                </span>
                                <span>
                                    CAO Feilong ( Corresponding author) , Ph. D. , professor. His research interests include deep learning and image processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-05</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.61672477);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="67">随着卫星遥感技术的不断发展, 遥感为地表植被变化监测、自然灾害预防、土地利用勘测等提供重要的数据来源.但是, 目前的遥感卫星不能同时满足时间和空间上高分辨率的要求.</p>
                </div>
                <div class="p1">
                    <p id="68">Landsat卫星的空间分辨率为30 m, 较高的空间分辨率特性使其可以为地表覆盖物的精细化研究提供有效数据, 但是Landsat卫星的时间采样周期为16天, 无法及时观测地表分布的变化.MODIS卫星具有较高的时间分辨率, 但是空间分辨率只有250 m～1 000 m.因此, 如果将Landsat和MODIS卫星的遥感影像进行融合, 可以得到同时具备高时间和高空间分辨率的遥感影像数据.</p>
                </div>
                <div class="p1">
                    <p id="70">根据时相变化在一个局部窗口内具有尺度不变性理论, Gao等<citation id="162" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出时空自适应反射率融合模型 (Spatial and Temporal Adaptive Reflectance Fusion Model, STARFM) , 融合高时间分辨率MODIS影像和高空间分辨率Landsat影像.Zhu等<citation id="163" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出改进的时空自适应融合模型 (An Enhanced STARFM for Complex Heterogeneous Regions, ESTARFM) , 不仅依赖对应像元的信息, 还综合考虑局部像元及地表反射率的时间变化, 对不同特性的地表具有更好的泛用性.但是, STARFM和ESTARFM都假设高分辨率与低分辨率影像像元之间的关系是线性的.为了克服这一局限性, Huang等<citation id="164" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出基于稀疏表示理论的时空反射率融合模型 (Sparse Representation-Based Spatial-Temporal Reflectance Fusion Model, SPSTFM) , 通过非线性的方式提取Landsat和MODIS影像字典对.Wu等<citation id="165" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出基于误差约束的半耦合字典学习模型 (Error-Bound-Regularized Semi-Coupled Dictionary Learning, ESCBDL) , 假设高空间和低空间分辨率遥感影像字典对的稀疏系数具有一定的扰动性.</p>
                </div>
                <div class="p1">
                    <p id="71">近些年, 深度学习算法在计算机视觉领域取得较大成功<citation id="167" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>.基于深度学习的图像超分辨率重建算法<citation id="168" type="reference"><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>的效果优于字典学习<citation id="169" type="reference"><link href="35" rel="bibliography" /><link href="37" rel="bibliography" /><link href="39" rel="bibliography" /><link href="41" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>,<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>等算法.在遥感图像时空融合问题上, 目前只有Song等<citation id="166" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提出基于卷积神经网络的遥感时空融合算法, 采用包含5个卷积层的神经网络, 分两步从MODIS影像恢复Landsat影像, 最后采用基于高通滤波模型的融合算法将网络学习的过渡影像与已知的2幅Landsat影像进行融合, 得到预测的中间时刻Landsat影像.但是该方法只是简单地将经典的前向卷积神经网络 (Convolutional Neural Network, CNN) 模型引入到遥感图像时空融合问题, 未改进CNN网络.</p>
                </div>
                <div class="p1">
                    <p id="72">基于此种情况, 为了解决地表反射率遥感卫星Landsat和MODIS影像的时空融合问题, 本文提出基于多输入密集连接网络的遥感图像时空融合算法.首先提出多输入的密集连接网络, 学习包含连续时刻间差异信息的过渡遥感影像.基于差异相似假设, 融合网络学习得到的2幅过渡影像与已知的2幅高空间分辨率影像, 得到最终的预测影像.对Landsat和MODIS遥感影像的融合实验表明, 本文算法在各项定量指标中均较优.从最终的预测图像也可明显看出, 本文算法对噪声具有较好的鲁棒性, 能较好地恢复细节信息.</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag">1 基于密集网络及误差相似插值的遥感图像时空融合算法</h3>
                <div class="p1">
                    <p id="74">图1给出Landsat和MODIS遥感影像时空融合问题的示意图.已知连续三个时刻的MODIS影像, 而Landsat影像只有<i>t</i><sub>1</sub>、<i>t</i><sub>3</sub>时刻, 缺失<i>t</i><sub>2</sub>时刻的影像.该问题需要利用已知的3幅MODIS影像和2幅Landsat影像预测缺失时刻的Landsat影像.</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905006_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Landsat和MODIS遥感影像时空融合问题" src="Detail/GetImg?filename=images/MSSB201905006_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Landsat和MODIS遥感影像时空融合问题  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905006_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Spatial and temporal fusion problem of Landsat and MODIS remote sensing images</p>

                </div>
                <div class="p1">
                    <p id="77">图2给出融合算法的处理流程图.</p>
                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905006_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 融合算法的整体处理流程图" src="Detail/GetImg?filename=images/MSSB201905006_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 融合算法的整体处理流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905006_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Flow chart of overall processing of fusion algorithm</p>

                </div>
                <div class="p1">
                    <p id="79">首先提出多输入的密集连接网络 (Multi-input Dense Neural Network, MDN) , 用于学习一幅过渡高分辨率遥感影像.然后根据连续时刻间差异相似假设 (Error Linear Interpolation, ELI) , 对网络学习到的过渡影像与已知的2幅<i>t</i><sub>1</sub>、<i>t</i><sub>3</sub>时刻的Landsat影像进行融合, 得到最终的<i>t</i><sub>2</sub>时刻的高空间分辨率影像.</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80"><b>1.1 基于多输入的密集连接网络</b></h4>
                <div class="p1">
                    <p id="81">基于深度学习的图像超分辨率重建算法主要解决单幅图像的重建, 即网络为单输入单输出, 输入和输出分别对应单幅低分辨率图像和高分辨率图像.而Landsat与MODIS卫星包含连续时刻的遥感影像, 相邻时刻的影像可以为高分辨率重建提供更多的细节信息.为了充分利用MODIS卫星的高时间分辨率特性, 设计MDN, 将2幅连续时刻的MODIS影像同时输入网络进行学习.网络结构示意图如图3所示.</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905006_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 多输入密集连接神经网络" src="Detail/GetImg?filename=images/MSSB201905006_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 多输入密集连接神经网络  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905006_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Multi-input dense connected neural network</p>

                </div>
                <div class="p1">
                    <p id="83">网络的输入层为2幅连续时刻的低分辨率影像<i>x</i><sub>1</sub>和<i>x</i><sub>2</sub>.网络的第一层包含16个1×1大小的卷积核.1×1卷积运算的数学公式为</p>
                </div>
                <div class="p1">
                    <p id="84"><i>T</i>=<i>w</i>*<i>x</i>,      (1) </p>
                </div>
                <div class="p1">
                    <p id="85">其中, <i>w</i>为一个实数, 表示卷积核的权重, <i>x</i>为输入的两幅图像, <i>T</i>为卷积之后得到的特征图像.从式 (1) 可看出, 1×1卷积是一个严格线性的运算, 卷积核尺寸为1×1, 因此提取的特征图像<i>T</i>与输入图像<i>x</i>的像素在同个尺度上.第一层卷积核数量设置为16, 可以在2幅低分辨率图像的像素尺度上扩增特征.同时, 为了提高网络的非线性表达能力, 选用非线性的Sigmoid函数作为第一层的激活函数, 第一个卷积层的运算表示为</p>
                </div>
                <div class="p1">
                    <p id="86"><i>y</i><sub>1</sub>=<i>f</i><sub>sigmoid</sub> (<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>;<i>θ</i><sub>1</sub>) , </p>
                </div>
                <div class="p1">
                    <p id="87">其中, <i>x</i><sub>1</sub>、<i>x</i><sub>2</sub>表示2幅连续时刻的低分辨率影像, <i>θ</i><sub>1</sub>表示第一层的所有权重, <i>f</i><sub>sigmoid</sub> (·) 表示激活函数为Sigmoid的卷积运算, <i>y</i><sub>1</sub>表示第一层提取到的所有特征图像.</p>
                </div>
                <div class="p1">
                    <p id="88">由图3可见, MDN网络从整体上可分为4个密集连接块, 每个密集块包含3层, 密集块的结构示意图如图4所示.</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905006_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 密集连接块" src="Detail/GetImg?filename=images/MSSB201905006_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 密集连接块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905006_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Dense connection block</p>

                </div>
                <div class="p1">
                    <p id="90">密集连接的含义如图4所示, 第一层的输出不仅作为第二层的输入, 同样要作为第三层的输入.通过引入密集连接, 可以增强特征的传播, 也可以提高特征的复用性, 这样可以充分利用网络所有层提取的不同尺度的特征.</p>
                </div>
                <div class="p1">
                    <p id="91">另外, 与一般用于图像超分辨率重建的深度网络不同的是, 在网络的中间层采用反卷积运算<citation id="170" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>代替卷积运算, 反卷积运算可以看成卷积运算的逆运算.同时, 为了避免在训练过程中出现梯度消失现象, 选择指数线性单元 (Exponential Linear Unit, ELU) <citation id="171" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>作为中间层的激活函数:</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mtext>E</mtext><mtext>L</mtext><mtext>U</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>x</mi><mo>, </mo></mtd><mtd columnalign="left"><mi>x</mi><mo>≥</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left"><mi>α</mi><mo stretchy="false"> (</mo><mi>exp</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>, </mo></mtd><mtd columnalign="left"><mi>x</mi><mo>&lt;</mo><mn>0</mn></mtd></mtr></mtable></mrow></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">第一层提取的特征与低分辨率图像像素在同一尺度下的特征在中间层经过反卷积运算可以恢复更高尺度的细节特征, 中间层的反卷积运算可表示为</p>
                </div>
                <div class="p1">
                    <p id="94"><i>y</i><sub><i>i</i></sub>=<i>f</i> ′<sub>ELU</sub> (<i>y</i><sub><i>i</i>-2</sub>, <i>y</i><sub><i>i</i>-1</sub>;<i>θ</i><sub><i>i</i></sub>) , </p>
                </div>
                <div class="p1">
                    <p id="95">其中, <i>y</i><sub><i>i</i></sub>表示第<i>i</i>层提取的特征图像, <i>θ</i><sub><i>i</i></sub>表示第<i>i</i>层的所有权重, <i>f</i> ′<sub>ELU</sub> (·) 表示激活函数为ELU的反卷积运算.</p>
                </div>
                <div class="p1">
                    <p id="96">由于MODIS影像的空间分辨率与Landsat影像的空间分辨率相差较大, 很难直接从一个较低的分辨率特征恢复较高的分辨率特征, 因此MDN网络中间层反卷积核的尺寸大小为递进式设置 (见图3) .不同大小的反卷积运算可提取不同尺度的特征, 使低分辨特征从低尺度逐步恢复到高尺度.</p>
                </div>
                <div class="p1">
                    <p id="97">网络的最后一层只含有一个1×1的卷积核, 最后一层的激活函数采用线性Linear函数.通过1×1的卷积运算再经过线性函数映射, 可以融合中间层提取的多个尺度的特征, 得到最终的高分辨率图像.最后一层的卷积运算可表示为</p>
                </div>
                <div class="p1">
                    <p id="98"><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>y</mi><mo>˜</mo></mover><mo>=</mo><mi>f</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mtext>a</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>n</mtext><mtext>d</mtext><mo>-</mo><mn>2</mn></mrow></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>n</mtext><mtext>d</mtext><mo>-</mo><mn>1</mn></mrow></msub></mrow></math></mathml>;<i>θ</i><sub>end</sub>) .</p>
                </div>
                <div class="p1">
                    <p id="100">MDN网络同样属于有监督学习网络.对于一个批次的训练样本集{ (<i>Y</i><sub><i>i</i></sub>, <i>X</i><sup>1</sup><sub><i>i</i></sub>, <i>X</i><sup>2</sup><sub><i>i</i></sub>) }<mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow></math></mathml>, 采用预测得到的高分辨率图像与实际高分辨率图像的均方误差 (Mean Square Error, MSE) 作为损失函数, 具体的损失函数公式为</p>
                </div>
                <div class="p1">
                    <p id="102"><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>Y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>F</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>D</mtext><mtext>Ν</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>X</mi><msubsup><mrow></mrow><mi>i</mi><mn>1</mn></msubsup><mo>, </mo><mi>X</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup><mo>;</mo><mi>θ</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>,       (2) </p>
                </div>
                <div class="p1">
                    <p id="104">其中, <i>n</i>表示一批训练样本的数量, 本文在训练时<i>n</i>=16, <i>θ</i>表示网络所有的权重和偏置项, <i>Y</i><sub><i>i</i></sub>表示实际的高分辨率图像, <i>F</i><sub>MDN</sub> (<i>X</i><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>1</mn></msubsup></mrow></math></mathml>, <i>X</i><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></math></mathml>;<i>θ</i>) 表示MDN网络预测的高分辨率图像.</p>
                </div>
                <div class="p1">
                    <p id="107">选用自适应估计矩算法 (Adaptive Moment Estimation, Adam) <citation id="172" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>对式 (2) 进行优化学习.Adam算法首先计算一阶估计矩的均值和二阶估计矩的方差:</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>m</mi><mo>^</mo></mover><mo>=</mo><mfrac><mrow><mi>m</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mn>1</mn><mo>-</mo><mi>β</mi><msubsup><mrow></mrow><mn>1</mn><mi>t</mi></msubsup></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><mover accent="true"><mi>v</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>v</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mn>1</mn><mo>-</mo><mi>β</mi><msubsup><mrow></mrow><mn>2</mn><mi>t</mi></msubsup></mrow></mfrac><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">然后得到最终的网络权值<i>θ</i>的迭代公式:</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>θ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mfrac><mi>η</mi><mrow><msqrt><mrow><mover accent="true"><mi>v</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></msqrt><mo>+</mo><mi>δ</mi></mrow></mfrac><mover accent="true"><mi>m</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">其中:<i>β</i><sub>1</sub>表示一阶估计矩的指数衰减率, 实验中设为0.9;<i>β</i><sub>2</sub>表示二阶估计矩的指数衰减率, 实验中设为0.999 9;<i>δ</i>表示为了防止出现除零操作而设置的一个非常小的实数, 实验中设为10<sup>-8</sup>.</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112"><b>1.2 基于误差相似的插值重建</b></h4>
                <div class="p1">
                    <p id="113">MDN网络在学习过程中只利用3幅低分辨率的MODIS影像<i>L</i><sup>1</sup>、<i>L</i><sup>2</sup>和<i>L</i><sup>3</sup>的信息, 还未利用2幅高分辨率的Landsat影像<i>H</i><sup>1</sup>和<i>H</i><sup>3</sup>的信息.本节提出ELI, 用于融合图像, 充分利用Landsat影像的高空间分辨率特性.</p>
                </div>
                <div class="p1">
                    <p id="114">地表反射率遥感卫星成像时会受到光照变化和阴雨天气的影响, 因此, 同一日期内不同时刻采集的遥感影像由于光照的不同会有一定的差异.由于硬件设备的限制, 目前MODIS遥感卫星和Landsat遥感卫星的采样周期均以天为单位.所以, 现有的关于遥感影像融合算法在实际处理中都不考虑同一日期内光照变化的情况.因此, 本文以日期为单位进行分析处理, 不考虑不同时刻间光照等差异的影响.</p>
                </div>
                <div class="p1">
                    <p id="115">利用MDN网络, 首先将<i>L</i><sup>1</sup>和<i>L</i><sup>2</sup>作为输入得到一幅过渡高分辨率影像<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Η</mi><mo>˜</mo></mover><msup><mrow></mrow><mspace width="0.25em" /></msup><mrow><mn>1</mn><mn>2</mn></mrow></mrow></math></mathml>.类似地, 将<i>L</i><sup>2</sup>和<i>L</i><sup>3</sup>作为输入得到一幅过渡高分辨率影像<mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Η</mi><mo>˜</mo></mover><msup><mrow></mrow><mspace width="0.25em" /></msup><mrow><mn>2</mn><mn>3</mn></mrow><mo>.</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="118">假设在相邻时刻<i>t</i><sub>1</sub>、<i>t</i><sub>2</sub>时间段内, MODIS卫星和Landsat卫星捕获到的该时间段内地表覆盖物的变化信息相同.由于MDN网络输入层是2幅连续时刻的低分辨率影像, 所以MDN网络输出的过渡影像包含连续时刻的2幅低分辨率影像间的差异信息和目标影像的高分辨率信息, 则MDN网络学习到的过渡影像去除目标影像的高分辨率信息后就是相邻时刻间遥感影像的差异信息.同时, 网络在学习时存在一定的逼近误差<i>ε</i>.据此, 可得</p>
                </div>
                <div class="p1">
                    <p id="119"><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Η</mi><mo>˜</mo></mover><msup><mrow></mrow><mspace width="0.25em" /></msup><mrow><mn>1</mn><mn>2</mn></mrow><mo>-</mo><mi>Η</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mi>ε</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub><mo>=</mo><mi>Η</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mi>Η</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></math></mathml>,       (3) </p>
                </div>
                <div class="p1">
                    <p id="121">其中<i>ε</i><sub>12</sub>为网络对<i>t</i><sub>1</sub>、<i>t</i><sub>2</sub>时刻影像进行学习时的逼近误差.类似地, 对于<i>t</i><sub>2</sub>、<i>t</i><sub>3</sub>时刻, 有</p>
                </div>
                <div class="p1">
                    <p id="122"><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Η</mi><mo>˜</mo></mover><msup><mrow></mrow><mspace width="0.25em" /></msup><mrow><mn>2</mn><mn>3</mn></mrow><mo>-</mo><mi>Η</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mi>ε</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>3</mn></mrow></msub><mo>=</mo><mi>Η</mi><msub><mrow></mrow><mn>3</mn></msub><mo>-</mo><mi>Η</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></math></mathml>,       (4) </p>
                </div>
                <div class="p1">
                    <p id="124">其中<i>ε</i><sub>23</sub>为网络对<i>t</i><sub>2</sub>、<i>t</i><sub>3</sub>时刻影像进行学习时的逼近误差.</p>
                </div>
                <div class="p1">
                    <p id="125">式 (4) 与式 (3) 作差, 得到<i>t</i><sub>2</sub>时刻的高分辨率影像</p>
                </div>
                <div class="p1">
                    <p id="126" class="code-formula">
                        <mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mfrac><mrow><mi>Η</mi><msub><mrow></mrow><mn>3</mn></msub><mo>+</mo><mi>Η</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mover accent="true"><mi>Η</mi><mo>˜</mo></mover><msup><mrow></mrow><mspace width="0.25em" /></msup><mrow><mn>2</mn><mn>3</mn></mrow><mo>+</mo><mover accent="true"><mi>Η</mi><mo>˜</mo></mover><msup><mrow></mrow><mspace width="0.25em" /></msup><mrow><mn>1</mn><mn>2</mn></mrow></mrow><mn>2</mn></mfrac><mo>-</mo><mfrac><mrow><mi>ε</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub><mo>-</mo><mi>ε</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>3</mn></mrow></msub></mrow><mn>2</mn></mfrac><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="127">由于<i>ε</i><sub>12</sub>和<i>ε</i><sub>23</sub>为同一区域相邻时刻影像的网络逼近误差, 则2个逼近误差的差值<i>ε</i><sub>12</sub>-<i>ε</i><sub>23</sub>趋近于0, 可以忽略不计, 因此代入化简后得到最终<i>t</i><sub>2</sub>时刻的预测高分辨率影像:</p>
                </div>
                <div class="p1">
                    <p id="128" class="code-formula">
                        <mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><msub><mrow></mrow><mn>2</mn></msub><mo>≈</mo><mfrac><mrow><mi>Η</mi><msub><mrow></mrow><mn>3</mn></msub><mo>+</mo><mi>Η</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mover accent="true"><mi>Η</mi><mo>˜</mo></mover><msup><mrow></mrow><mspace width="0.25em" /></msup><mrow><mn>2</mn><mn>3</mn></mrow><mo>+</mo><mover accent="true"><mi>Η</mi><mo>˜</mo></mover><msup><mrow></mrow><mspace width="0.25em" /></msup><mrow><mn>1</mn><mn>2</mn></mrow></mrow><mn>2</mn></mfrac><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <h3 id="129" name="129" class="anchor-tag">2 实验及结果分析</h3>
                <div class="p1">
                    <p id="130">为了验证本文算法的有效性, 选取对比算法为2个常用的遥感影像时空融合算法 (SPSTFM和ESCBDL) 和1个用于图像超分辨率重建的深度学习算法, 即基于深度学习的用于图像超分辨率重建的深度卷积神经网络 (Very Deep Convolutional Neural Network, VDSR) <citation id="173" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>.在Tensorflow1.9框架、Keras底层、Matlab 2016条件下进行实验, 实验环境为: 16 GB内存, Intel CORE i7处理器, Nvida GTX 1050ti GPU.</p>
                </div>
                <div class="p1">
                    <p id="131">为了公平起见, 本文使用的数据与以前工作中使用的数据保持一致, 高分辨率影像选用Landsat-7卫星数据, 低分辨率影像选用MODIS卫星数据.需要指出的是, MODIS影像需要与Landsat影像进行辐射测量配准, 利用MODIS重投影工具 (MODIS Reprojection Tools, MRT) 将MODIS影像重新投影到WGS84坐标系中, 使其与Landsat影像保持一致.Landsat和MODIS影像具有两百多个波段, 与以前的工作一样, 本文也选用常用于土地覆被分类和植被物候变化检测的3个波段 (近红外线波段、红色波段及绿色波段) 进行验证.详细的光谱范围如表1所示.</p>
                </div>
                <div class="area_img" id="132">
                    <p class="img_tit"><b>表1 Landsat卫星和MODIS卫星的光谱范围</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Spectral ranges of Landsat and MODIS</p>
                    <p class="img_note"></p>
                    <table id="132" border="1"><tr><td rowspan="2"><br />波段<br />名称</td><td colspan="3"><br />Landsat</td><td colspan="3"><br />MODIS</td></tr><tr><td colspan="2"><br />波段编号</td><td>带宽/nm</td><td colspan="2"><br />波段编号</td><td>带宽/nm</td></tr><tr><td><br />蓝色</td><td>1</td><td colspan="2">450～520</td><td>3</td><td colspan="2">459～479</td></tr><tr><td><br />绿色</td><td>2</td><td colspan="2">530～610</td><td>4</td><td colspan="2">545～565</td></tr><tr><td><br />红色</td><td>3</td><td colspan="2">630～690</td><td>1</td><td colspan="2">620～670</td></tr><tr><td><br />近红外</td><td>4</td><td colspan="2">780～900</td><td>2</td><td colspan="2">841～876</td></tr><tr><td><br />中红外</td><td>5</td><td colspan="2">1550～1750</td><td>6</td><td colspan="2">1628～1652</td></tr><tr><td><br />中红外</td><td>7</td><td colspan="2">2090～2350</td><td>7</td><td colspan="2">2105～2155</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="133">类似地, 在经纬度 (54°N, 104°W) 附近选取由加拿大北方生态-大气研究组织 (Boreal Ecosystem-Atmosphere Study, BOREAS) 采集并由Gao等<citation id="174" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提供的3对Landsat和MODIS遥感影像进行实验, 3对影像时间分别为2001年5月24日、2001年7月11日、2001年8月12日, 其中2001年7月11日的Landsat影像为待预测图像.</p>
                </div>
                <div class="p1">
                    <p id="135">图5给出由“近红外-红色-绿色”三波段组合的模拟自然图像展示3对连续时刻的MODIS和Landsat实验数据.</p>
                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905006_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 近红外-红色-绿色三波段组合的MODIS影像和Landsat 影像" src="Detail/GetImg?filename=images/MSSB201905006_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 近红外-红色-绿色三波段组合的MODIS影像和Landsat 影像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905006_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 NIR-red-green composites of MODIS images and Landsat images</p>

                </div>
                <div class="p1">
                    <p id="140">对于训练数据, 使用已知的<i>t</i><sub>2</sub>、<i>t</i><sub>3</sub>时刻的MODIS影像和<i>t</i><sub>3</sub>时刻的Landsat影像进行训练, 即7月11日和8月12日的MODIS影像作为输入的2幅连续时刻的低分辨率影像, 8月12日的Landsat影像作为输出的高分辨率影像.为了满足网络训练的需求, 将大小为1 200×1 200的MODIS影像和Landsat影像对应裁剪成50×50的图像块, 共3 364对用于网络训练.</p>
                </div>
                <div class="p1">
                    <p id="141">为了公平起见, VDSR网络的训练数据也使用相同的数据集.需要指出的是, 由于VDSR网络为单输入单输出, 所以<i>t</i><sub>3</sub>时刻的MODIS影像和<i>t</i><sub>3</sub>时刻的Landsat影像分别作为输入和输出, 也将图像裁剪为50×50的图像块, 共3 364对用于网络的训练.</p>
                </div>
                <div class="p1">
                    <p id="142">在预测阶段, 首先直接将已知的<i>t</i><sub>1</sub>、<i>t</i><sub>2</sub>时刻大小为1 200×1 200的2幅MODIS影像输入到已训练好的MDN网络中, 学习得到1幅过渡高分辨率影像.类似地, 将已知的<i>t</i><sub>2</sub>、<i>t</i><sub>3</sub>时刻大小为1200×1200的2幅MODIS影像输入到已训练好的网络中, 学习得到1幅过渡高分辨率影像.然后利用ELI将网络学习的2幅过渡影像与已知的<i>t</i><sub>1</sub>、<i>t</i><sub>3</sub>时刻的Landsat影像进行融合, 得到中间<i>t</i><sub>2</sub>时刻的Landsat影像.</p>
                </div>
                <div class="p1">
                    <p id="143">MDN网络包含4个密集连接块共12层, 每个密集连接块都包含3层, 每层的卷积核尺寸、卷积核数量及每层的激活函数在1.1节已进行详细介绍.对每层都采用Glorot正态分布初始化方法<citation id="175" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>对卷积核的权重进行初始化.采用Adam优化算法对网络的权重进行更新迭代, Adam的参数设置与原文献中保持一致.训练的批次样本数量为16, 最大迭代次数为100.损失函数采用经典的均方误差.</p>
                </div>
                <div class="p1">
                    <p id="144">选用4个评价指标进行算法性能评价:均方根误差 (Root Mean Square Error, RMSE) 、相关系数 (Correlation Coefficient, CC) 、全局相对误差 (Erre-ur Relative Globale Adimensionnelle de Synthèse, ERGAS) <citation id="176" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、全局质量度量 (Global Quality Measure-ment, Q4) <citation id="177" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>.Q4和ERGAS为综合3个波段一起评价.CC和Q4的值越大效果越好, RMSE和ERGAS值越小效果越好.ERGAS的计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="147" class="code-formula">
                        <mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mi>R</mi><mi>G</mi><mi>A</mi><mi>S</mi><mo>=</mo><mn>1</mn><mn>0</mn><mn>0</mn><mfrac><mi>h</mi><mi>l</mi></mfrac><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>b</mi><mi>a</mi><mi>n</mi></mrow></msub></mrow></mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>b</mi><mi>a</mi><mi>n</mi></mrow></msub></mrow></munderover><mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>R</mi><mi>Μ</mi><mi>S</mi><mi>E</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mi>Μ</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow></mrow></mstyle></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="148">其中, <i>h</i>为Landsat卫星的空间分辨率, <i>l</i>为MODIS卫星的空间分辨率, <i>N</i><sub><i>ban</i></sub>为光谱波段数量 (本文实验中值为3) , <i>M</i><sub><i>k</i></sub>为第<i>k</i>个波段的平均值.</p>
                </div>
                <div class="p1">
                    <p id="149">表2和表3给出4种算法对2001年7月11日经纬度 (54°N, 104°W) 附近Landsat影像预测结果的定量对比.表2给出4种算法分别在近红外、红色和绿色3个波段融合效果的定量对比结果.表3给出4种算法3个波段融合的综合定量对比结果.表中黑色数字表示效果最好.由表可见, MDN网络明显优于SPSTFM、ESCBDL、VDSR.</p>
                </div>
                <div class="area_img" id="150">
                    <p class="img_tit"><b>表2 4种算法融合效果在不同波段的定量对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Quantitative metrics comparison of fusion results of 3 methods on different bands</p>
                    <p class="img_note"></p>
                    <table id="150" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="3"><br />RMSE</td><td colspan="3"><br />CC</td></tr><tr><td><br />近红外</td><td>红色</td><td>绿色</td><td><br />近红外</td><td>红色</td><td>绿色</td></tr><tr><td>SPSTFM</td><td>0.0266</td><td>0.0196</td><td>0.0189</td><td>0.8942</td><td>0.4532</td><td>0.3607</td></tr><tr><td>ESCBDL</td><td>0.0265</td><td>0.0196</td><td>0.0189</td><td>0.8945</td><td>0.4537</td><td>0.3611</td></tr><tr><td><br />VDSR</td><td>0.0271</td><td>0.0199</td><td>0.0192</td><td>0.8891</td><td>0.4452</td><td>0.3525</td></tr><tr><td><br />MDN</td><td>0.0392</td><td><b>0.0166</b></td><td><b>0.0164</b></td><td><b>0.9140</b></td><td><b>0.5799</b></td><td><b>0.5296</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="151">
                    <p class="img_tit"><b>表3 4种算法融合效果的综合定量对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"> Table 3 Comprehensive quantitative metrics comparison of fusion results of 4 methods</p>
                    <p class="img_note"></p>
                    <table id="151" border="1"><tr><td><br /></td><td>SPSTFM</td><td>ESCBDL</td><td>VDSR</td><td>MDN</td></tr><tr><td><br />ERGAS</td><td>2.3947</td><td>2.3932</td><td>2.4265</td><td><b>2.1089</b></td></tr><tr><td><br />Q4</td><td>0.8033</td><td>0.8036</td><td>0.7980</td><td><b>0.8182</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="152">图6给出2001年7月11日的真实<i>Landsat</i>影像和4种算法的预测结果 (图像以“近红外-红色-绿色”三波段进行组合构成) .对图像的右上角进行局部放大.由图可以明显看出, <i>SPSTFM</i>、<i>ESCBDL</i>和<i>VDSR</i>都产生较明显的白色噪声, 而<i>MDN</i>网络和<i>ELI</i>算法预测的图像未产生大片的白色噪声.相比真实图像, <i>SPSTFM</i>、<i>ESCBDL</i>预测的图像颜色有失真现象, 图像颜色偏红, 而本文预测的图像颜色更接近真实图像, 颜色偏紫.因此, 本文的高光谱图像融合算法对噪声具有更好的鲁棒性和更好的细节恢复效果.</p>
                </div>
                <div class="area_img" id="158">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905006_15800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 4种算法对真实图像的预测结果" src="Detail/GetImg?filename=images/MSSB201905006_15800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 4种算法对真实图像的预测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905006_15800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.6 <i>Predicted results comparison of</i> 4 <i>methods for ground</i> truth images </p>

                </div>
                <div class="area_img" id="158">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905006_15801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 4种算法对真实图像的预测结果" src="Detail/GetImg?filename=images/MSSB201905006_15801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 4种算法对真实图像的预测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905006_15801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.6 <i>Predicted results comparison of</i> 4 <i>methods for ground</i> truth images </p>

                </div>
                <h3 id="160" name="160" class="anchor-tag">3 结 束 语</h3>
                <div class="p1">
                    <p id="161">本文提出多输入的密集连接网络 (<i>MDN</i>) 和误差线性插值模型 (<i>ELI</i>) , 用于高光谱图像的时空融合问题.第一阶段将连续时刻的低空间分辨率<i>MODIS</i>影像同时输入到<i>MDN</i>网络, 学习相邻时刻之间地表覆盖物的变化信息.第二阶段利用<i>ELI</i>将第一阶段学到的2幅包含连续时刻间地表覆盖物信息的过渡图像与已知的2幅高分辨率<i>Landsat</i>影像进行融合, 得到最终中间时刻的<i>Landsat</i>影像.<i>MDN</i>+<i>ELI</i>融合模型较好地利用已知的3幅连续时刻低分辨率<i>MODIS</i>影像随时间变化的信息, 同时也结合已知的2幅高分辨率<i>Landsat</i>影像, 充分利用已知信息.实验的定量对比结果和预测图像的视觉效果表明, 本文算法的恢复效果明显优于基于稀疏字典学习的机器学习算法和单输入单输出的用于图像重建的深度卷积网络模型.遥感卫星影像具有多个波段, 今后会尝试利用多个波段的信息进行融合, 充分利用遥感卫星影像的特性.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="179" type="formula" href="images/MSSB201905006_17900.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">姚凯旋</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="181" type="formula" href="images/MSSB201905006_18100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">曹飞龙</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the blending of the landsat and MODIS surface reflectance: Predicting daily landsat surface reflectance">

                                <b>[1]</b> GAO F, MASEK J, SCHWALLER M, <i>et al</i>.On the Blending of the Landsat and MODIS Surface Reflectance:Predicting Daily Landsat Surface Reflectance.IEEE Transactions on Geoscience and Remote Sensing, 2006, 44 (8) :2207-2218.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600662630&amp;v=MDQ3MDNOcVk5Rll1ME5Dbjg1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZzU2J4TT1OaWZPZmJLN0h0RA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> ZHU X L, CHEN J, GAO F, <i>et al</i>.An Enhanced Spatial and Temporal Adaptive Reflectance Fusion Model for Complex Heterogeneous Regions.Remote Sensing of Environment, 2010, 114 (11) :2610-2623.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatiotemporal reflectance fusion via sparse representation">

                                <b>[3]</b> HUANG B, SONG H H.Spatiotemporal Reflectance Fusion via Spa-rse Representation.IEEE Transactions on Geoscience and Remote Sensing, 2012, 50 (10) :3707-3716.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An error-bound-regularized sparse coding for spatiotemporal reflectance fusion">

                                <b>[4]</b> WU B, HUANG B, ZHANG L P.An Error-Bound-Regularized Spa-rse Coding for Spatiotemporal Reflectance Fusion.IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (12) :6791-6803.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMD71033EF8EE149F894A1F173A0F662B0&amp;v=MTEyMTNORmh3TDI1eGFrPU5pZklZOGUvSDlIUHJQb3piSjU2RFhnd3VSNGE3azU4UG43bHIyTTFEN1NTUjhpZkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> KRIZHEVSKY A, SUTKEVER I, HINTON G E.ImageNet Classification with Deep Convolutional Neural Networks.Communications of the ACM, 2017, 60 (6) :84-90.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[6]</b> HE K M, ZHANG X Y, REN S Q, <i>et al</i>.Deep Residual Learning for Image Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[7]</b> GIRSHICK R, DONAHUE J, DARRELL T, <i>et al</i>.Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:580-587.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[8]</b> LONG J, SHELHAMER E, DARRELL T.Fully Convolutional Networks for Semantic Segmentation.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Network in Network[C/OL]">

                                <b>[9]</b> LIN M, CHEN Q, YAN S C.Network in Network[C/OL].[2019-01-15].https://arxiv.org/pdf/1312.4400.pdf.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going Deeper with Convolutions">

                                <b>[10]</b> SZEGEDY C, LIU W, JIA Y Q, <i>et al</i>.Going Deeper with Convolutions // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015.DOI:10.1109/CVPR.2015.7298594.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 DONG C, LOY C C, HE K M, <i>et al</i>.Image Super-Resolution Using Deep Convolutional Networks.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (2) :295-307.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accelerating the Super-Resolution Convolutional Neural Network[C/OL]">

                                <b>[12]</b> DONG C, LOY C C, TANG X O.Accelerating the Super-Resolution Convolutional Neural Network[C/OL].[2019-01-15].https://arxiv.org/pdf/1608.00367.pdf.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate image super-resolution using very deep convolutional networks">

                                <b>[13]</b> KIM J, LEE J K, LEE K M.Accurate Image Super-Resolution Using Very Deep Convolutional Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:1646-1654.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deeply-recursive convolutional network for image super-resolution">

                                <b>[14]</b> KIM J, LEE J K, LEE K M.Deeply-Recursive Convolutional Network for Image Super-Resolution // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:1637-1645.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep edge guided recurrent residual learning for image super-resolution">

                                <b>[15]</b> YANG W H, FENG J S, YANG J C, <i>et al</i>.Deep Edge Guided Recurrent Residual Learning for Image Super-Resolution.IEEE Transactions on Image Processing, 2017, 26 (12) :5895-5907.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coupled Deep Autoencoder for Single Image Super-Resolution">

                                <b>[16]</b> ZENG K, YU J, WANG R X, <i>et al</i>.Coupled Deep Autoencoder for Single Image Super-Resolution.IEEE Transactions on Cybernetics, 2017, 47 (1) :27-37.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coupled Dictionary Training for Image Super-Resolution">

                                <b>[17]</b> YANG J C, WANG Z W, LIN Z, <i>et al</i>.Coupled Dictionary Trai-ning for Image Super-Resolution.IEEE Transactions on Image Processing, 2012, 21 (8) :3467-3478.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On single image scale-up using sparse-representations">

                                <b>[18]</b> ZEYDE R, ELAD M, PROTTER M.On Single Image Scale-up Using Sparse-Representations // Proc of the International Confe-rence on Curves and Surfaces.Berlin, Germany:Springer, 2010:711-730.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single image interpolation via adaptive non-local sparsity-based modeling">

                                <b>[19]</b> ROMANO Y, PROTTER M, ELAD M.Single Image Interpolation via Adaptive Nonlocal Sparsity-Based Modeling.IEEE Transactions on Image Processing, 2014, 23 (7) :3085-3098.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Nonlocally Centralized Sparse Representation for Image Restoration">

                                <b>[20]</b> DONG W S, ZHANG L, SHI G M, <i>et al</i>.Nonlocally Centralized Sparse Representation for Image Restoration.IEEE Transactions on Image Processing, 2013, 22 (4) :1620-1630.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatiotemporal Satellite Image Fusion Using Deep Convolutional Neural Networks">

                                <b>[21]</b> SONG H H, LIU Q S, WANG G J, <i>et al</i>.Spatiotemporal Satellite Image Fusion Using Deep Convolutional Neural Networks.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2018, 11 (3) :821-829.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deconvolutional networks">

                                <b>[22]</b> ZEILER M D, KRISHNAN D, TAYLOR G W, <i>et al</i>.Deconvolutional Networks // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2010:2528-2535.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) [C/OL]">

                                <b>[23]</b> CLEVERT D, UNTERTHINER T, HOCHREITER S.Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) [C/OL].[2019-01-15].https://arxiv.org/pdf/1511.07289.pdf.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:A Method for Stochastic Optimization[C/OL]">

                                <b>[24]</b> KINGMA D P, BA J L.Adam:A Method for Stochastic Optimization[C/OL].[2019-01-15].https://arxiv.org/pdf/1412.6980.pdf.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding the difficulty of training deep feedforward neural networks">

                                <b>[25]</b> GLOROT X, BENGIO Y.Understanding the Difficulty of Training Deep Feedforward Neural Networks // Proc of the International Conference on Artificial Intelligence and Statistics.New York, USA:ACM, 2010:249-256.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300249404&amp;v=MDE0NTY2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGc1NieE09TmlmT2ZiSzlIOVBPckk5Rlp1OEdDSHc5b0JNVA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> GEVAERT C, GARCIA-HARO F.A Comparison of StarFM and an Unmixing-Based Algorithm for Landsat and MODIS Data Fusion.Remote Sensing of Environment, 2015, 156:34-44.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A global quality measurement of pan-sharpened multispectral imagery">

                                <b>[27]</b> ALPARONE L, BARONTI S, GARZELLI A, <i>et al</i>.A Global Qua-lity Measurement of Pan-Sharpened Multispectral Imagery.IEEE Geoscience and Remote Sensing Letters, 2004, 1 (4) :313-317.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201905006" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201905006&amp;v=MjM0NzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpnVmIzQUtEN1liTEc0SDlqTXFvOUZZb1E=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
