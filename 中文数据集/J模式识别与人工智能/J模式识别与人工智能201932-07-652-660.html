<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131453962217500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201907009%26RESULT%3d1%26SIGN%3dWLLIn4o%252fGjLneleP1mUKQRBemhk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201907009&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201907009&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201907009&amp;v=MTU0MjBLRDdZYkxHNEg5ak1xSTlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZMM1A=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#114" data-title="图1 低秩约束先验与自编码结合的结构示意图">图1 低秩约束先验与自编码结合的结构示意图</a></li>
                                                <li><a href="#202" data-title="表1 实验数据集">表1 实验数据集</a></li>
                                                <li><a href="#213" data-title="表2 6种算法在5个数据集上的实验结果对比">表2 6种算法在5个数据集上的实验结果对比</a></li>
                                                <li><a href="#224" data-title="表3 不同低秩约束先验的算法性能对比">表3 不同低秩约束先验的算法性能对比</a></li>
                                                <li><a href="#228" data-title="图2 M的选取及维度对算法性能的影响">图2 M的选取及维度对算法性能的影响</a></li>
                                                <li><a href="#235" data-title="图3 3种算法在MNIST数据集上的结果对比">图3 3种算法在MNIST数据集上的结果对比</a></li>
                                                <li><a href="#235" data-title="图3 3种算法在MNIST数据集上的结果对比">图3 3种算法在MNIST数据集上的结果对比</a></li>
                                                <li><a href="#240" data-title="图4 3种算法在COIL20数据集上的结果对比">图4 3种算法在COIL20数据集上的结果对比</a></li>
                                                <li><a href="#240" data-title="图4 3种算法在COIL20数据集上的结果对比">图4 3种算法在COIL20数据集上的结果对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="285">


                                    <a id="bibliography_1" title=" ZHANG C Q, FU H Z, LIU S, et al.Low-Rank Tensor Constrained Multiview Subspace Clustering // Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:1582-1590." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Low-Rank Tensor Constrained Multiview Subspace Clustering">
                                        <b>[1]</b>
                                         ZHANG C Q, FU H Z, LIU S, et al.Low-Rank Tensor Constrained Multiview Subspace Clustering // Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:1582-1590.
                                    </a>
                                </li>
                                <li id="287">


                                    <a id="bibliography_2" title=" PENG X, XIAO S J, FENG J S, et al.Deep Subspace Clustering with Sparsity Prior // Proc of the 25th International Joint Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2016:1925-1931." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep subspace clustering with sparsity prior">
                                        <b>[2]</b>
                                         PENG X, XIAO S J, FENG J S, et al.Deep Subspace Clustering with Sparsity Prior // Proc of the 25th International Joint Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2016:1925-1931.
                                    </a>
                                </li>
                                <li id="289">


                                    <a id="bibliography_3" title=" YIN M, WU Z Z, ZENG D Y, et al.Sparse Subspace Clustering with Jointly Learning Representation and Affinity Matrix.Journal of the Franklin Institute, 2018, 355 (8) :3795-3811." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESD5DAC7C0EF6C3E14CE6F37D5668662AD&amp;v=MTQzNjZ4RkVaMEpmMzlNemhKZ256a0xTM2lXcVJRemNiU1NSOHZyQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0x5NHdxOD1OaWZPZmNlOWFxQy9xUA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         YIN M, WU Z Z, ZENG D Y, et al.Sparse Subspace Clustering with Jointly Learning Representation and Affinity Matrix.Journal of the Franklin Institute, 2018, 355 (8) :3795-3811.
                                    </a>
                                </li>
                                <li id="291">


                                    <a id="bibliography_4" title=" LIU G C, LIN Z C, YAN S C, et al.Robust Recovery of Subspace Structures by Low-Rank Representation.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (1) :171-184." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust Recovery of Subspace Structures by Low-Rank Representation">
                                        <b>[4]</b>
                                         LIU G C, LIN Z C, YAN S C, et al.Robust Recovery of Subspace Structures by Low-Rank Representation.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (1) :171-184.
                                    </a>
                                </li>
                                <li id="293">


                                    <a id="bibliography_5" title=" ZHANG M M, LI W, DU Q.Joint Low Rank and Sparse Representation-Based Hyperspectral Image Classification // Proc of the 8th Workshop on Hyperspectral Image and Signal Processing:Evolution in Remote Sensing.Washington, USA:IEEE, 2016.DOI:10.1109/WHISPERS.2016.8071748." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint Low Rank and Sparse Representation-Based Hyperspectral Image Classification">
                                        <b>[5]</b>
                                         ZHANG M M, LI W, DU Q.Joint Low Rank and Sparse Representation-Based Hyperspectral Image Classification // Proc of the 8th Workshop on Hyperspectral Image and Signal Processing:Evolution in Remote Sensing.Washington, USA:IEEE, 2016.DOI:10.1109/WHISPERS.2016.8071748.
                                    </a>
                                </li>
                                <li id="295">


                                    <a id="bibliography_6" title=" YAO C, HAN J W, NIE F P, et al.Local Regression and Global Information-Embedded Dimension Reduction.IEEE Transactions on Neural Networks and Learning Systems, 2018, 29 (10) :4882-4893." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Local Regression and Global Information-Embedded Dimension Reduction">
                                        <b>[6]</b>
                                         YAO C, HAN J W, NIE F P, et al.Local Regression and Global Information-Embedded Dimension Reduction.IEEE Transactions on Neural Networks and Learning Systems, 2018, 29 (10) :4882-4893.
                                    </a>
                                </li>
                                <li id="297">


                                    <a id="bibliography_7" title=" 谈超, 关佶红, 周水庚.增量与演化流形学习综述.智能系统学报, 2012, 7 (5) :377-388. (TAN C, GUAN J H, ZHOU S G.Incremental and Evolutionary Manifold Learning:A Survey.CAAI Transactions on Intelligent Systems, 2012, 7 (5) :377-388.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZNXT201205002&amp;v=MjYwMTh0R0ZyQ1VSTE9lWmVSbkZ5emhWTDNQUHlQVGVyRzRIOVBNcW85RlpvUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         谈超, 关佶红, 周水庚.增量与演化流形学习综述.智能系统学报, 2012, 7 (5) :377-388. (TAN C, GUAN J H, ZHOU S G.Incremental and Evolutionary Manifold Learning:A Survey.CAAI Transactions on Intelligent Systems, 2012, 7 (5) :377-388.) 
                                    </a>
                                </li>
                                <li id="299">


                                    <a id="bibliography_8" title=" TANG K W, SU Z X, JIANG W, et al.Robust Subspace Learning-Based Low-Rank Representation for Manifold Clustering.Neural Computing and Applications, 2018.DOI:https://doi.org/10.1007." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust Subspace Learning-Based Low-Rank Representation for Manifold Clustering">
                                        <b>[8]</b>
                                         TANG K W, SU Z X, JIANG W, et al.Robust Subspace Learning-Based Low-Rank Representation for Manifold Clustering.Neural Computing and Applications, 2018.DOI:https://doi.org/10.1007.
                                    </a>
                                </li>
                                <li id="301">


                                    <a id="bibliography_9" title=" DONG Y X, YANG C Z.Cluster-Based Least Absolute Deviation Regression for Dimension Reduction.Journal of Statistical Theory and Practice, 2016, 10 (1) :121-132." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJDC54F6F1ED23E0D945A172C08FD7B8D5C&amp;v=MTk5NzYyWTR3RU9rTWVYeE54aElXbXo1NlNnemlwR1JCZnNDY01iL3NDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3THk0d3E4PU5qbkJhc0M5R3FmSw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         DONG Y X, YANG C Z.Cluster-Based Least Absolute Deviation Regression for Dimension Reduction.Journal of Statistical Theory and Practice, 2016, 10 (1) :121-132.
                                    </a>
                                </li>
                                <li id="303">


                                    <a id="bibliography_10" title=" JIANG Z, ZHENG Y, TAN H C, et al.Variational Deep Embe-dding:A Generative Approach to Clustering // Proc of the 26th International Joint Conference on Artificial Intelligence.Washington, USA:IEEE, 2017:1965-1972." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Variational Deep Embe-dding:A Generative Approach to Clustering">
                                        <b>[10]</b>
                                         JIANG Z, ZHENG Y, TAN H C, et al.Variational Deep Embe-dding:A Generative Approach to Clustering // Proc of the 26th International Joint Conference on Artificial Intelligence.Washington, USA:IEEE, 2017:1965-1972.
                                    </a>
                                </li>
                                <li id="305">


                                    <a id="bibliography_11" title=" YANG B, FU X, SIDIROPOULOS N D, et al.Towards K-means-Friendly Spaces:Simultaneous Deep Learning and Clustering // Proc of the 34th International Conference on Machine Learning.New York, USA:ACM, 2017:3861-3870." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Towards k-meansfriendly spaces:simultaneous deep learning and clustering&amp;quot;">
                                        <b>[11]</b>
                                         YANG B, FU X, SIDIROPOULOS N D, et al.Towards K-means-Friendly Spaces:Simultaneous Deep Learning and Clustering // Proc of the 34th International Conference on Machine Learning.New York, USA:ACM, 2017:3861-3870.
                                    </a>
                                </li>
                                <li id="307">


                                    <a id="bibliography_12" title=" 黄健航, 雷迎科.基于边际Fisher深度自编码器的电台指纹特征提取.模式识别与人工智能, 2017, 30 (11) :1030-1038. (HUANG J H, LEI Y K.Radio Fingerprint Extraction Based on Marginal Fisher Deep Autoencoder.Pattern Recognition and Artificial Intelligence, 2017, 30 (11) :1030-1038.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201711009&amp;v=MDEyMDVMRzRIOWJOcm85RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emhWTDNQS0Q3WWI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         黄健航, 雷迎科.基于边际Fisher深度自编码器的电台指纹特征提取.模式识别与人工智能, 2017, 30 (11) :1030-1038. (HUANG J H, LEI Y K.Radio Fingerprint Extraction Based on Marginal Fisher Deep Autoencoder.Pattern Recognition and Artificial Intelligence, 2017, 30 (11) :1030-1038.) 
                                    </a>
                                </li>
                                <li id="309">


                                    <a id="bibliography_13" title=" CARON M, BOJANOWSKI P, JOULIN A, et al.Deep Clustering for Unsupervised Learning of Visual Features // Proc of the 15th European Conference on Computer Vision.Berlin, Germany:Springer, 2018:139-156." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Clustering for Unsupervised Learning of Visual Features">
                                        <b>[13]</b>
                                         CARON M, BOJANOWSKI P, JOULIN A, et al.Deep Clustering for Unsupervised Learning of Visual Features // Proc of the 15th European Conference on Computer Vision.Berlin, Germany:Springer, 2018:139-156.
                                    </a>
                                </li>
                                <li id="311">


                                    <a id="bibliography_14" title=" PENG X, FENG J S, XIAO S J, et al.Structured AutoEncoders for Subspace Clustering.IEEE Transactions on Image Processing, 2018, 27 (10) :5076-5086." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structured AutoEncoders for Subspace Clustering">
                                        <b>[14]</b>
                                         PENG X, FENG J S, XIAO S J, et al.Structured AutoEncoders for Subspace Clustering.IEEE Transactions on Image Processing, 2018, 27 (10) :5076-5086.
                                    </a>
                                </li>
                                <li id="313">


                                    <a id="bibliography_15" title=" ZHAO J B, MATHIEU M, GOROSHIN R, et al.Stacked What-Where Auto-Encoders.Computer Science, 2015, 15 (1) :3563-3593." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked What-Where Auto-encoders">
                                        <b>[15]</b>
                                         ZHAO J B, MATHIEU M, GOROSHIN R, et al.Stacked What-Where Auto-Encoders.Computer Science, 2015, 15 (1) :3563-3593.
                                    </a>
                                </li>
                                <li id="315">


                                    <a id="bibliography_16" title=" DIZAJI K G, HERANDI A, DENG C, et al.Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization // Proc of the 2th International Conference on Computer Vision.Washington, USA:IEEE, 2017:5747-5756." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization">
                                        <b>[16]</b>
                                         DIZAJI K G, HERANDI A, DENG C, et al.Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization // Proc of the 2th International Conference on Computer Vision.Washington, USA:IEEE, 2017:5747-5756.
                                    </a>
                                </li>
                                <li id="317">


                                    <a id="bibliography_17" title=" YANG J W, PARIKH D, BATRA D.Joint Unsupervised Learning of Deep Representations and Image Clusters // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:1354-1362." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint unsupervised learning of deep repre-sentations and image clusters">
                                        <b>[17]</b>
                                         YANG J W, PARIKH D, BATRA D.Joint Unsupervised Learning of Deep Representations and Image Clusters // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:1354-1362.
                                    </a>
                                </li>
                                <li id="319">


                                    <a id="bibliography_18" title=" CHEN Y Y, ZHANG L, ZHANG Y.A Novel Low Rank Representation Algorithm for Subspace Clustering.International Journal of Pattern Recognition and Artificial Intelligence, 2016, 30 (4) .DOI:10.1142/S0218001416500075." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel low rank representation algorithm for subspace clustering">
                                        <b>[18]</b>
                                         CHEN Y Y, ZHANG L, ZHANG Y.A Novel Low Rank Representation Algorithm for Subspace Clustering.International Journal of Pattern Recognition and Artificial Intelligence, 2016, 30 (4) .DOI:10.1142/S0218001416500075.
                                    </a>
                                </li>
                                <li id="321">


                                    <a id="bibliography_19" title=" 张涛, 唐振民, 吕建勇.一种基于低秩表示的子空间聚类改进算法.电子与信息学报, 2016, 38 (11) :2811-2818. (ZHANG T, TANG Z M, L&#220; J Y.Improved Algorithm Based on Low Rank Representation for Subspace Clustering.Journal of Electronics and Information Technology, 2016, 38 (11) :2811-2818.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201611016&amp;v=MDIzMTE0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpoVkwzUElUZlNkckc0SDlmTnJvOUVZb1FLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         张涛, 唐振民, 吕建勇.一种基于低秩表示的子空间聚类改进算法.电子与信息学报, 2016, 38 (11) :2811-2818. (ZHANG T, TANG Z M, L&#220; J Y.Improved Algorithm Based on Low Rank Representation for Subspace Clustering.Journal of Electronics and Information Technology, 2016, 38 (11) :2811-2818.) 
                                    </a>
                                </li>
                                <li id="323">


                                    <a id="bibliography_20" title=" FAVARO P, VIDAL R, RAVICHANDRAN A.A Closed form Solution to Robust Subspace Estimation and Clustering // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2011:1801-1807." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Closed Form Solution to Robust Subspace Estimation and Clustering">
                                        <b>[20]</b>
                                         FAVARO P, VIDAL R, RAVICHANDRAN A.A Closed form Solution to Robust Subspace Estimation and Clustering // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2011:1801-1807.
                                    </a>
                                </li>
                                <li id="325">


                                    <a id="bibliography_21" title=" GUO X F, GAO L, LIU X W, et al.Improved Deep Embedded Clustering with Local Structure Preservation // Proc of the 26th International Joint Conference on Artificial Intelligence.Washington, USA:IEEE, 2017:1753-1759." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved deep embedded clustering with local structure preservation">
                                        <b>[21]</b>
                                         GUO X F, GAO L, LIU X W, et al.Improved Deep Embedded Clustering with Local Structure Preservation // Proc of the 26th International Joint Conference on Artificial Intelligence.Washington, USA:IEEE, 2017:1753-1759.
                                    </a>
                                </li>
                                <li id="327">


                                    <a id="bibliography_22" title=" ELHAMIFAR E, VIDAL R.Sparse Subspace Clustering:Algorithm, Theory, and Applications.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (11) :2765-2781." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparse subspace clustering:Algorithm,theory,and applications">
                                        <b>[22]</b>
                                         ELHAMIFAR E, VIDAL R.Sparse Subspace Clustering:Algorithm, Theory, and Applications.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (11) :2765-2781.
                                    </a>
                                </li>
                                <li id="329">


                                    <a id="bibliography_23" title=" TIAN F, GAO B, CUI Q, et al.Learning Deep Representations for Graph Clustering // Proc of the 28th AAAI Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2014:1293-1299." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Deep Representations for Graph Clustering">
                                        <b>[23]</b>
                                         TIAN F, GAO B, CUI Q, et al.Learning Deep Representations for Graph Clustering // Proc of the 28th AAAI Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2014:1293-1299.
                                    </a>
                                </li>
                                <li id="331">


                                    <a id="bibliography_24" title=" LU C Y, MIN H, ZHAO Z Q, et al.Robust and Efficient Subspace Segmentation via Least Squares Regression // Proc of the 12th European Conference on Computer Vision.Berlin, Germany:Springer, 2012:347-360." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust and efficient subspace segmentation via least squares regression">
                                        <b>[24]</b>
                                         LU C Y, MIN H, ZHAO Z Q, et al.Robust and Efficient Subspace Segmentation via Least Squares Regression // Proc of the 12th European Conference on Computer Vision.Berlin, Germany:Springer, 2012:347-360.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(07),652-660 DOI:10.16451/j.cnki.issn1003-6059.201907009            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>引入低秩约束先验的深度子空间聚类</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%95%8F&amp;code=07770627&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张敏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E6%B2%BB%E5%B9%B3&amp;code=07760241&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周治平</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E5%8D%97%E5%A4%A7%E5%AD%A6%E7%89%A9%E8%81%94%E7%BD%91%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0074200&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江南大学物联网工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E5%8D%97%E5%A4%A7%E5%AD%A6%E7%89%A9%E8%81%94%E7%BD%91%E6%8A%80%E6%9C%AF%E5%BA%94%E7%94%A8%E6%95%99%E8%82%B2%E9%83%A8%E5%B7%A5%E7%A8%8B%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江南大学物联网技术应用教育部工程研究中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>大多数子空间聚类算法将高维数据映射到低维子空间时不能较好捕获数据间几何结构.针对上述问题, 文中提出引入低秩约束先验的深度子空间聚类算法, 兼顾数据全局和局部结构信息.算法结合低秩表示与深度自编码器, 利用低秩约束捕获数据全局结构, 并将约束神经网络的潜在特征表示为低秩.自编码通过最小化重构误差进行非线性低维子空间映射, 保留数据的局部特性.以多元逻辑回归函数作为判别模型, 预测子空间分割.整个算法在无监督联合学习框架下进行优化.在5个数据集上的实验验证文中方法的有效性.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BD%8E%E7%A7%A9%E7%BA%A6%E6%9D%9F%E5%85%88%E9%AA%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">低秩约束先验;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自编码器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">归一化层;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%94%E5%90%88%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">联合学习框架;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张敏, 硕士研究生, 主要研究方向为聚类分析、模式识别.E-mail:m15061882373_1@163.com.;
                                </span>
                                <span>
                                    *周治平 (通讯作者) , 博士, 教授, 主要研究方向为检测技术、自动化装置等.E-mail:zzp@jiangnan.edu.cn.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-25</p>

            </div>
                    <h1><b>Deep Subspace Clustering with Low Rank Constrained Prior</b></h1>
                    <h2>
                    <span>ZHANG Min</span>
                    <span>ZHOU Zhiping</span>
            </h2>
                    <h2>
                    <span>School of Internet of Things Engineering, Jiangnan University</span>
                    <span>Engineering Research Center of Internet of Things Technology Applications, Ministry of Education, Jiangnan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Most subspace clustering methods cannot capture geometric structures of data effetively while mapping high-dimensional data into a low-dimensional subspace. Aiming at this problem, a deep subspace clustering algorithm with low rank constrained prior (DSC-LRC) is proposed, maintaining both global and local structure information. Low-rank representation (LRR) is combined with depth autoencoder, global structures of data are captured by low rank constraint, and potential characteristics of constrained neural network are represented as low rank. Data are nonlinearly mapped into a latent space by minimizing differences between reconstructions and inputs with the local features of the data maintained. Multivariate logistic regression function is considered as a discriminant model to predict subspace segmentation. Parameters updating and clustering performance optimization are conducted in an unsupervised joint learning framework. Experiments on five datasets validate the effectiveness of DSC-LRC.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Low%20Rank%20Constrained%20Prior&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Low Rank Constrained Prior;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Autoencoder&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Autoencoder;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Soft-Max%20Layer&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Soft-Max Layer;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Joint%20Learning%20Framework&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Joint Learning Framework;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Min, master student. Her research interests include clustering analysis and pattern recognition.;
                                </span>
                                <span>
                                    ZHOU Zhiping ( Corresponding author ) , Ph. D. , professor. His research interests include detection technology and automatic device.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-01-25</p>
                            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="63">子空间聚类的目的是寻求一组隐式的低维子空间, 用于拟合未标注的高维数据, 并根据相应的结构关系划分这些子空间.子空间聚类目前已广泛应用于数据分析及可视化应用中, 如图像和视频中不同对象的分割、混合语音的分离等.子空间聚类与基于谱图划分的谱聚类结合的子空间分割方法作为处理高维数据的重要分支<citation id="333" type="reference"><link href="285" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 在图像分割、人脸识别等方面得到较好的效果.</p>
                </div>
                <div class="p1">
                    <p id="64">基于谱聚类的子空间聚类算法往往需要对自身数据字典构造亲和矩阵, 充分利用数据点间相似性, 进行样本自表达.Peng等<citation id="334" type="reference"><link href="287" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出深度稀疏子空间聚类算法, 通过在中间层引入稀疏约束先验, 整个数据集保持稀疏重构关系.Yin等<citation id="335" type="reference"><link href="289" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出在无监督联合学习框架下, 同时学习亲和矩阵和数据的稀疏自表达.上述算法提到的稀疏子空间聚类 (Sparse Sub-space Clustering, SSC) 旨在获得每个样本的稀疏表示, 却忽略数据全局结构.为了保留数据的全局特性, 低秩表示 (Low-Rank Representation, LRR) 被广泛应用以求得亲和矩阵.Liu等<citation id="336" type="reference"><link href="291" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>通过寻求数据样本最低秩表示检测离群值及划分样本至对应子空间中.Zhang等<citation id="337" type="reference"><link href="293" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>结合秩最小化与稀疏表示, 提出基于联合低秩稀疏表示的分类方法, 应用于高光谱图像分类.这些方法大多力求构建高质量的亲和矩阵, 却忽略亲和矩阵低维表示的重要性.</p>
                </div>
                <div class="p1">
                    <p id="65">现阶段数据呈现高维度特征, 在进行子空间分割时, 容易引发“维度灾难”.为了解决线性降维方法不利于应对原始数据缺失损坏的情况, Yao等<citation id="338" type="reference"><link href="295" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出无监督降维方法, 通过保留局部数据的主分量, 消除相关噪声, 再采用线性回归模型捕获局部几何结构.文献<citation id="339" type="reference">[<a class="sup">7</a>]</citation>和文献<citation id="340" type="reference">[<a class="sup">8</a>]</citation>基于数据均匀采样于一个高维欧氏空间中低维流形的假设, 将高维数据样本映射到低维子流形体中, 实现降维目的.但是, 流形学习忽略数据全局几何结构<citation id="341" type="reference"><link href="301" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 在现实应用中, 学习数据特征需要同时考虑局部特性及全局特性, 现有的算法在建模时往往不能兼顾.</p>
                </div>
                <div class="p1">
                    <p id="66">随着深度学习在机器学习的各种应用的不断深化<citation id="342" type="reference"><link href="303" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 深度自编码器成为目前较通用的数据特征表示方法, 通过多层神经网络将原始数据编码成低维数据, 再进行解码重构数据.与流形学习不同, 自编码器并行执行, 较好保留数据分布的局部特性.Yang等<citation id="343" type="reference"><link href="305" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>通过学习深度神经网络进行降维操作, 将神经网络近似成任意非线性函数, 提高聚类性能.黄健航等<citation id="344" type="reference"><link href="307" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>将自编码器应用于电台指纹特征提取, 采用边际Fisher深度自编码器算法, 准确表示电台个体非线性特征.Caron等<citation id="345" type="reference"><link href="309" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出用于大规模卷积神经网络端到端训练的聚类方法, 聚类中心和卷积神经网络权值更新交替进行.此外, 自编码器与样本自表示的结合也得到一定发展.Peng等<citation id="346" type="reference"><link href="311" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>通过引入结构自编码器, 学习一组显示转换, 结合先验信息, 将输入数据点逐步映射到非线性潜在空间.上述算法在一定程度上考虑保留数据集全局及局部结构信息, 但仍不能有效描述数据集的局部类间相似性和全局的几何结构.</p>
                </div>
                <div class="p1">
                    <p id="67">因此, 本文结合低秩表示与深度自编码器, 提出引入低秩约束先验的无监督深度子空间聚类算法 (Deep Subspace Clustering with Low Rank Constrained Prior, DSC-LRC) , 便于数据集获得良好的样本自表达, 从而作为深度自编码器的输入, 学习到数据的良好特征表示.在自编码器训练过程中, 需要先利用重构损耗对编码器和译码器参数进行分层预训练, 然后微调编码器参数, 但微调环节会覆盖前面所得最优参数<citation id="347" type="reference"><link href="313" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>.因此, 参考Dizaji等<citation id="348" type="reference"><link href="315" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出的联合学习框架, 联立聚类函数和辅助重构误差函数, 引入归一化层, 同时训练所有网络层, 进行聚类和参数更新.另一方面, 结合无监督学习与聚类, 解决先验知识匮乏及高成本人工标注样本的问题, 使算法具有较强的扩展性<citation id="349" type="reference"><link href="317" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>.总之, 本文算法结合自编码器与低秩表示, 通过自编码器提供非线性判别子空间, 同时引入低秩约束先验寻求整个数据集的低秩表示, 充分考虑数据的全局信息和局部信息.通过归一化层的引入, 以概率形式预测相关簇划分, 在无监督联合学习框架中进行参数更新和聚类优化.</p>
                </div>
                <div class="p1">
                    <p id="68">1 相关工作</p>
                </div>
                <div class="p1">
                    <p id="69"><b>1.1 子空间聚类和低秩表示</b></p>
                </div>
                <div class="p1">
                    <p id="70">假设数据矩阵</p>
                </div>
                <div class="p1">
                    <p id="71"><b><i>X</i></b>=[<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>n</i></sub>]∈<b>R</b><sup><i>d</i>×<i>n</i></sup></p>
                </div>
                <div class="p1">
                    <p id="72">包含<i>n</i>个向量, 从<i>K</i>个不同线性子空间提取.子空间聚类的任务是求解子空间数<i>K</i>并将数据向量<b><i>x</i></b><sub><i>i</i></sub>划分至相应子空间中.在基于谱聚类的子空间聚类中, 基本思想是将数据矩阵<b><i>X</i></b>表示为字典矩阵<b><i>A</i></b>下的线性组合<citation id="350" type="reference"><link href="319" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="73"><b><i>X</i></b>=<b><i>AC</i></b>.      (1) </p>
                </div>
                <div class="p1">
                    <p id="74"><b><i>C</i></b>=[<b><i>c</i></b><sub>1</sub>, <b><i>c</i></b><sub>2</sub>, …, <b><i>c</i></b><sub><i>n</i></sub>]为系数矩阵.相应地, <b><i>X</i></b>中的每个列向量<b><i>x</i></b><sub><i>i</i></sub>=<b><i>Ac</i></b><sub><i>i</i></sub>.构造优化框架求解得到系数矩阵<b><i>C</i></b>后, 通过<mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">C</mi><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">C</mi><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow><mo>|</mo></mrow></mrow></math></mathml>构造亲和矩阵<b><i>F</i></b>, 用于计算拉普拉斯矩阵<b><i>L</i></b>:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">L</mi><mo>=</mo><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mrow><mi mathvariant="bold-italic">F</mi><mi mathvariant="bold-italic">D</mi></mrow><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">其中</p>
                </div>
                <div class="p1">
                    <p id="78"><b><i>D</i></b><sub><b><i>ii</i></b></sub>=∑<sub><b><i>j</i></b></sub><b><i>F</i></b><sub><b><i>ij</i></b></sub>, </p>
                </div>
                <div class="p1">
                    <p id="79"><b><i>F</i></b><sub><b><i>ij</i></b></sub>为亲和矩阵<b><i>F</i></b>中第<i>i</i>行第<i>j</i>列元素.大部分子空间聚类算法均是采用主成分分析 (Principal Component Analysis, PCA) 获取拉普拉斯矩阵<b><i>L</i></b>的特征向量, 再结合经典聚类算法, 如<i>K</i>-means算法, 获得相关子空间分割.</p>
                </div>
                <div class="p1">
                    <p id="80">低秩表示在非监督子空间划分中引起广泛关注, 其将矩阵的秩函数凸松弛为核范数, 通过求解核范数最小化问题, 求得基于低秩表示的亲和矩阵.在式 (1) 基础上, <b><i>X</i></b>的低秩输出是通过最低秩化<b><i>C</i></b>实现:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">C</mi></munder><mspace width="0.25em" /><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>k</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">C</mi><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mspace width="0.25em" /><mi mathvariant="bold-italic">X</mi><mo>=</mo><mi mathvariant="bold-italic">A</mi><mi mathvariant="bold-italic">C</mi><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">低秩表示的基本模型为</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable><mtr><mtd columnalign="left"><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">C</mi></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">C</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mo>*</mo></msub><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">E</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mi>l</mi></msub><mo>, </mo></mtd></mtr><mtr><mtd columnalign="left"><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mi mathvariant="bold-italic">X</mi><mo>=</mo><mi mathvariant="bold-italic">A</mi><mi mathvariant="bold-italic">C</mi><mo>+</mo><mi mathvariant="bold-italic">E</mi><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中:‖<b><i>C</i></b>‖<sub>*</sub>表示矩阵<b><i>C</i></b>的核范数, 即<b><i>C</i></b>的奇异值之和;<b><i>E</i></b>为噪声项, ‖·‖<sub><i>l</i></sub>表示<i>l</i>范数;<i>λ</i>&gt;0用于平衡两项.模型一般采用增 广 拉 格 朗 日 乘 子 法 (Aug-mented Lagrange Method, ALM) 求解上述凸优化问题, 获得最优解<b><i>C</i></b><sup>*</sup><citation id="351" type="reference"><link href="321" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>.此外, 文献<citation id="352" type="reference">[<a class="sup">20</a>]</citation>采用奇异值分解的方法, 在ALM框架下求解封闭解.最优解</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">C</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>C</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mtd><mtd><mn>0</mn></mtd><mtd><mo>⋯</mo></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mi>C</mi><msubsup><mrow></mrow><mn>2</mn><mo>*</mo></msubsup></mtd><mtd><mo>⋯</mo></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>C</mi><msubsup><mrow></mrow><mi>Κ</mi><mo>*</mo></msubsup></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">为分块对角矩阵, 描述类内密相似及类间零相似.</p>
                </div>
                <div class="p1">
                    <p id="88"><b>1.2 自编码器</b></p>
                </div>
                <div class="p1">
                    <p id="89">自编码器的提出旨在解决反向传播无引导问题, 可在无监督环境下自动从特定数据集中获取信息特征.自编码器本质上可作为特征提取器, 对输入的编码及解码重构数据<citation id="353" type="reference"><link href="303" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>.近年来, 自编码器逐渐与深层架构结合, 以无监督的方式通过神经网络的堆叠及多层训练进行参数优化.假定</p>
                </div>
                <div class="p1">
                    <p id="90"><b><i>X</i></b>=[<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>n</i></sub>]∈<b>R</b><sup><i>d</i>×<i>n</i></sup></p>
                </div>
                <div class="p1">
                    <p id="91">为原始输入数据, <b><i>W</i></b><sup> (1) </sup>和<i>b</i><sup> (1) </sup>、<b><i>W</i></b><sup> (2) </sup>和<i>b</i><sup> (2) </sup>分别为第1、2层编码层的权重和偏移量, <i>f</i><sub>1</sub>、<i>f</i><sub>2</sub>为对应层的激活函数, 前一层的输出作为后一层的输入, 则每个样本<b><i>x</i></b><sub><i>i</i></sub>∈<b>R</b><sup><i>d</i></sup>的第2层的输出</p>
                </div>
                <div class="p1">
                    <p id="92"><i>h</i><mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>=<i>f</i><sub>2</sub> (<b><i>W</i></b><sup> (2) </sup>·<i>f</i><sub>1</sub> (<b><i>W</i></b><sup> (1) </sup>·<b><i>x</i></b><sub><i>i</i></sub>+<i>b</i><sup> (1) </sup>) +<i>b</i><sup> (2) </sup>) .</p>
                </div>
                <div class="p1">
                    <p id="94">在通常情况下, 激活函数<i>f</i><sub><i>m</i></sub> (·) 将第<i>m</i>层输出非线性映射到下一层.不同于三层架构的神经网络, 自编码器包含解码过程, 输出<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>为输入<b><i>x</i></b><sub><i>i</i></sub>的数据重构, 具有相同维度.</p>
                </div>
                <div class="p1">
                    <p id="96">2 引入低秩约束先验的深度子空间聚类</p>
                </div>
                <div class="p1">
                    <p id="97"><b>2.1 深度嵌入模型</b></p>
                </div>
                <div class="p1">
                    <p id="98">如图1所示, 本文算法由多层神经网络构成, 假定模型包含<i>M</i>+1层 (<i>M</i>为偶数) , 其中, 前<i>M</i>/2层作为编码层, 用于学习潜在特征表示, 即降维, 后<i>M</i>/2层作为解码层, 逐层重构原始输入.给定聚类任务为含<i>n</i>个样本的数据集:</p>
                </div>
                <div class="p1">
                    <p id="99"><b><i>X</i></b>=[<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>n</i></sub>]∈<b>R</b><sup><i>d</i>×<i>n</i></sup>, </p>
                </div>
                <div class="p1">
                    <p id="100">将其划分成<i>K</i>个子空间, 每个样本<b><i>x</i></b><sub><i>i</i></sub>∈<b>R</b><sup><i>d</i></sup>, 激活函数<i>f</i> (·) ∶<b><i>X</i></b>→<b><i>H</i></b>, 在本文中, </p>
                </div>
                <div class="area_img" id="101">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201907009_10100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="102"><image id="280" type="formula" href="images/MSSB201907009_28000.jpg" display="inline" placement="inline"><alt></alt></image>表示样本.<b><i>x</i></b><sub><i>i</i></sub>输入到网络第一层, <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mfrac><mi>Μ</mi><mn>2</mn></mfrac><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>表示编码层对<b><i>x</i></b><sub><i>i</i></sub>的低维特征表示, 且<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mrow><mfrac><mi>Μ</mi><mn>2</mn></mfrac></mrow></msub><mo>≪</mo><mi>d</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>d</mi><msub><mrow></mrow><mi>m</mi></msub></mrow></math></mathml>表示第<i>m</i>层输出的维度 (<i>m</i>为网络层的索引, 取值为1, 2, …, <i>M</i>) .由1.2节可知, 样本<b><i>x</i></b><sub><i>i</i></sub>第<i>m</i>层输出</p>
                </div>
                <div class="p1">
                    <p id="105"><b><i>h</i></b><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>=<i>f</i> (<b><i>W</i></b><sup> (<i>m</i>) </sup><b><i>h</i></b><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>+<i>b</i><sup> (<i>m</i>) </sup>) ∈<b>R</b><sup><i>d</i><sub><i>m</i></sub></sup>,      (2) </p>
                </div>
                <div class="p1">
                    <p id="108">其中<b><i>W</i></b><sup> (<i>m</i>) </sup>和<i>b</i><sup> (<i>m</i>) </sup> 表示第<i>m</i>层网络的权重和偏移量.</p>
                </div>
                <div class="p1">
                    <p id="109">模型最终输出</p>
                </div>
                <div class="p1">
                    <p id="110"><b><i>H</i></b><sup> (<i>M</i>) </sup>=[<b><i>h</i></b><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>Μ</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, <b><i>h</i></b><mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false"> (</mo><mi>Μ</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, …, <b><i>h</i></b><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false"> (</mo><mi>Μ</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>].</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907009_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 低秩约束先验与自编码结合的结构示意图" src="Detail/GetImg?filename=images/MSSB201907009_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 低秩约束先验与自编码结合的结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907009_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Structure of combination of low-rank constraint prior and autoencoder</p>

                </div>
                <div class="p1">
                    <p id="116">训练模型的目标是通过优化网络参数达到最小化重构误差函数的目的.通过对网络施加约束, 如限制神经元的数量或权重大小, 容易挖掘数据的潜在结构.相比自编码器, 本文模型在网络结构上施加更多的限制, 不仅通过非监督学习最小化重构误差, 而且引入低秩约束先验, 联合计算整个数据集的低秩表示最优解<b><i>C</i></b><sup>*</sup> (为了使公式表达简洁, 下文表述的<b><i>C</i></b>均指求解获得的最优解) 及相应的拉普拉斯矩阵<b><i>L</i></b>, 将<b><i>L</i></b>作为自编码器的输入, 获得低维的数据特征表示.拉普拉斯矩阵<b><i>L</i></b>是从数据点的亲和矩阵<b><i>F</i></b>中推导的反映点之间关系的矩阵, 在文献<citation id="354" type="reference">[<a class="sup">2</a>]</citation>和文献<citation id="355" type="reference">[<a class="sup">14</a>]</citation>中均提及为了避免亲和矩阵<b><i>F</i></b>的构造会影响算法性能及降低时间复杂度, 将拉普拉斯矩阵<b><i>L</i></b>定义为</p>
                </div>
                <div class="p1">
                    <p id="117"><b><i>L</i></b>=<b><i>C</i></b>+<b><i>C</i></b><sup>T</sup>-<b><i>CC</i></b><sup>T</sup>.</p>
                </div>
                <div class="p1">
                    <p id="118">为了同时保留数据全局特性和局部特性, 在学习特征表示时, 重构误差函数<i>J</i><sub><i>r</i></sub>需多方面考虑:</p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow></msup></mrow></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>Μ</mi><mo stretchy="false">) </mo></mrow></msup></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mrow></mstyle><mrow><mi>J</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munder><mo>+</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mfrac><mrow><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mn>2</mn></mfrac><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mfrac><mi>Μ</mi><mn>2</mn></mfrac><mo stretchy="false">) </mo></mrow></msup><mo>-</mo><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mfrac><mi>Μ</mi><mn>2</mn></mfrac><mo stretchy="false">) </mo></mrow></msup><mi mathvariant="bold-italic">C</mi></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mrow></mstyle><mrow><mi>J</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></munder><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mfrac><mrow><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mo stretchy="false"> (</mo></mstyle><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mrow><mo stretchy="false">∥</mo><mi>b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">) </mo></mrow></mstyle><mrow><mi>J</mi><msub><mrow></mrow><mn>3</mn></msub></mrow></munder><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">其中, <b><i>C</i></b>表示引入的低秩先验, <i>λ</i><sub>1</sub>、<i>λ</i><sub>2</sub>表示两个正则项的权衡参数.第一项<i>J</i><sub>1</sub>表示重构误差, 旨在通过最小化输入<b><i>X</i></b>和重构输出<b><i>H</i></b><sup> (<i>M</i>) </sup>之间的误差以保留数据的局部结构, 每个样本相当于学习低维特征表示<mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mfrac><mi>Μ</mi><mn>2</mn></mfrac><mo stretchy="false">) </mo></mrow></msup></mrow></math></mathml>的监督信号<citation id="356" type="reference"><link href="325" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>.第二项<i>J</i><sub>2</sub>基于流形体假设设计, 认为流形体上的某些性质对不同的投影空间恒定, 通过低秩先验矩阵的引入, 约束编码层的低秩输出, 保留全局信息.为了缓解过拟合, 常见方法是限制网络复杂性, 最小化模型权重, 使权重值分布更“规则”.因此, 在损失函数中引入<i>J</i><sub>3</sub>正则项, 一定程度上限制网络的权重大小<citation id="357" type="reference"><link href="287" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.为了方便说明, 式 (3) 标量形式表示为</p>
                </div>
                <div class="area_img" id="122">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201907009_12200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="124">该深度嵌入模型将输入作为自监督信号学习低秩表示, 同时利用低秩先验确保流形结构在递进学习表示中的不变性.通过模型学到的特征表示兼顾数据的全局与局部结构, 有利于后续聚类过程.</p>
                </div>
                <div class="p1">
                    <p id="125"><b>2.2 参数优化与聚类过程</b></p>
                </div>
                <div class="p1">
                    <p id="126">为了优化聚类结果, 避免微调步骤覆盖逐层贪婪预训练获得的参数, 采用端到端的联合学习框架, 构造一个联合损失目标函数以进行整个网络的优化.</p>
                </div>
                <div class="p1">
                    <p id="127">因此, 本文引入归一化层, 通过多元逻辑回归函数<i>f</i><sub><i>θ</i></sub>∶<b><i>H</i></b>→<b><i>Y</i></b>, 相当于softmax分类器, 叠加在编码层顶部, 作为判别模型, 以概率形式预测相关簇分配, <b><i>P</i></b>为模型后验概率分布, 该分布分量<i>p</i><sub><i>ik</i></sub>表示第<i>i</i>个样本属于第<i>k</i>类的概率:</p>
                </div>
                <div class="area_img" id="128">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201907009_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="130">其中, <i>Θ</i>={<i>θ</i><sub>1</sub>, <i>θ</i><sub>2</sub>, …, <i>θ</i><sub><i>K</i></sub>}为函数参数.为了防止函数数值溢出, 在分式上下分别乘以一个非零常数<i>U</i>:</p>
                </div>
                <div class="p1">
                    <p id="131" class="code-formula">
                        <mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>U</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mo>-</mo><mi>max</mi></mrow></mstyle><mrow><mi>k</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Κ</mi></mrow></munder><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">θ</mi><msubsup><mrow></mrow><mi>k</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">h</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mfrac><mi>Μ</mi><mn>2</mn></mfrac><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="132">参照引入的辅助目标分布<b><i>Q</i></b>, 迭代修正模型预测, 优化聚类结果.为了避免退化解, 将大量样本划分到一个或几个簇中, 或离群点参与聚类, 引入正则项.通过<b><i>Q</i></b>的标签经验分布<mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Q</mi><mo>^</mo></mover></math></mathml>, 并结合最小化相关熵KL散度以降低模型后验概率分布<b><i>P</i></b>与辅助目标分布<b><i>Q</i></b>之间的差异, 聚类损失目标函数<i>J</i><sub><i>c</i></sub>定义为</p>
                </div>
                <div class="p1">
                    <p id="134" class="code-formula">
                        <mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>J</mi><msub><mrow></mrow><mi>c</mi></msub><mo>=</mo><mi>Κ</mi><mi>L</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Q</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ρ</mi><mo stretchy="false">) </mo><mo>+</mo><mi>Κ</mi><mi>L</mi><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Q</mi><mo>^</mo></mover><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><mo stretchy="false">) </mo><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="135">其中<b><i>u</i></b>为标签经验分布<mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Q</mi><mo>^</mo></mover></math></mathml>的均匀先验.函数的第一项最小化<b><i>P</i></b>与<b><i>Q</i></b>之间的差异, 第二项是为了平衡簇中样本数量.将KL散度定义代入<i>J</i><sub><i>c</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="137" class="code-formula">
                        <mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>J</mi><msub><mrow></mrow><mi>c</mi></msub><mo>=</mo></mtd></mtr><mtr><mtd><mo stretchy="false">[</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mo stretchy="false"> (</mo></mstyle></mrow></mstyle><mi>q</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mfrac><mrow><mi>q</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub></mrow><mrow><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub></mrow></mfrac><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo><mo stretchy="false">[</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mo stretchy="false"> (</mo></mstyle><mrow><mover accent="true"><mi>q</mi><mo>^</mo></mover><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mfrac><mrow><mover accent="true"><mi>q</mi><mo>^</mo></mover><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mi>u</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mo stretchy="false"> (</mo></mstyle></mrow></mstyle><mi>q</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mfrac><mrow><mi>q</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub></mrow><mrow><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub></mrow></mfrac><mo stretchy="false">) </mo><mo>+</mo><mi>q</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mfrac><mrow><mover accent="true"><mi>q</mi><mo>^</mo></mover><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mi>u</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="138">其中, 辅助目标分布分量<i>q</i><sub><i>ik</i></sub>表示第<i>i</i>个样本属于第<i>k</i>类的概率, 标签经验分布<mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Q</mi><mo>^</mo></mover></math></mathml>的分量</p>
                </div>
                <div class="p1">
                    <p id="140" class="code-formula">
                        <mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover accent="true"><mi>q</mi><mo>^</mo></mover><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>q</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>, </mo><mspace width="0.25em" /><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>q</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="141"><mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∀</mo><mi>k</mi><mo>, </mo><mi>u</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>Κ</mi></mfrac></mrow></math></mathml>为均匀先验分量.基于文献<citation id="358" type="reference">[<a class="sup">16</a>]</citation>方法, 辅助目标分布<b><i>Q</i></b>通过计算<mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi><msub><mrow></mrow><mi>c</mi></msub></mrow><mrow><mo>∂</mo><mi>q</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub></mrow></mfrac></mrow></math></mathml>使其为零, 求取相应封闭解获得, 即</p>
                </div>
                <div class="p1">
                    <p id="144"><mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>q</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><msup><mi>i</mi><mo>′</mo></msup></munder><mi>p</mi></mstyle><msub><mrow></mrow><mrow><msup><mi>i</mi><mo>′</mo></msup><mi>k</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><msup><mi>k</mi><mo>′</mo></msup></munder><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><msup><mi>k</mi><mo>′</mo></msup></mrow></msub><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><msup><mi>i</mi><mo>′</mo></msup></munder><mi>p</mi></mstyle><msub><mrow></mrow><mrow><msup><mi>i</mi><mo>′</mo></msup><msup><mi>k</mi><mo>′</mo></msup></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup></mrow></mfrac></mrow></math></mathml>.      (7) </p>
                </div>
                <div class="p1">
                    <p id="146">softmax函数关于函数参数<i>Θ</i>优化可被视为分类任务中交叉熵成本函数最小化问题<citation id="359" type="reference"><link href="315" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>.为了优化整个模型网络参数{<b><i>W</i></b><sup> (<i>m</i>) </sup>, <i>b</i><sup> (<i>m</i>) </sup>}<mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></msubsup></mrow></math></mathml>及<i>Θ</i>, 构造联合损失目标函数<i>J</i>, 包含交叉熵成本函数及重构误差函数两部分, 利用多层神经网络的训练过程对网络参数逐步更新:</p>
                </div>
                <div class="p1">
                    <p id="148"><i>J</i>=<i>J</i><sub><i>c</i></sub> (<i>Θ</i>) +<i>J</i><sub><i>r</i></sub>.      (8) </p>
                </div>
                <div class="p1">
                    <p id="149">式 (8) 第一项为交叉熵成本函数</p>
                </div>
                <div class="p1">
                    <p id="150" class="code-formula">
                        <mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>J</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Θ</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>q</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="151">式 (9) 计算关于<i>θ</i><sub><i>k</i></sub>的梯度向量, 再使用随机子梯度算法找到最小化成本函数的函数参数<i>Θ</i>:</p>
                </div>
                <div class="p1">
                    <p id="152"><mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">θ</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>J</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Θ</mi><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">θ</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>-</mo><mi>q</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false">) </mo><mi mathvariant="bold-italic">h</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mfrac><mi>Μ</mi><mn>2</mn></mfrac><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>.      (9) </p>
                </div>
                <div class="p1">
                    <p id="154">第二项是关于自编码器参数{<b><i>W</i></b><sup> (<i>m</i>) </sup>, <i>b</i><sup> (<i>m</i>) </sup>}<mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></msubsup></mrow></math></mathml>优化部分, 采用反向传播算法, 利用下层神经元梯度可由上层神经元残差导出的规律, 自上而下反向逐层计算, 获得关于<b><i>W</i></b><sup> (<i>m</i>) </sup>及<i>b</i><sup> (<i>m</i>) </sup>梯度:</p>
                </div>
                <div class="area_img" id="281">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201907009_28100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="281">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201907009_28101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="282">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201907009_28200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="283">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201907009_28300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="160"><i>f</i> ′ (·) 为激活函数<i>f</i> (·) 的导数, </p>
                </div>
                <div class="p1">
                    <p id="161"><i>z</i><mathml id="162"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>=<b><i>W</i></b><sup> (<i>m</i>) </sup><b><i>h</i></b><mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>+<i>b</i><sup> (<i>m</i>) </sup>, </p>
                </div>
                <div class="p1">
                    <p id="164">采用随机子梯度算法反向传播逐层训练网络以更新{<b><i>W</i></b><sup> (<i>m</i>) </sup>, <i>b</i><sup> (<i>m</i>) </sup>}<mathml id="165"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></msubsup></mrow></math></mathml>, 直至达到预先设定的迭代次数.自编码器权重{<b><i>W</i></b><sup> (<i>m</i>) </sup>, <i>b</i><sup> (<i>m</i>) </sup>}<mathml id="166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></msubsup></mrow></math></mathml>及函数参数<i>Θ</i>的更新公式如下:</p>
                </div>
                <div class="area_img" id="167">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201907009_16700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="169">其中<i>γ</i>表示算法的学习率.</p>
                </div>
                <div class="p1">
                    <p id="170"><b>2.3 算法步骤</b></p>
                </div>
                <div class="p1">
                    <p id="171">本文算法主要分为如下步骤:1) 计算输入数据的低秩表示, 构造相应的拉普拉斯矩阵作为自编码的输入;2) 将数据映射到多层神经网络中, 进行数据的低维特征学习;3) 根据低维特征表示将数据划分至多个子空间中.具体算法步骤如下.</p>
                </div>
                <div class="area_img" id="284">
                                <img alt="" src="Detail/GetImg?filename=images/MSSB201907009_28400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="284">
                                <img alt="" src="Detail/GetImg?filename=images/MSSB201907009_28401.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="197"><b>2.4 算法复杂度分析</b></p>
                </div>
                <div class="p1">
                    <p id="198">假设输入<b><i>X</i></b>∈<b>R</b><sup><i>d</i>×<i>n</i></sup>, 引入低秩约束先验的深度子空间聚类算法需要计算低秩约束先验, 此步骤的时间复杂度<i>O</i> (<i>n</i><sup>3</sup>+<i>dn</i><sup>2</sup>) .构造拉普拉斯矩阵作为自编码器的输入, 此环节时间复杂度可忽略不计.设定<i>M</i>=2, 2层编码器及2层译码层, 此部分包括反向传播神经网络训练中参数优化及聚类过程, 时间复杂度为<i>O</i> (<i>nD</i><sup>2</sup>+<i>nvK</i>) , 其中, <i>D</i>为设置的各隐藏层节点数 (即每层输出维度) 中的最大值, <i>v</i>为特征表示维数 (即<mathml id="199"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mrow><mfrac><mi>Μ</mi><mn>2</mn></mfrac></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mi>Κ</mi></mrow></math></mathml>为所需划分的子空间数, 由于<i>K</i>≤<i>v</i>≤<i>D</i>≤<i>d</i>, 所以算法总的时间复杂度为<i>O</i> (<i>dn</i><sup>2</sup>+<i>nD</i><sup>2</sup>+<i>n</i><sup>3</sup>) .StructAE (Structured AutoEncoder) <citation id="360" type="reference"><link href="311" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>结合稀疏表示与深度模型, 时间复杂度为<i>O</i> (<i>dn</i><sup>2</sup>+<i>nD</i><sup>2</sup>) , 略低于本文算法, 而子空间聚类算法如LRR<citation id="361" type="reference"><link href="291" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>时间复杂度为<i>O</i> (<i>n</i><sup>3</sup>+<i>dn</i><sup>2</sup>) .</p>
                </div>
                <div class="p1">
                    <p id="200">3 实验及结果分析</p>
                </div>
                <div class="p1">
                    <p id="201">为了证明本文算法适用于高维数据集, 选取如下5个数据集:MNIST、CIFAR10、COIL20、YaleB和ORL.这5个数据集包含不同类型的图像, 如手写数字、物体和面部图像等, 维数较高, 可以有效表明本文算法对高维数据特征的学习能力.表1为相关数据集的详细描述.考虑到聚类任务是完全无监督的, 训练样本和测试样本共用.</p>
                </div>
                <div class="area_img" id="202">
                    <p class="img_tit">表1 实验数据集 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table1 Experimental datasets</p>
                    <p class="img_note"></p>
                    <table id="202" border="1"><tr><td><br />数据集</td><td>样本数</td><td>类</td><td>维数</td></tr><tr><td><br />MNIST</td><td>70000</td><td>10</td><td>784</td></tr><tr><td><br />CIFAR10</td><td>60000</td><td>10</td><td>1024</td></tr><tr><td><br />COIL20</td><td>1440</td><td>20</td><td>1024</td></tr><tr><td><br />YaleB</td><td>2414</td><td>38</td><td>32256</td></tr><tr><td><br />ORL</td><td>400</td><td>40</td><td>2576</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="203">仿真实验硬件环境为:<i>Intel</i> (<i>R</i>) <i>Core</i> (<i>TM</i>) <i>i</i>7-6700 <i>CPU</i> @3.40 <i>GHz</i>; 16 <i>GB RAM</i>; <i>Windows</i> 7操作系统;<i>Python</i>编程语言.</p>
                </div>
                <div class="p1">
                    <p id="204">为了验证算法性能, 通过聚类结果和样本真实标签进行对比.采用准确率 (<i>Accuracy</i>, <i>ACC</i>) 、标准化互信息 (<i>Normalized Mutual Information</i>, <i>NMI</i>) 和调整兰德指数 (<i>Adjusted Rand Index</i>, <i>ARI</i>) 作为度量指标进行评估对比.<i>NMI</i>计算同一数据两个标签之间相似度的标准化度量.<i>ACC</i>和<i>NMI</i>这两种度量标准在[0, 1]内取值, 值越大聚类性能越好, 而<i>ARI</i>取值为[-1, 1], 1为最佳结果.</p>
                </div>
                <div class="p1">
                    <p id="205">对比算法如下:</p>
                </div>
                <div class="p1">
                    <p id="206">1) 3种经典的子空间算法:<i>LRR</i><citation id="362" type="reference"><link href="291" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、<i>LRSC</i><citation id="363" type="reference"><link href="323" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、<i>SSC</i><citation id="364" type="reference"><link href="327" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="208">2) 深度子空间聚类算法:采用深度模型、未引入任何约束的<i>AESC</i> (<i>AutoEncoder Based on</i><i>Spectral Clustering</i>) <citation id="365" type="reference"><link href="329" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、引入稀疏约束的<i>StructAE</i><citation id="366" type="reference"><link href="311" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="209">为了保证算法之间公平对比, 相关参数进行如下设定:<i>DSC</i>-<i>LRC</i>中M=4, 每层神经单元为300-200-150-200-300, 由于5个数据集维数不同, 需先采用<i>PCA</i>预处理将原始数据降到300维.在式 (4) 提及的参数中, 将λ<sub>2</sub>固定设定为10<sup>-3</sup>, 通过实验从</p>
                </div>
                <div class="p1">
                    <p id="210" class="code-formula">
                        <mathml id="210"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">{</mo><mn>1</mn><mn>0</mn><msup><mrow></mrow><mrow><mo>-</mo><mn>4</mn></mrow></msup><mo>, </mo><mn>5</mn><mo>×</mo><mn>1</mn><mn>0</mn><msup><mrow></mrow><mrow><mo>-</mo><mn>4</mn></mrow></msup><mo>, </mo><mn>1</mn><mn>0</mn><msup><mrow></mrow><mrow><mo>-</mo><mn>3</mn></mrow></msup><mo>, </mo><mn>5</mn><mo>×</mo><mn>1</mn><mn>0</mn><msup><mrow></mrow><mrow><mo>-</mo><mn>3</mn></mrow></msup><mo>, </mo><mn>1</mn><mn>0</mn><msup><mrow></mrow><mrow><mo>-</mo><mn>2</mn></mrow></msup><mo>, </mo><mn>5</mn><mo>×</mo><mn>1</mn><mn>0</mn><msup><mrow></mrow><mrow><mo>-</mo><mn>2</mn></mrow></msup><mo stretchy="false">}</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="211">中选出最优λ<sub>1</sub>或迭代次数达到100, 默认算法收敛, 停止训练.此外, 实验中学习率γ设为2<sup>-10</sup>.3种子空间聚类算法及<i>StructAE</i>参数设定如下:在<i>LRR</i>中λ=0.1;<i>LRSC</i>从实验结果中可以看出λ影响不大, 可设定在<citation id="367" type="reference">[<a class="sup">20</a>]</citation>内;<i>SSC</i>中α=20, ρ=1.为了公平地与本文算法对比, <i>AESC</i>、<i>StructAE</i>的各网络层维数同样设为300-200-150-200-300, 涉及的λ<sub>1</sub>、λ<sub>2</sub>、γ及激活函数与本文算法保持一致.</p>
                </div>
                <div class="p1">
                    <p id="212">为了深入研究算法性能, 引入平方密度尺度不变特征变换 (<i>Square Dense Scale</i>-<i>Invariant Feature Transform</i>, <i>SDSIFT</i>) 代替原始数据 (即灰度值) , 将每个图像划分为多个小区, 然后进行密集采样.<i>YaleB</i>、<i>ORL</i>数据集小区尺寸为12×12, <i>MNIST</i>、<i>CIFAR</i>10、<i>COIL</i>20数据集尺寸为4×4.由于算法初始化过程随机, 上述6种算法在5个数据集上重复5次, 计算3个指标的平均值, 结果如表2所示.</p>
                </div>
                <div class="area_img" id="213">
                    <p class="img_tit">表2 6种算法在5个数据集上的实验结果对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Experimental results of 6 algorithms on 5 datasets</p>
                    <p class="img_note"></p>
                    <table id="213" border="1"><tr><td><br />数据集</td><td>算法</td><td>ACC</td><td>NMI</td><td>ARI</td></tr><tr><td><br />MNIST</td><td>LRR<br />LRSC<br />SSC<br />AESC<br />StructAE<br />DSC-LRC</td><td>0.6369<br />0.5696<br />0.6385<br />0.5831<br />0.6570<br /><b>0.7562</b></td><td>0.6637<br />0.5672<br />0.6533<br />0.6139<br />0.6898<br /><b>0.7832</b></td><td>0.5388<br />0.4458<br />0.5336<br />0.4564<br />0.5799<br /><b>0.6865</b></td></tr><tr><td><br />CIFAR10</td><td>LRR<br />LRSC<br />SSC<br />AESC<br />StructAE<br />DSC-LRC</td><td>0.1505<br />0.1709<br />0.1592<br />0.1534<br />0.1800<br /><b>0.2001</b></td><td>0.0364<br />0.0342<br />0.0296<br />0.0328<br />0.0386<br /><b>0.0400</b></td><td>0.0892<br />0.0154<br />0.0105<br />0.0124<br />0.0156<br /><b>0.0213</b></td></tr><tr><td><br />COIL20</td><td>LRR<br />LRSC<br />SSC<br />AESC<br />StructAE<br />DSC-LRC</td><td>0.8757<br />0.5508<br />0.8217<br />0.5350<br />0.9104<br /><b>0.9324</b></td><td>0.9428<br />0.7126<br />0.9113<br />0.7911<br />0.9467<br />0.9406</td><td>0.8434<br />0.4795<br />0.7886<br />0.5023<br />0.8861<br /><b>0.9085</b></td></tr><tr><td><br />YaleB</td><td>LRR<br />LRSC<br />SSC<br />AESC<br />StructAE<br />DSC-LRC</td><td>0.8562<br />0.7474<br />0.6385<br />0.5203<br />0.9470<br />0.9206</td><td>0.9299<br />0.7963<br />0.6533<br />0.5593<br />0.9658<br />0.9450</td><td>0.8325<br />0.5806<br />0.5336<br />0.3871<br />0.9229<br /><b>0.9426</b></td></tr><tr><td><br />ORL</td><td>LRR<br />LRSC<br />SSC<br />AESC<br />StructAE<br />DSC-LRC</td><td>0.7720<br />0.7542<br />0.7692<br />0.7830<br />0.8561<br /><b>0.8691</b></td><td>0.8753<br />0.8404<br />0.8977<br />0.8958<br />0.9089<br /><b>0.9100</b></td><td>0.6846<br />0.6655<br />0.7025<br />0.6890<br />0.8056<br /><b>0.8124</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="214">由表2可得如下结论.</p>
                </div>
                <div class="p1">
                    <p id="215">1) 相比未引入深度模型的<i>LRR</i>、<i>LRSC</i>、<i>SSC</i>, <i>DSC</i>-<i>LRC</i>的3种性能指标均得到较大提升, 获得更理想的聚类结果.可见深度自编码器的引入可有效获取良好的样本潜在空间特征表示, 保留数据的局部结构信息.</p>
                </div>
                <div class="p1">
                    <p id="216">2) 相比<i>StructAE</i>、<i>DSC</i>-<i>LRC</i>, <i>AESC</i>处于劣势, 表明样本自表达模型的构造有利于数据整体空间结构的保留, 提高聚类精度.</p>
                </div>
                <div class="p1">
                    <p id="217">3) 稀疏约束先验是通过求解</p>
                </div>
                <div class="p1">
                    <p id="218" class="code-formula">
                        <mathml id="218"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ζ</mi><mo>, </mo><mi mathvariant="bold-italic">E</mi></mrow></munder><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">E</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mspace width="0.25em" /></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">X</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">Ζ</mi><mo>+</mo><mi mathvariant="bold-italic">E</mi><mo>, </mo><mtext>d</mtext><mtext>i</mtext><mtext>a</mtext><mtext>g</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">) </mo><mo>=</mo><mn>0</mn><mo>, </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="219">求得数据最稀疏表示<b><i>Z</i></b>, 该约束有利于数据集的局部结构保留.从表2中可看出, 相比StructAE, 本文算法在MNIST、CIFAR10、COIL20、ORL数据集上表现良好.在MNIST数据集上ACC、NMI和ARI分别提升9.92%、9.34%和10.66%, 在COIL20数据集上ACC和ARI分别提升2.2%和2.24%, NMI值仅降低0.61%.在YaleB数据集上, 虽然本文算法ACC和NMI稍有降低, 但ARI值提高1.97%.这种情况原因可能是在处理高维数据时, 算法中的稀疏表示能约束一些异常点和噪声, 有效提高聚类效率, 而低秩表示主要针对数据集的全局特性, 对于异常点的过滤存在忽略, 这是本文算法需要改进的地方.从运行时间上看, 在YaleB数据集上, 相比StructAE, 本文算法运行时间降低1 000～2 000 s左右.</p>
                </div>
                <div class="p1">
                    <p id="220">本文算法结合LRR与自编码器, 从上述结论 2) 可以看出, 算法性能在一定程度上会受样本自表达模型的影响.为了深入分析, 分别通过LRR、LRSC、LSR<citation id="368" type="reference"><link href="331" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>获得低秩约束先验, 进行性能对比.考虑到本文算法在MNIST、COIL20、ORL数据集上表现较优, 选择在这3个数据集上进行实验, 结果如表3所示.可以看出, 在MNIST、COIL20数据集上, LRR获得的低秩约束有利于簇划分, 而在ORL数据集上, 采用LRSC可以获得较优的聚类结果.因此, 改进低秩约束先验的求解算法有助于提高本文算法精度.</p>
                </div>
                <div class="area_img" id="224">
                    <p class="img_tit">表3 不同低秩约束先验的算法性能对比 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Performance comparison of algorithms with different low-rank constrained priors</p>
                    <p class="img_note"></p>
                    <table id="224" border="1"><tr><td><br />数据集</td><td>先验算法</td><td>ACC</td><td>NMI</td><td>ARI</td></tr><tr><td><br />MNIST</td><td>LRR<br />LRSC<br />LSR</td><td>0.7562<br />0.6517<br />0.6715</td><td>0.7832<br />0.6987<br />0.7248</td><td>0.6865<br />0.6041<br />0.6644</td></tr><tr><td><br />COIL20</td><td>LRR<br />LRSC<br />LSR</td><td>0.9324<br />0.9140<br />0.9200</td><td>0.9406<br />0.9264<br />0.9310</td><td>0.9085<br />0.8921<br />0.8812</td></tr><tr><td><br />ORL</td><td>LRR<br />LRSC<br />LSR</td><td>0.8691<br />0.8501<br />0.8705</td><td>0.9100<br />0.9041<br />0.9210</td><td>0.8124<br />0.8101<br />0.8241</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="225">本文算法引入深度自编码器, 用于学习复杂数据结构, 进行特征提取, 从理论上说, 神经网络层数更多的自编码器更有可能学到更高层次的特征表示, 同时各层维度设置的不同也影响算法性能.更改M值, M=2 (300-150-300) 和M=4 (300-200-150-200-300) , 更改中间层维度M=4 (300-200-50-200-300) , 通过本文算法在<i>COIL</i>20数据集上的运行结果进行衡量.由图2可以看出, 随着算法模型层数M的增加, <i>ACC</i>、<i>NMI</i>和<i>ARI</i>值均有所提高.当中间层数据特征降维到50时, 整体算法性能有所下降.</p>
                </div>
                <div class="area_img" id="228">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907009_228.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 M的选取及维度对算法性能的影响" src="Detail/GetImg?filename=images/MSSB201907009_228.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 M的选取及维度对算法性能的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907009_228.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.2 <i>Selection of</i> M <i>and effect of dimension on the performance</i> of proposed algorithm</p>

                </div>
                <div class="p1">
                    <p id="230">由于本文算法引入归一化层, 以概率形式预测相关簇划分, 因此为了对比该聚类算法与一般子空间聚类算法, 训练模型获得低维特征表示后, 将采用经典聚类算法K-<i>means</i>、<i>SSC</i>, 分别定义为<i>DSC</i>-<i>LRCk</i>和<i>DSC</i>-<i>LRCs</i>, 对比<i>DSC</i>-<i>LRC</i>.同时, 为了分析激活函数选择对聚类结果的影响, 将<i>DSC</i>-<i>LRC</i>、<i>DSC</i>-<i>LRCk</i>、<i>DSC</i>-<i>LRCs</i>中的激活函数分别设定为4种非线性函数<i>tanh</i>、<i>sigmoid</i>、<i>nssigmoid</i>和<i>softplus</i>, 选择在<i>MNIST</i>、<i>COIL</i>20数据集上进行实验, 评估3种算法性能.实验结果如图3和图4所示.</p>
                </div>
                <div class="area_img" id="235">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907009_23500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 3种算法在MNIST数据集上的结果对比" src="Detail/GetImg?filename=images/MSSB201907009_23500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 3种算法在MNIST数据集上的结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907009_23500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Comparison of 3 algorithms on MNIST dataset</p>

                </div>
                <div class="area_img" id="235">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907009_23501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 3种算法在MNIST数据集上的结果对比" src="Detail/GetImg?filename=images/MSSB201907009_23501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 3种算法在MNIST数据集上的结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907009_23501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Comparison of 3 algorithms on MNIST dataset</p>

                </div>
                <div class="area_img" id="240">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907009_24000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 3种算法在COIL20数据集上的结果对比" src="Detail/GetImg?filename=images/MSSB201907009_24000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 3种算法在COIL20数据集上的结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907009_24000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Comparison of 3 algorithms on COIL20 dataset</p>

                </div>
                <div class="area_img" id="240">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907009_24001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 3种算法在COIL20数据集上的结果对比" src="Detail/GetImg?filename=images/MSSB201907009_24001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 3种算法在COIL20数据集上的结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907009_24001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Comparison of 3 algorithms on COIL20 dataset</p>

                </div>
                <div class="p1">
                    <p id="241">由图3和图4可看出, 相比采用经典聚类算法的<i>DSC</i>-<i>LRCk</i>和<i>DSC</i>-<i>LRCs</i>, 引入辅助目标分布<i>DSC</i>-<i>LRC</i>聚类性能处于优势.对于激活函数选择, 非线性较强的<i>tanh</i>函数表现更优异, 其次为<i>nssigmoid</i>函数.总之, 这4种函数效果相差不大, 在大多情况下能获得较理想的聚类结果.</p>
                </div>
                <div class="p1">
                    <p id="242">4 结 束 语</p>
                </div>
                <div class="p1">
                    <p id="243">随着数据呈现高维度特征, 在对其进行聚类时, 往往不能较好捕获数据间几何结构的问题, 获得的特征表示不能有效反映数据全局和局部结构信息.本文结合低秩表示和深度自编码器, 在对样本进行相关低秩约束后, 再通过深度自编码器进行非线性的低维子空间映射.此外, 结合非监督学习与子空间聚类, 本文算法通过端到端联合学习方法, 在同一学习框架下进行参数更新和聚类优化.实验表明本文算法具有良好的聚类性能, 适用于高维数据集, 但对于某些数据集的聚类性能未达到期望结果.今后可以考虑在样本自表示中结合低秩表示和稀疏表示, 在保留数据全局特性的同时, 约束异常点和噪声的存在, 有效提高聚类精度.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="285">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Low-Rank Tensor Constrained Multiview Subspace Clustering">

                                <b>[1]</b> ZHANG C Q, FU H Z, LIU S, et al.Low-Rank Tensor Constrained Multiview Subspace Clustering // Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:1582-1590.
                            </a>
                        </p>
                        <p id="287">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep subspace clustering with sparsity prior">

                                <b>[2]</b> PENG X, XIAO S J, FENG J S, et al.Deep Subspace Clustering with Sparsity Prior // Proc of the 25th International Joint Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2016:1925-1931.
                            </a>
                        </p>
                        <p id="289">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESD5DAC7C0EF6C3E14CE6F37D5668662AD&amp;v=MjMzNTJwYlEzNU5GaHdMeTR3cTg9TmlmT2ZjZTlhcUMvcVB4RkVaMEpmMzlNemhKZ256a0xTM2lXcVJRemNiU1NSOHZyQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> YIN M, WU Z Z, ZENG D Y, et al.Sparse Subspace Clustering with Jointly Learning Representation and Affinity Matrix.Journal of the Franklin Institute, 2018, 355 (8) :3795-3811.
                            </a>
                        </p>
                        <p id="291">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust Recovery of Subspace Structures by Low-Rank Representation">

                                <b>[4]</b> LIU G C, LIN Z C, YAN S C, et al.Robust Recovery of Subspace Structures by Low-Rank Representation.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (1) :171-184.
                            </a>
                        </p>
                        <p id="293">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint Low Rank and Sparse Representation-Based Hyperspectral Image Classification">

                                <b>[5]</b> ZHANG M M, LI W, DU Q.Joint Low Rank and Sparse Representation-Based Hyperspectral Image Classification // Proc of the 8th Workshop on Hyperspectral Image and Signal Processing:Evolution in Remote Sensing.Washington, USA:IEEE, 2016.DOI:10.1109/WHISPERS.2016.8071748.
                            </a>
                        </p>
                        <p id="295">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Local Regression and Global Information-Embedded Dimension Reduction">

                                <b>[6]</b> YAO C, HAN J W, NIE F P, et al.Local Regression and Global Information-Embedded Dimension Reduction.IEEE Transactions on Neural Networks and Learning Systems, 2018, 29 (10) :4882-4893.
                            </a>
                        </p>
                        <p id="297">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZNXT201205002&amp;v=MzI0MTVGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZMM1BQeVBUZXJHNEg5UE1xbzk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 谈超, 关佶红, 周水庚.增量与演化流形学习综述.智能系统学报, 2012, 7 (5) :377-388. (TAN C, GUAN J H, ZHOU S G.Incremental and Evolutionary Manifold Learning:A Survey.CAAI Transactions on Intelligent Systems, 2012, 7 (5) :377-388.) 
                            </a>
                        </p>
                        <p id="299">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust Subspace Learning-Based Low-Rank Representation for Manifold Clustering">

                                <b>[8]</b> TANG K W, SU Z X, JIANG W, et al.Robust Subspace Learning-Based Low-Rank Representation for Manifold Clustering.Neural Computing and Applications, 2018.DOI:https://doi.org/10.1007.
                            </a>
                        </p>
                        <p id="301">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJDC54F6F1ED23E0D945A172C08FD7B8D5C&amp;v=MjQ2MTZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHdMeTR3cTg9TmpuQmFzQzlHcWZLMlk0d0VPa01lWHhOeGhJV216NTZTZ3ppcEdSQmZzQ2NNYi9zQ09Odg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> DONG Y X, YANG C Z.Cluster-Based Least Absolute Deviation Regression for Dimension Reduction.Journal of Statistical Theory and Practice, 2016, 10 (1) :121-132.
                            </a>
                        </p>
                        <p id="303">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Variational Deep Embe-dding:A Generative Approach to Clustering">

                                <b>[10]</b> JIANG Z, ZHENG Y, TAN H C, et al.Variational Deep Embe-dding:A Generative Approach to Clustering // Proc of the 26th International Joint Conference on Artificial Intelligence.Washington, USA:IEEE, 2017:1965-1972.
                            </a>
                        </p>
                        <p id="305">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Towards k-meansfriendly spaces:simultaneous deep learning and clustering&amp;quot;">

                                <b>[11]</b> YANG B, FU X, SIDIROPOULOS N D, et al.Towards K-means-Friendly Spaces:Simultaneous Deep Learning and Clustering // Proc of the 34th International Conference on Machine Learning.New York, USA:ACM, 2017:3861-3870.
                            </a>
                        </p>
                        <p id="307">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201711009&amp;v=MTkyNjFEN1liTEc0SDliTnJvOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpoVkwzUEs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 黄健航, 雷迎科.基于边际Fisher深度自编码器的电台指纹特征提取.模式识别与人工智能, 2017, 30 (11) :1030-1038. (HUANG J H, LEI Y K.Radio Fingerprint Extraction Based on Marginal Fisher Deep Autoencoder.Pattern Recognition and Artificial Intelligence, 2017, 30 (11) :1030-1038.) 
                            </a>
                        </p>
                        <p id="309">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Clustering for Unsupervised Learning of Visual Features">

                                <b>[13]</b> CARON M, BOJANOWSKI P, JOULIN A, et al.Deep Clustering for Unsupervised Learning of Visual Features // Proc of the 15th European Conference on Computer Vision.Berlin, Germany:Springer, 2018:139-156.
                            </a>
                        </p>
                        <p id="311">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structured AutoEncoders for Subspace Clustering">

                                <b>[14]</b> PENG X, FENG J S, XIAO S J, et al.Structured AutoEncoders for Subspace Clustering.IEEE Transactions on Image Processing, 2018, 27 (10) :5076-5086.
                            </a>
                        </p>
                        <p id="313">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked What-Where Auto-encoders">

                                <b>[15]</b> ZHAO J B, MATHIEU M, GOROSHIN R, et al.Stacked What-Where Auto-Encoders.Computer Science, 2015, 15 (1) :3563-3593.
                            </a>
                        </p>
                        <p id="315">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization">

                                <b>[16]</b> DIZAJI K G, HERANDI A, DENG C, et al.Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization // Proc of the 2th International Conference on Computer Vision.Washington, USA:IEEE, 2017:5747-5756.
                            </a>
                        </p>
                        <p id="317">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint unsupervised learning of deep repre-sentations and image clusters">

                                <b>[17]</b> YANG J W, PARIKH D, BATRA D.Joint Unsupervised Learning of Deep Representations and Image Clusters // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:1354-1362.
                            </a>
                        </p>
                        <p id="319">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel low rank representation algorithm for subspace clustering">

                                <b>[18]</b> CHEN Y Y, ZHANG L, ZHANG Y.A Novel Low Rank Representation Algorithm for Subspace Clustering.International Journal of Pattern Recognition and Artificial Intelligence, 2016, 30 (4) .DOI:10.1142/S0218001416500075.
                            </a>
                        </p>
                        <p id="321">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201611016&amp;v=MTY5Mjc0SDlmTnJvOUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpoVkwzUElUZlNkckc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> 张涛, 唐振民, 吕建勇.一种基于低秩表示的子空间聚类改进算法.电子与信息学报, 2016, 38 (11) :2811-2818. (ZHANG T, TANG Z M, LÜ J Y.Improved Algorithm Based on Low Rank Representation for Subspace Clustering.Journal of Electronics and Information Technology, 2016, 38 (11) :2811-2818.) 
                            </a>
                        </p>
                        <p id="323">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Closed Form Solution to Robust Subspace Estimation and Clustering">

                                <b>[20]</b> FAVARO P, VIDAL R, RAVICHANDRAN A.A Closed form Solution to Robust Subspace Estimation and Clustering // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2011:1801-1807.
                            </a>
                        </p>
                        <p id="325">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved deep embedded clustering with local structure preservation">

                                <b>[21]</b> GUO X F, GAO L, LIU X W, et al.Improved Deep Embedded Clustering with Local Structure Preservation // Proc of the 26th International Joint Conference on Artificial Intelligence.Washington, USA:IEEE, 2017:1753-1759.
                            </a>
                        </p>
                        <p id="327">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparse subspace clustering:Algorithm,theory,and applications">

                                <b>[22]</b> ELHAMIFAR E, VIDAL R.Sparse Subspace Clustering:Algorithm, Theory, and Applications.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (11) :2765-2781.
                            </a>
                        </p>
                        <p id="329">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Deep Representations for Graph Clustering">

                                <b>[23]</b> TIAN F, GAO B, CUI Q, et al.Learning Deep Representations for Graph Clustering // Proc of the 28th AAAI Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2014:1293-1299.
                            </a>
                        </p>
                        <p id="331">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust and efficient subspace segmentation via least squares regression">

                                <b>[24]</b> LU C Y, MIN H, ZHAO Z Q, et al.Robust and Efficient Subspace Segmentation via Least Squares Regression // Proc of the 12th European Conference on Computer Vision.Berlin, Germany:Springer, 2012:347-360.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201907009" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201907009&amp;v=MTU0MjBLRDdZYkxHNEg5ak1xSTlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZMM1A=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
