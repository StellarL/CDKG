<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131458930030000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dMSSB201909007%26RESULT%3d1%26SIGN%3duZgwrl6n%252bZ9DGBDCZe61pLI6lqM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201909007&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201909007&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201909007&amp;v=MTA0NTBqTXBvOUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVnJ2S0tEN1liTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#62" data-title="1 结合非局部与分块特征的跨视角步态识别 ">1 结合非局部与分块特征的跨视角步态识别</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="&lt;b&gt;1.1 步态能量图&lt;/b&gt;"><b>1.1 步态能量图</b></a></li>
                                                <li><a href="#70" data-title="&lt;b&gt;1.2 步态非局部网络参数&lt;/b&gt;"><b>1.2 步态非局部网络参数</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;1.3 分块特征分类&lt;/b&gt;"><b>1.3 分块特征分类</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#82" data-title="2 实验及结果分析 ">2 实验及结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#105" data-title="3 结 束 语 ">3 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#64" data-title="图1 非局部分块网络结构">图1 非局部分块网络结构</a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;表1 CNN模块参数配置&lt;/b&gt;"><b>表1 CNN模块参数配置</b></a></li>
                                                <li><a href="#77" data-title="图2 步态非局部特征获取流程图">图2 步态非局部特征获取流程图</a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;表2 各方法在OU-ISIR-LP数据集上跨视角的Rank1识 别率&lt;/b&gt;"><b>表2 各方法在OU-ISIR-LP数据集上跨视角的Rank1识 别率</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;表3 各方法在OU-ISIR-LP数据集上不同视角差下的 Rank1识别率&lt;/b&gt;"><b>表3 各方法在OU-ISIR-LP数据集上不同视角差下的 Rank1识别率</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;表4 各方法在CASIA-B数据集上跨视角的Rank1识别率&lt;/b&gt;"><b>表4 各方法在CASIA-B数据集上跨视角的Rank1识别率</b></a></li>
                                                <li><a href="#155" data-title="图3 不同视角组合下的ROC曲线对比．">图3 不同视角组合下的ROC曲线对比．</a></li>
                                                <li><a href="#155" data-title="图3 不同视角组合下的ROC曲线对比．">图3 不同视角组合下的ROC曲线对比．</a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;表5 各方法在不同视角差下的EER对比&lt;/b&gt;"><b>表5 各方法在不同视角差下的EER对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="156">


                                    <a id="bibliography_1" title=" HAN J,BHANN B.Individual Recognition Using Gait Energy Image.IEEE Transactions on Pattern Analysis and Machine Intelligence,2006,28(2):316-322." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Individual recognition using gait energy image">
                                        <b>[1]</b>
                                         HAN J,BHANN B.Individual Recognition Using Gait Energy Image.IEEE Transactions on Pattern Analysis and Machine Intelligence,2006,28(2):316-322.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_2" title=" 何逸炜,张军平.步态识别的深度学习:综述.模式识别与人工智能,2018,31(5):442-451.(HE Y W,ZHANG J P.Deep Learning for Gait Recognition:A Survey.Pattern Recognition and Artificial Intelligence,2018,31(5):442-451.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201805006&amp;v=MjcwNDZPZVplUm5GeS9rVnJ2S0tEN1liTEc0SDluTXFvOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         何逸炜,张军平.步态识别的深度学习:综述.模式识别与人工智能,2018,31(5):442-451.(HE Y W,ZHANG J P.Deep Learning for Gait Recognition:A Survey.Pattern Recognition and Artificial Intelligence,2018,31(5):442-451.)
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_3" title=" KUSAKUNNIRAN W,WU Q,LI H D,et al.Multiple Views Gait Recognition Using View Transformation Model Based on Optimized Gait Energy Image // Proc of the 12th IEEE International Confe-rence on Computer Vision Workshops.Washington,USA:IEEE,2009:1058-1064." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiple views gait recognition using view transformation model based on optimized Gait Energy Image">
                                        <b>[3]</b>
                                         KUSAKUNNIRAN W,WU Q,LI H D,et al.Multiple Views Gait Recognition Using View Transformation Model Based on Optimized Gait Energy Image // Proc of the 12th IEEE International Confe-rence on Computer Vision Workshops.Washington,USA:IEEE,2009:1058-1064.
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_4" title=" MURAMATSU D,MAKIHARA Y,YAGI Y.Cross-View Gait Re-cognition by Fusion of Multiple Transformation Consistency Measures // Proc of the 2nd International Workshop on Biometrics and Forensics.Washington,USA:IEEE,2014.DOI:10.1109/IWBF.2014.6914253." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-View Gait Re-cognition by Fusion of Multiple Transformation Consistency Measures">
                                        <b>[4]</b>
                                         MURAMATSU D,MAKIHARA Y,YAGI Y.Cross-View Gait Re-cognition by Fusion of Multiple Transformation Consistency Measures // Proc of the 2nd International Workshop on Biometrics and Forensics.Washington,USA:IEEE,2014.DOI:10.1109/IWBF.2014.6914253.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_5" title=" MURAMATSU D,MAKIHARA Y,YAGI Y.View Transformation Model Incorporating Quality Measures for Cross-View Gait Recognition.IEEE Transactions on Cybernetics,2016,46(7):1602-1615." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=View transformation model incorporating quality measures for cross-view gait recognition">
                                        <b>[5]</b>
                                         MURAMATSU D,MAKIHARA Y,YAGI Y.View Transformation Model Incorporating Quality Measures for Cross-View Gait Recognition.IEEE Transactions on Cybernetics,2016,46(7):1602-1615.
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_6" title=" 王科俊,阎涛,吕卓纹,等.核稀疏保留投影及在步态识别中的应用.中国图象图形学报,2013,18(3):257-263.(WANG K J,YAN T,L&#220; Z W,et al.Kernel Sparsity Preserving Projections and Its Application to Gait Recognition.Journal of Image and Graphics,2013,18(3):257-263.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201303003&amp;v=MjQyNjBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tWcnZLUHlyZmJMRzRIOUxNckk5Rlo0UUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         王科俊,阎涛,吕卓纹,等.核稀疏保留投影及在步态识别中的应用.中国图象图形学报,2013,18(3):257-263.(WANG K J,YAN T,L&#220; Z W,et al.Kernel Sparsity Preserving Projections and Its Application to Gait Recognition.Journal of Image and Graphics,2013,18(3):257-263.)
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_7" title=" WANG X H,WANG J,YAN K.Gait Recognition Based on Gabor Wavelets and (2D)&lt;sup&gt;2&lt;/sup&gt;PCA.Multimedia Tools and Applications,2018,77(10):12545-12561." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gait Recognition Based on Gabor Wavelets and (2D)2PCA">
                                        <b>[7]</b>
                                         WANG X H,WANG J,YAN K.Gait Recognition Based on Gabor Wavelets and (2D)&lt;sup&gt;2&lt;/sup&gt;PCA.Multimedia Tools and Applications,2018,77(10):12545-12561.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_8" title=" 王修晖,严珂.基于连续密度隐马尔可夫模型的人体步态识别.模式识别与人工智能,2016,29(8):709-716.(WANG X H,YAN K.Human Gait Recognition Using Continuous Density Hidden Markov Models.Pattern Recognition and Artificial Intelligence,2016,29(8):709-716.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201608005&amp;v=MDc4ODBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tWcnZLS0Q3WWJMRzRIOWZNcDQ5RllZUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         王修晖,严珂.基于连续密度隐马尔可夫模型的人体步态识别.模式识别与人工智能,2016,29(8):709-716.(WANG X H,YAN K.Human Gait Recognition Using Continuous Density Hidden Markov Models.Pattern Recognition and Artificial Intelligence,2016,29(8):709-716.)
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_9" title=" BEN X Y,ZHANG P,LAI Z H,et al.A General Tensor Representation Framework for Cross-View Gait Recognition.Pattern Recognition,2019,90:87-98." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES16083F8E39C987816EC60B34E5807BE7&amp;v=MjgxODJ4N1NBM2hxR2N3Y2JLVE44K1lDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3N202eEtvPU5pZk9mYksrSHRuUDJZY3daK0o4QlhRK3h4Y1ZuMA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         BEN X Y,ZHANG P,LAI Z H,et al.A General Tensor Representation Framework for Cross-View Gait Recognition.Pattern Recognition,2019,90:87-98.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_10" title=" WOLF T,BABAEE M,RIGOLL G.Multi-view Gait Recognition Using 3D Convolutional Neural Networks // Proc of the IEEE International Conference on Image Processing.Washington,USA:IEEE,2016:4165-4169." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-view gait recognition using 3D convolutional neural networks">
                                        <b>[10]</b>
                                         WOLF T,BABAEE M,RIGOLL G.Multi-view Gait Recognition Using 3D Convolutional Neural Networks // Proc of the IEEE International Conference on Image Processing.Washington,USA:IEEE,2016:4165-4169.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_11" title=" LAM T H W,CHEUNG K H,LIU J N K.Gait Flow Image:A Silhouette-Based Gait Representation for Human Identification.Pa-ttern Recognition,2011,44(4):973-987." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738259&amp;v=MTg4OTJVTGJJSjE4UmJoQT1OaWZPZmJLN0h0RE5xWTlGWStnSERua3dvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         LAM T H W,CHEUNG K H,LIU J N K.Gait Flow Image:A Silhouette-Based Gait Representation for Human Identification.Pa-ttern Recognition,2011,44(4):973-987.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_12" title=" TONG S B,FU Y Z,YUE X W,et al.Multi-view Gait Recognition Based on a Spatial-Temporal Deep Neural Network.IEEE Access,2018,6:57583-57596." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-view Gait Recognition Based on a Spatial-Temporal Deep Neural Network">
                                        <b>[12]</b>
                                         TONG S B,FU Y Z,YUE X W,et al.Multi-view Gait Recognition Based on a Spatial-Temporal Deep Neural Network.IEEE Access,2018,6:57583-57596.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_13" title=" CHEN Q,WANG Y H,LIU Z,et al.Feature Map Pooling for Cross-View Gait Recognition Based on Silhouette Sequence Images[C/OL].[2019-02-22].https://arxiv.org/pdf/1711.09358.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature Map Pooling for Cross-View Gait Recognition Based on Silhouette Sequence Images[C/OL]">
                                        <b>[13]</b>
                                         CHEN Q,WANG Y H,LIU Z,et al.Feature Map Pooling for Cross-View Gait Recognition Based on Silhouette Sequence Images[C/OL].[2019-02-22].https://arxiv.org/pdf/1711.09358.pdf.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_14" title=" WU H M,WENG J,CHEN X,et al.Feedback Weight Convolutional Neural Network for Gait Recognition.Journal of Visual Communication and Image Representation,2018,55:424-432." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1AA5D468991FD02C0250EB0A1631931E&amp;v=MTI5ODM0cTRsTmJlSU9lZ2c1eldVVDZEcDlQUTNpM1JNemVyT2RScnZxQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZodzdtNnhLbz1OaWZPZmJMSmI5Uw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         WU H M,WENG J,CHEN X,et al.Feedback Weight Convolutional Neural Network for Gait Recognition.Journal of Visual Communication and Image Representation,2018,55:424-432.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_15" title=" SHIRAGA K,MAKIHARA Y,MURAMATSU D,et al.GEINet:View-Invariant Gait Recognition Using a Convolutional Neural Network // Proc of the International Conference on Biometrics.Wa-shington,USA:IEEE,2016.DOI:10.1109/ICB.2016.7550060." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GEINet:View-Invariant Gait Recognition Using a Convolutional Neural Network">
                                        <b>[15]</b>
                                         SHIRAGA K,MAKIHARA Y,MURAMATSU D,et al.GEINet:View-Invariant Gait Recognition Using a Convolutional Neural Network // Proc of the International Conference on Biometrics.Wa-shington,USA:IEEE,2016.DOI:10.1109/ICB.2016.7550060.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_16" title=" WU Z F,HUANG Y Z,WANG L,et al.A Comprehensive Study on Cross-View Gait Based Human Identification with Deep CNNs.IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(2):209-226." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Comprehensive Study on Cross-View Gait Based Human Identification with Deep CNNs">
                                        <b>[16]</b>
                                         WU Z F,HUANG Y Z,WANG L,et al.A Comprehensive Study on Cross-View Gait Based Human Identification with Deep CNNs.IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(2):209-226.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_17" title=" KRIZHEVSKY A,SUTSKEVER J,HINTON G.ImageNet Classification with Deep Convolutional Neural Networks // Proc of the 25th International Conference on Neural Information Processing Systems.Cambridge,USA:The MIT Press,2012,I:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">
                                        <b>[17]</b>
                                         KRIZHEVSKY A,SUTSKEVER J,HINTON G.ImageNet Classification with Deep Convolutional Neural Networks // Proc of the 25th International Conference on Neural Information Processing Systems.Cambridge,USA:The MIT Press,2012,I:1097-1105.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_18" title=" HE K M,ZHANG X Y,REN S Q,et al.Deep Residual Learning for Image Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016,I:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[18]</b>
                                         HE K M,ZHANG X Y,REN S Q,et al.Deep Residual Learning for Image Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016,I:770-778.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_19" title=" WANG X H,YAN W Q.Cross-View Gait Recognition through Ensemble Learning // Proc of the Thematic Workshops of ACM Multimedia.New York,USA:ACM,2017:385-392." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-View Gait Recognition through Ensemble Learning">
                                        <b>[19]</b>
                                         WANG X H,YAN W Q.Cross-View Gait Recognition through Ensemble Learning // Proc of the Thematic Workshops of ACM Multimedia.New York,USA:ACM,2017:385-392.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_20" title=" IWAMA H,OKUMURA M,MAKIHARA Y,et al.The OU-ISIR Gait Database Comprising the Large Population Dataset and Performance Evaluation of Gait Recognition.IEEE Transactions on Information Forensics and Security,2012,7(5):1511-1521." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The OU-ISIR Gait Database Comprising the Large Population Dataset and Performance Evaluation of Gait Recognition">
                                        <b>[20]</b>
                                         IWAMA H,OKUMURA M,MAKIHARA Y,et al.The OU-ISIR Gait Database Comprising the Large Population Dataset and Performance Evaluation of Gait Recognition.IEEE Transactions on Information Forensics and Security,2012,7(5):1511-1521.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_21" title=" TAKEMURA N,MAKIHARA Y,MURAMATSU D,et al.Multi-view Large Population Gait Dataset and Its Performance Evaluation for Cross-View Gait Recognition[C/OL].[2019-02-22].https://link.springer.com/content/pdf/10.1186%2Fs41074-018-0039-6.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-view Large Population Gait Dataset and Its Performance Evaluation for Cross-View Gait Recognition[C/OL]">
                                        <b>[21]</b>
                                         TAKEMURA N,MAKIHARA Y,MURAMATSU D,et al.Multi-view Large Population Gait Dataset and Its Performance Evaluation for Cross-View Gait Recognition[C/OL].[2019-02-22].https://link.springer.com/content/pdf/10.1186%2Fs41074-018-0039-6.pdf.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_22" title=" YU S Q,TAN D L,TAN T N.A Framework for Evaluating the Effect of View Angle,Clothing and Carrying Condition on Gait Recognition // Proc of the 18th International Conference on Pattern Recognition.Washington,USA:IEEE,2006,IV:441-444." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Framework for Evaluating the Effect of View Angle, Clothing and Carrying Condition on Gait Recognition">
                                        <b>[22]</b>
                                         YU S Q,TAN D L,TAN T N.A Framework for Evaluating the Effect of View Angle,Clothing and Carrying Condition on Gait Recognition // Proc of the 18th International Conference on Pattern Recognition.Washington,USA:IEEE,2006,IV:441-444.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(09),821-827 DOI:10.16451/j.cnki.issn1003-6059.201909006            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>结合非局部与分块特征的跨视角步态识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%AF%E4%B8%96%E7%81%B5&amp;code=39999065&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冯世灵</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E4%BF%AE%E6%99%96&amp;code=38247259&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王修晖</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E8%AE%A1%E9%87%8F%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=1699595&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国计量大学信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>目前基于深度学习的步态识别方法大多通过叠加卷积层获取全局特征,忽略有利于细粒度分类的局部特征.针对上述问题,文中提出结合非局部与分块特征的跨视角步态识别方法.将一对步态能量图(GEI)作为输入,提取单样本的非局部信息与样本对之间的相对非局部信息.为了更好地提取局部特征,根据GEI的几何特性,将人体区域水平切分为静态块、微动态块和强动态块,连接至3个二值分类器分别进行训练.在OU-ISIR-LP和CASIA-B步态数据集上的对比实验表明,文中方法的正确识别率较高.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AD%A5%E6%80%81%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">步态识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B7%A8%E8%A7%86%E8%A7%92%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">跨视角识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9D%9E%E5%B1%80%E9%83%A8%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">非局部特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E5%9D%97%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分块特征;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    冯世灵，硕士研究生，主要研究方向为计算机视觉、模式识别．E-mail:fsl1994@163.com.&lt;image id="153" type="formula" href="images/MSSB201909007_15300.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    *王修晖(通讯作者)，博士，副教授，主要研究方向为模式识别、计算机视觉、计算机图形学．E-mail:wangxiuhui@cjlu.edu.cn.&lt;image id="154" type="formula" href="images/MSSB201909007_15400.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-21</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(No.61602431,61303146)资助;</span>
                    </p>
            </div>
                    <h1><b>Cross-View Gait Recognition Combined with Non-local and Part-level Features</b></h1>
                    <h2>
                    <span>FENG Shiling</span>
                    <span>WANG Xiuhui</span>
            </h2>
                    <h2>
                    <span>College of Information Engineering,China Jiliang University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In most existing gait recognition methods based on deep learning, global features are acquired by stacking convolutional layers, and local features beneficial to fine-grained classification are ignored. Aiming at this problem, a cross-view gait recognition method is proposed by combining non-local and part-level features. A pair of gait energy images(GEIs) is used as input to extract the non-local information of a single sample and the relative non-local information of the sample pairs. Then, human body regions are divided horizontally into static blocks, micro-dynamic blocks and strong dynamic blocks to extract better local features according to the geometric characteristics of GEI. Furthermore, the segmented regions are connected to three binary classifiers for training respectively. Finally, experiments on OU-ISIR-LP and CASIA-B gait datasets show that the proposed method produces a higher correct recognition rate.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Gait%20Recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Gait Recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Cross-View%20Recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Cross-View Recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Non-local%20Features&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Non-local Features;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Part-Level%20Features&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Part-Level Features;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    FENG Shiling,master student. His research interests include computer vision and pattern recognition.;
                                </span>
                                <span>
                                    WANG Xiuhui(Corresponding author), Ph. D.,associate professor. His research interests include pattern recognition,computer vision and computer graphics.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-21</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China(No.61602431,61303146);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="58">步态描述人在行走过程中姿态的规律性变化情况.医学和心理学证据表明,每个人的步态存在一定的差异<citation id="200" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,可用于身份的鉴别.相比其它生物识别方式,步态识别更适用于远距离的身份识别,虽然虹膜和人脸识别准确率较高,但易受距离和故意遮挡等因素的严重影响.步态的有效识别距离较远,并且在低视频质量下也可完成识别,同时具有非侵犯性和难于隐藏性等特点.目前的犯罪分子反侦察能力较强,利用口罩、帽子、手套等工具使指纹与人脸识别技术失效,此时步态识别成为唯一有效的识别方式.城市中安装大量摄像头,使步态成为犯罪与法医鉴定的重要依据,并在实际案例中得以应用<citation id="201" type="reference"><link href="158" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.然而,步态会受服装、步速、视角等因素影响,类内差异可能大于类间差异.</p>
                </div>
                <div class="p1">
                    <p id="59">针对跨视角问题,Kusakunniran等<citation id="202" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出以步态能量图(Gait Energy Image, GEI)<citation id="203" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>为步态特征的视角转换模型,利用基于奇异值分解的矩阵分解方法将待测样本(Probe)视角,转换成目标样本(Gallery)视角,获得较好效果.学者们随之提出几个视角转换模型的变体,如利用转换一致性度量<citation id="204" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>和基于质量度量<citation id="205" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的视角转换模型,并在OU-ISIR数据集官网公布测试协议和基准.该类方法不能保证在识别任务中达到最佳效果,因为转化过程中难以避免噪声的传播,并且难以提升特征的有效性.此外,还有研究利用核稀疏保留投影<citation id="206" type="reference"><link href="166" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、支持向量机(Support Vector Machine, SVM)<citation id="207" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、隐马尔可夫模型(Hidden Markov Model, HMM)<citation id="208" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等进行判别.Ben等<citation id="209" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>进一步研究耦合度量学习理论,解决传统度量学习在形态差异较大时无法匹配的问题.</p>
                </div>
                <div class="p1">
                    <p id="60">近些年,深度学习技术在步态识别和分类中也受到广泛关注.Wolf等<citation id="210" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出基于3D卷积捕捉步态序列中时空信息的方法,使用多视角3D卷积网络将步态轮廓图序列作为网络输入,并利用光流处理衣着与颜色对识别结果的影响.对于时域步态特征,Lam等<citation id="211" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出将步态流图像(Gait Flow Image, GFI)作为时间特征模板.Tong等<citation id="212" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出应用于步态识别的时空深度神经网络(Spatial-Temporal Deep Neural Network, STDNN),包括时间特征网络(Temporal Feature Network, TFN)和空间特征网络(Spatial Feature Network, SFN),丰富步态特征.Chen等<citation id="213" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation> 提出基于特征图池化 (Feature Map Pool-ing, FMP)的步态识别网络,从步态轮廓序列中提取并聚合序列特征,而非使用平均轮廓图像简单地表示步态.Wu等<citation id="214" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出基于反馈权重的步态识别方法,通过反馈权重计算优化卷积神经网络(Convolutional Neural Network, CNN)架构,并利用感受野权重提高CNN特征的判别能力.Shiraga等<citation id="215" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出步态能量图网络(Gait Energy Image Net, GEINet),由两组卷积、池化和归一化层及两个全连接层组成,最后一层全连接层的节点数等于训练ID的个数.网络以GEI为输入样本,最后一层的softmax值为相应类别的概率.Wu等<citation id="216" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出利用深度卷积神经网络(DeepCNNs)直接学习一对GEI或序列间的相似度.</p>
                </div>
                <div class="p1">
                    <p id="61">综上所述,现有跨视角步态识别方法大多通过叠加卷积层获取全局特征,忽略具有重要意义的可以反映步态细粒度信息的局部特征.本文提出结合非局部特征与分块特征的跨视角步态识别方法,利用非局部神经网络提取单样本的非局部信息与样本对之间的相对非局部信息,并根据GEI的几何特性将人体区域水平切分为具有较强区分性的区域块,用于获取局部特征,增强步态特征的类间区分度.在OU-ISIR-LP、CASIA-B步态数据集上的对比实验表明,本文方法在正确识别率方面具有显著提升.</p>
                </div>
                <h3 id="62" name="62" class="anchor-tag">1 结合非局部与分块特征的跨视角步态识别</h3>
                <div class="p1">
                    <p id="63">本文提出非局部分块网络(Nonlocal-Cut Net),结构如图1所示.</p>
                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909007_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 非局部分块网络结构" src="Detail/GetImg?filename=images/MSSB201909007_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 非局部分块网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909007_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Nonlocal-cut network structure</p>

                </div>
                <div class="p1">
                    <p id="65">网络主要由CNN模块和非局部模块组成,类似孪生(Siamese)网络双通道输入.以GEI<citation id="217" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>作为步态特征,随机生成正、负GEI样本对输入到网络中的两个对应通道.在下一层各自获得两个通道卷积后的非局部信息并叠加,连接至后续的卷积模块.利用非局部模块提取叠加后的相对非局部特征.网络融合单样本的非局部信息与样本对之间的相对非局部信息,有效提升网络模型的特征提取和表达能力.根据GEI对齐的特点将卷积后的256幅特征图水平切分为3块,分别连接至3个二值分类器进行训练,综合生成输出结果.其中,3个水平切块展示步态能量图的身体摆动特点,分别表示静态块、微动态块和强动态块.</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66"><b>1.1 步态能量图</b></h4>
                <div class="p1">
                    <p id="67">GEI由一个步态周期内的轮廓序列经过缩放、对齐、平均等操作生成,可看作简单混合行走过程中的静态与动态部分.GEI公式如下:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>B</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中,<i>G</i>(<i>x</i>,<i>y</i>)表示步态能量图,(<i>x</i>,<i>y</i>)表示各坐标点的值,<i>B</i><sub><i>t</i></sub>(<i>x</i>,<i>y</i>)表示<i>t</i>帧时的二值轮廓图,<i>n</i>表示一个步态周期内所有帧的总和.将一整个步态周期内标准化后的二值轮廓图叠加求和并取平均值,生成GEI.</p>
                </div>
                <h4 class="anchor-tag" id="70" name="70"><b>1.2 步态非局部网络参数</b></h4>
                <div class="p1">
                    <p id="71">受非局部神经网络的启发,本文不提取步态序列的非局部信息,而是提取单张GEI的非局部信息. 在利用卷积模块对两个通道的GEI进行特征提取的基础上,融合非局部模块提取的单样本非局部信息与样本对之间的相对非局部信息.为了验证本文方法的有效性,使用Wu等<citation id="218" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出的低层特征匹配网络(Matching Local Features at the Bottom Layer, LBNet)作为基础CNN模块框架,包括卷积层、池化层和归一化层.所有卷积核尺寸均为7×7,归一化层使用局部响应归一化(Local Response Normaliza-tion,LRN)<citation id="219" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>．图1中各CNN模块参数的具体设置如表1所示．</p>
                </div>
                <div class="area_img" id="73">
                    <p class="img_tit"><b>表1 CNN模块参数配置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 CNN module parameter setting</p>
                    <p class="img_note"></p>
                    <table id="73" border="1"><tr><td>模块</td><td>层</td><td>核</td><td>尺寸</td><td>步长</td><td>激活函数</td></tr><tr><td><br />CNN1</td><td>conv1<br />pool</td><td>16<br /></td><td>7×7×1<br />2×2</td><td>1<br />2</td><td>ReLU<br /></td></tr><tr><td>CNN1′</td><td>conv1′<br />pool</td><td>16<br /></td><td>7×7×1<br />2×2</td><td>1<br />2</td><td>ReLU<br /></td></tr><tr><td>CNN2</td><td>conv2<br />pool</td><td>64<br /></td><td>7×7×16<br />2×2</td><td>1<br />2</td><td>ReLU<br /></td></tr><tr><td>CNN3</td><td>conv3</td><td>256</td><td>7×7×64</td><td>1</td><td>-</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="74">值得注意的是CNN3模块取消归一化层和池化层,加入丢弃法(Dropout)以防止过拟合.此外,考虑到现有步态数据集中数据量相对较少,取消全连接层,减少待定参数的数量.参照Wolf等<citation id="220" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出的非局部神经网络,并将其应用于步态非局部网络,非局部操作简化为</p>
                </div>
                <div class="p1">
                    <p id="75"><i>z</i><sub><i>i</i></sub>=<i>G</i><sub><i>i</i></sub>+<i>W</i><sub><i>z</i></sub><i>y</i><sub><i>i</i></sub> ,</p>
                </div>
                <div class="p1">
                    <p id="76">其中,<i>z</i><sub><i>i</i></sub>为两种特征的融合,<i>G</i><sub><i>i</i></sub>表示GEI经过卷积模块后的输出,<i>y</i><sub><i>i</i></sub>表示非局部模块后的输出,<i>W</i><sub><i>z</i></sub>表示将<i>y</i><sub><i>i</i></sub> 尺寸放大到与<i>G</i><sub><i>i</i></sub>相同维度.实现过程如图2所示.</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909007_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 步态非局部特征获取流程图" src="Detail/GetImg?filename=images/MSSB201909007_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 步态非局部特征获取流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909007_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Flow chart of acquiring gait non-local features</p>

                </div>
                <div class="p1">
                    <p id="78">图2以第一层非局部网络为例,GEI样本经过第一层CNN模块后获得16幅61×41的特征图,<i>Φ</i>、<i>θ</i>、<i>g</i>三个1×1的卷积用于降低通道数,减少计算量.将<i>Φ</i>通道输出转置与<i>θ</i>通道输出进行矩阵乘,计算相似性.然后,通过执行softmax操作获取通道注意力,并计算当前特征图中每个像素与其它所有位置像素的归一化相关性,用于生成注意力图.同时,将<i>g</i>通道也采用一样的操作,先降维再与注意力图进行矩阵乘运算.最后,利用1×1的卷积运算将输出恢复到与输入相同的尺寸,更易于结合卷积模块的步态特征.该模块实现非局部操作,获取全局信息,并利用残差<citation id="221" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>结合局部与非局部的输出,丰富步态特征.</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79"><b>1.3 分块特征分类</b></h4>
                <div class="p1">
                    <p id="80">细粒度分类简单描述就是将一个类别中的子类分别进行精确分类.步态分类也可看作是细粒度图像分类<citation id="222" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>.细粒度分类更注重局部信息,重要的是在图中找到类间具有较强区分度的区域块.行人重识别侧重于使用外部线索,通过姿态估计定位到身体部件,获取局部特征进而识别.考虑到在GEI生成过程中预先经过对齐操作,因此将经过卷积模块与非局部模块后得到的256幅特征图水平切分为3块(静态块、微动态块和强动态块).提取局部特征,使用3个二分类softmax作为分类器分别训练,损失函数选择二分类的交叉熵损失,训练过程中将一系列卷积后生成的特征图水平切分成3块,并各接入一个二值分类器,使用交叉熵计算每个二值分类器的输出与真实标签的损失值,并将3块损失值相加后进行损失的反向传播.在测试时,计算每块的输出经过softmax后判断为正的概率并相加获得总得分.</p>
                </div>
                <div class="p1">
                    <p id="81">在目标检测等领域证明堆砌网络层数可达到更好的识别效果,使用深层预训练模型也使行人重识别得到较大提升,但是经过实验发现预训练模型用于步态识别效果不佳.这是由于步态的数据量较小并且行人重识别利用颜色和纹理等信息,而步态识别只使用单通道轮廓图像,类间差距更小.此外,Wu等<citation id="223" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>已经证实,在基于GEI的跨视角步态识别任务中,具有3个卷积层的步态识别网络性能在具有2个、3个或5个卷积层中最佳.根据上述结果,本文设计具有3个卷积层的主体网络,并且利用残差融合卷积结果与非局部操作结果,用于提取细粒度分类效果更强的步态特征.</p>
                </div>
                <h3 id="82" name="82" class="anchor-tag">2 实验及结果分析</h3>
                <div class="p1">
                    <p id="83">为了验证本文方法的有效性,选择OU-ISIR-LP<citation id="226" type="reference"><link href="194" rel="bibliography" /><link href="196" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>和CASIA-B<citation id="224" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>两个数据集进行实验.OU-ISIR 包括多个子集,其中OU-ISIR-LP<citation id="225" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>是目前人数最多的数据集之一,年龄跨度较广,性别比例均衡.每个对象拍摄2个序列:Gallery序列和Probe序列,预处理为归一化至128×88的轮廓图.此外,每个序列根据相机观测角度分为(55°,65°,75°,85).CASIA-B为目前视角跨度最广泛的数据集之一,包含来自11个视角124位不同行人,视角为(0°,18°,36°,…,180°).在每个视角下,为每人拍摄10个步态序列,包括正常步行(NM)中的6个序列,携带背包(BG)时步行的2个序列,穿着外套(CL)时步行的2个序列.</p>
                </div>
                <div class="p1">
                    <p id="84">由于设计的网络以双通道输入计算样本对的相似度,需要保证正正例和正负例样本的均衡.在OU-ISIR-LP数据集实验中,本文首先从训练集中抽取一个ID,并在角度集合</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>V</mi><mo>=</mo><mo stretchy="false">{</mo><mtext>G</mtext><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext><mtext>e</mtext><mtext>r</mtext><mtext>y</mtext><mn>5</mn><mn>5</mn><mo>,</mo><mtext>G</mtext><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext><mtext>e</mtext><mtext>r</mtext><mtext>y</mtext><mn>6</mn><mn>5</mn><mo>,</mo><mtext>G</mtext><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext><mtext>e</mtext><mtext>r</mtext><mtext>y</mtext><mn>7</mn><mn>5</mn><mo>,</mo><mtext>G</mtext><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext><mtext>e</mtext><mtext>r</mtext><mtext>y</mtext><mn>8</mn><mn>5</mn><mo>,</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>Ρ</mtext><mtext>r</mtext><mtext>o</mtext><mtext>b</mtext><mtext>e</mtext><mn>5</mn><mn>5</mn><mo>,</mo><mtext>Ρ</mtext><mtext>r</mtext><mtext>o</mtext><mtext>b</mtext><mtext>e</mtext><mn>6</mn><mn>5</mn><mo>,</mo><mtext>Ρ</mtext><mtext>r</mtext><mtext>o</mtext><mtext>b</mtext><mtext>e</mtext><mn>7</mn><mn>5</mn><mo>,</mo><mtext>Ρ</mtext><mtext>r</mtext><mtext>o</mtext><mtext>b</mtext><mtext>e</mtext><mn>8</mn><mn>5</mn><mo stretchy="false">}</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">中随机抽取一个角度.对于正正例,选取相同ID,并在角度集合中再抽取一个角度生成正样本对,标签值设为1.对于正负例,随机选择除已选ID外另一个ID,并在角度集合中随机抽取一个角度生成负样本对,标签值设为0.为了保证训练集正负样本均衡,抽取过程按照1∶1比例交换随机抽取正正例和正负例.CASIA-B数据集采用同样的抽样方法.该采样方法可以得到大量不同组合的训练样本对,有利于解决步态样本过少的问题.</p>
                </div>
                <div class="p1">
                    <p id="87">实验采用同样的参数设置,批尺寸为128,选择带有动量的随机梯度下降(Stochastic Gradient Descent, SGD)优化器,动量设置为0.9,权重衰减为5e-4,初始化权重采用均值为0、标准差为0.01的高斯分布,激活函数选择ReLU.学习率变化设置固定为[0.001,0.0001,0.00001],共迭代70万次.代码利用PyTorch 深度学习框架编写,在NVIDIA GeForce GTX TITAN X上训练.</p>
                </div>
                <div class="p1">
                    <p id="88">OU-ISIR-LP数据集上实验按照跨视角步态识别测试协议<citation id="227" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>.该协议选用子集解决原始数据集中部分角度缺失的问题.将数据集中1 912个所有视角齐全的子集分成2份,956个ID用于训练,956个ID用于测试.对比方法包括高质量视角转换模型(Wide Quality View Transformation Model, WQVTM)<citation id="228" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、</p>
                </div>
                <div class="p1">
                    <p id="89">基于深度学习的GEINet<citation id="229" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、带权重反馈的卷积神经网络(Feedback Weight CNN, FBW-CNN)<citation id="230" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、FMP<citation id="231" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、卷积神经网络-计时步态图(CNN-Chrono Gait Image, CNN-CGI)<citation id="232" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>,同时计算误识率(False Acceptance Rate, FAR)、拒识率(False Rejection Rate, FRR)、等错误率(Equal Error Rate, EER)和第一次命中率(Rank1),绘制受试者工作特性(Receiver Operating Characteristics, ROC)曲线.由于CNN-CGI在OU-ISIR-LP数据集上并未使用该协议子集,导致(55,85)跨视角组合情况下识别率降至80%.为与相同实验设定的论文对比,本文复现CNN-CGI在该子集的效果.由于此测试协议的数据集角度齐全,复现结果高于在论文中提供的数据,以此作为对比数据.</p>
                </div>
                <div class="p1">
                    <p id="90">表2为各方法在OU-ISIR-LP数据集上跨视角的Rank1识别率对比.由表可知本文方法在OU-ISIR-LP数据集各种跨视角组合上识别率的优越性,随着视角跨度的增大,识别效果显著高于其它方法.</p>
                </div>
                <div class="area_img" id="91">
                                            <p class="img_tit">
                                                <b>表2 各方法在OU-ISIR-LP数据集上跨视角的Rank1识 别率</b>
                                                    <br />
                                                Table 2 Rank1 cross-view recognition rate comparison of different methods on OU-ISIR-LP datasets 
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909007_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201909007_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909007_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 各方法在OU-ISIR-LP数据集上跨视角的Rank1识 别率" src="Detail/GetImg?filename=images/MSSB201909007_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="92">为了更明显展示跨视角的性能,表3为各方法在不同视角差下的识别率.当视角差达到30°,识别结果仍然较理想,比WQVTM提升近一倍,比FBW-CNN、FMP、GEINet和CNN-CGI分别提升43%、38%、16%和4%.</p>
                </div>
                <div class="area_img" id="93">
                    <p class="img_tit"><b>表3 各方法在OU-ISIR-LP数据集上不同视角差下的 Rank1识别率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Rank1 recognition rate comparison of different methods with view differences on the OU-ISIR-LP dataset </p>
                    <p class="img_note"></p>
                    <table id="93" border="1"><tr><td rowspan="2"><br />方法</td><td colspan="5"><br />视角差/°</td></tr><tr><td><br />0</td><td>10</td><td>20</td><td>30</td><td>均值</td></tr><tr><td><br />WQVTM</td><td>-</td><td>79.4</td><td>67.6</td><td>49.9</td><td>65.6</td></tr><tr><td>FBW</td><td>87.3</td><td>83.2</td><td>71.8</td><td>53.8</td><td>74.0</td></tr><tr><td><br />FMP</td><td>95.3</td><td>93.9</td><td>84.0</td><td>58.8</td><td>83.0</td></tr><tr><td><br />GEINet</td><td>94.9</td><td>93.9</td><td>90.3</td><td>80.7</td><td>89.9</td></tr><tr><td><br />CNN-CGI</td><td>98.6</td><td>98.2</td><td>97.1</td><td>92.6</td><td>96.6</td></tr><tr><td><br />本文方法</td><td>99.3</td><td>99.2</td><td>98.6</td><td>96.7</td><td>98.5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="94">表4为各方法在CASIA-B数据集上的识别率.实验采用与文献<citation id="233" type="reference">[<a class="sup">12</a>]</citation>同样的实验配置,输入图像尺寸为126×126,测试并计算在36°和54°视角跨度下的Rank1识别率.静态轮廓模板-运动轮廓模板(Static Silhouette Template-Motion Silhouette Contour Template, SST-MSCT)<citation id="234" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、CNN-CGI<citation id="235" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>和STDNN<citation id="236" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>实验结果数据来自文献<citation id="237" type="reference">[<a class="sup">12</a>]</citation>.实验表明本文方法在CASIA-B数据集上的识别结果依然较优.</p>
                </div>
                <div class="area_img" id="95">
                    <p class="img_tit"><b>表4 各方法在CASIA-B数据集上跨视角的Rank1识别率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Rank1 recognition rate comparison of different methods at a cross-view angle evaluated on CASIA-B datasets</p>
                    <p class="img_note"></p>
                    <table id="95" border="1"><tr><td>视角/°</td><td>Gallery/°</td><td>Probe/°</td><td>SST-<br />MSCT</td><td>CNN-<br />CGI</td><td>STDNN</td><td>本文<br />方法</td></tr><tr><td>36</td><td>36<br />36<br />90<br />90<br />144<br />144</td><td>0<br />72<br />54<br />126<br />108<br />180</td><td>63.4<br />59.7<br />61.2<br />66.3<br />57.8<br />64.7</td><td>71.5<br />74.6<br />76.6<br />74.3<br />72.9<br />70.2</td><td>87.3<br />90.5<br />91.2<br />92.1<br />90.1<br />89.7</td><td>97.9<br />97.9<br />97.9<br />91.7<br />95.8<br />91.7</td></tr><tr><td>54</td><td>54<br />54<br />90<br />90<br />126<br />126</td><td>0<br />108<br />36<br />144<br />72<br />180</td><td>47.8<br />49.2<br />62.5<br />57.8<br />61.5<br />51.8</td><td>54.7<br />58.5<br />62.9<br />61.1<br />57.4<br />54.9</td><td>83.7<br />86.3<br />86.4<br />87.5<br />84.3<br />82.4</td><td>83.3<br />97.9<br />91.7<br />85.4<br />97.9<br />87.5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="96">由于本文模型属于二分类验证模型,使用ROC曲线和EER衡量模型在OU-ISIR-LP数据集上的性能.对比方法包括:WQVTM<citation id="238" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,变换一致性度量(Transformation Consistency Measures, TCM+)<citation id="239" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>,FMP<citation id="240" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>,GEINet<citation id="241" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>,CNN-CGI<citation id="242" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>.图3为Gallery(g)等于55°,Probe(p)分别为55°、65°、75°的ROC曲线图.</p>
                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909007_15500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同视角组合下的ROC曲线对比．" src="Detail/GetImg?filename=images/MSSB201909007_15500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 不同视角组合下的ROC曲线对比．  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909007_15500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Comparison of ROC curves under different view combinations</p>

                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909007_15501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同视角组合下的ROC曲线对比．" src="Detail/GetImg?filename=images/MSSB201909007_15501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 不同视角组合下的ROC曲线对比．  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909007_15501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Comparison of ROC curves under different view combinations</p>

                </div>
                <div class="p1">
                    <p id="103">表5为各方法在不同视角差下的EER对比.由图3和表5可看出,本文方法分类能力较强,同时EER远低于其它方法.</p>
                </div>
                <div class="area_img" id="104">
                    <p class="img_tit"><b>表5 各方法在不同视角差下的EER对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 EER comparison of different methods with view differences on OU-ISIR-LP datasets </p>
                    <p class="img_note"></p>
                    <table id="104" border="1"><tr><td rowspan="2"><br />方法</td><td colspan="4"><br />样本对视角差/°</td><td rowspan="2">均值</td></tr><tr><td><br />0</td><td>10</td><td>20</td><td>30</td></tr><tr><td><br />WQVTM</td><td>-</td><td>3.6</td><td>4.9</td><td>6.5</td><td>5.0</td></tr><tr><td>TCM+</td><td>-</td><td>3.4</td><td>4.2</td><td>5.6</td><td>4.4</td></tr><tr><td><br />FMP</td><td>1.3</td><td>1.8</td><td>1.7</td><td>3.4</td><td>2.0</td></tr><tr><td><br />GEINet</td><td>1.2</td><td>1.3</td><td>1.6</td><td>2.5</td><td>1.7</td></tr><tr><td><br />CNN-CGI</td><td>0.3</td><td>0.6</td><td>0.8</td><td>1.1</td><td>0.7</td></tr><tr><td><br />本文方法</td><td>0.2</td><td>0.3</td><td>0.5</td><td>0.6</td><td>0.4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="105" name="105" class="anchor-tag">3 结 束 语</h3>
                <div class="p1">
                    <p id="106">针对现有步态识别方法仅通过叠加卷积层获取全局特征而缺少步态细粒度特征的局限性,本文提出结合非局部与分块特征的跨视角步态识别方法.通过提取单幅步态能量图的非局部信息,以及在中层提取融合后的相对非局部信息,有效提升步态特征的全局性.利用步态能量图的几何特性将人体水平分块后分别训练3个二值分类器,提取步态中的细粒度信息,增强类间的区分能力,最终取得较高的识别率,验证网络模型的有效性.</p>
                </div>
                <div class="p1">
                    <p id="107">本文的步态识别网络以步态能量图为输入,在实际场景中行人检测与分割、步态能量图的质量等都面临较大考验,今后将致力于提升算法的鲁棒性,提高在复杂场景下的应用性能.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="156">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Individual recognition using gait energy image">

                                <b>[1]</b> HAN J,BHANN B.Individual Recognition Using Gait Energy Image.IEEE Transactions on Pattern Analysis and Machine Intelligence,2006,28(2):316-322.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201805006&amp;v=MTQ1OTZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVnJ2S0tEN1liTEc0SDluTXFvOUY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 何逸炜,张军平.步态识别的深度学习:综述.模式识别与人工智能,2018,31(5):442-451.(HE Y W,ZHANG J P.Deep Learning for Gait Recognition:A Survey.Pattern Recognition and Artificial Intelligence,2018,31(5):442-451.)
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiple views gait recognition using view transformation model based on optimized Gait Energy Image">

                                <b>[3]</b> KUSAKUNNIRAN W,WU Q,LI H D,et al.Multiple Views Gait Recognition Using View Transformation Model Based on Optimized Gait Energy Image // Proc of the 12th IEEE International Confe-rence on Computer Vision Workshops.Washington,USA:IEEE,2009:1058-1064.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-View Gait Re-cognition by Fusion of Multiple Transformation Consistency Measures">

                                <b>[4]</b> MURAMATSU D,MAKIHARA Y,YAGI Y.Cross-View Gait Re-cognition by Fusion of Multiple Transformation Consistency Measures // Proc of the 2nd International Workshop on Biometrics and Forensics.Washington,USA:IEEE,2014.DOI:10.1109/IWBF.2014.6914253.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=View transformation model incorporating quality measures for cross-view gait recognition">

                                <b>[5]</b> MURAMATSU D,MAKIHARA Y,YAGI Y.View Transformation Model Incorporating Quality Measures for Cross-View Gait Recognition.IEEE Transactions on Cybernetics,2016,46(7):1602-1615.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201303003&amp;v=MDA4NjlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tWcnZLUHlyZmJMRzRIOUxNckk5Rlo0UUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 王科俊,阎涛,吕卓纹,等.核稀疏保留投影及在步态识别中的应用.中国图象图形学报,2013,18(3):257-263.(WANG K J,YAN T,LÜ Z W,et al.Kernel Sparsity Preserving Projections and Its Application to Gait Recognition.Journal of Image and Graphics,2013,18(3):257-263.)
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gait Recognition Based on Gabor Wavelets and (2D)2PCA">

                                <b>[7]</b> WANG X H,WANG J,YAN K.Gait Recognition Based on Gabor Wavelets and (2D)<sup>2</sup>PCA.Multimedia Tools and Applications,2018,77(10):12545-12561.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201608005&amp;v=Mjc0MTh2S0tEN1liTEc0SDlmTXA0OUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 王修晖,严珂.基于连续密度隐马尔可夫模型的人体步态识别.模式识别与人工智能,2016,29(8):709-716.(WANG X H,YAN K.Human Gait Recognition Using Continuous Density Hidden Markov Models.Pattern Recognition and Artificial Intelligence,2016,29(8):709-716.)
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES16083F8E39C987816EC60B34E5807BE7&amp;v=MDA4NjZjd2NiS1ROOCtZQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZodzdtNnhLbz1OaWZPZmJLK0h0blAyWWN3WitKOEJYUSt4eGNWbjB4N1NBM2hxRw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> BEN X Y,ZHANG P,LAI Z H,et al.A General Tensor Representation Framework for Cross-View Gait Recognition.Pattern Recognition,2019,90:87-98.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-view gait recognition using 3D convolutional neural networks">

                                <b>[10]</b> WOLF T,BABAEE M,RIGOLL G.Multi-view Gait Recognition Using 3D Convolutional Neural Networks // Proc of the IEEE International Conference on Image Processing.Washington,USA:IEEE,2016:4165-4169.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738259&amp;v=MDgzMjVUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSjE4UmJoQT1OaWZPZmJLN0h0RE5xWTlGWStnSERua3dvQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> LAM T H W,CHEUNG K H,LIU J N K.Gait Flow Image:A Silhouette-Based Gait Representation for Human Identification.Pa-ttern Recognition,2011,44(4):973-987.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-view Gait Recognition Based on a Spatial-Temporal Deep Neural Network">

                                <b>[12]</b> TONG S B,FU Y Z,YUE X W,et al.Multi-view Gait Recognition Based on a Spatial-Temporal Deep Neural Network.IEEE Access,2018,6:57583-57596.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature Map Pooling for Cross-View Gait Recognition Based on Silhouette Sequence Images[C/OL]">

                                <b>[13]</b> CHEN Q,WANG Y H,LIU Z,et al.Feature Map Pooling for Cross-View Gait Recognition Based on Silhouette Sequence Images[C/OL].[2019-02-22].https://arxiv.org/pdf/1711.09358.pdf.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1AA5D468991FD02C0250EB0A1631931E&amp;v=MjMxOTc0bE5iZUlPZWdnNXpXVVQ2RHA5UFEzaTNSTXplck9kUnJ2cUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHc3bTZ4S289TmlmT2ZiTEpiOVM0cQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> WU H M,WENG J,CHEN X,et al.Feedback Weight Convolutional Neural Network for Gait Recognition.Journal of Visual Communication and Image Representation,2018,55:424-432.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GEINet:View-Invariant Gait Recognition Using a Convolutional Neural Network">

                                <b>[15]</b> SHIRAGA K,MAKIHARA Y,MURAMATSU D,et al.GEINet:View-Invariant Gait Recognition Using a Convolutional Neural Network // Proc of the International Conference on Biometrics.Wa-shington,USA:IEEE,2016.DOI:10.1109/ICB.2016.7550060.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Comprehensive Study on Cross-View Gait Based Human Identification with Deep CNNs">

                                <b>[16]</b> WU Z F,HUANG Y Z,WANG L,et al.A Comprehensive Study on Cross-View Gait Based Human Identification with Deep CNNs.IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(2):209-226.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">

                                <b>[17]</b> KRIZHEVSKY A,SUTSKEVER J,HINTON G.ImageNet Classification with Deep Convolutional Neural Networks // Proc of the 25th International Conference on Neural Information Processing Systems.Cambridge,USA:The MIT Press,2012,I:1097-1105.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[18]</b> HE K M,ZHANG X Y,REN S Q,et al.Deep Residual Learning for Image Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016,I:770-778.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-View Gait Recognition through Ensemble Learning">

                                <b>[19]</b> WANG X H,YAN W Q.Cross-View Gait Recognition through Ensemble Learning // Proc of the Thematic Workshops of ACM Multimedia.New York,USA:ACM,2017:385-392.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The OU-ISIR Gait Database Comprising the Large Population Dataset and Performance Evaluation of Gait Recognition">

                                <b>[20]</b> IWAMA H,OKUMURA M,MAKIHARA Y,et al.The OU-ISIR Gait Database Comprising the Large Population Dataset and Performance Evaluation of Gait Recognition.IEEE Transactions on Information Forensics and Security,2012,7(5):1511-1521.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-view Large Population Gait Dataset and Its Performance Evaluation for Cross-View Gait Recognition[C/OL]">

                                <b>[21]</b> TAKEMURA N,MAKIHARA Y,MURAMATSU D,et al.Multi-view Large Population Gait Dataset and Its Performance Evaluation for Cross-View Gait Recognition[C/OL].[2019-02-22].https://link.springer.com/content/pdf/10.1186%2Fs41074-018-0039-6.pdf.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Framework for Evaluating the Effect of View Angle, Clothing and Carrying Condition on Gait Recognition">

                                <b>[22]</b> YU S Q,TAN D L,TAN T N.A Framework for Evaluating the Effect of View Angle,Clothing and Carrying Condition on Gait Recognition // Proc of the 18th International Conference on Pattern Recognition.Washington,USA:IEEE,2006,IV:441-444.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201909007" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201909007&amp;v=MTA0NTBqTXBvOUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVnJ2S0tEN1liTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
