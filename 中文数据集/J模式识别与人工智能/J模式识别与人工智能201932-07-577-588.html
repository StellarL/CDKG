<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131453724405000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201907001%26RESULT%3d1%26SIGN%3daR5M1eZJc66hvdYIyLhTLZhx0dA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201907001&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201907001&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201907001&amp;v=Mjk1NjdlWmVSbkZ5emhWTC9MS0Q3WWJMRzRIOWpNcUk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#175" data-title="1 基本概念 ">1 基本概念</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#176" data-title="&lt;b&gt;1.1 ACP与平行学习&lt;/b&gt;"><b>1.1 ACP与平行学习</b></a></li>
                                                <li><a href="#181" data-title="&lt;b&gt;1.2 平行医学图像&lt;/b&gt;"><b>1.2 平行医学图像</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#183" data-title="2 平行皮肤框架及相关工作 ">2 平行皮肤框架及相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#187" data-title="&lt;b&gt;2.1 皮肤数据收集&lt;/b&gt;"><b>2.1 皮肤数据收集</b></a></li>
                                                <li><a href="#213" data-title="&lt;b&gt;2.2 计算实验方法与预测学习&lt;/b&gt;"><b>2.2 计算实验方法与预测学习</b></a></li>
                                                <li><a href="#217" data-title="&lt;b&gt;2.3 基于平行学习的闭环优化&lt;/b&gt;"><b>2.3 基于平行学习的闭环优化</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#221" data-title="3 基于生成对抗方法的平行皮肤应用实例 ">3 基于生成对抗方法的平行皮肤应用实例</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#223" data-title="&lt;b&gt;3.1 基于描述学习与指示学习生成仿真皮肤图像&lt;/b&gt;"><b>3.1 基于描述学习与指示学习生成仿真皮肤图像</b></a></li>
                                                <li><a href="#229" data-title="&lt;b&gt;3.2 基于预测学习的分类验证实验&lt;/b&gt;"><b>3.2 基于预测学习的分类验证实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#237" data-title="4 思考与展望 ">4 思考与展望</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#239" data-title="&lt;b&gt;4.1 医学可解释性&lt;/b&gt;"><b>4.1 医学可解释性</b></a></li>
                                                <li><a href="#241" data-title="&lt;b&gt;4.2 美容皮肤学&lt;/b&gt;"><b>4.2 美容皮肤学</b></a></li>
                                                <li><a href="#243" data-title="&lt;b&gt;4.3 生物皮肤研究&lt;/b&gt;"><b>4.3 生物皮肤研究</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#245" data-title="5 结 束 语 ">5 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#180" data-title="图1 平行学习框架">图1 平行学习框架</a></li>
                                                <li><a href="#185" data-title="图2 平行皮肤框架">图2 平行皮肤框架</a></li>
                                                <li><a href="#194" data-title="图3 人工皮肤图像系统显式扩充真实图像示例">图3 人工皮肤图像系统显式扩充真实图像示例</a></li>
                                                <li><a href="#199" data-title="图4 基于生成对抗网络结构的皮肤图像生成">图4 基于生成对抗网络结构的皮肤图像生成</a></li>
                                                <li><a href="#203" data-title="图5 基于风格转换的皮肤图像生成">图5 基于风格转换的皮肤图像生成</a></li>
                                                <li><a href="#212" data-title="&lt;b&gt;表1 开源皮肤图像数据集&lt;/b&gt;"><b>表1 开源皮肤图像数据集</b></a></li>
                                                <li><a href="#216" data-title="图6 基于GoogleNet实现皮肤分类的网络结构">图6 基于GoogleNet实现皮肤分类的网络结构</a></li>
                                                <li><a href="#225" data-title="图7 DCGAN生成器网络结构">图7 DCGAN生成器网络结构</a></li>
                                                <li><a href="#227" data-title="&lt;b&gt;表2 HAM10000 数据集扩充前后各类图像数量&lt;/b&gt;"><b>表2 HAM10000 数据集扩充前后各类图像数量</b></a></li>
                                                <li><a href="#228" data-title="图8 生成的仿真皮肤图像">图8 生成的仿真皮肤图像</a></li>
                                                <li><a href="#231" data-title="图9 基于平行皮肤的分类验证实验流程图">图9 基于平行皮肤的分类验证实验流程图</a></li>
                                                <li><a href="#233" data-title="&lt;b&gt;表3 真实皮肤图像分类结果&lt;/b&gt;"><b>表3 真实皮肤图像分类结果</b></a></li>
                                                <li><a href="#234" data-title="&lt;b&gt;表4 基于平行皮肤优化后的分类结果&lt;/b&gt;"><b>表4 基于平行皮肤优化后的分类结果</b></a></li>
                                                <li><a href="#236" data-title="图10 基于平行皮肤的分类结果混淆矩阵">图10 基于平行皮肤的分类结果混淆矩阵</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" ESTEVA A, KUPREL B, NOVOA R A, &lt;i&gt;et al&lt;/i&gt;.Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks.Nature, 2017, 542 (7639) :115-118." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dermatologist-level classification of skin cancer with deep neural networks">
                                        <b>[1]</b>
                                         ESTEVA A, KUPREL B, NOVOA R A, &lt;i&gt;et al&lt;/i&gt;.Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks.Nature, 2017, 542 (7639) :115-118.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" HAY R J, JOHNS N E, WILLIAMS H C, &lt;i&gt;et al&lt;/i&gt;.The Global Burden of Skin Disease in 2010:An Analysis of the Prevalence and Impact of Skin Conditions.Journal of Investigative Dermatology, 2014, 134 (6) :1527-1534." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES04E6EF0FAA0B9ED3F471E3798297A7A3&amp;v=MDY1MDJwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0x5NHdLcz1OaWZPZmJPOGE5ZTUyWTh6RlpvUGZuVk11eFZsN2poOFBYemxwUm8zY0xYbFFzdWNDT052RlNpV1dyN0pJRg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         HAY R J, JOHNS N E, WILLIAMS H C, &lt;i&gt;et al&lt;/i&gt;.The Global Burden of Skin Disease in 2010:An Analysis of the Prevalence and Impact of Skin Conditions.Journal of Investigative Dermatology, 2014, 134 (6) :1527-1534.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" ROGERS H W, WEINSTOCK M A, FELDMAN S R, &lt;i&gt;et al&lt;/i&gt;.Incidence Estimate of Nonmelanoma Skin Cancer (Keratinocyte Carcinomas) in the US Population, 2012.JAMA Dermatology, 2015, 151 (10) :1081-1086." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Incidence Estimate of Nonmelanoma Skin Cancer (Keratinocyte Carcinomas)in the U.S.Population,2012">
                                        <b>[3]</b>
                                         ROGERS H W, WEINSTOCK M A, FELDMAN S R, &lt;i&gt;et al&lt;/i&gt;.Incidence Estimate of Nonmelanoma Skin Cancer (Keratinocyte Carcinomas) in the US Population, 2012.JAMA Dermatology, 2015, 151 (10) :1081-1086.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" BRAY F, FERLAY J, SOERJOMATARAM I, &lt;i&gt;et al&lt;/i&gt;.Global Cancer Statistics 2018:Globocan Estimates of Incidence and Mortality Worldwide for 36 Cancers in 185 Countries.CA:A Cancer Journal for Clinicians, 2018, 68 (6) :394-424." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWDD72DD4502879DD556EBFEC6ABBE160F5&amp;v=MTcxMTNpZmNhc2UvSEtXNHE0cEZadU1JQlFoTnloTVZuMDBMUFF6azNXQkhETE9TUmN5YUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHdMeTR3S3M9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         BRAY F, FERLAY J, SOERJOMATARAM I, &lt;i&gt;et al&lt;/i&gt;.Global Cancer Statistics 2018:Globocan Estimates of Incidence and Mortality Worldwide for 36 Cancers in 185 Countries.CA:A Cancer Journal for Clinicians, 2018, 68 (6) :394-424.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 何雪英, 韩忠义, 魏本征.基于深度卷积神经网络的色素性皮肤病识别分类.计算机应用, 2018, 38 (11) :3236-3240. (HE X Y, HAN Z Y, WEI B Z.Pigmented Skin Lesion Recognition and Classification Based on Deep Convolutional Neural Network.Journal of Computer Applications, 2018, 38 (11) :3236-3240.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201811034&amp;v=Mjg3OTJGeXpoVkwvTEx6N0JkN0c0SDluTnJvOUdZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         何雪英, 韩忠义, 魏本征.基于深度卷积神经网络的色素性皮肤病识别分类.计算机应用, 2018, 38 (11) :3236-3240. (HE X Y, HAN Z Y, WEI B Z.Pigmented Skin Lesion Recognition and Classification Based on Deep Convolutional Neural Network.Journal of Computer Applications, 2018, 38 (11) :3236-3240.) 
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" LECUN Y, BENGIO Y, HINTON G.Deep Learning.Nature, 2015, 521 (7553) :436-444." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning">
                                        <b>[6]</b>
                                         LECUN Y, BENGIO Y, HINTON G.Deep Learning.Nature, 2015, 521 (7553) :436-444.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" 田娟秀, 刘国才, 谷珊珊, 等.医学图像分析深度学习方法研究与挑战.自动化学报, 2018, 44 (3) :401-424. (TIAN J X, LIU G X C, GU S S, &lt;i&gt;et al&lt;/i&gt;.Deep Learning in Medical Image Analysis and Its Challenges.Acta Automatica Sinica, 2018, 44 (3) :401-424.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201803002&amp;v=MTQxODBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emhWTC9MS0NMZlliRzRIOW5Nckk5RlpvUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         田娟秀, 刘国才, 谷珊珊, 等.医学图像分析深度学习方法研究与挑战.自动化学报, 2018, 44 (3) :401-424. (TIAN J X, LIU G X C, GU S S, &lt;i&gt;et al&lt;/i&gt;.Deep Learning in Medical Image Analysis and Its Challenges.Acta Automatica Sinica, 2018, 44 (3) :401-424.) 
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" LITJENS G, KOOI T, BEJNORDI B E, &lt;i&gt;et al&lt;/i&gt;.A Survey on Deep Learning in Medical Image Analysis.Medical Image Analysis, 2017, 42:60-88." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES30D6C0E9B3B77AC4A1F9C7A8227303EC&amp;v=MTg5OTdzL3NDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3THk0d0tzPU5pZk9mYkM0YXRlL3IvcE1GdWg5QzN0SXZCSmk2MGwwTzNpVHBCQTNmckdVUg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         LITJENS G, KOOI T, BEJNORDI B E, &lt;i&gt;et al&lt;/i&gt;.A Survey on Deep Learning in Medical Image Analysis.Medical Image Analysis, 2017, 42:60-88.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" XIE F Y, FAN H D, LI Y, &lt;i&gt;et al&lt;/i&gt;.Melanoma Classification on Dermoscopy Images Using a Neural Network Ensemble Model.IEEE Transactions on Medical Imaging, 2017, 36 (3) :849-858." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Melanoma classification on dermoscopy images using a neural network ensemble model">
                                        <b>[9]</b>
                                         XIE F Y, FAN H D, LI Y, &lt;i&gt;et al&lt;/i&gt;.Melanoma Classification on Dermoscopy Images Using a Neural Network Ensemble Model.IEEE Transactions on Medical Imaging, 2017, 36 (3) :849-858.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" HAENSSLE H A, FINK C, SCHNEIDERBAUER R, &lt;i&gt;et al&lt;/i&gt;.Man Against Machine:Diagnostic Performance of a Deep Learning Convolutional Neural Network for Dermoscopic Melanoma Recognition in Comparison to 58 Dermatologists.Annals of Oncology, 2018, 29 (8) :1836-1842." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Man against machine:diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58dermatologists">
                                        <b>[10]</b>
                                         HAENSSLE H A, FINK C, SCHNEIDERBAUER R, &lt;i&gt;et al&lt;/i&gt;.Man Against Machine:Diagnostic Performance of a Deep Learning Convolutional Neural Network for Dermoscopic Melanoma Recognition in Comparison to 58 Dermatologists.Annals of Oncology, 2018, 29 (8) :1836-1842.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" CELEBI M E, CODELLA N, HALPERN A.Dermoscopy Image Analysis:Overview and Future Directions.IEEE Journal of Biomedical and Health Informatics, 2019, 23 (2) :474-478." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dermoscopy Image Analysis:Overview and Future Directions">
                                        <b>[11]</b>
                                         CELEBI M E, CODELLA N, HALPERN A.Dermoscopy Image Analysis:Overview and Future Directions.IEEE Journal of Biomedical and Health Informatics, 2019, 23 (2) :474-478.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 王飞跃.平行系统方法与复杂系统的管理和控制.控制与决策, 2004, 19 (5) :485-489, 514. (WANG F Y.Parallel System Methods for Management and Control of Complex Systems.Control and Decision, 2004, 19 (5) :485-489, 514.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC200405001&amp;v=MDU2ODJDVVJMT2VaZVJuRnl6aFZML0xMamZTYmJHNEh0WE1xbzlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         王飞跃.平行系统方法与复杂系统的管理和控制.控制与决策, 2004, 19 (5) :485-489, 514. (WANG F Y.Parallel System Methods for Management and Control of Complex Systems.Control and Decision, 2004, 19 (5) :485-489, 514.) 
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" WANG F Y.Parallel Control and Nanagement for Intelligent Transportation Systems:Concepts, Architectures, and Applications.IEEE Transactions on Intelligent Transportation Systems, 2010, 11 (3) :630-638." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parallel control and management for intelligent transportation systems: Concepts, architectures, and applications">
                                        <b>[13]</b>
                                         WANG F Y.Parallel Control and Nanagement for Intelligent Transportation Systems:Concepts, Architectures, and Applications.IEEE Transactions on Intelligent Transportation Systems, 2010, 11 (3) :630-638.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" WANG F Y, ZHENG N N, CAO D P, &lt;i&gt;et al&lt;/i&gt;.Parallel Driving in CPSS:A Unified Approach for Transport Automation and Vehicle Intelligence.IEEE/CAA Journal of AutomaticaSinica, 2017, 4 (4) :577-587." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDHB201704001&amp;v=MTMxNDVMRzRIOWJNcTQ5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emhWTC9MUHluRGI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         WANG F Y, ZHENG N N, CAO D P, &lt;i&gt;et al&lt;/i&gt;.Parallel Driving in CPSS:A Unified Approach for Transport Automation and Vehicle Intelligence.IEEE/CAA Journal of AutomaticaSinica, 2017, 4 (4) :577-587.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" LI L, WANG X, WANG K F, &lt;i&gt;et al&lt;/i&gt;.Parallel Testing of Vehicle Intelligence via Virtual-Real Interaction.Science Robotics, 2019, 4 (28) .DOI:10.1126/scirobotics.aaw4106." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parallel testing of vehicle intelligence via virtual-real interaction">
                                        <b>[15]</b>
                                         LI L, WANG X, WANG K F, &lt;i&gt;et al&lt;/i&gt;.Parallel Testing of Vehicle Intelligence via Virtual-Real Interaction.Science Robotics, 2019, 4 (28) .DOI:10.1126/scirobotics.aaw4106.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" >
                                        <b>[16]</b>
                                     吕宜生, 陈圆圆, 金峻臣, 等.平行交通:虚实互动的智能交通管理与控制.智能科学与技术学报, 2019, 1 (1) :21-33. (L&#220; Y S, CHEN Y Y, JIN J C, &lt;i&gt;et al&lt;/i&gt;.Parallel Transportation:Virtual-Real Interaction for Intelligent Traffic Management and Control.Chinese Journal of Intelligent Science and Technology, 2019, 1 (1) :21-33.) </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" >
                                        <b>[17]</b>
                                     刘腾, 王晓, 邢阳, 等.基于数字四胞胎的平行驾驶系统及应用.智能科学与技术学报, 2019, 1 (1) :40-51. (LIU T, WANG X, XING Y, &lt;i&gt;et al&lt;/i&gt;.Research on Digital Quadruplets in Cyber-Physical-Social Space-Based Parallel Driving.Chinese Journal of Intelligent Science and Technology, 2019, 1 (1) :40-51.) </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" WANG F Y, WANG X, LI L X, &lt;i&gt;et al&lt;/i&gt;.Steps toward Parallel Intelligence.IEEE/CAA Journal of Automatica Sinica, 2016, 3 (4) :345-348." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDHB201604001&amp;v=Mjc5NjVMRzRIOWZNcTQ5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emhWTC9MUHluRGI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         WANG F Y, WANG X, LI L X, &lt;i&gt;et al&lt;/i&gt;.Steps toward Parallel Intelligence.IEEE/CAA Journal of Automatica Sinica, 2016, 3 (4) :345-348.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" WANG F Y, ZHANG J J, ZHENG X H, &lt;i&gt;et al&lt;/i&gt;.Where Does Alphago Go:From Church-Turing Thesis to Alphago Thesis and Beyond.IEEE/CAA Journal of Automatica Sinica, 2016, 3 (2) :113-120." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDHB201602001&amp;v=MDM0NTA1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZML0xQeW5EYkxHNEg5Zk1yWTlGWllRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         WANG F Y, ZHANG J J, ZHENG X H, &lt;i&gt;et al&lt;/i&gt;.Where Does Alphago Go:From Church-Turing Thesis to Alphago Thesis and Beyond.IEEE/CAA Journal of Automatica Sinica, 2016, 3 (2) :113-120.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" WANG F Y, YUAN Y, LI J J, &lt;i&gt;et al&lt;/i&gt;.From Intelligent Vehicles to Smart Societies:A Parallel Driving Approach.IEEE Transactions on Computational Social Systems, 2018, 5 (3) :594-604." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=From Intelligent Vehicles to Smart Societies:A Parallel Driving Approach">
                                        <b>[20]</b>
                                         WANG F Y, YUAN Y, LI J J, &lt;i&gt;et al&lt;/i&gt;.From Intelligent Vehicles to Smart Societies:A Parallel Driving Approach.IEEE Transactions on Computational Social Systems, 2018, 5 (3) :594-604.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" WANG F Y, WANG P, LI J J, &lt;i&gt;et al&lt;/i&gt;.Social Transportation:Social Signal and Technology for Transportation Engineering.IEEE Tran-sactions on Computational Social Systems, 2019, 6 (1) :2-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Social Transportation:Social Signal and Technology for Transportation Engineering">
                                        <b>[21]</b>
                                         WANG F Y, WANG P, LI J J, &lt;i&gt;et al&lt;/i&gt;.Social Transportation:Social Signal and Technology for Transportation Engineering.IEEE Tran-sactions on Computational Social Systems, 2019, 6 (1) :2-7.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" WANG F Y, ZHANG J J, QIN R, &lt;i&gt;et al&lt;/i&gt;.Social Energy:Emerging Token Economy for Energy Production and Consumption.IEEE Transactions on Computational Social Systems, 2019, 6 (3) :388-393." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Social Energy:Emerging Token Economy for Energy Production and Consumption">
                                        <b>[22]</b>
                                         WANG F Y, ZHANG J J, QIN R, &lt;i&gt;et al&lt;/i&gt;.Social Energy:Emerging Token Economy for Energy Production and Consumption.IEEE Transactions on Computational Social Systems, 2019, 6 (3) :388-393.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" WANG K F, GOU C, ZHENG N N, &lt;i&gt;et al&lt;/i&gt;.Parallel Vision for Perception and Understanding of Complex Scenes:Methods, Framework, and Perspectives.Artificial Intelligence Review, 2017, 48 (3) :299-329." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parallel Vision for Perception and Understanding of Complex Scenes:Methods,Framework,and Perspectives">
                                        <b>[23]</b>
                                         WANG K F, GOU C, ZHENG N N, &lt;i&gt;et al&lt;/i&gt;.Parallel Vision for Perception and Understanding of Complex Scenes:Methods, Framework, and Perspectives.Artificial Intelligence Review, 2017, 48 (3) :299-329.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" GOU C, ZHANG H, WANG K F, &lt;i&gt;et al&lt;/i&gt;.Cascade Learning from Adversarial Synthetic Images for Accurate Pupil Detection.Pattern Recognition, 2019, 88:584-594." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES77299E0B8A188971A7818AF4D9CA52B7&amp;v=MzIwNTZDc09SUjhpWUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHdMeTR3S3M9TmlmT2ZiUy9ITmpGMm84M2JKb09CSFF3eUJkaTdUZDhRQTZVcUdZOA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                         GOU C, ZHANG H, WANG K F, &lt;i&gt;et al&lt;/i&gt;.Cascade Learning from Adversarial Synthetic Images for Accurate Pupil Detection.Pattern Recognition, 2019, 88:584-594.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" WANG F Y, WONG P K.Intelligent Systems and Technology for Integrative and Predictive Medicine:An ACP Approach.ACM Transactions on Intelligent Systems and Technology (TIST) , 2013, 4 (2) .DOI:10.1145/2438653.2438667." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Intelligent Systems and Technologyfor Integrative and Predictive Medicine:An ACP Approach">
                                        <b>[25]</b>
                                         WANG F Y, WONG P K.Intelligent Systems and Technology for Integrative and Predictive Medicine:An ACP Approach.ACM Transactions on Intelligent Systems and Technology (TIST) , 2013, 4 (2) .DOI:10.1145/2438653.2438667.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_26" title=" 王飞跃, 李长贵, 国元元, 等.平行高特:基于ACP的平行痛风诊疗系统框架.模式识别与人工智能, 2017, 30 (12) :1057-1068. (WANG F Y, LI C G, GUO Y Y, &lt;i&gt;et al&lt;/i&gt;.Parallel Gout:An ACP-Based System Framework for Gout Diagnosis and Treatment.Pattern Recognition and Artificial Intelligence, 2017, 30 (12) :1057-1068.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201712001&amp;v=MTk5ODBLRDdZYkxHNEg5Yk5yWTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZML0w=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         王飞跃, 李长贵, 国元元, 等.平行高特:基于ACP的平行痛风诊疗系统框架.模式识别与人工智能, 2017, 30 (12) :1057-1068. (WANG F Y, LI C G, GUO Y Y, &lt;i&gt;et al&lt;/i&gt;.Parallel Gout:An ACP-Based System Framework for Gout Diagnosis and Treatment.Pattern Recognition and Artificial Intelligence, 2017, 30 (12) :1057-1068.) 
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_27" title=" 孟祥冰, 王蓉, 张梅, 等.平行感知:ACP 理论在视觉SLAM技术中的应用.指挥与控制学报, 2017, 3 (4) :350-358. (MENG X B, WAGN R, ZHANG M, &lt;i&gt;et al&lt;/i&gt;.Parallel Perception:An ACP-Based Approach to Visual SLAM.Journal of Command and Control, 2017, 3 (4) :350-358.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZHKZ201704015&amp;v=MDE4MTJxNDlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZML0xQeVhBZExHNEg5Yk0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         孟祥冰, 王蓉, 张梅, 等.平行感知:ACP 理论在视觉SLAM技术中的应用.指挥与控制学报, 2017, 3 (4) :350-358. (MENG X B, WAGN R, ZHANG M, &lt;i&gt;et al&lt;/i&gt;.Parallel Perception:An ACP-Based Approach to Visual SLAM.Journal of Command and Control, 2017, 3 (4) :350-358.) 
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_28" title=" 王飞跃, 张梅, 孟祥冰, 等.平行手术:基于ACP的智能手术计算方法.模式识别与人工智能, 2017, 30 (11) :961-970. (WANG F Y, ZHANG M, MEGN X B, &lt;i&gt;et al&lt;/i&gt;.Parallel Surgery:An ACP-Based Approach for Intelligent Operations.Pattern Recognition and Artificial Intelligence, 2017, 30 (11) :961-970.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201711001&amp;v=MzAzMTBiTnJvOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpoVkwvTEtEN1liTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                         王飞跃, 张梅, 孟祥冰, 等.平行手术:基于ACP的智能手术计算方法.模式识别与人工智能, 2017, 30 (11) :961-970. (WANG F Y, ZHANG M, MEGN X B, &lt;i&gt;et al&lt;/i&gt;.Parallel Surgery:An ACP-Based Approach for Intelligent Operations.Pattern Recognition and Artificial Intelligence, 2017, 30 (11) :961-970.) 
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_29" title=" WANG F Y, TANG Y, LIU X, &lt;i&gt;et al&lt;/i&gt;.Social Education:Opportunities and Challenges in Cyber-Physical-Social Space.IEEE Transactions on Computational Social Systems, 2019, 6 (2) :191-196." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Social Education:Opportunities and Challenges in Cyber-Physical-Social Space">
                                        <b>[29]</b>
                                         WANG F Y, TANG Y, LIU X, &lt;i&gt;et al&lt;/i&gt;.Social Education:Opportunities and Challenges in Cyber-Physical-Social Space.IEEE Transactions on Computational Social Systems, 2019, 6 (2) :191-196.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_30" title=" ZHANG C, LIU Y H, LI L, &lt;i&gt;et al&lt;/i&gt;.Joint Task Difficulties Estimation and Testees Ranking for Intelligence Evaluation.IEEE Tran-sactions on Computational Social Systems, 2019, 6 (2) :221-226." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint Task Difficulties Estimation and Testees Ranking for Intelligence Evaluation">
                                        <b>[30]</b>
                                         ZHANG C, LIU Y H, LI L, &lt;i&gt;et al&lt;/i&gt;.Joint Task Difficulties Estimation and Testees Ranking for Intelligence Evaluation.IEEE Tran-sactions on Computational Social Systems, 2019, 6 (2) :221-226.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_31" title=" 王坤峰, 鲁越, 王雨桐, 等.平行图像:图像生成的一个新型理论框架.模式识别与人工智能, 2017, 30 (7) :577-587. (WANG K F, LU Y, WANG Y T, &lt;i&gt;et al&lt;/i&gt;.Parallel Imaging:A New Theoretical Framework for Image Generation.Pattern Recognition and Artificial Intelligence, 2017, 30 (7) :577-587.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201707001&amp;v=MTY5OThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZML0xLRDdZYkxHNEg5Yk1xSTlGWlk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[31]</b>
                                         王坤峰, 鲁越, 王雨桐, 等.平行图像:图像生成的一个新型理论框架.模式识别与人工智能, 2017, 30 (7) :577-587. (WANG K F, LU Y, WANG Y T, &lt;i&gt;et al&lt;/i&gt;.Parallel Imaging:A New Theoretical Framework for Image Generation.Pattern Recognition and Artificial Intelligence, 2017, 30 (7) :577-587.) 
                                    </a>
                                </li>
                                <li id="65">


                                    <a id="bibliography_32" title=" 王坤峰, 苟超, 王飞跃.平行视觉:基于ACP的智能视觉计算方法.自动化学报, 2016, 42 (10) :1490-1500. (WANG K F, GOU C, WANG F Y.Parallel Vision:An ACP-Based Approach to Intelligent Vision Computing.Acta Automatica Sinica, 2016, 42 (10) :1490-1500.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201610003&amp;v=MDUzNjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZML0xLQ0xmWWJHNEg5Zk5yNDlGWjRRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[32]</b>
                                         王坤峰, 苟超, 王飞跃.平行视觉:基于ACP的智能视觉计算方法.自动化学报, 2016, 42 (10) :1490-1500. (WANG K F, GOU C, WANG F Y.Parallel Vision:An ACP-Based Approach to Intelligent Vision Computing.Acta Automatica Sinica, 2016, 42 (10) :1490-1500.) 
                                    </a>
                                </li>
                                <li id="67">


                                    <a id="bibliography_33" title=" PEZESHK A, PETRICK N, CHEN W, &lt;i&gt;et al&lt;/i&gt;.Seamless Lesion Insertion for Data Augmentation in CAD Training.IEEE Transactions on Medical Imaging, 2017, 36 (4) :1005-1015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Seamless Lesion Insertion for Data Augmentation in CAD Training">
                                        <b>[33]</b>
                                         PEZESHK A, PETRICK N, CHEN W, &lt;i&gt;et al&lt;/i&gt;.Seamless Lesion Insertion for Data Augmentation in CAD Training.IEEE Transactions on Medical Imaging, 2017, 36 (4) :1005-1015.
                                    </a>
                                </li>
                                <li id="69">


                                    <a id="bibliography_34" title=" GOU C, WU Y, WANG K, &lt;i&gt;et al&lt;/i&gt;.Learning-by-Synthesis for Accurate Eye Detection // Proc of the 23rd International Conference on Pattern Recognition.Washington, USA:IEEE, 2016:3362-3367." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning-by-Synthesis for Accurate Eye Detection">
                                        <b>[34]</b>
                                         GOU C, WU Y, WANG K, &lt;i&gt;et al&lt;/i&gt;.Learning-by-Synthesis for Accurate Eye Detection // Proc of the 23rd International Conference on Pattern Recognition.Washington, USA:IEEE, 2016:3362-3367.
                                    </a>
                                </li>
                                <li id="71">


                                    <a id="bibliography_35" title=" GOU C, WU Y, WANG K F, &lt;i&gt;et al&lt;/i&gt;.A Joint Cascaded Framework for Simultaneous Eye Detection and Eye State Estimation.Pattern Recognition, 2017, 67:23-31." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES69A82D54179C110F0C0B42C20756622F&amp;v=MTEzNTNoSXlmTFNTUjdqcENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHdMeTR3S3M9TmlmT2ZiV3hiOW5PMjRwQlpld0dmMzA0ejJBVG1UOFBUSDJScg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[35]</b>
                                         GOU C, WU Y, WANG K F, &lt;i&gt;et al&lt;/i&gt;.A Joint Cascaded Framework for Simultaneous Eye Detection and Eye State Estimation.Pattern Recognition, 2017, 67:23-31.
                                    </a>
                                </li>
                                <li id="73">


                                    <a id="bibliography_36" title=" 李力, 林懿伦, 曹东璞, 等.平行学习——机器学习的一个新型理论框架.自动化学报, 2017, 43 (1) :1-8. (LI L, LIN Y L, CAO D P, &lt;i&gt;et al&lt;/i&gt;.Parallel Learning-A New Framework for Machine Learning.Acta Automatica Sinica, 2017, 43 (1) :1-8.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201701002&amp;v=MjQ4MjNVUkxPZVplUm5GeXpoVkwvTEtDTGZZYkc0SDliTXJvOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[36]</b>
                                         李力, 林懿伦, 曹东璞, 等.平行学习——机器学习的一个新型理论框架.自动化学报, 2017, 43 (1) :1-8. (LI L, LIN Y L, CAO D P, &lt;i&gt;et al&lt;/i&gt;.Parallel Learning-A New Framework for Machine Learning.Acta Automatica Sinica, 2017, 43 (1) :1-8.) 
                                    </a>
                                </li>
                                <li id="75">


                                    <a id="bibliography_37" title=" LI L, ZHENG N N, WANG F Y.On the Crossroad of Artificial Intelligence:A Revisit to Alan Turing and Norbert Wiener.IEEE Transactions on Cybernetics, 2019, 49 (10) :3618-3626." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the Crossroad of Artificial Intelligence:A Revisit to Alan Turing and Norbert Wiener">
                                        <b>[37]</b>
                                         LI L, ZHENG N N, WANG F Y.On the Crossroad of Artificial Intelligence:A Revisit to Alan Turing and Norbert Wiener.IEEE Transactions on Cybernetics, 2019, 49 (10) :3618-3626.
                                    </a>
                                </li>
                                <li id="77">


                                    <a id="bibliography_38" title=" GOU C, SHEN T Y, ZHENG W B, &lt;i&gt;et al&lt;/i&gt;.Parallel Medical Imaging:A New Data-Knowledge-Driven Evolutionary Framework for Medical Image Analysis[C/OL].[2019-06-25].https://arxiv.org/pdf/1903.04855.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parallel Medical Imaging:A New Data-Knowledge-Driven Evolutionary Framework for Medical Image Analysis[C/OL]">
                                        <b>[38]</b>
                                         GOU C, SHEN T Y, ZHENG W B, &lt;i&gt;et al&lt;/i&gt;.Parallel Medical Imaging:A New Data-Knowledge-Driven Evolutionary Framework for Medical Image Analysis[C/OL].[2019-06-25].https://arxiv.org/pdf/1903.04855.pdf.
                                    </a>
                                </li>
                                <li id="79">


                                    <a id="bibliography_39" title=" 刘昕, 王晓, 张卫山, 等.平行数据:从大数据到数据智能.模式识别与人工智能, 2017, 30 (8) :673-681. (LIU X, WANG X, ZHANG W S, &lt;i&gt;et al&lt;/i&gt;.Parallel Data:From Big Data to Data Intelligence.Pattern Recognition and Artificial Intelligence, 2017, 30 (8) :673-681.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201708001&amp;v=MDY4NjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpoVkwvTEtEN1liTEc0SDliTXA0OUZaWVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[39]</b>
                                         刘昕, 王晓, 张卫山, 等.平行数据:从大数据到数据智能.模式识别与人工智能, 2017, 30 (8) :673-681. (LIU X, WANG X, ZHANG W S, &lt;i&gt;et al&lt;/i&gt;.Parallel Data:From Big Data to Data Intelligence.Pattern Recognition and Artificial Intelligence, 2017, 30 (8) :673-681.) 
                                    </a>
                                </li>
                                <li id="81">


                                    <a id="bibliography_40" title=" ZHENG N N, LIU Z Y, REN P J, &lt;i&gt;et al&lt;/i&gt;.Hybrid-Augmented Intelligence:Collaboration and Cognition.Frontiers of Information Technology and Electronic Engineering, 2017, 18 (2) :153-179." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JZUS201702001&amp;v=MTk4NzJyWTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZML0xMemZlZmJHNEg5Yk0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[40]</b>
                                         ZHENG N N, LIU Z Y, REN P J, &lt;i&gt;et al&lt;/i&gt;.Hybrid-Augmented Intelligence:Collaboration and Cognition.Frontiers of Information Technology and Electronic Engineering, 2017, 18 (2) :153-179.
                                    </a>
                                </li>
                                <li id="83">


                                    <a id="bibliography_41" >
                                        <b>[41]</b>
                                     郑南宁.人工智能新时代.智能科学与技术学报, 2019, 1 (1) :1-3. (ZHENG N N.The New Era of Artificial Intelligence.Chinese Journal of Intelligent Science and Technology, 2019, 1 (1) :1-3.) </a>
                                </li>
                                <li id="85">


                                    <a id="bibliography_42" title=" KWASIGROCH A, MIKOŁAJCZYK A, GROCHOWSKI M.Deep Neural Networks Approach to Skin Lesions Classification-A Comparative Analysis // Proc of the 22nd International Conference on Methods and Models in Automation and Robotics (MMAR) .Washington, USA:IEEE, 2017:1069-1074." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Neural Networks Approach to Skin Lesions Classification-A Comparative Analysis">
                                        <b>[42]</b>
                                         KWASIGROCH A, MIKOŁAJCZYK A, GROCHOWSKI M.Deep Neural Networks Approach to Skin Lesions Classification-A Comparative Analysis // Proc of the 22nd International Conference on Methods and Models in Automation and Robotics (MMAR) .Washington, USA:IEEE, 2017:1069-1074.
                                    </a>
                                </li>
                                <li id="87">


                                    <a id="bibliography_43" title=" GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, &lt;i&gt;et al&lt;/i&gt;.Ge-nerative Adversarial Nets[M/OL].[2019-06-25].https://arxiv.org/pdf/1406.2661.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ge-nerative Adversarial Nets[M/OL]">
                                        <b>[43]</b>
                                         GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, &lt;i&gt;et al&lt;/i&gt;.Ge-nerative Adversarial Nets[M/OL].[2019-06-25].https://arxiv.org/pdf/1406.2661.pdf.
                                    </a>
                                </li>
                                <li id="89">


                                    <a id="bibliography_44" title=" 王坤峰, 苟超, 段艳杰, 等.生成式对抗网络GAN的研究进展与展望.自动化学报, 2017, 43 (3) :321-332. (WANG K F, GOU C, DUAN Y J, &lt;i&gt;et al&lt;/i&gt;.Generative Adversarial Networks:The State of the Art and Beyond.Acta Automatica Sinica, 2017, 43 (3) :321-332.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201703001&amp;v=MDc2MzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZML0xLQ0xmWWJHNEg5Yk1ySTlGWlk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[44]</b>
                                         王坤峰, 苟超, 段艳杰, 等.生成式对抗网络GAN的研究进展与展望.自动化学报, 2017, 43 (3) :321-332. (WANG K F, GOU C, DUAN Y J, &lt;i&gt;et al&lt;/i&gt;.Generative Adversarial Networks:The State of the Art and Beyond.Acta Automatica Sinica, 2017, 43 (3) :321-332.) 
                                    </a>
                                </li>
                                <li id="91">


                                    <a id="bibliography_45" title=" BAUR C, ALBARQOUNI S, NAVAB N.MelanoGANs:High Re- solution Skin Lesion Synthesis with GANs[C/OL].[2019-06-25].https://arxiv.org/pdf/1804.04338v1.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MelanoGANs:High Re- solution Skin Lesion Synthesis with GANs[C/OL]">
                                        <b>[45]</b>
                                         BAUR C, ALBARQOUNI S, NAVAB N.MelanoGANs:High Re- solution Skin Lesion Synthesis with GANs[C/OL].[2019-06-25].https://arxiv.org/pdf/1804.04338v1.pdf.
                                    </a>
                                </li>
                                <li id="93">


                                    <a id="bibliography_46" title=" RADFORD A, METZ L, CHINTALA S.Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks[C/OL].[2019-06-25].https://arxiv.org/pdf/1511.06434.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks[C/OL]">
                                        <b>[46]</b>
                                         RADFORD A, METZ L, CHINTALA S.Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks[C/OL].[2019-06-25].https://arxiv.org/pdf/1511.06434.pdf.
                                    </a>
                                </li>
                                <li id="95">


                                    <a id="bibliography_47" title=" DENTON E L, CHINTALA S, SZLAM A, &lt;i&gt;et al&lt;/i&gt;.Deep Generative Image Models Using a Laplacian Pyramid of Adversarial Networks // CORTES C, LAWRENCE N D, LEE D, eds.Advances in Neural Information Processing Systems 28.Cambridge, USA:The MIT Press, 2015:1486-1494." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks">
                                        <b>[47]</b>
                                         DENTON E L, CHINTALA S, SZLAM A, &lt;i&gt;et al&lt;/i&gt;.Deep Generative Image Models Using a Laplacian Pyramid of Adversarial Networks // CORTES C, LAWRENCE N D, LEE D, eds.Advances in Neural Information Processing Systems 28.Cambridge, USA:The MIT Press, 2015:1486-1494.
                                    </a>
                                </li>
                                <li id="97">


                                    <a id="bibliography_48" title=" BISSOTO A, PEREZ F, VALLE E, &lt;i&gt;et al&lt;/i&gt;.Skin Lesion Synthesis with Generative Adversarial Networks[C/OL].[2019-06-25].https://arxiv.org/pdf/1902.03253.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Skin Lesion Synthesis with Generative Adversarial Networks[C/OL]">
                                        <b>[48]</b>
                                         BISSOTO A, PEREZ F, VALLE E, &lt;i&gt;et al&lt;/i&gt;.Skin Lesion Synthesis with Generative Adversarial Networks[C/OL].[2019-06-25].https://arxiv.org/pdf/1902.03253.pdf.
                                    </a>
                                </li>
                                <li id="99">


                                    <a id="bibliography_49" title=" WANG T C, LIU M Y, ZHU J Y, &lt;i&gt;et al&lt;/i&gt;.High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:8798-8807." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-resolution image synthesis and semantic manipulation with conditional gans">
                                        <b>[49]</b>
                                         WANG T C, LIU M Y, ZHU J Y, &lt;i&gt;et al&lt;/i&gt;.High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:8798-8807.
                                    </a>
                                </li>
                                <li id="101">


                                    <a id="bibliography_50" title=" MIKOAJCZYK A, GROCHOWSKI M.Data Augmentation for Improving Deep Learning in Image Classification Problem // Proc of the International Interdisciplinary Ph.D.Workshop.Washington, USA:IEEE, 2018:117-122." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data Augmentation for Improving Deep Learning in Image Classification Problem">
                                        <b>[50]</b>
                                         MIKOAJCZYK A, GROCHOWSKI M.Data Augmentation for Improving Deep Learning in Image Classification Problem // Proc of the International Interdisciplinary Ph.D.Workshop.Washington, USA:IEEE, 2018:117-122.
                                    </a>
                                </li>
                                <li id="103">


                                    <a id="bibliography_51" title=" GALDRAN A, ALVAREZ-GILA A, MEYER M I, &lt;i&gt;et al&lt;/i&gt;.Data-Driven Color Augmentation Techniques for Deep Skin Image Analysis[C/OL].[2019-06-25].https://arxiv.org/pdf/1703.03702v1.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data-Driven Color Augmentation Techniques for Deep Skin Image Analysis[C/OL]">
                                        <b>[51]</b>
                                         GALDRAN A, ALVAREZ-GILA A, MEYER M I, &lt;i&gt;et al&lt;/i&gt;.Data-Driven Color Augmentation Techniques for Deep Skin Image Analysis[C/OL].[2019-06-25].https://arxiv.org/pdf/1703.03702v1.pdf.
                                    </a>
                                </li>
                                <li id="105">


                                    <a id="bibliography_52" title=" HAEGHEN Y V, NAEYAERT J M A D, LEMAHIEU I, &lt;i&gt;et al&lt;/i&gt;.An Imaging System with Calibrated Color Image Acquisition for Use in Dermatology.IEEE Transactions on Medical Imaging, 2000, 19 (7) :722-730." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An imaging system with calibrated color image acquisition for use in dermatology">
                                        <b>[52]</b>
                                         HAEGHEN Y V, NAEYAERT J M A D, LEMAHIEU I, &lt;i&gt;et al&lt;/i&gt;.An Imaging System with Calibrated Color Image Acquisition for Use in Dermatology.IEEE Transactions on Medical Imaging, 2000, 19 (7) :722-730.
                                    </a>
                                </li>
                                <li id="107">


                                    <a id="bibliography_53" title=" QUINTANA J, GARCIA R, NEUMANN L.A Novel Method for Color Correction in Epiluminescence Microscopy.Computerized Medical Imaging and Graphics, 2011, 35 (7/8) :646-652." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300048871&amp;v=MDU2NjRCSHM0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZvVGFoRT1OaWZPZmJLN0h0RE9ySTlGWk84SA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[53]</b>
                                         QUINTANA J, GARCIA R, NEUMANN L.A Novel Method for Color Correction in Epiluminescence Microscopy.Computerized Medical Imaging and Graphics, 2011, 35 (7/8) :646-652.
                                    </a>
                                </li>
                                <li id="109">


                                    <a id="bibliography_54" title=" ARGENZIANO G, SOYER H, DE GIORGI V, &lt;i&gt;et al&lt;/i&gt;.Interactive Atlas of Dermoscopy (Book and CD-ROM) .Milan, Italy:EDRA Medical Publishing &amp;amp; New Media, 2000." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interactive Atlas of Dermoscopy">
                                        <b>[54]</b>
                                         ARGENZIANO G, SOYER H, DE GIORGI V, &lt;i&gt;et al&lt;/i&gt;.Interactive Atlas of Dermoscopy (Book and CD-ROM) .Milan, Italy:EDRA Medical Publishing &amp;amp; New Media, 2000.
                                    </a>
                                </li>
                                <li id="111">


                                    <a id="bibliography_55" title=" MENDONCŸA T, FERREIRA P M, MARQUES J S, &lt;i&gt;et al&lt;/i&gt;.PH&lt;sup&gt;2&lt;/sup&gt;-A Dermoscopic Image Database for Research and Benchmarking // Proc of the 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society.Washington, USA:IEEE, 2013:5437-5440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PH2-A Dermoscopic Image Database for Research and Benchmarking">
                                        <b>[55]</b>
                                         MENDONCŸA T, FERREIRA P M, MARQUES J S, &lt;i&gt;et al&lt;/i&gt;.PH&lt;sup&gt;2&lt;/sup&gt;-A Dermoscopic Image Database for Research and Benchmarking // Proc of the 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society.Washington, USA:IEEE, 2013:5437-5440.
                                    </a>
                                </li>
                                <li id="113">


                                    <a id="bibliography_56" title=" SUN X X, YANG J F, SUN M, &lt;i&gt;et al&lt;/i&gt;.A Benchmark for Automatic Visual Classification of Clinical Skin Disease Images // Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:206-222." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Benchmark for Automatic Visual Classification of Clinical Skin Disease Images">
                                        <b>[56]</b>
                                         SUN X X, YANG J F, SUN M, &lt;i&gt;et al&lt;/i&gt;.A Benchmark for Automatic Visual Classification of Clinical Skin Disease Images // Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:206-222.
                                    </a>
                                </li>
                                <li id="115">


                                    <a id="bibliography_57" title=" KAWAHARA J, DANESHVAR S, ARGENZIANO G, &lt;i&gt;et al&lt;/i&gt;.Se-ven-Point Checklist and Skin Lesion Classification Using Multi-task Multi-modal Neural Nets.IEEE Journal of Biomedical and Health Informatics, 2018, 23 (2) :538-546." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Se-ven-Point Checklist and Skin Lesion Classification Using Multi-task Multi-modal Neural Nets">
                                        <b>[57]</b>
                                         KAWAHARA J, DANESHVAR S, ARGENZIANO G, &lt;i&gt;et al&lt;/i&gt;.Se-ven-Point Checklist and Skin Lesion Classification Using Multi-task Multi-modal Neural Nets.IEEE Journal of Biomedical and Health Informatics, 2018, 23 (2) :538-546.
                                    </a>
                                </li>
                                <li id="117">


                                    <a id="bibliography_58" title=" GU Y Y, PARTRIDGE Y P, ZHOU J.A Hyperspectral Dermoscopy Dataset for Melanoma Detection // Proc of the International Workshop on Skin Image Analysis.Berlin, Germany:Springer, 2018:268-276." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Hyperspectral Dermoscopy Dataset for Melanoma Detection">
                                        <b>[58]</b>
                                         GU Y Y, PARTRIDGE Y P, ZHOU J.A Hyperspectral Dermoscopy Dataset for Melanoma Detection // Proc of the International Workshop on Skin Image Analysis.Berlin, Germany:Springer, 2018:268-276.
                                    </a>
                                </li>
                                <li id="119">


                                    <a id="bibliography_59" title=" CODELLA N C F, GUTMAN D, CELEBI M E, &lt;i&gt;et al&lt;/i&gt;.Skin Lesion Analysis toward Melanoma Detection:A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI) , Hosted by the International Skin Imaging Collaboration (ISIC) // Proc of the 15th IEEE International Symposium on Biomedical Imaging.Washington, USA:IEEE, 2018:168-172." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Skin Lesion Analysis toward Melanoma Detection:A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI),Hosted by the International Skin Imaging Collaboration (ISIC)">
                                        <b>[59]</b>
                                         CODELLA N C F, GUTMAN D, CELEBI M E, &lt;i&gt;et al&lt;/i&gt;.Skin Lesion Analysis toward Melanoma Detection:A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI) , Hosted by the International Skin Imaging Collaboration (ISIC) // Proc of the 15th IEEE International Symposium on Biomedical Imaging.Washington, USA:IEEE, 2018:168-172.
                                    </a>
                                </li>
                                <li id="121">


                                    <a id="bibliography_60" title=" TSCHANDL P, ROSENDAHL C, KITTLER H.The Ham10000 Dataset, A Large Collection of Multi-source Dermatoscopic Images of Common Pigmented Skin Lesions.Scientific Data, 2018, 5.DOI:10.1038/sdata.2019.161." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Ham10000 Dataset,A Large Collection of Multi-source Dermatoscopic Images of Common Pigmented Skin Lesions">
                                        <b>[60]</b>
                                         TSCHANDL P, ROSENDAHL C, KITTLER H.The Ham10000 Dataset, A Large Collection of Multi-source Dermatoscopic Images of Common Pigmented Skin Lesions.Scientific Data, 2018, 5.DOI:10.1038/sdata.2019.161.
                                    </a>
                                </li>
                                <li id="123">


                                    <a id="bibliography_61" title=" GILLIES R J, KINAHAN P E, HRICAK H.Radiomics:Images Are More Than Pictures, They Are Data.Radiology, 2015, 278 (2) :563-577." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Radiomics:Images are more than pictures,they are data">
                                        <b>[61]</b>
                                         GILLIES R J, KINAHAN P E, HRICAK H.Radiomics:Images Are More Than Pictures, They Are Data.Radiology, 2015, 278 (2) :563-577.
                                    </a>
                                </li>
                                <li id="125">


                                    <a id="bibliography_62" title=" YANG J F, SUN X X, LIANG J, &lt;i&gt;et al&lt;/i&gt;.Clinical Skin Lesion Diagnosis Using Representations Inspired by Dermatologist Criteria // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:1258-1266." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clinical Skin Lesion Diagnosis Using Representations Inspired by Dermatologist Criteria">
                                        <b>[62]</b>
                                         YANG J F, SUN X X, LIANG J, &lt;i&gt;et al&lt;/i&gt;.Clinical Skin Lesion Diagnosis Using Representations Inspired by Dermatologist Criteria // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:1258-1266.
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_63" title=" SZEGEDY C, VANHOUCKE V, IOFFE S, &lt;i&gt;et al&lt;/i&gt;.Rethinking the Inception Architecture for Computer Vision // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:2818-2826." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rethinking the inception architecture for computer vision">
                                        <b>[63]</b>
                                         SZEGEDY C, VANHOUCKE V, IOFFE S, &lt;i&gt;et al&lt;/i&gt;.Rethinking the Inception Architecture for Computer Vision // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:2818-2826.
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_64" title=" ULLRICH C.Descriptive and Prescriptive Learning Theories // Proc of the 15th IEEE International Symposium on Biomedical Imaging.Washington, USA:IEEE, 2008:37-42." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Descriptive and Prescriptive Learning Theories">
                                        <b>[64]</b>
                                         ULLRICH C.Descriptive and Prescriptive Learning Theories // Proc of the 15th IEEE International Symposium on Biomedical Imaging.Washington, USA:IEEE, 2008:37-42.
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_65" title=" HE K M, ZHANG X Y, REN S Q, &lt;i&gt;et al&lt;/i&gt;.Identity Mappings in Deep Residual Networks // Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:630-645." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">
                                        <b>[65]</b>
                                         HE K M, ZHANG X Y, REN S Q, &lt;i&gt;et al&lt;/i&gt;.Identity Mappings in Deep Residual Networks // Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:630-645.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_66" title=" TSCHANDL P, ROSENDAHL C, AKAY B N, &lt;i&gt;et al&lt;/i&gt;.Expert-Level Diagnosis of Nonpigmented Skin Cancer by Combined Convolutional Neural Networks.JAMA Dermatology, 2018.DOI:10.1001/jamadermatol.2018.4378." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Expert-Level Diagnosis of Nonpigmented Skin Cancer by Combined Convolutional Neural Networks">
                                        <b>[66]</b>
                                         TSCHANDL P, ROSENDAHL C, AKAY B N, &lt;i&gt;et al&lt;/i&gt;.Expert-Level Diagnosis of Nonpigmented Skin Cancer by Combined Convolutional Neural Networks.JAMA Dermatology, 2018.DOI:10.1001/jamadermatol.2018.4378.
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_67" title=" HOLZINGER A, BIEMANN C, PATTICHIS C S, &lt;i&gt;et al&lt;/i&gt;.What Do We Need to Build Explainable AI Systems for the Medical Domain?[C/OL].[2019-06-25].https://arxiv.org/pdf/1712.09923.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=What Do We Need to Build Explainable AI Systems for the Medical Domain?[C/OL]">
                                        <b>[67]</b>
                                         HOLZINGER A, BIEMANN C, PATTICHIS C S, &lt;i&gt;et al&lt;/i&gt;.What Do We Need to Build Explainable AI Systems for the Medical Domain?[C/OL].[2019-06-25].https://arxiv.org/pdf/1712.09923.pdf.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_68" title=" PETERS J, JANZING D, SCH&#214;LKOPF B.Elements of Causal Inference:Foundations and Learning Algorithms.Cambridge, USA:The MIT Press, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Elements of Causal Inference:Foundations and Learning Algorithms">
                                        <b>[68]</b>
                                         PETERS J, JANZING D, SCH&#214;LKOPF B.Elements of Causal Inference:Foundations and Learning Algorithms.Cambridge, USA:The MIT Press, 2017.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_69" title=" MAIBACH H I.Evidence Based Dermatology.Raleigh, USA:PMPH-USA Limited, 2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Evidence Based Dermatology">
                                        <b>[69]</b>
                                         MAIBACH H I.Evidence Based Dermatology.Raleigh, USA:PMPH-USA Limited, 2012.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_70" title=" SHAI A, MAIBACH H I, BARAN R.Handbook of Cosmetic Skin Care.London, UK:Informa UK Ltd, 2009." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Handbook of Cosmetic Skin Care">
                                        <b>[70]</b>
                                         SHAI A, MAIBACH H I, BARAN R.Handbook of Cosmetic Skin Care.London, UK:Informa UK Ltd, 2009.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_71" title=" BARAN R, MAIBACH H I.Textbook of Cosmetic Dermatology.Boca Raton, USA:Taylor &amp;amp; Francis Group, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Textbook of Cosmetic Dermatology">
                                        <b>[71]</b>
                                         BARAN R, MAIBACH H I.Textbook of Cosmetic Dermatology.Boca Raton, USA:Taylor &amp;amp; Francis Group, 2017.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_72" title=" ENGASSER P G, MAIBACH H I.Cosmetics and Dermatology:Bleaching Creams.Journal of the American Academy of Dermatology, 1981, 5 (2) :143-147." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cosmetic and dermatology: bleaching creams">
                                        <b>[72]</b>
                                         ENGASSER P G, MAIBACH H I.Cosmetics and Dermatology:Bleaching Creams.Journal of the American Academy of Dermatology, 1981, 5 (2) :143-147.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_73" title=" NAIK S, LARSEN S B, GOMEZ N C, &lt;i&gt;et al&lt;/i&gt;.Inflammatory Memory Sensitizes Skin Epithelial Stem Cells to Tissue Damage.Nature, 2017, 550 (7677) :475-480." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inflammatory memory sensitizes skin epithelial stem cells to tissue damage">
                                        <b>[73]</b>
                                         NAIK S, LARSEN S B, GOMEZ N C, &lt;i&gt;et al&lt;/i&gt;.Inflammatory Memory Sensitizes Skin Epithelial Stem Cells to Tissue Damage.Nature, 2017, 550 (7677) :475-480.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_74" title=" KURITA M, ARAOKA T, HISHIDA T, &lt;i&gt;et al&lt;/i&gt;.In Vivo Reprogramming of Wound-Resident Cells Generates Skin Epithelial Tissue.Nature, 2018, 561 (7722) :243-247." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=In Vivo Reprogramming of Wound-Resident Cells Generates Skin Epithelial Tissue">
                                        <b>[74]</b>
                                         KURITA M, ARAOKA T, HISHIDA T, &lt;i&gt;et al&lt;/i&gt;.In Vivo Reprogramming of Wound-Resident Cells Generates Skin Epithelial Tissue.Nature, 2018, 561 (7722) :243-247.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_75" title=" LAI Y C, DENG J N, LIU R Y, &lt;i&gt;et al&lt;/i&gt;.Actively Perceiving and Responsive Soft Robots Enabled by Self-powered, Highly Extensible, And Highly Sensitive Triboelectric Proximity-and Pressure-Sensing Skins.Advanced Materials, 2018, 30 (28) :1801114." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD62322ED87E49F40EEB98F630A8B3A72A&amp;v=MTg4NjRhclc2SGRQTzJ2dE5ZNTRMQlFvOXoyTm1tRFoxUG5uaHJHTTlDN0hsUXJqdUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHdMeTR3S3M9TmlmYw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[75]</b>
                                         LAI Y C, DENG J N, LIU R Y, &lt;i&gt;et al&lt;/i&gt;.Actively Perceiving and Responsive Soft Robots Enabled by Self-powered, Highly Extensible, And Highly Sensitive Triboelectric Proximity-and Pressure-Sensing Skins.Advanced Materials, 2018, 30 (28) :1801114.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(07),577-588 DOI:10.16451/j.cnki.issn1003-6059.201907001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>平行皮肤:基于视觉的皮肤病分析框架</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E9%A3%9E%E8%B7%83&amp;code=10532516&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王飞跃</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%8B%9F%E8%B6%85&amp;code=31992399&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苟超</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%BB%BA%E5%8A%9F&amp;code=42447452&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王建功</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B2%88%E7%94%9C%E9%9B%A8&amp;code=42447454&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">沈甜雨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%91%E6%96%87%E5%8D%9A&amp;code=39322960&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郑文博</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E6%85%A7&amp;code=42447456&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">于慧</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E8%87%AA%E5%8A%A8%E5%8C%96%E7%A0%94%E7%A9%B6%E6%89%80%E5%A4%8D%E6%9D%82%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%E4%B8%8E%E6%8E%A7%E5%88%B6%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0143551&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院自动化研究所复杂系统管理与控制国家重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%9D%92%E5%B2%9B%E6%99%BA%E8%83%BD%E4%BA%A7%E4%B8%9A%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E9%99%A2%E5%B9%B3%E8%A1%8C%E5%81%A5%E5%BA%B7%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E4%B8%AD%E5%BF%83&amp;code=1701713&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">青岛智能产业技术研究院平行健康技术创新中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E9%99%A2&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学人工智能学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%AE%89%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E8%BD%AF%E4%BB%B6%E5%AD%A6%E9%99%A2&amp;code=0189085&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西安交通大学软件学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=School%20of%20Creative%20Technologies%2CUniversity%20of%20Portsmouth&amp;code=0074372&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">School of Creative Technologies,University of Portsmouth</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>随着计算机与人工智能的快速发展, 基于图像感知的皮肤病分析方法取得一些成果.然而, 以深度学习为主的计算机辅助分析方法依赖于领域专家标注的医学大数据, 诊断结果缺乏医学可解释性.为此, 文中提出基于视觉的皮肤病分析统一框架——平行皮肤.启发于ACP方法与平行医学图像分析框架, 通过构建人工皮肤图像系统实现数据选择与生成, 通过预测学习的计算实验完成诊断分析模型构建与评估, 并利用描述学习与指示学习融合专家知识, 引导人工图像系统数据生成与选择, 从而实现闭环诊断分析模型优化.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B9%B3%E8%A1%8C%E7%9A%AE%E8%82%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">平行皮肤;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B9%B3%E8%A1%8C%E6%99%BA%E8%83%BD&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">平行智能;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成式模型;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *王飞跃 (通讯作者) , 博士, 研究员, 主要研究方向为智能系统和复杂系统的建模、分析与控制.E-mail:feiyue.wang@ia.ac.cn.;
                                </span>
                                <span>
                                    苟超, 博士, 助理研究员, 主要研究方向为计算机视觉、模式识别、机器学习.E-mail:chao.gou@ia.ac.cn.;
                                </span>
                                <span>
                                    王建功, 博士研究生, 主要研究方向为计算机图形学、医学图像处理、机器学习.E-mail:wangjiangong2018@ia.ac.cn.;
                                </span>
                                <span>
                                    沈甜雨, 博士研究生, 主要研究方向为计算机视觉、医学图像处理、机器学习.E-mail:shentianyu2016@ia.ac.cn.;
                                </span>
                                <span>
                                    郑文博, 博士研究生, 主要研究方向为计算机视觉、医学图像处理、机器学习.E-mail:zwb2017@stu.xjtu.edu.cn.;
                                </span>
                                <span>
                                    于慧, 博士, 教授, 主要研究方向为人脸分析、人体运动分析、人机交互.E-mail:hui.yu@port.ac.uk.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-06-21</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.61806198, 61304200, 61533019) 资助;</span>
                    </p>
            </div>
                    <h1><b>Parallel Skin: A Vision-Based Dermatological Analysis Framework</b></h1>
                    <h2>
                    <span>WANG Fei-Yue</span>
                    <span>GOU Chao</span>
                    <span>WANG Jiangong</span>
                    <span>SHEN Tianyu</span>
                    <span>ZHENG Wenbo</span>
                    <span>YU Hui</span>
            </h2>
                    <h2>
                    <span>The State Key Laboratory of Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences</span>
                    <span>School of Artificial Intelligence, University of Chinese Academy of Sciences</span>
                    <span>Parallel Healthcare Technology Innovation Center, Qingdao Academy of Intelligent Industries</span>
                    <span>School of Software Engineering, Xi' an Jiaotong University</span>
                    <span>School of Creative Technologies, University of Portsmouth</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>With the rapid development of computer and artificial intelligence, image-based methods for skin analysis have achieved preferable results. However, the performance of computer aided diagnosis systems based on deep learning methods relies on big medical data labeled by domain experts. In addition, there is limitation of interpretability for the diagnosis results. To address aforementioned problems, a vision-based unified framework for dermatological analysis termed as parallel skin is proposed. Inspired by the ACP method and the parallel medical image analysis framework, the artificial skin image system to perform data selection and generation is constructed. Then, computational experiments are conducted with predictive learning for model building and evaluation. Descriptive and prescriptive learning to leverage the power of domain knowledge to guide data selection and generation are further introduced. In the proposed parallel-skin framework, the closed-loop diagnostic analysis model can be optimized.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Parallel%20Skin&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Parallel Skin;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Parallel%20Intelligence&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Parallel Intelligence;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Generative%20Models&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Generative Models;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Fei-Yue ( Corresponding author) , Ph. D. , professor. His research interests include modeling, analysis, and control of intelligent systems and complex systems.;
                                </span>
                                <span>
                                    GOU Chao, Ph.D. , assistant professor. His research interests include computer vision, pattern recognition and machine learning.;
                                </span>
                                <span>
                                    WANG Jiangong, Ph. D. candidate. His research interests include computer graphics, medical image processing and machine learning.;
                                </span>
                                <span>
                                    SHEN Tianyu, Ph. D. candidate. Her research interests include computer vision, medical image processing and machine learning.;
                                </span>
                                <span>
                                    ZHENG Wenbo, Ph.D. candidate. His research interests include computer vision, medical image processing and machine learning.;
                                </span>
                                <span>
                                    YU Hui, Ph.D. , professor. His research interests include face analysis, human motion analysis and human-computer interaction.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-06-21</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.61806198, 61304200, 61533019);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="172">皮肤病是发生在皮肤和皮肤附属器官疾病的总称.皮肤病病种繁多, 目前已知超过2 000种皮肤病<citation id="251" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>, 很多皮肤病种之间及其相似, 即使有经验的专家也无法准确辨别.皮肤病的病因也较复杂, 外部环境、食物结构、遗传等都是致病因素.皮肤癌是皮肤病中危害尤为严重的一种常见恶性肿瘤, 它发生于头部、面部、颈部、下肢等部位, 多见于老年患者, 在美国每年就有540余万新增皮肤癌患者<citation id="247" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>.黑色素瘤是一种致命皮肤癌, 虽然在美国皮肤癌患者中仅有大约5%的患者是黑色素瘤, 但是它造成皮肤癌致死人数的75%<citation id="248" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 在欧洲每年有超过10万人被诊断为黑色素瘤, 有大约2.2万致死病例<citation id="249" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>.我国虽为黑色素瘤低发区, 但随着环境污染加重与人们健康意识的增强, 确诊病例以每年3%至8%的速度增长.黑色素瘤在皮肤表面表现为色素性病变, 可以通过专家视觉检测进行早期检测, 但是易与黑色素痣等良性皮肤病混淆.皮肤病的早期诊断、早期治疗尤为重要, 特别是针对黑色素瘤的早期诊断治疗, 5年存活率可以由14%提高到99%左右<citation id="250" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>.当前我国皮肤病诊断面临着病种繁多、病因复杂、皮肤科专家医生短缺的问题.</p>
                </div>
                <div class="p1">
                    <p id="173">随着计算机普及与人工智能技术的快速发展, 以深度学习为核心的人工智能方法在图像分析、语音识别、自然语言处理等领域取得突破性成果<citation id="252" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>.近年来, 深度神经网络方法逐步应用于医疗领域, 尤其在图像分析、电子病历管理、移动医疗等应用上取得开创性成果<citation id="253" type="reference"><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>.目前, 基于深度学习方法的皮肤病智能诊断是智能医疗的重要研究方向<citation id="254" type="reference"><link href="3" rel="bibliography" /><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><link href="23" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>.由于深度神经网络需要大量皮肤图像标注数据, 同时需要具有医学知识的皮肤科医生来标注, 因此, 对于医学皮肤数据采集及标注都是目前基于深度学习辅助皮肤诊断研究的难点问题.此外, 深度神经网络模型是一个“黑盒”模型, 给定皮肤数据, 输出良恶性分类结果不具有医学可解释性.</p>
                </div>
                <div class="p1">
                    <p id="174">2004年王飞跃<citation id="255" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出平行智能方法论, 核心是通过计算实验, 描述、预测并引导复杂系统现象, 通过整合人工社会、计算实验和平行执行 (Artificial Societies, Computational Experiments, and Parallel Execution) 的方法解决实际系统中不可预测、难以拆分、无法重复实验的复杂系统问题.经过10多年的发展, 平行智能的计算研究体系不断丰富和完善, 并在智能交通<citation id="256" type="reference"><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>、社会计算<citation id="257" type="reference"><link href="37" rel="bibliography" /><link href="39" rel="bibliography" /><link href="41" rel="bibliography" /><link href="43" rel="bibliography" /><link href="45" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>、视觉感知<citation id="258" type="reference"><link href="47" rel="bibliography" /><link href="49" rel="bibliography" /><sup>[<a class="sup">23</a>,<a class="sup">24</a>]</sup></citation>、智慧医疗<citation id="259" type="reference"><link href="51" rel="bibliography" /><link href="53" rel="bibliography" /><link href="55" rel="bibliography" /><link href="57" rel="bibliography" /><sup>[<a class="sup">25</a>,<a class="sup">26</a>,<a class="sup">27</a>,<a class="sup">28</a>]</sup></citation>、智慧教育<citation id="260" type="reference"><link href="59" rel="bibliography" /><link href="61" rel="bibliography" /><sup>[<a class="sup">29</a>,<a class="sup">30</a>]</sup></citation>等领域取得良好效果.针对目前基于计算机皮肤智能诊断领域存在的难点问题, 本文提出基于平行智能方法的皮肤病智能分析框架, 为皮肤病诊断应用实现可解释性推理问题提供一个统一的解决框架.</p>
                </div>
                <h3 id="175" name="175" class="anchor-tag">1 基本概念</h3>
                <h4 class="anchor-tag" id="176" name="176"><b>1.1 ACP与平行学习</b></h4>
                <div class="p1">
                    <p id="177">基于ACP的平行智能方法<citation id="261" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>最早由王飞跃提出, 试图解决复杂系统的建模与控制问题, 分为人工社会 (Artificial Societies) 、计算实验 (Computational Experiments) 、平行执行 (Parallel Execution) 三部分.它进一步定义为一种基于真实与虚拟系统交互并执行的智能方法, 可以实现描述、预测与引导智能<citation id="264" type="reference"><link href="37" rel="bibliography" /><link href="39" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>.本文将ACP方法进一步推广于视觉感知领域<citation id="262" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>, 提出平行视觉方法<citation id="265" type="reference"><link href="47" rel="bibliography" /><link href="63" rel="bibliography" /><link href="65" rel="bibliography" /><sup>[<a class="sup">23</a>,<a class="sup">31</a>,<a class="sup">32</a>]</sup></citation>, 通过人工场景、计算实验、平行执行以实现基于视觉的复杂场景感知与理解.而已有的基于虚拟图像的学习方法<citation id="266" type="reference"><link href="67" rel="bibliography" /><link href="69" rel="bibliography" /><link href="71" rel="bibliography" /><sup>[<a class="sup">33</a>,<a class="sup">34</a>,<a class="sup">35</a>]</sup></citation>为平行视觉的虚拟人工场景与计算实验部分.平行执行的目的是构建一个闭环, 实现虚实互动在线模型优化<citation id="263" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="178">平行学习是一个新型机器学习框架, 在描述、预测与指示学习基础上, 将数据、知识、行动整合于一个闭环系统, 目的是解决目前机器学习方法中数据收集、策略选择受限的问题<citation id="267" type="reference"><link href="73" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>.平行学习大致可以分为描述学习 (Descriptive Learning) 、 预测学习 (Pre-dictive Learning) 、指示学习 (Prescriptive Learning) 三个互相耦合关联的阶段.平行学习通过描述学习得到与真实数据分布一致的生成模型, 通过预测学习从数据中蒸馏知识, 通过指示学习根据不断增长的知识引导系统训练与测试评估, 从而实现优化.对抗学习是平行学习一种特殊实现形式<citation id="268" type="reference"><link href="73" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>.平行学习已在自动泊车规划等领域得到应用<citation id="269" type="reference"><link href="73" rel="bibliography" /><link href="75" rel="bibliography" /><sup>[<a class="sup">36</a>,<a class="sup">37</a>]</sup></citation>.</p>
                </div>
                <div class="area_img" id="180">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907001_180.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 平行学习框架[36]" src="Detail/GetImg?filename=images/MSSB201907001_180.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 平行学习框架<citation id="270" type="reference"><link href="73" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation><sup></sup>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907001_180.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Framework of parallel learning</p>

                </div>
                <h4 class="anchor-tag" id="181" name="181"><b>1.2 平行医学图像</b></h4>
                <div class="p1">
                    <p id="182">平行医学图像是最近提出的一种医学图像分析框架<citation id="271" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>.与传统自然图像分析不同, 医学图像分析需要融合专家领域知识, 由此, 平行医学图像将医学图像数据、专家领域知识整合于一个系统中, 提出“数据驱动+知识驱动”的双向平行进化优化, 从而解决医学影像数据收集耗时费力且不具有可解释性的难点问题.从数据驱动角度出发<citation id="272" type="reference"><link href="79" rel="bibliography" /><sup>[<a class="sup">39</a>]</sup></citation>, 该框架提出从实际临床医学场景中获取特定的医疗图像“小数据”, 输入构建的人工医学图像系统, 生成大量人工图像数据.原始特定的图像“小数据”与生成的数据构成解决复杂医疗图像分析问题所需的“大数据”集合, 用于诊断模型的学习与评估, 从而实现医学“小知识”的提取.反之, 从知识驱动角度出发, 启发于人在环路的混合增强智能<citation id="274" type="reference"><link href="81" rel="bibliography" /><link href="83" rel="bibliography" /><sup>[<a class="sup">40</a>,<a class="sup">41</a>]</sup></citation>, 提出将提取的医学“小知识”与医生专家先验知识被解码为模型可识别的描述, 由顶向下指导数据收集与生成, 由此得到数据知识具有解释性, 从而有利于构建可解释诊断模型, 并提高可解释性<citation id="273" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>.</p>
                </div>
                <h3 id="183" name="183" class="anchor-tag">2 平行皮肤框架及相关工作</h3>
                <div class="p1">
                    <p id="184">医疗数据, 特别是皮肤病图像数据难以获取, 一方面是由于我国医疗机构的封闭运行及隐私保护, 另一方面我国皮肤病病症相对较少.启发于基于ACP思想的平行医学图像分析框架, 本文提出平行皮肤分析框架, 为基于视觉感知方法的皮肤病智能诊断分析提供一种解决方法.具体框架如图2所示.</p>
                </div>
                <div class="area_img" id="185">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907001_18500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 平行皮肤框架" src="Detail/GetImg?filename=images/MSSB201907001_18500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 平行皮肤框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907001_18500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Framework of parallel skin</p>

                </div>
                <div class="p1">
                    <p id="186">本文提出构建人工图像系统, 实现基于真实皮肤图像分布的数据扩充与生成, 并开展基于数据驱动的计算实验.与基于视觉的分析方法不同, 引入平行学习, 融合医学专家知识, 实现闭环反馈系统优化.</p>
                </div>
                <h4 class="anchor-tag" id="187" name="187"><b>2.1 皮肤数据收集</b></h4>
                <div class="p1">
                    <p id="188">目前已有的皮肤病图像分析方法大多基于模型学习, 需要准确的标注信息, 而皮肤病类别繁多且需要领域医学知识才能标注, 因此, 对于数据驱动的皮肤诊断模型学习而言, 收集大规模有效训练样本图像耗时费力, 皮肤图像数据收集仍然是一个具有挑战性的问题.</p>
                </div>
                <div class="p1">
                    <p id="189">本文提出构建一个人工图像系统, 由系统实现特定数据删选及生成, 从而实现有效皮肤数据收集.人工图像系统的目的是实现数据收集, 具体实现是通过得到真实图像的分布模型, 并根据真实皮肤数据分布选择与生成与真实分布一致的数据.本文将人工图像系统的实现方法分为显式真实图像扩充和隐式仿真图像生成两类方法.</p>
                </div>
                <div class="p1">
                    <p id="190">1) 真实数据扩充.具体显式的方法可以为传统图像扩充方法, 包含旋转、加噪、缩放等操作, 如图3所示.</p>
                </div>
                <div class="area_img" id="194">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907001_19400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 人工皮肤图像系统显式扩充真实图像示例" src="Detail/GetImg?filename=images/MSSB201907001_19400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 人工皮肤图像系统显式扩充真实图像示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907001_19400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Example of explicitly generated skin images by artificial skin image system</p>

                </div>
                <div class="p1">
                    <p id="196">通过传统仿射变化, 如旋转、缩放等及扭曲、加噪等操作, 依然保持原皮肤病理特性, 从而有效实现真实数据扩充.类似的方法<citation id="275" type="reference"><link href="3" rel="bibliography" /><link href="85" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">42</a>]</sup></citation>用于解决数据平衡, 为深度网络模型提供更多样数据.此外, 也可以根据临床病理检测结果有监督地删选特定病种的皮肤影像, 如选择特定患癌样本图像, 用于后续总结分析癌变图像特征.</p>
                </div>
                <div class="p1">
                    <p id="197">2) 仿真数据生成.隐式的方法主要基于模型学习数据分布, 得到生成模型, 用于生成分布一致的仿真数据.生成对抗网络 (Generative Adversanal Networks, GANs) <citation id="276" type="reference"><link href="87" rel="bibliography" /><link href="89" rel="bibliography" /><sup>[<a class="sup">43</a>,<a class="sup">44</a>]</sup></citation>广泛应用于隐式图像生成.如图4所示, 构建一个生成器<i>G</i>和一个判别器<i>D</i>, <i>G</i>和<i>D</i>一般是可微分的深度神经网络, 它们的输入分别为随机变量<i>z</i>和真实皮肤图像数据<i>x</i>, <i>G</i> (<i>z</i>) 为由<i>G</i>生成的尽量服从真实皮肤图像分布<i>p</i><sub>data</sub>的样本.<i>D</i>输出的输入数据来源于真实图像的概率, 其优化目标是实现对数据来源的二分类判别: 来源于真实图像还是生成的仿真皮肤图像.而<i>G</i>的目标是使自己生成的数据<i>G</i> (<i>z</i>) 在<i>D</i>上的表现<i>D</i> (<i>G</i> (<i>z</i>) ) 和真实数据<i>x</i>在其上的表现<i>D</i> (<i>x</i>) 一致.二者对抗优化, 损失函数定义如下:</p>
                </div>
                <div class="p1">
                    <p id="198" class="code-formula">
                        <mathml id="198"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mo stretchy="false">{</mo><mi>ε</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">[</mo><mrow><mi>ln</mi></mrow><mspace width="0.25em" /><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">[</mo><mrow><mi>ln</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="199">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907001_199.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 基于生成对抗网络结构的皮肤图像生成" src="Detail/GetImg?filename=images/MSSB201907001_199.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 基于生成对抗网络结构的皮肤图像生成  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907001_199.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Skin image generation based on generative adversarial nets structure</p>

                </div>
                <div class="p1">
                    <p id="201">在已有工作<citation id="277" type="reference"><link href="91" rel="bibliography" /><sup>[<a class="sup">45</a>]</sup></citation>中, 已提出利用深度卷积生成式对抗网络 (Deep Convolutional GAN, DCGAN) <citation id="278" type="reference"><link href="93" rel="bibliography" /><sup>[<a class="sup">46</a>]</sup></citation>和拉普拉斯金字塔生成式对抗网络 (Laplacian Pyramid of GAN, LAPGAN) <citation id="279" type="reference"><link href="95" rel="bibliography" /><sup>[<a class="sup">47</a>]</sup></citation>生成高清 (256×256) 皮肤病图像, 实验部分作者仅使用训练和验证损失作为生成图像的评价标准.Bissoto等<citation id="280" type="reference"><link href="97" rel="bibliography" /><sup>[<a class="sup">48</a>]</sup></citation>利用pix2pixHD<citation id="281" type="reference"><link href="99" rel="bibliography" /><sup>[<a class="sup">49</a>]</sup></citation>生成更高清 (1024×512) 皮肤图像, 生成的图像直观上部分不真实, 但是包含病症信息, 通过生成图像数据扩充后, 分类模型效果也有所提升.风格转换方法也常用于数据生成<citation id="282" type="reference"><link href="101" rel="bibliography" /><sup>[<a class="sup">50</a>]</sup></citation>, 如图5所示, 利用良性原图结构, 融合特定病种 (恶性) 皮肤特性生成仿真图像.该类方法可以解决恶性皮肤少的数据不平衡问题.</p>
                </div>
                <div class="p1">
                    <p id="202">除了利用GANs得到生成模型外, Galdran等<citation id="283" type="reference"><link href="103" rel="bibliography" /><sup>[<a class="sup">51</a>]</sup></citation>提出从已有数据中学习得到光照分布, 并将该部分作用于颜色归一化后的皮肤图像, 从而生成新的皮肤数据, 在ISIC2017数据集上的分割和分类效果均证明其基于颜色信息扩充数据的有效性.此外, 还有部分工作基于专家领域知识, 在图像获取端进行颜色处理<citation id="284" type="reference"><link href="23" rel="bibliography" /><link href="105" rel="bibliography" /><link href="107" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">52</a>,<a class="sup">53</a>]</sup></citation>, 这类方法比对获取图像进行归一化后处理更有效.</p>
                </div>
                <div class="area_img" id="203">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907001_203.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 基于风格转换的皮肤图像生成[50]" src="Detail/GetImg?filename=images/MSSB201907001_203.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 基于风格转换的皮肤图像生成<citation id="285" type="reference"><link href="101" rel="bibliography" /><sup>[<a class="sup">50</a>]</sup></citation><sup></sup>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907001_203.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Skin-image generation based on style conversion</p>

                </div>
                <div class="p1">
                    <p id="204">3) 开源皮肤数据.本节总结目前已有的开源皮肤镜及临床医学图像数据集, 如表1所示, 为基于数据驱动的皮肤病分析提供更多真实数据源.</p>
                </div>
                <div class="p1">
                    <p id="205">Interactive Atlas数据集由Argenziano等<citation id="286" type="reference"><link href="109" rel="bibliography" /><sup>[<a class="sup">54</a>]</sup></citation>于2000年出版的《Interactive Atlas of Dermoscopy》的附赠光盘里1 024张皮肤镜图像组成, 具有7种皮肤病的分类标签, 大致可以分为167张非黑色素瘤图、857张痣和黑色素瘤图.虽然这是一个涵盖病种数最多的可用数据集之一, 但由于可访问性较低, 数据获取较困难.</p>
                </div>
                <div class="p1">
                    <p id="206">PH2数据集是Mendonÿa等<citation id="287" type="reference"><link href="111" rel="bibliography" /><sup>[<a class="sup">55</a>]</sup></citation>在2013年公布, 一共有200张皮肤镜图, 包括160张痣图、40张黑色素瘤图.其中黑色素瘤图均已经过病理学验证, 但是大多数的痣并不适于用病理学的方法验证.由于数据是公开的, 而且包括较全面的疾病图像, 因此PH2数据集一直被用于黑色素瘤的计算机诊断研究.</p>
                </div>
                <div class="p1">
                    <p id="207">SD198由Sun等<citation id="288" type="reference"><link href="113" rel="bibliography" /><sup>[<a class="sup">56</a>]</sup></citation>公布于2016年, 是当时用于皮肤病视觉识别最大的数据集, 包括根据规模、颜色、形状和结构分为198类的6 584张图像, 同时也可以使用不同的分类标准, 得到更少的类别数, 使每类有更多的样本数量.</p>
                </div>
                <div class="p1">
                    <p id="208">7-Point是2018年Kawahara等<citation id="289" type="reference"><link href="115" rel="bibliography" /><sup>[<a class="sup">57</a>]</sup></citation>在研究多任务神经网络对皮肤病图像7-Point标准分类时使用的数据集.共有2 045张彩色临床和皮肤镜图像, 有标注信息的有效数据为1 011张, 这些图像具有多种分类标准下的类别标签, 其中有252张黑色素瘤图, 759张非黑色素瘤图.</p>
                </div>
                <div class="p1">
                    <p id="209">Hyperspectral数据集是由Gu等<citation id="290" type="reference"><link href="117" rel="bibliography" /><sup>[<a class="sup">58</a>]</sup></citation>于2018年公开的第一个高光谱皮肤镜数据集, 包括330张16个可见光波段的高光谱皮肤镜图像, 共有80张黑色素癌图、180张异常痣图及70张其它种类皮肤病的图像, 一共分成6种类别.同时所有图像都经过病理学验证.</p>
                </div>
                <div class="p1">
                    <p id="210">ISIC2017是收集、整理多个数据库后的一个公开数据集<citation id="291" type="reference"><link href="119" rel="bibliography" /><sup>[<a class="sup">59</a>]</sup></citation>.截止2018年2月12日, 包含13 786张皮肤镜图, 主要由黑素细胞病变图像组成, 13 786张图像中有多达12 893张图像是痣或黑色素瘤.就目前而言, ISIC2017数据集易于获取, 数据量大, 是皮肤镜图像分析研究的标准数据源.</p>
                </div>
                <div class="p1">
                    <p id="211">HAM10000是Tschandl等<citation id="292" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">60</a>]</sup></citation>从奥地利维也纳医科大学的皮肤病学系和澳大利亚昆士兰大学Cliff Rosendahl的皮肤癌研究中收集20年内的10 015张不同人群的皮肤镜图, 病例基本包括色素病变领域中所有重要的诊断类别, 其中53.3%都得到病理学验证.</p>
                </div>
                <div class="area_img" id="212">
                    <p class="img_tit"><b>表1 开源皮肤图像数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Open source skin image datasets</p>
                    <p class="img_note"></p>
                    <table id="212" border="1"><tr><td>名称</td><td>图像数量</td><td>病理验证<br />比例/%</td><td>病种数量</td></tr><tr><td><br />InteractiveAtlas<sup>[54]</sup></td><td>1024</td><td>未知</td><td>7</td></tr><tr><td><br />PH2<sup>[55]</sup></td><td>200</td><td>20.5</td><td>2</td></tr><tr><td><br />SD198<sup>[56]</sup></td><td>6584</td><td>未知</td><td>198</td></tr><tr><td><br />7-Point<sup>[57]</sup></td><td>1011</td><td>未知</td><td>2</td></tr><tr><td><br />Hyperspectral<sup>[58]</sup></td><td>330</td><td>100</td><td>6</td></tr><tr><td><br />ISIC2017<sup>[59]</sup></td><td>13786</td><td>26.3</td><td>7</td></tr><tr><td><br />HAM10000<sup>[60]</sup></td><td>10015</td><td>53.3</td><td>7<br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="213" name="213"><b>2.2 计算实验方法与预测学习</b></h4>
                <div class="p1">
                    <p id="214">通过人工皮肤图像系统实现数据增广扩充后, 进一步完成计算实验, 从而实现医学诊断.从影像组学<citation id="293" type="reference"><link href="123" rel="bibliography" /><sup>[<a class="sup">61</a>]</sup></citation>的角度来说, 通过开展计算实验从皮肤图像中高通量地提取影像信息, 实现病灶分割、特征提取与预测模型建立, 凭借对海量图像数据信息进行更深层次的挖掘、预测和分析以辅助医师做出最准确的诊断.平行医学图像分析框架<citation id="294" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>指出预测学习可用于挖掘医学知识, 实现辅助诊断.</p>
                </div>
                <div class="p1">
                    <p id="215">在计算实验步骤, 利用预测学习方法得到鲁棒有效的诊断模型.基于数据驱动的方法, 如深度模型等, 可以在本文框架中通过预测学习得到有效鲁棒的皮肤病诊断模型.传统目标检测、分割、分类等基于视觉的分析方法均可以用于计算实验.在一个基于深度学习的黑色素瘤分类方法中<citation id="295" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 首先利用一个自生成神经网络提取病灶区域, 然后提取一些人工特征表征病灶的颜色、纹理与边界信息, 最后通过集成神经网络方法实现分类.Yang等<citation id="296" type="reference"><link href="125" rel="bibliography" /><sup>[<a class="sup">62</a>]</sup></citation>提出融合皮肤专家知识, 在皮肤诊断准则ABCD (Asymmetry, Border, Color, Diameter) 指导下, 提取人工特征 (非对称、边界、颜色、直径) , 并融合特征实现皮肤病诊断, 在SD198数据集<citation id="297" type="reference"><link href="113" rel="bibliography" /><sup>[<a class="sup">56</a>]</sup></citation>上的198类分类识别准确率达到56.47%.Esteva等<citation id="298" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>利用GoogleNet<citation id="299" type="reference"><link href="127" rel="bibliography" /><sup>[<a class="sup">63</a>]</sup></citation>网络结构, 在ImageNet数据集合上预训练一个大概具有128万个参数的深度网络, 网络结构如图6所示, 在近13万张皮肤病变数据集上进行迁移学习优化, 这些皮肤图像覆盖2 032种不同的皮肤疾病, 分为757个大类进行实验.实验结果显示模型敏感特异性AUC为91%, 达到媲美专业皮肤科医生水平.</p>
                </div>
                <div class="area_img" id="216">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907001_216.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 基于GoogleNet实现皮肤分类的网络结构[1]" src="Detail/GetImg?filename=images/MSSB201907001_216.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 基于GoogleNet实现皮肤分类的网络结构<citation id="300" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation><sup></sup>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907001_216.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Network structure for skin classification based on googleNet</p>

                </div>
                <h4 class="anchor-tag" id="217" name="217"><b>2.3 基于平行学习的闭环优化</b></h4>
                <div class="p1">
                    <p id="218">平行学习框架通过整合预测、描述、指示学习实现数据、知识与决策的系统优化, 基于平行学习<citation id="301" type="reference"><link href="73" rel="bibliography" /><link href="75" rel="bibliography" /><sup>[<a class="sup">36</a>,<a class="sup">37</a>]</sup></citation>框架如图2所示, 从数据角度, 利用描述、指示学习方法融合医学专家知识, 实现整个系统闭环优化.与传统方法不同, 通过预测学习得到诊断模型后, 提出通过描述与指示学习反馈优化指导皮肤数据收集与生成.</p>
                </div>
                <div class="p1">
                    <p id="219">描述学习的目的是构建模型并对预测结果进行描述解释<citation id="302" type="reference"><link href="129" rel="bibliography" /><sup>[<a class="sup">64</a>]</sup></citation>.本文提出基于描述学习在人工图像系统中得到已有数据的分布模型, 可以有效表征已有数据分布.描述学习可以通过半监督或无监督的学习.而指示学习的目的是引导系统下一步行动生成特定输出<citation id="303" type="reference"><link href="129" rel="bibliography" /><sup>[<a class="sup">64</a>]</sup></citation>.在基于图像的皮肤病分析框架中, 提出利用指示学习实现特定样本的删选与生成.特别地, 可以根据医学专家常识删选无效皮肤图像数据, 减小外点数据对模型训练的干扰.同时, 利用预测学习得到模型分析结果或医学专家领域知识以指导数据生成.例如需要大量黑色素瘤的恶性皮肤数据, 可以利用生成对抗等方法在恶性皮肤数据上进行数据分布学习, 并根据领域知识 (恶性皮肤通常含有支路条纹、呈片块状或束状排列<citation id="304" type="reference"><link href="101" rel="bibliography" /><sup>[<a class="sup">50</a>]</sup></citation>) 设定数据分布条件限制, 从而生成特定类别的肿瘤图像<citation id="305" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="220">通过上述平行学习方法, 平行皮肤框架可以不断优化系统模型, 采集并生成更有效多样的皮肤图像数据, 提取更精准有效知识, 实现更准确的皮肤病辅助诊断及分析.</p>
                </div>
                <h3 id="221" name="221" class="anchor-tag">3 基于生成对抗方法的平行皮肤应用实例</h3>
                <div class="p1">
                    <p id="222">以皮肤病分类实现辅助诊断为例, 在平行皮肤框架下, 本文利用生成对抗方法对HAM10000数据集<citation id="306" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">60</a>]</sup></citation>中部分样本数量较少的类别进行显式和隐式的增广扩充, 生成仿真皮肤图像, 然后使用混合人工生成数据及原始数据的新数据集进行基于预测学习的皮肤分类对比实验, 相比直接利用原始数据集进行分类, 本文方法可以得到更准确的结果, 有效提高辅助诊断效果.</p>
                </div>
                <h4 class="anchor-tag" id="223" name="223"><b>3.1 基于描述学习与指示学习生成仿真皮肤图像</b></h4>
                <div class="p1">
                    <p id="224">首先进行翻转、裁剪、缩放等显式数据扩充.在此基础上, 利用DCGAN网络模型<citation id="307" type="reference"><link href="93" rel="bibliography" /><sup>[<a class="sup">46</a>]</sup></citation>预测真实皮肤图像分布, 实现预测学习, 从而利用得到的生成器模型生成仿真皮肤图像.生成器网络结构如图7所示.</p>
                </div>
                <div class="area_img" id="225">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907001_225.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 DCGAN生成器网络结构" src="Detail/GetImg?filename=images/MSSB201907001_225.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 DCGAN生成器网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907001_225.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Network structure of DCGAN generator</p>

                </div>
                <div class="p1">
                    <p id="226">为了解决训练数据平衡问题, 在平行皮肤框架下, 利用指示学习方法, 在医学专家标注的标签特定类别下进行生成, 从而实现医学专家知识的融合与引导.具体地, 有监督地对HAM10000数据集中医生标注数量较少的4类图像进行4个不同的DCGAN训练, 原则上对应的DCGAN生成对应的类别图像, 从而实现数据特定扩充, 扩充前后各类图像数量如表2所示.生成的仿真皮肤图像如图8所示, 相比真实图像, 这些仿真图像有类似的外观和纹理.</p>
                </div>
                <div class="area_img" id="227">
                    <p class="img_tit"><b>表2 HAM10000 数据集扩充前后各类图像数量</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Number of pictures before and after the expansion of the HAM10000 dataset</p>
                    <p class="img_note"></p>
                    <table id="227" border="1"><tr><td>患病种类 (缩写) </td><td>原始训练集</td><td>扩充后训练集</td><td>测试集</td></tr><tr><td><br />akiec</td><td>295</td><td>1295</td><td>32</td></tr><tr><td><br />bcc</td><td>463</td><td>1463</td><td>51</td></tr><tr><td><br />bkl</td><td>990</td><td>990</td><td>109</td></tr><tr><td><br />df</td><td>104</td><td>1104</td><td>11</td></tr><tr><td><br />mel</td><td>1002</td><td>1002</td><td>111</td></tr><tr><td><br />nv</td><td>6035</td><td>6035</td><td>670</td></tr><tr><td><br />vasc</td><td>128</td><td>1128</td><td>14</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="228">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907001_228.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 生成的仿真皮肤图像" src="Detail/GetImg?filename=images/MSSB201907001_228.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 生成的仿真皮肤图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907001_228.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Generated skin images</p>

                </div>
                <h4 class="anchor-tag" id="229" name="229"><b>3.2 基于预测学习的分类验证实验</b></h4>
                <div class="p1">
                    <p id="230">预测学习旨在挖掘医学影像深层病理特征并实现分类预测.本文利用ResNet<citation id="308" type="reference"><link href="131" rel="bibliography" /><sup>[<a class="sup">65</a>]</sup></citation>对HAM10000数据集进行预测分类实验, 然后加入生成的仿真皮肤图像后进行对比, 可以验证依据平行皮肤框架的思想对于计算机辅助皮肤病医学诊断的有效性.本文使用的验证实验流程如图9所示.在原始数据集的实验中, 本文使用共9 017张各类的皮肤病图像作为训练集, 并在998张图像上进行测试.基于平行皮肤方法优化后的训练图像数量为13 017, 并在相同的测试数据集中完成对比实验.</p>
                </div>
                <div class="area_img" id="231">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907001_231.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 基于平行皮肤的分类验证实验流程图" src="Detail/GetImg?filename=images/MSSB201907001_231.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 基于平行皮肤的分类验证实验流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907001_231.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Framework of classification based on parallel skin</p>

                </div>
                <div class="p1">
                    <p id="232">表3和表4分别给出平行皮肤优化前后在各类指标下的实验效果.优化后的归一化混淆矩阵如图10所示.</p>
                </div>
                <div class="area_img" id="233">
                    <p class="img_tit"><b>表3 真实皮肤图像分类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Real skin image classification results %</p>
                    <p class="img_note"></p>
                    <table id="233" border="1"><tr><td><br />皮肤类别</td><td>准确率</td><td>召回率</td><td>精确度</td><td>F1</td></tr><tr><td><br />akiec</td><td>-</td><td>89.29</td><td>78.13</td><td>83.33</td></tr><tr><td><br />bcc</td><td>-</td><td>82.69</td><td>84.31</td><td>83.50</td></tr><tr><td><br />bkl</td><td>-</td><td>84.11</td><td>82.56</td><td>83.33</td></tr><tr><td><br />df</td><td>-</td><td>77.77</td><td>63.64</td><td>70.00</td></tr><tr><td><br />mel</td><td>-</td><td>89.33</td><td>60.36</td><td>72.04</td></tr><tr><td><br />nv</td><td>-</td><td>92.13</td><td>97.91</td><td>94.93</td></tr><tr><td><br />vasc</td><td>-</td><td>86.67</td><td>92.86</td><td>89.66</td></tr><tr><td><br />平均值</td><td>90.28</td><td>86.00</td><td>79.97</td><td>82.40</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="234">
                    <p class="img_tit"><b>表4 基于平行皮肤优化后的分类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Classification results based on parallel skin optimization %</p>
                    <p class="img_note"></p>
                    <table id="234" border="1"><tr><td><br />皮肤类别</td><td>准确率</td><td>召回率</td><td>精确度</td><td>F1</td></tr><tr><td><br />akiec</td><td>-</td><td>95.83</td><td>71.88</td><td>82.14</td></tr><tr><td><br />bcc</td><td>-</td><td>85.45</td><td>92.16</td><td>88.68</td></tr><tr><td><br />bkl</td><td>-</td><td>79.46</td><td>81.65</td><td>80.54</td></tr><tr><td><br />df</td><td>-</td><td>70.00</td><td>63.64</td><td>66.67</td></tr><tr><td><br />mel</td><td>-</td><td>86.90</td><td>65.77</td><td>74.87</td></tr><tr><td><br />nv</td><td>-</td><td>93.71</td><td>97.76</td><td>95.69</td></tr><tr><td><br />vasc</td><td>-</td><td>100.00</td><td>100.00</td><td>100.00</td></tr><tr><td><br />平均值</td><td>90.98</td><td>87.34</td><td>81.84</td><td>84.08</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="235">通过对比可以发现, 分类的平均准确度、召回率、精确度和F1值等各项指标都有1%～2%的提升, 这说明平行皮肤框架确实有助于皮肤病的计算机辅助医学诊断.值得注意的是, 在种类vasc上, 通过平行学习引导生成实现数据扩充后, F1值达到100%.这说明融合领域知识实现特定类别数据生成扩充后, 可以提高模型泛化能力.由于文章篇幅有限, 只简单进行一步优化, 基于平行学习的迭代优化将在后续工作中展开.同时, 进一步融合领域专家知识, 将医学知识转为计算模型可识别认知的条件也将是今后工作的一部分.</p>
                </div>
                <div class="area_img" id="236">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907001_236.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 基于平行皮肤的分类结果混淆矩阵" src="Detail/GetImg?filename=images/MSSB201907001_236.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 基于平行皮肤的分类结果混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907001_236.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 Confusion matrix based on parallel skin</p>

                </div>
                <h3 id="237" name="237" class="anchor-tag">4 思考与展望</h3>
                <div class="p1">
                    <p id="238">皮肤作为人体最大的器官, 直接与外界环境接触, 具有调节体温、排泄与感受外界刺激的作用.针对皮肤的相关研究正受到广泛关注, 本文从视觉感知角度提出平行皮肤框架, 并概述已有的研究成果.而在皮肤相关研究的领域, 还有很多开放的研究方向.</p>
                </div>
                <h4 class="anchor-tag" id="239" name="239"><b>4.1 医学可解释性</b></h4>
                <div class="p1">
                    <p id="240">尽管目前基于深度网络方法的皮肤病诊断达到甚至超过皮肤科专家的水平<citation id="310" type="reference"><link href="3" rel="bibliography" /><link href="21" rel="bibliography" /><link href="133" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">10</a>,<a class="sup">66</a>]</sup></citation>, 但是这类方法缺乏医学可解释性.尽管平行皮肤框架通过平行学习选择与生成特定数据, 通过明确数据构成及来源一定程度上可以增加模型的可解释性, 但是仍具有一定局限性.例如无法将医学领域知识输出, 并给出判断具体依据.特别是在医学皮肤图像分析领域, 计算机给出医疗诊断结果时, 患者及医生都有必要知道计算机提取的语义特征, 以及做出判断的医学依据<citation id="311" type="reference"><link href="135" rel="bibliography" /><link href="137" rel="bibliography" /><sup>[<a class="sup">67</a>,<a class="sup">68</a>]</sup></citation>.作者建议深入了解皮肤病理学及生物解剖学, 与临床医生开展跨领域深入合作, 将领域知识转化为计算机可识别的语言表征, 从而设计可解释诊断模型.Maibach等<citation id="309" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">69</a>]</sup></citation>详细介绍皮肤病学的基本原理, 如临床研究的可解释性、疾病导向的证据与病人的护理证据等, 可以为基于深度网络进行辅助诊断提供可解释性.作者认为可解释的智能诊断方法可以增加辅助诊断的透明度与可信性, 在基于视觉分析的皮肤病分析应用领域, 可解释性将是医学教育和临床诊断等领域最亟需解决的问题之一.</p>
                </div>
                <h4 class="anchor-tag" id="241" name="241"><b>4.2 美容皮肤学</b></h4>
                <div class="p1">
                    <p id="242">深入了解皮肤的解剖组织特性, 从护理及保养的角度研究皮肤病问题受到越来越多的关注.基于皮肤影像分析广泛应用于皮肤病的诊断, 也可以应用于医学美容治疗前后的效果评估.所以, 平行皮肤可以实现皮肤病的智能诊断, 同样可以推广应用于美容皮肤学 (Cosmetic Dermatology) <citation id="312" type="reference"><link href="141" rel="bibliography" /><link href="143" rel="bibliography" /><link href="145" rel="bibliography" /><sup>[<a class="sup">70</a>,<a class="sup">71</a>,<a class="sup">72</a>]</sup></citation>.目前较少的工作利用人工智能方法应用于皮肤美容学.针对皮肤术前术后恢复情况、动态检测皮肤病发展状态、准确计算评估预后情况等领域, 平行皮肤分析方法具有无创、实时及动态的特定, 一定会日渐受到广泛关注.</p>
                </div>
                <h4 class="anchor-tag" id="243" name="243"><b>4.3 生物皮肤研究</b></h4>
                <div class="p1">
                    <p id="244">从生物医学角度而言, Naik等<citation id="313" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">73</a>]</sup></citation>的研究表明皮肤能够形成对皮肤炎症的记忆.具体地, 伤口或其它触发炎症的有害经历会使驻留在皮肤中的上皮干细胞带来持久记忆, 教导它们更快愈合.他们在实验中证明, 一个被称作Aim2的基因尤为重要, 这个基因编码一种“损伤与危险”的感知蛋白, 最初的炎症促进它的表达持续增加.炎症再次来临会迅速激活这种蛋白, 从而导致一种炎性信号产生, 而且这种信号增强这些干细胞迁移到伤口中的能力.针对皮肤损伤自我修复能力有限的问题, Kurita等<citation id="314" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">74</a>]</sup></citation>介绍将开放性伤口中的细胞直接转化为新皮肤细胞的方法, 依赖于将细胞重新编程为干细胞样状态, 并且可用于治疗皮肤损伤、抗衰老, 也能进一步理解皮肤癌.启发于人体皮肤特性, Lai等<citation id="315" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">75</a>]</sup></citation>利用摩擦纳米发电机, 开发第一个可自驱动、自主感知并响应内部运动和外部刺激的软体机器人皮肤.</p>
                </div>
                <h3 id="245" name="245" class="anchor-tag">5 结 束 语</h3>
                <div class="p1">
                    <p id="246">本文提出平行皮肤的概念及框架, 为基于皮肤影像数据分析的视觉感知问题提出一种新的基于数据驱动的分析方法.基于ACP方法及平行医学图像框架, 提出构建人工皮肤图像系统以选择与生成数据, 并通过平行学习实现诊断分析系统的闭环优化的统一框架.本文概述系统构成环节中已有相关工作, 并通过对已有工作调研分析给出具体思考与该领域未来研究展望.同时, 还开展初步应用实例研究.需要指出的是, 细节之处还需要进一步完善, 在皮肤影像分析领域的平行学习理论还需要进一步研究.作者相信, 整合“描述智能-预测智能-指示智能”的平行皮肤方法一定会受到更广泛的关注, 并实际应用于皮肤相关的各个领域.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="317" type="" href="images/MSSB201907001_31700.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">王飞跃</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="319" type="" href="images/MSSB201907001_31900.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">苟超</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="321" type="" href="images/MSSB201907001_32100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">王建功</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="325" type="" href="images/MSSB201907001_32500.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">沈甜雨</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="326" type="" href="images/MSSB201907001_32600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">郑文博</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="327" type="" href="images/MSSB201907001_32700.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">于慧</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dermatologist-level classification of skin cancer with deep neural networks">

                                <b>[1]</b> ESTEVA A, KUPREL B, NOVOA R A, <i>et al</i>.Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks.Nature, 2017, 542 (7639) :115-118.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES04E6EF0FAA0B9ED3F471E3798297A7A3&amp;v=MDUxNjM1Mlk4ekZab1BmblZNdXhWbDdqaDhQWHpscFJvM2NMWGxRc3VjQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0x5NHdLcz1OaWZPZmJPOGE5ZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> HAY R J, JOHNS N E, WILLIAMS H C, <i>et al</i>.The Global Burden of Skin Disease in 2010:An Analysis of the Prevalence and Impact of Skin Conditions.Journal of Investigative Dermatology, 2014, 134 (6) :1527-1534.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Incidence Estimate of Nonmelanoma Skin Cancer (Keratinocyte Carcinomas)in the U.S.Population,2012">

                                <b>[3]</b> ROGERS H W, WEINSTOCK M A, FELDMAN S R, <i>et al</i>.Incidence Estimate of Nonmelanoma Skin Cancer (Keratinocyte Carcinomas) in the US Population, 2012.JAMA Dermatology, 2015, 151 (10) :1081-1086.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWDD72DD4502879DD556EBFEC6ABBE160F5&amp;v=MDU2MDZ5NHdLcz1OaWZjYXNlL0hLVzRxNHBGWnVNSUJRaE55aE1WbjAwTFBRemszV0JIRExPU1JjeWFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3TA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> BRAY F, FERLAY J, SOERJOMATARAM I, <i>et al</i>.Global Cancer Statistics 2018:Globocan Estimates of Incidence and Mortality Worldwide for 36 Cancers in 185 Countries.CA:A Cancer Journal for Clinicians, 2018, 68 (6) :394-424.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201811034&amp;v=MzE1MjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZML0xMejdCZDdHNEg5bk5ybzlHWUlRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 何雪英, 韩忠义, 魏本征.基于深度卷积神经网络的色素性皮肤病识别分类.计算机应用, 2018, 38 (11) :3236-3240. (HE X Y, HAN Z Y, WEI B Z.Pigmented Skin Lesion Recognition and Classification Based on Deep Convolutional Neural Network.Journal of Computer Applications, 2018, 38 (11) :3236-3240.) 
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning">

                                <b>[6]</b> LECUN Y, BENGIO Y, HINTON G.Deep Learning.Nature, 2015, 521 (7553) :436-444.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201803002&amp;v=MDg3NzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpoVkwvTEtDTGZZYkc0SDluTXJJOUZab1E=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 田娟秀, 刘国才, 谷珊珊, 等.医学图像分析深度学习方法研究与挑战.自动化学报, 2018, 44 (3) :401-424. (TIAN J X, LIU G X C, GU S S, <i>et al</i>.Deep Learning in Medical Image Analysis and Its Challenges.Acta Automatica Sinica, 2018, 44 (3) :401-424.) 
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES30D6C0E9B3B77AC4A1F9C7A8227303EC&amp;v=MDk2MzVIWWZPR1FsZkNwYlEzNU5GaHdMeTR3S3M9TmlmT2ZiQzRhdGUvci9wTUZ1aDlDM3RJdkJKaTYwbDBPM2lUcEJBM2ZyR1VScy9zQ09OdkZTaVdXcjdKSUZwbWFCdQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> LITJENS G, KOOI T, BEJNORDI B E, <i>et al</i>.A Survey on Deep Learning in Medical Image Analysis.Medical Image Analysis, 2017, 42:60-88.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Melanoma classification on dermoscopy images using a neural network ensemble model">

                                <b>[9]</b> XIE F Y, FAN H D, LI Y, <i>et al</i>.Melanoma Classification on Dermoscopy Images Using a Neural Network Ensemble Model.IEEE Transactions on Medical Imaging, 2017, 36 (3) :849-858.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Man against machine:diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58dermatologists">

                                <b>[10]</b> HAENSSLE H A, FINK C, SCHNEIDERBAUER R, <i>et al</i>.Man Against Machine:Diagnostic Performance of a Deep Learning Convolutional Neural Network for Dermoscopic Melanoma Recognition in Comparison to 58 Dermatologists.Annals of Oncology, 2018, 29 (8) :1836-1842.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dermoscopy Image Analysis:Overview and Future Directions">

                                <b>[11]</b> CELEBI M E, CODELLA N, HALPERN A.Dermoscopy Image Analysis:Overview and Future Directions.IEEE Journal of Biomedical and Health Informatics, 2019, 23 (2) :474-478.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC200405001&amp;v=MzA5NDZPZVplUm5GeXpoVkwvTExqZlNiYkc0SHRYTXFvOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 王飞跃.平行系统方法与复杂系统的管理和控制.控制与决策, 2004, 19 (5) :485-489, 514. (WANG F Y.Parallel System Methods for Management and Control of Complex Systems.Control and Decision, 2004, 19 (5) :485-489, 514.) 
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parallel control and management for intelligent transportation systems: Concepts, architectures, and applications">

                                <b>[13]</b> WANG F Y.Parallel Control and Nanagement for Intelligent Transportation Systems:Concepts, Architectures, and Applications.IEEE Transactions on Intelligent Transportation Systems, 2010, 11 (3) :630-638.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDHB201704001&amp;v=MjA1NDdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZML0xQeW5EYkxHNEg5Yk1xNDlGWllRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> WANG F Y, ZHENG N N, CAO D P, <i>et al</i>.Parallel Driving in CPSS:A Unified Approach for Transport Automation and Vehicle Intelligence.IEEE/CAA Journal of AutomaticaSinica, 2017, 4 (4) :577-587.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parallel testing of vehicle intelligence via virtual-real interaction">

                                <b>[15]</b> LI L, WANG X, WANG K F, <i>et al</i>.Parallel Testing of Vehicle Intelligence via Virtual-Real Interaction.Science Robotics, 2019, 4 (28) .DOI:10.1126/scirobotics.aaw4106.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" >
                                    <b>[16]</b>
                                 吕宜生, 陈圆圆, 金峻臣, 等.平行交通:虚实互动的智能交通管理与控制.智能科学与技术学报, 2019, 1 (1) :21-33. (LÜ Y S, CHEN Y Y, JIN J C, <i>et al</i>.Parallel Transportation:Virtual-Real Interaction for Intelligent Traffic Management and Control.Chinese Journal of Intelligent Science and Technology, 2019, 1 (1) :21-33.) 
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" >
                                    <b>[17]</b>
                                 刘腾, 王晓, 邢阳, 等.基于数字四胞胎的平行驾驶系统及应用.智能科学与技术学报, 2019, 1 (1) :40-51. (LIU T, WANG X, XING Y, <i>et al</i>.Research on Digital Quadruplets in Cyber-Physical-Social Space-Based Parallel Driving.Chinese Journal of Intelligent Science and Technology, 2019, 1 (1) :40-51.) 
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDHB201604001&amp;v=MjM1MzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpoVkwvTFB5bkRiTEc0SDlmTXE0OUZaWVE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> WANG F Y, WANG X, LI L X, <i>et al</i>.Steps toward Parallel Intelligence.IEEE/CAA Journal of Automatica Sinica, 2016, 3 (4) :345-348.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDHB201602001&amp;v=MTMyNzdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emhWTC9MUHluRGJMRzRIOWZNclk5Rlo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> WANG F Y, ZHANG J J, ZHENG X H, <i>et al</i>.Where Does Alphago Go:From Church-Turing Thesis to Alphago Thesis and Beyond.IEEE/CAA Journal of Automatica Sinica, 2016, 3 (2) :113-120.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=From Intelligent Vehicles to Smart Societies:A Parallel Driving Approach">

                                <b>[20]</b> WANG F Y, YUAN Y, LI J J, <i>et al</i>.From Intelligent Vehicles to Smart Societies:A Parallel Driving Approach.IEEE Transactions on Computational Social Systems, 2018, 5 (3) :594-604.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Social Transportation:Social Signal and Technology for Transportation Engineering">

                                <b>[21]</b> WANG F Y, WANG P, LI J J, <i>et al</i>.Social Transportation:Social Signal and Technology for Transportation Engineering.IEEE Tran-sactions on Computational Social Systems, 2019, 6 (1) :2-7.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Social Energy:Emerging Token Economy for Energy Production and Consumption">

                                <b>[22]</b> WANG F Y, ZHANG J J, QIN R, <i>et al</i>.Social Energy:Emerging Token Economy for Energy Production and Consumption.IEEE Transactions on Computational Social Systems, 2019, 6 (3) :388-393.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parallel Vision for Perception and Understanding of Complex Scenes:Methods,Framework,and Perspectives">

                                <b>[23]</b> WANG K F, GOU C, ZHENG N N, <i>et al</i>.Parallel Vision for Perception and Understanding of Complex Scenes:Methods, Framework, and Perspectives.Artificial Intelligence Review, 2017, 48 (3) :299-329.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES77299E0B8A188971A7818AF4D9CA52B7&amp;v=MjcxMDFTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0x5NHdLcz1OaWZPZmJTL0hOakYybzgzYkpvT0JIUXd5QmRpN1RkOFFBNlVxR1k4Q3NPUlI4aVlDT052Rg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b> GOU C, ZHANG H, WANG K F, <i>et al</i>.Cascade Learning from Adversarial Synthetic Images for Accurate Pupil Detection.Pattern Recognition, 2019, 88:584-594.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Intelligent Systems and Technologyfor Integrative and Predictive Medicine:An ACP Approach">

                                <b>[25]</b> WANG F Y, WONG P K.Intelligent Systems and Technology for Integrative and Predictive Medicine:An ACP Approach.ACM Transactions on Intelligent Systems and Technology (TIST) , 2013, 4 (2) .DOI:10.1145/2438653.2438667.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201712001&amp;v=MTcwNDhIOWJOclk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emhWTC9MS0Q3WWJMRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> 王飞跃, 李长贵, 国元元, 等.平行高特:基于ACP的平行痛风诊疗系统框架.模式识别与人工智能, 2017, 30 (12) :1057-1068. (WANG F Y, LI C G, GUO Y Y, <i>et al</i>.Parallel Gout:An ACP-Based System Framework for Gout Diagnosis and Treatment.Pattern Recognition and Artificial Intelligence, 2017, 30 (12) :1057-1068.) 
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZHKZ201704015&amp;v=Mjc3NDg2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpoVkwvTFB5WEFkTEc0SDliTXE0OUVZWVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> 孟祥冰, 王蓉, 张梅, 等.平行感知:ACP 理论在视觉SLAM技术中的应用.指挥与控制学报, 2017, 3 (4) :350-358. (MENG X B, WAGN R, ZHANG M, <i>et al</i>.Parallel Perception:An ACP-Based Approach to Visual SLAM.Journal of Command and Control, 2017, 3 (4) :350-358.) 
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201711001&amp;v=MDk4NDVMRzRIOWJOcm85RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emhWTC9MS0Q3WWI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b> 王飞跃, 张梅, 孟祥冰, 等.平行手术:基于ACP的智能手术计算方法.模式识别与人工智能, 2017, 30 (11) :961-970. (WANG F Y, ZHANG M, MEGN X B, <i>et al</i>.Parallel Surgery:An ACP-Based Approach for Intelligent Operations.Pattern Recognition and Artificial Intelligence, 2017, 30 (11) :961-970.) 
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Social Education:Opportunities and Challenges in Cyber-Physical-Social Space">

                                <b>[29]</b> WANG F Y, TANG Y, LIU X, <i>et al</i>.Social Education:Opportunities and Challenges in Cyber-Physical-Social Space.IEEE Transactions on Computational Social Systems, 2019, 6 (2) :191-196.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint Task Difficulties Estimation and Testees Ranking for Intelligence Evaluation">

                                <b>[30]</b> ZHANG C, LIU Y H, LI L, <i>et al</i>.Joint Task Difficulties Estimation and Testees Ranking for Intelligence Evaluation.IEEE Tran-sactions on Computational Social Systems, 2019, 6 (2) :221-226.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_31" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201707001&amp;v=MDYwMzMzenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZML0xLRDdZYkxHNEg5Yk1xSTlGWllRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[31]</b> 王坤峰, 鲁越, 王雨桐, 等.平行图像:图像生成的一个新型理论框架.模式识别与人工智能, 2017, 30 (7) :577-587. (WANG K F, LU Y, WANG Y T, <i>et al</i>.Parallel Imaging:A New Theoretical Framework for Image Generation.Pattern Recognition and Artificial Intelligence, 2017, 30 (7) :577-587.) 
                            </a>
                        </p>
                        <p id="65">
                            <a id="bibliography_32" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201610003&amp;v=MjE4MjZPZVplUm5GeXpoVkwvTEtDTGZZYkc0SDlmTnI0OUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[32]</b> 王坤峰, 苟超, 王飞跃.平行视觉:基于ACP的智能视觉计算方法.自动化学报, 2016, 42 (10) :1490-1500. (WANG K F, GOU C, WANG F Y.Parallel Vision:An ACP-Based Approach to Intelligent Vision Computing.Acta Automatica Sinica, 2016, 42 (10) :1490-1500.) 
                            </a>
                        </p>
                        <p id="67">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Seamless Lesion Insertion for Data Augmentation in CAD Training">

                                <b>[33]</b> PEZESHK A, PETRICK N, CHEN W, <i>et al</i>.Seamless Lesion Insertion for Data Augmentation in CAD Training.IEEE Transactions on Medical Imaging, 2017, 36 (4) :1005-1015.
                            </a>
                        </p>
                        <p id="69">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning-by-Synthesis for Accurate Eye Detection">

                                <b>[34]</b> GOU C, WU Y, WANG K, <i>et al</i>.Learning-by-Synthesis for Accurate Eye Detection // Proc of the 23rd International Conference on Pattern Recognition.Washington, USA:IEEE, 2016:3362-3367.
                            </a>
                        </p>
                        <p id="71">
                            <a id="bibliography_35" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES69A82D54179C110F0C0B42C20756622F&amp;v=MjM2Mzh5NHdLcz1OaWZPZmJXeGI5bk8yNHBCWmV3R2YzMDR6MkFUbVQ4UFRIMlJyaEl5ZkxTU1I3anBDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3TA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[35]</b> GOU C, WU Y, WANG K F, <i>et al</i>.A Joint Cascaded Framework for Simultaneous Eye Detection and Eye State Estimation.Pattern Recognition, 2017, 67:23-31.
                            </a>
                        </p>
                        <p id="73">
                            <a id="bibliography_36" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201701002&amp;v=Mjc5MjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZML0xLQ0xmWWJHNEg5Yk1ybzlGWm9RS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[36]</b> 李力, 林懿伦, 曹东璞, 等.平行学习——机器学习的一个新型理论框架.自动化学报, 2017, 43 (1) :1-8. (LI L, LIN Y L, CAO D P, <i>et al</i>.Parallel Learning-A New Framework for Machine Learning.Acta Automatica Sinica, 2017, 43 (1) :1-8.) 
                            </a>
                        </p>
                        <p id="75">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the Crossroad of Artificial Intelligence:A Revisit to Alan Turing and Norbert Wiener">

                                <b>[37]</b> LI L, ZHENG N N, WANG F Y.On the Crossroad of Artificial Intelligence:A Revisit to Alan Turing and Norbert Wiener.IEEE Transactions on Cybernetics, 2019, 49 (10) :3618-3626.
                            </a>
                        </p>
                        <p id="77">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parallel Medical Imaging:A New Data-Knowledge-Driven Evolutionary Framework for Medical Image Analysis[C/OL]">

                                <b>[38]</b> GOU C, SHEN T Y, ZHENG W B, <i>et al</i>.Parallel Medical Imaging:A New Data-Knowledge-Driven Evolutionary Framework for Medical Image Analysis[C/OL].[2019-06-25].https://arxiv.org/pdf/1903.04855.pdf.
                            </a>
                        </p>
                        <p id="79">
                            <a id="bibliography_39" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201708001&amp;v=MjE5MTJwNDlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZML0xLRDdZYkxHNEg5Yk0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[39]</b> 刘昕, 王晓, 张卫山, 等.平行数据:从大数据到数据智能.模式识别与人工智能, 2017, 30 (8) :673-681. (LIU X, WANG X, ZHANG W S, <i>et al</i>.Parallel Data:From Big Data to Data Intelligence.Pattern Recognition and Artificial Intelligence, 2017, 30 (8) :673-681.) 
                            </a>
                        </p>
                        <p id="81">
                            <a id="bibliography_40" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JZUS201702001&amp;v=MjYwNTBiTXJZOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpoVkwvTEx6ZmVmYkc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[40]</b> ZHENG N N, LIU Z Y, REN P J, <i>et al</i>.Hybrid-Augmented Intelligence:Collaboration and Cognition.Frontiers of Information Technology and Electronic Engineering, 2017, 18 (2) :153-179.
                            </a>
                        </p>
                        <p id="83">
                            <a id="bibliography_41" >
                                    <b>[41]</b>
                                 郑南宁.人工智能新时代.智能科学与技术学报, 2019, 1 (1) :1-3. (ZHENG N N.The New Era of Artificial Intelligence.Chinese Journal of Intelligent Science and Technology, 2019, 1 (1) :1-3.) 
                            </a>
                        </p>
                        <p id="85">
                            <a id="bibliography_42" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Neural Networks Approach to Skin Lesions Classification-A Comparative Analysis">

                                <b>[42]</b> KWASIGROCH A, MIKOŁAJCZYK A, GROCHOWSKI M.Deep Neural Networks Approach to Skin Lesions Classification-A Comparative Analysis // Proc of the 22nd International Conference on Methods and Models in Automation and Robotics (MMAR) .Washington, USA:IEEE, 2017:1069-1074.
                            </a>
                        </p>
                        <p id="87">
                            <a id="bibliography_43" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ge-nerative Adversarial Nets[M/OL]">

                                <b>[43]</b> GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, <i>et al</i>.Ge-nerative Adversarial Nets[M/OL].[2019-06-25].https://arxiv.org/pdf/1406.2661.pdf.
                            </a>
                        </p>
                        <p id="89">
                            <a id="bibliography_44" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201703001&amp;v=MTkwMjZHNEg5Yk1ySTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6aFZML0xLQ0xmWWI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[44]</b> 王坤峰, 苟超, 段艳杰, 等.生成式对抗网络GAN的研究进展与展望.自动化学报, 2017, 43 (3) :321-332. (WANG K F, GOU C, DUAN Y J, <i>et al</i>.Generative Adversarial Networks:The State of the Art and Beyond.Acta Automatica Sinica, 2017, 43 (3) :321-332.) 
                            </a>
                        </p>
                        <p id="91">
                            <a id="bibliography_45" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MelanoGANs:High Re- solution Skin Lesion Synthesis with GANs[C/OL]">

                                <b>[45]</b> BAUR C, ALBARQOUNI S, NAVAB N.MelanoGANs:High Re- solution Skin Lesion Synthesis with GANs[C/OL].[2019-06-25].https://arxiv.org/pdf/1804.04338v1.pdf.
                            </a>
                        </p>
                        <p id="93">
                            <a id="bibliography_46" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks[C/OL]">

                                <b>[46]</b> RADFORD A, METZ L, CHINTALA S.Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks[C/OL].[2019-06-25].https://arxiv.org/pdf/1511.06434.pdf.
                            </a>
                        </p>
                        <p id="95">
                            <a id="bibliography_47" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks">

                                <b>[47]</b> DENTON E L, CHINTALA S, SZLAM A, <i>et al</i>.Deep Generative Image Models Using a Laplacian Pyramid of Adversarial Networks // CORTES C, LAWRENCE N D, LEE D, eds.Advances in Neural Information Processing Systems 28.Cambridge, USA:The MIT Press, 2015:1486-1494.
                            </a>
                        </p>
                        <p id="97">
                            <a id="bibliography_48" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Skin Lesion Synthesis with Generative Adversarial Networks[C/OL]">

                                <b>[48]</b> BISSOTO A, PEREZ F, VALLE E, <i>et al</i>.Skin Lesion Synthesis with Generative Adversarial Networks[C/OL].[2019-06-25].https://arxiv.org/pdf/1902.03253.pdf.
                            </a>
                        </p>
                        <p id="99">
                            <a id="bibliography_49" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-resolution image synthesis and semantic manipulation with conditional gans">

                                <b>[49]</b> WANG T C, LIU M Y, ZHU J Y, <i>et al</i>.High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:8798-8807.
                            </a>
                        </p>
                        <p id="101">
                            <a id="bibliography_50" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data Augmentation for Improving Deep Learning in Image Classification Problem">

                                <b>[50]</b> MIKOAJCZYK A, GROCHOWSKI M.Data Augmentation for Improving Deep Learning in Image Classification Problem // Proc of the International Interdisciplinary Ph.D.Workshop.Washington, USA:IEEE, 2018:117-122.
                            </a>
                        </p>
                        <p id="103">
                            <a id="bibliography_51" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data-Driven Color Augmentation Techniques for Deep Skin Image Analysis[C/OL]">

                                <b>[51]</b> GALDRAN A, ALVAREZ-GILA A, MEYER M I, <i>et al</i>.Data-Driven Color Augmentation Techniques for Deep Skin Image Analysis[C/OL].[2019-06-25].https://arxiv.org/pdf/1703.03702v1.pdf.
                            </a>
                        </p>
                        <p id="105">
                            <a id="bibliography_52" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An imaging system with calibrated color image acquisition for use in dermatology">

                                <b>[52]</b> HAEGHEN Y V, NAEYAERT J M A D, LEMAHIEU I, <i>et al</i>.An Imaging System with Calibrated Color Image Acquisition for Use in Dermatology.IEEE Transactions on Medical Imaging, 2000, 19 (7) :722-730.
                            </a>
                        </p>
                        <p id="107">
                            <a id="bibliography_53" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300048871&amp;v=MTgxNTlPZmJLN0h0RE9ySTlGWk84SEJIczRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRm9UYWhFPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[53]</b> QUINTANA J, GARCIA R, NEUMANN L.A Novel Method for Color Correction in Epiluminescence Microscopy.Computerized Medical Imaging and Graphics, 2011, 35 (7/8) :646-652.
                            </a>
                        </p>
                        <p id="109">
                            <a id="bibliography_54" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interactive Atlas of Dermoscopy">

                                <b>[54]</b> ARGENZIANO G, SOYER H, DE GIORGI V, <i>et al</i>.Interactive Atlas of Dermoscopy (Book and CD-ROM) .Milan, Italy:EDRA Medical Publishing &amp; New Media, 2000.
                            </a>
                        </p>
                        <p id="111">
                            <a id="bibliography_55" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PH2-A Dermoscopic Image Database for Research and Benchmarking">

                                <b>[55]</b> MENDONCŸA T, FERREIRA P M, MARQUES J S, <i>et al</i>.PH<sup>2</sup>-A Dermoscopic Image Database for Research and Benchmarking // Proc of the 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society.Washington, USA:IEEE, 2013:5437-5440.
                            </a>
                        </p>
                        <p id="113">
                            <a id="bibliography_56" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Benchmark for Automatic Visual Classification of Clinical Skin Disease Images">

                                <b>[56]</b> SUN X X, YANG J F, SUN M, <i>et al</i>.A Benchmark for Automatic Visual Classification of Clinical Skin Disease Images // Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:206-222.
                            </a>
                        </p>
                        <p id="115">
                            <a id="bibliography_57" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Se-ven-Point Checklist and Skin Lesion Classification Using Multi-task Multi-modal Neural Nets">

                                <b>[57]</b> KAWAHARA J, DANESHVAR S, ARGENZIANO G, <i>et al</i>.Se-ven-Point Checklist and Skin Lesion Classification Using Multi-task Multi-modal Neural Nets.IEEE Journal of Biomedical and Health Informatics, 2018, 23 (2) :538-546.
                            </a>
                        </p>
                        <p id="117">
                            <a id="bibliography_58" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Hyperspectral Dermoscopy Dataset for Melanoma Detection">

                                <b>[58]</b> GU Y Y, PARTRIDGE Y P, ZHOU J.A Hyperspectral Dermoscopy Dataset for Melanoma Detection // Proc of the International Workshop on Skin Image Analysis.Berlin, Germany:Springer, 2018:268-276.
                            </a>
                        </p>
                        <p id="119">
                            <a id="bibliography_59" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Skin Lesion Analysis toward Melanoma Detection:A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI),Hosted by the International Skin Imaging Collaboration (ISIC)">

                                <b>[59]</b> CODELLA N C F, GUTMAN D, CELEBI M E, <i>et al</i>.Skin Lesion Analysis toward Melanoma Detection:A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI) , Hosted by the International Skin Imaging Collaboration (ISIC) // Proc of the 15th IEEE International Symposium on Biomedical Imaging.Washington, USA:IEEE, 2018:168-172.
                            </a>
                        </p>
                        <p id="121">
                            <a id="bibliography_60" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Ham10000 Dataset,A Large Collection of Multi-source Dermatoscopic Images of Common Pigmented Skin Lesions">

                                <b>[60]</b> TSCHANDL P, ROSENDAHL C, KITTLER H.The Ham10000 Dataset, A Large Collection of Multi-source Dermatoscopic Images of Common Pigmented Skin Lesions.Scientific Data, 2018, 5.DOI:10.1038/sdata.2019.161.
                            </a>
                        </p>
                        <p id="123">
                            <a id="bibliography_61" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Radiomics:Images are more than pictures,they are data">

                                <b>[61]</b> GILLIES R J, KINAHAN P E, HRICAK H.Radiomics:Images Are More Than Pictures, They Are Data.Radiology, 2015, 278 (2) :563-577.
                            </a>
                        </p>
                        <p id="125">
                            <a id="bibliography_62" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clinical Skin Lesion Diagnosis Using Representations Inspired by Dermatologist Criteria">

                                <b>[62]</b> YANG J F, SUN X X, LIANG J, <i>et al</i>.Clinical Skin Lesion Diagnosis Using Representations Inspired by Dermatologist Criteria // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:1258-1266.
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_63" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rethinking the inception architecture for computer vision">

                                <b>[63]</b> SZEGEDY C, VANHOUCKE V, IOFFE S, <i>et al</i>.Rethinking the Inception Architecture for Computer Vision // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:2818-2826.
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_64" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Descriptive and Prescriptive Learning Theories">

                                <b>[64]</b> ULLRICH C.Descriptive and Prescriptive Learning Theories // Proc of the 15th IEEE International Symposium on Biomedical Imaging.Washington, USA:IEEE, 2008:37-42.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_65" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">

                                <b>[65]</b> HE K M, ZHANG X Y, REN S Q, <i>et al</i>.Identity Mappings in Deep Residual Networks // Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:630-645.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_66" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Expert-Level Diagnosis of Nonpigmented Skin Cancer by Combined Convolutional Neural Networks">

                                <b>[66]</b> TSCHANDL P, ROSENDAHL C, AKAY B N, <i>et al</i>.Expert-Level Diagnosis of Nonpigmented Skin Cancer by Combined Convolutional Neural Networks.JAMA Dermatology, 2018.DOI:10.1001/jamadermatol.2018.4378.
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_67" target="_blank" href="http://scholar.cnki.net/result.aspx?q=What Do We Need to Build Explainable AI Systems for the Medical Domain?[C/OL]">

                                <b>[67]</b> HOLZINGER A, BIEMANN C, PATTICHIS C S, <i>et al</i>.What Do We Need to Build Explainable AI Systems for the Medical Domain?[C/OL].[2019-06-25].https://arxiv.org/pdf/1712.09923.pdf.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_68" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Elements of Causal Inference:Foundations and Learning Algorithms">

                                <b>[68]</b> PETERS J, JANZING D, SCHÖLKOPF B.Elements of Causal Inference:Foundations and Learning Algorithms.Cambridge, USA:The MIT Press, 2017.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_69" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Evidence Based Dermatology">

                                <b>[69]</b> MAIBACH H I.Evidence Based Dermatology.Raleigh, USA:PMPH-USA Limited, 2012.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_70" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Handbook of Cosmetic Skin Care">

                                <b>[70]</b> SHAI A, MAIBACH H I, BARAN R.Handbook of Cosmetic Skin Care.London, UK:Informa UK Ltd, 2009.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_71" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Textbook of Cosmetic Dermatology">

                                <b>[71]</b> BARAN R, MAIBACH H I.Textbook of Cosmetic Dermatology.Boca Raton, USA:Taylor &amp; Francis Group, 2017.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_72" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cosmetic and dermatology: bleaching creams">

                                <b>[72]</b> ENGASSER P G, MAIBACH H I.Cosmetics and Dermatology:Bleaching Creams.Journal of the American Academy of Dermatology, 1981, 5 (2) :143-147.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_73" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inflammatory memory sensitizes skin epithelial stem cells to tissue damage">

                                <b>[73]</b> NAIK S, LARSEN S B, GOMEZ N C, <i>et al</i>.Inflammatory Memory Sensitizes Skin Epithelial Stem Cells to Tissue Damage.Nature, 2017, 550 (7677) :475-480.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_74" target="_blank" href="http://scholar.cnki.net/result.aspx?q=In Vivo Reprogramming of Wound-Resident Cells Generates Skin Epithelial Tissue">

                                <b>[74]</b> KURITA M, ARAOKA T, HISHIDA T, <i>et al</i>.In Vivo Reprogramming of Wound-Resident Cells Generates Skin Epithelial Tissue.Nature, 2018, 561 (7722) :243-247.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_75" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD62322ED87E49F40EEB98F630A8B3A72A&amp;v=MjE5ODR3S3M9TmlmY2FyVzZIZFBPMnZ0Tlk1NExCUW85ejJObW1EWjFQbm5ockdNOUM3SGxRcmp1Q09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0x5NA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[75]</b> LAI Y C, DENG J N, LIU R Y, <i>et al</i>.Actively Perceiving and Responsive Soft Robots Enabled by Self-powered, Highly Extensible, And Highly Sensitive Triboelectric Proximity-and Pressure-Sensing Skins.Advanced Materials, 2018, 30 (28) :1801114.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201907001" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201907001&amp;v=Mjk1NjdlWmVSbkZ5emhWTC9MS0Q3WWJMRzRIOWpNcUk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
