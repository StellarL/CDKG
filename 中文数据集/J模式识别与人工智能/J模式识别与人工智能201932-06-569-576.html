<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131451185186250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201906011%26RESULT%3d1%26SIGN%3d2q%252buf5amNyL6WRRUJprth%252bxENEg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201906011&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201906011&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201906011&amp;v=MjAxNjFEN1liTEc0SDlqTXFZOUVaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpoVUw3Sks=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#60" data-title="1 支持向量机 ">1 支持向量机</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#78" data-title="2 基于识别关键样本点的非平衡数据核SVM算法 ">2 基于识别关键样本点的非平衡数据核SVM算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#79" data-title="&lt;b&gt;2.1&lt;/b&gt; 问题分析"><b>2.1</b> 问题分析</a></li>
                                                <li><a href="#86" data-title="&lt;b&gt;2.2&lt;/b&gt; 识别关键样本点"><b>2.2</b> 识别关键样本点</a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;2.3&lt;/b&gt; 基于超平面的划分"><b>2.3</b> 基于超平面的划分</a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;2.4&lt;/b&gt; 算法步骤"><b>2.4</b> 算法步骤</a></li>
                                                <li><a href="#135" data-title="&lt;b&gt;2.5&lt;/b&gt; 时间复杂度分析"><b>2.5</b> 时间复杂度分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#138" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#209" data-title="4结束语 ">4结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#88" data-title="图1 支持向量">图1 支持向量</a></li>
                                                <li><a href="#102" data-title="图2 选择多数类中的关键样本点">图2 选择多数类中的关键样本点</a></li>
                                                <li><a href="#106" data-title="图3 异类近邻抽样方法可能造成的模型误差">图3 异类近邻抽样方法可能造成的模型误差</a></li>
                                                <li><a href="#109" data-title="图4 基于超平面的划分方法">图4 基于超平面的划分方法</a></li>
                                                <li><a href="#120" data-title="图5 IK-KSVM图示">图5 IK-KSVM图示</a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;表1 数据集的基本信息&lt;/b&gt;"><b>表1 数据集的基本信息</b></a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;表2 两类问题的混淆矩阵&lt;/b&gt;"><b>表2 两类问题的混淆矩阵</b></a></li>
                                                <li><a href="#212" data-title="图6 6种算法在8个数据集上的G-mean值对比">图6 6种算法在8个数据集上的G-mean值对比</a></li>
                                                <li><a href="#212" data-title="图6 6种算法在8个数据集上的G-mean值对比">图6 6种算法在8个数据集上的G-mean值对比</a></li>
                                                <li><a href="#213" data-title="图7 6种算法在8个数据集上的AUC对比">图7 6种算法在8个数据集上的AUC对比</a></li>
                                                <li><a href="#164" data-title="&lt;b&gt;表3 不同非平衡度下IK-KSVM的G-mean差异&lt;/b&gt;"><b>表3 不同非平衡度下IK-KSVM的G-mean差异</b></a></li>
                                                <li><a href="#165" data-title="&lt;b&gt;表4 不同非平衡度下IK-KSVM的AUC差异&lt;/b&gt;"><b>表4 不同非平衡度下IK-KSVM的AUC差异</b></a></li>
                                                <li><a href="#167" data-title="&lt;b&gt;表5 6种算法在大规模数据集上的运行时间&lt;/b&gt;"><b>表5 6种算法在大规模数据集上的运行时间</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="214">


                                    <a id="bibliography_1" title=" HE H B, GARCIA E A.Learning from Imbalanced Data.IEEE Transactions on Knowledge and Data Engineering, 2009, 21 (9) :1263-1284." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning from Imbalanced Data">
                                        <b>[1]</b>
                                         HE H B, GARCIA E A.Learning from Imbalanced Data.IEEE Transactions on Knowledge and Data Engineering, 2009, 21 (9) :1263-1284.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_2" title=" WANG S, MINKU L L, YAO X.Resampling-Based Ensemble Methods for Online Class Imbalance Learning.IEEE Transactions on Knowledge and Data Engineering, 2015, 27 (5) :1356-1368." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Resampling-based ensemble methods for online class imbalance learning">
                                        <b>[2]</b>
                                         WANG S, MINKU L L, YAO X.Resampling-Based Ensemble Methods for Online Class Imbalance Learning.IEEE Transactions on Knowledge and Data Engineering, 2015, 27 (5) :1356-1368.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_3" title=" TAHIR M A, KITTLER J, YAN F.Inverse Random under Sampling for Class Imbalance Problem and Its Application to Multi-label Classification.Pattern Recognition, 2012, 45 (10) :3738-3750." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600737766&amp;v=MzA0NzE3SHRETnFZOUZZK2dJQzNvL29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGb1hheE09TmlmT2ZiSw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         TAHIR M A, KITTLER J, YAN F.Inverse Random under Sampling for Class Imbalance Problem and Its Application to Multi-label Classification.Pattern Recognition, 2012, 45 (10) :3738-3750.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_4" title=" CHAWLA N V, BOWYER K, HALL L O, et al.SMOTE:Synthe-tic Minority Over-Sampling Technique.Journal of Artificial Intelligence Research, 2011, 16:321-357." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SMOTE: synthetic minority over-sampling technique">
                                        <b>[4]</b>
                                         CHAWLA N V, BOWYER K, HALL L O, et al.SMOTE:Synthe-tic Minority Over-Sampling Technique.Journal of Artificial Intelligence Research, 2011, 16:321-357.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_5" title=" SHAO Y H, CHEN W J, ZHANG J J, et al.An Efficient Weighted Lagrangian Twin Support Vector Machine for Imbalanced Data Cla-ssification.Pattern Recognition, 2014, 47 (9) :3158-3167." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061600062017&amp;v=MTQ5NDV5am1VTGJJSkZvWGF4TT1OaWZPZmJLOEh0Zk5xWTlGWk8wTkRIMCtvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         SHAO Y H, CHEN W J, ZHANG J J, et al.An Efficient Weighted Lagrangian Twin Support Vector Machine for Imbalanced Data Cla-ssification.Pattern Recognition, 2014, 47 (9) :3158-3167.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_6" title=" AKBAIN R, KWEK S, JAPKOWICZ N.Applying Support Vector Machines to Imbalanced Data Sets // Proc of the European Confe-rence on Machine Learning.Berlin, Germany:Springer, 2004:39-50." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Applying Support Vector Machines to Imbalanced Datasets">
                                        <b>[6]</b>
                                         AKBAIN R, KWEK S, JAPKOWICZ N.Applying Support Vector Machines to Imbalanced Data Sets // Proc of the European Confe-rence on Machine Learning.Berlin, Germany:Springer, 2004:39-50.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_7" title=" WANG B X, JAPKOWICZ N.Boosting Support Vector Machines for Imbalanced Data Sets.Knowledge and Information Systems, 2010, 25 (1) :1-20" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003722389&amp;v=MTU1NTJOajdCYXJPNEh0SFBxSTFIWitNR1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkNIbFY3dkxJVjg9&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         WANG B X, JAPKOWICZ N.Boosting Support Vector Machines for Imbalanced Data Sets.Knowledge and Information Systems, 2010, 25 (1) :1-20
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_8" title=" SUN Z B, SONG Q B, ZHU X Y, et al.A Novel Ensemble Method for Classifying Imbalanced Data.Pattern Recognition, 2015, 48 (5) :1623-1637." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14121200017863&amp;v=MDM0Njc9TmlmT2ZiSzhIOVBOclk5RlpPb0lCSG82b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZvWGF4TQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         SUN Z B, SONG Q B, ZHU X Y, et al.A Novel Ensemble Method for Classifying Imbalanced Data.Pattern Recognition, 2015, 48 (5) :1623-1637.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_9" title=" GUO H X, LI Y J, JENNIFER S, et al.Learning from Class-Imba-lanced Data:Review of Methods and Applications.Expert Systems with Applications, 2016, 73 (1) :220-239." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFDEC6FD6ECC485AE16402459199FD5F1&amp;v=MDcxNDBXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0x5OHdhaz1OaWZPZmNYTWE2TEsyZnRERVpoOENIUTh2bU1TN0R0OVNudm5wUk04Y01UZ1FNeWVDT052RlNpVw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         GUO H X, LI Y J, JENNIFER S, et al.Learning from Class-Imba-lanced Data:Review of Methods and Applications.Expert Systems with Applications, 2016, 73 (1) :220-239.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_10" title=" ZHANG J P, MANI I.KNN Approach to Unbalanced Data Distributions:A Case Study Involving Information Extraction // Proc of the International Conference on Machine Learning.Palo Alto, USA:AAAI Press, 2003:42-48." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=KNN Approach to Unbalanced Data Distributions:A Case Study Involving Information Extraction">
                                        <b>[10]</b>
                                         ZHANG J P, MANI I.KNN Approach to Unbalanced Data Distributions:A Case Study Involving Information Extraction // Proc of the International Conference on Machine Learning.Palo Alto, USA:AAAI Press, 2003:42-48.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_11" title=" LIN W C, TSAI C F, HU Y H, et al.Clustering-Based Under-sampling in Class-Imbalanced Data.Information Sciences, 2017, 409/410:17-26." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESDFB713E40B45707321DAC87E21B62038&amp;v=MDU0Njk3U1dSYm1YQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0x5OHdhaz1OaWZPZmNmT2JOYk5yUHBCWkprTENYczV5QlVSNjBzTU8zZmwyUkEwQw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         LIN W C, TSAI C F, HU Y H, et al.Clustering-Based Under-sampling in Class-Imbalanced Data.Information Sciences, 2017, 409/410:17-26.
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_12" title=" KANG Q, SHI L, ZHOU M C, et al.A Distance-Based Weighted Undersampling Scheme for Support Vector Machines and Its Application to Imbalanced Classification.IEEE Transactions on Neural Networks and Learning Systems, 2018, 29 (9) :4152-4165." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Distance-Based Weighted Undersampling Scheme for Support Vector Machines and Its Application to Imbalanced Classification">
                                        <b>[12]</b>
                                         KANG Q, SHI L, ZHOU M C, et al.A Distance-Based Weighted Undersampling Scheme for Support Vector Machines and Its Application to Imbalanced Classification.IEEE Transactions on Neural Networks and Learning Systems, 2018, 29 (9) :4152-4165.
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_13" title=" JIAN C X, GAO J, AO Y H.A New Sampling Method for Classi-fying Imbalanced Data Based on Support Vector Machine Ensemble.Neurocomputing, 2016, 193:115-122." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A new sampling method for classifying imbalanced data based on support vector machine ensemble">
                                        <b>[13]</b>
                                         JIAN C X, GAO J, AO Y H.A New Sampling Method for Classi-fying Imbalanced Data Based on Support Vector Machine Ensemble.Neurocomputing, 2016, 193:115-122.
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_14" title=" 孙建涛, 郭崇慧, 陆玉昌, 等.多项式核支持向量机文本分类器泛化性能分析.计算机研究与发展, 2004, 41 (8) :1321-1326. (SUN J T, GUO C H, LU Y C, et al.Estimating the Generalization Performance of Polynomial SVM Classifier for Text Categorization.Journal of Computer Research and Development, 2004, 41 (8) :1321-1326.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ200408000&amp;v=MDE4ODM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emhVTDdKTHl2U2RMRzRIdFhNcDQ5RlpJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         孙建涛, 郭崇慧, 陆玉昌, 等.多项式核支持向量机文本分类器泛化性能分析.计算机研究与发展, 2004, 41 (8) :1321-1326. (SUN J T, GUO C H, LU Y C, et al.Estimating the Generalization Performance of Polynomial SVM Classifier for Text Categorization.Journal of Computer Research and Development, 2004, 41 (8) :1321-1326.) 
                                    </a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_15" title=" KANG S, CHO S.Approximating Support Vector Machine with Artificial Neural Network for Fast Prediction.Expert Systems with Applications, 2014, 41 (10) :4989-4995." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300111028&amp;v=MjkyNjlUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZvWGF4TT1OaWZPZmJLOEh0TE9ySTlGWmVvT0RINHhvQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         KANG S, CHO S.Approximating Support Vector Machine with Artificial Neural Network for Fast Prediction.Expert Systems with Applications, 2014, 41 (10) :4989-4995.
                                    </a>
                                </li>
                                <li id="244">


                                    <a id="bibliography_16" title=" 张学工.关于统计学习理论与支持向量机.自动化学报, 2000, 26 (1) :32-42. (ZHANG X G.Introduction to Statistical Learning Theory and Support Vector Machines.Acta Automatica Sinica, 2000, 26 (1) :32-42.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO200001005&amp;v=MDY2NTFuRnl6aFVMN0pLQ0xmWWJHNEh0SE1ybzlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         张学工.关于统计学习理论与支持向量机.自动化学报, 2000, 26 (1) :32-42. (ZHANG X G.Introduction to Statistical Learning Theory and Support Vector Machines.Acta Automatica Sinica, 2000, 26 (1) :32-42.) 
                                    </a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_17" title=" ANGIULLI F, FOLINO G.Distributed Nearest Neighbor-Based Condensation of Very Large Data Sets.IEEE Transactions on Knowledge and Data Engineering, 2007, 19 (12) :1593-1606." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributed Nearest Neighbor-Based Condensation of Very Large Data Sets">
                                        <b>[17]</b>
                                         ANGIULLI F, FOLINO G.Distributed Nearest Neighbor-Based Condensation of Very Large Data Sets.IEEE Transactions on Knowledge and Data Engineering, 2007, 19 (12) :1593-1606.
                                    </a>
                                </li>
                                <li id="248">


                                    <a id="bibliography_18" title=" LIN C T, HSIEH T Y, LIU Y T, et al.Minority Oversampling in Kernel Adaptive Subspaces for Class Imbalanced Datasets.IEEE Transactions on Knowledge and Data Engineering, 2017, 30 (5) :950-961." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Minority Oversampling in Kernel Adaptive Subspaces for Class Imbalanced Datasets">
                                        <b>[18]</b>
                                         LIN C T, HSIEH T Y, LIU Y T, et al.Minority Oversampling in Kernel Adaptive Subspaces for Class Imbalanced Datasets.IEEE Transactions on Knowledge and Data Engineering, 2017, 30 (5) :950-961.
                                    </a>
                                </li>
                                <li id="250">


                                    <a id="bibliography_19" title=" SU C T, CHEN L S, YI Y.Knowledge Acquisition through Information Granulation for Imbalanced Data.Expert Systems with Applications, 2006, 31 (3) :531-541." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501643865&amp;v=MDk5ODF5am1VTGJJSkZvWGF4TT1OaWZPZmJLN0h0RE5xbzlFWXU4TUJIbzhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         SU C T, CHEN L S, YI Y.Knowledge Acquisition through Information Granulation for Imbalanced Data.Expert Systems with Applications, 2006, 31 (3) :531-541.
                                    </a>
                                </li>
                                <li id="252">


                                    <a id="bibliography_20" title=" TANTITHAMTHAVORN C, MCINTOSH S, HASSAN A E, et al.An Empirical Comparison of Model Validation Techniques for Defect Prediction Models.IEEE Transactions on Software Enginee-ring, 2016, 43 (1) :1-18." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQDF145F30BABED1F5F68EAB11A02352110&amp;v=MzE0OTVNT243ajNSSTNlcmVXUkx1ZkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHdMeTh3YWs9TmozYWFzVzVHdFM2ckk4M0ZabDZlSDFQeW1BVjRrbw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         TANTITHAMTHAVORN C, MCINTOSH S, HASSAN A E, et al.An Empirical Comparison of Model Validation Techniques for Defect Prediction Models.IEEE Transactions on Software Enginee-ring, 2016, 43 (1) :1-18.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(06),569-576 DOI:10.16451/j.cnki.issn1003-6059.201906009            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于识别关键样本点的非平衡数据核SVM算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E5%A9%B7&amp;code=09165228&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭婷</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%9D%B0&amp;code=25633834&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王杰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%85%A8%E6%98%8E&amp;code=09165839&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘全明</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%A2%81%E5%90%89%E4%B8%9A&amp;code=08408575&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">梁吉业</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E8%A5%BF%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0176514&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山西大学计算机与信息技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E8%A5%BF%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%99%BA%E8%83%BD%E4%B8%8E%E4%B8%AD%E6%96%87%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E6%95%99%E8%82%B2%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山西大学计算智能与中文信息处理教育部重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>非平衡数据处理中常采用的欠采样方法很少考虑支持向量机 (SVM) 的特性, 并且在原始空间进行采样会导致多数类样本部分关键信息的丢失.针对上述问题, 文中提出基于识别关键样本点的非平衡数据核SVM算法.基于初始超平面有效划分多数类样本, 在高维空间中对每个分块进行核异类近邻抽样, 得到多数类中的关键样本点, 使用关键样本点和少数类样本训练最终核SVM分类器.在多个数据集上的实验证明文中算法的可行性和有效性, 特别是在非平衡度高于10∶1的数据集上, 文中算法优势明显.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9D%9E%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E9%9B%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">非平衡数据集;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%B8%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">核支持向量机;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%92%E5%88%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">划分;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AC%A0%E9%87%87%E6%A0%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">欠采样;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    郭婷, 硕士研究生, 主要研究方向为数据挖掘、机器学习.E-mail:876067312@qq.com.;
                                </span>
                                <span>
                                    王杰, 博士研究生, 主要研究方向为数据挖掘、机器学习.E-mail:812849431@qq.com.;
                                </span>
                                <span>
                                    刘全明, 博士, 副教授, 主要研究方向为云存储与云安全、网络行为分析、数据挖掘.E-mail:liuqm@sxu.edu.cn.;
                                </span>
                                <span>
                                    *梁吉业 (通讯作者) , 博士, 教授, 主要研究方向为粒计算、数据挖掘、机器学习.E-mail:ljy@sxu.edu.cn.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-05</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.61876103);</span>
                                <span>山西省重点研发计划重点项目 (No.201603D111014);</span>
                                <span>山西省1331工程项目资助;</span>
                    </p>
            </div>
                    <h1><b>Kernel SVM Algorithm Based on Identifying Key Samples for Imbalanced Data</b></h1>
                    <h2>
                    <span>GUO Ting</span>
                    <span>WANG Jie</span>
                    <span>LIU Quanming</span>
                    <span>LIANG Jiye</span>
            </h2>
                    <h2>
                    <span>School of Computer and Information Technology, Shanxi University</span>
                    <span>Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Under-sampling is often employed in imbalanced data processing. However, the characteristics of support vector machine (SVM) are seldom taken into account in the existing under-sampling methods, and the problem of losing some key information of the majority class is caused by the sampling in the original space. To solve these problems, a kernel SVM algorithm based on identifying key samples for imbalanced data (IK-KSVM) is proposed in this paper. Firstly, the majority class is divided effectively based on the initial hyperplane. Then, kernel heterogeneous nearest neighbor sampling is conducted on each partition to obtain the key samples of the majority class in the high-dimensional space. Finally, the final SVM classifier is trained by the key samples and the minority class samples. Experiments on several datasets show that IK-KSVM is feasible and effective and its advantages are evident while the imbalance degree of the dataset is higher than 10∶1.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Imbalanced%20Data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Imbalanced Data;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Kernel%20Support%20Vector%20Machine&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Kernel Support Vector Machine;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Partition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Partition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Under-Sampling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Under-Sampling;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    GUO Ting, master student. Her research interests include data mining and machine learning.;
                                </span>
                                <span>
                                    WANG Jie, Ph. D. candidate. His research interests include data mining and machine learning.;
                                </span>
                                <span>
                                    LIU Quanming, Ph. D., associate professor. His research interests include cloud storage and cloud security, network behavior analysis and data mining.;
                                </span>
                                <span>
                                    LIANG Jiye (Corresponding author) , Ph.D., professor. His research interests include granular computing, data mining and machine learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-05</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.61876103);</span>
                                <span>Key Projects of Key Research and Development Program of Shanxi Province (No.201603D111014);</span>
                                <span>1331 Engineering Project of Shanxi Province;</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="56">传统分类算法如决策树、支持向量机 (SVM) 、朴素贝叶斯在平衡数据上的性能良好.但是在网络入侵、商业欺诈、医疗数据分类等应用领域存在极度非平衡的数据<citation id="254" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.非平衡数据是指在同个数据集中某些类的样本数量远大于其它类的样本数量, 其中样本数少的类为少数类, 样本量多的类为多数类<citation id="255" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.由于样本数量存在较大差异, 传统分类算法得到的模型在进行预测时一般偏向于多数类, 多数类的分类正确率很高, 少数类的分类正确率很低, 但是实际应用领域中少数类通常包含数据集中的重要信息, 因此非平衡数据分类极其重要.</p>
                </div>
                <div class="p1">
                    <p id="57">现有不少文献研究致力于提高非平衡数据下的分类性能, 包括采样方法、代价敏感方法、基于集成的方法、修改算法的方法等.采样方法通常对多数类欠采样<citation id="256" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 对少数类过采样<citation id="257" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>, 或混合欠采样和过采样<citation id="258" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>.代价敏感方法<citation id="259" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>考虑到多数类和少数类样本误分的代价不同, 使用代价矩阵描述不同的误分代价.基于集成的方法<citation id="260" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>将集成思想运用到非平衡数据分类中, 提高非平衡数据的泛化性能.修改算法的方法<citation id="261" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>通过修改传统算法以适应非平衡数据特性, 削减传统算法对非平衡数据的敏感性.</p>
                </div>
                <div class="p1">
                    <p id="58">在统计非平衡研究中采样方法的研究居多, 达到29.6%<citation id="262" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 其中由于欠采样方法简单快速, 因此广泛使用在非平衡数据SVM处理中.欠采样方法以一定的策略从多数类样本中选取一个子集, 该子集样本数与少数类样本数达到平衡, 从而降低数据不平衡对分类算法的影响.抽样策略包括基于类边界的采样<citation id="263" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、基于聚类的采样<citation id="264" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和基于样本权重的采样<citation id="265" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等.Zhang等<citation id="266" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出NearMiss-2 (Near) 算法, 根据多数类样本中每个样本到少数类样本的3个最远距离的平均距离对多数类样本进行欠采样, NearMiss-2欠采样方法对边界点的选取不准确, 并未考虑非平衡数据潜在分布.Lin等<citation id="267" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出基于聚类的欠采样算法 (Under-sampling Based on Clus-tering, BCS) , 聚类多数类, 并利用这些聚类中心与少数类样本构成新的训练集.但BCS未考虑支持向量机的特性, 丢失很多多数类的边界样本.Shao等<citation id="268" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出基于不同训练点的双加权支持向量机算法 (Twin Weighted SVM, TWSVM) , 构建两个分类超平面, 引入欠采样和过采样, 使数据样本保持在接近信息中, 克服双支持向量机算法 (Twin SVM, TSVM) 在处理非平衡数据时的偏置问题, 但执行时间较长.Jian等<citation id="269" type="reference"><link href="238" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出不同贡献抽样算法 (Different Contri-bution Sampling Method, DCS) , 使用偏差支持向量机 (Bias-SVM, B-SVM) 识别支持向量 (Support Vectors, SVs) 和非支持向量 (Nonsupport Vectors, NSVs) , 对少数类SVs进行SMOTE (Synthetic Mino-rity Oversampling Technique) 过采样, 对多数类NSVs进行随机欠采样.算法在原始空间进行采样, 在高维空间进行支持向量机训练, 导致分布在分类决策边界周围样本丢失.Kang等<citation id="270" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出基于空间几何距离的支持向量机加权欠采样算法 (Weighted Under-Sampling SVM, WU-SVM) , 将多数类样本分成若干子区域, 并根据它们到超平面的欧氏距离分配不同权重.算法实现基于权重的欠采样, 但并未在高维空间中进行欠采样, 导致关键样本点丢失, 并且在处理大规模非平衡数据分类时运行时间较长.</p>
                </div>
                <div class="p1">
                    <p id="59">针对以往非平衡数据支持向量机中存在欠采样不准确、处理大规模数据运行时间较长等问题, 本文提出基于识别关键样本点的非平衡数据核SVM算法 (A Kernel SVM Algorithm Based on Identifying Key Samples, IK-KSVM) , 基于超平面的划分对多数类进行有效划分, 在高维空间中利用核异类近邻对多数类的每个划分进行采样, 抽取多数类中关键样本点与少数类样本量以达到平衡, 在提高少数类分类精度的同时提高分类器的整体性能.算法避免传统欠采样采样不准导致关键样本点丢失的问题, 对多数类样本分块处理可有效处理大规模非平衡数据分类问题.本文使用G-mean和曲线下面积 (Area under Curve, AUC) 指标共同量化算法的有效性, 在UCI数据集上的实验表明, 本文算法可对非平衡数据集进行有效分类, 整体分类性能良好, 在非平衡度高于10∶1的数据集上, IK-KSVM优势明显.</p>
                </div>
                <h3 id="60" name="60" class="anchor-tag">1 支持向量机</h3>
                <div class="p1">
                    <p id="61">支持向量机是定义在特征空间上间隔最大的线性分类器<citation id="271" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>.支持向量机的学习策略是将间隔最大化形式化为一个求解凸二次规划问题, 利用核技巧将低维不可分数据映射到高维空间, 使其线性可分, 从而解决非线性分类问题<citation id="272" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="62">已知样本<b><i>x</i></b>为<i>m</i>维向量, 训练集<i>T</i>中存在<i>n</i>个样本 (<b><i>x</i></b><sub>1</sub>, <i>y</i><sub>1</sub>) , (<b><i>x</i></b><sub>2</sub>, <i>y</i><sub>2</sub>) , …, (<b><i>x</i></b><sub><i>n</i></sub>, <i>y</i><sub><i>n</i></sub>) , <b><i>x</i></b><sub><i>i</i></sub>∈<b>R</b><sup><i>m</i></sup>, <i>y</i><sub><i>i</i></sub>∈{±1}, <i>i</i>=1, 2, …, <i>n</i>.SVM利用最大间隔寻找决策分类超平面<b><i>w</i></b><sup>T</sup><b><i>x</i></b>+<i>b</i>=0, 将<i>n</i>个样本分为两类, 分类决策函数</p>
                </div>
                <div class="p1">
                    <p id="63"><i>f</i> (<b><i>x</i></b>) =sign (<b><i>w</i></b><sup>T</sup><b><i>x</i></b>+<i>b</i>) , </p>
                </div>
                <div class="p1">
                    <p id="64">其中<b><i>w</i></b>⊆<b>R</b><sup><i>n</i></sup>为权向量, <i>b</i>⊆<b>R</b>为阈值, 构造并求解如下约束最优化问题:</p>
                </div>
                <div class="area_img" id="211">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201906011_21100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="67">其中:<i>ξ</i><sub><i>i</i></sub>为松弛变量, 反映样本错分的程度; <i>C</i>为惩罚函数, 控制对错分样本的惩罚程度.为了求解式 (1) 二次规划问题, 构造Lagrange函数<citation id="273" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi>b</mi><mo>, </mo><mi mathvariant="bold-italic">ξ</mi><mo>, </mo><mi>α</mi><mo>, </mo><mi>μ</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>C</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>ξ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn><mo>+</mo><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>μ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中<i>α</i><sub><i>i</i></sub>为Lagrange算子.对该Lagrange函数求极大极小, 可得原问题的对偶问题如下:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi mathvariant="bold-italic">α</mi></munder><mo stretchy="false"> (</mo><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>α</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>0</mn><mo>, </mo><mspace width="0.25em" /><mn>0</mn><mo>≤</mo><mi>α</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≤</mo><mi>C</mi><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">根据KKT条件, 得</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi mathvariant="bold-italic">x</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mi>α</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>, </mo></mtd></mtr><mtr><mtd><mi>b</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>α</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">将上式代入式 (1) , 求得判别函数</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>=</mo><mtext>s</mtext><mtext>i</mtext><mtext>g</mtext><mtext>n</mtext><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>α</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">对于核支持向量机, 判别函数为</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>=</mo><mtext>s</mtext><mtext>i</mtext><mtext>g</mtext><mtext>n</mtext><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>α</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mi>Κ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">其中<i>K</i> (<b><i>x</i></b>, <b><i>x</i></b><sub><i>i</i></sub>) 为核函数.</p>
                </div>
                <h3 id="78" name="78" class="anchor-tag">2 基于识别关键样本点的非平衡数据核SVM算法</h3>
                <h4 class="anchor-tag" id="79" name="79"><b>2.1</b> 问题分析</h4>
                <div class="p1">
                    <p id="80">对于基于欠采样的非平衡数据核支持向量机, 传统算法存在如下问题:</p>
                </div>
                <div class="p1">
                    <p id="81">1) SVM在处理非平衡数据过程中, 分类器会偏向多数类, 将少数类样本判断为多数类, 造成分类器整体性能不佳<citation id="274" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="82">2) 欠采样方法未考虑数据潜在分布, 导致分布在分类决策边界周围的样本丢失, 影响支持向量机的分类准确率.</p>
                </div>
                <div class="p1">
                    <p id="83">3) 欠采样方法一般在原始空间进行欠采样, 在高维空间进行SVM模型的训练, 可能导致低维空间抽取含有较多分类信息的样本在映射到高维空间后发生信息丢失的问题.</p>
                </div>
                <div class="p1">
                    <p id="84">4) 有些欠采样方法在面对大规模非平衡数据时执行效率较低.</p>
                </div>
                <div class="p1">
                    <p id="85">针对上述问题, 在欠采样过程中, 如何快速准确地抽取多数类中的关键样本点是本文研究的重点.</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86"><b>2.2</b> 识别关键样本点</h4>
                <div class="p1">
                    <p id="87">SVM在线性可分的情况下, 训练数据集中与分离超平面<i>f</i>距离最近的样本点为支持向量, 如图1所示, 到超平面的距离为<i>d</i>的样本为支持向量, 支持向量决定最优决策面的方向和位置<citation id="275" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>.</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906011_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 支持向量" src="Detail/GetImg?filename=images/MSSB201906011_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 支持向量  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906011_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Support vector</p>

                </div>
                <div class="p1">
                    <p id="89">根据支持向量的定义, 在非平衡支持向量机中靠近理想超平面的样本对分类起关键作用, 因此本文利用异类近邻对多数类进行欠采样, 通过构造两类之间的近邻关系, 计算多数类样本作为少数类样本异类近邻的次数, 以此度量多数类样本的关键性, 得到多数类中点的关键样本点<sup></sup><citation id="276" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>.假设训练集<b><i>D</i></b>中多数类</p>
                </div>
                <div class="p1">
                    <p id="90"><b><i>D</i></b><sub>maj</sub>={<b><i>p</i></b><sub>1</sub>, <b><i>p</i></b><sub>2</sub>, …, <b><i>p</i></b><sub><i>S</i><sub>maj</sub></sub>}, <i>i</i>=1, 2, …, <i>S</i><sub>maj</sub>, </p>
                </div>
                <div class="p1">
                    <p id="91">其中<i>S</i><sub>maj</sub>为多数类样本量, 少数类</p>
                </div>
                <div class="p1">
                    <p id="92"><b><i>D</i></b><sub>min</sub>={<b><i>q</i></b><sub>1</sub>, <b><i>q</i></b><sub>2</sub>, …, <b><i>q</i></b><sub><i>S</i><sub>min</sub></sub>}, <i>j</i>=1, 2, …, <i>S</i><sub>min</sub>, </p>
                </div>
                <div class="p1">
                    <p id="93">其中<i>S</i><sub>min</sub>为少数类样本量.本文异类近邻是指少数类样本在多数类中的近邻, 通过计算可得多数类样本是否为少数类样本的异类近邻:</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>, </mo></mtd><mtd columnalign="left"><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>, </mo></mtd><mtd columnalign="left"><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∉</mo><mi>Ν</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">其中, <i>N</i><sub><i>k</i></sub> (<i>j</i>) 为少数类中第<i>j</i>个样本<b><i>q</i></b><sub><i>j</i></sub>在多数类中的<i>k</i>近邻集合, <i>I</i><sub><i>ij</i></sub>为多数类中样本<b><i>p</i></b><sub><i>i</i></sub>是否为少数类样本<b><i>q</i></b><sub><i>j</i></sub>的异类近邻, 取值为0或1.</p>
                </div>
                <div class="p1">
                    <p id="96">多数类样本作为少数类样本异类近邻的次数越多, 表明该样本越靠近分类边界, 因此将近邻次数大于某个阈值的多数类样本作为关键点, 通过统计多数类样本<b><i>p</i></b><sub><i>i</i></sub>的<i>I</i><sub><i>ij</i></sub>定义关键样本点:</p>
                </div>
                <div class="p1">
                    <p id="97"><mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>n</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>S</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow></munderover><mi>Ι</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="99">其中, <i>n</i><sub><i>i</i></sub>为多数类样本<b><i>p</i></b><sub><i>i</i></sub>作为少数类样本异类近邻的次数, 当<i>n</i><sub><i>i</i></sub>≥<i>α</i><sub><i>k</i></sub>时, 称该样本为多数类中的关键样本点, <i>α</i><sub><i>k</i></sub>≥1, <i>α</i><sub><i>k</i></sub>的取值和近邻值<i>k</i>的取值有关.</p>
                </div>
                <div class="p1">
                    <p id="100">选择过程包括少数类的异类近邻计算和选择<i>n</i><sub><i>i</i></sub>≥<i>α</i><sub><i>k</i></sub>的多数类样本点.选择结果<i>R</i>初始化为空集∅, 计算少数类在多数类的近邻, 对每个样本<b><i>q</i></b><sub><i>j</i></sub>, 计算该样本在多数类中的<i>k</i>近邻, 统计每个多数类样本作为少数类近邻的次数, 即当次数满足条件时, 该样本为关键样本点.</p>
                </div>
                <div class="p1">
                    <p id="101">如图2所示, 取<i>k</i>=2, 求少数类在多数类中的2近邻, <b><i>p</i></b><sub>1</sub>∶1为<b><i>p</i></b><sub>1</sub>样本作为少数类近邻的次数<i>n</i><sub>1</sub>=1, 经过近邻计算及统计多数类样本的<i>n</i><sub><i>i</i></sub>, 选择<i>α</i><sub><i>k</i></sub>=1, 即<i>n</i><sub><i>i</i></sub>≥1的样本点, 即<b><i>p</i></b><sub>1</sub>、 <b><i>p</i></b><sub>4</sub>、 <b><i>p</i></b><sub>6</sub>、 <b><i>p</i></b><sub>8</sub>、 <b><i>p</i></b><sub>10</sub>、 <b><i>p</i></b><sub>12</sub>, 这些样本点为图2中多数类的关键样本点.由图中多数类样本的分布可知, 选择的关键样本靠近分类边界, 在分类中起关键作用.</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906011_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 选择多数类中的关键样本点" src="Detail/GetImg?filename=images/MSSB201906011_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 选择多数类中的关键样本点  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906011_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Selecting key samples in majority class</p>

                </div>
                <div class="p1">
                    <p id="103">由于以往的方法是在原始空间进行欠采样, 在高维空间进行SVM模型训练, 导致低维空间抽取的含有较多分类信息的样本在映射到高维空间后发生信息丢失的问题, 因此本文在高维空间利用核异类近邻采样得到多数类中的关键样本点, 使采样后的数据能较好地反映原始数据在高维空间的特征分布.在核异类近邻中计算任意两点<b><i>p</i></b><sub><i>i</i></sub>与<b><i>q</i></b><sub><i>j</i></sub>的距离:</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>d</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">q</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mroot><mrow><mrow><mrow><mo>|</mo><mrow><mi>φ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>φ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">q</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mtext> </mtext></mroot><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mroot><mrow><mi>Κ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>2</mn><mi>Κ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">q</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>Κ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">q</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi mathvariant="bold-italic">q</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mtext> </mtext></mroot><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105">在利用核异类近邻进行采样时, 可能丢失多数类中的关键样本点.如图3所示, 利用核异类近邻采样可能只得到多数类中形状为空心正方形的样本, 丢失其余位置的关键样本点, 使得到的分类超平面与理想超平面相差较大, 导致分类性能不佳.此外, 在整个训练集中计算近邻, 导致计算复杂度较高.</p>
                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906011_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 异类近邻抽样方法可能造成的模型误差" src="Detail/GetImg?filename=images/MSSB201906011_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 异类近邻抽样方法可能造成的模型误差  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906011_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Model error caused by heterogeneous nearest neighbor sampling method</p>

                </div>
                <div class="p1">
                    <p id="108">本文提出将多数类进行划分处理, 如图4所示, 将多数类划分为若干独立的区域, 利用少数类对每个分块进行核异类近邻抽样, 缩小搜索范围, 提高搜索效率.</p>
                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906011_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 基于超平面的划分方法" src="Detail/GetImg?filename=images/MSSB201906011_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 基于超平面的划分方法  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906011_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Partition method based on hyperplane</p>

                </div>
                <h4 class="anchor-tag" id="110" name="110"><b>2.3</b> 基于超平面的划分</h4>
                <div class="p1">
                    <p id="111">将原始多数类划分为若干区域, 这些分块区域需满足如下条件:</p>
                </div>
                <div class="p1">
                    <p id="112">1) 在对原始多数类进行划分时, 保证划分后每个分块均包含原始分类决策边界的样本.</p>
                </div>
                <div class="p1">
                    <p id="113">2) 每个分块的样本量应与少数类样本量相差不大.</p>
                </div>
                <div class="p1">
                    <p id="114">为了满足上述条件, 在非平衡支持向量机中基于初始超平面划分多数类, 划分块个数<mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><mo>=</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>a</mtext><mtext>j</mtext></mrow></msub></mrow><mrow><mi>S</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow></mfrac></mrow></math></mathml>.在划分过程中, 应保持每个分块样本量和少数类样本量相差不大.将多数类划分为<i>l</i>个互不相交的区域<i>R</i><sub><i>j</i></sub>, <i>j</i>=1, 2, …, <i>l</i>, 需要利用空间中<i>l</i>-1个平行的超平面划分.</p>
                </div>
                <div class="p1">
                    <p id="116">基于超平面的划分首先将原始训练集映射到高维空间, 进行核支持向量机的训练, 得到初始超平面.然后利用该超平面的法向量作为多数类的划分方向, 将初始得到的超平面的法向量<b><i>w</i></b><sub>1</sub>作为<i>l</i>-1个平面的方向.采用初始超平面的原因是考虑数据的潜在分布, 对数据进行有效划分, 避免出现图3中采样不足的现象.为了保证每个分块的样本个数差异不太大, 将所有的多数类样本点投影至超平面, 按取值大小排序, 并等量划分.得到<i>l</i>-1个平面</p>
                </div>
                <div class="p1">
                    <p id="117"><i>y</i>=<b><i>w</i></b><sup>T</sup><sub>1</sub><b><i>x</i></b><sub><i>i</i></sub>+<i>b</i><sub><i>j</i></sub>, <i>j</i>=1, 2, …, <i>l</i>-1.</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118"><b>2.4</b> 算法步骤</h4>
                <div class="p1">
                    <p id="119">如图5所示, IK-KSVM根据得到的初始超平面划分多数类, 每个分块的样本与少数类样本量保持相近, 利用核异类近邻方法对每一分块进行欠采样.</p>
                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906011_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 IK-KSVM图示" src="Detail/GetImg?filename=images/MSSB201906011_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 IK-KSVM图示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906011_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Illustration of IK-KSVM algorithm</p>

                </div>
                <div class="p1">
                    <p id="121">具体IK-KSVM步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="122"><b>算法</b> IK-KSVM</p>
                </div>
                <div class="p1">
                    <p id="123"><b>输入</b> 多数类样本, 样本量<i>S</i><sub>maj</sub>, </p>
                </div>
                <div class="p1">
                    <p id="124">少数类样本, 样本量<i>S</i><sub>min</sub>, </p>
                </div>
                <div class="p1">
                    <p id="125">惩罚因子<i>C</i></p>
                </div>
                <div class="p1">
                    <p id="126"><b>输出</b> 最终的分类超平面<i>f</i></p>
                </div>
                <div class="p1">
                    <p id="127">step 1 将原始训练集映射到高维空间, 进行核支持向量机训练, 得到初始的超平面</p>
                </div>
                <div class="p1">
                    <p id="128"><i>f</i><sub>1</sub>=<b><i>w</i></b><sup>T</sup><sub>1</sub><b><i>x</i></b>+<i>b</i><sub>1</sub>.</p>
                </div>
                <div class="p1">
                    <p id="129">step 2 基于初始超平面划分方法, 将多数类样本投影至超平面, 按照取值大小排序, 划分块个数<mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><mo>=</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>a</mtext><mtext>j</mtext></mrow></msub></mrow><mrow><mi>S</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow></mfrac><mo>.</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="131">step 3 求取少数类样本在每个分块的核异类近邻.</p>
                </div>
                <div class="p1">
                    <p id="132">step 4 统计每一分块的样本作为少数类样本近邻的次数<i>n</i><sub><i>i</i></sub>, 当<i>n</i><sub><i>i</i></sub>≥<i>α</i><sub><i>k</i></sub>时, 选择该样本作为多数类中的关键样本点.</p>
                </div>
                <div class="p1">
                    <p id="133">step 5 针对少数类样本和多数类中的关键样本点进行核支持向量机的训练, 得到最终分类超平面</p>
                </div>
                <div class="p1">
                    <p id="134"><i>f</i>=<b><i>w</i></b><sup>T</sup><b><i>x</i></b>+<i>b</i>.</p>
                </div>
                <h4 class="anchor-tag" id="135" name="135"><b>2.5</b> 时间复杂度分析</h4>
                <div class="p1">
                    <p id="136">IK-KSVM主要包括划分与采样.在划分阶段, 需要对多数类样本在超平面的投影值进行排序, 划分算法的时间复杂度为<i>O</i> (<i>S</i><sub>maj</sub>log<sub>2</sub><i>S</i><sub>maj</sub>) .将多数类划分为若干块, 每块和少数类样本量近似, 时间复杂度为<i>O</i> (<i>mS</i><mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>min</mi></mrow><mn>2</mn></msubsup></mrow></math></mathml>) , 其中<i>m</i>为训练集样本的属性.抽样复杂度取决于少数类的样本量.</p>
                </div>
                <h3 id="138" name="138" class="anchor-tag">3 实验及结果分析</h3>
                <div class="p1">
                    <p id="139">本文从UCI数据集中选择8个非平衡数据集, 多数类和少数类样本量比均高于5∶1, 其中4个数据集的样本量比低于10∶1, 另外4个数据集的样本量比高于10∶1.数据集详细信息如表1所示, 非平衡度为多数类样本量和少数类样本量之比, 按非平衡度从低到高排列数据集.</p>
                </div>
                <div class="area_img" id="140">
                    <p class="img_tit"><b>表1 数据集的基本信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Basic information of datasets</p>
                    <p class="img_note"></p>
                    <table id="140" border="1"><tr><td>名称</td><td>大小</td><td>维度</td><td>多数类<br />样本量</td><td>少数类<br />样本量</td><td>非平衡度</td></tr><tr><td><br />yeast3</td><td>1484</td><td>9</td><td>1321</td><td>163</td><td>8.1∶1</td></tr><tr><td><br />ijcnn1</td><td>49990</td><td>23</td><td>45137</td><td>4853</td><td>9.3∶1</td></tr><tr><td><br />vowel0</td><td>988</td><td>14</td><td>898</td><td>90</td><td>9.89∶1</td></tr><tr><td><br />led7digit</td><td>443</td><td>8</td><td>406</td><td>40</td><td>10∶1</td></tr><tr><td><br />glass4</td><td>214</td><td>10</td><td>201</td><td>13</td><td>15.5∶1</td></tr><tr><td><br />w6a</td><td>17188</td><td>301</td><td>16663</td><td>525</td><td>31∶1</td></tr><tr><td><br />yeast6</td><td>1484</td><td>9</td><td>1449</td><td>35</td><td>41.4∶1</td></tr><tr><td><br />shuttle</td><td>3316</td><td>10</td><td>2613</td><td>39</td><td>67∶1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="141">在机器学习中一般将准确率 (Accuracy) 作为评价指标, 但非平衡数据分类器会偏向多数类, 导致多数类样本大部分分类正确, 准确率较高, 但这并不代表分类器的性能较好<citation id="277" type="reference"><link href="248" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>.因此, 本文考虑分类器整体性能, 使用G-mean和AUC作为评价指标, 评价指标建立在混淆矩阵上, 混淆矩阵如表2所示.</p>
                </div>
                <div class="area_img" id="142">
                    <p class="img_tit"><b>表2 两类问题的混淆矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Confusion matrix of two classes of problems</p>
                    <p class="img_note"></p>
                    <table id="142" border="1"><tr><td><br /></td><td>实际正类</td><td>实际负类</td></tr><tr><td><br />分为正类</td><td><i>TP</i></td><td><i>FP</i></td></tr><tr><td><br />分为负类</td><td><i>FN</i></td><td><i>TN</i></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="143">非平衡数据集常采用G-mean值作为衡量指标, G-mean值表示少数类分类精度和多数类分类精度的几何平均值<citation id="278" type="reference"><link href="250" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, 可有效衡量非平衡数据的分类精度.一般G-mean值越大分类效果越好, 定义如下:</p>
                </div>
                <div class="p1">
                    <p id="144"><mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mo>-</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>=</mo><mroot><mrow><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mi>Τ</mi><mi>Ν</mi></mrow><mrow><mi>Τ</mi><mi>Ν</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac></mrow><mtext> </mtext></mroot><mspace width="0.25em" /></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="146">接收者操作特征 (Receiver Operating Characte-ristic, ROC) 曲线能较全面描述分类器在不同判决阈值时的性能, 是目前评价非平衡数据集分类性能的通用方法<citation id="279" type="reference"><link href="250" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>.ROC曲线的横坐标为伪阳性率 (False Positive Rate, FPR) , 在实际应用中为多数类样本中预测错误的比率, 纵坐标是真阳性率 (True Positive Rate, TPR) , 在实际应用中为少数类样本中预测正确的比率.AUC值为ROC曲线覆盖的区域面积, 显然AUC越大, 分类器分类效果越好.FPR和TPR计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="147"><mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mi>Ρ</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>Ρ</mi></mrow><mrow><mi>F</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><mi>Τ</mi><mi>Ρ</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="149">为了验证本文算法的有效性, 使用五折交叉验证法进行实验<citation id="280" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>.选择随机欠采样 (Random Under Sampling SVM, RUS) <citation id="281" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、Near算法<citation id="282" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、BCS<sup></sup><citation id="283" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、DCS<citation id="284" type="reference"><link href="238" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和WU-SVM<sup></sup><citation id="285" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>作为对比算法.</p>
                </div>
                <div class="p1">
                    <p id="150">图6为6种算法在8个数据集上的G-mean值对比.G-mean考虑两个类别的性能, 是衡量数据集整体分类性能的指标.IK-KSVM在8个数据集上的G-mean值均高于其它算法, 都达到较好效果.</p>
                </div>
                <div class="p1">
                    <p id="151">图7为6种算法在8个数据集上的AUC值对比, AUC衡量算法的整体分类性能, IK-KSVM的AUC高于其余欠采样算法.对于非平衡度低于10∶1的数据集, IK-KSVM的AUC值略高于其余算法, 对于ijcnn数据集, IK-KSVM的AUC值略低, 而相对其它数据集, IK-KSVM的AUC值更大.</p>
                </div>
                <div class="area_img" id="212">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906011_21200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 6种算法在8个数据集上的G-mean值对比" src="Detail/GetImg?filename=images/MSSB201906011_21200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 6种算法在8个数据集上的G-mean值对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906011_21200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 G-mean comparison of 6 algorithms on 8 datasets</p>

                </div>
                <div class="area_img" id="212">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906011_21201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 6种算法在8个数据集上的G-mean值对比" src="Detail/GetImg?filename=images/MSSB201906011_21201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 6种算法在8个数据集上的G-mean值对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906011_21201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 G-mean comparison of 6 algorithms on 8 datasets</p>

                </div>
                <div class="area_img" id="213">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906011_21300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 6种算法在8个数据集上的AUC对比" src="Detail/GetImg?filename=images/MSSB201906011_21300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 6种算法在8个数据集上的AUC对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906011_21300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 AUC comparison of 6 algorithms on 8 datasets</p>

                </div>
                <div class="p1">
                    <p id="160">由图6和图7可见, IK-KSVM的整体分类性能良好, 多数类和少数类都能达到较高的分类精度.</p>
                </div>
                <div class="p1">
                    <p id="161">为了探讨IK-KSVM在不同非平衡度下的性能差异, 在每个数据集上对比IK-KSVM与其余算法中最佳性能.</p>
                </div>
                <div class="p1">
                    <p id="162">表3为不同非平衡度下IK-KSVM的G-mean差异, 提升值为IK-KSVM的G-mean与其余对比算法中G-mean结果最好的差值.对比4个数据集的平均提升值, IK-KSVM在非平衡度高于10∶1的数据集的平均提升值大于非平衡度低于10∶1的数据集.由此可见, 对于非平衡度高于10∶1的数据集, IK-KSVM的G-mean值明显大于其余对比算法.</p>
                </div>
                <div class="p1">
                    <p id="163">表4为不同非平衡度下IK-KSVM的AUC差异, 提升值为IK-KSVM与其余对比算法中AUC最好结果的差值, 同样对比4个数据集的平均提升值.IK-KSVM在非平衡度高于10∶1的数据集的平均提升值高于非平衡度低于10∶1的数据集, 因此得出对于非平衡度高于10∶1的数据集, IK-KSVM的AUC值明显大于其余对比算法.</p>
                </div>
                <div class="area_img" id="164">
                    <p class="img_tit"><b>表3 不同非平衡度下IK-KSVM的G-mean差异</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 G-mean difference of IK-KSVM algorithm under different imbalanced ratios</p>
                    <p class="img_note"></p>
                    <table id="164" border="1"><tr><td rowspan="2"><br />数据集</td><td colspan="4"><br />非平衡度低于10∶1</td><td colspan="4"><br />非平衡度高于10∶1</td></tr><tr><td><br />yeast3</td><td>ijcnn1</td><td>vowel0</td><td>led7digit</td><td><br />glass4</td><td>w6a</td><td>yeast6</td><td>shuttle</td></tr><tr><td>提升值</td><td>0.01</td><td>0.0048</td><td>0.0180</td><td>0.0225</td><td>0.0399</td><td>0.0278</td><td>0.0384</td><td>0.1111</td></tr><tr><td><br />平均值</td><td colspan="4">0.0138</td><td colspan="4"><b>0.0543</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="165">
                    <p class="img_tit"><b>表4 不同非平衡度下IK-KSVM的AUC差异</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 AUC difference of IK-KSVM algorithm under different imbalanced ratios</p>
                    <p class="img_note"></p>
                    <table id="165" border="1"><tr><td rowspan="2"><br />数据集</td><td colspan="4"><br />非平衡度低于10∶1</td><td colspan="4"><br />非平衡度高于10∶1</td></tr><tr><td><br />yeast3</td><td>ijcnn1</td><td>vowel0</td><td>led7digit</td><td><br />glass4</td><td>w6a</td><td>yeast6</td><td>shuttle</td></tr><tr><td>提升值</td><td>0.0082</td><td>-0.0143</td><td>0.0142</td><td>0.02</td><td>0.0431</td><td>0.0731</td><td>0.0474</td><td>0.0766</td></tr><tr><td><br />平均值</td><td colspan="4">0.0070</td><td colspan="4"><b>0.0600</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="166">本文算法在处理大规模数据时使用分块采样, 运行时间能与之前的一些算法保持相近或优于某些算法.表5为不同算法在w6a、ijcnn1数据集上的运行时间.IK-KSVM的运行时间与RUS、BCS和Near保持一致, 但RUS、BCS和Near的分类性能不及IK-KSVM.相比WU-SVM, IK-KSVM能加速3～4倍.相比DCS, IK-KSVM能加速2～3倍.相比之前算法, IK-KSVM在ijcnn1、w6a数据集上的运行时间有一定提高, 因此适用于大规模非平衡数据的分类问题.</p>
                </div>
                <div class="area_img" id="167">
                    <p class="img_tit"><b>表5 6种算法在大规模数据集上的运行时间</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Running time of 6 different algorithms on large scale datasets s</p>
                    <p class="img_note"></p>
                    <table id="167" border="1"><tr><td>数据集</td><td>IK-KSVM</td><td>WU-SVM</td><td>DCS</td><td>RUS</td><td>BCS</td><td>Near</td></tr><tr><td><br />ijcnn1</td><td>75.8</td><td>225.6</td><td>139.1</td><td>49.7</td><td>78.8</td><td>96.8</td></tr><tr><td><br />w6a</td><td>29.5</td><td>97.2</td><td>73.2</td><td>25.0</td><td>76.8</td><td>24.6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="168">综上所述, 无论是对于非平衡度低于10∶1的数据集还是非平衡度高于10∶1的数据集, IK-KSVM的G-mean值和AUC值都达到较好的效果, 整体分类性能良好.特别地, 对于非平衡度高于10∶1的数据集, IK-KSVM能将多数类划分为更多的分块, 更好地对多数类进行欠采样, 相比其它欠采样算法, 优势明显.</p>
                </div>
                <h3 id="209" name="209" class="anchor-tag">4结束语</h3>
                <div class="p1">
                    <p id="210">本文提出基于识别关键样本点的非平衡数据核SVM算法 (IK-KSVM) , 利用基于超平面的划分方法划分多数类样本, 在高维空间中对每一分块进行核异类近邻采样.IK-KSVM既考虑支持向量机本身的特性又考虑潜在的数据分布, 实现对多数类的划分抽样, 有效缓解非平衡数据对超平面造成的偏移, 而分块采样使算法可有效处理大规模非平衡数据的分类问题.实验表明, 本文算法整体分类性能良好, 特别是在处理非平衡度高于10∶1的数据集时, 优势明显.目前, 半监督学习是机器学习的热点之一, 如何解决在半监督学习中非平衡数据带来的问题是今后工作的下一个目标.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="205" type="formula" href="images/MSSB201906011_20500.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">郭婷</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="206" type="formula" href="images/MSSB201906011_20600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">王杰</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="207" type="formula" href="images/MSSB201906011_20700.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">刘全明</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="208" type="formula" href="images/MSSB201906011_20800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">梁吉业</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="214">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning from Imbalanced Data">

                                <b>[1]</b> HE H B, GARCIA E A.Learning from Imbalanced Data.IEEE Transactions on Knowledge and Data Engineering, 2009, 21 (9) :1263-1284.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Resampling-based ensemble methods for online class imbalance learning">

                                <b>[2]</b> WANG S, MINKU L L, YAO X.Resampling-Based Ensemble Methods for Online Class Imbalance Learning.IEEE Transactions on Knowledge and Data Engineering, 2015, 27 (5) :1356-1368.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600737766&amp;v=MTQ1NjhZK2dJQzNvL29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGb1hheE09TmlmT2ZiSzdIdEROcVk5Rg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> TAHIR M A, KITTLER J, YAN F.Inverse Random under Sampling for Class Imbalance Problem and Its Application to Multi-label Classification.Pattern Recognition, 2012, 45 (10) :3738-3750.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SMOTE: synthetic minority over-sampling technique">

                                <b>[4]</b> CHAWLA N V, BOWYER K, HALL L O, et al.SMOTE:Synthe-tic Minority Over-Sampling Technique.Journal of Artificial Intelligence Research, 2011, 16:321-357.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061600062017&amp;v=MDQwMzAwTkRIMCtvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRm9YYXhNPU5pZk9mYks4SHRmTnFZOUZaTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> SHAO Y H, CHEN W J, ZHANG J J, et al.An Efficient Weighted Lagrangian Twin Support Vector Machine for Imbalanced Data Cla-ssification.Pattern Recognition, 2014, 47 (9) :3158-3167.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Applying Support Vector Machines to Imbalanced Datasets">

                                <b>[6]</b> AKBAIN R, KWEK S, JAPKOWICZ N.Applying Support Vector Machines to Imbalanced Data Sets // Proc of the European Confe-rence on Machine Learning.Berlin, Germany:Springer, 2004:39-50.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003722389&amp;v=MjQxOTE9Tmo3QmFyTzRIdEhQcUkxSForTUdZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDSGxWN3ZMSVY4&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> WANG B X, JAPKOWICZ N.Boosting Support Vector Machines for Imbalanced Data Sets.Knowledge and Information Systems, 2010, 25 (1) :1-20
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14121200017863&amp;v=MTA4NDhyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRm9YYXhNPU5pZk9mYks4SDlQTnJZOUZaT29JQkhvNm9CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> SUN Z B, SONG Q B, ZHU X Y, et al.A Novel Ensemble Method for Classifying Imbalanced Data.Pattern Recognition, 2015, 48 (5) :1623-1637.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFDEC6FD6ECC485AE16402459199FD5F1&amp;v=MTg1NTlIWWZPR1FsZkNwYlEzNU5GaHdMeTh3YWs9TmlmT2ZjWE1hNkxLMmZ0REVaaDhDSFE4dm1NUzdEdDlTbnZucFJNOGNNVGdRTXllQ09OdkZTaVdXcjdKSUZwbWFCdQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> GUO H X, LI Y J, JENNIFER S, et al.Learning from Class-Imba-lanced Data:Review of Methods and Applications.Expert Systems with Applications, 2016, 73 (1) :220-239.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=KNN Approach to Unbalanced Data Distributions:A Case Study Involving Information Extraction">

                                <b>[10]</b> ZHANG J P, MANI I.KNN Approach to Unbalanced Data Distributions:A Case Study Involving Information Extraction // Proc of the International Conference on Machine Learning.Palo Alto, USA:AAAI Press, 2003:42-48.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESDFB713E40B45707321DAC87E21B62038&amp;v=MTA4MzB1SFlmT0dRbGZDcGJRMzVORmh3THk4d2FrPU5pZk9mY2ZPYk5iTnJQcEJaSmtMQ1hzNXlCVVI2MHNNTzNmbDJSQTBDN1NXUmJtWENPTnZGU2lXV3I3SklGcG1hQg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> LIN W C, TSAI C F, HU Y H, et al.Clustering-Based Under-sampling in Class-Imbalanced Data.Information Sciences, 2017, 409/410:17-26.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Distance-Based Weighted Undersampling Scheme for Support Vector Machines and Its Application to Imbalanced Classification">

                                <b>[12]</b> KANG Q, SHI L, ZHOU M C, et al.A Distance-Based Weighted Undersampling Scheme for Support Vector Machines and Its Application to Imbalanced Classification.IEEE Transactions on Neural Networks and Learning Systems, 2018, 29 (9) :4152-4165.
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A new sampling method for classifying imbalanced data based on support vector machine ensemble">

                                <b>[13]</b> JIAN C X, GAO J, AO Y H.A New Sampling Method for Classi-fying Imbalanced Data Based on Support Vector Machine Ensemble.Neurocomputing, 2016, 193:115-122.
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ200408000&amp;v=MTk0NzJPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emhVTDdKTHl2U2RMRzRIdFhNcDQ5RlpJUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 孙建涛, 郭崇慧, 陆玉昌, 等.多项式核支持向量机文本分类器泛化性能分析.计算机研究与发展, 2004, 41 (8) :1321-1326. (SUN J T, GUO C H, LU Y C, et al.Estimating the Generalization Performance of Polynomial SVM Classifier for Text Categorization.Journal of Computer Research and Development, 2004, 41 (8) :1321-1326.) 
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300111028&amp;v=MjQxMDlSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGb1hheE09TmlmT2ZiSzhIdExPckk5Rlplb09ESDR4b0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> KANG S, CHO S.Approximating Support Vector Machine with Artificial Neural Network for Fast Prediction.Expert Systems with Applications, 2014, 41 (10) :4989-4995.
                            </a>
                        </p>
                        <p id="244">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO200001005&amp;v=MDY4NzMzenFxQnRHRnJDVVJMT2VaZVJuRnl6aFVMN0pLQ0xmWWJHNEh0SE1ybzlGWVlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 张学工.关于统计学习理论与支持向量机.自动化学报, 2000, 26 (1) :32-42. (ZHANG X G.Introduction to Statistical Learning Theory and Support Vector Machines.Acta Automatica Sinica, 2000, 26 (1) :32-42.) 
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributed Nearest Neighbor-Based Condensation of Very Large Data Sets">

                                <b>[17]</b> ANGIULLI F, FOLINO G.Distributed Nearest Neighbor-Based Condensation of Very Large Data Sets.IEEE Transactions on Knowledge and Data Engineering, 2007, 19 (12) :1593-1606.
                            </a>
                        </p>
                        <p id="248">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Minority Oversampling in Kernel Adaptive Subspaces for Class Imbalanced Datasets">

                                <b>[18]</b> LIN C T, HSIEH T Y, LIU Y T, et al.Minority Oversampling in Kernel Adaptive Subspaces for Class Imbalanced Datasets.IEEE Transactions on Knowledge and Data Engineering, 2017, 30 (5) :950-961.
                            </a>
                        </p>
                        <p id="250">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501643865&amp;v=MTY0NDZiSUpGb1hheE09TmlmT2ZiSzdIdEROcW85RVl1OE1CSG84b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> SU C T, CHEN L S, YI Y.Knowledge Acquisition through Information Granulation for Imbalanced Data.Expert Systems with Applications, 2006, 31 (3) :531-541.
                            </a>
                        </p>
                        <p id="252">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQDF145F30BABED1F5F68EAB11A02352110&amp;v=MjQ3MDlTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0x5OHdhaz1OajNhYXNXNUd0UzZySTgzRlpsNmVIMVB5bUFWNGtvTU9uN2ozUkkzZXJlV1JMdWZDT052Rg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> TANTITHAMTHAVORN C, MCINTOSH S, HASSAN A E, et al.An Empirical Comparison of Model Validation Techniques for Defect Prediction Models.IEEE Transactions on Software Enginee-ring, 2016, 43 (1) :1-18.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201906011" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201906011&amp;v=MjAxNjFEN1liTEc0SDlqTXFZOUVaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpoVUw3Sks=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
