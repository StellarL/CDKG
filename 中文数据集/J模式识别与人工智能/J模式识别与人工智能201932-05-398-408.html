<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131448508780000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201905002%26RESULT%3d1%26SIGN%3dGZ0S9elsgUsWfAZYxH0qdy6GKv8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201905002&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201905002&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201905002&amp;v=MDM5MzdCdEdGckNVUkxPZVplUm5GeXpnVmIzSktEN1liTEc0SDlqTXFvOUZab1FLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#65" data-title="1 相位拉伸变换 ">1 相位拉伸变换</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#84" data-title="2 基于PST相位差权重的多光谱和全色稀疏融合算法 ">2 基于PST相位差权重的多光谱和全色稀疏融合算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#85" data-title="&lt;b&gt;2.1 遥感影像训练集对字典影响分析&lt;/b&gt;"><b>2.1 遥感影像训练集对字典影响分析</b></a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;2.2 PST&lt;/b&gt;&lt;b&gt;对遥感影像边缘纹理提取的有效性分析&lt;/b&gt;"><b>2.2 PST</b><b>对遥感影像边缘纹理提取的有效性分析</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;2.3 算法步骤&lt;/b&gt;"><b>2.3 算法步骤</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#148" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#155" data-title="&lt;b&gt;3.1 主观评价&lt;/b&gt;"><b>3.1 主观评价</b></a></li>
                                                <li><a href="#185" data-title="&lt;b&gt;3.2 客观评价&lt;/b&gt;"><b>3.2 客观评价</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#196" data-title="4 结 束 语 ">4 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#100" data-title="图1 2种不同训练集训练获得的字典">图1 2种不同训练集训练获得的字典</a></li>
                                                <li><a href="#112" data-title="图2 &lt;i&gt;W&lt;/i&gt;不同时3幅遥感影像的检测结果">图2 <i>W</i>不同时3幅遥感影像的检测结果</a></li>
                                                <li><a href="#112" data-title="图2 &lt;i&gt;W&lt;/i&gt;不同时3幅遥感影像的检测结果">图2 <i>W</i>不同时3幅遥感影像的检测结果</a></li>
                                                <li><a href="#118" data-title="图3 &lt;i&gt;W&lt;/i&gt;不同时3幅遥感影像的相位梯度统计结果">图3 <i>W</i>不同时3幅遥感影像的相位梯度统计结果</a></li>
                                                <li><a href="#118" data-title="图3 &lt;i&gt;W&lt;/i&gt;不同时3幅遥感影像的相位梯度统计结果">图3 <i>W</i>不同时3幅遥感影像的相位梯度统计结果</a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;表1 &lt;i&gt;W&lt;/i&gt;不同时图像的ENL评价统计结果&lt;/b&gt;"><b>表1 <i>W</i>不同时图像的ENL评价统计结果</b></a></li>
                                                <li><a href="#147" data-title="图4 本文算法的图像融合流程图">图4 本文算法的图像融合流程图</a></li>
                                                <li><a href="#152" data-title="图5 5组测试的MS和PAN遥感影像">图5 5组测试的MS和PAN遥感影像</a></li>
                                                <li><a href="#152" data-title="图5 5组测试的MS和PAN遥感影像">图5 5组测试的MS和PAN遥感影像</a></li>
                                                <li><a href="#169" data-title="图6 7种算法的融合影像对比">图6 7种算法的融合影像对比</a></li>
                                                <li><a href="#169" data-title="图6 7种算法的融合影像对比">图6 7种算法的融合影像对比</a></li>
                                                <li><a href="#182" data-title="图7 图6中部分测试图像融合结果区域放大对比">图7 图6中部分测试图像融合结果区域放大对比</a></li>
                                                <li><a href="#182" data-title="图7 图6中部分测试图像融合结果区域放大对比">图7 图6中部分测试图像融合结果区域放大对比</a></li>
                                                <li><a href="#187" data-title="&lt;b&gt;表2 7种算法融合结果定量分析对比&lt;/b&gt;"><b>表2 7种算法融合结果定量分析对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                     POHL C, CAN GENDEREN L.Remote Sensing Image Fusion:A Practical Guide.Boca Raton, USA:CRC Press, 2017.</a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" >
                                        <b>[2]</b>
                                     GHASSEMIAN H.A Review of Remote Sensing Image Fusion Me-thods.Information Fusion, 2016, 32:75-89.</a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" MENG X C, SHEN H F, LI H F, &lt;i&gt;et al&lt;/i&gt;.Review of the Pansharpe-ning Methods for Remote Sensing Images Based on the Idea of Meta-Analysis:Practical Discussion and Challenges.Information Fusion, 2019, 46:102-113." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESA3CE593C015D22CB894DB1FB826ED21C&amp;v=MDIyMzViNHpzSk9uNlUzaG8zZjhmZ1I3dnNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3TDI1d3FnPU5pZk9mY0s3YmFUSnBvdzJaT29LZUg0N3ZHUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         MENG X C, SHEN H F, LI H F, &lt;i&gt;et al&lt;/i&gt;.Review of the Pansharpe-ning Methods for Remote Sensing Images Based on the Idea of Meta-Analysis:Practical Discussion and Challenges.Information Fusion, 2019, 46:102-113.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" ZHOU Y W, YANG P L, CHEN Q, &lt;i&gt;et al&lt;/i&gt;.Pan-Sharpening Model Based on MTF and Variational Method.Acta Automatica Sinica, 2015, 41 (2) :342-352." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201502011&amp;v=Mjk4NTA1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6Z1ZiM0lLQ0xmWWJHNEg5VE1yWTlFWllRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         ZHOU Y W, YANG P L, CHEN Q, &lt;i&gt;et al&lt;/i&gt;.Pan-Sharpening Model Based on MTF and Variational Method.Acta Automatica Sinica, 2015, 41 (2) :342-352.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" >
                                        <b>[5]</b>
                                     LUO X Q, ZHANG Z C, ZHANG B C, &lt;i&gt;et al&lt;/i&gt;.Image Fusion with Contextual Statistical Similarity and Nonsubsampled Shearlet Transform.IEEE Sensors Journal, 2017, 17 (6) :1760-1771.</a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                     VIVONE G, ALPARONE L, CHANUSSOT J, &lt;i&gt;et al&lt;/i&gt;.A Critical Comparison among Pansharpening Algorithms.IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (5) :2565-2586.</a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" NIELSEN M M.Remote Sensing for Urban Planning and Management:The Use of Window-Independent Context Segmentation to Extract Urban Features in Stockholm.Computers, Environment and Urban Systems, 2015, 52:1-9." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB6A41F31099287F3C20B07EA6FD48389&amp;v=MTY4NjY4UFNIaVgzUlJERGJhY1JyS1dDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3TDI1d3FnPU5pZk9mY0crYjlYTjJZeEVaT0lHRG5RK3VSVmc2RA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         NIELSEN M M.Remote Sensing for Urban Planning and Management:The Use of Window-Independent Context Segmentation to Extract Urban Features in Stockholm.Computers, Environment and Urban Systems, 2015, 52:1-9.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" ZHANG Y Z, ZHANG H S, LIN H.Improving the Impervious Surface Estimation with Combined Use of Optical and SAR Remote Sensing Images.Remote Sensing of Environment, 2014, 141:155-167." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061600064636&amp;v=MDc1MDFuOC9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRnNTYUJJPU5pZk9mYks4SHRmTnFZOUZaTzBMQw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         ZHANG Y Z, ZHANG H S, LIN H.Improving the Impervious Surface Estimation with Combined Use of Optical and SAR Remote Sensing Images.Remote Sensing of Environment, 2014, 141:155-167.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     MA K D, DUANMU Z F, YEGANEH H, &lt;i&gt;et al&lt;/i&gt;.Multi-exposure Image Fusion by Optimizing a Structural Similarity Index.IEEE Transactions on Computational Imaging, 2018, 4 (1) :60-72.</a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     LI H, SONG Y L, CHEN C L P.Hyperspectral Image Classification Based on Multiscale Spatial Information Fusion.IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (9) :5302-5312.</a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     HAN C, ZHANG H Y, CAO C X, &lt;i&gt;et al&lt;/i&gt;.A Remote Sensing Image Fusion Method Based on the Analysis Sparse Model.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2016, 9 (1) :439-453.</a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                     CAND&#200;S E J, WAKIN M B.An Introduction to Compressive Sampling.IEEE Signal Processing Magazine, 2008, 25 (2) :14-20</a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" 邵文泽, 韦志辉.压缩感知基本理论:回顾与展望.中国图象图形学报, 2012, 17 (1) :1-12. (SHAO W Z, WEI Z H.Advances and Perspectives on Compressed Sensing Theory.Journal of Image and Graphics, 2012, 17 (1) :1-12.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201201002&amp;v=MDgwNTQ5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emdWYjNJUHlyZmJMRzRIOVBNcm8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         邵文泽, 韦志辉.压缩感知基本理论:回顾与展望.中国图象图形学报, 2012, 17 (1) :1-12. (SHAO W Z, WEI Z H.Advances and Perspectives on Compressed Sensing Theory.Journal of Image and Graphics, 2012, 17 (1) :1-12.) 
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" 张良培, 李家艺.高光谱图像稀疏信息处理综述与展望.遥感学报, 2016, 20 (5) :1091-1101. (ZHANG L P, LI J Y.Development and Prospect of Sparse Representation-Based Hyperspectral Image Processing and Analysis.Journal of Remote Sensing, 2016, 20 (5) :1091-1101.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGXB201605033&amp;v=MDAzMDFDclRiTEc0SDlmTXFvOUdaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpnVmIzSVA=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         张良培, 李家艺.高光谱图像稀疏信息处理综述与展望.遥感学报, 2016, 20 (5) :1091-1101. (ZHANG L P, LI J Y.Development and Prospect of Sparse Representation-Based Hyperspectral Image Processing and Analysis.Journal of Remote Sensing, 2016, 20 (5) :1091-1101.) 
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     YU X C, CAO G Y, XU J D, &lt;i&gt;et al&lt;/i&gt;.Remote Sensing Image Fusion Based on Sparse Representation // Proc of the IEEE Geoscience and Remote Sensing Symposium.Washington, USA:IEEE, 2014:2858-2861.</a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" LIU Y, LIU S P, WANG Z F.A General Framework for Image Fusion Based on Multi-scale Transform and Sparse Representation.Information Fusion, 2015, 24:147-164." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700451440&amp;v=Mjk0NzdIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRnNTYUJJPU5pZk9mYks4SDlETXFJOUZZTzRPQ0hnNW9CTVQ2VDRQUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         LIU Y, LIU S P, WANG Z F.A General Framework for Image Fusion Based on Multi-scale Transform and Sparse Representation.Information Fusion, 2015, 24:147-164.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" 陈木生.结合NSCT和压缩感知的红外与可见光图像融合.中国图象图形学报, 2016, 21 (1) :39-44. (CHEN M S.Image Fusion of Visual and Infrared Image Based on NSCT and Compressed Sensing.Journal of Image and Graphics, 2016, 21 (1) :39-44.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201601005&amp;v=MTA0NDc0SDlmTXJvOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpnVmIzSVB5cmZiTEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         陈木生.结合NSCT和压缩感知的红外与可见光图像融合.中国图象图形学报, 2016, 21 (1) :39-44. (CHEN M S.Image Fusion of Visual and Infrared Image Based on NSCT and Compressed Sensing.Journal of Image and Graphics, 2016, 21 (1) :39-44.) 
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" >
                                        <b>[18]</b>
                                     WANG J, PENG J Y, JIANG X Y, &lt;i&gt;et al&lt;/i&gt;.Remote-Sensing Image Fusion Using Sparse Representation with Sub-dictionaries.International Journal of Remote Sensing, 2017, 38 (12) :3564-3585.</a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" 尹雯, 李元祥.基于稀疏表示的遥感影像融合方法.光学学报, 2013, 33 (4) .DOI:10.3788/AOS201333.0428003. (YIN W, LI Y X.Remote Sensing Image Fusion Based on Sparse Representation.Acta Optica Sinica, 2013, 33 (4) .DOI:10.3788/AOS201333.0428003.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201304042&amp;v=MTI4NzZxQnRHRnJDVVJMT2VaZVJuRnl6Z1ZiM0lJalhUYkxHNEg5TE1xNDlCWm9RS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         尹雯, 李元祥.基于稀疏表示的遥感影像融合方法.光学学报, 2013, 33 (4) .DOI:10.3788/AOS201333.0428003. (YIN W, LI Y X.Remote Sensing Image Fusion Based on Sparse Representation.Acta Optica Sinica, 2013, 33 (4) .DOI:10.3788/AOS201333.0428003.) 
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" ASGHARI M H, JALALI B.Edge Detection in Digital Images Using Dispersive Phase Stretch Transform.International Journal of Biomedical Imaging, 2015.DOI:10.1155/2015/687819." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJHD&amp;filename=SJHD15040900000338&amp;v=MjgzNzNPc1BEMzh4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZzU2FCST1OaWZEYXJLOUh0WE1wbzlGWg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         ASGHARI M H, JALALI B.Edge Detection in Digital Images Using Dispersive Phase Stretch Transform.International Journal of Biomedical Imaging, 2015.DOI:10.1155/2015/687819.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" >
                                        <b>[21]</b>
                                     ILOVITSH T, JALALI B, ASGHARI M H, &lt;i&gt;et al&lt;/i&gt;.Phase Stretch Transform for Super-Resolution Localization Microscopy.Biomedi-cal Optics Express, 2016, 7 (10) :4198-4209.</a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" >
                                        <b>[22]</b>
                                     AHARON M, ELAD M, BRUCKSTEIN A.K-SVD:An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation.IEEE Transactions on Image Processing, 2006, 54 (11) :4311-4322.</a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" >
                                        <b>[23]</b>
                                     PATI Y C, REZAIIFAR R, KRISHNAPRASAD P S.Orthogonal Matching Pursuit:Recursive Function Approximation with Applications to Wavelet Decomposition // Proc of the 27th Asilomar Conference on Signals, Systems and Computers.Berlin, Germany:Springer, 1993:40-44.</a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" >
                                        <b>[24]</b>
                                     OJHA C, FUSCO A, MANUNTA M.Denoising of Full Resolution Differential SAR Interferogram Based on K-SVD Technique // Proc of the IEEE International Geoscience and Remote Sensing Symposium.Washington, USA:IEEE, 2015:2461-2464.</a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" >
                                        <b>[25]</b>
                                     MARUTURI H, HIMA B C, SATYA P K.Image Fusion with Biorthogonal Wavelet Transform Based on Maximum Selection and Region Energy // Proc of the IEEE Conference on Computer Communication and Informatics.Washington, USA:IEEE, 2014.DOI:10.1109/ICCCI.2014.6921720.</a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_26" >
                                        <b>[26]</b>
                                     KAUR H, RANI J.Image Fusion on Digital Images Using Lapla-cian Pyramid with DWT // Proc of the 3rd International Conference on Image Information Processing.Berlin, Germany:Springer, 2015:393-398.</a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_27" >
                                        <b>[27]</b>
                                     MANU C S, JIJI C V.A Novel Remote Sensing Image Fusion Algorithm Using ICA Bases // Proc of the 8th IEEE International Conference on Advances in Pattern Recognition.Washington, USA:IEEE, 2015.DOI:10.1109/ICAPR.2015.7050690.</a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_28" >
                                        <b>[28]</b>
                                     LIU Y, WANG Z F.Simultaneous Image Fusion and Denoising with Adaptive Sparse Representation.IET Image Processing, 2015, 9 (5) :347-357.</a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_29" >
                                        <b>[29]</b>
                                     LI H, WU X J, KITTLER J.Infrared and Visible Image Fusion Using a Deep Learning Framework[C/OL].[2018-08-20].https://arxiv.org/pdf/1804.06992.pdf.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(05),398-408 DOI:10.16451/j.cnki.issn1003-6059.201905002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于PST相位约束和稀疏表示的MS和PAN影像融合算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E7%9B%B8%E6%B5%B7&amp;code=07959690&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王相海</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%99%BD%E4%B8%96%E5%A4%AB&amp;code=41999546&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">白世夫</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%99%BA&amp;code=22088530&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李智</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AE%8B%E8%8B%A5%E6%9B%A6&amp;code=34970308&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">宋若曦</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%B6%E5%85%A2%E5%96%86&amp;code=28455930&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陶兢喆</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%BE%BD%E5%AE%81%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0007183&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">辽宁师范大学计算机与信息技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%BE%BD%E5%AE%81%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E5%9F%8E%E5%B8%82%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%AD%A6%E9%99%A2&amp;code=0007183&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">辽宁师范大学城市与环境学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在基于多光谱 (MS) 影像和全色 (PAN) 遥感影像融合中, 提高融合影像质量的一个关键问题是如何有效提取PAN影像的纹理特征信息, 并有针对性地对MS影像进行信息注入.因此, 文中提出基于相位拉伸变换 (PST) 相位约束的MS和PAN影像稀疏融合算法.首先对MS和PAN影像进行高斯滤波.对于中低频信息, 基于PST相位差对影像中边缘和纹理区域的敏感性, 通过高频信息PST的相位差获得融合权重约束.对于高频信息, 通过学习PAN影像的高频信息获得训练字典, 并利用字典对MS和PAN影像的高频信息进行稀疏表示和融合, 提高融合高频信息的准确度.算法在一定程度上克服传统融合方法对边缘纹理区域融合效果较差和光谱信息扭曲等现象, 取得更好的融合效果.大量仿真实验验证算法的有效性.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%A5%E6%84%9F%E5%BD%B1%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遥感影像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E4%BD%8D%E6%8B%89%E4%BC%B8%E5%8F%98%E6%8D%A2%20(PST)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相位拉伸变换 (PST) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%80%E7%96%8F%E8%A1%A8%E7%A4%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稀疏表示;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E6%96%AF%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高斯滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E9%A2%91%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高频信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%AD%E4%BD%8E%E9%A2%91%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中低频信息;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *王相海 (通讯作者) , 博士, 教授.主要研究方向为遥感影像处理、多媒体信息处理.E-mail:xhwang@lnnu.edu.cn.;
                                </span>
                                <span>
                                    白世夫, 硕士研究生, 主要研究方向为遥感影像信息处理.E-mail:1023113604@qq.com.;
                                </span>
                                <span>
                                    李智, 硕士研究生, 主要研究方向为图像分割、遥感影像融合.E-mail:1300852174@qq.com.;
                                </span>
                                <span>
                                    宋若曦, 博士研究生, 主要研究方向为遥感图像处理、数学建模.E-mail:ruoxisong@qq.com.;
                                </span>
                                <span>
                                    陶兢喆, 博士研究生, 主要研究方向为遥感影像处理、超分辨率重建.E-mail:blueuranus@qq.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-10</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.41671439, 61402214);</span>
                                <span>辽宁省高等学校创新团队支持计划项目 (No.LT2017013) 资助;</span>
                    </p>
            </div>
                    <h1><b>MS and PAN Image Fusion Algorithm Based on PST Phase Constraint and Sparse Representation</b></h1>
                    <h2>
                    <span>WANG Xianghai</span>
                    <span>BAI Shifu</span>
                    <span>LI Zhi</span>
                    <span>SONG Ruoxi</span>
                    <span>TAO Jingzhe</span>
            </h2>
                    <h2>
                    <span>School of Computer and Information Technology, Liaoning Normal University</span>
                    <span>College of Urban and Environmental Sciences, Liaoning Normal Universtiy</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In the remote sensing image fusion based on multi-spectral (MS) image and panchromatic (PAN) image, effective extracting the texture feature information of PAN and injecting targeted information into MS image are crucial to the high quality of image fusion. Therefore, the MS and PAN image pansharpening algorithm based on phase constraint of phase stretch transform (PST) and sparse representation is proposed in this paper. Firstly, the MS and PAN images are filtered by Gaussian filter. For the low and medium frequency information, the fusion weight constraint is obtained by the phase difference of high frequency based on the sensitivity of the PST phase difference to the edge and texture region in the image. For the high frequency information, a training dictionary is obtained by learning the high frequency information of the PAN image, and the dictionary is used to sparsely represent and fuse the high frequency information of MS and PAN images, therefore the accuracy of high frequency fusion is improved. The proposed algorithm overcomes the poor fusion effect of traditional fusion methods on the edge texture region and the distortion of spectral information, achieves better fusion result. A large number of simulation experiments verify the effectiveness of the proposed method.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Remote%20Sensing%20Image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Remote Sensing Image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Phase%20Stretch%20Transform%20(PST)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Phase Stretch Transform (PST) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Sparse%20Representation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Sparse Representation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Gaussian%20Filter&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Gaussian Filter;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=High%20Frequency%20Information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">High Frequency Information;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Intermediate%20Frequency%20and%20Low%20Frequency%20Information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Intermediate Frequency and Low Frequency Information;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Xianghai ( Corresponding author) , Ph. D. , professor. His research interests include remote sensing image processing and multimedia information processing.;
                                </span>
                                <span>
                                    BAI Shifu, master student. His research interests include remote sensing image processing.;
                                </span>
                                <span>
                                    LI Zhi, master student. His research interests include image segmentation and remote sensing image fusion.;
                                </span>
                                <span>
                                    SONG Ruoxi, Ph. D. candidate. Her research interests include remote sensing image processing and mathematical modeling.;
                                </span>
                                <span>
                                    TAO Jingzhe, Ph. D. candidate. His research interests include remote sensing fusion and super-resolution reconstruction.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-10</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.41671439, 61402214);</span>
                                <span>Program for Liaoning Innovation Research Team in University (No.LT2017013);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="61">受传感技术和成像机理等诸多因素的限制, 目前还无法通过某一类特定的传感器获得同时具备高光谱分辨率和高空间分辨率的遥感影像.作为对该种限制的一个补充, 遥感影像多源融合技术将两幅或多幅来自不同传感器的影像融合成一幅影像, 通过信息互补, 使融合影像具有比任何单源影像更完整丰富的信息.近些年, 遥感影像多源融合技术在地形、地貌、地质构造等地理环境信息的获取, 以及地缘结构分析、特征提取和地质分析与反演等领域发挥重要作用<citation id="198" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="62">基于像素级的融合技术能根据待融合影像像素的统计特性等对光谱影像和纹理影像信息进行融合, 可尽可能多地保持地物场景中的原始信息, 有助于遥感影像的进一步应用<citation id="200" type="reference"><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>.随着遥感技术的发展, 高分辨率影像中包含的纹理边缘信息越来越复杂, 信息量越来越大, 如极化雷达 (SAR) 等影像中还存在斑块和噪声等信息, 这些都在不同程度上影响最后的融合效果, 而实际应用对地物的特征提取、分类和目标识别精度的要求越来越高, 如土地覆盖/土地利用制图的研究, 特别是对于城市地区由于人为因素所进行的基于固有光谱异质性地物改变的状况, 通常需要依靠遥感影像的特征分类、增加遥感影像的空间分辨率等以获得地物更多更好的描述<citation id="199" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>.所有这些都导致遥感影像特征级融合技术的研究受到关注并成为当前的研究热点<citation id="201" type="reference"><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><link href="23" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>.该类融合技术使用一组图像像素形成连续区域, 同时需要从源数据中提取不同特征, 这些特征可来自同一地理位置不同类型的图像中的像素强度、边缘或纹理特征等, 可通过色度信息变换、引导滤波、稀疏表示等方法对其进行提取, 对这些特征信息进行综合分析和融合处理, 可提高融合图像对特征属性判断的可信度和准确性.</p>
                </div>
                <div class="p1">
                    <p id="63">近些年, 随着过完备稀疏表示信号分解理论的不断发展<citation id="203" type="reference"><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>, 人们将其应用于遥感影像融合中, 提出相应的融合算法<citation id="204" type="reference"><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><link href="37" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>, 这些方法利用遥感影像的稀疏表示可较好地捕捉影像结构特征的特点, 保留高分辨率影像的纹理特征, 并融合低分辨率影像, 在有效保持光谱信息的同时可在一定程度上提高融合影像的结构特征精度.然而, 该类方法为了使字典块中尽可能多地包含遥感影像的细节特征, 一般会基于一组图像的训练集进行学习以获得字典<citation id="202" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, 提高字典学习的时间复杂性.此外, 这种一般性的学习字典通常会由于对光谱影像细节纹理特征表示的不充分, 使重构后的融合影像在纹理较丰富且密集的区域或色度较深的纹理区域容易产生光谱扭曲, 影响融合影像的质量.</p>
                </div>
                <div class="p1">
                    <p id="64">基于此种情况, 本文提出基于相位拉伸变换 (Phase Stretch Transform, PST) <citation id="205" type="reference"><link href="41" rel="bibliography" /><link href="43" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>约束的多光谱 (Multi-spectral, MS) 和全色 (Panchromatic, PAN) 影像稀疏融合算法.首先对待融合的MS和PAN影像进行高斯滤波, 获得低频信息和高频信息.对于中低频信息, 通过基于高频信息PST的相位差获得融合的权重约束, 指导中低频信息的融合.基于PST相位差对影像中边缘和纹理区域的敏感性, 保证在处理边缘和纹理区域过程中, 对光谱影像的中低频分配较小的权重, 对纹理影像分配一个较大的权重, 保证高分辨率影像的纹理信息能充分注入到融合影像中.反之, 在处理影像的平滑区域过程中, 对光谱影像分配一个较大的权重, 而对纹理影像的中低频系数分配较小的权重, 保证纹理信息的注入不会造成光谱信息的扭曲.对于高频信息, 通过学习全色影像PAN的高频信息获得字典, 利用该字典对待融合的MS和PAN影像的高频信息进行稀疏表示并融合.由于采用更具针对性的PAN高频信息进行训练, 相比采用传统的过完备字典, 不仅可节省训练字典的时间, 更重要的是由于高频信息中包含影像的绝大部分边缘和纹理信息, 获得的字典会保留图像中必要的特殊拐点和曲线, 更具针对性, 从而对高频信息的稀疏表示更充分, 提高高频信息融合的准确度.大量仿真实验验证本文算法的有效性.</p>
                </div>
                <h3 id="65" name="65" class="anchor-tag">1 相位拉伸变换</h3>
                <div class="p1">
                    <p id="66">PST是Asghari等<citation id="206" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>提出的一种非线性频率相关相位操作, 主要用于图像的边缘检测.PST将2D相位函数应用于图像的频率域, 同时关联应用的相位数与频率的特性, 即更高的相位量应用于图像的更高频率特性.由于图像边缘等纹理信息包含更高的频率特征, 因此PST可通过对高频特征施加更多相位以突出图像中的边缘信息.PST的具体实现过程如下.</p>
                </div>
                <div class="p1">
                    <p id="67">对于图像<i>B</i>[<i>n</i>, <i>m</i>], 定义PST如下:</p>
                </div>
                <div class="area_img" id="71">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201905002_07100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="72">其中, <i>n</i>、<i>m</i>表示图像长、宽的两个空间变量, <i>p</i>、<i>q</i>表示频域变量, <i>A</i>[<i>n</i>, <i>m</i>]表示变换后的相位图像, <image href="images/MSSB201905002_073.jpg" type="" display="inline" placement="inline"><alt></alt></image>〈·〉表示角度运算符, <i>FFT</i>2表示2D快速傅里叶变换, <i>IFFT</i>2表示2D快速傅里叶逆变换, <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>L</mi><mo>˜</mo></mover></math></mathml>表示局部核函数的频域响应函数, <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Κ</mi><mo>˜</mo></mover></math></mathml>表示扭曲相位核, </p>
                </div>
                <div class="p1">
                    <p id="76"><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Κ</mi><mo>˜</mo></mover><mo stretchy="false">[</mo><mi>p</mi><mo>, </mo><mi>q</mi><mo stretchy="false">]</mo><mo>=</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mrow><mi>j</mi><mo>⋅</mo><mtext>ϕ</mtext><mo stretchy="false">[</mo><mi>p</mi><mo>, </mo><mi>q</mi><mo stretchy="false">]</mo></mrow><mo stretchy="false">) </mo></mrow></math></mathml>.      (2) </p>
                </div>
                <div class="p1">
                    <p id="78">相位谱ϕ[<i>p</i>, <i>q</i>]定义如下:</p>
                </div>
                <div class="p1">
                    <p id="79">ϕ[<i>p</i>, <i>q</i>]=</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mfrac><mrow><mi>W</mi><mi>r</mi><mtext>t</mtext><mtext>a</mtext><mtext>n</mtext><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mi>W</mi><mi>r</mi><mo stretchy="false">) </mo><mo>-</mo><mn>0</mn><mo>.</mo><mn>5</mn><mi>ln</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mrow><mo stretchy="false"> (</mo><mi>W</mi><mi>r</mi><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo></mrow><mrow><mi>W</mi><mi>r</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mi>tan</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mi>W</mi><mi>r</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false">) </mo><mo>-</mo><mn>0</mn><mo>.</mo><mn>5</mn><mi>ln</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mrow><mo stretchy="false"> (</mo><mi>W</mi><mi>r</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo></mrow></mfrac><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">其中, <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><mo>=</mo><msqrt><mrow><mi>p</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>q</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mo>, </mo><mi>S</mi></mrow></math></mathml>表示相位谱强度, <i>W</i>表示扭曲度, 二者均为实数, <i>r</i><sub>max</sub>表示<i>r</i>的最大值.</p>
                </div>
                <div class="p1">
                    <p id="83">在实际应用中, <i>S</i>和<i>W</i>分别对应PST核相位谱的宽度和扭曲度阈值, 用于控制边缘纹理信息提取过程.过大的<i>S</i>会使边缘信息的提取过程具有抗噪能力, 但会降低提取边缘信息的分辨率.同样较大的<i>W</i>会使提取的边缘锐化, 但也会增加边缘中的噪声.合理选择<i>S</i>、<i>W</i>, 一般需要在提取边缘信息的分辨率和所含噪声之间进行折衷.此外, 为了简化运算过程, 一般可假设PST核相位谱关于频率变量是圆形对称的, 对于较小的扭曲因子 (即<i>W</i>≪1) , 相位谱ϕ[<i>p</i>, <i>q</i>]会变成二次相位, 并较好地表达线性相位导数.</p>
                </div>
                <h3 id="84" name="84" class="anchor-tag">2 基于PST相位差权重的多光谱和全色稀疏融合算法</h3>
                <h4 class="anchor-tag" id="85" name="85"><b>2.1 遥感影像训练集对字典影响分析</b></h4>
                <div class="p1">
                    <p id="86">稀疏表示是以压缩感知 (Compressed Sensing, CS) 理论为基础, 寻找信号在一组字典下的最稀疏表示.对图像进行稀疏表示是指将输入的图像信号<b><i>x</i></b>线性投影到过完备字典<b><i>D</i></b>上, 假设投影在<b><i>D</i></b>上的系数表示为<i>α</i>, 则<i>α</i>是稀疏的.此时有<b><i>x</i></b>=<b><i>D</i></b><i>α</i>.</p>
                </div>
                <div class="p1">
                    <p id="87">稀疏表示问题就是求解最优<i>α</i>, 即使<i>α</i>中非零元素的个数最少.此时问题可转化</p>
                </div>
                <div class="p1">
                    <p id="88"><mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">α</mi></munder><mo stretchy="false">{</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">x</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">α</mi></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">α</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">}</mo></mrow></math></mathml>,      (3) </p>
                </div>
                <div class="p1">
                    <p id="90">其中, <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mo>⋅</mo><mo>|</mo></mrow><msub><mrow></mrow><mn>0</mn></msub></mrow></math></mathml>为<i>l</i><sub>0</sub>范数, <i>λ</i>为常数.</p>
                </div>
                <div class="p1">
                    <p id="92">由于式 (3) 为一个非凸函数, 为了获得最优解, 实际应用中一般将其转化为</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">α</mi></munder><mo stretchy="false">{</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">x</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">α</mi></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">α</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">}</mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">这个<i>l</i><sub>1</sub>范数问题, 使其为近凸函数.</p>
                </div>
                <div class="p1">
                    <p id="95">上述过完备字典<b><i>D</i></b>一般由一组图像集或图像本身的序列通过<i>K</i>次奇异值分解 (<i>K</i>-Singular Value Decomposition, K-SVD) <citation id="207" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>等训练得到, 这种字典一般能表示大多数图像的特征, 通用性较好, 但并不能充分表示遥感影像中的一些特殊曲边或拐点等融合相对重要的信息, 在一定程度上影响融合效果.</p>
                </div>
                <div class="p1">
                    <p id="96">图1为分别采用遥感影像集和单幅遥感影像本身训练获得的两个字典.后者通过把单幅图像分成相同的小块, 再拉成列向量, 并根据正交匹配跟踪 (Orthogonal Matching Pursuit, OMP) <citation id="208" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>等对其进行归类形成特征集.</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905002_10000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 2种不同训练集训练获得的字典" src="Detail/GetImg?filename=images/MSSB201905002_10000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 2种不同训练集训练获得的字典  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905002_10000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Comparison of dictionaries trained on 2 datasets</p>

                </div>
                <div class="p1">
                    <p id="101">由图1可见, 采用一组遥感影像训练的字典 (a) 的确包含遥感影像中大部分边缘纹理特征块, 但相对采用待融合遥感影像自身训练的字典 (b) , 后者更有针对性地包含影像的细节特征信息, 形成字典所用的时间复杂度更小.</p>
                </div>
                <div class="p1">
                    <p id="102">进一步地, 本文对遥感影像分别采用图1中的两个字典进行稀疏表示重建图像.可以看出, 采用基于图像本身序列训练得到的字典对图像的稀疏能力更强, 字典更有针对性地包含影像中更多的边缘和纹理信息, 重构后影像的峰值信噪比 (Peak Signal to Noise Ratio, PSNR) 也更高.</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103"><b>2.2 PST</b><b>对遥感影像边缘纹理提取的有效性分析</b></h4>
                <div class="p1">
                    <p id="104">为了说明PST对遥感影像边缘纹理信息提取的有效性, 本文选自WorldView02卫星的MS和PAN影像, 以及RADARSAT-2卫星SAR传感器拍摄的HH通道的SAR影像, 检测结果和相位梯度统计分别见图2和图3, 图3中三个维度坐标分别表示影像沿<i>x</i>轴、<i>y</i>轴和<i>z</i>轴的方向导数.</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905002_11200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 W不同时3幅遥感影像的检测结果" src="Detail/GetImg?filename=images/MSSB201905002_11200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 <i>W</i>不同时3幅遥感影像的检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905002_11200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Detection results of 3 remote sensing images with different W values</p>

                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905002_11201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 W不同时3幅遥感影像的检测结果" src="Detail/GetImg?filename=images/MSSB201905002_11201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 <i>W</i>不同时3幅遥感影像的检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905002_11201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Detection results of 3 remote sensing images with different W values</p>

                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905002_11800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 W不同时3幅遥感影像的相位梯度统计结果" src="Detail/GetImg?filename=images/MSSB201905002_11800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 <i>W</i>不同时3幅遥感影像的相位梯度统计结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905002_11800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Statistical results of phase gradient of 3 remote sensing images with different <i>W</i> values</p>

                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905002_11801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 W不同时3幅遥感影像的相位梯度统计结果" src="Detail/GetImg?filename=images/MSSB201905002_11801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 <i>W</i>不同时3幅遥感影像的相位梯度统计结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905002_11801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Statistical results of phase gradient of 3 remote sensing images with different <i>W</i> values</p>

                </div>
                <div class="p1">
                    <p id="119">进一步选用等效视数 (Equivalent Number of Looks, ENL) <citation id="209" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>统计图3中实验结果.ENL用于测评影像中均匀区域的光滑性, 定义如下:<i>ENL</i>=<i>μ</i><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ι</mi><mn>2</mn></msubsup></mrow></math></mathml>/<i>σ</i><mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ι</mi><mn>2</mn></msubsup></mrow></math></mathml>, 其中, <i>μ</i>表示图像<i>I</i>灰度的算术平均值, <i>σ</i>表示图像的标准偏差.ENL值越大, 表示影像受斑块和噪声效应的影响越小, 否则, 表示影像受斑块噪声干扰较大.统计结果如表1所示.</p>
                </div>
                <div class="area_img" id="122">
                    <p class="img_tit"><b>表1 <i>W</i>不同时图像的ENL评价统计结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"> Table 1 ENL statistic results with different W values</p>
                    <p class="img_note"></p>
                    <table id="122" border="1"><tr><td><br />图像</td><td><i>S</i>=5, <i>W</i>=0</td><td><i>S</i>=5, <i>W</i>=20</td><td><i>S</i>=5, <i>W</i>=80</td></tr><tr><td><br />PAN</td><td>0.25</td><td>0.27</td><td>0.28</td></tr><tr><td><br />MS</td><td>0.29</td><td>0.36</td><td>0.42</td></tr><tr><td><br />SAR</td><td>0.32</td><td>0.38</td><td>0.43</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="123">由实验结果可看出, 针对PAN和MS影像, 当<i>S</i>一定时, <i>W</i>越大, 对边缘的检测越精确, 但相对会引入一定的冗余信息.当模型应用于SAR影像时, <i>W</i>越大, 对影像边缘检测的轮廓越清晰, 同时ENL也越大, 说明可以有效抑制影像中的斑块和噪声影响, 更便于提取影像的纹理边缘.可见PST扭曲度和强度参数会影响图像纹理信息提取的精度, 当扭曲度和强度过大时, 会检测边缘纹理以外的冗余信息, 在融合过程中容易导致光谱扭曲现象.而强度和扭曲度过小会造成信息遗漏, 不能充分提取高分辨率影像的纹理特征或容易受到影像中斑块或噪声信息的影响, 导致融合效果不理想.</p>
                </div>
                <div class="p1">
                    <p id="124">为了使PST更适用于MS和PAN影像的融合过程, 在算法的实现过程中, 根据遥感影像自身纹理的丰富程度, 自适应确定PST模型中<i>S</i>和<i>W</i>的取值.具体地, 选取区域能量作为纹理丰富程度的度量标准<citation id="210" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>, 即能量较大的区域为纹理信息密集区域, 此时对<i>S</i>和<i>W</i>分配一个较大的数值, 否则为较平滑区域, 此时对<i>S</i>和<i>W</i>分配一个较小的数值, 以此提高PST模型对遥感影像纹理特征的提取精度.实验中根据统计结果, 对能量较大的区域, <i>S</i>=10, <i>W</i>=25, 对能量较小的区域, <i>S</i>=5, <i>W</i>=15.</p>
                </div>
                <h4 class="anchor-tag" id="125" name="125"><b>2.3 算法步骤</b></h4>
                <div class="p1">
                    <p id="126">MS和PAN影像融合算法的难点问题是如何使融合影像有效保持光谱质量和纹理特征, 也是衡量融合算法有效性的核心指标.为了更好地保证融合影像的光谱分辨率, 本文首先将MS影像的RGB色度空间变换到YC<sub>b</sub>C<sub>r</sub>色度空间中, 将<i>Y</i>分量作为与PAN融合纹理信息部分, 保留两个色度分量, 进一步将MS影像的<i>Y</i>分量与PAN影像分别进行高斯滤波, 获得对应的低频信息和高频信息, 对低频信息和高频信息分别采用如下的融合策略.</p>
                </div>
                <div class="p1">
                    <p id="127">低频信息的融合.将MS影像的<i>Y</i>分量与PAN影像的高频信息代入式 (1) 中进行PST操作, 获得对应的相位图像, 将MS和PAN的相位图像作差, 进而确定二者的相位差, 得到相位差矩阵, 并将矩阵元素值最小值和最大值分别对应0和1进行归一化处理, 获得的归一化矩阵作为低频信息融合的权重系数矩阵.在此基础上利用该权重矩阵融合两个低频信息, 在一定程度上提高低频信息的分辨率.</p>
                </div>
                <div class="p1">
                    <p id="128">高频信息的融合.通过对PAN影像的高频信息进行学习, 获得包含高频细节特征的字典, 基于该字典分别对MS影像<i>Y</i>分量高频信息和PAN影像的高频信息进行稀疏表示, 并按绝对值取大原则融合两个稀疏系数, 获得融合高频信息的稀疏表示, 通过稀疏重建, 最终得到融合后的高频分量.</p>
                </div>
                <div class="p1">
                    <p id="129">最后通过将融合后的高频纹理信息重新注入到融合后的低频信息中, 获得融合后影像的亮度分量, 再结合之前保存的MS的<i>C</i><sub><i>b</i></sub>和<i>C</i><sub><i>r</i></sub>色度分量获得最后的融合遥感影像.具体融合过程如算法所示.</p>
                </div>
                <div class="p1">
                    <p id="130"><b>算法</b> MS影像与PAN影像的融合</p>
                </div>
                <div class="p1">
                    <p id="131"><b>输入</b> 配准MS影像<b><i>M</i></b>, PAN影像<b><i>P</i></b></p>
                </div>
                <div class="p1">
                    <p id="132"><b>输出</b> 融合后的影像<b><i>Q</i></b></p>
                </div>
                <div class="p1">
                    <p id="133">step 1 将<b><i>M</i></b>进行YC<sub>b</sub>C<sub>r</sub>变换, 提取出待融合的亮度分量<b><i>Y</i></b>, 保留<i>C</i><sub><i>b</i></sub>和<i>C</i><sub><i>r</i></sub>两个色度分量.</p>
                </div>
                <div class="p1">
                    <p id="134">step 2 对<b><i>Y</i></b>和<b><i>P</i></b>进行高斯滤波, 获得<b><i>Y</i></b>的高频和低频分量<b><i>Y</i></b><sub>lowfreq</sub>和<b><i>Y</i></b><sub>highfreq</sub>及<b><i>P</i></b>的高频和低频分量<b><i>P</i></b><sub>lowfreq</sub>和<b><i>P</i></b><sub>highfreq</sub>.</p>
                </div>
                <div class="p1">
                    <p id="135">step 3 通过式 (2) 确 定<b><i>Y</i></b><sub>highfreq</sub>的 相 位 核 函 数<mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Κ</mi><mo>˜</mo></mover><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub></mrow></math></mathml>, 其中<i>S</i>、<i>W</i>采用基于区域能量的自适应选择方式获得.</p>
                </div>
                <div class="p1">
                    <p id="138">step 4 将<b><i>Y</i></b><sub>highfreq</sub>、<b><i>P</i></b><sub>highfreq</sub>和<mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Κ</mi><mo>˜</mo></mover><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub></mrow></math></mathml>代入式 (1) , 获得对应的相位图像<b><i>A</i></b><sub>Y_highfreq</sub>和<b><i>A</i></b><sub>P_highfreq</sub>, 进一步, 计算<b><i>Y</i></b><sub>highfreq</sub>和<b><i>P</i></b><sub>highfreq</sub>的相位差矩阵<b><i>A</i></b><sub>P_highfreq</sub>-<b><i>A</i></b><sub>Y_highfreq</sub>, 并 将 矩 阵 元 素 值 最小值和最大值分别对应0和1, 进行归一化处理, 获得的归一化矩阵作为低频信息融合的权重系数矩阵<b><i>W</i></b><sub>fused</sub>.</p>
                </div>
                <div class="p1">
                    <p id="140">step 5 对低频分量<b><i>Y</i></b><sub>lowfreq</sub>和<b><i>P</i></b><sub>lowfreq</sub>进行基于<b><i>W</i></b><sub>fused</sub>约束的加权融合, 获得融合后的低频分量</p>
                </div>
                <div class="p1">
                    <p id="141"><b><i>Y</i></b>′<sub>lowfreq</sub>=<b><i>Y</i></b><sub>lowfreq</sub>+<b><i>W</i></b><sub>fused</sub><b><i>P</i></b><sub>lowfreq</sub>.</p>
                </div>
                <div class="p1">
                    <p id="142">step 6 通过学习<b><i>P</i></b><sub>highfreq</sub>训练得到字典<b><i>D</i></b><sub>fused</sub>, 并通过<b><i>D</i></b><sub>fused</sub>对<b><i>Y</i></b><sub>highfreq</sub>和<b><i>P</i></b><sub>highfreq</sub>进行稀疏表示, 获得稀疏矩阵<b><i>Y</i></b><sub>sparse_highfreq</sub>和<b><i>P</i></b><sub>Sparse_highfreq</sub>.</p>
                </div>
                <div class="p1">
                    <p id="143">step 7 根据绝对值取大的原则融合<b><i>Y</i></b><sub>sparse_highfreq</sub>和<b><i>P</i></b><sub>Sparse_highfreq</sub>, 获得融稀疏矩阵<b><i>Y</i></b>′<sub>sparse_highfreq</sub>.</p>
                </div>
                <div class="p1">
                    <p id="144">step 8 对<b><i>Y</i></b>′<sub>sparse_highfreq</sub>进行稀疏重建, 获得融合后的高频分量<b><i>Y</i></b>′<sub>highfreq</sub>.</p>
                </div>
                <div class="p1">
                    <p id="145">step 9 将<b><i>Y</i></b>′<sub>highfreq</sub>重新注入到低频分量<b><i>Y</i></b>′<sub>lowfreq</sub>, 得到MS影像的融合后的亮度分量<b><i>Y</i></b>′<sub>fused</sub>, 将<i>C</i><sub><i>b</i></sub>和<i>C</i><sub><i>r</i></sub>进行<i>YC</i><sub><i>b</i></sub><i>C</i><sub><i>r</i></sub>逆变换, 获得最后的融合影像<b><i>Q</i></b>.</p>
                </div>
                <div class="p1">
                    <p id="146">具体流程如图4所示.</p>
                </div>
                <div class="area_img" id="147">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905002_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 本文算法的图像融合流程图" src="Detail/GetImg?filename=images/MSSB201905002_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 本文算法的图像融合流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905002_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Flow chart of image fusion by the proposed algorithm</p>

                </div>
                <h3 id="148" name="148" class="anchor-tag">3 实验及结果分析</h3>
                <div class="p1">
                    <p id="149">为了验证本文算法的有效性, 选取5组分别来自Worldview-3和QuickBird02拍摄的澳大利亚悉尼地区和美国加利福尼亚州地区的MS和PAN匹配遥感影像进行仿真实验, 测试图像如图5所示.</p>
                </div>
                <div class="area_img" id="152">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905002_15200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 5组测试的MS和PAN遥感影像" src="Detail/GetImg?filename=images/MSSB201905002_15200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 5组测试的MS和PAN遥感影像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905002_15200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Five groups of MS and PAN testing remote sensing images</p>

                </div>
                <div class="area_img" id="152">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905002_15201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 5组测试的MS和PAN遥感影像" src="Detail/GetImg?filename=images/MSSB201905002_15201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 5组测试的MS和PAN遥感影像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905002_15201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Five groups of MS and PAN testing remote sensing images</p>

                </div>
                <div class="p1">
                    <p id="153">图像的分辨率比为1∶4.选取的MS和PAN影像不仅包含山地、水域和植被等自然纹理, 也包括城镇等复杂纹理区域.</p>
                </div>
                <div class="p1">
                    <p id="154">实验环境为Windows 10操作系统和Matlab R2015a实验平台.</p>
                </div>
                <h4 class="anchor-tag" id="155" name="155"><b>3.1 主观评价</b></h4>
                <div class="p1">
                    <p id="156">同时将本文算法与脉冲神经网络 (Pulse Cou-pled Neural Network, PCNN) 、梯 度 域 加 权 (Gradient Domain Weighted, GDW) 融合、拉普拉斯金字塔 (Laplace Pyramid, LP) <citation id="211" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、加权独立成分分析 (Weighted Independent Component Analysis, WICA) <citation id="212" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>、自适应稀疏表示 (Adaptive Sparse Representation, ASR) <citation id="213" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>、深度学习算法 (Deep Learning, DL) <citation id="214" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>进行对比实验.具体实验结果如图6所示.图7为图6中MS的第1幅和第5幅融合影像的局部放大影像.</p>
                </div>
                <div class="area_img" id="169">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905002_16900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 7种算法的融合影像对比" src="Detail/GetImg?filename=images/MSSB201905002_16900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 7种算法的融合影像对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905002_16900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Comparison of panshapening results by 7 algorithms</p>

                </div>
                <div class="area_img" id="169">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905002_16901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 7种算法的融合影像对比" src="Detail/GetImg?filename=images/MSSB201905002_16901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 7种算法的融合影像对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905002_16901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Comparison of panshapening results by 7 algorithms</p>

                </div>
                <div class="area_img" id="182">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905002_18200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 图6中部分测试图像融合结果区域放大对比" src="Detail/GetImg?filename=images/MSSB201905002_18200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 图6中部分测试图像融合结果区域放大对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905002_18200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Comparison of zoom pansharpening results of test images in Fig.6</p>

                </div>
                <div class="area_img" id="182">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201905002_18201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 图6中部分测试图像融合结果区域放大对比" src="Detail/GetImg?filename=images/MSSB201905002_18201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 图6中部分测试图像融合结果区域放大对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201905002_18201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Comparison of zoom pansharpening results of test images in Fig.6</p>

                </div>
                <div class="p1">
                    <p id="184">从图6和图7可看出, 在主观视觉方面, 拉普拉斯金字塔、梯度域加权方法、WICA对图像纹理提取不够精细, 在光谱上也产生严重的扭曲现象.PCNN和ASR对图像纹理细节提取较好, 但在植被区域光谱扭曲较严重, DL对光谱信息保持 较 好, 但 对 图 像边缘细节提取不够精准.本文算法得到的融合结果影像的纹理特征较清晰, 尤其对于城镇等区域的处理方面, 效果较突出.在光谱质量方面, 本文算法的光谱扭曲度较小, 光谱信息较完整, 对于森林植被覆盖的色度较深或纹理相对密集的区域, 本文的光谱信息与MS影像的原始信息较接近.</p>
                </div>
                <h4 class="anchor-tag" id="185" name="185"><b>3.2 客观评价</b></h4>
                <div class="p1">
                    <p id="186">为了从客观上验证本文算法的有效性, 采用相对全局综合误差 (Erreur Relative Global Adimen-sionnelle de Synthesis, ERGAS) 指数和均方根误差 (Root Mean Square Error, RMSE) 客观评价融合结果的光谱质量, 采用边缘强度 (Edge Intensity, EIN) 和结构相似性 (Structural Similarity Index, SSIM) 客观评价融合影像的纹理特征, 并采用QNR (Quality with No Reference) 作为整体评判指标.不同算法实验结果如表2所示.</p>
                </div>
                <div class="area_img" id="187">
                    <p class="img_tit"><b>表2 7种算法融合结果定量分析对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">Table 2 Quantitative analysis of fusion images obtained by different methods</p>
                    <table id="187" border="1"><tr><td>测试集图像</td><td>评价指标</td><td>LP</td><td>GDW</td><td>WICA</td><td>PCNN</td><td>ASR</td><td>DL</td><td>本文算法</td></tr><tr><td>Test-Images 1</td><td>RMSE<br />ERGAS<br />SSIM<br />EIN<br />QNR<br />时间复杂度/s</td><td>47.98<br />31.87<br />0.60<br />87.55<br />0.67<br />3.41</td><td>25.21<br />16.71<br />0.79<br />100.58<br />0.64<br />2.66</td><td>30.86<br />20.45<br />0.68<br />101.83<br />0.58<br />11.00</td><td>40.28<br />26.65<br />0.59<br />107.73<br />0.62<br />150.59</td><td>30.56<br />20.24<br />0.63<br />120.98<br />0.60<br />187.47</td><td>23.13<br />15.35<br />0.79<br />91.14<br />0.66<br />4.77</td><td>21.42<br />14.22<br />0.77<br />122.12<br />0.70<br />116.83</td></tr><tr><td>Test-Images 2</td><td>RMSE<br />ERGAS<br />SSIM<br />EIN<br />QNR<br />时间复杂度/s</td><td>49.14<br />26.40<br />0.65<br />87.17<br />0.73<br />3.42</td><td>27.14<br />14.52<br />0.79<br />98.07<br />0.66<br />2.68</td><td>31.53<br />16.90<br />0.69<br />96.67<br />0.60<br />10.80</td><td>45.03<br />24.08<br />0.59<br />107.80<br />0.62<br />154.28</td><td>32.24<br />17.26<br />0.65<br />117.29<br />0.61<br />190.70</td><td>25.29<br />13.56<br />0.79<br />89.29<br />0.68<br />4.47</td><td>21.09<br />11.33<br />0.80<br />117.66<br />0.75<br />116.40</td></tr><tr><td>Test-Images 3</td><td>RMSE<br />ERGAS<br />SSIM<br />EIN<br />QNR<br />时间复杂度/s</td><td>35.52<br />23.59<br />0.87<br />60.15<br />0.88<br />3.43</td><td>17.75<br />11.67<br />0.95<br />78.37<br />0.75<br />2.97</td><td>17.24<br />11.13<br />0.95<br />78.91<br />0.82<br />10.38</td><td>24.25<br />15.96<br />0.91<br />80.48<br />0.82<br />150.82</td><td>20.03<br />13.02<br />0.93<br />86.46<br />0.73<br />176.24</td><td>17.40<br />11.55<br />0.92<br />75.30<br />0.80<br />4.67</td><td>14.76<br />9.64<br />0.96<br />93.07<br />0.96<br />128.31</td></tr><tr><td>Test-Images 4</td><td>RMSE<br />ERGAS<br />SSIM<br />EIN<br />QNR<br />时间复杂度/s</td><td>62.50<br />46.83<br />0.67<br />130.83<br />0.80<br />3.37</td><td>24.23<br />18.13<br />0.89<br />173.73<br />0.73<br />2.89</td><td>31.11<br />23.27<br />0.82<br />172.79<br />0.70<br />10.76</td><td>39.06<br />29.21<br />0.77<br />188.10<br />0.72<br />152.72</td><td>28.72<br />21.49<br />0.82<br />204.01<br />0.72<br />191.03</td><td>24.42<br />18.36<br />0.88<br />161.11<br />0.73<br />4.74</td><td>20.10<br />15.22<br />0.89<br />213.76<br />0.95<br />106.79</td></tr><tr><td>Test-Images 5</td><td>RMSE<br />ERGAS<br />SSIM<br />EIN<br />QNR<br />时间复杂度/s</td><td>49.86<br />37.40<br />0.74<br />128.28<br />0.88<br />3.35</td><td>16.66<br />12.33<br />0.96<br />162.45<br />0.82<br />2.86</td><td>19.77<br />14.63<br />0.91<br />161.57<br />0.84<br />11.29</td><td>24.66<br />18.25<br />0.91<br />163.14<br />0.83<br />152.65</td><td>19.97<br />14.77<br />0.92<br />184.35<br />0.81<br />189.52</td><td>16.90<br />12.51<br />0.95<br />152.39<br />0.82<br />4.63</td><td>16.48<br />12.18<br />0.94<br />195.41<br />0.97<br />108.8<br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="188">各指标具体定义如下.</p>
                </div>
                <div class="p1">
                    <p id="189" class="code-formula">
                        <mathml id="189"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mi>R</mi><mi>G</mi><mi>A</mi><mi>S</mi><mo>=</mo><mn>1</mn><mn>0</mn><mn>0</mn><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mi>h</mi></msub></mrow><mrow><mi>d</mi><msub><mrow></mrow><mi>l</mi></msub></mrow></mfrac><msqrt><mrow><mfrac><mn>1</mn><mi>L</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mo stretchy="false"> (</mo></mstyle><mfrac><mrow><mi>R</mi><mi>Μ</mi><mi>S</mi><mi>E</mi><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow><mrow><mi>μ</mi><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></mfrac><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="190" class="code-formula">
                        <mathml id="190"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><mi>Μ</mi><mi>S</mi><mi>E</mi><mo>=</mo><msqrt><mrow><mi>E</mi><mo stretchy="false"> (</mo><mrow><mo stretchy="false"> (</mo><mi>A</mi><mo>-</mo><mi>B</mi><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo></mrow></msqrt><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mi>E</mi><mi>Ι</mi><mi>Ν</mi><mo>=</mo><mrow><mo>|</mo><mrow><mo>∇</mo><mi>x</mi><mspace width="0.25em" /><mi>f</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mrow><mo>∇</mo><mi>y</mi><mspace width="0.25em" /><mi>f</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mi>S</mi><mi>S</mi><mi>Ι</mi><mi>Μ</mi><mo stretchy="false"> (</mo><mi>A</mi><mo>, </mo><mi>B</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mn>2</mn><mi>μ</mi><msub><mrow></mrow><mi>A</mi></msub><mspace width="0.25em" /><mi>μ</mi><msub><mrow></mrow><mi>B</mi></msub><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mn>2</mn><mi>σ</mi><msub><mrow></mrow><mrow><mi>A</mi><mi>B</mi></mrow></msub><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow><mrow><mo stretchy="false"> (</mo><mi>μ</mi><msubsup><mrow></mrow><mi>A</mi><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><msubsup><mrow></mrow><mi>B</mi><mn>2</mn></msubsup><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>σ</mi><msubsup><mrow></mrow><mi>A</mi><mn>2</mn></msubsup><mo>+</mo><mi>σ</mi><msubsup><mrow></mrow><mi>B</mi><mn>2</mn></msubsup><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mi>Q</mi><mi>Ν</mi><mi>R</mi><mo>=</mo><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>λ</mi></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mi>α</mi></msup><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mi>β</mi></msup><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="191">ERGAS用于考察光谱变化的整体情况, <i>d</i><sub><i>h</i></sub>/<i>d</i><sub><i>l</i></sub>为全色图像与多光谱图像分辨率比率, <i>μ</i> (<i>l</i>) 为多光谱影像的均值, <i>L</i>为波段数, ERGAS值越小, 表示光谱质量越高.RMSE用于度量融合结果图像<i>A</i>与理想融合图像<i>B</i>的偏差, 可度量有参考的影像, 值越小, 表明融合效果越好 (实验中<i>B</i>是以原始未进行采样操作的MS影像的融合结果作为理想影像) .EIN用于衡量融合影像的边缘保持程度, 值越大说明纹理边缘信息越多, ∇<i>x f</i> (<i>i</i>, <i>j</i>) 、∇<i>y f</i> (<i>i</i>, <i>j</i>) 为影像第<i>i</i>行、 <i>j</i>列的<i>x</i>和<i>y</i>方向的一阶差分.SSIM是衡量两幅图像相似性的评价指标, <i>μ</i>、<i>σ</i><sup>2</sup>分别表示影像的均值、方差, <i>σ</i><sub><i>AB</i></sub>为两影像的协方差, <i>c</i><sub>1</sub>= (<i>k</i><sub>1</sub><i>L</i>) <sup>2</sup>, <i>c</i><sub>2</sub>= (<i>k</i><sub>2</sub><i>L</i>) <sup>2</sup>, 用于维持稳定的常数, <i>L</i>为像素值的动态范围, <i>k</i><sub>1</sub>=0.01, <i>k</i><sub>2</sub>=0.03.QNR通过计算不同波段之间的互协方差以捕捉影像的相似度、均值和对比度差异, 取值范围为[0, 1], 最优值为1, 在无参考图像的前提下能在原影像尺度上对融合结果图像进行一个整体评价, <i>D</i><sub><i>λ</i></sub>和<i>D</i><sub><i>s</i></sub>分别为光谱失真量和空间细节失真量, </p>
                </div>
                <div class="p1">
                    <p id="192" class="code-formula">
                        <mathml id="192"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>D</mi><msub><mrow></mrow><mi>λ</mi></msub><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>Ι</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Ι</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ι</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>Q</mi><mo stretchy="false"> (</mo><mi>Μ</mi><msub><mrow></mrow><mi>m</mi></msub><mo>, </mo><mi>Μ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>Q</mi><mo stretchy="false"> (</mo><mi>F</mi><msub><mrow></mrow><mi>m</mi></msub><mo>, </mo><mi>F</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></mstyle></mrow></msqrt><mo>, </mo></mtd></mtr><mtr><mtd><mi>D</mi><msub><mrow></mrow><mi>s</mi></msub><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>Ι</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Ι</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>Q</mi><mo stretchy="false"> (</mo><mi>F</mi><msub><mrow></mrow><mi>m</mi></msub><mo>, </mo><mn>1</mn><mo stretchy="false">) </mo><mo>-</mo><mi>Q</mi><mo stretchy="false"> (</mo><mi>Μ</mi><msub><mrow></mrow><mi>m</mi></msub><mo>, </mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>Ρ</mtext></mrow></msub><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></msqrt><mo>, </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="193"><i>I</i>表示MS影像的波段数, <i>M</i>表示原光谱影像, <i>F</i>表示融合影像, <i>P</i><sub>LP</sub>表示利用MTF低通滤波器降采样的影像.</p>
                </div>
                <div class="p1">
                    <p id="194">从表2的客观评价对比可看出, 在实验选取的6组数据中, 本文算法无论在光谱方面的ERGAS指数, 还是纹理结构方面的RMSE、EIN、SSIM, 还是整体评价指标QNR, 数值均优于其它对比算法.相比其它对比算法, 本文算法对PAN影像的纹理特征提取较充分, 在提高MS影像空间分辨率的同时, 较好地保留MS影像的光谱信息, 在融合影像中得到较好的体现.</p>
                </div>
                <div class="p1">
                    <p id="195">在时间复杂度方面, 一些传统的算法, 如拉普拉斯金字塔、梯度域加权、WICA虽然时间复杂度较低, 但是在光谱保持和纹理提取方面都表现较差.表2中DL时间复杂度不包含训练时间, 深度学习训练时间非常耗时.相比PCNN、ASR和DL, 本文算法在光谱保持、纹理细节提取和时间复杂度方面表现较好.所以, 本文算法在提高融合质量的同时也兼顾运算效率.</p>
                </div>
                <h3 id="196" name="196" class="anchor-tag">4 结 束 语</h3>
                <div class="p1">
                    <p id="197">本文提出基于PST相位约束的MS和PAN影像稀疏融合算法.基于PST相位差对影像中边缘和纹理区域的敏感性, 中低频信息通过基于高频信息PST的相位差获得融合的权重约束, 即保证PAN影像的纹理信息能够充分注入到MS影像中, 同时又能使注入的纹理信息不会造成光谱信息的扭曲.此外, 通过学习PAN的高频信息获得字典更具针对性, 利用该字典对MS和PAN影像的高频信息进行稀疏表示并融合, 提高高频信息融合的准确度.如何进一步减少本文算法的时间复杂度以及如何进一步保持影响的光谱信息都需要进一步研究.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="217" type="formula" href="images/MSSB201905002_21700.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">王相海</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="219" type="formula" href="images/MSSB201905002_21900.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">白世夫</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="221" type="formula" href="images/MSSB201905002_22100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">李智</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="223" type="formula" href="images/MSSB201905002_22300.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">宋若曦</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="225" type="formula" href="images/MSSB201905002_22500.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">陶兢喆</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                 POHL C, CAN GENDEREN L.Remote Sensing Image Fusion:A Practical Guide.Boca Raton, USA:CRC Press, 2017.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" >
                                    <b>[2]</b>
                                 GHASSEMIAN H.A Review of Remote Sensing Image Fusion Me-thods.Information Fusion, 2016, 32:75-89.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESA3CE593C015D22CB894DB1FB826ED21C&amp;v=MjU4NDFORmh3TDI1d3FnPU5pZk9mY0s3YmFUSnBvdzJaT29LZUg0N3ZHUWI0enNKT242VTNobzNmOGZnUjd2c0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> MENG X C, SHEN H F, LI H F, <i>et al</i>.Review of the Pansharpe-ning Methods for Remote Sensing Images Based on the Idea of Meta-Analysis:Practical Discussion and Challenges.Information Fusion, 2019, 46:102-113.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201502011&amp;v=MDcwMTdiM0lLQ0xmWWJHNEg5VE1yWTlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6Z1Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> ZHOU Y W, YANG P L, CHEN Q, <i>et al</i>.Pan-Sharpening Model Based on MTF and Variational Method.Acta Automatica Sinica, 2015, 41 (2) :342-352.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" >
                                    <b>[5]</b>
                                 LUO X Q, ZHANG Z C, ZHANG B C, <i>et al</i>.Image Fusion with Contextual Statistical Similarity and Nonsubsampled Shearlet Transform.IEEE Sensors Journal, 2017, 17 (6) :1760-1771.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                 VIVONE G, ALPARONE L, CHANUSSOT J, <i>et al</i>.A Critical Comparison among Pansharpening Algorithms.IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (5) :2565-2586.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB6A41F31099287F3C20B07EA6FD48389&amp;v=MDY2MDFmT0dRbGZDcGJRMzVORmh3TDI1d3FnPU5pZk9mY0crYjlYTjJZeEVaT0lHRG5RK3VSVmc2RDhQU0hpWDNSUkREYmFjUnJLV0NPTnZGU2lXV3I3SklGcG1hQnVIWQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> NIELSEN M M.Remote Sensing for Urban Planning and Management:The Use of Window-Independent Context Segmentation to Extract Urban Features in Stockholm.Computers, Environment and Urban Systems, 2015, 52:1-9.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061600064636&amp;v=MTM1MDZxUVRNbndaZVp1SHlqbVVMYklKRnNTYUJJPU5pZk9mYks4SHRmTnFZOUZaTzBMQ244L29CTVQ2VDRQUUgvaXJSZEdlcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> ZHANG Y Z, ZHANG H S, LIN H.Improving the Impervious Surface Estimation with Combined Use of Optical and SAR Remote Sensing Images.Remote Sensing of Environment, 2014, 141:155-167.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 MA K D, DUANMU Z F, YEGANEH H, <i>et al</i>.Multi-exposure Image Fusion by Optimizing a Structural Similarity Index.IEEE Transactions on Computational Imaging, 2018, 4 (1) :60-72.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 LI H, SONG Y L, CHEN C L P.Hyperspectral Image Classification Based on Multiscale Spatial Information Fusion.IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (9) :5302-5312.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 HAN C, ZHANG H Y, CAO C X, <i>et al</i>.A Remote Sensing Image Fusion Method Based on the Analysis Sparse Model.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2016, 9 (1) :439-453.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                 CANDÈS E J, WAKIN M B.An Introduction to Compressive Sampling.IEEE Signal Processing Magazine, 2008, 25 (2) :14-20
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201201002&amp;v=Mjg0NzlHRnJDVVJMT2VaZVJuRnl6Z1ZiM0lQeXJmYkxHNEg5UE1ybzlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 邵文泽, 韦志辉.压缩感知基本理论:回顾与展望.中国图象图形学报, 2012, 17 (1) :1-12. (SHAO W Z, WEI Z H.Advances and Perspectives on Compressed Sensing Theory.Journal of Image and Graphics, 2012, 17 (1) :1-12.) 
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGXB201605033&amp;v=Mjk4MjJyVGJMRzRIOWZNcW85R1o0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emdWYjNJUEM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 张良培, 李家艺.高光谱图像稀疏信息处理综述与展望.遥感学报, 2016, 20 (5) :1091-1101. (ZHANG L P, LI J Y.Development and Prospect of Sparse Representation-Based Hyperspectral Image Processing and Analysis.Journal of Remote Sensing, 2016, 20 (5) :1091-1101.) 
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 YU X C, CAO G Y, XU J D, <i>et al</i>.Remote Sensing Image Fusion Based on Sparse Representation // Proc of the IEEE Geoscience and Remote Sensing Symposium.Washington, USA:IEEE, 2014:2858-2861.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700451440&amp;v=MDA0NjZiSUpGc1NhQkk9TmlmT2ZiSzhIOURNcUk5RllPNE9DSGc1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> LIU Y, LIU S P, WANG Z F.A General Framework for Image Fusion Based on Multi-scale Transform and Sparse Representation.Information Fusion, 2015, 24:147-164.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201601005&amp;v=MDQzNzFuRnl6Z1ZiM0lQeXJmYkxHNEg5Zk1ybzlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> 陈木生.结合NSCT和压缩感知的红外与可见光图像融合.中国图象图形学报, 2016, 21 (1) :39-44. (CHEN M S.Image Fusion of Visual and Infrared Image Based on NSCT and Compressed Sensing.Journal of Image and Graphics, 2016, 21 (1) :39-44.) 
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" >
                                    <b>[18]</b>
                                 WANG J, PENG J Y, JIANG X Y, <i>et al</i>.Remote-Sensing Image Fusion Using Sparse Representation with Sub-dictionaries.International Journal of Remote Sensing, 2017, 38 (12) :3564-3585.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201304042&amp;v=MjEwNjJYVGJMRzRIOUxNcTQ5QlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emdWYjNJSWo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> 尹雯, 李元祥.基于稀疏表示的遥感影像融合方法.光学学报, 2013, 33 (4) .DOI:10.3788/AOS201333.0428003. (YIN W, LI Y X.Remote Sensing Image Fusion Based on Sparse Representation.Acta Optica Sinica, 2013, 33 (4) .DOI:10.3788/AOS201333.0428003.) 
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJHD&amp;filename=SJHD15040900000338&amp;v=MTczMzNGc1NhQkk9TmlmRGFySzlIdFhNcG85RlpPc1BEMzh4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> ASGHARI M H, JALALI B.Edge Detection in Digital Images Using Dispersive Phase Stretch Transform.International Journal of Biomedical Imaging, 2015.DOI:10.1155/2015/687819.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" >
                                    <b>[21]</b>
                                 ILOVITSH T, JALALI B, ASGHARI M H, <i>et al</i>.Phase Stretch Transform for Super-Resolution Localization Microscopy.Biomedi-cal Optics Express, 2016, 7 (10) :4198-4209.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" >
                                    <b>[22]</b>
                                 AHARON M, ELAD M, BRUCKSTEIN A.K-SVD:An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation.IEEE Transactions on Image Processing, 2006, 54 (11) :4311-4322.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" >
                                    <b>[23]</b>
                                 PATI Y C, REZAIIFAR R, KRISHNAPRASAD P S.Orthogonal Matching Pursuit:Recursive Function Approximation with Applications to Wavelet Decomposition // Proc of the 27th Asilomar Conference on Signals, Systems and Computers.Berlin, Germany:Springer, 1993:40-44.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" >
                                    <b>[24]</b>
                                 OJHA C, FUSCO A, MANUNTA M.Denoising of Full Resolution Differential SAR Interferogram Based on K-SVD Technique // Proc of the IEEE International Geoscience and Remote Sensing Symposium.Washington, USA:IEEE, 2015:2461-2464.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" >
                                    <b>[25]</b>
                                 MARUTURI H, HIMA B C, SATYA P K.Image Fusion with Biorthogonal Wavelet Transform Based on Maximum Selection and Region Energy // Proc of the IEEE Conference on Computer Communication and Informatics.Washington, USA:IEEE, 2014.DOI:10.1109/ICCCI.2014.6921720.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_26" >
                                    <b>[26]</b>
                                 KAUR H, RANI J.Image Fusion on Digital Images Using Lapla-cian Pyramid with DWT // Proc of the 3rd International Conference on Image Information Processing.Berlin, Germany:Springer, 2015:393-398.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_27" >
                                    <b>[27]</b>
                                 MANU C S, JIJI C V.A Novel Remote Sensing Image Fusion Algorithm Using ICA Bases // Proc of the 8th IEEE International Conference on Advances in Pattern Recognition.Washington, USA:IEEE, 2015.DOI:10.1109/ICAPR.2015.7050690.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_28" >
                                    <b>[28]</b>
                                 LIU Y, WANG Z F.Simultaneous Image Fusion and Denoising with Adaptive Sparse Representation.IET Image Processing, 2015, 9 (5) :347-357.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_29" >
                                    <b>[29]</b>
                                 LI H, WU X J, KITTLER J.Infrared and Visible Image Fusion Using a Deep Learning Framework[C/OL].[2018-08-20].https://arxiv.org/pdf/1804.06992.pdf.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201905002" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201905002&amp;v=MDM5MzdCdEdGckNVUkxPZVplUm5GeXpnVmIzSktEN1liTEc0SDlqTXFvOUZab1FLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
