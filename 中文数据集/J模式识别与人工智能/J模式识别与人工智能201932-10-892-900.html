<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131460985498750%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dMSSB201910003%26RESULT%3d1%26SIGN%3dA6gBwvd%252fiqv5QKILhMVFs9ezxsw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201910003&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201910003&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201910003&amp;v=MzIzOTFuRnkva1dyL0JLRDdZYkxHNEg5ak5yNDlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#215" data-title="1 基于注意力机制的时间分组深度网络行为识别算法 ">1 基于注意力机制的时间分组深度网络行为识别算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#216" data-title="&lt;b&gt;1.1&lt;/b&gt; 模型结构"><b>1.1</b> 模型结构</a></li>
                                                <li><a href="#224" data-title="&lt;b&gt;1.2&lt;/b&gt; 视频分组稀疏抽样策略"><b>1.2</b> 视频分组稀疏抽样策略</a></li>
                                                <li><a href="#236" data-title="&lt;b&gt;1.3&lt;/b&gt; 通道注意力映射模块"><b>1.3</b> 通道注意力映射模块</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#261" data-title="2 实验及结果分析 ">2 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#264" data-title="&lt;b&gt;2.1&lt;/b&gt; 实验设置"><b>2.1</b> 实验设置</a></li>
                                                <li><a href="#266" data-title="&lt;b&gt;2.2&lt;/b&gt; 实验结果对比"><b>2.2</b> 实验结果对比</a></li>
                                                <li><a href="#272" data-title="&lt;b&gt;2.3&lt;/b&gt; 消融实验"><b>2.3</b> 消融实验</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#282" data-title="3 结 束 语 ">3 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#223" data-title="图1 本文算法整体结构图">图1 本文算法整体结构图</a></li>
                                                <li><a href="#241" data-title="图2 SE通道注意力模块">图2 SE通道注意力模块</a></li>
                                                <li><a href="#250" data-title="图3 将SE模块嵌入到ResNet中组成SE-ResNet模块的示意图">图3 将SE模块嵌入到ResNet中组成SE-ResNet模块的示意图</a></li>
                                                <li><a href="#317" data-title="图4 Grad-CAM可视化">图4 Grad-CAM可视化</a></li>
                                                <li><a href="#317" data-title="图4 Grad-CAM可视化">图4 Grad-CAM可视化</a></li>
                                                <li><a href="#271" data-title="&lt;b&gt;表1 各算法的性能对比&lt;/b&gt;"><b>表1 各算法的性能对比</b></a></li>
                                                <li><a href="#275" data-title="&lt;b&gt;表2 不同主干网络对精度的影响&lt;/b&gt;"><b>表2 不同主干网络对精度的影响</b></a></li>
                                                <li><a href="#279" data-title="&lt;b&gt;表3 抽样策略对行为识别准确率的影响&lt;/b&gt;"><b>表3 抽样策略对行为识别准确率的影响</b></a></li>
                                                <li><a href="#281" data-title="&lt;b&gt;表4 不同片段融合方式对准确率的影响&lt;/b&gt;"><b>表4 不同片段融合方式对准确率的影响</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="318">


                                    <a id="bibliography_1" title=" WANG H,SCHIMID C.Action Recognition with Improved Trajectories // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2013:3551-3558." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Action recognition with improved trajectories">
                                        <b>[1]</b>
                                         WANG H,SCHIMID C.Action Recognition with Improved Trajectories // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2013:3551-3558.
                                    </a>
                                </li>
                                <li id="320">


                                    <a id="bibliography_2" title=" SANDLER M,HOWARD A,ZHU M L,et al.Mobilenetv2:Inverted Residuals and Linear Bottlenecks // Proc of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:4510-4520." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mobilenetv2:Inverted residuals and linear bottlenecks">
                                        <b>[2]</b>
                                         SANDLER M,HOWARD A,ZHU M L,et al.Mobilenetv2:Inverted Residuals and Linear Bottlenecks // Proc of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:4510-4520.
                                    </a>
                                </li>
                                <li id="322">


                                    <a id="bibliography_3" title=" ZHANG X Y,ZHOU X Y,LIN M X,et al.Shufflenet:An Extremely Efficient Convolutional Neural Network for Mobile Devices // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:6848-6856." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shuffle Net an extremely efficient convolutional neural network for mobile devices">
                                        <b>[3]</b>
                                         ZHANG X Y,ZHOU X Y,LIN M X,et al.Shufflenet:An Extremely Efficient Convolutional Neural Network for Mobile Devices // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:6848-6856.
                                    </a>
                                </li>
                                <li id="324">


                                    <a id="bibliography_4" title=" HE K M,ZHANG X Y,REN S Q,et al.Identity Mappings in Deep Residual Networks // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2016:630-645." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">
                                        <b>[4]</b>
                                         HE K M,ZHANG X Y,REN S Q,et al.Identity Mappings in Deep Residual Networks // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2016:630-645.
                                    </a>
                                </li>
                                <li id="326">


                                    <a id="bibliography_5" title=" WANG X L,GIRSHICK R,GUPTA A,et al.Non-local Neural Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:7794-7803." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Non-local neural networks">
                                        <b>[5]</b>
                                         WANG X L,GIRSHICK R,GUPTA A,et al.Non-local Neural Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:7794-7803.
                                    </a>
                                </li>
                                <li id="328">


                                    <a id="bibliography_6" title=" HUANG G,LIU Z,VAN DER MAATEN L,et al.Densely Connected Convolutional Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2017:2261-2269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DenselyConnected Convolutional Networks">
                                        <b>[6]</b>
                                         HUANG G,LIU Z,VAN DER MAATEN L,et al.Densely Connected Convolutional Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2017:2261-2269.
                                    </a>
                                </li>
                                <li id="330">


                                    <a id="bibliography_7" title=" KARPATHY A,TODERICI G,SHETTY S,et al.Large-Scale Vi-deo Classification with Convolutional Neural Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2014:1725-1732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Largescale video classification with convolutional neural networks">
                                        <b>[7]</b>
                                         KARPATHY A,TODERICI G,SHETTY S,et al.Large-Scale Vi-deo Classification with Convolutional Neural Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2014:1725-1732.
                                    </a>
                                </li>
                                <li id="332">


                                    <a id="bibliography_8" title=" WANG L M,QIAO Y,TANG X O.Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:4305-4314." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Action recognition with trajectory-pooled deepconvolutional descriptors">
                                        <b>[8]</b>
                                         WANG L M,QIAO Y,TANG X O.Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:4305-4314.
                                    </a>
                                </li>
                                <li id="334">


                                    <a id="bibliography_9" title=" SIMONYAN K,ZISSERMAN A.Two-Stream Convolutional Networks for Action Recognition in Videos // Proc of the 27th International Conference on Neural Information Processing Systems.Cambridge,USA:The MIT Press,2014,I:568-576." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Two-stream convolutional networks for action recognition in videos">
                                        <b>[9]</b>
                                         SIMONYAN K,ZISSERMAN A.Two-Stream Convolutional Networks for Action Recognition in Videos // Proc of the 27th International Conference on Neural Information Processing Systems.Cambridge,USA:The MIT Press,2014,I:568-576.
                                    </a>
                                </li>
                                <li id="336">


                                    <a id="bibliography_10" title=" NG J Y H,HAUSKNECHT M,VIJAYANARASIMHAN S,et al.Beyond Short Snippets:Deep Networks for Video Classification // Proc of the IEEE Conference on Computer Vision and Pattern Re-cognition.Washington,USA:IEEE,2015:4694-4702." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond short snippets:deep networks for video classification">
                                        <b>[10]</b>
                                         NG J Y H,HAUSKNECHT M,VIJAYANARASIMHAN S,et al.Beyond Short Snippets:Deep Networks for Video Classification // Proc of the IEEE Conference on Computer Vision and Pattern Re-cognition.Washington,USA:IEEE,2015:4694-4702.
                                    </a>
                                </li>
                                <li id="338">


                                    <a id="bibliography_11" title=" TRAN D,BOURDEV L,FERGUS R,et al.Learning Spatiotemporal Features with 3D Convolutional Networks // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2015:4489-4497." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatiotemporal features with 3D convolutional networks">
                                        <b>[11]</b>
                                         TRAN D,BOURDEV L,FERGUS R,et al.Learning Spatiotemporal Features with 3D Convolutional Networks // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2015:4489-4497.
                                    </a>
                                </li>
                                <li id="340">


                                    <a id="bibliography_12" title=" SIMONYAN K,ZISSERMAN A.Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL].[2019-05-20].https://arxiv.org/pdf/1409.1556.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[12]</b>
                                         SIMONYAN K,ZISSERMAN A.Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL].[2019-05-20].https://arxiv.org/pdf/1409.1556.pdf.
                                    </a>
                                </li>
                                <li id="342">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     HE K M,ZHANG X Y,REN S Q,et al.Deep Residual Learning for Image Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:770-778.</a>
                                </li>
                                <li id="344">


                                    <a id="bibliography_14" title=" SZEGEDY C,LIU W,JIA Y Q,et al.Going Deeper with Convolutions // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[14]</b>
                                         SZEGEDY C,LIU W,JIA Y Q,et al.Going Deeper with Convolutions // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:1-9.
                                    </a>
                                </li>
                                <li id="346">


                                    <a id="bibliography_15" title=" XIE S N,GIRSHICK R,DOLL&#193;R P,et al.Aggregated Residual Transformations for Deep Neural Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2017:5987-5995." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Aggregated Residual Transformations for Deep Neural Networks">
                                        <b>[15]</b>
                                         XIE S N,GIRSHICK R,DOLL&#193;R P,et al.Aggregated Residual Transformations for Deep Neural Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2017:5987-5995.
                                    </a>
                                </li>
                                <li id="348">


                                    <a id="bibliography_16" title=" HU J,SHEN L,ALBANIE S,et al.Squeeze-and-Excitation Networks // Proc of the IEEE Conference on Computer Vision and Pa-ttern Recognition.Washington,USA:IEEE,2018:7132-7141." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Squeeze-and-Excitation Networks">
                                        <b>[16]</b>
                                         HU J,SHEN L,ALBANIE S,et al.Squeeze-and-Excitation Networks // Proc of the IEEE Conference on Computer Vision and Pa-ttern Recognition.Washington,USA:IEEE,2018:7132-7141.
                                    </a>
                                </li>
                                <li id="350">


                                    <a id="bibliography_17" title=" MA C Y,CHEN M H,KIRA Z,et al.TS-LSTM and Temporal-Inception:Exploiting Spatiotemporal Dynamics for Activity Recognition.Signal Processing(Image Communication),2019,71:76-87." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7D5AADFAD3177DC7B754CAB0C800827F&amp;v=MTQ2MDRZZk9HUWxmQ3BiUTM1TkZodzdtMndLQT1OaWZPZmJUTUc2QzkyL2swRU9nT0MzdE52QkZoN1RwNU93NlFyR0U5ZWJLY1I3M3BDT052RlNpV1dyN0pJRnBtYUJ1SA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         MA C Y,CHEN M H,KIRA Z,et al.TS-LSTM and Temporal-Inception:Exploiting Spatiotemporal Dynamics for Activity Recognition.Signal Processing(Image Communication),2019,71:76-87.
                                    </a>
                                </li>
                                <li id="352">


                                    <a id="bibliography_18" title=" SELVARAJU R R,COGSWELL M,DAS A,et al.Grad-CAM:Visual Explanations from Deep Networks via Gradient-Based Loca-lization // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2017:618-626." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Grad-CAM:visual explanations from deep networks via gradient-based localization">
                                        <b>[18]</b>
                                         SELVARAJU R R,COGSWELL M,DAS A,et al.Grad-CAM:Visual Explanations from Deep Networks via Gradient-Based Loca-lization // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2017:618-626.
                                    </a>
                                </li>
                                <li id="354">


                                    <a id="bibliography_19" title=" KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Imagenet Cla-ssification with Deep Convolutional Neural Networks // Proc of the 25th International Conference on Neural Information Processing Systems.Cambridge,USA:The MIT Press,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net classification with deep convolutional neural networks">
                                        <b>[19]</b>
                                         KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Imagenet Cla-ssification with Deep Convolutional Neural Networks // Proc of the 25th International Conference on Neural Information Processing Systems.Cambridge,USA:The MIT Press,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="356">


                                    <a id="bibliography_20" title=" WU C Y,ZAHEER M,HU H X,et al.Compressed Video Action Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:6026-6035." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Compressed Video Action Recognition">
                                        <b>[20]</b>
                                         WU C Y,ZAHEER M,HU H X,et al.Compressed Video Action Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:6026-6035.
                                    </a>
                                </li>
                                <li id="358">


                                    <a id="bibliography_21" title=" QIU Z F,YAO T,MEI T.Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2017:5533-5541." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatio-temporal representation with pseudo-3D residual networks">
                                        <b>[21]</b>
                                         QIU Z F,YAO T,MEI T.Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2017:5533-5541.
                                    </a>
                                </li>
                                <li id="360">


                                    <a id="bibliography_22" title=" DIBA A,FAYYAZ M,SHARMA V,et al.Temporal 3D ConvNets Using Temporal Transition Layer // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:1230-1234." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Temporal 3D ConvNets Using Temporal Transition Layer">
                                        <b>[22]</b>
                                         DIBA A,FAYYAZ M,SHARMA V,et al.Temporal 3D ConvNets Using Temporal Transition Layer // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:1230-1234.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(10),892-900 DOI:10.16451/j.cnki.issn1003-6059.201910003            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于注意力机制的时间分组深度网络行为识别算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E6%AD%A3%E5%B9%B3&amp;code=09262050&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡正平</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%81%E9%B9%8F%E6%88%90&amp;code=43237363&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刁鹏成</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E7%91%9E%E9%9B%AA&amp;code=43237364&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张瑞雪</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%B7%91%E8%8A%B3&amp;code=22617190&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李淑芳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E6%A2%A6%E7%91%B6&amp;code=43237365&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵梦瑶</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%87%95%E5%B1%B1%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0022354&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">燕山大学信息科学与工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%87%95%E5%B1%B1%E5%A4%A7%E5%AD%A6%E6%B2%B3%E5%8C%97%E7%9C%81%E4%BF%A1%E6%81%AF%E4%BC%A0%E8%BE%93%E4%B8%8E%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">燕山大学河北省信息传输与信号处理重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>受人脑视觉感知机制启发,在深度学习框架下提出基于注意力机制的时间分组深度网络行为识别算法.针对局部时序信息在描述持续时间较长的复杂动作上的不足,使用视频分组稀疏抽样策略,以更低的成本进行视频级时间建模.在识别阶段引入通道注意力映射,进一步利用全局特征信息和捕捉分类兴趣点,执行通道特征重新校准,提高网络的表达能力.实验表明,文中算法在UCF101、HMDB51数据集上的识别准确率较高.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">行为识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *胡正平(通讯作者),博士,教授,主要研究方向为模式识别．E-mail:hzp@ysu.edu.cn.&lt;image id="312" type="formula" href="images/MSSB201910003_31200.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    刁鹏成,硕士研究生,主要研究方向为行为识别．E-mail:948277428@qq.com.&lt;image id="313" type="formula" href="images/MSSB201910003_31300.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    张瑞雪,硕士研究生,主要研究方向为视频分类．E-mail:2625941087@qq.com.&lt;image id="314" type="formula" href="images/MSSB201910003_31400.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    李淑芳,博士研究生,主要研究方向为模式识别．E-mail:lishufang116003@sina.com.cn.&lt;image id="315" type="formula" href="images/MSSB201910003_31500.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    赵梦瑶,博士研究生,主要研究方向为视频异常检测．E-mail:zhaomengyao0826@126.com.&lt;image id="316" type="formula" href="images/MSSB201910003_31600.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-06-17</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金面上项目(No.61771420);</span>
                                <span>河北省自然科学基金项目(No.F2016203422)资助;</span>
                    </p>
            </div>
                    <h1><b>Temporal Group Deep Network Action Recognition Algorithm Based on Attention Mechanism</b></h1>
                    <h2>
                    <span>HU Zhengping</span>
                    <span>DIAO Pengcheng</span>
                    <span>ZHANG Ruixue</span>
                    <span>LI Shufang</span>
                    <span>ZHAO Mengyao</span>
            </h2>
                    <h2>
                    <span>School of Information Science and Engineering,Yanshan University</span>
                    <span>Hebei Key Laboratory of Information Transmission and Signal Processing,Yanshan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Inspired by the mechanism of human visual perception, a temporal group deep network action recognition algorithm based on attention mechanism is proposed under the framework of deep learning. Aiming at the deficiency of local temporal information in describing complex actions with a long duration, the video packet sparse sampling strategy is employed to conduct video level time modeling at a lower cost. In the recognition stage, channel attention mapping is introduced to further utilize global feature information and capture classified interest points, and channel feature recalibration is performed to improve the expression ability of the network. Experimental results on UCF101 and HMDB51 datasets show that the recognition accuracy of the proposed algorithm is high.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Action%20Recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Action Recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Attention&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Attention;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    HU Zhengping(Corresponding author), Ph. D.,professor. His research interests include pattern recognition.;
                                </span>
                                <span>
                                    DIAO Pengcheng,master student. His research interests include action recognition.;
                                </span>
                                <span>
                                    ZHANG Ruixue,master student. Her research interests include video classification.;
                                </span>
                                <span>
                                    LI Shufang,Ph.D. candidate. Her research interests include pattern recognition.;
                                </span>
                                <span>
                                    ZHAO Mengyao,Ph.D. candidate. Her research interests include video anomaly detection.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-06-17</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by General Program of National Natural Science Foundation of China(No.61771420);</span>
                                <span>Natural Science Foundation of Hebei Province(No.F2016203422);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="210">随着信息化程度的提高,视频行为识别算法已成为国内外人工智能领域的研究热点,在安全监控、机器人设计、无人驾驶和智能家庭设计等方面都具有广泛的应用前景.在行为识别中,有两个重要且互补的信息:空间信息和时间信息,识别系统的性能在较大程度上取决于它能否有效提取和利用这些信息.但是,由于姿态、尺度和视角的变化等复杂因素的影响,高效提取这些信息存在困难,因此设计包含视频语义的有效表达形式至关重要.</p>
                </div>
                <div class="p1">
                    <p id="211">视频中人体行为识别本质上是视频分类任务,传统方法主要通过摄像机预处理、帧间特征提取、帧内特征提取、特征编码与特征分类五个过程完成.当前行为识别领域中效果最好的传统方法是改进的密集轨道(Improved Dense Trajectories, iDT)算法<citation id="362" type="reference"><link href="318" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,在特征提取上采用整体表示的时空体积方法,在特征融合上采用Fisher Vector方法,在特征分类上采用径向基核函数支持向量机(Radial Basis Function Support Vector Machines, RBF-SVM),并使用One-Versus-Rest策略训练多分类器.</p>
                </div>
                <div class="p1">
                    <p id="212">虽然iDT在传统算法中性能最好,但相比基于深度学习的行为识别算法,在速度和精度上仍有差距.最近,卷积神经网络(Convolutional Neural Network, CNN)在对场景、对象和复杂事件的图像分类方面取得较大进展<citation id="368" type="reference"><link href="320" rel="bibliography" /><link href="322" rel="bibliography" /><link href="324" rel="bibliography" /><link href="326" rel="bibliography" /><link href="328" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>.但CNN在视频分类方面的表现却不佳,这主要是由于长期时间结构对理解视频的动作过程具有重要影响,而主流卷积神经网络框架通常侧重于外观表示,缺乏长期时间建模的能力,因此设计可进行长期时间建模的卷积网络成为解决问题的关注点.Karpathy等<citation id="363" type="reference"><link href="330" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出SlowFusion融合方式,探索在考虑时间连续性的前提下设计网络架构,达到提取运动信息的目的,但在Sports-1M数据集上的低准确率表明单流卷积网络难以较好捕捉运动信息.Wang等<citation id="364" type="reference"><link href="332" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>结合手动特征和深度学习的方法,使用轨迹汇集深度卷积描述符,将iDT中的传统特征替换成深度学习的特征.该方法虽获得一定的性能提升,但由于依赖Fisher Vector编码,通常会产生非常高维的特征向量,在存储和计算等方面效率较差.Simonyan等<citation id="365" type="reference"><link href="334" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>考虑到卷积网络学习运动特征的困难,以叠加光流矢量的形式对运动特征进行显式建模,设计两个独立的空间和时间卷积神经网络,输入分别为RGB帧图像和光流图,在分别提取外观特征信息和运动特征信息后进行融合.相比单流方法,虽然双流方法性能有较大提升,但由于作为输入的帧图像和光流图仅能表示短期时间内的特征信息,难以进行长期时间建模.Ng等<citation id="366" type="reference"><link href="336" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>使用长短期记忆网络(Long-Short Term Memory, LTSM)进行双流框架中的时域信息融合,效果一般,说明即使LSTM也难以从原始帧序列中建模复杂的运动特征.Tran等<citation id="367" type="reference"><link href="338" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出三维卷积(Convolutional 3D, C3D),通过在大规模视频数据集上进行3D卷积操作学习外观和运动特征,虽然理论上可以通过加大抽样时长进行长期时间建模,但由于硬件计算能力的限制而难以实现.</p>
                </div>
                <div class="p1">
                    <p id="213">上述方法主要有3个缺陷:1)依赖于针对预先定义的采样间隔的密集时间采样,应用于长视频序列时会产生过高的计算成本,限制实际应用.2)通常直接处理16帧至64帧不等的固定长度的序列,由于时间覆盖有限,难以对整段视频进行时间建模,对于超过最大序列长度的视频,有丢失重要信息的风险.3)训练深层卷积神经网络需要大量的学习样本以获得最佳性能,然而,由于数据收集和注释方面的困难,公开可用的行动识别数据集(如UCF101、HMDB51数据集)在规模和多样性方面仍然有限,因此,在图像分类方面取得较大成功的深度卷积网络面临过拟合的风险.</p>
                </div>
                <div class="p1">
                    <p id="214">考虑到视频信息的大量帧间冗余,使用视频分组稀疏采样对视频长时间结构进行建模,为此本文提出基于注意力机制的时间分组行为识别算法.针对CNN难以提取运动信息的问题,使用光流矢量图代表运动信息,对时间特征信息进行显式建模.针对视频长时间结构建模的问题,使用视频分组稀疏采样组成片段序列,序列中每个片段生成自己对行为类别的初步预测后进行分数融合.在学习过程中,通过迭代更新模型参数优化视频级预测的损失值,使模型不受序列长度的限制,有效地对整个视频进行长期时间建模.同时,使用通道注意力映射对通道信息进行注意力建模,提高网络的表达能力.</p>
                </div>
                <h3 id="215" name="215" class="anchor-tag">1 基于注意力机制的时间分组深度网络行为识别算法</h3>
                <h4 class="anchor-tag" id="216" name="216"><b>1.1</b> 模型结构</h4>
                <div class="p1">
                    <p id="217">行为识别问题主要可分为短时序动作和长时序动作两类.对于短时序动作,多数情况下仅利用空间信息就可以大致判断出行为的类别.短时序动作的识别难点在于是否能够避免复杂背景、尺度变化和视角变化等干扰噪声的影响,准确识别用于判决的关键物体.针对该问题,本文引入注意力机制对特征(通道)信息进行建模,在增强重要特征信息的同时抑制次要特征信息,使模型识别更精准.</p>
                </div>
                <div class="p1">
                    <p id="218">对于时间跨度较大的长时序动作,仅凭单帧的空间信息难以满足正确识别的需求.长时序动作的识别难点在于如何合理利用输入视频的时序信息,尤其是长期时序信息.针对该问题,本文采用时间分组稀疏采样策略,对整段视频进行分组稀疏抽样,在有效进行长期时间建模的同时减少视频时间维度信息的冗余度.</p>
                </div>
                <div class="p1">
                    <p id="219">本文通过结合通道注意力映射模块和时间分组稀疏抽样策略,增强模型的空间建模和时间建模能力,对短时序动作和长时序动作都能保持较高的敏感度.</p>
                </div>
                <div class="p1">
                    <p id="220">基于注意力机制的时间分段深度网络行为识别算法整体结构如图1所示,分为采样和识别两个阶段.</p>
                </div>
                <div class="p1">
                    <p id="221">1)采样阶段.在某种程度上,由于连续视频帧中提取的视觉特征不会发生显著变化,因此密集的时间采样通常会导致相邻采样帧之间存在较大信息冗余.为此稀疏时间采样策略可去除冗余信息,同时降低计算量.基于此思路,对视频进行等时长分组后,使用稀疏采样方案在长视频序列上沿时间维度提取短片段.具体来说,就是将一个完整视频分为<i>n</i>段,在每段中随机抽样出1帧RGB图像和1组光流图,用作网络的输入进行训练,建立一个长期时间结构模型.</p>
                </div>
                <div class="p1">
                    <p id="222">2)识别阶段.首先,使用抽样得到的RGB帧图像和光流图像分别输入空间和时间网络进行训练,并分别对多个视频分段对应的预测结果(片段分数)进行融合(平均处理),得到空间判决分数和时间判决分数,取平均值作为输出判决分数,完成行为识别.</p>
                </div>
                <div class="area_img" id="223">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910003_223.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文算法整体结构图" src="Detail/GetImg?filename=images/MSSB201910003_223.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文算法整体结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910003_223.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Overall structure of the proposed algorithm</p>

                </div>
                <h4 class="anchor-tag" id="224" name="224"><b>1.2</b> 视频分组稀疏抽样策略</h4>
                <div class="p1">
                    <p id="225">时间建模分为短期建模和长期建模,长期建模在面对一些跨越时间较长的复杂动作时表现更优.现有双流网络面临的主要问题是难以对长时间结构进行建模,这是由于设计仅在单个帧(空间网络)或短片段(时间网络)上操作,导致时间覆盖有限.然而,对于一些跨越时间较长的复杂动作,由于CNN难以进行长期时间建模,丢失部分重要特征信息,导致识别性能下降.为了解决此问题,使用视频分组稀疏抽样,可以在不受序列长度限制的情况下有效地对整个视频进行长期时间建模.</p>
                </div>
                <div class="p1">
                    <p id="226">对视频<i>V</i>进行等时长分组,分组数为<i>K</i>,即将视频划分为<i>K</i>个等时长的片段{<i>S</i><sub>1</sub>,<i>S</i><sub>2</sub>,…,<i>S</i><sub><i>K</i></sub>}.然后,对一系列片段进行建模:</p>
                </div>
                <div class="p1">
                    <p id="227" class="code-formula">
                        <mathml id="227"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>Μ</mi><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>Τ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>Τ</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mi>Η</mi><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">(</mo><mi>F</mi><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>;</mo><mi>W</mi><mo stretchy="false">)</mo><mo>,</mo><mi>F</mi><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>;</mo><mi>W</mi><mo stretchy="false">)</mo><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>F</mi><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo>;</mo><mi>W</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="228">其中:(<i>T</i><sub>1</sub>,<i>T</i><sub>2</sub>,…,<i>T</i><sub><i>K</i></sub>)为一个片段序列,每个片段<i>T</i><sub><i>K</i></sub>都是从其对应的分段<i>S</i><sub><i>K</i></sub>中随机抽取;<i>F</i>(<i>T</i><sub><i>K</i></sub>;<i>W</i>)为采用<i>W</i>作为参数的卷积网络作用于短片段<i>T</i><sub><i>K</i></sub>时返回的相对于所有类别的片段分数;段共识函数<i>g</i>(平均函数)用于将多个片段级判决分数聚合生成视频级判决分数.最后,使用预测函数<i>H</i>(Softmax函数)预测整个视频对应每个动作类的概率,概率最高的类别为模型预测该视频所属的类别.</p>
                </div>
                <div class="p1">
                    <p id="229">需要注意的是,<i>K</i>个分组之间参数共享,在输入<i>H</i>函数之前合并视频级的空间和时间判决结果,结合标准分类交叉熵损失,最终损失函数为</p>
                </div>
                <div class="p1">
                    <p id="230" class="code-formula">
                        <mathml id="230"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>y</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi>y</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mrow><mi>ln</mi></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mrow><mi>exp</mi></mrow></mstyle><mo stretchy="false">(</mo><mi>G</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="231">其中,<i>C</i>为动作类的数量,<i>y</i><sub><i>i</i></sub>为第<i>i</i>类的真实标签.在实验中,片段数<i>K</i>=3,共识函数</p>
                </div>
                <div class="p1">
                    <p id="232"><i>G</i>=<i>g</i>(<i>F</i>(<i>T</i><sub>1</sub>;<i>W</i>),<i>F</i>(<i>T</i><sub>2</sub>;<i>W</i>),…,<i>F</i>(<i>T</i><sub><i>K</i></sub>;<i>W</i>)).</p>
                </div>
                <div class="p1">
                    <p id="233">使用标准反向传播算法,利用多个片段共同优化模型参数<i>W</i>,在反向传播过程中,模型参数的梯度为</p>
                </div>
                <div class="p1">
                    <p id="234" class="code-formula">
                        <mathml id="234"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi><mo stretchy="false">(</mo><mi>y</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>W</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi>G</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mo stretchy="false">(</mo></mstyle><mfrac><mrow><mo>∂</mo><mi>g</mi></mrow><mrow><mo>∂</mo><mi>F</mi><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mfrac><mrow><mo>∂</mo><mi>F</mi><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>W</mi></mrow></mfrac><mo stretchy="false">)</mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="235">在使用随机梯度下降优化算法学习模型参数时,上式保证参数更新是利用从所有片段级预测得到的分数,可以从整个视频中学习模型参数.同时,相比之前使用密集采样帧的工作,视频分组稀疏抽样降低CNN在视频级别进行动作识别的计算成本.</p>
                </div>
                <h4 class="anchor-tag" id="236" name="236"><b>1.3</b> 通道注意力映射模块</h4>
                <div class="p1">
                    <p id="237">CNN丰富的特征抽取能力推动视觉学习任务的进展.CNN由一系列卷积层、非线性层和池化层构成,其中卷积层作为CNN的核心组件,在特征维度(通道)上使用局部感受野对空间信息进行聚合.随着网络深度的增加,整个网络可以从全局感受野获取图像特征以进行图像的描述.</p>
                </div>
                <div class="p1">
                    <p id="238">为了提高CNN的性能,研究者主要从网络的深度、宽度、基数展开研究.深层卷积网络(Very Deep Convolutional Networks, VGG Net)<citation id="369" type="reference"><link href="340" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>表明,随着深度的增加,具有相同形状的堆叠块堆叠效果良好.为了解决训练深层网络带来的梯度弥漫/爆炸问题,残差网络(Residual Network, ResNet)<citation id="370" type="reference"><link href="342" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>通过残差连接进行特征复用,降低网络优化难度,使训练极深架构成为可能.GoogLeNet<citation id="371" type="reference"><link href="344" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>通过堆叠1×1、3×3、5×5的卷积核,增加网络对尺度的适应性,表明宽度是提高模型性能的另一个重要因素.分组残差网络(Residual X Network, ResNeXt)<citation id="372" type="reference"><link href="346" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出基数的概念,通过增加分组卷积的组数,在处理通道时,像处理长宽一样,只考虑局部信息,最后融合这些局部信息.已有研究表明,一定程度上增加网络的基数不仅可以节省参数,而且比深度和宽度这两个因素具有更强的影响力.</p>
                </div>
                <div class="p1">
                    <p id="239">受人类注意力感知机制启发,本文还考虑特征间的注意力机制.注意力机制本质上是为了模仿人脑收集信息的方式.一般来说,人脑在进行图像识别时,除了从全局把握该图像外,还更关注图像中的某个局部信息.人脑的注意力机制本质上主要包含两点:注意力机制需要判断输入信息中哪部分需要特殊关注和从关键的部分进行特征提取以得到重要的信息.本文针对这两部分操作分别设计注意力模块中的压缩和激励操作,实现全局和局部信息的综合利用.</p>
                </div>
                <div class="p1">
                    <p id="240">卷积运算仅在2D空间中操作,只能建模图像的空间信息,不能建模特征(通道)之间的信息.针对该问题,本文引入压缩和激励网络(Squeeze and Excitation Network, SE Net)<citation id="373" type="reference"><link href="348" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>,对通道特征信息进行自适应注意力建模,通过学习的方式自动获取每个特征(通道)之间的重要程度,增强重要特征并抑制次要特征,由传统卷积仅能在空间维度上判断“是什么”,改进为先在特征维度上判断“在哪里”,再在空间维度上判断“是什么”.SE模块结构如图2所示.</p>
                </div>
                <div class="area_img" id="241">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910003_241.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 SE通道注意力模块" src="Detail/GetImg?filename=images/MSSB201910003_241.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 SE通道注意力模块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910003_241.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 SE channel attention module</p>

                </div>
                <div class="p1">
                    <p id="242">对于卷积层中给定的变换<i>X</i>→<i>U</i>,可以通过执行SE通道注意力模块进行特征重新校准,其中,<i>X</i>为输入特征图谱,<i>U</i>为输出特征图谱.具体通过如下3个操作重标定前面得到的特征.1)压缩操作.常规卷积核仅能在局部感受野内进行操作,虽然卷积层的堆叠可以增大感受野,但如果能在浅层引入全局信息,就会为深层带去更丰富的特征信息,从而提高性能.为了引入全局信息,沿空间维度使用全局均值池化进行特征压缩,将每个二维的特征通道图谱映射为一个实数,输出维度和输入的特征通道数相匹配.这个实数在某种程度上表示其对应通道特征响应的全局分布,使靠近输入的层也可以获得全局的感受野.2)激励操作.类似于循环神经网络中门的机制<citation id="374" type="reference"><link href="350" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>,为了合理利用压缩操作得到的全局信息,将其输入至多层感知机(Multi-layer Perception, MLP)中学习得到权重参数<i>W</i>,为每个特征通道生成对应的重要程度,其中<i>W</i>用于学习显式建模特征通道间的相关性.再通过一个Sigmoid门函数获得 0～1 之间归一化的权重<i>M</i><sub><i>C</i></sub>(<i>U</i>).3)特征重定向的操作.将激励操作输出的权重<i>M</i><sub><i>C</i></sub>(<i>U</i>)通过乘法逐通道加权到先前的特征上,完成在通道维度上对原始特征的重标定.简而言之,通道注意力映射(压缩和激励)的计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="243"><i>M</i><sub><i>C</i></sub>(<i><b>U</b></i>)=<i>σ</i>(<i>MLP</i>(<i>AvgPool</i>(<i><b>U</b></i>)))=<i>σ</i>(<i>W</i>(<i><b>U</b></i><mathml id="284"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>A</mtext><mtext>v</mtext><mtext>g</mtext></mrow><mi>C</mi></msubsup></mrow></math></mathml>)),</p>
                </div>
                <div class="p1">
                    <p id="244">其中,<i>σ</i>表示Sigmoid激活函数,<i>MLP</i>表示多层感知机,<i>AvgPool</i>表示沿着通道维度进行空间全局均值池化,<i><b>U</b></i>∈<b>R</b><sup><i>C</i></sup><sup>×</sup><sup><i>H</i></sup><sup>×</sup><sup><i>W</i></sup>表示注意力模块的输入特征图谱,<i>M</i><sub><i>C</i></sub>(<i><b>U</b></i>)∈<b>R</b><sup><i>C</i></sup><sup>×1×1</sup>表示生成的注意力映射.压缩和激励操作完成后,需要对通道特征信息进行重标定,计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="245" class="code-formula">
                        <mathml id="245"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo>=</mo><mi mathvariant="bold-italic">U</mi><mo>⊗</mo><mi>Μ</mi><msub><mrow></mrow><mi>C</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">U</mi><mo stretchy="false">)</mo><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="246">其中,⨂表示逐元素乘法,<mathml id="285"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mrow><mi>C</mi><mo>×</mo><mi>Η</mi><mo>×</mo><mi>W</mi></mrow></msup></mrow></math></mathml>表示注意力模块的输出特征图谱.至此,注意力模块的全部数学运算均已执行完毕.</p>
                </div>
                <div class="p1">
                    <p id="247">图3为将SE模块嵌入到ResNet中组成SE-ResNet模块的示意图,方框旁边的维度信息表示该层输出.首先将初始输入特征信息沿着通道维度进行全局均值池化处理,完成特征压缩.然后通过两个全连接层组建的瓶颈结构建模通道依赖关系,第一个全连接层将输入通道维度降低为原来的1/<i>r</i>(本文中<i>r</i>=16)以降低参数量和计算开销,第二个全连接层再将通道维度恢复至初始水平.相比一层全连接层,这样设置的优点是在降低计算开销的同时增加更高阶的非线性,增强网络的表达能力.再通过一个Sigmoid门函数获得 0～1 之间归一化的权重,最后通过一个特征重定向的操作将归一化后的权重加权到每个通道的特征上.</p>
                </div>
                <div class="p1">
                    <p id="248">在行为识别任务中,有效的特征信息是进行准确分类的前提,CNN作为关键的特征提取组件,性能的优劣在较大程度上影响识别性能.双流卷积神经网络使用相对较浅的BN-inception结构,本文设计性能更优的SE-Net-101作为主干网络架构,可显式地建模特征通道之间的相互依赖关系,通过学习的方式自动获取每个特征通道的重要程度,在增强重要特征信息的同时抑制次要特征信息.</p>
                </div>
                <div class="p1">
                    <p id="249">为了凸显注意力机制带来的性能提升,使用梯度加权类激活映射(Gradient-Weighted Class Activa-tion Mapping, Grad-CAM)技术<citation id="375" type="reference"><link href="352" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,选用UCF101数据集中的6类样本进行可视化分析,类别激活映射可以使用热力图的形式表达模型通过哪些像素点判断图像所属类别,结果如图4所示.</p>
                </div>
                <div class="area_img" id="250">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910003_25000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 将SE模块嵌入到ResNet中组成SE-ResNet模块的示意图" src="Detail/GetImg?filename=images/MSSB201910003_25000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 将SE模块嵌入到ResNet中组成SE-ResNet模块的示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910003_25000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Illustration of embedding SE module into ResNet to form SE-ResNet module</p>

                </div>
                <div class="p1">
                    <p id="252">由图4可知,相比BN Inception和ResNet-101,SE-ResNet-101具有较大优势,总能找到正确识别分类的关键位置.例如在涂唇膏动作中:BN Inception的激活映射在头部和嘴部都有覆盖,判决位置不够准确;ResNet-101的激活映射覆盖区域相对集中,但覆盖面积较大,判决位置不够精确;SE-ResNet-101的激活映射覆盖区域更精准.图4中其它动作类别也得到相同结论,由此也表达注意力机制对卷积网络识别性能的影响程度.</p>
                </div>
                <div class="area_img" id="317">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910003_31700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Grad-CAM可视化" src="Detail/GetImg?filename=images/MSSB201910003_31700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 Grad-CAM可视化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910003_31700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Grad-CAM visualization</p>

                </div>
                <div class="area_img" id="317">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910003_31701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Grad-CAM可视化" src="Detail/GetImg?filename=images/MSSB201910003_31701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 Grad-CAM可视化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910003_31701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Grad-CAM visualization</p>

                </div>
                <h3 id="261" name="261" class="anchor-tag">2 实验及结果分析</h3>
                <div class="p1">
                    <p id="262">实验环境为Pytorch 0.4.实验平台设置为Intel(R) Core i7-7800X处理器(六核心,3.5 GHz),4块NVIDIA GTX 1080Ti显卡(显存为11 GB),Ubuntu 16.04操作系统.</p>
                </div>
                <div class="p1">
                    <p id="263">实验数据集包括UCF101数据集和HMDB51数据集.UCF101数据集主要来源于体育运动,包含共计101类人体行为的13 320个视频.HMDB51数据集主要来源于电影和网络视频,共计51类人体行为的6 849段样本.对于这2个数据集,官方都提供3种训练集/验证集划分方式,本文使用3种划分方式的平均结果作为最终结果.</p>
                </div>
                <h4 class="anchor-tag" id="264" name="264"><b>2.1</b> 实验设置</h4>
                <div class="p1">
                    <p id="265">采用随机裁剪方式将图像大小调整为224×224,使用水平随机翻转(概率为0.5)进行数据增强,再送入网络.使用在ImageNet上预训练<citation id="376" type="reference"><link href="354" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>的权重在UCF101、HMDB51数据集上进行微调,初始学习率为0.001,当连续10个批次损失函数不再下降时将学习率降至原来的1/10,整个训练过程在60轮训练后停止.采用交叉熵损失函数作为优化标准,批处理大小(Batch Size)设置为64,使用权重衰减为5e<sup>-4</sup>和动量为0.9的小批量随机梯度下降(Mini-Batch Stochastic Gradient Descent, mini-batch SGD)优化器对网络参数进行更新.</p>
                </div>
                <h4 class="anchor-tag" id="266" name="266"><b>2.2</b> 实验结果对比</h4>
                <div class="p1">
                    <p id="267">使用视频分组稀疏抽样策略(分组数为3),主干网络选择SE-ResNet-101,片段融合方式选择平均融合.对比算法如下:改进的密集轨迹和混合超向量(Improved Dense Trajectories and Hybrid Super Vector, iDT+HSV)<citation id="377" type="reference"><link href="318" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,双流(Two Stream)<citation id="378" type="reference"><link href="334" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>,C3D<citation id="379" type="reference"><link href="338" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>,压缩视频的行为识别(Compressed Video Action Re-cognition, CoViAR)<citation id="380" type="reference"><link href="356" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>,伪三维(Pseudo-3D, P3D)<citation id="381" type="reference"><link href="358" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>,时间三维(Temporal 3D, T3D)<citation id="382" type="reference"><link href="360" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="268">各算法性能对比如表1所示.由表可知,iDT+HSV虽在传统算法中表现良好,但与基于深度学习的算法相比仍具有明显差距.Two Stream作为经典的双流算法,比传统算法具有更好的性能指标,但由于其仅能进行短期时序建模且主干网络性能较落后,导致准确率受到限制.C3D、P3D和T3D采用3D卷积方式对时空信息同时进行建模,性能获得一定提升,但占用资源较多,由于输入片段长度的限制,难以对整段视频进行长期时序建模.本文算法在UCF101、HMDB51数据集上均具有更高的准确率,长期时间建模和注意力机制的优势得以显现.</p>
                </div>
                <div class="p1">
                    <p id="269">此外,为了凸显本文算法在计算成本上的优势,还在帧/片段级计算量和视频级计算量上进行对比.</p>
                </div>
                <div class="p1">
                    <p id="270">为了简洁起见,帧/片段级计算量和视频级计算量都是以批处理大小为1为前提计算的空间网络计算量,假设完整视频帧数为80.对于2D卷积模型,整个视频需要进行逐帧判别,模型输入为80帧图像.对于3D卷积模型,输入模型的单个视频片段长度为16帧,模型输入为5个视频片段.由表1可知,本文算法在帧/片段级计算量上不是最优,但对比除Two Stream之外的其它算法仍具有一定优势.在视频级计算量上,得益于时间分组稀疏抽样策略,本文算法具有明显优势.</p>
                </div>
                <div class="area_img" id="271">
                    <p class="img_tit"><b>表1 各算法的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Performance comparison of different algorithms</p>
                    <p class="img_note"></p>
                    <table id="271" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="2"><br />准确率/%</td><td rowspan="2">输入尺寸</td><td rowspan="2">帧/片段级计算量/G</td><td rowspan="2">视频级计算量/G</td></tr><tr><td><br />UCF101</td><td>HMDB51</td></tr><tr><td>iDT+HSV</td><td>87.9</td><td>61.1</td><td>-</td><td>-</td><td>-</td></tr><tr><td><br />Two Stream</td><td>88.0</td><td>59.4</td><td>3×224×224</td><td>3.3</td><td>264</td></tr><tr><td><br />CoViAR</td><td>90.4</td><td>59.1</td><td>3×224×224</td><td>4.2</td><td>336</td></tr><tr><td><br />C3D</td><td>85.2</td><td>-</td><td>3×16×112×112</td><td>38.5</td><td>192.5</td></tr><tr><td><br />P3D</td><td>88.6</td><td>-</td><td>3×16×160×160</td><td>18.6</td><td>93</td></tr><tr><td><br />T3D</td><td>91.7</td><td>61.1</td><td>3×16×224×224</td><td>34.5</td><td>172.5</td></tr><tr><td><br />本文算法</td><td>92.1</td><td>64.0</td><td>3×224×224</td><td>7.9</td><td>23.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="272" name="272"><b>2.3</b> 消融实验</h4>
                <div class="p1">
                    <p id="273">为了进一步验证时间分组稀疏抽样策略和通道注意力映射模块两者在识别模型中的相对重要性,将2.2节算法分别与不含有前者和后者的模型进行两组对比实验,并研究片段融合方式.由于不同网络提取特征的能力存在差异,所以网络结构也是识别系统中的另一个影响因素.</p>
                </div>
                <div class="p1">
                    <p id="274">本节使用视频分组稀疏抽样策略(分组数为3),主干网络选择SE-ResNet-101,片段融合方式选择平均融合,对比双流网络中常用的BN-Inception、不含注意力模块的ResNet-101和含有注意力模块的SE-ResNet-101这3个网络的精度,结果见表2.</p>
                </div>
                <div class="area_img" id="275">
                    <p class="img_tit"><b>表2 不同主干网络对精度的影响</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Effect of different backbone networks on accuracy </p>
                    <p class="img_note"></p>
                    <table id="275" border="1"><tr><td rowspan="2"><br />主干网络</td><td colspan="3"><br />UCF101</td><td colspan="3"><br />HMDB51</td></tr><tr><td><br />空间网络</td><td>时间网络</td><td>双流融合</td><td><br />空间网络</td><td>时间网络</td><td>双流融合</td></tr><tr><td>BN-Inception</td><td>84.2</td><td>86.3</td><td>89.1</td><td>54.3</td><td>56.1</td><td>59.6</td></tr><tr><td><br />ResNet-101</td><td>86.4</td><td>88.4</td><td>90.9</td><td>56.8</td><td>59.6</td><td>62.2</td></tr><tr><td><br />SE-ResNet-101</td><td>88.3</td><td>89.2</td><td>92.1</td><td>60.5</td><td>62.7</td><td>64.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="276">由表2可知,在参与对比的3种CNN体系结构中,无论是ResNet-101还是SE-ResNet-101都要优于较浅的BN-Inception,这主要是由于ResNet-101和SE-ResNet-101中都使用残差连接,缓解梯度消失问题,通过更深的层数提高网络的表达能力.采用SE-ResNet-101卷积神经网络的方案在UCF101、HMDB51数据集上分别达到92.1%和64.0%的最佳精度,较未使用注意力模块的ResNet-101分别提高1.2%和1.8%,通道注意力映射模块在识别系统中的性能优势得以显现.</p>
                </div>
                <div class="p1">
                    <p id="277">为了验证抽样策略对行为识别准确率的影响,分别测试不使用视频分组稀疏抽样的密集抽样(表中用A表示)和使用视频分组稀疏抽样(表中用B表示)在UCF101、HMDB51数据集上的行为识别准确率,结果如表3所示.</p>
                </div>
                <div class="p1">
                    <p id="278">由表3可知,在UCF101、HMDB51数据集上使用视频分组稀疏抽样策略前后的准确率差距分别为2.8%和2.4%,说明在行为识别任务中对视频使用密集抽样策略会使识别性能变差,这主要是由于视频中冗余信息较多.在某种程度上,由于连续视频帧中提取的视觉特征并不会发生显著变化,因此密集的时间采样通常会导致相邻采样帧之间存在较大的信息冗余.视频分组稀疏采样策略的优势在于去除部分冗余信息的同时可以借助视频级损失函数进行长期时间建模.表3中的实验结果也表明长期时间结构建模可以更有效地理解视频中的行为.</p>
                </div>
                <div class="area_img" id="279">
                    <p class="img_tit"><b>表3 抽样策略对行为识别准确率的影响</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Effect of sampling strategy on action recognition accuracy </p>
                    <p class="img_note"></p>
                    <table id="279" border="1"><tr><td rowspan="2"><br />抽样策略</td><td colspan="3"><br />UCF101</td><td colspan="3"><br />HMDB51</td></tr><tr><td><br />空间网络</td><td>时间网络</td><td>双流融合</td><td><br />空间网络</td><td>时间网络</td><td>双流融合</td></tr><tr><td>A</td><td>86.0</td><td>87.7</td><td>89.3</td><td>58.4</td><td>60.3</td><td>61.6</td></tr><tr><td><br />B</td><td>88.3</td><td>89.2</td><td>92.1</td><td>60.5</td><td>62.7</td><td>64.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="280">最后测试片段融合方式对系统准确率的影响,在UCF101、HMDB51数据集上分别评估最大值、平均和加权平均这3种融合方式对识别性能的影响,结果见表4.由表可知,平均融合可以对来自空间网络和时间网络的信息进行更好的融合,性能更优,因此在实验中,默认选择平均融合作为片段融合方式.</p>
                </div>
                <div class="area_img" id="281">
                    <p class="img_tit"><b>表4 不同片段融合方式对准确率的影响</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Effect of different fragment fusion modes on accuracy </p>
                    <p class="img_note"></p>
                    <table id="281" border="1"><tr><td rowspan="2"><br />片段融合方式</td><td colspan="3"><br />UCF101</td><td colspan="3"><br />HMDB51</td></tr><tr><td><br />空间网络</td><td>时间网络</td><td>双流融合</td><td><br />空间网络</td><td>时间网络</td><td>双流融合</td></tr><tr><td>最大值</td><td>87.0</td><td>88.1</td><td>89.7</td><td>60.2</td><td>61.0</td><td>61.4</td></tr><tr><td><br />平均</td><td>88.3</td><td>89.2</td><td>92.1</td><td>60.5</td><td>62.7</td><td>64.0</td></tr><tr><td><br />加权平均</td><td>86.2</td><td>89.0</td><td>91.2</td><td>59.7</td><td>62.1</td><td>63.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="282" name="282" class="anchor-tag">3 结 束 语</h3>
                <div class="p1">
                    <p id="283">针对现有行为识别算法难以进行长期时间建模的问题,本文提出基于注意力机制的时间分组深度网络行为识别算法,通过对整段视频进行分组稀疏抽样,对视频进行长期时间建模,有效解决识别序列时间覆盖有限导致的信息丢失问题和数据集过小导致的过拟合问题,实现小数据集下的视频级行为识别.同时,受人类视觉感知机制的启发,使用通道注意力映射对通道间的依赖关系进行显式建模,在关注重要特征的同时抑制次要特征,更有效地提取关键特征信息.相比目前较通用的行为识别算法,本文算法具有更好的准确性和鲁棒性.为了进一步提高算法的识别性能,今后还可以从时空特征融合方式和更高效的主干网络等方面展开深入的研究.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="318">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Action recognition with improved trajectories">

                                <b>[1]</b> WANG H,SCHIMID C.Action Recognition with Improved Trajectories // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2013:3551-3558.
                            </a>
                        </p>
                        <p id="320">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mobilenetv2:Inverted residuals and linear bottlenecks">

                                <b>[2]</b> SANDLER M,HOWARD A,ZHU M L,et al.Mobilenetv2:Inverted Residuals and Linear Bottlenecks // Proc of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:4510-4520.
                            </a>
                        </p>
                        <p id="322">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shuffle Net an extremely efficient convolutional neural network for mobile devices">

                                <b>[3]</b> ZHANG X Y,ZHOU X Y,LIN M X,et al.Shufflenet:An Extremely Efficient Convolutional Neural Network for Mobile Devices // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:6848-6856.
                            </a>
                        </p>
                        <p id="324">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">

                                <b>[4]</b> HE K M,ZHANG X Y,REN S Q,et al.Identity Mappings in Deep Residual Networks // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2016:630-645.
                            </a>
                        </p>
                        <p id="326">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Non-local neural networks">

                                <b>[5]</b> WANG X L,GIRSHICK R,GUPTA A,et al.Non-local Neural Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:7794-7803.
                            </a>
                        </p>
                        <p id="328">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DenselyConnected Convolutional Networks">

                                <b>[6]</b> HUANG G,LIU Z,VAN DER MAATEN L,et al.Densely Connected Convolutional Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2017:2261-2269.
                            </a>
                        </p>
                        <p id="330">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Largescale video classification with convolutional neural networks">

                                <b>[7]</b> KARPATHY A,TODERICI G,SHETTY S,et al.Large-Scale Vi-deo Classification with Convolutional Neural Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2014:1725-1732.
                            </a>
                        </p>
                        <p id="332">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Action recognition with trajectory-pooled deepconvolutional descriptors">

                                <b>[8]</b> WANG L M,QIAO Y,TANG X O.Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:4305-4314.
                            </a>
                        </p>
                        <p id="334">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Two-stream convolutional networks for action recognition in videos">

                                <b>[9]</b> SIMONYAN K,ZISSERMAN A.Two-Stream Convolutional Networks for Action Recognition in Videos // Proc of the 27th International Conference on Neural Information Processing Systems.Cambridge,USA:The MIT Press,2014,I:568-576.
                            </a>
                        </p>
                        <p id="336">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond short snippets:deep networks for video classification">

                                <b>[10]</b> NG J Y H,HAUSKNECHT M,VIJAYANARASIMHAN S,et al.Beyond Short Snippets:Deep Networks for Video Classification // Proc of the IEEE Conference on Computer Vision and Pattern Re-cognition.Washington,USA:IEEE,2015:4694-4702.
                            </a>
                        </p>
                        <p id="338">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatiotemporal features with 3D convolutional networks">

                                <b>[11]</b> TRAN D,BOURDEV L,FERGUS R,et al.Learning Spatiotemporal Features with 3D Convolutional Networks // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2015:4489-4497.
                            </a>
                        </p>
                        <p id="340">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[12]</b> SIMONYAN K,ZISSERMAN A.Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL].[2019-05-20].https://arxiv.org/pdf/1409.1556.pdf.
                            </a>
                        </p>
                        <p id="342">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 HE K M,ZHANG X Y,REN S Q,et al.Deep Residual Learning for Image Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:770-778.
                            </a>
                        </p>
                        <p id="344">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[14]</b> SZEGEDY C,LIU W,JIA Y Q,et al.Going Deeper with Convolutions // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:1-9.
                            </a>
                        </p>
                        <p id="346">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Aggregated Residual Transformations for Deep Neural Networks">

                                <b>[15]</b> XIE S N,GIRSHICK R,DOLLÁR P,et al.Aggregated Residual Transformations for Deep Neural Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2017:5987-5995.
                            </a>
                        </p>
                        <p id="348">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Squeeze-and-Excitation Networks">

                                <b>[16]</b> HU J,SHEN L,ALBANIE S,et al.Squeeze-and-Excitation Networks // Proc of the IEEE Conference on Computer Vision and Pa-ttern Recognition.Washington,USA:IEEE,2018:7132-7141.
                            </a>
                        </p>
                        <p id="350">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7D5AADFAD3177DC7B754CAB0C800827F&amp;v=MDM5MDJFOWViS2NSNzNwQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZodzdtMndLQT1OaWZPZmJUTUc2QzkyL2swRU9nT0MzdE52QkZoN1RwNU93NlFyRw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> MA C Y,CHEN M H,KIRA Z,et al.TS-LSTM and Temporal-Inception:Exploiting Spatiotemporal Dynamics for Activity Recognition.Signal Processing(Image Communication),2019,71:76-87.
                            </a>
                        </p>
                        <p id="352">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Grad-CAM:visual explanations from deep networks via gradient-based localization">

                                <b>[18]</b> SELVARAJU R R,COGSWELL M,DAS A,et al.Grad-CAM:Visual Explanations from Deep Networks via Gradient-Based Loca-lization // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2017:618-626.
                            </a>
                        </p>
                        <p id="354">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net classification with deep convolutional neural networks">

                                <b>[19]</b> KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Imagenet Cla-ssification with Deep Convolutional Neural Networks // Proc of the 25th International Conference on Neural Information Processing Systems.Cambridge,USA:The MIT Press,2012:1097-1105.
                            </a>
                        </p>
                        <p id="356">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Compressed Video Action Recognition">

                                <b>[20]</b> WU C Y,ZAHEER M,HU H X,et al.Compressed Video Action Recognition // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:6026-6035.
                            </a>
                        </p>
                        <p id="358">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatio-temporal representation with pseudo-3D residual networks">

                                <b>[21]</b> QIU Z F,YAO T,MEI T.Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2017:5533-5541.
                            </a>
                        </p>
                        <p id="360">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Temporal 3D ConvNets Using Temporal Transition Layer">

                                <b>[22]</b> DIBA A,FAYYAZ M,SHARMA V,et al.Temporal 3D ConvNets Using Temporal Transition Layer // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2018:1230-1234.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201910003" />
        <input id="dpi" type="hidden" value="96" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201910003&amp;v=MzIzOTFuRnkva1dyL0JLRDdZYkxHNEg5ak5yNDlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
