<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131445476123750%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dMSSB201904011%26RESULT%3d1%26SIGN%3dWaIWJWtsUKQez%252fGts%252fgCw6vk5m0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201904011&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201904011&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201904011&amp;v=MTAxMDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emdVTHpPS0Q3WWJMRzRIOWpNcTQ5RVpZUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#74" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#75" data-title="&lt;b&gt;1.1 WordNet&lt;/b&gt;"><b>1.1 WordNet</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;1.2&lt;/b&gt; 语义相似度度量"><b>1.2</b> 语义相似度度量</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#85" data-title="2 草图语义网络 ">2 草图语义网络</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#87" data-title="&lt;b&gt;2.1&lt;/b&gt; 部件分割"><b>2.1</b> 部件分割</a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;2.2&lt;/b&gt; 迁移的网络模型参数"><b>2.2</b> 迁移的网络模型参数</a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;2.3&lt;/b&gt; 语义树"><b>2.3</b> 语义树</a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;2.4&lt;/b&gt; 基于上下文的语义融合"><b>2.4</b> 基于上下文的语义融合</a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;2.5&lt;/b&gt; 方法步骤"><b>2.5</b> 方法步骤</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#128" data-title="4 结 束 语 ">4 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#84" data-title="表1 WordNet∶∶Similarity相似度计算方法">表1 WordNet∶∶Similarity相似度计算方法</a></li>
                                                <li><a href="#90" data-title="图1 飞机的部件分割示例">图1 飞机的部件分割示例</a></li>
                                                <li><a href="#96" data-title="图2 语义树样例">图2 语义树样例</a></li>
                                                <li><a href="#112" data-title="图3 Sketch-Semantic Net框架图">图3 Sketch-Semantic Net框架图</a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表2 草图类别与部件语义标签对应表&lt;/b&gt;"><b>表2 草图类别与部件语义标签对应表</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;表3 不同微调策略识别准确率对比&lt;/b&gt;"><b>表3 不同微调策略识别准确率对比</b></a></li>
                                                <li><a href="#123" data-title="&lt;b&gt;表4 不同融合策略下的草图识别准确率&lt;/b&gt;"><b>表4 不同融合策略下的草图识别准确率</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;表5 各草图识别方法的识别准确率&lt;/b&gt;"><b>表5 各草图识别方法的识别准确率</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" KAZMI I K, YOU L H, ZHANG J J.A Survey of Sketch Based Modeling Systems // Proc of the 11th International Conference on Computer Graphics, Imaging and Visualization.Washington, USA:IEEE, 2014:27-36." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Survey of Sketch Based Modeling Systems">
                                        <b>[1]</b>
                                         KAZMI I K, YOU L H, ZHANG J J.A Survey of Sketch Based Modeling Systems // Proc of the 11th International Conference on Computer Graphics, Imaging and Visualization.Washington, USA:IEEE, 2014:27-36.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 袁贞明, 金贵朝, 张佳.基于贝叶斯网络的在线草图识别算法.计算机工程, 2010, 36 (5) :32-34. (YUAN Z M, JIN G C, ZHANG J.Online Sketch Recognition Algorithm Based on Bayesian Network.Computer Engineering, 2010, 36 (5) :32-34.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201005014&amp;v=MTIzNzJxbzlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6Z1VMek9MejdCYmJHNEg5SE0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         袁贞明, 金贵朝, 张佳.基于贝叶斯网络的在线草图识别算法.计算机工程, 2010, 36 (5) :32-34. (YUAN Z M, JIN G C, ZHANG J.Online Sketch Recognition Algorithm Based on Bayesian Network.Computer Engineering, 2010, 36 (5) :32-34.) 
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" LI B, LU Y J, SHEN J.A Semantic Tree-Based Approach for Sketch-Based 3D Model Retrieval // Proc of the 23rd International Conference on Pattern Recognition.Washington, USA:IEEE, 2017:3880-3885." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A semantic tree-based approach for sketch-based 3D model retrieval">
                                        <b>[3]</b>
                                         LI B, LU Y J, SHEN J.A Semantic Tree-Based Approach for Sketch-Based 3D Model Retrieval // Proc of the 23rd International Conference on Pattern Recognition.Washington, USA:IEEE, 2017:3880-3885.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" EITZ M, HILDEBRAND K, BOUBEKEUR T, &lt;i&gt;et al&lt;/i&gt;.Sketch-Based Image Retrieval:Benchmark and Bag-of-Features Descriptors.IEEE Transactions on Visualization and Computer Graphics, 2011, 17 (11) :1624-1636." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sketch-based image retrieval: Benchmark and bag-of-features descriptors">
                                        <b>[4]</b>
                                         EITZ M, HILDEBRAND K, BOUBEKEUR T, &lt;i&gt;et al&lt;/i&gt;.Sketch-Based Image Retrieval:Benchmark and Bag-of-Features Descriptors.IEEE Transactions on Visualization and Computer Graphics, 2011, 17 (11) :1624-1636.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" SCHNEIDER R G, TUYTELAARS T.Sketch Classification and Classification-Driven Analysis Using Fisher Vectors.ACM Transactions on Graphics, 2014, 33 (6) .DOI:10.1145/2661229.2661231." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM2D4DE418C43080AE7BACF7EF13E36FB6&amp;v=MjQ5NDFxVzVxNDVORis4TURIUTV2bU1VbUU0T1BuaVgyaE0yRExHU004aVpDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3TDI4dzY0PU5pZklZN0hNRw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         SCHNEIDER R G, TUYTELAARS T.Sketch Classification and Classification-Driven Analysis Using Fisher Vectors.ACM Transactions on Graphics, 2014, 33 (6) .DOI:10.1145/2661229.2661231.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" LI Y, HOSPEDALES T M, SONG Y Z, &lt;i&gt;et al&lt;/i&gt;.Free-Hand Sketch Recognition by Multi-kernel Feature Learning.Computer Vision and Image Understanding, 2015, 137:1-11." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESBEB941BF65DB91928A7822E6E58DD7B2&amp;v=MzA1Njc0eGhRYm16aDFTbjJYcW1jd2NjYmdRc2lkQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0wyOHc2ND1OaWZPZmNITmJOaklydjB6WXU1N2ZuVQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         LI Y, HOSPEDALES T M, SONG Y Z, &lt;i&gt;et al&lt;/i&gt;.Free-Hand Sketch Recognition by Multi-kernel Feature Learning.Computer Vision and Image Understanding, 2015, 137:1-11.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" SEDDATI O, DUPONT S, MAHMOUDI S.DeepSketch:Deep Con-volutional Neural Networks for Sketch Recognition and Similarity Search // Proc of the 13th International Workshop on Content-Based Multimedia Indexing.Washington, USA:IEEE, 2015.DOI:10.1109/CBMI.2015.7153606." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepSketch:Deep convolutional neural networks for sketch recognition and similarity search">
                                        <b>[7]</b>
                                         SEDDATI O, DUPONT S, MAHMOUDI S.DeepSketch:Deep Con-volutional Neural Networks for Sketch Recognition and Similarity Search // Proc of the 13th International Workshop on Content-Based Multimedia Indexing.Washington, USA:IEEE, 2015.DOI:10.1109/CBMI.2015.7153606.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" DSEDDATI O, DUPONT S, MAHMOUDI S.DeepSketch 2:Deep Convolutional Neural Networks for Partial Sketch Recognition // Proc of the 14th International Workshop on Content-Based Multimedia Indexing.Washington, USA:IEEE, 2016.DOI:10.1109/CBMI.2016.7500261." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Sketch 2:Deep convolutional neural networks for partial sketch recognition">
                                        <b>[8]</b>
                                         DSEDDATI O, DUPONT S, MAHMOUDI S.DeepSketch 2:Deep Convolutional Neural Networks for Partial Sketch Recognition // Proc of the 14th International Workshop on Content-Based Multimedia Indexing.Washington, USA:IEEE, 2016.DOI:10.1109/CBMI.2016.7500261.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" YU Q, YANG Y X, LIU F, &lt;i&gt;et al&lt;/i&gt;.Sketch-a-Net:A Deep Neural Network that Beats Humans.International Journal of Computer Vision, 2017, 122 (3) :411-425." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD481512743524655FC9CC12AF263B32B2&amp;v=MjE1OTBwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0wyOHc2ND1OajdCYXJld0g5VE5yWWhCWis0TkNIbzh5bUJnNDB3T1NYMlQyaEF6ZXNDWFI4aWRDT052RlNpV1dyN0pJRg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         YU Q, YANG Y X, LIU F, &lt;i&gt;et al&lt;/i&gt;.Sketch-a-Net:A Deep Neural Network that Beats Humans.International Journal of Computer Vision, 2017, 122 (3) :411-425.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet Cla-ssification with Deep Convolutional Neural Networks // PEREIRA F, BURGES C J C, BOTTOU L, &lt;i&gt;et al&lt;/i&gt;., eds.Advances in Neural Information Processing Systems 25.Cambridge, USA:The MIT Press, 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with Deep Convolutional Neural Networks">
                                        <b>[10]</b>
                                         KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet Cla-ssification with Deep Convolutional Neural Networks // PEREIRA F, BURGES C J C, BOTTOU L, &lt;i&gt;et al&lt;/i&gt;., eds.Advances in Neural Information Processing Systems 25.Cambridge, USA:The MIT Press, 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" SONG J F, YU Q, SONG Y Z, &lt;i&gt;et al&lt;/i&gt;.Deep Spatial-Semantic Attention for Fine-Grained Sketch-Based Image Retrieval // Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2017:5552-5561." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Spatial-Semantic Attention for Fine-Grained Sketch-Based Image Retrieval">
                                        <b>[11]</b>
                                         SONG J F, YU Q, SONG Y Z, &lt;i&gt;et al&lt;/i&gt;.Deep Spatial-Semantic Attention for Fine-Grained Sketch-Based Image Retrieval // Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2017:5552-5561.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" FELLBAUM C, MILLER G A.WordNet:An Electronic Lexical Database.Cambridge, USA:The MIT Press, 1998." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=WordNet: An Electronic Lexical Database">
                                        <b>[12]</b>
                                         FELLBAUM C, MILLER G A.WordNet:An Electronic Lexical Database.Cambridge, USA:The MIT Press, 1998.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" VOORHEES E M.Using WordNet to Disambiguate Word Senses for Text Retrieval // Proc of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM, 1993:171-180." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using WordNet to disambiguate word senses for text Retrieval">
                                        <b>[13]</b>
                                         VOORHEES E M.Using WordNet to Disambiguate Word Senses for Text Retrieval // Proc of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM, 1993:171-180.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" ASLANDOGAN Y A, THIER C, YU C T, &lt;i&gt;et al&lt;/i&gt;.Using Semantic Contents and WordNet in Image Retrieval.ACM SIGIR Forum, 1997, 31:286-295." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000073624&amp;v=MjA3MDlSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGc1hhUlE9TmlmSVk3SzdIdGpOcjQ5RlpPd01DbjQ5b0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         ASLANDOGAN Y A, THIER C, YU C T, &lt;i&gt;et al&lt;/i&gt;.Using Semantic Contents and WordNet in Image Retrieval.ACM SIGIR Forum, 1997, 31:286-295.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" JIN Y H, KHAN L, WANG L, &lt;i&gt;et al&lt;/i&gt;.Image Annotations by Combining Multiple Evidence &amp;amp; Wordnet // Proc of the 13th Annual ACM International Conference on Multimedia.New York, USA:ACM, 2005:706-715." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image annotations by combining multiple evidence &amp;amp; WordNet">
                                        <b>[15]</b>
                                         JIN Y H, KHAN L, WANG L, &lt;i&gt;et al&lt;/i&gt;.Image Annotations by Combining Multiple Evidence &amp;amp; Wordnet // Proc of the 13th Annual ACM International Conference on Multimedia.New York, USA:ACM, 2005:706-715.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" TOUSCH A M, HERBIN S, AUDIBERT J Y.Semantic Hierarchies for Image Annotation:A Survey.Pattern Recognition, 2012, 45 (1) :333-345." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738050&amp;v=MDQ0MzBETnFZOUZZK2dIREhrNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGc1hhUlE9TmlmT2ZiSzdIdA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         TOUSCH A M, HERBIN S, AUDIBERT J Y.Semantic Hierarchies for Image Annotation:A Survey.Pattern Recognition, 2012, 45 (1) :333-345.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" PEDERSEN T, PATWARDHAN S, MICHELIZZI J.WordNet∶∶Similarity-Measuring the Relatedness of Concepts[C/OL].[2018-11-15].http://www.d.umn.edu/～tpederse/Pubs/AAAI04PedersenT.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=WordNet ∶Similarity-Measuring the Relatedness of Concepts[C/OL]">
                                        <b>[17]</b>
                                         PEDERSEN T, PATWARDHAN S, MICHELIZZI J.WordNet∶∶Similarity-Measuring the Relatedness of Concepts[C/OL].[2018-11-15].http://www.d.umn.edu/～tpederse/Pubs/AAAI04PedersenT.pdf
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" LEACOCK C, CHODOROW M.Combining Local Context and WordNet Similarity for Word Sense Identification.WordNet:An Electronic Lexical Database, 1998, 49 (2) :265-283." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Combining local context and Word-Net similarity for word sense identification">
                                        <b>[18]</b>
                                         LEACOCK C, CHODOROW M.Combining Local Context and WordNet Similarity for Word Sense Identification.WordNet:An Electronic Lexical Database, 1998, 49 (2) :265-283.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" WU Z B, PALMER M.Verbs Semantics and Lexical Selection // Proc of the 32nd Annual Meeting on Association for Computational Linguistics.Stroudsburg, USA:ACL, 1994:133-138." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Verb semantics and lexical selection">
                                        <b>[19]</b>
                                         WU Z B, PALMER M.Verbs Semantics and Lexical Selection // Proc of the 32nd Annual Meeting on Association for Computational Linguistics.Stroudsburg, USA:ACL, 1994:133-138.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" LIN D K.An Information-Theoretic Definition of Similarity // Proc of the 15th International Conference on Machine Learning.San Francisco, USA:Morgan Kaufmann Publishers, 1998:296-304." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An information-theoretic definition of similarity">
                                        <b>[20]</b>
                                         LIN D K.An Information-Theoretic Definition of Similarity // Proc of the 15th International Conference on Machine Learning.San Francisco, USA:Morgan Kaufmann Publishers, 1998:296-304.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" RESNIK P.Using Information Content to Evaluate Semantic Similarity in a Taxonomy // Proc of the 14th International Joint Conference on Artificial Intelligence.New York, USA:ACM, 1995:448-453." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using information content to evaluate semantic similarity in a taxonomy">
                                        <b>[21]</b>
                                         RESNIK P.Using Information Content to Evaluate Semantic Similarity in a Taxonomy // Proc of the 14th International Joint Conference on Artificial Intelligence.New York, USA:ACM, 1995:448-453.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" JIANG J J, CONRATH D W.Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy // Proc of the International Confe-rence on Research in Computational Linguistics.Berlin, Germany:Springer, 1997:19-33." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Austenite stability and its effect on the toughness of a high strength ultra-low carbon medium manganese steel plate">
                                        <b>[22]</b>
                                         JIANG J J, CONRATH D W.Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy // Proc of the International Confe-rence on Research in Computational Linguistics.Berlin, Germany:Springer, 1997:19-33.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" HIRST G, ST-ONGE D.Lexical Chains as Representations of Context for the Detection and Correction of Malapropisms.Lecture Notes in Physics, 1997, 728 (9) :123-149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lexical Chains as Representations of Context for the Detection and Correction of Malapropisms">
                                        <b>[23]</b>
                                         HIRST G, ST-ONGE D.Lexical Chains as Representations of Context for the Detection and Correction of Malapropisms.Lecture Notes in Physics, 1997, 728 (9) :123-149.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" BANERJEE S, PEDERSEN T.Extended Gloss Overlaps as a Mea-sure of Semantic Relatedness[C/OL].[2018-11-15].http://www.d.umn.edu/～tpederse/Pubs/ijcai03.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extended Gloss Overlaps as a Mea-sure of Semantic Relatedness[C/OL]">
                                        <b>[24]</b>
                                         BANERJEE S, PEDERSEN T.Extended Gloss Overlaps as a Mea-sure of Semantic Relatedness[C/OL].[2018-11-15].http://www.d.umn.edu/～tpederse/Pubs/ijcai03.pdf.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" PEDERSEN T, BANERJEE S, PATWARDHAN S.Maximizing Semantic Relatedness to Perform Word Sense Disambiguation.Research Report, UMSI 2005/25.Duluth, USA:University of Minnesota, 2005." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Maximizing Semantic Relatedness top erform Word Sense Disambiguation">
                                        <b>[25]</b>
                                         PEDERSEN T, BANERJEE S, PATWARDHAN S.Maximizing Semantic Relatedness to Perform Word Sense Disambiguation.Research Report, UMSI 2005/25.Duluth, USA:University of Minnesota, 2005.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_26" title=" HUANG Z, FU H B, LAU R W H.Data-Driven Segmentation and Labeling of Freehand Sketches.ACM Transactions on Graphics, 2014, 33 (6) .DOI:10.1145/2661229.2661280." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMB26B2D2AD9B50623C6E932A0D9AE3A98&amp;v=MDc0OTNNZVhOTE9YQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0wyOHc2ND1OaWZJWThHNkdLUE8yNDAwRU9KOUNYdy96UlZnN0VwMFMzMlRyR1k4Qw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         HUANG Z, FU H B, LAU R W H.Data-Driven Segmentation and Labeling of Freehand Sketches.ACM Transactions on Graphics, 2014, 33 (6) .DOI:10.1145/2661229.2661280.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_27" title=" 赵鹏, 王斐, 刘慧婷, 等.基于深度学习的手绘草图识别.四川大学学报 (工程科学版) , 2016, 48 (3) :94-99. (ZHAO P, WANG F, LIU H T, &lt;i&gt;et al&lt;/i&gt;.Sketch Recognition Using Deep Learning.Journal of Sichuan University (Engineering Science Edition) , 2016, 48 (3) :94-99.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SCLH201603012&amp;v=MDA4OTJGeXpnVUx6T05pN0hackc0SDlmTXJJOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         赵鹏, 王斐, 刘慧婷, 等.基于深度学习的手绘草图识别.四川大学学报 (工程科学版) , 2016, 48 (3) :94-99. (ZHAO P, WANG F, LIU H T, &lt;i&gt;et al&lt;/i&gt;.Sketch Recognition Using Deep Learning.Journal of Sichuan University (Engineering Science Edition) , 2016, 48 (3) :94-99.) 
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_28" title=" 赵鹏, 刘杨, 刘慧婷, 等.基于深度卷积-递归神经网络的手绘草图识别方法.计算机辅助设计与图形学学报, 2018, 30 (2) :217-224. (ZHAO P, LIU Y, LIU H T, &lt;i&gt;et al&lt;/i&gt;.A Sketch Recognition Method Based on Deep Convolutional-Recurrent Neural Network.Journal of Computer-Aided Design and Computer Graphics, 2018, 30 (2) :217-224.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201802002&amp;v=MDc1MzMzenFxQnRHRnJDVVJMT2VaZVJuRnl6Z1VMek9MejdCYUxHNEg5bk1yWTlGWm9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                         赵鹏, 刘杨, 刘慧婷, 等.基于深度卷积-递归神经网络的手绘草图识别方法.计算机辅助设计与图形学学报, 2018, 30 (2) :217-224. (ZHAO P, LIU Y, LIU H T, &lt;i&gt;et al&lt;/i&gt;.A Sketch Recognition Method Based on Deep Convolutional-Recurrent Neural Network.Journal of Computer-Aided Design and Computer Graphics, 2018, 30 (2) :217-224.) 
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(04),361-368 DOI:10.16451/j.cnki.issn1003-6059.201904009            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>融合深度学习和语义树的草图识别方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="javascript:;">赵鹏</a>
                                <a href="javascript:;">冯晨成</a>
                                <a href="javascript:;">韩莉</a>
                                <a href="javascript:;">纪霞</a>
                </h2>
                    <h2>

                    <span>安徽大学计算机科学与技术学院</span>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>现有的草图识别框架利用整幅图像作为网络输入, 草图识别过程可解释性较差.文中融合深度学习和语义树, 提出草图语义网 (Sketch-Semantic Net) .首先对草图进行部件分割, 将单幅完整的草图分割为多个具有语义概念的部件图.然后利用深度迁移学习识别草图部件.最后通过语义树的语义概念关联部件同部件所属草图对象类别, 较好地弥补sketch图像从底层语义到高层语义之间的语义鸿沟.在广泛应用的草图分割数据集上的实验验证文中方法的有效性.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%8D%89%E5%9B%BE%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">草图识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E6%A0%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义树;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *赵鹏 (通讯作者) , 博士, 副教授, 主要研究方向为智能信息处理、机器学习.E-mail:zhaopeng_ad@163.com.;
                                </span>
                                <span>
                                    冯晨成, 硕士研究生, 主要研究方向为模式识别.E-mail:767254498@qq.com.;
                                </span>
                                <span>
                                    韩莉, 博士, 讲师, 主要研究方向为数据挖掘.E-mail:1617226670@qq.com.;
                                </span>
                                <span>
                                    纪霞, 博士, 讲师, 主要研究方向为数据挖掘.E-mail:86315260@qq.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-27</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.61602004);</span>
                                <span>安徽省重点研究与开发计划项目 (No.1804d08020309);</span>
                                <span>安徽省自然科学基金项目 (No.1908085MF188, 1908085MF182);</span>
                                <span>安徽省高校自然科学研究重点项目 (No.KJ2016A041, KJ2017A011) 资助;</span>
                    </p>
            </div>
                    <h1><b>Sketch Recognition Combining Deep Learning and Semantic Tree</b></h1>
                    <h2>
                    <span>ZHAO Peng</span>
                    <span>FENG Chencheng</span>
                    <span>HAN Li</span>
                    <span>JI Xia</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, Anhui University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In the existing sketch recognition based on deep learning, a whole sketch is employed as an input of network, and therefore the recognition process is uninterpretable. The semantic tree is introduced into sketch recognition based on deep learning, and a sketch recognition method, sketch-semantic net, is proposed in this paper. Firstly, data-driven segmentation method is utilized to divide a whole sketch into component sketches with the semantic information. Secondly, the component sketches are recognized by transfer deep learning. Finally, the component sketches are associated with the sketch categories according to the semantic concepts of the semantic tree, and thus the gap between low level semantics and high level semantics is reduced. The experimental results on the popular Sketch_ dataset demonstrate the effectiveness of the proposed method.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Sketch%20Recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Sketch Recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Semantic%20Tree&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Semantic Tree;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHAO Peng ( Corresponding author ) , Ph.D., associate professor. Her research interests include intelligent information processing and machine learning.;
                                </span>
                                <span>
                                    FENG Chencheng, master student. Her research interests include pattern recognition.;
                                </span>
                                <span>
                                    HAN Li, Ph.D., lecturer. Her research interests include data mining.;
                                </span>
                                <span>
                                    JI Xia, Ph.D., lecturer. Her research interests include data mining.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-27</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.61602004);</span>
                                <span>Key Research and Development Program of AnhuiProvince (No.1804d8020309);</span>
                                <span>Natural Science Foundation of Anhui Province (No.1908085MF188, 1908085MF182);</span>
                                <span>Natural Science Foundation of the Education Department of Anhui Province (No.KJ2016A041, KJ2017A011);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="70">随着各类便携式手写设备的普及, 人们可以使用更便捷的方式手绘草图, 描绘客观世界.越来越多的基于草图的应用应运而生, 如草图建模<citation id="130" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、交互式草图识别<citation id="131" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、基于草图语义的图像检索<citation id="132" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等.但已标注的草图相对缺乏, 尤其在某些特定的领域;并且草图结构单一, 无色彩、纹理等特征, 使得草图识别具有较大的挑战性.</p>
                </div>
                <div class="p1">
                    <p id="71">早期的草图识别主要为手工提取图像特征, 再结合各种分类方法进行识别.Eitz等<citation id="133" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>使用特征袋 (Bag-of-Features) 的特征提取方法.Schneider等<citation id="134" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>使用Fisher提取图像特征.Li 等<citation id="135" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>为了保留图像的整体结构特征, 使用星图编码局部特征和整体结构.随着深度学习在各类自然图像的识别中的性能提升, 在草图识别的各类应用中卷积神经网络多被采用以提取草图特征<citation id="140" type="reference"><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>.Seddati等<citation id="136" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>微调AlexNet<citation id="137" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 适合于草图分类.Dseddati等<citation id="138" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提取草图的时序子图, 分别训练网络, 最后采用特征融合识别草图.Yu 等<citation id="139" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出多尺度多通道的深度卷积神经网络, 并结合贝叶斯分类器处理多尺度的融合.</p>
                </div>
                <div class="p1">
                    <p id="72">从基于传统手工特征的方法到目前通用的基于卷积神经网络的方法, 草图识别已取得较大突破.但是, 卷积神经网络在草图识别上的应用需要大量标注的草图数据集作为训练集.而目前已经收集的标注草图数据集规模十分有限, 尤其对于特定领域, 已标注的数据集更是匮乏.单纯使用完整的草图作为网络的输入无法达到较理想的识别效果.草图本身虽然结构简单, 无色彩纹理等特征, 但是草图自身并不是孤立的存在, 它实际上是由各个具有明确语义概念的部件构成.在3Dsketch中, Li等<citation id="141" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>把语义引入3D模型的检索中.Song等<citation id="142" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>在细粒度的图像检索中引入空间语义.</p>
                </div>
                <div class="p1">
                    <p id="73">针对传统草图识别方法采用整幅草图作为输入, 忽视草图的语义组成结构关系, 造成草图识别过程可解释性较差的问题, 本文融合深度学习和语义树, 提出草图语义网络 (Sketch-Semantic Net) .引入语义树概念, 将神经网络对图像的强感知和强抽象结合成具有丰富语义的语义树, 利用语义网中语义相关性, 有效解释部件标签与部件所属对象的标签的关联性, 弥补端到端图像识别过程中的语义鸿沟, 使图像的识别具有可解释性.在广泛应用的草图分割数据集 (Sketch_dataset) 上的实验验证本文方法的有效性.</p>
                </div>
                <h3 id="74" name="74" class="anchor-tag">1 相关工作</h3>
                <h4 class="anchor-tag" id="75" name="75"><b>1.1 WordNet</b></h4>
                <div class="p1">
                    <p id="76">WordNet<citation id="143" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>是一个大型英语词汇字典, 与传统的字典不同.传统字典是以26个英文字母为序查找词汇的信息, 各词汇间相互独立, 无联系.而WordNet起源于人类心理学, 通过网状的形式聚集相同概念、相近语义的词汇.在WordNet中, 所有的概念都可以使用这一个网关联.</p>
                </div>
                <div class="p1">
                    <p id="77">Voorhees等<citation id="144" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>利用WordNet中“is-a”关系, 在包含多个名词集合的一段文本中, 为每个有歧义的名词选择正确语义, 达到消除词语二义性的目的.Aslandogan等<citation id="145" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>使用WordNet进行文本数据库的扩展, 并使用WordNet的语义内容进行图像检索.Jin等<citation id="146" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>使用WordNet剔除图像标注中的无关单词.Tousch等<citation id="147" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>指出结构化的词汇语义信息对图像的自动标注的重要性.</p>
                </div>
                <div class="p1">
                    <p id="78">WordNet实际上由4个子网组成:名词集、动词集、形容词集和副词集.名词集为表示从属、包含、同义这三种关系的集合.动词集为表达具体行为特征的集合.形容词集为语义上的反义词对的集合.副词集更多地是形容词和动词组合衍生而来的词语集合.</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79"><b>1.2</b> 语义相似度度量</h4>
                <div class="p1">
                    <p id="80">WordNet∶∶Similarity<citation id="148" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>为一个专门度量WordNet中语义相似度方法的软件包, 提供10种度量2个概念间的语义相关度的方法, 包括:path, lch<citation id="149" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>, wup<citation id="150" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, lin<citation id="151" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, res<citation id="152" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, jcn<citation id="153" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>, hso<citation id="154" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>, lesk<citation id="155" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>, vector<citation id="156" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>和vector_pair.</p>
                </div>
                <div class="p1">
                    <p id="81">这10种方法大致可分为3类:基于路径的方法、基于概念所含信息量的方法、基于概念解释的方法.表1对这些方法进行详细的列表说明, 其中<i>Sim</i> (<i>A</i>, <i>B</i>) 表示概念<i>A</i>和概念<i>B</i>的语义相似度值.</p>
                </div>
                <div class="p1">
                    <p id="82">基于路径的方法依据两个概念在WordNet中的位置以及到根节点间距离等计算相似度.基于概念信息量的方法计算当前概念的所有语义解释中的主谓、动宾、形容词修饰, 限定关系占语料库的比例, 作为当前概念的信息量, 记为IC.基于概念解释的方法寻找两个概念间的所有解释之间的关系.</p>
                </div>
                <div class="area_img" id="84">
                    <p class="img_tit">表1 WordNet∶∶Similarity相似度计算方法 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Similarity calculation methods in WordNet</p>
                    <p class="img_note"></p>
                    <table id="84" border="1"><tr><td colspan="2"><br />计算方法</td><td>计算公式</td><td>说明</td></tr><tr><td rowspan="3"><br />基于路径<br />的方法</td><td><br />path</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>i</mi><mi>m</mi><mo stretchy="false"> (</mo><mi>A</mi><mo>, </mo><mi>B</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>L</mi><mtext>e</mtext><mtext>n</mtext><mtext>g</mtext><mi>t</mi><mi>h</mi><mo stretchy="false"> (</mo><mi>A</mi><mo>, </mo><mi>B</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></td><td><i>Length</i> (<i>A</i>, <i>B</i>) 表示<i>A</i>到<i>B</i>经过的节点个数</td><td rowspan="3"></td><td rowspan="3"></td></tr><tr><td><br />lch</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>i</mi><mi>m</mi><mo stretchy="false"> (</mo><mi>A</mi><mo>, </mo><mi>B</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mrow><mi>lg</mi></mrow><mo stretchy="false"> (</mo><mfrac><mrow><mi>L</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo stretchy="false"> (</mo><mi>A</mi><mo>, </mo><mi>B</mi><mo stretchy="false">) </mo></mrow><mrow><mn>2</mn><mi>D</mi><mi>e</mi><mi>p</mi><mi>t</mi><mi>h</mi><msub><mrow></mrow><mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub></mrow></mfrac><mo stretchy="false">) </mo></mrow></math></td><td><i>Depth</i><sub><i>LCS</i></sub>表示根节点到<i>A</i>、<i>B</i>公共节点的深度<br /><i>LCS</i>表示<i>A</i>、<i>B</i>最近邻的公共节点</td></tr><tr><td><br />wup</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>i</mi><mi>m</mi><mo stretchy="false"> (</mo><mi>A</mi><mo>, </mo><mi>B</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mn>2</mn><mi>D</mi><mi>e</mi><mi>p</mi><mi>t</mi><mi>h</mi><msub><mrow></mrow><mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub></mrow><mrow><mi>D</mi><mi>e</mi><mi>p</mi><mi>t</mi><mi>h</mi><msub><mrow></mrow><mi>A</mi></msub><mo>+</mo><mi>D</mi><mi>e</mi><mi>p</mi><mi>t</mi><mi>h</mi><msub><mrow></mrow><mi>B</mi></msub></mrow></mfrac></mrow></math></td><td><i>Depth</i><sub><i>A</i></sub>表示根节点到<i>A</i>的深度<br /><i>Depth</i><sub><i>B</i></sub>表示根节点到<i>B</i>的深度</td></tr><tr><td rowspan="3"><br />基于概念<br />所含信息量<br />的方法</td><td><br />lin</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>i</mi><mi>m</mi><mo stretchy="false"> (</mo><mi>A</mi><mo>, </mo><mi>B</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mn>2</mn><mi>Ι</mi><mi>C</mi><mo stretchy="false"> (</mo><mi>L</mi><mi>C</mi><mi>S</mi><mo stretchy="false">) </mo></mrow><mrow><mi>Ι</mi><mi>C</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mo>+</mo><mi>Ι</mi><mi>C</mi><mo stretchy="false"> (</mo><mi>B</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></td><td rowspan="3"><i>IC</i> (<i>A</i>) 表示<i>A</i>的信息量<br /><i>IC</i> (<i>B</i>) 表示<i>B</i>的信息量<br /><i>IC</i> (<i>LCS</i>) 表示<i>A</i>、<i>B</i>公共子节点的信息量</td></tr><tr><td><br />res</td><td><i>Sim</i> (<i>A</i>, <i>B</i>) =<i>IC</i> (<i>LCS</i>) </td></tr><tr><td><br />jcn</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>i</mi><mi>m</mi><mo stretchy="false"> (</mo><mi>A</mi><mo>, </mo><mi>B</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ι</mi><mi>C</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mo>+</mo><mi>Ι</mi><mi>C</mi><mo stretchy="false"> (</mo><mi>B</mi><mo stretchy="false">) </mo><mo>-</mo><mn>2</mn><mi>Ι</mi><mi>C</mi><mo stretchy="false"> (</mo><mi>L</mi><mi>C</mi><mi>S</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></td></tr><tr><td rowspan="4"><br />基于概念<br />解释的<br />方法</td><td><br />lesk</td><td>计算概念<i>A</i>和概念<i>B</i>及它们上下位词的所有<br />解释中重叠的单词个数的平方和, 连续重叠<br />的个数越多, 相似度值越大.</td><td></td><td rowspan="4"></td></tr><tr><td><br />hso</td><td>寻找两个概念的语义链的连接关系.<br />两个概念的关系分为极强, 强, 一般.<br /><i>Sim</i> (<i>A</i>, <i>B</i>) =<br /><i>C</i>-<i>path</i><sub><i>length</i></sub>- (<i>k</i>·<i>Num</i><sub><i>directionChange</i></sub>) </td><td>通常, <i>C</i>=8, <i>k</i>=1<br /><i>path</i><sub><i>length</i></sub>为概念<i>A</i>、<i>B</i>在语义链中的路径长度<br /><i>Num</i><sub><i>directionChange</i></sub>为从概念<i>A</i>到概念<i>B</i>的路径中<br />改变的链的次数</td></tr><tr><td><br />vector</td><td>对概念的每条解释都通过共现矩阵得到一<br />个解释的向量.概念<i>A</i>和<i>B</i>的相似度由解释<br />向量的最大余弦值决定.</td><td></td></tr><tr><td><br />vector_pair</td><td>概念<i>A</i>与概念<i>B</i>的相似度值为这两个概念<br />的所有向量两两组合的余弦值总和</td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="85" name="85" class="anchor-tag">2 草图语义网络</h3>
                <div class="p1">
                    <p id="86">区别于基于整幅草图的卷积神经网络的识别框架, 本文将sketch图像分割成各个具有语义概念的部件图, 以部件图的识别辅助草图的识别.同时, 本文采用sketch分割数据集 (Sketch_dataset) , 它的数据量只有300幅sketch草图图像, 对于神经网络而言, 很难训练较稳定的网络模型, 本文通过分割草图, 用于结合神经网络与语义树.</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87"><b>2.1</b> 部件分割</h4>
                <div class="p1">
                    <p id="88">图像分割对于图像理解而言至关重要.在草图研究领域, Huang等<citation id="157" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>创建Sketch_dataset数据集, 并提出数据驱动的草图分割标注方法, 将草图拆分到笔画层, 再将笔画的序列组合为笔画组, 与数据库中的3D图形的各个部件进行迭代比对, 最终得到基于笔画层的sketch草图的标注.在一幅草图中, 相同颜色的笔画共享同一个标签.Sketch_dataset数据集包含10个类别, 每个类别有30幅草图, 提供的分割信息存于对应的XML文件中.本文基于此数据集, 进行进一步的处理:1) 将XML文件中同一笔画的点连接;2) 寻找相同标签的最大连通的笔画;3) 提取最大连通的笔画图, 绘制成部件图;4) 赋予部件图语义.</p>
                </div>
                <div class="p1">
                    <p id="89">一副完整的草图通过本文的处理, 最终呈现如图1所示, 飞机可以按照其语义分割成机翼 (wing) 、引擎 (engine) 、机身 (body) 、水平尾翼 (tailplane) .</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904011_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 飞机的部件分割示例" src="Detail/GetImg?filename=images/MSSB201904011_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 飞机的部件分割示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904011_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 An illustration for segmentation of airplane</p>

                </div>
                <h4 class="anchor-tag" id="91" name="91"><b>2.2</b> 迁移的网络模型参数</h4>
                <div class="p1">
                    <p id="92">卷积神经网络能够较好地表征图像特征, 但是卷积神经网络的学习依赖大量的训练数据.当训练数据缺乏时, 容易产生过拟合, 致使识别率下降.本文引入迁移学习的思想, 将预训练的AlexNet网络模型及其参数迁移至Sketch_dataset数据集上.AlexNet是在大型的自然图像数据集ImageNet上训练而成, 由5个卷积-池化层和3个全连接层组成.选择迁移AlexNet的网络参数是由于ImageNet数据集囊括众多的自然图像, 具有较好的泛化能力, 并且AlexNet深度尚可, 参数也容易获取.当然, 自然图像同草图图像一样也有诸多不同, 仅迁移AlexNet远远不够, 需在其基础上, 对网络权值进行训练调整.由于低层的特征图主要是提取图像的边缘特征, 特征图变化很小, 所以, 本文采取的微调策略如下:不调整低层特征图的卷积核权值, 而微调中高层特征, 并且去掉之前的全连接层, 加入新的全连接层以适应本文的草图数据集.</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93"><b>2.3</b> 语义树</h4>
                <div class="p1">
                    <p id="94">本文基于WordNet-3.1版本构建语义树, 针对根据部件标签及部件所属对象标签的语义信息, 从WordNet中抽取它们之间的联系.由于本文使用的数据集的标签都为名词, 所以考虑标签之间的三种关系:从属关系 (is a) , 包含关系 (part of) , 同义关系 (synonymy) .根据WordNet中的词间关系, 对所有标签 (部件标签和图像标签) 构造一棵以entity为根节点的语义树, 该树共有12层, 119个节点.</p>
                </div>
                <div class="p1">
                    <p id="95">一张标签为飞机的sketch草图图像, 可以分解为4个部件图 (如图1所示) , 分别是标签为机翼的部件图2幅, 标签为引擎的部件图2幅, 标签为尾翼的部件图2幅, 标签为机身的部件图1幅.这4个部件标签名同它们所属对象的标签名所在的一棵语义树如图2所示.</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904011_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 语义树样例" src="Detail/GetImg?filename=images/MSSB201904011_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 语义树样例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904011_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Example of semantic tree</p>

                </div>
                <div class="p1">
                    <p id="97">图2中:“<image href="images/MSSB201904011_098.jpg" type="" display="inline" placement="inline"><alt></alt></image>”连接线对应子节点同父节点的关系为is a;“<image href="images/MSSB201904011_099.jpg" type="" display="inline" placement="inline"><alt></alt></image>”连接线表示子节点同父节点的关系为part of;虚线箭头虚线指向以灰色为背景的圆角矩形框, 表示节点间的关系为synonymy;浅灰色的圆形节点表示叶子节点到根节点的中间节点, 也是计算语义相似度的重要依据.</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100"><b>2.4</b> 基于上下文的语义融合</h4>
                <div class="p1">
                    <p id="101">共享语义标签普遍存在, 例如airplane与human共享同个部件语义标签body.一般而言, body对于airplane的重要程度等同于body对于human的重要程度.但是WordNet∶∶Similarity提供的10种方法计算得出的结果均认为body对于human的重要程度远高于body对于airplane的重要程度.为了缓解共享语义标签下的语义相似度计算偏差, 本文采用基于上下文的语义融合策略.</p>
                </div>
                <div class="p1">
                    <p id="102">本文将WordNet∶∶Similarity计算得到的部件所属对象定义为部件的潜在语义标签, 而不是最终的部件所属对象.基于上下文的语义融合策略根据同幅草图中的各个部件的潜在语义标签进行投票, 获得投票最多的潜在语义标签, 作为这幅草图中所有部件最终所属对象的标签.</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103"><b>2.5</b> 方法步骤</h4>
                <div class="p1">
                    <p id="104">Sketch-Semantic Net分为迁移神经网络的训练阶段和草图的识别阶段.在训练阶段, 首先将草图进行部件分割, 扩充数据集, 每幅图像分割成数量不等的部件图.然后利用草图部件图, 训练迁移的AlexNet, 微调参数得到Alex-Sketch Net.在识别阶段, 首先将待识别的草图分割成部件, 输入Alex-Sketch Net中, 获得部件的预测标签 (Predict Tag) .然后, 利用语义树的相似度计算方法, 计算所有部件预测标签的潜在语义标签 (Latent Tag) .最后, 使用基于上下文的语义融合策略将潜在语义标签 (Latent Tag) 映射成最终的草图标签 (Tag) .Sketch-Semantic Net的识别阶段框图如图3所示, 识别算法步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="105"><b>算法</b> Sketch-Semantic Net识别算法</p>
                </div>
                <div class="p1">
                    <p id="106"><b>输入</b> 待识别草图</p>
                </div>
                <div class="p1">
                    <p id="107"><b>输出</b> 草图的标签tag</p>
                </div>
                <div class="p1">
                    <p id="108">step 1 分割草图, 获得草图部件集.</p>
                </div>
                <div class="p1">
                    <p id="109">step 2 将所有部件输入Alex-Sketch Net, 获得部件的预测标签.</p>
                </div>
                <div class="p1">
                    <p id="110">step 3 根据语义树, 计算各部件预测标签和各草图标签的语义相似度, 取相似度最大的草图标签作为该部件的潜在语义标签.</p>
                </div>
                <div class="p1">
                    <p id="111">step 4 根据2.4节, 对各部件的潜在标签进行上下文融合获得草图的标签tag.</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904011_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Sketch-Semantic Net框架图" src="Detail/GetImg?filename=images/MSSB201904011_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 Sketch-Semantic Net框架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904011_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Framework of sketch-semantic net</p>

                </div>
                <h3 id="113" name="113" class="anchor-tag">3 实验及结果分析</h3>
                <div class="p1">
                    <p id="114">Sketch_dataset是目前广泛应用的草图分割数据集, 包含10个类别, 每个类别有30幅草图, 共计300幅草图.每幅图像的XML描述文件记录每笔笔画的信息, 包括点坐标、当前笔画的标签.本文将10个类别共300幅图像按照2.1节的分割方法分割, 共得到2374幅部件图.草图按照部件语义可以分成如表2所示的部件图.</p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表2 草图类别与部件语义标签对应表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Sketch categories and their corresponding component semantic labels</p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td><br />类别</td><td>可分割的部件名</td></tr><tr><td><br />飞机</td><td>引擎, 机身, 机翼, 水平尾翼, 垂直尾翼, 螺旋桨</td></tr><tr><td><br />自行车</td><td>前轴, 后轴, 轮子, 把手, 链条, 前叉, 坐垫, <br />脚蹬, 刹车</td></tr><tr><td><br />烛台</td><td>底座, 手柄, 竖直支撑杆, 蜡烛, 火光, <br />水平支撑杆</td></tr><tr><td><br />椅子</td><td>轴, 气压筒, 基座, 靠背, 扶手, 凳子腿, 坐垫</td></tr><tr><td><br />四只腿<br />的动物</td><td>身体, 耳朵, 头, 腿, 尾巴</td></tr><tr><td><br />人类</td><td>头, 身体, 胳膊, 腿, 手, 脚</td></tr><tr><td><br />灯</td><td>灯管, 基座, 灯盖</td></tr><tr><td><br />步枪</td><td>弹药库, 枪管, 枪身, 握柄, 扳机, 枪尾, 瞄准器</td></tr><tr><td><br />桌子</td><td>延伸面, 桌面支撑, 基座, 中层支撑, 腿支撑, <br />桌面, 桌腿</td></tr><tr><td><br />花瓶</td><td>瓶身, 瓶口, 基座, 手柄</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="116">考虑到本文采用的数据量较少, 故采用数据扩增策略, 首先将所有的部件图缩放到同一分辨率 (256×256) , 对每幅部件图分别旋转0°, 90°, 180°和270°.然后, 对每幅旋转后的图像分别取左上、左下、右上、右下和中部, 裁剪至227×227.对部件图进行旋转, 一方面是为了使数据具有旋转不变性, 另一方面是为了数据扩增, 因此每幅部件图扩充至20幅.最后获得47 480幅部件图集.本文选取3/4的数据作为训练集, 剩余1/4的数据作为测试集, 即225幅图像的所有部件图作为训练集, 75幅图像的所有部件图作为测试集.</p>
                </div>
                <div class="p1">
                    <p id="117">本文设计3组对比实验, 第一组实验为迁移AlexNet之后对比网络参数的调整策略, 第二组实验是对本文的基于上下文的语义融合方式同其他融合方式进行对比, 在第三组实验中, 为了验证Ske-tch-Semantic Net的有效性, 选取通用的基于手工特征提取的方法 (HOG-SVM, SIFT-Fisher Vector<citation id="158" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>) 和基于深度学习的方法 (Deep-Sketch<citation id="159" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>、Deep-CRNN-sketch<citation id="160" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>、sketch-a-net) 进行对比.</p>
                </div>
                <div class="p1">
                    <p id="118">第一组实验对训练阶段中将AlexNet迁移到Sketch_dataset进行微调的微调策略进行对比实验.分别采用6种策略, 结果如表3所示.策略一 (fc) 仅调整全连接层fc的参数;策略二 (5<sup>th</sup>+fc) 调整第五层卷积层和全连接层的参数;策略三 (4<sup>th</sup>+5<sup>th</sup>+fc) 调整第四、第五层卷积层及全连接层的参数;策略四 (3<sup>th</sup>+4<sup>th</sup>+5<sup>th</sup>+fc) 调整第三、第四、第五层卷积层及全连接层的参数;策略五 (2<sup>th</sup>+3<sup>th</sup>+4<sup>th</sup>+5<sup>th</sup>+fc) 调整第二、第三、第四和第五层卷积层及全连接层的参数;策略六 (1<sup>th</sup>+2<sup>th</sup>+3<sup>th</sup>+4<sup>th</sup>+5<sup>th</sup>+fc) 调整所有层的网络参数.</p>
                </div>
                <div class="p1">
                    <p id="119">由表3可知, 直接使用训练阶段获得的Alex-Sketch Net识别部件, 识别准确率较低, 这是由于草图部件图自身稀疏, 数据量较少, 难以通过单纯的深度学习获得较高的识别效果.针对不同的微调策略, 当采用策略五时, 部件识别准确率为45.41%, 分别高于策略一和策略六25.88%和41.07%, 这是由于策略一仅调整全连接层参数, 并未捕获较好的草图部件特征, 而策略六的全调策略相当于重新训练网络模型, 但是由于数据总量偏少, 在重新训练的过程中调整的参数太多, 导致该策略下网络始终欠拟合, 无法收敛.为了对比其它微调策略, 本次实验选取与其它策略相同的迭代次数, 取第20代的训练网络作为策略六的结果.</p>
                </div>
                <div class="area_img" id="120">
                    <p class="img_tit"><b>表3 不同微调策略识别准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Comparison of recognition accuracy of different fine-tuning strategies </p>
                    <p class="img_note">%</p>
                    <table id="120" border="1"><tr><td><br />权值调整策略</td><td>Alex-<br />Sketch <br />Net部件</td><td>Sketch-<br />Semantic <br />Net草图</td></tr><tr><td><br />策略一 (fc) </td><td>19.53</td><td>45.33</td></tr><tr><td><br />策略二 (5<sup>th</sup>+fc) </td><td>39.40</td><td>64.00</td></tr><tr><td><br />策略三 (4<sup>th</sup>+5<sup>th</sup>+fc) </td><td>43.45</td><td>73.33</td></tr><tr><td><br />策略四 (3<sup>th</sup>+4<sup>th</sup>+5<sup>th</sup>+fc) </td><td>44.74</td><td>77.33</td></tr><tr><td><br />策略五 (2<sup>th</sup>+3<sup>th</sup>+4<sup>th</sup>+5<sup>th</sup>+fc) </td><td>45.41</td><td>80.00</td></tr><tr><td><br />策略六 (1<sup>th</sup>+2<sup>th</sup>+3<sup>th</sup>+4<sup>th</sup>+5<sup>th</sup>+fc) </td><td>4.34</td><td>9.33</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="121">在本组实验中, 采用部件分割的方式虽然扩充了数据集, 但是由于sketch草图部件图形状单一, 色彩匮乏, 在小数据集下微调训练获得的卷积神经网络对部件图的识别效果较差.但在Sketch-Semantic Net识别阶段引入语义树, 识别结果明显上升.在不同的微调策略下, 草图的识别精度分别提升25.8%, 24.6%, 29.88%, 32.59%, 34.59%, 4.99%, 说明通过引入语义树关联部件图与部件所属对象, 较好地利用草图的结构组成关系, 既借助卷积神经网络的特征提取能力, 又不完全依赖卷积后的结果, 提高草图识别的准确率.本组实验表明, 从第二层卷积层开始微调参数的策略更符合本文的数据集特点.</p>
                </div>
                <div class="p1">
                    <p id="122">第二组实验利用不同相似度度量方法, 在不同的融合策略下获得草图识别准确率, 结果如表4所示.在基于部件权重的融合策略中, 部件权重值为部件的周长占整个草图周长的比例.一共分为如下4个策略:策略1为由最小权重的部件潜在语义标签决定草图标签的融合策略, 意在区分草图类间差异较小的草图类别.策略2为由最大权重部件的潜在语义标签决定草图标签的融合策略, 意在区分草图类间差异较大的类别.策略3为不采用基于上下文的语义融合策略, 策略4为本文的基于上下文的语义融合策略下的识别准确率.</p>
                </div>
                <div class="area_img" id="123">
                    <p class="img_tit"><b>表4 不同融合策略下的草图识别准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Sketch recognition accuracy of different fusion strategies </p>
                    <p class="img_note">%</p>
                    <table id="123" border="1"><tr><td><br />相似度<br />度量方法</td><td>策略1</td><td>策略2</td><td>策略3</td><td>策略4</td></tr><tr><td><br />lesk</td><td>51.25</td><td>60.93</td><td>56.93</td><td>80.00</td></tr><tr><td><br />hso</td><td>37.56</td><td>48.58</td><td>56.93</td><td>69.33</td></tr><tr><td><br />jcn</td><td>44.07</td><td>57.43</td><td>56.43</td><td>66.67</td></tr><tr><td><br />res</td><td>51.25</td><td>60.93</td><td>56.93</td><td>80.00</td></tr><tr><td><br />lin</td><td>44.07</td><td>57.43</td><td>51.59</td><td>65.33</td></tr><tr><td><br />vector</td><td>51.25</td><td>60.93</td><td>56.93</td><td>80.00</td></tr><tr><td><br />vectorpair</td><td>51.25</td><td>60.93</td><td>56.93</td><td>80.00</td></tr><tr><td><br />wup</td><td>51.25</td><td>60.93</td><td>56.93</td><td>80.00</td></tr><tr><td><br />lch</td><td>51.25</td><td>60.93</td><td>49, 08</td><td>80.00</td></tr><tr><td><br />path</td><td>51.25</td><td>60.93</td><td>51.09</td><td>80.00</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="124">表4中不同策略计算的侧重点不同, 效果也有差异.基于最小权重部件标签的融合策略和基于最大权重部件标签的融合策略的平均准确率分别为48.45%和59.00%.当采用基于最小权重部件标签的融合策略时, hso识别精度最低, 为37.56%, 低于最高值13.69%, 低于均值10.89%, 当采用基于最大权重部件标签的融合策略时, hso的识别精度最低, 为48.58%, 低于最高值13.35%, 低于均值10.42%.基于最大权重部件标签的融合策略的平均准确率高于基于最小权重部件标签的融合策略10.55%, 这是由于实验所用的数据集中, 类间差距较大的类别占据多数.本文的基于上下文的语义融合策略约束草图的潜在标签, 缓解语义二义性, 草图识别平均准确率为76%, 分别高于基于最小权重部件标签的融合策略、基于最大权重部件标签的融合策略、不采用基于上下文的语义融合策略, 具体值为27.55%、17%、21.02%.实验表明, 基于上下文的语义融合策略不仅考虑草图组成结构关系, 也缓解语义二义性对于草图识别精度的影响.</p>
                </div>
                <div class="p1">
                    <p id="125">第三组实验将本文方法与其它通用方法进行对比, 结果如表5所示.</p>
                </div>
                <div class="area_img" id="126">
                    <p class="img_tit"><b>表5 各草图识别方法的识别准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Recognition accuracy of different sketch recognition methods </p>
                    <p class="img_note">%</p>
                    <table id="126" border="1"><tr><td><br />实验方法</td><td>准确率</td></tr><tr><td><br />HOG-SVM</td><td>56.00</td></tr><tr><td><br />SIFT-Fisher Vector</td><td>61.50</td></tr><tr><td><br />Deep-Sketch</td><td>69.15</td></tr><tr><td><br />Deep-CRNN-sketch</td><td>69.89</td></tr><tr><td><br />sketch-a-net</td><td>74.67</td></tr><tr><td><br />Sketch-Semantic Net</td><td>80.00</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="127">由表5可知, 相比HOG-SVM和SIFT-Fisher Vector, Sketch-Semantic Net识别准确率分别提高24%和18.5%, 相比Deep-Sketch、Deep-CRNN-sketch和Sketch-a-Net, Sketch-Semantic Net识别准确率分别提高10.85%、10.11%和5.33%.实验结果说明在小数据集上融合深度学习和语义树方法的有效性.</p>
                </div>
                <h3 id="128" name="128" class="anchor-tag">4 结 束 语</h3>
                <div class="p1">
                    <p id="129">本文提出数据集扩充方式, 即使用部件分割的方式对小数据集图像进行数据集扩充, 使得通用的草图分割数据集 (如Sketch_dataset数据集) 也可以使用卷积神经网络提取图像特征.同时引入语义树的概念, 联系部件标签与部件所属对象的标签.结合神经网络和语义树, 在提高识别精度的同时, 也弥补端到端草图图像的识别过程不可解释性的缺陷.文中针对标签语义的二义性, 引入潜在语义标签的概念, 采用基于上下文的语义融合策略, 缓和二义性的标签对图像分类的干扰, 增强图像分类的鲁棒性.受现有数据集限制, 本文算法目前只应用在小数据集上.今后随着草图部件数据集的扩充, 将进一步研究算法在大规模数据集上的性能和改进措施.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="165" type="formula" href="images/MSSB201904011_16500.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">赵鹏</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="166" type="formula" href="images/MSSB201904011_16600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">冯晨成</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="167" type="formula" href="images/MSSB201904011_16700.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">韩莉</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="168" type="formula" href="images/MSSB201904011_16800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">纪霞</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Survey of Sketch Based Modeling Systems">

                                <b>[1]</b> KAZMI I K, YOU L H, ZHANG J J.A Survey of Sketch Based Modeling Systems // Proc of the 11th International Conference on Computer Graphics, Imaging and Visualization.Washington, USA:IEEE, 2014:27-36.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201005014&amp;v=MjY4OTBITXFvOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpnVUx6T0x6N0JiYkc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 袁贞明, 金贵朝, 张佳.基于贝叶斯网络的在线草图识别算法.计算机工程, 2010, 36 (5) :32-34. (YUAN Z M, JIN G C, ZHANG J.Online Sketch Recognition Algorithm Based on Bayesian Network.Computer Engineering, 2010, 36 (5) :32-34.) 
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A semantic tree-based approach for sketch-based 3D model retrieval">

                                <b>[3]</b> LI B, LU Y J, SHEN J.A Semantic Tree-Based Approach for Sketch-Based 3D Model Retrieval // Proc of the 23rd International Conference on Pattern Recognition.Washington, USA:IEEE, 2017:3880-3885.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sketch-based image retrieval: Benchmark and bag-of-features descriptors">

                                <b>[4]</b> EITZ M, HILDEBRAND K, BOUBEKEUR T, <i>et al</i>.Sketch-Based Image Retrieval:Benchmark and Bag-of-Features Descriptors.IEEE Transactions on Visualization and Computer Graphics, 2011, 17 (11) :1624-1636.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM2D4DE418C43080AE7BACF7EF13E36FB6&amp;v=MzE4MzRmSVk3SE1HcVc1cTQ1TkYrOE1ESFE1dm1NVW1FNE9QbmlYMmhNMkRMR1NNOGlaQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0wyOHc2ND1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> SCHNEIDER R G, TUYTELAARS T.Sketch Classification and Classification-Driven Analysis Using Fisher Vectors.ACM Transactions on Graphics, 2014, 33 (6) .DOI:10.1145/2661229.2661231.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESBEB941BF65DB91928A7822E6E58DD7B2&amp;v=MjI2OTNGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHdMMjh3NjQ9TmlmT2ZjSE5iTmpJcnYwell1NTdmblU0eGhRYm16aDFTbjJYcW1jd2NjYmdRc2lkQ09OdkZTaVdXcjdKSQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> LI Y, HOSPEDALES T M, SONG Y Z, <i>et al</i>.Free-Hand Sketch Recognition by Multi-kernel Feature Learning.Computer Vision and Image Understanding, 2015, 137:1-11.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepSketch:Deep convolutional neural networks for sketch recognition and similarity search">

                                <b>[7]</b> SEDDATI O, DUPONT S, MAHMOUDI S.DeepSketch:Deep Con-volutional Neural Networks for Sketch Recognition and Similarity Search // Proc of the 13th International Workshop on Content-Based Multimedia Indexing.Washington, USA:IEEE, 2015.DOI:10.1109/CBMI.2015.7153606.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Sketch 2:Deep convolutional neural networks for partial sketch recognition">

                                <b>[8]</b> DSEDDATI O, DUPONT S, MAHMOUDI S.DeepSketch 2:Deep Convolutional Neural Networks for Partial Sketch Recognition // Proc of the 14th International Workshop on Content-Based Multimedia Indexing.Washington, USA:IEEE, 2016.DOI:10.1109/CBMI.2016.7500261.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD481512743524655FC9CC12AF263B32B2&amp;v=MjIyNzBld0g5VE5yWWhCWis0TkNIbzh5bUJnNDB3T1NYMlQyaEF6ZXNDWFI4aWRDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3TDI4dzY0PU5qN0Jhcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> YU Q, YANG Y X, LIU F, <i>et al</i>.Sketch-a-Net:A Deep Neural Network that Beats Humans.International Journal of Computer Vision, 2017, 122 (3) :411-425.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with Deep Convolutional Neural Networks">

                                <b>[10]</b> KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet Cla-ssification with Deep Convolutional Neural Networks // PEREIRA F, BURGES C J C, BOTTOU L, <i>et al</i>., eds.Advances in Neural Information Processing Systems 25.Cambridge, USA:The MIT Press, 2012:1097-1105.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Spatial-Semantic Attention for Fine-Grained Sketch-Based Image Retrieval">

                                <b>[11]</b> SONG J F, YU Q, SONG Y Z, <i>et al</i>.Deep Spatial-Semantic Attention for Fine-Grained Sketch-Based Image Retrieval // Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2017:5552-5561.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=WordNet: An Electronic Lexical Database">

                                <b>[12]</b> FELLBAUM C, MILLER G A.WordNet:An Electronic Lexical Database.Cambridge, USA:The MIT Press, 1998.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using WordNet to disambiguate word senses for text Retrieval">

                                <b>[13]</b> VOORHEES E M.Using WordNet to Disambiguate Word Senses for Text Retrieval // Proc of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM, 1993:171-180.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000073624&amp;v=MjQ2OTRqTnI0OUZaT3dNQ240OW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGc1hhUlE9TmlmSVk3SzdIdA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> ASLANDOGAN Y A, THIER C, YU C T, <i>et al</i>.Using Semantic Contents and WordNet in Image Retrieval.ACM SIGIR Forum, 1997, 31:286-295.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image annotations by combining multiple evidence &amp;amp; WordNet">

                                <b>[15]</b> JIN Y H, KHAN L, WANG L, <i>et al</i>.Image Annotations by Combining Multiple Evidence &amp; Wordnet // Proc of the 13th Annual ACM International Conference on Multimedia.New York, USA:ACM, 2005:706-715.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738050&amp;v=MTUzNDllWnVIeWptVUxiSUpGc1hhUlE9TmlmT2ZiSzdIdEROcVk5RlkrZ0hESGs1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> TOUSCH A M, HERBIN S, AUDIBERT J Y.Semantic Hierarchies for Image Annotation:A Survey.Pattern Recognition, 2012, 45 (1) :333-345.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=WordNet ∶Similarity-Measuring the Relatedness of Concepts[C/OL]">

                                <b>[17]</b> PEDERSEN T, PATWARDHAN S, MICHELIZZI J.WordNet∶∶Similarity-Measuring the Relatedness of Concepts[C/OL].[2018-11-15].http://www.d.umn.edu/～tpederse/Pubs/AAAI04PedersenT.pdf
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Combining local context and Word-Net similarity for word sense identification">

                                <b>[18]</b> LEACOCK C, CHODOROW M.Combining Local Context and WordNet Similarity for Word Sense Identification.WordNet:An Electronic Lexical Database, 1998, 49 (2) :265-283.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Verb semantics and lexical selection">

                                <b>[19]</b> WU Z B, PALMER M.Verbs Semantics and Lexical Selection // Proc of the 32nd Annual Meeting on Association for Computational Linguistics.Stroudsburg, USA:ACL, 1994:133-138.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An information-theoretic definition of similarity">

                                <b>[20]</b> LIN D K.An Information-Theoretic Definition of Similarity // Proc of the 15th International Conference on Machine Learning.San Francisco, USA:Morgan Kaufmann Publishers, 1998:296-304.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using information content to evaluate semantic similarity in a taxonomy">

                                <b>[21]</b> RESNIK P.Using Information Content to Evaluate Semantic Similarity in a Taxonomy // Proc of the 14th International Joint Conference on Artificial Intelligence.New York, USA:ACM, 1995:448-453.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Austenite stability and its effect on the toughness of a high strength ultra-low carbon medium manganese steel plate">

                                <b>[22]</b> JIANG J J, CONRATH D W.Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy // Proc of the International Confe-rence on Research in Computational Linguistics.Berlin, Germany:Springer, 1997:19-33.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lexical Chains as Representations of Context for the Detection and Correction of Malapropisms">

                                <b>[23]</b> HIRST G, ST-ONGE D.Lexical Chains as Representations of Context for the Detection and Correction of Malapropisms.Lecture Notes in Physics, 1997, 728 (9) :123-149.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extended Gloss Overlaps as a Mea-sure of Semantic Relatedness[C/OL]">

                                <b>[24]</b> BANERJEE S, PEDERSEN T.Extended Gloss Overlaps as a Mea-sure of Semantic Relatedness[C/OL].[2018-11-15].http://www.d.umn.edu/～tpederse/Pubs/ijcai03.pdf.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Maximizing Semantic Relatedness top erform Word Sense Disambiguation">

                                <b>[25]</b> PEDERSEN T, BANERJEE S, PATWARDHAN S.Maximizing Semantic Relatedness to Perform Word Sense Disambiguation.Research Report, UMSI 2005/25.Duluth, USA:University of Minnesota, 2005.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMB26B2D2AD9B50623C6E932A0D9AE3A98&amp;v=MjkwNDFGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHdMMjh3NjQ9TmlmSVk4RzZHS1BPMjQwMEVPSjlDWHcvelJWZzdFcDBTMzJUckdZOENNZVhOTE9YQ09OdkZTaVdXcjdKSQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> HUANG Z, FU H B, LAU R W H.Data-Driven Segmentation and Labeling of Freehand Sketches.ACM Transactions on Graphics, 2014, 33 (6) .DOI:10.1145/2661229.2661280.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SCLH201603012&amp;v=MTA3NTdvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emdVTHpPTmk3SFpyRzRIOWZNckk5RVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> 赵鹏, 王斐, 刘慧婷, 等.基于深度学习的手绘草图识别.四川大学学报 (工程科学版) , 2016, 48 (3) :94-99. (ZHAO P, WANG F, LIU H T, <i>et al</i>.Sketch Recognition Using Deep Learning.Journal of Sichuan University (Engineering Science Edition) , 2016, 48 (3) :94-99.) 
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201802002&amp;v=MDg3OTh0R0ZyQ1VSTE9lWmVSbkZ5emdVTHpPTHo3QmFMRzRIOW5Nclk5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b> 赵鹏, 刘杨, 刘慧婷, 等.基于深度卷积-递归神经网络的手绘草图识别方法.计算机辅助设计与图形学学报, 2018, 30 (2) :217-224. (ZHAO P, LIU Y, LIU H T, <i>et al</i>.A Sketch Recognition Method Based on Deep Convolutional-Recurrent Neural Network.Journal of Computer-Aided Design and Computer Graphics, 2018, 30 (2) :217-224.) 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201904011" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201904011&amp;v=MTAxMDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emdVTHpPS0Q3WWJMRzRIOWpNcTQ5RVpZUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
