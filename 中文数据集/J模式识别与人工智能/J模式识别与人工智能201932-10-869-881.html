<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131460942998750%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dMSSB201910001%26RESULT%3d1%26SIGN%3drZwFLXMlOY8zE6p6vRlxHgR87Ic%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201910001&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201910001&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201910001&amp;v=MzIzOTFuRnkva1dyL05LRDdZYkxHNEg5ak5yNDlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#70" data-title="1 核化的相关滤波跟踪 ">1 核化的相关滤波跟踪</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#91" data-title="2 尺度感知的分块协同式相关滤波跟踪算法 ">2 尺度感知的分块协同式相关滤波跟踪算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#97" data-title="&lt;b&gt;2.1&lt;/b&gt; 改进的子块跟踪状态评分策略"><b>2.1</b> 改进的子块跟踪状态评分策略</a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;2.2&lt;/b&gt; 协同的子块运动策略设计"><b>2.2</b> 协同的子块运动策略设计</a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;2.3&lt;/b&gt; 目标位置估计"><b>2.3</b> 目标位置估计</a></li>
                                                <li><a href="#161" data-title="&lt;b&gt;2.4&lt;/b&gt; 目标尺度感知策略"><b>2.4</b> 目标尺度感知策略</a></li>
                                                <li><a href="#179" data-title="&lt;b&gt;2.5&lt;/b&gt; 子块尺寸的自适应更新"><b>2.5</b> 子块尺寸的自适应更新</a></li>
                                                <li><a href="#187" data-title="&lt;b&gt;2.6&lt;/b&gt; 滤波器更新"><b>2.6</b> 滤波器更新</a></li>
                                                <li><a href="#196" data-title="&lt;b&gt;2.7&lt;/b&gt; 算法步骤"><b>2.7</b> 算法步骤</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#205" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#206" data-title="&lt;b&gt;3.1&lt;/b&gt; 实验环境及参数设置"><b>3.1</b> 实验环境及参数设置</a></li>
                                                <li><a href="#212" data-title="&lt;b&gt;3.2&lt;/b&gt; 定量分析"><b>3.2</b> 定量分析</a></li>
                                                <li><a href="#238" data-title="&lt;b&gt;3.3&lt;/b&gt; 定性分析"><b>3.3</b> 定性分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#247" data-title="4 结 束 语 ">4 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#95" data-title="图1 本文算法框架">图1 本文算法框架</a></li>
                                                <li><a href="#99" data-title="图2 本文算法分块示意图">图2 本文算法分块示意图</a></li>
                                                <li><a href="#357" data-title="图3 子块跟踪置信图变化情况">图3 子块跟踪置信图变化情况</a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;表1 PSR、TSCC与&lt;/b&gt;&lt;i&gt;&lt;b&gt;w&lt;/b&gt;&lt;/i&gt;&lt;b&gt;取值说明&lt;/b&gt;"><b>表1 PSR、TSCC与</b><i><b>w</b></i><b>取值说明</b></a></li>
                                                <li><a href="#359" data-title="图4 Airport＿ce序列中PSR和w随帧变化曲线">图4 Airport＿ce序列中PSR和w随帧变化曲线</a></li>
                                                <li><a href="#360" data-title="图5 有遮挡时加权方式产生的目标中心估计偏移">图5 有遮挡时加权方式产生的目标中心估计偏移</a></li>
                                                <li><a href="#361" data-title="图6 Women序列上2种方法的中心位置误差曲线">图6 Women序列上2种方法的中心位置误差曲线</a></li>
                                                <li><a href="#362" data-title="图7 目标尺度变化与局部采样窗口的关系">图7 目标尺度变化与局部采样窗口的关系</a></li>
                                                <li><a href="#219" data-title="&lt;b&gt;表2 7种算法在OTB100数据集上的AUC结果&lt;/b&gt;"><b>表2 7种算法在OTB100数据集上的AUC结果</b></a></li>
                                                <li><a href="#220" data-title="&lt;b&gt;表3 7种算法在OTB100数据集上的DP结果&lt;/b&gt;"><b>表3 7种算法在OTB100数据集上的DP结果</b></a></li>
                                                <li><a href="#221" data-title="&lt;b&gt;表4 8种算法在TC128数据集上的AUC结果&lt;/b&gt;"><b>表4 8种算法在TC128数据集上的AUC结果</b></a></li>
                                                <li><a href="#222" data-title="&lt;b&gt;表5 8种算法在TC128数据集上的DP结果&lt;/b&gt;"><b>表5 8种算法在TC128数据集上的DP结果</b></a></li>
                                                <li><a href="#363" data-title="图8 7种算法在OTB100数据集上的OPE结果曲线">图8 7种算法在OTB100数据集上的OPE结果曲线</a></li>
                                                <li><a href="#363" data-title="图8 7种算法在OTB100数据集上的OPE结果曲线">图8 7种算法在OTB100数据集上的OPE结果曲线</a></li>
                                                <li><a href="#364" data-title="图9 7种算法在TC128数据集上的OPE结果曲线">图9 7种算法在TC128数据集上的OPE结果曲线</a></li>
                                                <li><a href="#365" data-title="图10 两种评分策略在OTB100数据集上OPE结果曲线">图10 两种评分策略在OTB100数据集上OPE结果曲线</a></li>
                                                <li><a href="#365" data-title="图10 两种评分策略在OTB100数据集上OPE结果曲线">图10 两种评分策略在OTB100数据集上OPE结果曲线</a></li>
                                                <li><a href="#242" data-title="图11 3种算法在Dog1序列集上的结果截图">图11 3种算法在Dog1序列集上的结果截图</a></li>
                                                <li><a href="#244" data-title="图12 3种算法在Jogging序列集上的结果截图">图12 3种算法在Jogging序列集上的结果截图</a></li>
                                                <li><a href="#246" data-title="图13 3种算法在Board序列集上的结果截图">图13 3种算法在Board序列集上的结果截图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="366">


                                    <a id="bibliography_1" title=" 黄凯奇,陈晓棠,康运锋,等.智能视频监控技术综述.计算机学报,2015,38(6):1093-1118.(HUANG K Q,CHEN X T,KANG Y F,et al.Intelligent Visual Surveillance:A Review.Chinese Journal of Computers,2015,38(6):1093-1118.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201506001&amp;v=MTQ2MDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1dyL05MejdCZHJHNEg5VE1xWTlGWllRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         黄凯奇,陈晓棠,康运锋,等.智能视频监控技术综述.计算机学报,2015,38(6):1093-1118.(HUANG K Q,CHEN X T,KANG Y F,et al.Intelligent Visual Surveillance:A Review.Chinese Journal of Computers,2015,38(6):1093-1118.)
                                    </a>
                                </li>
                                <li id="368">


                                    <a id="bibliography_2" title=" WU Y,LIM J,YANG M H.Online Object Tracking:A Benchmark // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2013:2411-2418." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online object tracking:a benchmark">
                                        <b>[2]</b>
                                         WU Y,LIM J,YANG M H.Online Object Tracking:A Benchmark // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2013:2411-2418.
                                    </a>
                                </li>
                                <li id="370">


                                    <a id="bibliography_3" title=" NUMMIARO K,KOLLER-MEIER E,VAN GOOL L.An Adaptive Color-Based Particle Filter.Image and Vision Computing,2003,21(1):99-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349994&amp;v=MDM5MDJNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSjE4ZGFoYz1OaWZPZmJLN0h0RE9yWTlFWis4R0JYVTlvQg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         NUMMIARO K,KOLLER-MEIER E,VAN GOOL L.An Adaptive Color-Based Particle Filter.Image and Vision Computing,2003,21(1):99-110.
                                    </a>
                                </li>
                                <li id="372">


                                    <a id="bibliography_4" title=" COMANICIU D,RAMESH V,MEER P.Real-Time Tracking of Non-rigid Objects Using Mean Shift // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2000:142-149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time tracking of non-rigid objects using mean shift">
                                        <b>[4]</b>
                                         COMANICIU D,RAMESH V,MEER P.Real-Time Tracking of Non-rigid Objects Using Mean Shift // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2000:142-149.
                                    </a>
                                </li>
                                <li id="374">


                                    <a id="bibliography_5" title=" AVIDAN S.Support Vector Tracking // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2001.DOI:10.1109/CVPR.2001.990474." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Support vector tracking">
                                        <b>[5]</b>
                                         AVIDAN S.Support Vector Tracking // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2001.DOI:10.1109/CVPR.2001.990474.
                                    </a>
                                </li>
                                <li id="376">


                                    <a id="bibliography_6" title=" HARE S,GOLODETZ S,SAFFARI A,et al.Struck:Structured Output Tracking with Kernels.IEEE Transactions on Pattern Analysis and Machine Intelligence.2016,38(10):2096-2109." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Struck:Structured Output Tracking with Kernels">
                                        <b>[6]</b>
                                         HARE S,GOLODETZ S,SAFFARI A,et al.Struck:Structured Output Tracking with Kernels.IEEE Transactions on Pattern Analysis and Machine Intelligence.2016,38(10):2096-2109.
                                    </a>
                                </li>
                                <li id="378">


                                    <a id="bibliography_7" title=" BOLME D S,BEVERIDGE J R,DRAPER B A,et al.Visual Object Tracking Using Adaptive Correlation Filters // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Re-cognition.Washington,USA:IEEE,2010:2544-2550." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">
                                        <b>[7]</b>
                                         BOLME D S,BEVERIDGE J R,DRAPER B A,et al.Visual Object Tracking Using Adaptive Correlation Filters // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Re-cognition.Washington,USA:IEEE,2010:2544-2550.
                                    </a>
                                </li>
                                <li id="380">


                                    <a id="bibliography_8" title=" HENRIQUES J F,CASEIRO R,MARTINS P,et al.Exploiting the Circulant Structure of Tracking-by-Detection with Kernels // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2012:702-715." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">
                                        <b>[8]</b>
                                         HENRIQUES J F,CASEIRO R,MARTINS P,et al.Exploiting the Circulant Structure of Tracking-by-Detection with Kernels // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2012:702-715.
                                    </a>
                                </li>
                                <li id="382">


                                    <a id="bibliography_9" title=" DANELLJAN M,KHAN S F,FELSBERG M,et al.Adaptive Color Attributes for Real-Time Visual Tracking // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2014:1090-1097." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive Color Attributes for Real-Time Visual Tracking">
                                        <b>[9]</b>
                                         DANELLJAN M,KHAN S F,FELSBERG M,et al.Adaptive Color Attributes for Real-Time Visual Tracking // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2014:1090-1097.
                                    </a>
                                </li>
                                <li id="384">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     HENRIQUES J F,CASEIRO R,MARTINS P,et al.High-Speed Tracking with Kernelized Correlation Filters.IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(3):583-596.</a>
                                </li>
                                <li id="386">


                                    <a id="bibliography_11" title=" WOODLEY T,STENGER B,CIPOLLA R.Tracking Using Online Feature Selection and a Local Generative Model[C/OL].[2019-03-15].http://bjornstenger.github.io/papers/woodley_bmvc2007.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tracking Using Online Feature Selection and a Local Generative Model[C/OL]">
                                        <b>[11]</b>
                                         WOODLEY T,STENGER B,CIPOLLA R.Tracking Using Online Feature Selection and a Local Generative Model[C/OL].[2019-03-15].http://bjornstenger.github.io/papers/woodley_bmvc2007.pdf.
                                    </a>
                                </li>
                                <li id="388">


                                    <a id="bibliography_12" title=" NAM H,HAN B.Learning Multi-domain Convolutional Neural Networks for Visual Tracking // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:4293-4302." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning multi-domain convolutional neural networks for visual tracking">
                                        <b>[12]</b>
                                         NAM H,HAN B.Learning Multi-domain Convolutional Neural Networks for Visual Tracking // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:4293-4302.
                                    </a>
                                </li>
                                <li id="390">


                                    <a id="bibliography_13" title=" BERTINETTO L,VALMADRE J,HENRIQUES J F,et al.Fully-Convolutional Siamese Networks for Object Tracking // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2016:850-865." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully-convolutional siamese networks for object tracking">
                                        <b>[13]</b>
                                         BERTINETTO L,VALMADRE J,HENRIQUES J F,et al.Fully-Convolutional Siamese Networks for Object Tracking // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2016:850-865.
                                    </a>
                                </li>
                                <li id="392">


                                    <a id="bibliography_14" title=" DANELLJAN M,H&#196;GER G,KHAN F S,et al.Learning Spatially Regularized Correlation Filters for Visual Tracking // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2015:4310-4318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">
                                        <b>[14]</b>
                                         DANELLJAN M,H&#196;GER G,KHAN F S,et al.Learning Spatially Regularized Correlation Filters for Visual Tracking // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2015:4310-4318.
                                    </a>
                                </li>
                                <li id="394">


                                    <a id="bibliography_15" title=" LI X,LIU Q,HE Z Y,et al.A Multi-view Model for Visual Tracking via Correlation Filters.Knowledge-Based Systems,2016,113:88-99." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Multi-view Model for Visual Tracking via Correlation Filters">
                                        <b>[15]</b>
                                         LI X,LIU Q,HE Z Y,et al.A Multi-view Model for Visual Tracking via Correlation Filters.Knowledge-Based Systems,2016,113:88-99.
                                    </a>
                                </li>
                                <li id="396">


                                    <a id="bibliography_16" title=" LIU Q,LU X H,HE Z Y,et al.Deep Convolutional Neural Networks for Thermal Infrared Object Tracking.Knowledge-Based Systems,2017,134:189-198." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESDE5D5DED64783B5BF9A99EDEB5646CF4&amp;v=MDAxNTNGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHc3bTJ3SzA9TmlmT2ZjZk5HNlhKMi9veFl1OElCSDlMeW1SbDQwNTBRUXFXMldBd2Y3YVNOc3liQ09OdkZTaVdXcjdKSQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         LIU Q,LU X H,HE Z Y,et al.Deep Convolutional Neural Networks for Thermal Infrared Object Tracking.Knowledge-Based Systems,2017,134:189-198.
                                    </a>
                                </li>
                                <li id="398">


                                    <a id="bibliography_17" title=" 冯棐,吴小俊,徐天阳.基于子空间和直方图的多记忆自适应相关滤波目标跟踪算法.模式识别与人工智能,2018,31(7):612-624.(FENG F,WU X J,XU T Y.Object Tracking with Multiple Memory Learning and Adaptive Correlation Filter Based on Subspace and Histogram.Pattern Recognition and Artificial Intelligence,2018,31(7):612-624.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201807005&amp;v=MDAyOTJGeS9rV3IvTktEN1liTEc0SDluTXFJOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         冯棐,吴小俊,徐天阳.基于子空间和直方图的多记忆自适应相关滤波目标跟踪算法.模式识别与人工智能,2018,31(7):612-624.(FENG F,WU X J,XU T Y.Object Tracking with Multiple Memory Learning and Adaptive Correlation Filter Based on Subspace and Histogram.Pattern Recognition and Artificial Intelligence,2018,31(7):612-624.)
                                    </a>
                                </li>
                                <li id="400">


                                    <a id="bibliography_18" title=" LI Y,ZHU J K.A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2014:254-265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A scale adaptive kernel correlation filter tracker with feature integration">
                                        <b>[18]</b>
                                         LI Y,ZHU J K.A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2014:254-265.
                                    </a>
                                </li>
                                <li id="402">


                                    <a id="bibliography_19" title=" DANELLJAN M,H&#196;GER G,KHAN F S,et al.Accurate Scale Estimation for Robust Visual Tracking // Proc of the British Machine Vision Conference.Nottingham,UK:BMVA Press,2014.DOI:10.5244/C.28.65." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate scale estimation for robust visual tracking">
                                        <b>[19]</b>
                                         DANELLJAN M,H&#196;GER G,KHAN F S,et al.Accurate Scale Estimation for Robust Visual Tracking // Proc of the British Machine Vision Conference.Nottingham,UK:BMVA Press,2014.DOI:10.5244/C.28.65.
                                    </a>
                                </li>
                                <li id="404">


                                    <a id="bibliography_20" title=" AKIN O,ERDEM E,ERDEM A,et al.Deformable Part-Based Tracking by Coupled Global and Local Correlation Filters.Journal of Visual Communication and Image Representation,2016,38:763-774." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES11BF436F828C498A30E0327808729EA2&amp;v=MTIzODJpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3N20yd0swPU5pZk9mYks1YktmSXJJa3piT2tIZjNnd3gyY1E2a3A5UzMzbHBCSTlmckNkTU11ZENPTnZGUw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         AKIN O,ERDEM E,ERDEM A,et al.Deformable Part-Based Tracking by Coupled Global and Local Correlation Filters.Journal of Visual Communication and Image Representation,2016,38:763-774.
                                    </a>
                                </li>
                                <li id="406">


                                    <a id="bibliography_21" title=" LIU T,WANG G,YANG Q X.Real-Time Part-Based Visual Tracking via Adaptive Correlation Filters // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:4902-4912." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time part-based visual tracking via adaptive correlation filters">
                                        <b>[21]</b>
                                         LIU T,WANG G,YANG Q X.Real-Time Part-Based Visual Tracking via Adaptive Correlation Filters // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:4902-4912.
                                    </a>
                                </li>
                                <li id="408">


                                    <a id="bibliography_22" title=" LI Y,ZHU J K,HOI S C H.Reliable Patch Trackers:Robust Visual Tracking by Exploiting Reliable Patches // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:353-361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reliable patch trackers:robust visual tracking by exploiting reliable patches">
                                        <b>[22]</b>
                                         LI Y,ZHU J K,HOI S C H.Reliable Patch Trackers:Robust Visual Tracking by Exploiting Reliable Patches // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:353-361.
                                    </a>
                                </li>
                                <li id="410">


                                    <a id="bibliography_23" >
                                        <b>[23]</b>
                                     WU Y,LIM J,YANG M H.Object Tracking Benchmark.IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(9):1834-1848.</a>
                                </li>
                                <li id="412">


                                    <a id="bibliography_24" title=" LIANG P P,BLASCH E,LING H B.Encoding Color Information for Visual Tracking:Algorithms and Benchmark.IEEE Transactions on Image Processing,2015,24(12):5630-5644." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Encoding color information for visual tracking:Algorithms and bechmark,&amp;quot;">
                                        <b>[24]</b>
                                         LIANG P P,BLASCH E,LING H B.Encoding Color Information for Visual Tracking:Algorithms and Benchmark.IEEE Transactions on Image Processing,2015,24(12):5630-5644.
                                    </a>
                                </li>
                                <li id="414">


                                    <a id="bibliography_25" title=" DANELLJAN M,H&#196;GER G,KHAN F S,et al.Discriminative Scale Space Tracking.IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,39(8):1561-1575." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative scale space tracking">
                                        <b>[25]</b>
                                         DANELLJAN M,H&#196;GER G,KHAN F S,et al.Discriminative Scale Space Tracking.IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,39(8):1561-1575.
                                    </a>
                                </li>
                                <li id="416">


                                    <a id="bibliography_26" title=" BERTINETTO L,VALMADRE J,GOLODETZ S,et al.Staple:Complementary Learners for Real-Time Tracking // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:1401-1409." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for realtime tracking">
                                        <b>[26]</b>
                                         BERTINETTO L,VALMADRE J,GOLODETZ S,et al.Staple:Complementary Learners for Real-Time Tracking // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:1401-1409.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(10),869-881 DOI:10.16451/j.cnki.issn1003-6059.201910001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>尺度感知的分块协同式相关滤波跟踪算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E7%81%BF&amp;code=42910506&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈灿</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E6%98%AD%E7%82%AF&amp;code=06680030&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈昭炯</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A1%BE%E6%9D%A8&amp;code=42910505&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">顾杨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B6%E4%B8%9C%E6%AF%85&amp;code=06682886&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">叶东毅</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6%E6%95%B0%E5%AD%A6%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0094575&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福州大学数学与计算机科学学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6%E7%A9%BA%E9%97%B4%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%85%B1%E4%BA%AB%E6%95%99%E8%82%B2%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福州大学空间数据挖掘与信息共享教育部重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于分块的相关滤波跟踪算法在处理目标尺度变化和遮挡问题时,对局部子块跟踪状态的评估及局部子块与尺度变化的关系刻画不够准确.针对此问题,文中提出尺度感知的分块协同式相关滤波跟踪算法.首先提出结合时序平滑约束的局部子块遮挡判别方法,改进现有算法的评分策略.设计子块协同运动策略,使被遮挡或形变的子块跟随未被遮挡的子块趋向正确的位置.同时发现跟踪过程中子块聚散变化的分布位置与目标尺度之间的比例关系,实现对目标尺度变化的感知和大小估计.实验表明,文中算法性能较优.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%BA%E5%BA%A6%E6%84%9F%E7%9F%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">尺度感知;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E5%9D%97%E5%8D%8F%E5%90%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分块协同;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%85%B3%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相关滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标跟踪;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    陈灿,硕士研究生,主要研究方向为图像处理.E-mail:756154017@qq.com.&lt;image id="353" type="formula" href="images/MSSB201910001_35300.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    *陈昭炯(通讯作者),硕士,教授,主要研究方向为智能图像处理、计算智能．E-mail:chenzj@fzu.edu.cn.&lt;image id="354" type="formula" href="images/MSSB201910001_35400.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    顾杨,硕士研究生,主要研究方向为智能图像处理．E-mail:573711345@qq.com.&lt;image id="355" type="formula" href="images/MSSB201910001_35500.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    叶东毅,博士,教授,主要研究方向为计算智能、数据挖掘．E-mail:yiedy@fzu.edu.cn.&lt;image id="356" type="formula" href="images/MSSB201910001_35600.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(No.61672158);</span>
                                <span>福建省自然科学基金项目(No.2018J01798)资助;</span>
                    </p>
            </div>
                    <h1><b>Scale-Aware Partition-Based Cooperative Correlation Filter Tracking Algorithm</b></h1>
                    <h2>
                    <span>CHEN Can</span>
                    <span>CHEH Zhaojiong</span>
                    <span>GU Yang</span>
                    <span>YE Dongyi</span>
            </h2>
                    <h2>
                    <span>College of Mathematics and Computer Science,Fuzhou University</span>
                    <span>Key Laboratory of Spatial Data Mining and Information Sharing of Ministry of Education,Fuzhou University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>When partition-based correlation filtering tracking algorithm is applied to deal with scale changes and target occlusion problems, the estimation of local sub-block state tracking and the relationship between local sub-block and scale change are inaccurate.To address this issue, a scale-aware partition-based cooperative correlation filter tracking algorithm is proposed. A method of local sub-block occlusion discrimination based on time-sequence smooth constraint is adopted, and the scoring strategy of the existing algorithm is improved. A cooperative motion strategy for sub-blocks is then designed to make the occluded or the deformed sub-blocks follow the normal ones to their due positions. And the ratio between target scale and distributed location of sub-blocks aggregation and dispersion is discovered to perceive target scale changes and estimate sizes. Experiments indicate that the proposed algorithm achieves better performance.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Scale-Aware&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Scale-Aware;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Partition-Based%20Cooperation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Partition-Based Cooperation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Correlation%20Filter&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Correlation Filter;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Target%20Tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Target Tracking;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    CHEN Can,master student. His research interests include image processing.;
                                </span>
                                <span>
                                    CHEH Zhaojiong(Corresponding author), master,professor. Her research interests include intelligent image processing and computational intelligence.;
                                </span>
                                <span>
                                    GU Yang,master student. Her research interests include image processing.;
                                </span>
                                <span>
                                    YE Dongyi,Ph. D.,professor. His research interests include computational intelligence and data mining.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-28</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China(No.61672158);</span>
                                <span>Natural Science Foundation of Fujian Province(No.2018J01798);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="65">目标跟踪是计算机视觉领域中一个具有挑战性的任务,在智能视频监控<citation id="418" type="reference"><link href="366" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、无人驾驶等领域具有广泛的应用.目标跟踪的任务是给定运动目标初始状态,在后续视频序列中获取目标的位置、大小等信息.目标跟踪效果受诸多因素影响,如光照变化、尺度变化、遮挡和形态变化、快速运动和模糊运动等,因此难以得到一个可以应对各类环境的跟踪算法<citation id="419" type="reference"><link href="368" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.现有的目标跟踪方法依据建模策略的不同,主要分为生成式方法<citation id="421" type="reference"><link href="370" rel="bibliography" /><link href="372" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>、判别式方法<citation id="422" type="reference"><link href="374" rel="bibliography" /><link href="376" rel="bibliography" /><link href="378" rel="bibliography" /><link href="380" rel="bibliography" /><link href="382" rel="bibliography" /><link href="384" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>和两者结合的方法<citation id="420" type="reference"><link href="386" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="66">生成式方法首先根据当前信息对目标的外观特征建模,再基于最小化重构误差的搜索方式定位下一帧目标.代表性的方法是基于粒子滤波的跟踪方法<citation id="423" type="reference"><link href="370" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>.近年来,有学者引入深度神经网络对目标特征进行刻画<citation id="424" type="reference"><link href="388" rel="bibliography" /><link href="390" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>,但这类算法需要大量的训练样本,实时性难以满足实际需求,在应用层面仍有局限性.生成式方法着重对目标自身的建模,缺乏对背景信息的考量,在目标与背景相似或背景复杂的情况下,跟踪效果难以保证.</p>
                </div>
                <div class="p1">
                    <p id="67">不同于生成式方法,判别式方法将跟踪问题转化为分类问题,通过在线训练得到分类器,对目标的可能状态进行采样获得候选目标,再使用训练好的分类器选取得分最高的样本作为下一帧的目标.常用的分类器有SVM分类器<citation id="425" type="reference"><link href="374" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>和相关滤波器<citation id="426" type="reference"><link href="378" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>等.基于相关滤波器的跟踪算法计算简单,实时性能较好,近年来成为研究热点.代表性的方法是Henriques等<citation id="427" type="reference"><link href="384" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出的核化的相关滤波跟踪算法(Kernelized Correlation Filters, KCF),它成为后续相关改进工作的基本框架.</p>
                </div>
                <div class="p1">
                    <p id="68">在实时性需求较高的应用场景中,基于相关滤波的算法可以获得不错的跟踪效果,但仍存在不足.近期有学者从滤波器的训练方式<citation id="428" type="reference"><link href="392" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>和多个相关滤波跟踪器的集成<citation id="433" type="reference"><link href="394" rel="bibliography" /><link href="396" rel="bibliography" /><link href="398" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>等方面进行改进,获得一定的提升作用.作为模板类方法,相关滤波跟踪框架在应用过程中存在两个问题,在很大程度上制约算法性能.1)滤波器模板本身尺度不可变,导致跟踪器无法适应目标存在尺度变化的应用场景.针对这一问题,Li 等<citation id="429" type="reference"><link href="400" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>对目标邻域进行7个尺度的窗口样本采集,分别进行相关滤波跟踪,以响应值指标确定目标尺度.Danelljan等<citation id="430" type="reference"><link href="402" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>将采集的多个尺度样本一维展开再堆叠,然后进行独立的一维相关滤波以确定目标最终的尺度.这两种方法本质上都是近似穷举式的搜索方法,最优尺度的获取和算法的效率受尺度采样数量的制约.2)当目标发生遮挡或形变时,滤波器的跟踪性能下降.一种有效的解决方法是对目标的局部分别进行建模<citation id="434" type="reference"><link href="404" rel="bibliography" /><link href="406" rel="bibliography" /><link href="408" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>,当目标发生局部遮挡时,未遮挡部分仍然可以提供有效的定位信息.但目标经过分块之后,相比整体,子块的表征能力有所下降.Akin等<citation id="431" type="reference"><link href="404" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>将目标分成两个子块,分别使用KCF相关滤波器进行跟踪,并且通过子块运动的相对位置关系解决目标的尺度估计问题.当目标尺度发生变化时,重置滤波器模板进行重新学习.这种策略会导致滤波器丢失历史信息,并且使用当前帧重新训练滤波器还可能混入背景等干扰信息.对于分块模型,如何结合不同分块的跟踪信息十分重要.Liu等<citation id="432" type="reference"><link href="406" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>根据子块前后两帧跟踪置信图的相似度提出平滑约束,评估子块跟踪效果,自适应调整每个子块的贡献权重,通过对联合置信图进行粒子滤波确定目标位置和尺度.但是约束只考虑前后两帧的差异,若出现连续两帧跟踪状态不好的置信图时,可能出现较小差异得出较大权重的情况,因此无法正确评价子块跟踪状态.同时该方法基于粒子滤波进行尺度和坐标估计,耗时太多,限制算法的实时性能.</p>
                </div>
                <div class="p1">
                    <p id="69">上述基于分块思想的跟踪算法展示的效果表明:分块是一种可行的解决尺度变化和遮挡问题的策略,但上述工作对于子块及其相互关系的分析、评估还不够深入准确,导致遮挡发生时判断可能失偏,并且也未能有效刻画子块与尺度变化的关系.针对此问题,本文在分块KCF基础上,提出尺度感知的分块协同式相关滤波跟踪算法.首先提出结合时序平滑约束的子块遮挡判别方法.相关滤波的性质表明,跟踪状态良好的子块的相关滤波置信图应符合高斯单峰分布,而峰值旁瓣比(Peak to Sidelobe Ratio, PSR)<citation id="435" type="reference"><link href="378" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>难以准确捕捉这一特征.本文引入置信图时序平滑约束(Timing Smoothing Constraint of Confidence Map, TSCC),通过捕获置信图时序峰形变化情况,估计子块遮挡程度.再基于对子块的状态评估,设计子块协同运动策略,结合目标整体和局部的跟踪信息,使被遮挡或形变的子块可以跟随较好的子块并趋向正确位置.最后,发现跟踪过程中子块聚散变化的分布位置与目标尺度之间的比例关系,实现对目标尺度及变化的感知和大小的估计,并且这种尺度估计策略无需额外的计算量.实验表明,本文算法在OTB100测试集<citation id="436" type="reference"><link href="410" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>和TC128测试集<citation id="437" type="reference"><link href="412" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>上性能较优.</p>
                </div>
                <h3 id="70" name="70" class="anchor-tag">1 核化的相关滤波跟踪</h3>
                <div class="p1">
                    <p id="71">KCF的实现主要分为滤波器训练、目标预测和模型更新.</p>
                </div>
                <div class="p1">
                    <p id="72">1)滤波器训练.在视频序列的第<i>t</i>帧,以目标中心截取大小为<i>m</i>×<i>n</i>的图像块<i><b>x</b></i>,对图像块进行移位操作获得样本<i>x</i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>,其中,<i>i</i>=1,2,…,<i>m</i>, <i>j</i>=1,2,…,<i>n</i>.<i>y</i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>为样本<i>x</i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>期望的置信得分,由高斯分布生成,越靠近中心的样本得分越高.记相关滤波器</p>
                </div>
                <div class="p1">
                    <p id="73"><i>f</i>(<i><b>x</b></i>)=<i><b>w</b></i><sup>T</sup><i>φ</i>(<i><b>x</b></i>),</p>
                </div>
                <div class="p1">
                    <p id="74">由如下问题最小化获得:</p>
                </div>
                <div class="p1">
                    <p id="75"><mathml id="250"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">w</mi></munder><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><mo stretchy="false">(</mo></mstyle><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">φ</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>,      (1)</p>
                </div>
                <div class="p1">
                    <p id="76">其中,<i>λ</i>‖<i><b>w</b></i>‖<sup>2</sup>为正则项,用于控制训练的过拟合.<i>φ</i>(<i><b>x</b></i>)函数将样本<i><b>x</b></i>映射到Hilbert空间,提升滤波器的分类性能,因此该回归问题的解可写为</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">w</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><mi>α</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mi mathvariant="bold-italic">φ</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">定义核函数</p>
                </div>
                <div class="p1">
                    <p id="79"><i>k</i>(<i><b>x</b></i>,<i><b>x</b></i>′)=<i>φ</i>(<i><b>x</b></i>)<sup>T</sup><i>φ</i>(<i><b>x</b></i>′),</p>
                </div>
                <div class="p1">
                    <p id="80"><i>α</i>可定义如下:</p>
                </div>
                <div class="p1">
                    <p id="81"><i>α</i>=(<i><b>K</b></i>+<i>λ</i><i><b>I</b></i>)<sup>-1</sup><i><b>y</b></i><sub><i>s</i></sub>,</p>
                </div>
                <div class="p1">
                    <p id="82">其中,<i>α</i>由元素<i>α</i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>组成,<i><b>y</b></i><sub><i>s</i></sub>为得分<i>y</i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>构成的期望的置信图矩阵,<i><b>I</b></i>为单位矩阵,<i><b>K</b></i>为核矩阵.基于核矩阵具有的循环性质<citation id="438" type="reference"><link href="380" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>,结合傅里叶变换,有</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><mo>=</mo><mfrac><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>s</mi></msub></mrow><mrow><mover accent="true"><mi mathvariant="bold-italic">k</mi><mo>^</mo></mover><msup><mrow></mrow><mrow><mi>x</mi><mi>x</mi></mrow></msup><mo>+</mo><mi mathvariant="bold-italic">λ</mi></mrow></mfrac><mo>,</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">k</mi><msup><mrow></mrow><mrow><mi>x</mi><mi>z</mi></mrow></msup><mo>=</mo></mtd></mtr><mtr><mtd><mi>exp</mi><mo stretchy="false">[</mo><mo>-</mo><mfrac><mn>1</mn><mrow><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">(</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo><mn>2</mn><mi>F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>,</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中,^表示离散傅里叶变换,<i>F</i><sup>-1</sup>表示离散傅里叶反变换,*表示傅里叶变换的复共轭,<i><b>k</b></i><sup><i>xz</i></sup>表示输入<i><b>x</b></i>和<i><b>z</b></i>的高斯核相关矩阵.</p>
                </div>
                <div class="p1">
                    <p id="85">2)目标预测.以第<i>t</i>帧确定的目标坐标,在第<i>t</i>+1帧上截取<i>m</i>×<i>n</i>的图像块<i><b>z</b></i>,滤波器预测的相关置信图矩阵为</p>
                </div>
                <div class="p1">
                    <p id="86"><mathml id="251"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">y</mi><mo>=</mo><mi>F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">k</mi><mo>^</mo></mover><msup><mrow></mrow><mrow><mi>x</mi><msup><mrow></mrow><mi>t</mi></msup><mi>z</mi></mrow></msup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup><mo stretchy="false">)</mo></mrow></math></mathml>,      (2)</p>
                </div>
                <div class="p1">
                    <p id="87">其中,<mathml id="252"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover></math></mathml><sup><i>t</i></sup>为第<i>t</i>帧学习后的滤波器系数,则第<i>t</i>+1帧的目标位置偏移量由<i><b>y</b></i>的最大值位置与中心位置的偏移量确定.</p>
                </div>
                <div class="p1">
                    <p id="88">3)模型更新.为了适应目标因环境或自身发生的一些表观变化,对于滤波器需要进行在线学习.KCF把<mathml id="253"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover></math></mathml><sup><i>t</i></sup>和<mathml id="254"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover></math></mathml><sup><i>t</i></sup>作为滤波器模型,采用线性内插的方式进行更新:</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>η</mi><mo stretchy="false">)</mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><msup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>+</mo><mi>η</mi><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><mo>,</mo></mtd></mtr><mtr><mtd><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>η</mi><mo stretchy="false">)</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>+</mo><mi>η</mi><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>^</mo></mover><mo>,</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">其中<i>η</i>为模型的学习率.</p>
                </div>
                <h3 id="91" name="91" class="anchor-tag">2 尺度感知的分块协同式相关滤波跟踪算法</h3>
                <div class="p1">
                    <p id="93">传统的相关滤波以目标整体进行建模,以固定的学习率进行模型更新.当目标发生遮挡时,更新导致学习过多的背景信息,模型随时间积累容易发生漂移.</p>
                </div>
                <div class="p1">
                    <p id="94">为了解决上述问题,本文结合分块思想进行相关滤波跟踪.基于局部分块模型的优势在于,当目标发生部分遮挡或部分形变时,未被遮挡或形变程度不高的部分仍可继续对目标进行跟踪,同时可以有选择地更新未被遮挡的局部跟踪器,抑制模型学习错误的背景信息.本文算法整体框架如图1所示.</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文算法框架" src="Detail/GetImg?filename=images/MSSB201910001_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文算法框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Framework of the proposed algorithm</p>

                </div>
                <div class="p1">
                    <p id="96">首先采用格栅化的方式对目标进行四分块并分别进行相关滤波跟踪,通过结合PSR和时序平滑约束确定子块权重.再通过协同的子块运动策略,允许局部子块进行带约束的运动,利用子块的空间拓扑关系,实现全局目标尺度的确定.最后通过子块的空间拓扑关系调整子块采样窗口的尺寸,解决子块可能出现覆盖不足或堆叠现象的问题.</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97"><b>2.1</b> 改进的子块跟踪状态评分策略</h4>
                <div class="p1">
                    <p id="98">本文采用图2的分块方法,对每个局部子块运行一个相关滤波跟踪器,同时对目标整体运行一个相关滤波器.目标经过分块之后,相比整体,子块的表征能力有所下降,因此使用子块进行单独跟踪时,会比使用全局目标进行跟踪的方式更容易发生漂移现象.需要对子块的跟踪状态进行评估,再适当调整子块位置.</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 本文算法分块示意图" src="Detail/GetImg?filename=images/MSSB201910001_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 本文算法分块示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Partition of the proposed algorithm</p>

                </div>
                <div class="p1">
                    <p id="100">相关滤波跟踪器训练过程中样本的置信得分<i>y</i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>由单峰高斯分布获得.对于一帧新图像,如果目标发生的变化越少,经过跟踪器得到的样本得分置信图越接近高斯分布,越呈现高斯单峰状态.当目标形变或遮挡变化越严重,置信图与高斯分布的差异越大.因此置信图的峰形特征可以为目标的遮挡判断提供指示.</p>
                </div>
                <div class="p1">
                    <p id="101">Bolme等<citation id="439" type="reference"><link href="378" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出使用相关滤波置信图的峰值旁瓣比(PSR),用于评价滤波器跟踪的效果,即</p>
                </div>
                <div class="p1">
                    <p id="102"><mathml id="255"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mi>S</mi><mi>R</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>=</mo><mfrac><mrow><mi>max</mi><mo stretchy="false">{</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo stretchy="false">}</mo><mo>-</mo><mi>μ</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow><mrow><mi>σ</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></mfrac></mrow></math></mathml>,      (3)</p>
                </div>
                <div class="p1">
                    <p id="103">其中,<i>μ</i><mathml id="256"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>和<i>σ</i><mathml id="257"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>分别表示第<i>i</i>个子块在第<i>t</i>帧图像的跟踪置信图<i>y</i><mathml id="258"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>的峰值旁瓣的均值和方差,<i>i</i>=1,2,3,4.置信图的PSR表示最优目标样本评分与其邻域周围样本评分的比值.</p>
                </div>
                <div class="p1">
                    <p id="104">Bolme等<citation id="440" type="reference"><link href="378" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>指出:当目标跟踪正常、变化缓慢时,PSR值在10～60之间变动;当PSR值小于7时,表示目标发生遮挡等变化,跟踪失败.</p>
                </div>
                <div class="p1">
                    <p id="105">PSR在一定程度上可以刻画置信图最大响应值领域的峰形状态,但是当目标出现遮挡等背景干扰时,滤波置信图可能出现伪峰值和负值,造成PSR对于扰动的敏感度降低,继而导致遮挡状态判断失误,跟踪错误并发生漂移.</p>
                </div>
                <div class="p1">
                    <p id="106">对子块滤波器的置信图进行可视化的处理,图3为OTB数据集中Dudek子集上第190～230帧的跟踪情况.由图可看出,目标在未发生遮挡时,置信图的高响应区呈现聚拢状态,连续帧之间变化较平缓.当遮挡发生时,置信图出现明显变化,特别是第210帧出现2个峰值.在215帧遮挡物离开目标后,置信图重新呈现聚拢单峰值状态.</p>
                </div>
                <div class="area_img" id="357">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_35700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 子块跟踪置信图变化情况" src="Detail/GetImg?filename=images/MSSB201910001_35700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 子块跟踪置信图变化情况  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_35700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Changes of sub-block tracking confidence map</p>

                </div>
                <div class="p1">
                    <p id="115">模板类的目标跟踪算法通常要求目标发生的变化平缓而不突变,才能通过衡量目标与模板的相似性跟踪目标.因此,基于相关滤波器训练的跟踪算法得到的置信图的变化也平缓不突变,近似高斯单峰分布.通过捕获置信图突变状况,可有效评估目标的变化状态.</p>
                </div>
                <div class="p1">
                    <p id="116">基于上述分析,本文设计改进的子块跟踪状态评分方法,引入置信图时序平滑约束(TSCC),计算当前帧的置信图与历史置信图的峰形差异,用于衡量置信图时序上的突变程度,并将TSCC与PSR结合,设计局部分块的跟踪评分<i>w</i><mathml id="259"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>:</p>
                </div>
                <div class="p1">
                    <p id="117" class="code-formula">
                        <mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>=</mo><mo stretchy="false">(</mo><mi>Ρ</mi><mi>S</mi><mi>R</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo stretchy="false">)</mo><msup><mrow></mrow><mrow><mtext>ϑ</mtext><mo>-</mo><mi>Τ</mi><mi>S</mi><mi>C</mi><mi>C</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></msup><mo>,</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>Τ</mi><mi>S</mi><mi>C</mi><mi>C</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>=</mo><mfrac><mrow><mn>2</mn><mo stretchy="false">∥</mo><mi>u</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>-</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>⊕</mo><mi>Δ</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo stretchy="false">∥</mo><msub><mrow></mrow><mtext>F</mtext></msub></mrow><mrow><mrow><mo stretchy="false">∥</mo><mi>u</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>2</mn></msub><mo>+</mo><mrow><mo stretchy="false">∥</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>⊕</mo><mi>Δ</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>u</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>ρ</mi><mo stretchy="false">)</mo><mi>u</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>+</mo><mi>ρ</mi><mo stretchy="false">(</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>⊕</mo><mi>Δ</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo stretchy="false">)</mo><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="118">其中:<i>u</i><mathml id="260"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>为第<i>i</i>个子块跟踪器从第1帧至第<i>t</i>帧对于置信图的记忆,需要随跟踪过程进行学习;<i>ρ</i>为置信图学习率,当判断目标发生遮挡时,不执行历史置信图更新操作,随着置信图的学习,<i>u</i><mathml id="261"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>中较早历史帧的记忆比重呈指数递减;⊕为移位操作;<i>Δ</i><mathml id="262"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>为置信图最大值与矩阵中心点的偏移量;<image id="358" type="formula" href="images/MSSB201910001_35800.jpg" display="inline" placement="inline"><alt></alt></image>⊕<i>Δ</i><mathml id="264"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>表示将<i>y</i><mathml id="265"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>的最大值平移到矩阵中心点.</p>
                </div>
                <div class="p1">
                    <p id="119">表1为TSCC、PSR和<i>w</i>的含义与取值范围,由表可看出,TSCC与PSR量级差异较大,线性组合方式难以均衡反映两者的影响,而指数方式更能拉开正常和遮挡场景下目标变化后评分值的差异幅度,有利于区分不同的跟踪状态.因此,本文采用指数方式定义结合两者的评分指标</p>
                </div>
                <div class="p1">
                    <p id="120"><i>w</i>=<i>PSR</i><sup>ϑ-</sup><sup><i>TSCC</i></sup>,</p>
                </div>
                <div class="p1">
                    <p id="121">其中,平衡系数ϑ用于协调二者的贡献度,ϑ值越大,指数曲线越陡峭,TSCC的波动对<i>w</i>的影响越大.</p>
                </div>
                <div class="area_img" id="122">
                    <p class="img_tit"><b>表1 PSR、TSCC与</b><i><b>w</b></i><b>取值说明</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Description of values of PSR,TSCC and <i>w</i></p>
                    <p class="img_note"></p>
                    <table id="122" border="1"><tr><td><br />评价指标</td><td>PSR</td><td>TSCC</td><td><i>w</i></td></tr><tr><td><br />意义</td><td>评价当前帧<br />置信图峰形<br />(越大越好)</td><td>评价时序上置<br />信图峰形变化<br />(越小越好)</td><td>综合评价当前<br />帧跟踪状态<br />(越大越好)</td></tr><tr><td><br />理论取值</td><td>≥0</td><td>0～1</td><td>≥0</td></tr><tr><td><br />实验取值</td><td>3～60</td><td>0～0.3</td><td>5～300</td></tr><tr><td><br />目标正常</td><td>≥7</td><td>≤0.2</td><td>≥15</td></tr><tr><td><br />目标变化</td><td>≤7</td><td>≥0.2</td><td>≤15</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="123">为了均衡PSR和TSCC两项指标的综合影响效果,同时拉大跟踪好与差情况的区分度,评分<i>w</i>的指数应大于1,即</p>
                </div>
                <div class="p1">
                    <p id="124">ϑ-<i>TSCC</i>&gt;1,</p>
                </div>
                <div class="p1">
                    <p id="125">故有ϑ&gt;1.3.本文进一步根据实验观察发现,ϑ=1.5时TSCC对于跟踪评分<i>w</i>的贡献相对合适.</p>
                </div>
                <div class="p1">
                    <p id="126">为了方便对比,以TC128测试集中Airport_ce视频序列为例,绘制目标左上角子块的PSR和<i>w</i>的变化情况,如图4所示.</p>
                </div>
                <div class="p1">
                    <p id="127">Airport_ce序列帧数较短,有一个较明显的遮挡变化(65帧至80帧).由图4中可以看出,当目标发生遮挡时,PSR值与正常值的差异不大,难以区分,而本文的评分指标<i>w</i>的值均小于阈值15,并且与无遮挡情况差异明显.</p>
                </div>
                <div class="area_img" id="359">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_35900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Airport＿ce序列中PSR和w随帧变化曲线" src="Detail/GetImg?filename=images/MSSB201910001_35900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 Airport＿ce序列中PSR和w随帧变化曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_35900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Curves of PSR and w changing with frame in Airport＿ce sequence</p>

                </div>
                <h4 class="anchor-tag" id="133" name="133"><b>2.2</b> 协同的子块运动策略设计</h4>
                <div class="p1">
                    <p id="134">子块出现遮挡、剧烈形变等情况时,如果任由局部跟踪器以局部信息进行坐标更新,会导致子块发生漂移,进而影响对目标主体位置的判断.目标局部从属于目标整体,各分块的运动应具有一致性.本文根据这种运动趋势结合2.1节的子块跟踪评分设计协同运动策略.协同运动策略可以通过其它局部子块或全局跟踪提供的信息,使被遮挡子块的运动处于其正确区域附近,进而抑制子块漂移的现象.当遮挡物离开时,子块坐标可以重新进行正确的更新.这种协同的运动子块坐标的更新如下:</p>
                </div>
                <div class="area_img" id="135">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201910001_13500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="137">其中,<i>pos</i><mathml id="266"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>=(<i>x</i><mathml id="267"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>,<i>y</i><mathml id="268"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>)为第<i>i</i>子块第<i>t</i>帧时的中心坐标,<i>Δ</i><mathml id="269"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>为第<i>i</i>个子块滤波器确定的偏移量,<mathml id="270"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Δ</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>为结合全局信息得到的偏移量.不同子块对应的<mathml id="271"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Δ</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>计算如下:</p>
                </div>
                <div class="p1">
                    <p id="138" class="code-formula">
                        <mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Δ</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>β</mi><msub><mrow></mrow><mi>i</mi></msub><mi>Δ</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>β</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>A</mi></munderover><mi>w</mi></mstyle><msubsup><mrow></mrow><mi>j</mi><mi>t</mi></msubsup><mi>Δ</mi><msubsup><mrow></mrow><mi>j</mi><mi>t</mi></msubsup><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>A</mi></munderover><mi>w</mi></mstyle><msubsup><mrow></mrow><mi>j</mi><mi>t</mi></msubsup><mo stretchy="false">)</mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>,</mo></mtd><mtd columnalign="left"><mi>A</mi><mo>≠</mo><mo>∅</mo></mtd></mtr><mtr><mtd columnalign="left"><mi>Δ</mi><msubsup><mrow></mrow><mi>g</mi><mi>t</mi></msubsup><mo>,</mo></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>它</mtext></mtd></mtr></mtable></mrow></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="139">其中,<mathml id="272"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mo>=</mo><mo stretchy="false">{</mo><mi>j</mi><mo stretchy="false">|</mo><mi>w</mi><msubsup><mrow></mrow><mi>j</mi><mi>t</mi></msubsup><mo>&gt;</mo><mi>t</mi><mi>h</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>h</mi><mi>o</mi><mi>l</mi><mi>d</mi><mo stretchy="false">}</mo></mrow></math></mathml>为其它评分<i>w</i><mathml id="273"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>t</mi></msubsup></mrow></math></mathml>高于阈值的可靠子块下标的集合,<i>Δ</i><mathml id="274"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>g</mi><mi>t</mi></msubsup></mrow></math></mathml>为全局相关滤波器得到的偏移量,<i>β</i><sub><i>i</i></sub>为调节参数,协调子块自身偏移量和可靠子块的加权偏移量的组合比重:</p>
                </div>
                <div class="p1">
                    <p id="140" class="code-formula">
                        <mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mo stretchy="false">(</mo><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">(</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>w</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo stretchy="false">)</mo><msup><mrow></mrow><mrow><mo>-</mo><mn>2</mn></mrow></msup><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="141">根据上述公式,子块偏移量的确定大致可分为如下3种情况.</p>
                </div>
                <div class="p1">
                    <p id="142">1)目标状态正常,子块评分高于阈值,采用子块跟踪器确定的偏移量.</p>
                </div>
                <div class="p1">
                    <p id="143">2)全局目标发生部分遮挡,相比全局跟踪器,未被遮挡子块跟踪器的偏移量更准确,因此被遮挡子块偏移量主要由正常子块跟踪器的偏移量确定,同时偏移量公式中<i>β</i><sub><i>i</i></sub><i>Δ</i><mathml id="275"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>项可根据遮挡幅度,适当考虑遮挡子块自身偏移量,保留一定程度的自主性.</p>
                </div>
                <div class="p1">
                    <p id="144">3)目标发生较大形变,相比全局跟踪器,子块跟踪器因为尺度大小原因更敏感,全部子块判断为较差跟踪状态,此时采用全局跟踪器确定的偏移量.</p>
                </div>
                <div class="p1">
                    <p id="145">上述子块位置更新策略可以针对子块不同的跟踪状况有针对性的进行更新,保证子块处于良好的状态.</p>
                </div>
                <h4 class="anchor-tag" id="146" name="146"><b>2.3</b> 目标位置估计</h4>
                <div class="p1">
                    <p id="147">第<i>t</i>帧的子块位置确定后,目标整体的位置可由子块联合估计.文献<citation id="441" type="reference">[<a class="sup">20</a>]</citation>通过子块位置的加权求和确定第<i>t</i>帧目标整体的中心坐标<i>pos</i><mathml id="276"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>g</mi><mi>t</mi></msubsup></mrow></math></mathml>:</p>
                </div>
                <div class="p1">
                    <p id="148"><mathml id="277"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mi>o</mi><mi>s</mi><msubsup><mrow></mrow><mi>g</mi><mi>t</mi></msubsup><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>w</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mi>p</mi><mi>o</mi><mi>s</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>w</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></mfrac></mrow></math></mathml>.      (6)</p>
                </div>
                <div class="p1">
                    <p id="149">采用这种方法是由于文献<citation id="442" type="reference">[<a class="sup">20</a>]</citation>的子块运动过程中可能出现漂移现象,通过加权的方式抑制漂移子块对目标位置估计的贡献,但此方法会导致估计的目标位置偏向权重较高的子块,造成整体目标位置发生偏移.如图5所示,红色子块因为遮挡问题导致评分<i>w</i><mathml id="278"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>低于阈值,而其它3个绿色子块的评分<i>w</i><mathml id="279"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>高于阈值,采用式(6)加权估计的目标中心坐标(红色点)与真实的中心坐标(蓝色点)有一定偏移.若有更多的子块被遮挡,这种偏移会更严重.</p>
                </div>
                <div class="p1">
                    <p id="150">由于本文对子块运动处理方式的不同,通过协同的子块运动策略限制子块的自由运动,抑制子块的漂移现象,使被遮挡子块趋向于正确位置,所以目标坐标估计直接取子块位置的平均值,即认为每个子块权重相等,计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="151"><mathml id="280"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mi>o</mi><mi>s</mi><msubsup><mrow></mrow><mi>g</mi><mi>t</mi></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>p</mi></mstyle><mi>o</mi><mi>s</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>,      (7)</p>
                </div>
                <div class="p1">
                    <p id="152">其中<i>N</i>为子块数量.</p>
                </div>
                <div class="area_img" id="360">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_36000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 有遮挡时加权方式产生的目标中心估计偏移" src="Detail/GetImg?filename=images/MSSB201910001_36000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 有遮挡时加权方式产生的目标中心估计偏移  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_36000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Target center estimation offset generated by weighting under occlusion</p>

                </div>
                <div class="p1">
                    <p id="155">为了直观展示本文方法与文献<citation id="443" type="reference">[<a class="sup">20</a>]</citation>估计目标位置方法的优劣,选择women序列,绘制2种方法的目标位置误差曲线图,如图6(a)所示.women序列中目标下半部分出现较长时间的遮挡,(b)、(c)分别为序列目标未被遮挡和被遮挡的帧截图.由(a)可看出,本文方法的误差整体小于文献<citation id="444" type="reference">[<a class="sup">20</a>]</citation>方法的误差.</p>
                </div>
                <div class="area_img" id="361">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_36100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 Women序列上2种方法的中心位置误差曲线" src="Detail/GetImg?filename=images/MSSB201910001_36100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 Women序列上2种方法的中心位置误差曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_36100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Center position error curves of 2 methods on Women sequence</p>

                </div>
                <h4 class="anchor-tag" id="161" name="161"><b>2.4</b> 目标尺度感知策略</h4>
                <div class="p1">
                    <p id="162">全局目标的中心坐标确定后,可根据局部子块的空间位置关系估计全局目标的尺度.当子块互相靠近时,说明目标的尺度呈变小的趋势,如图7(a)、(b)左图.当子块之间互相远离时,目标的尺度呈变大的趋势,如图7(a)、(b)右图.</p>
                </div>
                <div class="area_img" id="362">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_36200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 目标尺度变化与局部采样窗口的关系" src="Detail/GetImg?filename=images/MSSB201910001_36200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 目标尺度变化与局部采样窗口的关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_36200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Relationship between target scale changes and local sampling windows</p>

                </div>
                <div class="p1">
                    <p id="172">结合局部分块空间关系与目标尺度变化的关联,本文给出基于子块相对位置的尺度感知策略.首先在跟踪的第1帧初始化子块的坐标后,以初始的子块分布和尺度作为基准,在随后图像帧跟踪中,对比与初始子块分布的差异,确定当前相应的尺度.为了使本文算法能同时适应尺度的大小变化和尺度的比例变化,对目标的宽高分别进行估计.第<i>t</i>帧尺度变化满足如下比例关系:</p>
                </div>
                <div class="p1">
                    <p id="173" class="code-formula">
                        <mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>s</mi><mi>z</mi><msup><mrow></mrow><mn>1</mn></msup></mrow><mrow><mi>s</mi><mi>r</mi><msup><mrow></mrow><mn>1</mn></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>s</mi><mi>z</mi><msup><mrow></mrow><mi>t</mi></msup></mrow><mrow><mi>s</mi><mi>r</mi><msup><mrow></mrow><mi>t</mi></msup></mrow></mfrac><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="174">其中:<i>sz</i><sup><i>t</i></sup>=(<i>w</i><sup><i>t</i></sup>,<i>h</i><sup><i>t</i></sup>)为目标尺度,<i>w</i><sup><i>t</i></sup>、<i>h</i><sup><i>t</i></sup>为第<i>t</i>帧时目标的宽、高;<i>sr</i><sup><i>t</i></sup>=(<i>sr</i><mathml id="281"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>w</mi><mi>t</mi></msubsup></mrow></math></mathml>,<i>sr</i><mathml id="282"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>h</mi><mi>t</mi></msubsup></mrow></math></mathml>)为目标在第<i>t</i>帧的宽高尺度率,用于刻画子块分散程度.<i>sr</i><mathml id="283"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>w</mi><mi>t</mi></msubsup></mrow></math></mathml>计算方式如下:</p>
                </div>
                <div class="p1">
                    <p id="175" class="code-formula">
                        <mathml id="175"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mi>r</mi><msubsup><mrow></mrow><mi>w</mi><mi>t</mi></msubsup><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mo stretchy="false">(</mo></mstyle><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>-</mo><mi>x</mi><msup><mrow></mrow><mi>t</mi></msup><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="176">其中,<i>x</i><mathml id="284"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>为子块中心坐标<i>pos</i><mathml id="285"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>=(<i>x</i><mathml id="286"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>,<i>y</i><mathml id="287"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>)的横坐标,<i>x</i><sup><i>t</i></sup>为全局目标的中心坐标<i>pos</i><mathml id="288"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>g</mi><mi>t</mi></msubsup></mrow></math></mathml>的横坐标,<i>N</i>为子块数量.同理可以计算高度的尺度率<i>sr</i><mathml id="289"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>h</mi><mi>t</mi></msubsup></mrow></math></mathml>.初始帧的信息可计算初始尺度率<i>sr</i><sup>1</sup>,因此,第<i>t</i>帧的尺度可估计为</p>
                </div>
                <div class="p1">
                    <p id="177"><mathml id="290"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mi>z</mi><msup><mrow></mrow><mi>t</mi></msup><mo>=</mo><mfrac><mrow><mi>s</mi><mi>r</mi><msup><mrow></mrow><mi>t</mi></msup></mrow><mrow><mi>s</mi><mi>r</mi><msup><mrow></mrow><mn>1</mn></msup></mrow></mfrac><mi>s</mi><mi>z</mi><msup><mrow></mrow><mn>1</mn></msup></mrow></math></mathml>.      (8)</p>
                </div>
                <div class="p1">
                    <p id="178">本文的目标尺度感知策略以子块的空间关系为基础,尺度估计的平滑性服从子块运动的平滑性.相比基于尺度搜索的估计方法<citation id="445" type="reference"><link href="400" rel="bibliography" /><link href="402" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>,可适应目标因平面外旋转等因素造成的尺度比例变化,无需额外的计算量.</p>
                </div>
                <h4 class="anchor-tag" id="179" name="179"><b>2.5</b> 子块尺寸的自适应更新</h4>
                <div class="p1">
                    <p id="180">基于相关滤波的目标跟踪存在的一个缺点是滤波器参数模板尺度不可变,因此结合分块思想的相关滤波跟踪器在目标尺度变小的过程中,子块运动容易产生重叠,如图7(a)、(b)左图所示.随着子块滤波器学习的相似信息增多,子块滤波器的多样性下降,在目标再次变大时,子块可能团聚在目标的同一个局部区域,导致尺度感知策略失效.另一种情况是,当目标尺度呈现由小向大变化时,子块互相远离而分散,不变的局部采样窗口对目标覆盖不足,如图7(a)、(b)右图所示,此时子块的采样窗口提取的目标局部信息不足,极易出现漂移现象.</p>
                </div>
                <div class="p1">
                    <p id="181">文献<citation id="446" type="reference">[<a class="sup">20</a>]</citation>通过丢弃已有的局部滤波器,重新训练合适尺寸的子块滤波器解决这一问题.但重置训练的方式丢失目标的历史信息,同时若重置训练的时刻目标存在干扰,则容易学到错误的背景信息.</p>
                </div>
                <div class="p1">
                    <p id="182">为了克服这个问题,本文同样依托子块的空间分布,调整子块滤波器的采样窗口尺寸.固定初始子块滤波器模板参数的尺寸,子块的采样窗口尺寸随目标的尺度变化进行相应调整,将局部样本金字塔采样成和滤波器相同的尺寸,再进行相关滤波跟踪.子块的采样窗口大小调整方法如下:</p>
                </div>
                <div class="area_img" id="183">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201910001_18300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="185">其中,<i>pr</i><sup><i>t</i></sup>为第<i>t</i>帧子块距离目标中心点的平均距离,刻画子块的分散程度,在调整子块采样尺寸时,不考虑比例变化.给定初始的子块尺寸<i>ps</i><sup>1</sup>=(<i>w</i><sup>1</sup>,<i>h</i><sup>1</sup>) ,通过式(9)可计算当前子块尺寸<i>ps</i><sup><i>t</i></sup>=(<i>w</i><sup><i>t</i></sup>,<i>h</i><sup><i>t</i></sup>) .</p>
                </div>
                <div class="p1">
                    <p id="186">图7(c)为调整子块采样框尺寸后的结果,对于局部信息的捕捉更合理.</p>
                </div>
                <h4 class="anchor-tag" id="187" name="187"><b>2.6</b> 滤波器更新</h4>
                <div class="p1">
                    <p id="188">跟踪过程中滤波器需进行在线更新,以适应目标发生的变化,本文对全局滤波器和局部滤波器采用如下2种更新机制.</p>
                </div>
                <div class="p1">
                    <p id="189">1)对于全局滤波器模型,只有当跟踪可靠的子块数量不少于总数的一半时,才会进行更新,否则会导致跟踪滤波器学习过多的背景知识.每一帧新的图像训练得到1个当前帧的全局滤波器参数(<mathml id="291"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>α</mi><mo>^</mo></mover></math></mathml>,<mathml id="292"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>x</mi><mo>^</mo></mover></math></mathml>),通过线性内插的方式更新模型,更新公式如下:</p>
                </div>
                <div class="area_img" id="190">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201910001_19000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="192">其中,<mathml id="293"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>A</mi><mo stretchy="false">|</mo></mrow></math></mathml>为评分大于阈值的可靠块的数量,<i>η</i>为学习率.</p>
                </div>
                <div class="p1">
                    <p id="193">2)对于子块滤波器模型,只有在其评分大于阈值时才会进行更新,跟踪当前帧局部采样训练得到子块的滤波器参数(<mathml id="294"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>α</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>,<mathml id="295"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>x</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>),模型更新如下:</p>
                </div>
                <div class="p1">
                    <p id="194" class="code-formula">
                        <mathml id="194"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi>α</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>η</mi><mo stretchy="false">)</mo><mover accent="true"><mi>α</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>+</mo><mi>η</mi><mover accent="true"><mi>α</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo></mtd><mtd columnalign="left"><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>&gt;</mo><mi>t</mi><mi>h</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>h</mi><mi>o</mi><mi>l</mi><mi>d</mi></mtd></mtr><mtr><mtd columnalign="left"><mover accent="true"><mi>α</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>,</mo></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>它</mtext></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr><mtd><mover accent="true"><mi>x</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>η</mi><mo stretchy="false">)</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>+</mo><mi>η</mi><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo></mtd><mtd columnalign="left"><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>&gt;</mo><mi>t</mi><mi>h</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>h</mi><mi>o</mi><mi>l</mi><mi>d</mi></mtd></mtr><mtr><mtd columnalign="left"><mover accent="true"><mi>x</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>,</mo></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>它</mtext></mtd></mtr></mtable></mrow></mrow></mtd></mtr></mtable><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="195">局部子块的评分<i>w</i><mathml id="296"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>低于阈值时,抑制模型的更新可避免局部滤波器学习到错误的背景信息.</p>
                </div>
                <h4 class="anchor-tag" id="196" name="196"><b>2.7</b> 算法步骤</h4>
                <div class="p1">
                    <p id="197"><b>算法</b> 尺度感知的分块协同式相关滤波跟踪算法</p>
                </div>
                <div class="p1">
                    <p id="198"><b>输入</b> 首帧图像目标的位置<i>pos</i><sup>1</sup>,尺寸<i>sz</i><sup>1</sup>,后续视频帧图像</p>
                </div>
                <div class="p1">
                    <p id="200"><b>输出</b> 第<i>t</i>帧图像目标的中心位置<i>pos</i><sup><i>t</i></sup>,尺度<i>sz</i><sup><i>t</i></sup>.</p>
                </div>
                <div class="p1">
                    <p id="201">step 1 局部分块初始化.按图2对目标进行四等分.</p>
                </div>
                <div class="p1">
                    <p id="202">step 2 滤波器训练.按式(1)分别训练全局相关滤波器参数<mathml id="297"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>α</mi><mo>^</mo></mover></math></mathml>和局部相关滤波器参数<mathml id="298"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>α</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo>.</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="203">step 3 目标位置与尺度估计.对<i>t</i>-1帧预测的<i>pos</i><sup><i>t</i></sup><sup>-1</sup>和<i>sz</i><sup><i>t</i></sup><sup>-1</sup>范围进行采样,按式(2)分别计算全局与局部子块滤波器的置信图.按式(4)计算子块的置信图得分<i>w</i><mathml id="299"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>.按式(5)更新子块的中心坐标<i>pos</i><mathml id="300"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>.按式(7)估计目标的中心坐标<i>pos</i><sup><i>t</i></sup>.按式(8)估计当前目标尺度<i>sz</i><sup><i>t</i></sup>.按式(9)调整子块采样窗口大小<i>ps</i><sup><i>t</i></sup>.</p>
                </div>
                <div class="p1">
                    <p id="204">step 4 滤波器模型更新.按式(10)更新全局相关滤波模型<mathml id="301"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>α</mi><mo>^</mo></mover></math></mathml><sup><i>t</i></sup>和<mathml id="302"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>x</mi><mo>^</mo></mover></math></mathml><sup><i>t</i></sup>.按式(11)更新各局部相关滤波器模型<mathml id="303"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>α</mi><mo>^</mo></mover></math></mathml><mathml id="304"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>和<mathml id="305"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>x</mi><mo>^</mo></mover></math></mathml><mathml id="306"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>.</p>
                </div>
                <h3 id="205" name="205" class="anchor-tag">3 实验及结果分析</h3>
                <h4 class="anchor-tag" id="206" name="206"><b>3.1</b> 实验环境及参数设置</h4>
                <div class="p1">
                    <p id="207">为了验证本文算法的有效性,选择OTB100<citation id="447" type="reference"><link href="410" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>和TC128<citation id="448" type="reference"><link href="412" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>标准测试数据集进行分析.测试数据集根据场景的不同,可分为含尺度变化(Scale Varia-tions, SV)、光照变化(Illumination Variation, IV)、形变(Deformation, DEF)、遮挡变化(Occlusion, OCC)、平面内旋转(In-Plane Rotation, IPR)、平面外旋转(Out-of-Plane Rotation, OPR)、运动模糊(Motion Blur, MB)、快速运动(Fast Motion, FM)、超出边界(Out of View, OV)、背景杂乱(Back-ground Clutter, BC)、低分别率(Low Resolution, LR),共11类不同情况的跟踪场景.</p>
                </div>
                <div class="p1">
                    <p id="208">本文选择7个具有代表性的相关滤波跟踪算法进行对比实验,包括:KCF<sup></sup><citation id="449" type="reference"><link href="384" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、多特征尺度自适应跟踪法(Scale Adaptive with Multiple Features Tracker, SAMF)<citation id="450" type="reference"><link href="400" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,快速判别式尺度空间跟踪法(Fast Discriminative Scale Space Tracking, fDSST)<citation id="451" type="reference"><link href="414" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、基于模板与像素学习器的和的跟踪法(Sum of Tem-plate and Pixel-wise Learners, Staple)<citation id="452" type="reference"><link href="416" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、基于可变形分块的相关滤波跟踪法(Deformable Part-Based Correlation Filter Tracking, DPCF)<citation id="453" type="reference"><link href="404" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、基于可靠分块的跟踪法(Reliable Patch Trackers, RPT)<citation id="454" type="reference"><link href="408" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、基于子空间和直方图的多记忆自适应相关滤波目标跟踪法(Multiple Memory Learning and Adaptive Corre-lation Filter Based on Subspace and Histogram, MCFSH)<citation id="455" type="reference"><link href="398" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="209">除了MCFSH因未公开源代码,采用原作者论文结果进行对比以外,其余算法代码均为原作者开源代码,参数采用源代码默认设置,并且采用文献<citation id="456" type="reference">[<a class="sup">2</a>]</citation>开源的评测代码对所有算法进行统一的对比实验.KCF为基于方向梯度直方图(Histogram of Oriented Gradient, HOG)特征的相关滤波跟踪算法.Staple在KCF基础上改进特征选取方法,同时Staple采用判别式尺度空间跟踪(Discriminative Scale Space Tracking, DSST)<citation id="457" type="reference"><link href="402" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>的尺度框架进行目标的尺度估计.SAMF和fDSST通过尺度池的方法,着重解决目标的尺度变化问题.DPCF和RPT为2个基于分块相关滤波算法.MCFSH针对多个相关滤波跟踪器集成进行改进.</p>
                </div>
                <div class="p1">
                    <p id="210">本文局部分块的基础滤波器采用KCF相关滤波器,参数与原文保持一致.置信图学习率<i>ρ</i>同KCF相关滤波器学习率一样,设置为0.02.另外本文两个重要参数为遮挡判断阈值<i>threshold</i>和跟踪评分平衡系数ϑ.通过实验逐帧对子块评分观察,发现当平衡系数ϑ=1.5时,评分<i>w</i>对于TSCC值的敏感程度适中,跟踪正常的取值和跟踪不正常的取值的差异较大,同时<i>threshold</i>=15时可以较好区分跟踪正常和不正常的状态.</p>
                </div>
                <div class="p1">
                    <p id="211">实验环境如下:英特尔Core i7-7700K的CPU,主频4.20 GHz,内存8 GB,编程平台Matlab2018.</p>
                </div>
                <h4 class="anchor-tag" id="212" name="212"><b>3.2</b> 定量分析</h4>
                <div class="p1">
                    <p id="213">本节设计两个针对性实验进行有效性评估: 1)各算法综合性能对比.2)本文跟踪状态评分策略有效性验证.</p>
                </div>
                <div class="p1">
                    <p id="214">算法评估采用文献<citation id="458" type="reference">[<a class="sup">2</a>]</citation>给出的评价指标,包括距离精度(Distance Precision, DP)、重叠精度(Overlap Precision, OP)、成功率曲线面积 (Area under Curve, AUC).</p>
                </div>
                <div class="p1">
                    <p id="215">DP为真实标定框与预测标定框中心误差(Center Location Error, CLE)小于某一阈值(通常取20像素)的帧数占视频序列总长度的比例.OP为真实标定框与预测标定框交并比(Intersection over Union, IOU)大于阈值(通常取0.5)的帧数占视频序列总长度的比例.AUC为成功率曲线(OP值随阈值变化曲线)图中曲线围成的面积.CLE和IOU计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="216" class="code-formula">
                        <mathml id="216"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>C</mi><mi>L</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>Τ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi>p</mi><mi>o</mi><mi>s</mi><msup><mrow></mrow><mi>t</mi></msup><mo>-</mo><mi>p</mi><mi>o</mi><mi>s</mi><msubsup><mrow></mrow><mi>g</mi><mi>t</mi></msubsup><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>,</mo></mtd></mtr><mtr><mtd><mi>Ι</mi><mi>Ο</mi><mi>U</mi><mo>=</mo><mfrac><mrow><mi>a</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false">(</mo><mi>R</mi><mstyle displaystyle="true"><mo>∩</mo><mi>R</mi></mstyle><msub><mrow></mrow><mi>g</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>a</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false">(</mo><mi>R</mi><mstyle displaystyle="true"><mo>∪</mo><mi>R</mi></mstyle><msub><mrow></mrow><mi>g</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="217" name="217"><b>3.2.1</b> 算法对比实验</h4>
                <div class="p1">
                    <p id="218">表2至表5给出本文算法与7种对比算法在OTB100、TC128测试集上不同环境场景序列集的DP值和AUC值对比.表中第1行表示11个不同类场景及分别包含的视频数目,如OCC-49表示49个包含遮挡的测试集.每种场景的最优结果使用黑体数字表示,次优结果使用斜体数字表示.MCFSH中OTB结果仅包含彩色序列,非完整OTB100测试集,因此本文算法与MCFSH仅在TC128测试集上进行对比.MCFSH结果为原论文公布结果.</p>
                </div>
                <div class="area_img" id="219">
                    <p class="img_tit"><b>表2 7种算法在OTB100数据集上的AUC结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 AUC results of 7 algorithms on OTB100 datasets </p>
                    <p class="img_note"></p>
                    <table id="219" border="1"><tr><td>算法</td><td>全集</td><td>IV-38</td><td>SV-64</td><td>OCC-49</td><td>DEF-44</td><td>MB-29</td><td>FM-39</td><td>IPR-51</td><td>OPR-63</td><td>OV-14</td><td>BC-31</td><td>LR-9</td></tr><tr><td><br />本文算法</td><td><b>60.5</b></td><td><b>60.3</b></td><td><b>56.7</b></td><td><b>57.9</b></td><td><b>55.8</b></td><td>55.2</td><td>54.1</td><td><b>55.3</b></td><td><b>58.1</b></td><td><b>51.5</b></td><td><b>61.8</b></td><td><b>47.7</b></td></tr><tr><td><br />Staple</td><td><i>57</i>.<i>8</i></td><td><i>59</i>.<i>3</i></td><td><i>52</i>.<i>0</i></td><td>54.2</td><td><i>55</i>.<i>0</i></td><td>54.0</td><td>54.1</td><td>54.8</td><td>53.3</td><td>47.6</td><td>56.1</td><td>39.9</td></tr><tr><td><br />fDSST</td><td>55.4</td><td>56.8</td><td>50.5</td><td>48.4</td><td>46.8</td><td>54.7</td><td><b>55.4</b></td><td><i>55</i>.<i>0</i></td><td>50.1</td><td>45.8</td><td><i>58</i>.<i>6</i></td><td><i>42</i>.<i>9</i></td></tr><tr><td><br />SAMF</td><td>56.1</td><td>54.4</td><td>50.8</td><td><i>55</i>.<i>0</i></td><td>50.9</td><td><i>55</i>.<i>7</i></td><td>53.0</td><td>53.1</td><td>54.1</td><td><i>49</i>.<i>2</i></td><td>53.6</td><td>42.5</td></tr><tr><td><br />KCF</td><td>47.7</td><td>47.9</td><td>39.4</td><td>44.3</td><td>43.6</td><td>45.8</td><td>45.8</td><td>46.9</td><td>45.3</td><td>39.4</td><td>49.8</td><td>29.0</td></tr><tr><td><br />RPT</td><td>53.1</td><td>53.4</td><td>47.4</td><td>47.0</td><td>48.4</td><td>50.3</td><td><i>54</i>.<i>2</i></td><td>52.2</td><td>49.7</td><td>47.0</td><td>57.5</td><td>37.5</td></tr><tr><td><br />DPCF</td><td>56.5</td><td>58.5</td><td>51.7</td><td>54.6</td><td>52.3</td><td><b>57.3</b></td><td>53.3</td><td>52.5</td><td><i>54</i>.<i>6</i></td><td>48.2</td><td>57.5</td><td>40.9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="220">
                    <p class="img_tit"><b>表3 7种算法在OTB100数据集上的DP结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 DP results of 7 algorithms on OTB100 datasets</p>
                    <p class="img_note"></p>
                    <table id="220" border="1"><tr><td>算法</td><td>全集</td><td>IV-38</td><td>SV-64</td><td>OCC-49</td><td>DEF-44</td><td>MB-29</td><td>FM-39</td><td>IPR-51</td><td>OPR-63</td><td>OV-14</td><td>BC-31</td><td>LR-9</td></tr><tr><td><br />本文算法</td><td><b>81.9</b></td><td>78.8</td><td><b>78.1</b></td><td><b>76.9</b></td><td><b>78.2</b></td><td><i>72</i>.<i>1</i></td><td>70.1</td><td><i>76</i>.<i>6</i></td><td><b>80.8</b></td><td><b>67.1</b></td><td><b>81.9</b></td><td><b>79.1</b></td></tr><tr><td><br />Staple</td><td><i>78</i>.<i>4</i></td><td>78.2</td><td>72.7</td><td>72.8</td><td><i>75</i>.<i>1</i></td><td>69.9</td><td><b>71.0</b></td><td><b>76.8</b></td><td>73.8</td><td><i>66</i>.<i>8</i></td><td>74.9</td><td>69.5</td></tr><tr><td><br />fDSST</td><td>72.5</td><td>75.1</td><td>66.4</td><td>63.6</td><td>61.0</td><td>69.1</td><td>69.8</td><td>73.4</td><td>66.6</td><td>57.7</td><td>78.0</td><td>67.5</td></tr><tr><td><br />SAMF</td><td>76.5</td><td>73.8</td><td><i>72</i>.<i>8</i></td><td><i>74</i>.<i>6</i></td><td>68.8</td><td>70.6</td><td>69.3</td><td>74.4</td><td>75.3</td><td>64.4</td><td>71.4</td><td><i>76</i>.<i>6</i></td></tr><tr><td><br />KCF</td><td>69.6</td><td>71.9</td><td>63.4</td><td>63.0</td><td>61.7</td><td>60.1</td><td>62.2</td><td>70.1</td><td>67.7</td><td>50.1</td><td>71.3</td><td>67.1</td></tr><tr><td><br />RPT</td><td>74.9</td><td><i>79</i>.<i>8</i></td><td>69.8</td><td>65.7</td><td>69.5</td><td>67.5</td><td><b>71.0</b></td><td>74.4</td><td>70.7</td><td>59.8</td><td><i>79</i>.<i>5</i></td><td>71.7</td></tr><tr><td><br />DPCF</td><td>77.6</td><td><b>80.8</b></td><td>72.4</td><td>74.0</td><td>72.9</td><td><b>75.3</b></td><td><i>70</i>.<i>7</i></td><td>73.7</td><td><i>75</i>.<i>4</i></td><td>61.0</td><td>78.2</td><td>71.1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="221">
                    <p class="img_tit"><b>表4 8种算法在TC128数据集上的AUC结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 AUC results of 8 algorithms on TC128 datasets</p>
                    <p class="img_note"></p>
                    <table id="221" border="1"><tr><td>算法</td><td>全集</td><td>IV-37</td><td>SV-65</td><td>OCC-62</td><td>DEF-36</td><td>MB-37</td><td>FM-57</td><td>IPR-57</td><td>OPR-73</td><td>OV-14</td><td>BC-31</td><td>LR-18</td></tr><tr><td><br />本文算法</td><td><b>51.7</b></td><td><b>55.0</b></td><td><b>52.2</b></td><td><b>50.4</b></td><td><i>57</i>.<i>3</i></td><td><i>42</i>.<i>9</i></td><td><b>49.5</b></td><td><i>47</i>.<i>9</i></td><td><b>52.2</b></td><td><i>37</i>.<i>1</i></td><td><i>50</i>.<i>6</i></td><td><i>31</i>.<i>6</i></td></tr><tr><td><br />Staple</td><td>50.2</td><td>52.9</td><td>49.2</td><td>46.9</td><td>56.0</td><td><b>43.8</b></td><td><i>48</i>.<i>6</i></td><td>47.3</td><td>49.9</td><td><b>39.9</b></td><td>50.0</td><td><b>35.6</b></td></tr><tr><td><br />fDSST</td><td>43.5</td><td>49.5</td><td>41.8</td><td>38.6</td><td>45.0</td><td>37.2</td><td>41.3</td><td>41.5</td><td>41.1</td><td>32.7</td><td>49.0</td><td>27.7</td></tr><tr><td><br />SAMF</td><td>46.6</td><td>48.4</td><td>45.8</td><td>45.8</td><td>54.0</td><td>41.5</td><td>45.8</td><td>42.7</td><td>47.3</td><td>35.1</td><td>44.2</td><td>25.9</td></tr><tr><td><br />KCF</td><td>38.7</td><td>41.7</td><td>34.0</td><td>34.7</td><td>46.1</td><td>34.6</td><td>37.8</td><td>35.2</td><td>37.6</td><td>29.8</td><td>42.3</td><td>23.9</td></tr><tr><td><br />RPT</td><td>44.9</td><td>48.6</td><td>43.4</td><td>41.8</td><td>44.8</td><td>37.9</td><td>43.4</td><td>44.2</td><td>45.0</td><td>33.2</td><td>48.1</td><td>29.8</td></tr><tr><td><br />DPCF</td><td>49.6</td><td>53.5</td><td>46.7</td><td><i>49</i>.<i>2</i></td><td>54.8</td><td>42.4</td><td>47.9</td><td>46.9</td><td>50.7</td><td>35.4</td><td>50.3</td><td>30.6</td></tr><tr><td><br />MCFSH</td><td><i>50</i>.<i>5</i></td><td><i>54</i>.<i>3</i></td><td><i>49</i>.<i>8</i></td><td>47.7</td><td><b>57.7</b></td><td>41.7</td><td>47.7</td><td><b>48.6</b></td><td><i>52</i>.<i>1</i></td><td>35.8</td><td><b>52.4</b></td><td>31.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="222">
                    <p class="img_tit"><b>表5 8种算法在TC128数据集上的DP结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 DP results of 8 algorithms on TC128 datasets</p>
                    <p class="img_note"></p>
                    <table id="222" border="1"><tr><td>算法</td><td>全集</td><td>IV-37</td><td>SV-65</td><td>OCC-62</td><td>DEF-36</td><td>MB-37</td><td>FM-57</td><td>IPR-57</td><td>OPR-73</td><td>OV-14</td><td>BC-45</td><td>LR-18</td></tr><tr><td><br />本文算法</td><td><b>70.7</b></td><td><b>71.9</b></td><td><b>69.1</b></td><td><b>65.7</b></td><td><i>80</i>.<i>0</i></td><td><i>59</i>.<i>5</i></td><td><b>61.6</b></td><td><i>62</i>.<i>4</i></td><td><b>68.6</b></td><td>47.3</td><td><b>73.9</b></td><td><i>54</i>.<i>9</i></td></tr><tr><td><br />Staple</td><td>67.3</td><td>68.6</td><td><i>66</i>.<i>5</i></td><td>60.3</td><td>77.4</td><td><b>60.4</b></td><td><i>60</i>.<i>1</i></td><td>60.4</td><td>65.0</td><td><b>56.1</b></td><td>70.0</td><td><b>56.0</b></td></tr><tr><td><br />fDSST</td><td>57.5</td><td>63.3</td><td>54.4</td><td>49.8</td><td>61.4</td><td>51.0</td><td>48.5</td><td>52.4</td><td>51.7</td><td>38.2</td><td>70.6</td><td>45.2</td></tr><tr><td><br />SAMF</td><td>62.9</td><td>61.9</td><td>61.9</td><td>59.4</td><td>73.4</td><td>55.5</td><td>55.2</td><td>53.2</td><td>60.3</td><td>41.4</td><td>62.6</td><td>44.4</td></tr><tr><td><br />KCF</td><td>55.8</td><td>58.4</td><td>52.9</td><td>48.0</td><td>65.5</td><td>48.8</td><td>49.2</td><td>51.2</td><td>52.5</td><td>37.4</td><td>62.4</td><td>44.7</td></tr><tr><td><br />RPT</td><td>61.4</td><td>66.4</td><td>59.6</td><td>54.3</td><td>63.6</td><td>53.2</td><td>51.4</td><td>57.6</td><td>58.1</td><td>45.5</td><td>69.8</td><td>53.2</td></tr><tr><td><br />DPCF</td><td><i>67</i>.<i>8</i></td><td><i>70</i>.<i>8</i></td><td>63.1</td><td><i>64</i>.<i>9</i></td><td>76.6</td><td>57.6</td><td>59.5</td><td>59.9</td><td>65.5</td><td>44.6</td><td>71.2</td><td>53.2</td></tr><tr><td><br />MCFSH</td><td>67.7</td><td>69.2</td><td>66.4</td><td>61.0</td><td><b>80.5</b></td><td>56.8</td><td>59.6</td><td><b>62.5</b></td><td><i>67</i>.<i>9</i></td><td><i>49</i>.<i>3</i></td><td><i>73</i>.<i>2</i></td><td>49.8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="223">由表2至表5实验结果可看出,本文算法总体结果优于其它算法,尤其在SV、OCC等多个场景上优于其它对比算法.在FM、MB上的表现略逊,但也基本处于次优.</p>
                </div>
                <div class="p1">
                    <p id="224">图8和图9给出各算法在OTB100、TC128数据集上的一次性评估(One-Pass Evaluation, OPE)<sup></sup><citation id="459" type="reference"><link href="368" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>结果曲线.图8(a)和图9(a)为精确度曲线图,展示CLE小于阈值的帧数占比随距离阈值的变化情况,算法以距离精度DP值排序.图8(b)和图9(b)为成功率曲线图,展示IOU大于阈值的帧数占比随重叠阈值的变化,算法以AUC值排序.</p>
                </div>
                <div class="area_img" id="363">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_36300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 7种算法在OTB100数据集上的OPE结果曲线" src="Detail/GetImg?filename=images/MSSB201910001_36300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 7种算法在OTB100数据集上的OPE结果曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_36300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 OPE curves of 7 algorithms on OTB100 datasets</p>

                </div>
                <div class="area_img" id="363">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_36301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 7种算法在OTB100数据集上的OPE结果曲线" src="Detail/GetImg?filename=images/MSSB201910001_36301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 7种算法在OTB100数据集上的OPE结果曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_36301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 OPE curves of 7 algorithms on OTB100 datasets</p>

                </div>
                <div class="p1">
                    <p id="228">由于MCFSH未公开源码和结果集,无法加入OPE曲线对比.由图8和图9可以看出,本文算法结果均优于其它对比算法.在OTB100、TC128数据集上多次测试表明,本文算法可以满足实时运行要求.</p>
                </div>
                <div class="area_img" id="364">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_36400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 7种算法在TC128数据集上的OPE结果曲线" src="Detail/GetImg?filename=images/MSSB201910001_36400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 7种算法在TC128数据集上的OPE结果曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_36400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 OPE curves of 7 algorithms on TC128 datasets</p>

                </div>
                <h4 class="anchor-tag" id="232" name="232"><b>3.2.2</b> 跟踪状态评分策略有效性验证</h4>
                <div class="p1">
                    <p id="233">为了验证结合PSR和TSCC的子块跟踪状态评分<i>w</i>的有效性,实验设计对比只采用PSR的评分策略(如式(3))和引入TSCC约束的评分策略(如式(4))的效果.算法实现除跟踪评分策略不同,其它模块均保持一致.OTB100测试集所得结果见图10.</p>
                </div>
                <div class="area_img" id="365">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_36500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 两种评分策略在OTB100数据集上OPE结果曲线" src="Detail/GetImg?filename=images/MSSB201910001_36500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 两种评分策略在OTB100数据集上OPE结果曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_36500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 OPE curves of 2 scoring strategies on OTB100 datasets</p>

                </div>
                <div class="area_img" id="365">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_36501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 两种评分策略在OTB100数据集上OPE结果曲线" src="Detail/GetImg?filename=images/MSSB201910001_36501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 两种评分策略在OTB100数据集上OPE结果曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_36501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 OPE curves of 2 scoring strategies on OTB100 datasets</p>

                </div>
                <div class="p1">
                    <p id="237">由图10可看出,本文采用的PSR结合TSCC的评分策略对子块跟踪的评估更准确,相比只采用PSR评估,在DP和AUC指标上具有明显提升.</p>
                </div>
                <h4 class="anchor-tag" id="238" name="238"><b>3.3</b> 定性分析</h4>
                <div class="p1">
                    <p id="239">本节选取具有代表性的不同场景下若干视频结果图进行定性分析.为了方便展示,仅选取3.2节测试前三名的算法进行分析,即本文算法、Staple和DPCF.</p>
                </div>
                <div class="p1">
                    <p id="240">图11是手持玩具狗的拍摄视频Dog1,玩具与摄像头距离时远时近,尺度时大时小.</p>
                </div>
                <div class="p1">
                    <p id="241">前两幅截图中3种算法尺度估计相近,但在第3幅的1 193帧处,Staple标定框右边留白较多,尺度估计偏大.DPCF尺度估计错误.本文算法的尺度估计更准确.</p>
                </div>
                <div class="area_img" id="242">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_242.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 3种算法在Dog1序列集上的结果截图" src="Detail/GetImg?filename=images/MSSB201910001_242.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 3种算法在Dog1序列集上的结果截图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_242.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.11 Screenshot of 3 algorithm results on Dog1 sequence</p>

                </div>
                <div class="p1">
                    <p id="243">图12为Jogging序列截图,图中两位女子在跑步,在第59帧处出现电线杆遮挡目标.由图可看出,未发生遮挡时,3种算法效果均较好.在第59帧左右遮挡发生时,Staple和DPCF估计的目标中心与真实目标中心偏移较大.在第249帧处,Staple跟丢目标,漂移到图像边界处.本文算法和DPCF跟住目标,但本文算法在尺度估计上更准确.</p>
                </div>
                <div class="area_img" id="244">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_244.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 3种算法在Jogging序列集上的结果截图" src="Detail/GetImg?filename=images/MSSB201910001_244.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 3种算法在Jogging序列集上的结果截图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_244.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.12 Screenshot of 3 algorithm results on Jogging sequence</p>

                </div>
                <div class="p1">
                    <p id="245">图13为Board序列集截图,图中有1块电路板在复杂背景下运动.在516帧左右目标出现平面外旋转变换.由图可看出,此时DPCF估计的目标中心坐标与真实中心坐标有偏移,本文算法估计的目标框在宽高尺度比例上更贴合电路板.在683帧左右,Staple跟丢目标,本文算法和DPCF均能跟住目标.平面外旋转变换会造成目标本身比例发生变化,影响特征提取,本文算法因为分别对尺度宽高进行估计,实验结果表现更优.</p>
                </div>
                <div class="area_img" id="246">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201910001_246.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 3种算法在Board序列集上的结果截图" src="Detail/GetImg?filename=images/MSSB201910001_246.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 3种算法在Board序列集上的结果截图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201910001_246.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.13 Screenshot of 3 algorithm results on Board sequence</p>

                </div>
                <h3 id="247" name="247" class="anchor-tag">4 结 束 语</h3>
                <div class="p1">
                    <p id="248">在基于分块的相关滤波跟踪器的基础上,本文对子块跟踪状态评分、局部子块协同运动、目标尺度变化估计和子块采样窗口尺寸自适应方面进行研究.提出的子块评分方法能够更准确地刻画子块被遮挡的程度.设计的子块协同运动策略抑制子块的漂移现象.对尺度变化与子块空间分布关系的分析使目标的尺度变化能被算法感知和计算,同时子块尺寸也能自适应地调整.实验结果表明本文算法具有较好的总体性能,特别是在尺度、形变、遮挡变化等场景表现更佳.</p>
                </div>
                <div class="p1">
                    <p id="249">但是本文算法也存在不足,如在一些快速运动、运动模糊场景下表现一般,这是因为运动场景下分块不易捕捉目标特征.同时基于置信图的跟踪状态评估方法目前无法区分较大形变与遮挡变化,导致模型更新不够准确.当前深度特征能够提取目标的高级特性,有助于区分形变和遮挡的情况.后续会考虑结合深度特征和其它思路的跟踪模型,对参数调整的定性方法进行研究,以期进一步增强跟踪效果.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="366">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201506001&amp;v=MTc0MjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1dyL05MejdCZHJHNEg5VE1xWTlGWllRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 黄凯奇,陈晓棠,康运锋,等.智能视频监控技术综述.计算机学报,2015,38(6):1093-1118.(HUANG K Q,CHEN X T,KANG Y F,et al.Intelligent Visual Surveillance:A Review.Chinese Journal of Computers,2015,38(6):1093-1118.)
                            </a>
                        </p>
                        <p id="368">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online object tracking:a benchmark">

                                <b>[2]</b> WU Y,LIM J,YANG M H.Online Object Tracking:A Benchmark // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2013:2411-2418.
                            </a>
                        </p>
                        <p id="370">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349994&amp;v=MTg3MTZaKzhHQlhVOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUoxOGRhaGM9TmlmT2ZiSzdIdERPclk5RQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> NUMMIARO K,KOLLER-MEIER E,VAN GOOL L.An Adaptive Color-Based Particle Filter.Image and Vision Computing,2003,21(1):99-110.
                            </a>
                        </p>
                        <p id="372">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time tracking of non-rigid objects using mean shift">

                                <b>[4]</b> COMANICIU D,RAMESH V,MEER P.Real-Time Tracking of Non-rigid Objects Using Mean Shift // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2000:142-149.
                            </a>
                        </p>
                        <p id="374">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Support vector tracking">

                                <b>[5]</b> AVIDAN S.Support Vector Tracking // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2001.DOI:10.1109/CVPR.2001.990474.
                            </a>
                        </p>
                        <p id="376">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Struck:Structured Output Tracking with Kernels">

                                <b>[6]</b> HARE S,GOLODETZ S,SAFFARI A,et al.Struck:Structured Output Tracking with Kernels.IEEE Transactions on Pattern Analysis and Machine Intelligence.2016,38(10):2096-2109.
                            </a>
                        </p>
                        <p id="378">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">

                                <b>[7]</b> BOLME D S,BEVERIDGE J R,DRAPER B A,et al.Visual Object Tracking Using Adaptive Correlation Filters // Proc of the IEEE Computer Society Conference on Computer Vision and Pattern Re-cognition.Washington,USA:IEEE,2010:2544-2550.
                            </a>
                        </p>
                        <p id="380">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">

                                <b>[8]</b> HENRIQUES J F,CASEIRO R,MARTINS P,et al.Exploiting the Circulant Structure of Tracking-by-Detection with Kernels // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2012:702-715.
                            </a>
                        </p>
                        <p id="382">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive Color Attributes for Real-Time Visual Tracking">

                                <b>[9]</b> DANELLJAN M,KHAN S F,FELSBERG M,et al.Adaptive Color Attributes for Real-Time Visual Tracking // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2014:1090-1097.
                            </a>
                        </p>
                        <p id="384">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 HENRIQUES J F,CASEIRO R,MARTINS P,et al.High-Speed Tracking with Kernelized Correlation Filters.IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(3):583-596.
                            </a>
                        </p>
                        <p id="386">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tracking Using Online Feature Selection and a Local Generative Model[C/OL]">

                                <b>[11]</b> WOODLEY T,STENGER B,CIPOLLA R.Tracking Using Online Feature Selection and a Local Generative Model[C/OL].[2019-03-15].http://bjornstenger.github.io/papers/woodley_bmvc2007.pdf.
                            </a>
                        </p>
                        <p id="388">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning multi-domain convolutional neural networks for visual tracking">

                                <b>[12]</b> NAM H,HAN B.Learning Multi-domain Convolutional Neural Networks for Visual Tracking // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:4293-4302.
                            </a>
                        </p>
                        <p id="390">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully-convolutional siamese networks for object tracking">

                                <b>[13]</b> BERTINETTO L,VALMADRE J,HENRIQUES J F,et al.Fully-Convolutional Siamese Networks for Object Tracking // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2016:850-865.
                            </a>
                        </p>
                        <p id="392">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">

                                <b>[14]</b> DANELLJAN M,HÄGER G,KHAN F S,et al.Learning Spatially Regularized Correlation Filters for Visual Tracking // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2015:4310-4318.
                            </a>
                        </p>
                        <p id="394">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Multi-view Model for Visual Tracking via Correlation Filters">

                                <b>[15]</b> LI X,LIU Q,HE Z Y,et al.A Multi-view Model for Visual Tracking via Correlation Filters.Knowledge-Based Systems,2016,113:88-99.
                            </a>
                        </p>
                        <p id="396">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESDE5D5DED64783B5BF9A99EDEB5646CF4&amp;v=MTk3MTg1MFFRcVcyV0F3ZjdhU05zeWJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3N20yd0swPU5pZk9mY2ZORzZYSjIvb3hZdThJQkg5THltUmw0MA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> LIU Q,LU X H,HE Z Y,et al.Deep Convolutional Neural Networks for Thermal Infrared Object Tracking.Knowledge-Based Systems,2017,134:189-198.
                            </a>
                        </p>
                        <p id="398">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201807005&amp;v=MTk4OTVxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tXci9OS0Q3WWJMRzRIOW5NcUk5RllZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> 冯棐,吴小俊,徐天阳.基于子空间和直方图的多记忆自适应相关滤波目标跟踪算法.模式识别与人工智能,2018,31(7):612-624.(FENG F,WU X J,XU T Y.Object Tracking with Multiple Memory Learning and Adaptive Correlation Filter Based on Subspace and Histogram.Pattern Recognition and Artificial Intelligence,2018,31(7):612-624.)
                            </a>
                        </p>
                        <p id="400">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A scale adaptive kernel correlation filter tracker with feature integration">

                                <b>[18]</b> LI Y,ZHU J K.A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2014:254-265.
                            </a>
                        </p>
                        <p id="402">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate scale estimation for robust visual tracking">

                                <b>[19]</b> DANELLJAN M,HÄGER G,KHAN F S,et al.Accurate Scale Estimation for Robust Visual Tracking // Proc of the British Machine Vision Conference.Nottingham,UK:BMVA Press,2014.DOI:10.5244/C.28.65.
                            </a>
                        </p>
                        <p id="404">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES11BF436F828C498A30E0327808729EA2&amp;v=MDU0NDdIZjNnd3gyY1E2a3A5UzMzbHBCSTlmckNkTU11ZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHc3bTJ3SzA9TmlmT2ZiSzViS2ZJcklremJPaw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> AKIN O,ERDEM E,ERDEM A,et al.Deformable Part-Based Tracking by Coupled Global and Local Correlation Filters.Journal of Visual Communication and Image Representation,2016,38:763-774.
                            </a>
                        </p>
                        <p id="406">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time part-based visual tracking via adaptive correlation filters">

                                <b>[21]</b> LIU T,WANG G,YANG Q X.Real-Time Part-Based Visual Tracking via Adaptive Correlation Filters // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:4902-4912.
                            </a>
                        </p>
                        <p id="408">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reliable patch trackers:robust visual tracking by exploiting reliable patches">

                                <b>[22]</b> LI Y,ZHU J K,HOI S C H.Reliable Patch Trackers:Robust Visual Tracking by Exploiting Reliable Patches // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2015:353-361.
                            </a>
                        </p>
                        <p id="410">
                            <a id="bibliography_23" >
                                    <b>[23]</b>
                                 WU Y,LIM J,YANG M H.Object Tracking Benchmark.IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(9):1834-1848.
                            </a>
                        </p>
                        <p id="412">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Encoding color information for visual tracking:Algorithms and bechmark,&amp;quot;">

                                <b>[24]</b> LIANG P P,BLASCH E,LING H B.Encoding Color Information for Visual Tracking:Algorithms and Benchmark.IEEE Transactions on Image Processing,2015,24(12):5630-5644.
                            </a>
                        </p>
                        <p id="414">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative scale space tracking">

                                <b>[25]</b> DANELLJAN M,HÄGER G,KHAN F S,et al.Discriminative Scale Space Tracking.IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,39(8):1561-1575.
                            </a>
                        </p>
                        <p id="416">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for realtime tracking">

                                <b>[26]</b> BERTINETTO L,VALMADRE J,GOLODETZ S,et al.Staple:Complementary Learners for Real-Time Tracking // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:1401-1409.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201910001" />
        <input id="dpi" type="hidden" value="96" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201910001&amp;v=MzIzOTFuRnkva1dyL05LRDdZYkxHNEg5ak5yNDlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
