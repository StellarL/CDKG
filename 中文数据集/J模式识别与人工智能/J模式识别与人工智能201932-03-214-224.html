<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131441145186250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201903004%26RESULT%3d1%26SIGN%3dACc%252fSc4AKujbmyjjcvo80wah7OA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201903004&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201903004&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201903004&amp;v=MDAwMzdCdEdGckNVUkxPZVplUm5GeXpuVjcvTktEN1liTEc0SDlqTXJJOUZZSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#227" data-title="1 相关知识 ">1 相关知识</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#228" data-title="1.1 零样本图像识别">1.1 零样本图像识别</a></li>
                                                <li><a href="#236" data-title="1.2 零样本识别模型">1.2 零样本识别模型</a></li>
                                                <li><a href="#243" data-title="1.3 投影域移位">1.3 投影域移位</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#251" data-title="2 关系网络改进语义自编码器的零样本识别算法 ">2 关系网络改进语义自编码器的零样本识别算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#253" data-title="2.1 语义自编码下的特征映射">2.1 语义自编码下的特征映射</a></li>
                                                <li><a href="#288" data-title="2.2 基于特征级联的关系网络">2.2 基于特征级联的关系网络</a></li>
                                                <li><a href="#296" data-title="2.3 基于关系网络改进的零样本图像识别">2.3 基于关系网络改进的零样本图像识别</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#314" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#315" data-title="3.1 实验设置">3.1 实验设置</a></li>
                                                <li><a href="#326" data-title="3.2 评价指标与可视化">3.2 评价指标与可视化</a></li>
                                                <li><a href="#339" data-title="3.3 识别性能对比分析">3.3 识别性能对比分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#379" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#240" data-title="图1 DAP与IAP模型结构示意图Fig.1 DAP and IAP model structure">图1 DAP与IAP模型结构示意图Fig.1 DAP and IAP model structure</a></li>
                                                <li><a href="#240" data-title="图1 DAP与IAP模型结构示意图Fig.1 DAP and IAP model structure">图1 DAP与IAP模型结构示意图Fig.1 DAP and IAP model structure</a></li>
                                                <li><a href="#273" data-title="图2 语义自编码器结构Fig.2 Semantic auto-encoder structure">图2 语义自编码器结构Fig.2 Semantic auto-encoder structure</a></li>
                                                <li><a href="#290" data-title="图3 基于特征级联的关系网络部分示意图Fig.3 Part of relation network based on feature concatenation">图3 基于特征级联的关系网络部分示意图Fig.3 Part of relation network ......</a></li>
                                                <li><a href="#299" data-title="图4 零样本图像识别框架Fig.4 Zero-shot image recognition structure">图4 零样本图像识别框架Fig.4 Zero-shot image recognition stru......</a></li>
                                                <li><a href="#322" data-title="表1 实验数据集详细信息Table 1 Details of experimental datasets">表1 实验数据集详细信息Table 1 Details of experimental datase......</a></li>
                                                <li><a href="#342" data-title="图5 在3个数据集上识别正确率随超参数λ变化曲线Fig.5 Recognition rate changing with hyperparameterλon 3datasets">图5 在3个数据集上识别正确率随超参数λ变化曲线Fig.5 Recognition rate cha......</a></li>
                                                <li><a href="#342" data-title="图5 在3个数据集上识别正确率随超参数λ变化曲线Fig.5 Recognition rate changing with hyperparameterλon 3datasets">图5 在3个数据集上识别正确率随超参数λ变化曲线Fig.5 Recognition rate cha......</a></li>
                                                <li><a href="#347" data-title="图6 图像特征和语义映射特征的分布可视化对比Fig.6 Visualization comparison of image features and semantic features">图6 图像特征和语义映射特征的分布可视化对比Fig.6 Visualization comparis......</a></li>
                                                <li><a href="#347" data-title="图6 图像特征和语义映射特征的分布可视化对比Fig.6 Visualization comparison of image features and semantic features">图6 图像特征和语义映射特征的分布可视化对比Fig.6 Visualization comparis......</a></li>
                                                <li><a href="#351" data-title="图7 测试类的语义向量可视化分析Fig.7 Visualization analysis of semantic vectors of test classes">图7 测试类的语义向量可视化分析Fig.7 Visualization analysis of se......</a></li>
                                                <li><a href="#354" data-title="图8 视觉-语义投影下不同层数对识别率影响Fig.8 Effect of different layer numbers in vision-semantic projection on recognition rate">图8 视觉-语义投影下不同层数对识别率影响Fig.8 Effect of different lay......</a></li>
                                                <li><a href="#358" data-title="图9 语义-视觉投影下不同层数对识别率影响Fig.9 Effect of different layer numbers in semantic-vision projection on recognition rate">图9 语义-视觉投影下不同层数对识别率影响Fig.9 Effect of different lay......</a></li>
                                                <li><a href="#362" data-title="表2 各方法在小样本数据集上的识别率对比Table 2 Comparison of recognition rates of different methods on small sample datasets">表2 各方法在小样本数据集上的识别率对比Table 2 Comparison of recognit......</a></li>
                                                <li><a href="#364" data-title="表3 各方法在大型数据集上的识别率对比Table 3 Comparison of recognition rates of different methods on large sample datasets%">表3 各方法在大型数据集上的识别率对比Table 3 Comparison of recogniti......</a></li>
                                                <li><a href="#368" data-title="图1 0 本文算法识别效果的混淆矩阵Fig.10 Confusion matrix of recognition results of the proposed algorithm">图1 0 本文算法识别效果的混淆矩阵Fig.10 Confusion matrix of recog......</a></li>
                                                <li><a href="#388" data-title="图1 1 投影域漂移示意图Fig.11 Sketch map of projection domain shift">图1 1 投影域漂移示意图Fig.11 Sketch map of projection domai......</a></li>
                                                <li><a href="#388" data-title="图1 1 投影域漂移示意图Fig.11 Sketch map of projection domain shift">图1 1 投影域漂移示意图Fig.11 Sketch map of projection domai......</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="9">


                                    <a id="bibliography_1" title="SMIRNOV E A, TIMOSHENKO D M, ANDRIANOV S N.Comparison of Regularization Methods for Image Net Classification with Deep Convolutional Neural Networks.AASRI Procedia, 2014, 6:89-94." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061700310924&amp;v=MjE0OTNiSzhIdGZOcUk5Rlorb1BCWDQ5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZ3UWFoYz1OaWZPZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        SMIRNOV E A, TIMOSHENKO D M, ANDRIANOV S N.Comparison of Regularization Methods for Image Net Classification with Deep Convolutional Neural Networks.AASRI Procedia, 2014, 6:89-94.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_2" title="LAMPERT C H, NICKISCH H, HARMELING S.Attribute-Based Classification for Zero-Shot Visual Object Categorization.IEEETransactions on Pattern Analysis and Machine Intelligence, 2014, 36 (3) :453-465." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attribute-Based Classification for Zero-Shot Visual Object Categorization">
                                        <b>[2]</b>
                                        LAMPERT C H, NICKISCH H, HARMELING S.Attribute-Based Classification for Zero-Shot Visual Object Categorization.IEEETransactions on Pattern Analysis and Machine Intelligence, 2014, 36 (3) :453-465.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_3" title="HWANG S J, SHA F, GRAUMAN K.Sharing Features between Objects and Their Attributes//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2011:1761-1768." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sharing features between objects and their attributes">
                                        <b>[3]</b>
                                        HWANG S J, SHA F, GRAUMAN K.Sharing Features between Objects and Their Attributes//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2011:1761-1768.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_4" title="CHEN L, ZHANG Q, LI B X.Predicting Multiple Attributes via Relative Multi-task Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:1027-1034." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Predicting multiple attributes via relative multi-task learning">
                                        <b>[4]</b>
                                        CHEN L, ZHANG Q, LI B X.Predicting Multiple Attributes via Relative Multi-task Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:1027-1034.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_5" title="AKATA Z, REED S, WALTER D, et al.Evaluation of Output Embeddings for Fine-Grained Image Classification//Proc of the IEEEConference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:2927-2936." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Evaluation of output embeddings for fine-grained image classification">
                                        <b>[5]</b>
                                        AKATA Z, REED S, WALTER D, et al.Evaluation of Output Embeddings for Fine-Grained Image Classification//Proc of the IEEEConference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:2927-2936.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_6" title="XIAN Y Q, AKATA Z, SHARMA G, et al.Latent Embeddings for Zero-Shot Classification//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:69-77." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Latent embeddings for zero-shot classification">
                                        <b>[6]</b>
                                        XIAN Y Q, AKATA Z, SHARMA G, et al.Latent Embeddings for Zero-Shot Classification//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:69-77.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_7" title="BA J L, SWERSKY K, FIDLER S, et al.Predicting Deep ZeroShot Convolutional Neural Networks Using Textual Descriptions//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:4247-4255." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Predicting deep zero-shot convolutional neural networks using textual descriptions">
                                        <b>[7]</b>
                                        BA J L, SWERSKY K, FIDLER S, et al.Predicting Deep ZeroShot Convolutional Neural Networks Using Textual Descriptions//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:4247-4255.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_8" title="乔雪, 彭晨, 段贺, 等.基于共享特征相对属性的零样本图像分类.电子与信息学报, 2017, 39 (7) :1563-1570. (QIAO X, PENG C, DUAN H, et al.Shared Features Based Relative Attributes for Zero-Shot Image Classification.Journal of Electronics and Information Technology, 2017, 39 (7) :1563-1570.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201707006&amp;v=MjU3NTdvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5em5WNy9OSVRmU2RyRzRIOWJNcUk5Rlk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        乔雪, 彭晨, 段贺, 等.基于共享特征相对属性的零样本图像分类.电子与信息学报, 2017, 39 (7) :1563-1570. (QIAO X, PENG C, DUAN H, et al.Shared Features Based Relative Attributes for Zero-Shot Image Classification.Journal of Electronics and Information Technology, 2017, 39 (7) :1563-1570.) 
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_9" title="程玉虎, 乔雪, 王雪松.基于混合属性的零样本图像分类.电子学报, 2017, 45 (6) :1462-1468. (CHENG Y H, QIAO X, WANG X S.Hybrid Attribute-Based ZeroShot Image Classification.Acta Electronica Sinica, 2017, 45 (6) :1462-1468.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201706026&amp;v=MzIyNjhaZVJuRnl6blY3L05JVGZUZTdHNEg5Yk1xWTlIWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        程玉虎, 乔雪, 王雪松.基于混合属性的零样本图像分类.电子学报, 2017, 45 (6) :1462-1468. (CHENG Y H, QIAO X, WANG X S.Hybrid Attribute-Based ZeroShot Image Classification.Acta Electronica Sinica, 2017, 45 (6) :1462-1468.) 
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_10" title="JI Z, YU Y L, PANG Y W, et al.Manifold Regularized Cross-Modal Embedding for Zero-Shot Learning.Information Sciences, 2017, 378:48-58." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC9226795E95D84ED42F5C361E675ED1F&amp;v=MTY1MDZjemZyZmhNYnZwQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0xxN3dLMD1OaWZPZmNDeEhOUEtxSVpBRWVJS2VIUTl1bUlYNkVsNE8zemtyVw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        JI Z, YU Y L, PANG Y W, et al.Manifold Regularized Cross-Modal Embedding for Zero-Shot Learning.Information Sciences, 2017, 378:48-58.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_11" title="SOCHER R, GANJOO M, BASTANI O, et al.Zero-Shot Lear-ning through Cross-Modal Transfer//BURGES C J C, BOTTOU L, WELLING W, et al., eds.Advances in Neural Information Processing Systems 26.Cambridge, USA:The MIT Press, 2013:935-943." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning through cross-modal transfer">
                                        <b>[11]</b>
                                        SOCHER R, GANJOO M, BASTANI O, et al.Zero-Shot Lear-ning through Cross-Modal Transfer//BURGES C J C, BOTTOU L, WELLING W, et al., eds.Advances in Neural Information Processing Systems 26.Cambridge, USA:The MIT Press, 2013:935-943.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_12" title="FU Y W, HOSPEDALES T M, XIANG T, et al.Transductive Multi-view Zero-Shot Learning.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (11) :2332-2345." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transductive multi-view zero-shot learning">
                                        <b>[12]</b>
                                        FU Y W, HOSPEDALES T M, XIANG T, et al.Transductive Multi-view Zero-Shot Learning.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (11) :2332-2345.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_13" title="KODIROV E, XIANG T, FU Z Y, et al.Unsupervised Domain Adaptation for Zero-Shot Learning//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2016:2452-2460." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised Domain Adaptation for Zero-Shot Learning">
                                        <b>[13]</b>
                                        KODIROV E, XIANG T, FU Z Y, et al.Unsupervised Domain Adaptation for Zero-Shot Learning//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2016:2452-2460.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_14" title="ZHANG L, XIANG T, GONG S G.Learning a Deep Embedding Model for Zero-Shot Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:2021-2030." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a deep embedding model for zero-shot learning">
                                        <b>[14]</b>
                                        ZHANG L, XIANG T, GONG S G.Learning a Deep Embedding Model for Zero-Shot Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:2021-2030.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_15" title="KODIROV E, XIANG T, GONG S G.Semantic Autoencoder for Zero-Shot Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:4447-4456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic Autoencoder for Zero-Shot Learning">
                                        <b>[15]</b>
                                        KODIROV E, XIANG T, GONG S G.Semantic Autoencoder for Zero-Shot Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:4447-4456.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_16" title="XIAN Y Q, LORENZ T, SCHIELE B, et al.Feature Generating Networks for Zero-Shot Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:5542-5551." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature Generating Networks for Zero-Shot Learning">
                                        <b>[16]</b>
                                        XIAN Y Q, LORENZ T, SCHIELE B, et al.Feature Generating Networks for Zero-Shot Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:5542-5551.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_17" title="PENNINGTON J, SOCHER R, MANNING C.Glo Ve:Global Vectors for Word Representation[C/OL].[2018-07-01].https://nlp.stanford.edu/pubs/glove.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Glo Ve:Global Vectors for Word Representation[C/OL]">
                                        <b>[17]</b>
                                        PENNINGTON J, SOCHER R, MANNING C.Glo Ve:Global Vectors for Word Representation[C/OL].[2018-07-01].https://nlp.stanford.edu/pubs/glove.pdf.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_18" title="SAENKO K, KULIS B, FRITZ M, et al.Adapting Visual Category Models to New Domains//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2010:213-226." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adapting Visual Category Models to New Domains">
                                        <b>[18]</b>
                                        SAENKO K, KULIS B, FRITZ M, et al.Adapting Visual Category Models to New Domains//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2010:213-226.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_19" title="BARTELS R H, STEWART G W.Algorithm 432:Solution of the Matrix Equation AX+XB=C.Communications of the ACM, 1972, 15 (9) :820-826." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000023944&amp;v=MTQ4OThLN0h0ak5yNDlGWk9rTUJYZzlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRndRYWhjPU5pZklZNw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                        BARTELS R H, STEWART G W.Algorithm 432:Solution of the Matrix Equation AX+XB=C.Communications of the ACM, 1972, 15 (9) :820-826.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_20" title="SUNG F, YANG Y X, ZHANG L, et al.Learning to Compare:Relation Network for Few-Shot Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:1199-1208." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to Compare:Relation Network for Few-Shot Learning">
                                        <b>[20]</b>
                                        SUNG F, YANG Y X, ZHANG L, et al.Learning to Compare:Relation Network for Few-Shot Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:1199-1208.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_21" title="ZHAO M Z, XU B, LIN H F, et al.Discover Potential Adverse Drug Reactions Using the Skip-Gram Model//Proc of the IEEEInternational Conference on Bioinformatics and Biomedicine.Washington, USA:IEEE, 2015:1765-1767." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discover potential adverse drug reactions using the skip-gram model">
                                        <b>[21]</b>
                                        ZHAO M Z, XU B, LIN H F, et al.Discover Potential Adverse Drug Reactions Using the Skip-Gram Model//Proc of the IEEEInternational Conference on Bioinformatics and Biomedicine.Washington, USA:IEEE, 2015:1765-1767.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_22" >
                                        <b>[22]</b>
                                    VAN DER MAATEN L, HINTON G.Visualizing Data Using tSNE.Journal of Machine Learning Research, 2008, 9:2579-2605.</a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_23" title="ROMERA-PAREDES B, TORR P H S.An Embarrassingly Simple Approach to Zero-Shot Learning//FERIS R S, LAMPERT C, PARIKH D, eds.Visual Attributes.Berlin, Germany:Springer, 2017:11-30." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Embarrassingly Simple Approach to Zero-Shot Learning">
                                        <b>[23]</b>
                                        ROMERA-PAREDES B, TORR P H S.An Embarrassingly Simple Approach to Zero-Shot Learning//FERIS R S, LAMPERT C, PARIKH D, eds.Visual Attributes.Berlin, Germany:Springer, 2017:11-30.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_24" title="ZHANG Z M, SALIGRAMA V.Zero-Shot Learning via Joint Latent Similarity Embedding//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:6034-6042." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-Shot Learning via Joint Latent Similarity Embedding">
                                        <b>[24]</b>
                                        ZHANG Z M, SALIGRAMA V.Zero-Shot Learning via Joint Latent Similarity Embedding//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:6034-6042.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_25" title="FU Z Y, XIANG T A, KODIROV E, et al.Zero-Shot Object Recognition by Semantic Manifold Distance//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:2635-2644." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-shot object recognition by semantic manifold distance">
                                        <b>[25]</b>
                                        FU Z Y, XIANG T A, KODIROV E, et al.Zero-Shot Object Recognition by Semantic Manifold Distance//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:2635-2644.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_26" title="NOROUZI M, MIKOLOV T, BENGIO S, et al.Zero-Shot Learning by Convex Combination of Semantic Embeddings[C/OL].[2018-07-01].https://arxiv.org/pdf/1312.5650.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Zero-Shot Learning by Convex Combination of Semantic Embeddings[C/OL]">
                                        <b>[26]</b>
                                        NOROUZI M, MIKOLOV T, BENGIO S, et al.Zero-Shot Learning by Convex Combination of Semantic Embeddings[C/OL].[2018-07-01].https://arxiv.org/pdf/1312.5650.pdf.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(03),214-224 DOI:10.16451/j.cnki.issn1003-6059.201903003            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">语义自编码结合关系网络的零样本图像识别算法</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9E%97%E5%85%8B%E6%AD%A3&amp;code=07008014&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">林克正</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%98%8A%E5%A4%A9&amp;code=41384441&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李昊天</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%99%BD%E5%A9%A7%E8%BD%A9&amp;code=41420906&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">白婧轩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E9%AA%9C&amp;code=33364177&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李骜</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%93%88%E5%B0%94%E6%BB%A8%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0003194&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">哈尔滨理工大学计算机科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了解决零样本图像识别中传统模型容易出现投影域移位问题以及提高距离相似度度量的鲁棒性, 提出关系网络改进语义自编码器的零样本识别算法.基于语义自编码器构建图像视觉特征和语义向量之间的特征映射, 并将重构向量与对应向量真值进行级联后送入神经网络, 最终利用输出的标量给出预测类别.实验表明, 相比传统距离度量方法, 文中算法在AWA、CUB和Image Net-2数据集上的识别率均有所提高, 在某些数据集上语义-视觉的投影效果优于反向投影.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义自编码器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%B3%E7%B3%BB%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">关系网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9B%B6%E6%A0%B7%E6%9C%AC%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">零样本识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E5%90%91%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义向量;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%8A%95%E5%BD%B1%E5%9F%9F%E7%A7%BB%E4%BD%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">投影域移位;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *林克正 (通讯作者) , 博士, 教授, 主要研究方向为图像处理、机器视觉、模式识别等.E-mail:link@hrbust.edu.cn.;
                                </span>
                                <span>
                                    李昊天, 硕士研究生, 主要研究方向为机器学习、计算机视觉.E-mail:18800426021@163.com.;
                                </span>
                                <span>
                                    白婧轩, 硕士研究生, 主要研究方向为图像处理、计算机视觉.E-mail:781758410@qq.com.;
                                </span>
                                <span>
                                    李骜, 博士, 讲师, 主要研究方向为稀疏表示、图像复原、计算机视觉等.E-mail:29371647@qq.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-25</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.61501147);</span>
                                <span>黑龙江省自然科学基金项目 (No.F2015040);</span>
                                <span>黑龙江省普通本科高等学校青年创新人才培养计划 (No.2018203) 资助;</span>
                    </p>
            </div>
                    <h1>Zero-Shot Image Recognition Algorithm via Semantic Auto-Encoder Combining Relation Network</h1>
                    <h2>
                    <span>LIN Kezheng</span>
                    <span>LI Haotian</span>
                    <span>BAI Jingxuan</span>
                    <span>LI Ao</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, Harbin University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A semantic auto-encoder structure improved by relation network is proposed and used for zero sample identification algorithm to handle the projection domain shift problem and improve the robustness of distance similarity measure in the traditional model of zero-shot recognition. The feature map between image visual features and semantic vectors is constructed by the proposed algorithm based on the semantic auto-encoder, and then the reconstructed vector is sent to the neural network after concatenating the true value of the corresponding vector. Finally, the prediction category is determined by the output scalar. The experimental results show that compared with the traditional distance measurement method, the recognition rate of the proposed algorithm on the public datasets AWA, CUB and ImageNet-2 is improved and its semantic-visual projection has a better effect than back projection on some datasets.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Semantic%20Auto-Encoder&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Semantic Auto-Encoder;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Relation%20Network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Relation Network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Zero-Shot%20Recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Zero-Shot Recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Semantic%20Vector&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Semantic Vector;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Projection%20Domain%20Shift&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Projection Domain Shift;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LIN Kezheng ( Corresponding author) , Ph. D., professor. His research interests include image processing, machine vision and pattern recognition.;
                                </span>
                                <span>
                                    LI Haotian, master student. His research interests include machine learning and computer vision.;
                                </span>
                                <span>
                                    BAI Jingxuan, master student. Her research interests include image processing and computer vision.;
                                </span>
                                <span>
                                    LI Ao, Ph.D., lecturer. His research interests include sparse representation, image restoration and computer vision.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-25</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.61501147);</span>
                                <span>Natural Science Foundation of Heilongjiang Province (No.F2015040);</span>
                                <span>University Nursing Program for Young Scholars with Creative Talents in Heilongjiang Province (No.2018203);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="221">基于深度学习的图像识别已取得重大进展<citation id="389" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 但对人工标注的训练样本需求越来越大.为了减少模型对训练样本的过度依赖, 并提高传统分类识别任务的延展性, 零样本图像识别逐渐成为目前机器视觉领域的研究热点之一.零样本学习的思想在于仿照人类无需真正看见实际视觉图像, 就能通过一些先验知识, 如属性外形等的描述, 识别新类别的能力.而人之所以具有这种能力, 是因为能将未知与已知类别通过语义描述信息建立联系.因此, 零样本图像识别也可通过有标签的训练数据, 在视觉和语义空间之间建立映射关系, 最后根据训练数据和未知类别的测试集在视觉和语义上的联系, 给出测试集的预测标签.</p>
                </div>
                <div class="p1">
                    <p id="223">近些年, 已有国内外学者提出零样本识别算法.Lampert等<citation id="390" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出两种基于语义描述特征向量的零样本分类方法, 在小范围数据集上均通过类间信息传递分类.Hwang等<citation id="393" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出使用多任务学习研究类别与属性的关联性, 并利用类别属性间的共享特征提高分类器性能, 但未进一步应用在相对属性学习上.Chen等<citation id="394" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>把每个属性的排序函数看成一个任务, 采用多任务学习的同时又学习所有的属性排序函数, 最后使用相对属性间的关联性提高精度, 然而该方法只对属性进行多任务学习, 缺乏考虑结合类别的关联分析, 依然存在噪声干扰.Akata等<citation id="395" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出联合嵌入模型, 将图像与语义特征共同嵌入到同个特征空间, 取得较好结果, 但该方法只是单纯利用卷积神经网络 (Convolutional Neural Networks, CNN) 提取图像特征, 在分类时仍需手工进行特征提取, 并不是一种端到端的深度学习方法.Xian等<citation id="396" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>在非线性映射中训练多个线性子分类器, 使用多个分类器进行预测, 并依照概率最大原则选取预测标签, 提高识别率.Ba等<citation id="397" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>在深度学习的基础上使用多特征融合, 但只使用样本的词向量, 没有加入图像特征, 最后的效果也不尽人意.乔雪等<citation id="398" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>利用属性间的关联性优化零样本识别模型.程玉虎等<citation id="399" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>考虑到语义属性维度的局限性, 将更深的底层特征作为辅助信息训练零样本模型.但文献<citation id="400" type="reference">[<a class="sup">8</a>]</citation>和文献<citation id="391" type="reference">[<a class="sup">9</a>]</citation>均在相近数据集上进行实验, 对于跨域实验仍有待验证.Ji等<citation id="392" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>将深度网络用于图像的视觉和语义特征间的跨模态映射, 并在语义空间中使用示例差异化进行零样本多标签分类, 同样未把深度网络用在端到端的学习, 在性能上有待提高.</p>
                </div>
                <div class="p1">
                    <p id="224">在大型数据集上, Socher等<citation id="401" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>对语义向量进行迁移学习.Fu等<citation id="402" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>指出文献<citation id="403" type="reference">[<a class="sup">11</a>]</citation>方法存在一定的投影域移位 (Projection Domain Shift) 问题, 并提出启发式的一步式自我训练方法, 首先学习源域数据, 然后使用基于典型相关分析的多视图嵌入, 对齐不同语义空间与低级特征空间, 适应目标域, 但由于过程分为两个步骤, 一旦第一步中的源域信息有缺失, 就无法恢复.Kodirov等<citation id="404" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>改进文献<citation id="405" type="reference">[<a class="sup">12</a>]</citation>方法, 提出无监督的域自适应方法, 基于迁移学习, 通过在稀疏编码学习中使用正则化的视觉语义相似性匹配以纠正域移位, 有效解决从源域学习到的投影函数不适应目标域的问题, 但样本内部存在的流形几何结构并未得到充分运用.Zhang等<citation id="406" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>基于深度卷积神经网络实现端到端映射的零样本识别模型.Kodirov等<citation id="407" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出基于语义自编码 (Semantic Auto Encoder, SAE) 的零样本模型, 在映射层添加具体语义信息的限制, 约束重建效果, 实现有监督下的投影函数学习.目前的零样本识别模型大多以语义属性描述或词向量作为迁移知识, 通过增加约束以提高视觉空间到语义空间的映射相似性, 在一定程度上解决投影域移位问题, 提升识别率.</p>
                </div>
                <div class="p1">
                    <p id="225">此外, Xian等<citation id="408" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>在改进基于沃瑟斯坦距离的生成对抗网络 (Wasserstein Generative Adversarial Networks, WGAN) 基础上, 提出基于变分散度最小化分类损失和沃瑟斯坦距离的生成对抗网络 (fDivergence Class Wasserstein Generative Adversarial Networks, f-CLSWGAN) , 通过融合未知类的一些CNN特征, 强制生成器网络学习模仿不可见类的CNN特征, 最后让网络更偏向于生成未知类的图像, 取得不错效果, 在主观视觉上具有较好的解释性, 但该方法更是一种基于语义描述的图像生成.生成对抗网络 (Generative Adversarial Networks, GAN) 的生成器依赖训练集的数量以提高泛化能力, 对提取后特征的分类需要强大的约束.零样本学习的根本问题是为了解决模型在源域转移到目标域上的泛化问题, 这类方法从另一个角度延伸零样本识别的解决思路.</p>
                </div>
                <div class="p1">
                    <p id="226">在映射后的语义空间下, 学习部分主要体现在特征嵌入方面, 零样本模型的推理预测部分还存在较大改进空间:仅使用预先定义好的距离函数不利于预测未知类别.因此, 本文引入基于关系网络改进的零样本识别模型 (Relation Network for Improving SAE Zero-Shot Recognition, RNSAE) , 借助自编码器在已知类别的图像特征与其语义向量之间学习一个映射矩阵, 实现高维数据之间的特征对齐, 最后将训练得到的向量和对应向量真值进行级联 (Concatenation) , 通过数层简单神经网络训练后输出一个[0, 1]内的标量, 用于表示和真值之间的相关性.对比它们的大小, 得到最终识别结果.</p>
                </div>
                <h3 id="227" name="227" class="anchor-tag">1 相关知识</h3>
                <h4 class="anchor-tag" id="228" name="228">1.1 零样本图像识别</h4>
                <div class="p1">
                    <p id="229">零样本图像识别是指识别在训练集中未曾见过的类别, 而训练集和测试集在类别上无重合, 因此需借助一些辅助信息构建图像与标签之间的关系.常用的辅助信息主要来源于通过大量文本提取的词向量信息, 如基于全局词频统计的词表征工具 (Global Vectors for World Representation, GloVe) <citation id="409" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>等.另一个常用的重要辅助信息是属性信息, 常在一些动物数据集中出现.这些信息通常需经过专门的数据分析后才能获得.目前, 属性信息已广泛运用于各种算法, 取得较好结果.假定Y={ (y<sub>1</sub>, y<sub>2</sub>, …, y<sub>s</sub>) }为可见类的类标签, Z={ (z<sub>1</sub>, z<sub>2</sub>, …, z<sub>u</sub>) }为不可见类的类标签, 并且Y和Z无交集, 即Y∩Z=.类似地, 定义</p>
                </div>
                <div class="area_img" id="230">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_23000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="231">为可见类语义信息, </p>
                </div>
                <div class="area_img" id="232">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_23200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="233">为不可见类语义信息.另外给定一组有N个图像样本的有标签的训练数据集 (已知类) </p>
                </div>
                <div class="area_img" id="234">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_23400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="235">其中:x<sub>i</sub>为从某已知类中选取的第i个图像特征向量, 维度为d;y<sub>i</sub>为其对应的类标签, 维度为k;s<sub>i</sub>为其对应的语义向量, 维度为k.则基本的零样本识别问题定义如下:需学习一种分类器f∶X<sub>Z</sub>→Z, 在Z中给出X<sub>Z</sub>的预测结果, 其中X<sub>Z</sub>={ (x<sub>i</sub>, z<sub>i</sub>, s<sub>i</sub>) }为测试集, 且z<sub>i</sub>和s<sub>i</sub>都未知.</p>
                </div>
                <h4 class="anchor-tag" id="236" name="236">1.2 零样本识别模型</h4>
                <div class="p1">
                    <p id="237">零样本识别模型中语义空间通常选择属性信息, 作为沟通已知类别与待测类别的中间层, 主要有直接属性预测 (Direct Attribute Prediction, DAP) 与间接属性预测 (Indirect Attribute Prediction, IAP) , 2种模型的结构分别如图1所示.X为样本的底部特征;S={s<sub>1</sub>, s<sub>2</sub>, …, s<sub>s</sub>}为可见类与不可见类的共享特征, 即属性信息;</p>
                </div>
                <div class="area_img" id="238">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_23800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="239">分别为对应的类标签.DAP与IAP都是将属性S作为桥梁进行迁移, 从而识别未知类, 在一定程度上改进模型对新样本的适应能力.</p>
                </div>
                <div class="area_img" id="240">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_24000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 DAP与IAP模型结构示意图Fig.1 DAP and IAP model structure" src="Detail/GetImg?filename=images/MSSB201903004_24000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 DAP与IAP模型结构示意图Fig.1 DAP and IAP model structure  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_24000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="240">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_24001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 DAP与IAP模型结构示意图Fig.1 DAP and IAP model structure" src="Detail/GetImg?filename=images/MSSB201903004_24001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 DAP与IAP模型结构示意图Fig.1 DAP and IAP model structure  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_24001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="243" name="243">1.3 投影域移位</h4>
                <div class="p1">
                    <p id="244">Saenko等<citation id="410" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>从自然语言处理和计算机视觉方面引入投影域移位 (Projection Domain Shift) 问题.在给定设置的数据上训练分类器, 并在不同场景中进行测试, 因为训练集与测试集属于不同的域, 最终导致性能不佳.该问题的一个规范形式描述如下:给定一个训练集 (源域) </p>
                </div>
                <div class="area_img" id="245">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_24500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="246">它有标签;一个测试集 (目标域) </p>
                </div>
                <div class="area_img" id="247">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_24700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="248">它没有标签, 从源域中学习分类器, 并应用到目标域的识别中, 源域和目标域无交集.</p>
                </div>
                <div class="p1">
                    <p id="249">基于1.1节中的零样本学习理论, 训练集和测试集无交集, 需通过语义等辅助信息进行域之间的信息传递, 传统的零样本学习方法多采用视觉特征到语义空间的映射, 或反向映射, 或将它们同时映射到一个子空间.假设在源域和目标域中分别存在两种类别:猪类和马类, 它们都具备尾巴这一属性, 但它们的属性在视觉上是不完全相同的, 分类器在猪类样本集上学习到的尾巴属性, 当用于马类的识别时就会出现问题.</p>
                </div>
                <div class="p1">
                    <p id="250">传统的零样本识别模型常面临投影领域移位的问题.由于在训练模型使用的数据集中并未包含需要测试的数据类别, 当训练集与测试集的类别相差较大时, 零样本识别模型的效果将受到很大影响.领域自适应 (Domain Adaptation, DA) 是一种解决该问题的常用方法, 它以直接推理形式, 将测试集样本去除标签后与训练样本一同学习投影函数, 在一定程度上避免漂移问题.</p>
                </div>
                <h3 id="251" name="251" class="anchor-tag">2 关系网络改进语义自编码器的零样本识别算法</h3>
                <h4 class="anchor-tag" id="253" name="253">2.1 语义自编码下的特征映射</h4>
                <div class="p1">
                    <p id="254">自编码器是神经网络的一种, 为一种数据的压缩算法, 这也意味着它是有损的, 即解压缩后的输出与原输入相比有退化.自编码器一般由三部分组成:编码器、解码器、损失函数.它的本质就是对输入信号按照某种变换进行输出.最简单的自编码器是一个线性且只有一个共享编码器与解码器的隐含层.</p>
                </div>
                <div class="p1">
                    <p id="255">设X∈R<sup>d×N</sup>为一组输入, 是由N个大小为d维的向量组成的矩阵.通过大小为k×d编码器W∈R<sup>k×d</sup>将X投影为矩阵S∈R<sup>k×N</sup>, 最后通过解码器W<sup>*</sup>∈R<sup>d×k</sup>将X投射回X<sup>*</sup>∈R<sup>d×N</sup>.为使投射回的矩阵和原矩阵的误差最小, 可将优化目标定义如下:</p>
                </div>
                <div class="area_img" id="256">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_25600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="257">传统的自编码器是无监督的, 学习到的映射子空间并无明确的语义信息.为了让映射后的子空间具备语义表示如属性或标签信息, 可将映射后的子空间S强制变为包含语义信息的数据点, 结构如图2所示, 因此目标函数也变为</p>
                </div>
                <div class="area_img" id="258">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_25800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="259">为了简化模型, 假设W<sup>*</sup>=W<sup>T</sup>, 目标函数只需求解一个映射矩阵W:</p>
                </div>
                <div class="area_img" id="260">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_26000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="261">但上式优化较困难, 考虑将强约束条件WX=S弱化, 并替换WX=S, 最终目标函数如下:</p>
                </div>
                <div class="area_img" id="262">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_26200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="263">其中, λ为加权系数, 对应编码器和解码器的损失.最终得到的式 (1) 为一个标准的二次函数, 同时是一个具有全局最优解的凸函数.</p>
                </div>
                <div class="p1">
                    <p id="264">对式 (1) 的优化可先求导, 再利用矩阵迹的性质化简, 结果如下:</p>
                </div>
                <div class="area_img" id="265">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_26500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="266">令其为0, 可得</p>
                </div>
                <div class="area_img" id="267">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_26700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="268">再令</p>
                </div>
                <div class="area_img" id="269">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_26900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="270">则最终变换为</p>
                </div>
                <div class="area_img" id="271">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_27100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="272">式 (2) 为一个Sylvester方程, 可用Bartels-Stewart算法<citation id="411" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>求解, 即可求得最终的最优映射矩阵W与W<sup>T</sup>.</p>
                </div>
                <div class="area_img" id="273">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_27300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 语义自编码器结构Fig.2 Semantic auto-encoder structure" src="Detail/GetImg?filename=images/MSSB201903004_27300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 语义自编码器结构Fig.2 Semantic auto-encoder structure  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_27300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="274">Bartels-Stewar算法是一种针对式 (2) 的数值求解方法, 利用舒尔分解 (Schur) 前向或后向替换求解.在式 (2) 中, 设A和B的Schur分解如下:</p>
                </div>
                <div class="area_img" id="275">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_27500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="276">设A和-B的特征值不相同, 即方程有唯一解;Q<sub>A</sub>和Q<sub>B</sub>为酉矩阵, R<sub>A</sub>和R<sub>B</sub>为某上三角矩阵.则方程 (3) 可等价为</p>
                </div>
                <div class="area_img" id="277">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_27700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="278">其中</p>
                </div>
                <div class="area_img" id="279">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_27900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="280">而X可通过</p>
                </div>
                <div class="area_img" id="281">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_28100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="282">求得.</p>
                </div>
                <div class="p1">
                    <p id="283">可将<image id="381" type="formula" href="images/MSSB201903004_38100.jpg" display="inline" placement="inline"><alt></alt></image>记为</p>
                </div>
                <div class="area_img" id="284">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_28400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="285">设R<sub>B</sub>=[r<sub>ij</sub><sup> (</sup><sup>2) </sup>], 可得到<image id="382" type="formula" href="images/MSSB201903004_38200.jpg" display="inline" placement="inline"><alt></alt></image>的递推公式如下:</p>
                </div>
                <div class="area_img" id="286">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_28600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="287">最终可通过前向迭代式 (4) 求出结果.</p>
                </div>
                <h4 class="anchor-tag" id="288" name="288">2.2 基于特征级联的关系网络</h4>
                <div class="p1">
                    <p id="289">由Sung等<citation id="412" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>提出的关系网络同样是为了解决零样本识别问题.针对原本只需进行距离度量的几组向量, 不使用单一的距离度量方法, 同时将它们送入数个由全连接层组成的神经网络中, 让网络自动学习这些向量之间的关系, 最后利用Sigmoid输出一个标量, 通过标量的大小反映类别之间的相似程度.本文设计的基于特征级联的关系网络如图3所示.</p>
                </div>
                <div class="area_img" id="290">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_29000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于特征级联的关系网络部分示意图Fig.3 Part of relation network based on feature concatenation" src="Detail/GetImg?filename=images/MSSB201903004_29000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 基于特征级联的关系网络部分示意图Fig.3 Part of relation network based on feature concatenation  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_29000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="291">在图3中, 特征级联 (Feature Concatenation) 依次将每对特征向量进行拼接后送入两个全连接层, 第一层使用ReLU激活并输出, 第二层使用Sigmoid函数输出一个指定的标量, 通过对比实验选取隐含层数量, 输出层维度为1.ReLU表示整流线性函数, 它的作用是进行非线性输出, 常用作隐层神经元输出:</p>
                </div>
                <div class="area_img" id="292">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_29200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="293">它可更有效地进行梯度下降与反向传播.在使用ReLU的过程中, 需要防止学习率选取过大, 导致网络中出现大量梯度为0的情况.在全连接网络的最后一层, 使用Sigmoid函数输出:</p>
                </div>
                <div class="area_img" id="294">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_29400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="295">将值映射到[0, 1]内, 并用于二分类.</p>
                </div>
                <h4 class="anchor-tag" id="296" name="296">2.3 基于关系网络改进的零样本图像识别</h4>
                <div class="p1">
                    <p id="297">按照1.1节中的假设, 并基于特征级联和语义自编码器, 本文设计的零样本图像识别框架如图4所示.</p>
                </div>
                <div class="p1">
                    <p id="298">该框架首先使用深度神经网络 (如并联卷积网络 (Inception Networks Version 2/3) , 残差网络 (Residual Networks, ResNet) 等) 对原始图像样本及测试图像样本进行特征提取, 分别得到X<sub>Y</sub>与X<sub>Z</sub>两组向量.同时使用语义模型提取已知类的语义向量S<sub>Y</sub>.再通过给定的语义自编码器求得映射矩阵W及其转置W<sup>T</sup>.最后分别生成测试样本的新图像与语义特征, 将其进行特征级联后送入两个全连接层, 并使用Sigmoid输出一个值, 通过对比大小得到未知图像的预测标签.子网络结构已在2.2节中给出.本文算法同时使用视觉空间与语义空间进行预测输出, 纵向对比编码器与解码器的效果.</p>
                </div>
                <div class="area_img" id="299">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_29900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 零样本图像识别框架Fig.4 Zero-shot image recognition structure" src="Detail/GetImg?filename=images/MSSB201903004_29900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 零样本图像识别框架Fig.4 Zero-shot image recognition structure  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_29900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="300">测试时, 使用该框架的映射矩阵W及其转置矩阵W<sup>T</sup>, 按照如下两个方向检验.</p>
                </div>
                <div class="p1">
                    <p id="301">1) 视觉特征-语义特征 (编码器) .将测试样本X<sub>Z</sub>={ (x<sub>i</sub>, z<sub>i</sub>, s<sub>i</sub>) }下的x<sub>i</sub>通过</p>
                </div>
                <div class="area_img" id="302">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_30200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="303">映射矩阵投影到语义空间.通过特征级联输出最大相似度类别标签</p>
                </div>
                <div class="area_img" id="304">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_30400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="305">其中:<image id="383" type="formula" href="images/MSSB201903004_38300.jpg" display="inline" placement="inline"><alt></alt></image>为第j个不可见类的语义向量真值;Φ (·) 用于返回一个预测的类别标签;∫ (, ·, ) 为特征级联运算并得到的输出.</p>
                </div>
                <div class="p1">
                    <p id="306">2) 语义特征-视觉特征 (解码器) .利用解码器映射矩阵W<sup>T</sup>, 可实现语义空间到视觉特征的投影.S<sub>Z</sub>={s<sub>1</sub>, s<sub>2</sub>, …, s<sub>u</sub>}为未知类别的语义向量, 通过</p>
                </div>
                <div class="area_img" id="307">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_30700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="384">得到<image id="385" type="formula" href="images/MSSB201903004_38500.jpg" display="inline" placement="inline"><alt></alt></image>图像特征的投影值, <image id="386" type="formula" href="images/MSSB201903004_38600.jpg" display="inline" placement="inline"><alt></alt></image>.最后, 使用</p>
                </div>
                <div class="area_img" id="309">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_30900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="310">得到预测类, 其中, X<sub>i</sub>为第i个已知类的图像特征.</p>
                </div>
                <div class="p1">
                    <p id="311">常见的特征-语义空间自编码器的映射函数如</p>
                </div>
                <div class="area_img" id="312">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_31200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="313">可发现其与岭回归 (Ridge Regression) 的损失函数类似.对比式 (5) 与式 (1) 可得, 双向投影的自编码器在映射矩阵上增加额外的惩罚项, 可以看作是基于L2范数的改进, 有效避免过拟合.</p>
                </div>
                <h3 id="314" name="314" class="anchor-tag">3 实验及结果分析</h3>
                <h4 class="anchor-tag" id="315" name="315">3.1 实验设置</h4>
                <div class="p1">
                    <p id="316">本文实验的软硬件环境为:Windows 7 (64位) , Python 3.6, PyTorch 0.4.0, NVIDIA GTX 750Ti, RAM 8G及i5-4590.</p>
                </div>
                <div class="p1">
                    <p id="319">本次实验选择的数据集有Animals with Attributes (AwA) 、Caltech-UCSD-Birds-200-2011 (CUB) 及ImageNet-2数据集.使用的数据均来源于网络, 所有数据集的详细参数如表1所示.</p>
                </div>
                <div class="area_img" id="322">
                                            <p class="img_tit">
                                                表1 实验数据集详细信息Table 1 Details of experimental datasets
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_32200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201903004_32200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_32200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 实验数据集详细信息Table 1 Details of experimental datasets" src="Detail/GetImg?filename=images/MSSB201903004_32200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="323">AwA为动物属性数据集, 按照官方划分, 40个可见类共有样本24 295个, 其余6 180个为测试样本.CUB鸟类数据集按官方划分, 可见类150个, 共有8 855个训练图像, 剩余50类, 共2 933个为测试图像.ImageNet-2为大型数据集, 分别由ILSVRC-2012/2010组成训练集与测试集.</p>
                </div>
                <div class="p1">
                    <p id="324">本文实验的所有数据集图像利用GoogleNet提取特征, 维度均为1 024.前两个数据集本身提供一组属性信息, ImageNet-2使用由随机相邻词向量模型 (Skip-Gram) <citation id="413" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>训练得到的1 000维词向量.</p>
                </div>
                <div class="p1">
                    <p id="325">本文实验的参数主要由2部分组成:1) 编码器只有一个超参数λ, 通过对训练集进行分类交叉验证的方式求得最优解, 嵌入层始终等于对应语义空间的维度;2) 关系网络中的全连接层权值参数已在2.2节中给出.</p>
                </div>
                <h4 class="anchor-tag" id="326" name="326">3.2 评价指标与可视化</h4>
                <div class="p1">
                    <p id="327">图像识别模型的性能主要体现在图像的特征空间上, 包含更多表示信息的图像特征能更好地训练模型, 提高准确率.为了说明将视觉投影到语义空间的优势, 考虑将本文提出的语义自编码器模型使用基于t分布的随机邻域嵌入法 (T-Distributed Stochastic Neighbor Embedding, T-SNE) 降维<citation id="414" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>, 进行可视化.T-SNE降维基于随机邻域嵌入法 (Stochastic Neighbor Embedding, SNE) 进行改进, 是一种非线性的降维方法, 适合将高维数据降至2或3维, 从而进行特征的可视化.</p>
                </div>
                <div class="p1">
                    <p id="328">SNE基于高维空间中相似的样本点映射到低维空间后, 距离也是相近的原理, 并以KL距离度量两个条件概率的相似性.设x<sub>i</sub>、x<sub>j</sub>为高维空间中的样本点对, y<sub>i</sub>、y<sub>j</sub>为低维空间中的样本点对, 衡量x<sub>j</sub>离x<sub>i</sub>的近邻程度的条件概率</p>
                </div>
                <div class="area_img" id="329">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_32900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="330">同理, y<sub>j</sub>与y<sub>i</sub>的距离概率</p>
                </div>
                <div class="area_img" id="331">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_33100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="332">其中高斯分布中的方差σ<sub>i</sub>均设置为<image id="387" type="formula" href="images/MSSB201903004_38700.jpg" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <div class="p1">
                    <p id="334">最后以最小化两者的KL距离为目标, 使用梯度下降法优化如下代价函数:</p>
                </div>
                <div class="area_img" id="335">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903004_33500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="336">但不足之处是, 代价函数过分考虑局部结构, 忽略全局结构的重要性.</p>
                </div>
                <div class="p1">
                    <p id="337">因此, 针对SNE中的p<sub>j i</sub>与p<sub>i j</sub>不相等及不同类别的簇拥挤等问题, 通过修改P和Q的定义, 并在低维空间中改高斯分布为t分布, 较好地解决上述问题, 即T-SNE.</p>
                </div>
                <div class="p1">
                    <p id="338">针对评价指标, 本文除选取正确率外, 还使用混淆矩阵 (Confusion Matrix) 评判识别的优劣程度.单一从识别正确率的角度分析模型的优劣是片面的, 因为它只能在被正确分类的样本数中考虑总体关系, 却不能纵观全局, 而混淆矩阵的引入可较好解决这一缺点, 它除了可观察每类的正确识别个数, 还可直观显示被错误识别的样本数量.</p>
                </div>
                <h4 class="anchor-tag" id="339" name="339">3.3 识别性能对比分析</h4>
                <div class="p1">
                    <p id="340">对于自编码器部分, 首先需要通过分类交叉验证找到最优的超参数λ.根据数据集的不同分别确定不同的最优参数.因此, 分别将训练数据集分为5份, 每次选取4份作为训练集, 余下的作为测试集, 依次进行测试并取平均值.得到3组数据集的实验结果如图5所示.</p>
                </div>
                <div class="p1">
                    <p id="341">由图5可知, 3组数据集的最优超参数λ分别选用5×10<sup>5</sup>, 0.2, 5, 可得到最佳效果.</p>
                </div>
                <div class="area_img" id="342">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_34200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 在3个数据集上识别正确率随超参数λ变化曲线Fig.5 Recognition rate changing with hyperparameterλon 3datasets" src="Detail/GetImg?filename=images/MSSB201903004_34200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 在3个数据集上识别正确率随超参数λ变化曲线Fig.5 Recognition rate changing with hyperparameterλon 3datasets  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_34200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="342">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_34201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 在3个数据集上识别正确率随超参数λ变化曲线Fig.5 Recognition rate changing with hyperparameterλon 3datasets" src="Detail/GetImg?filename=images/MSSB201903004_34201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 在3个数据集上识别正确率随超参数λ变化曲线Fig.5 Recognition rate changing with hyperparameterλon 3datasets  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_34201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="345">图像识别模型的性能可从图像特征空间上进行主观推测, 本文使用3.2节的T-SNE进行可视化研究.以AwA的测试数据集为例, 将原始只通过GoogleNet提取的视觉特征与经过自编码器映射成的语义特征进行可视化对比, 结果如图6所示.</p>
                </div>
                <div class="p1">
                    <p id="346">图6中 (a) 、 (b) 分别为原始的视觉向量与经过映射后的语义向量的特征分布情况, 共有10个类别.观察可知 (b) 比 (a) 的类间间隔更大, 而类内的间隔更小, 说明模型更有利于划分未知类别的标签.</p>
                </div>
                <div class="area_img" id="347">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_34700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 图像特征和语义映射特征的分布可视化对比Fig.6 Visualization comparison of image features and semantic features" src="Detail/GetImg?filename=images/MSSB201903004_34700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 图像特征和语义映射特征的分布可视化对比Fig.6 Visualization comparison of image features and semantic features  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_34700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="347">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_34701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 图像特征和语义映射特征的分布可视化对比Fig.6 Visualization comparison of image features and semantic features" src="Detail/GetImg?filename=images/MSSB201903004_34701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 图像特征和语义映射特征的分布可视化对比Fig.6 Visualization comparison of image features and semantic features  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_34701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="350">另外, 对测试集类别标签的语义向量进行可视化, 仍以AwA数据集为例, 可得到图7结果.</p>
                </div>
                <div class="area_img" id="351">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_35100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 测试类的语义向量可视化分析Fig.7 Visualization analysis of semantic vectors of test classes" src="Detail/GetImg?filename=images/MSSB201903004_35100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 测试类的语义向量可视化分析Fig.7 Visualization analysis of semantic vectors of test classes  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_35100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="352">观察图7可知, 黑猩猩、座头鲸、豹、波斯猫、大熊猫及浣熊的属性向量分布具有明显的距离差距, 相对容易区分.而河马分别与猪及海豹之间的属性空间分布较近, 有一些已相互干扰, 因此, 它们对应的视觉特征也容易混淆, 会对识别率造成一定影响.</p>
                </div>
                <div class="p1">
                    <p id="353">本文首先设置级联网络部分的自身对比实验, 探究不同的网络参数对识别率的影响, 基于多数理论, 在视觉-语义方向分别设置多种隐层数, 对比不同参数下的识别率变化, 结果如图8所示.</p>
                </div>
                <div class="area_img" id="354">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_35400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 视觉-语义投影下不同层数对识别率影响Fig.8 Effect of different layer numbers in vision-semantic projection on recognition rate" src="Detail/GetImg?filename=images/MSSB201903004_35400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 视觉-语义投影下不同层数对识别率影响Fig.8 Effect of different layer numbers in vision-semantic projection on recognition rate  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_35400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="355">观察图8可知, 在AwA数据集上, 隐层数在500附近到达最佳的识别率, 同时CUB、ImageNet-2数据集的最佳识别率分别在大约900与3000时达到, 并在此之后, 均有不同程度的降低, 但差距并不明显.</p>
                </div>
                <div class="p1">
                    <p id="356">对于语义-视觉方向的投影, 同样在级联层使用不同的隐含层数量, 对比不同参数对识别效果的影响, 结果见图9.</p>
                </div>
                <div class="p1">
                    <p id="357">由图9可得, 在语义-视觉投影方向, Aw A数据集在隐层数2 000附近可得最佳识别率, CUB、ImageNet2数据集分别在1 800与2 200附近达到.综上所述, 依次使用最佳值作为模型的网络参数进行实验.</p>
                </div>
                <div class="area_img" id="358">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_35800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 语义-视觉投影下不同层数对识别率影响Fig.9 Effect of different layer numbers in semantic-vision projection on recognition rate" src="Detail/GetImg?filename=images/MSSB201903004_35800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 语义-视觉投影下不同层数对识别率影响Fig.9 Effect of different layer numbers in semantic-vision projection on recognition rate  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_35800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="359">本文主要对比的零样本方法有直接属性预测法 (Direct Attribute Prediction, DAP) 、基于不可靠样本约束的零样本学习法 (Embarrassingly Simple ZeroShot Learning, ESZSL) <citation id="415" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、联合判别相似嵌入法 (Joint Latent Similarity Embedding, JLSE) <citation id="416" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、吸引马尔科夫链方法 (Absorbing Markov-chain Process, AMP) <citation id="417" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、语义嵌入的凸线性组合法 (Convex Combination of Semantic Embeddings, ConSE) <citation id="418" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>.在小样本数据集上, ESZSL<citation id="419" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>通过将特征、属性和类别之间的关系建模为两个线性网络进行学习判别.JLSE为一种基于字典学习的联合判别嵌入框架, 通过对大量实例是否匹配的后验概率建模, 学习模型参数.</p>
                </div>
                <div class="p1">
                    <p id="360">在大型数据集上, AMP利用语义类标签图模拟嵌入空间中的语义流形, 并重新定义距离度量方式.ConSE通过类标签嵌入向量的凸线性组合将图像映射到语义嵌入空间.</p>
                </div>
                <div class="p1">
                    <p id="361">各算法在小样本数据集AwA、CUB上的实验结果如表2所示.</p>
                </div>
                <div class="area_img" id="362">
                                            <p class="img_tit">
                                                表2 各方法在小样本数据集上的识别率对比Table 2 Comparison of recognition rates of different methods on small sample datasets
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_36200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201903004_36200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_36200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 各方法在小样本数据集上的识别率对比Table 2 Comparison of recognition rates of different methods on small sample datasets" src="Detail/GetImg?filename=images/MSSB201903004_36200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="363">由表2可知, 本文算法优于现有方法.由此可以说明, 关系网络的引入进一步增强语义编码器的识别能力.本文算法在CUB数据集上的正确率提高7.9%, 在AwA数据集上也提高4.2%.</p>
                </div>
                <div class="area_img" id="364">
                                            <p class="img_tit">
                                                表3 各方法在大型数据集上的识别率对比Table 3 Comparison of recognition rates of different methods on large sample datasets%
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_36400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201903004_36400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_36400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 各方法在大型数据集上的识别率对比Table 3 Comparison of recognition rates of different methods on large sample datasets%" src="Detail/GetImg?filename=images/MSSB201903004_36400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="365">在ImageNet-2数据集上的实验结果如表3所示.由表3可知, 本文算法在ImageNet-2数据集上的识别效果也优于现有最优方法, 提高约1.5%.</p>
                </div>
                <div class="p1">
                    <p id="366">此外, 本文算法各自在编码器W与解码器W<sup>T</sup>上进行两个方向上的投影实验, 结果在表2和表3中.表2中在AwA数据集上, 视觉-属性向量的投影效果优于反向投影, 约有1.7%的优势, 但在CUB数据集上却相反.表3中在ImageNet-2数据集上实验结果表明, 使用解码器W<sup>T</sup>有0.4%的提升, 说明识别效果更好.</p>
                </div>
                <div class="p1">
                    <p id="367">因为AwA数据集的测试集类别数较少, 故仍以此为例, 并且图像特征-属性空间的投影效果优于反向投影.利用3.2节中的混淆矩阵方法, 基于本文算法的编码函数W求得的混淆矩阵如图10所示.</p>
                </div>
                <div class="area_img" id="368">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_36800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 0 本文算法识别效果的混淆矩阵Fig.10 Confusion matrix of recognition results of the proposed algorithm" src="Detail/GetImg?filename=images/MSSB201903004_36800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 0 本文算法识别效果的混淆矩阵Fig.10 Confusion matrix of recognition results of the proposed algorithm  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_36800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="369">图10中共有10个测试类, 每行都表示真实类别, 对应的每列为预测类标签的概率.由图可见:波斯猫易被错分成老鼠;熊猫、浣熊及老鼠三者依次影响识别;猪与河马相互干扰;海豹被座头鲸与河马干扰;豹、座头鲸和黑猩猩最容易识别.相应地, 结合图6的语义向量分布, 也可得到相应结论.</p>
                </div>
                <div class="p1">
                    <p id="370">最后, 针对现有零样本模型大多存在的领域漂移问题, 以AwA数据集上混淆矩阵结果中识别结果较差的猪, 结合训练集 (源域) 中的水牛对比分析, 它们都具备“has Tail”的属性, 即都有尾巴, 但是它们的尾巴属性在视觉上并不一致, 见图11 (a) .</p>
                </div>
                <div class="p1">
                    <p id="371">首先使用训练集只学习单方向的视觉-语义空间的投影函数, 进行映射迁移, 和对应的语义属性原型一起使用3.2节中的T-SNE降维进行可视化, 结果如图11 (b) 所示, 其中所有的红色标记点均为样本的语义原型点.观察可知, 在训练集上的识别 ( (b) 上方部分) 效果较好, 但在测试集的识别上 ( (b) 下方部分) 出现一定程度的领域漂移, 若结合多个测试类, 在识别上必定会造成干扰.</p>
                </div>
                <div class="p1">
                    <p id="372">再使用本文算法, 结果如图11 (c) 所示, 上下分别是以视觉-语义方向的编码, 并以反向投影作为约束的识别结果在训练集与测试集上的表现.对比发现, 带有重建约束性的自编码器对反向恢复的视觉或语义特征要求尽可能地与初始接近, 在识别结果上, 能较好克制目标域移动问题, 因此在识别上的表现更好.</p>
                </div>
                <div class="p1">
                    <p id="373">另一方面, 在得到图像与类别属性等的嵌入向量后, 多数文献方法选择使用指定的相似性度量等方式.而文本在引入特征级联后, 通过使用简单的全连接网络, 使级联后的特征以一种关系网络的形式学习非线性度量, 以灵活的函数逼近形式判别相似度.所以从数据驱动的角度出发, 学习一个更好的度量, 替代人为选择的度量方式 (如欧氏、余弦及马氏距离等) , 使网络在自我学习中最大化减少人为选定因素的干扰, 相似性度量更鲁棒.</p>
                </div>
                <div class="area_img" id="388">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_38800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 1 投影域漂移示意图Fig.11 Sketch map of projection domain shift" src="Detail/GetImg?filename=images/MSSB201903004_38800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 1 投影域漂移示意图Fig.11 Sketch map of projection domain shift  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_38800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="388">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903004_38801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 1 投影域漂移示意图Fig.11 Sketch map of projection domain shift" src="Detail/GetImg?filename=images/MSSB201903004_38801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 1 投影域漂移示意图Fig.11 Sketch map of projection domain shift  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903004_38801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="379" name="379" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="380">针对传统距离度量方式的单一性与局限性, 提出基于关系网络改进的语义自编码零样本识别算法.一方面, 使用语义自编码器建立图像和语义特征之间的映射关系, 并通过映射矩阵分别得到新的视觉和语义特征, 较好解决投影域移位的问题.另一方面, 将对应特征级联后送入神经网络训练, 从一个代表类别相似度的数值预测未知类别, 增强距离度量方法的鲁棒性.在多个数据集上的实验表明, 本文算法可以有效提高零样本识别的正确率, 并在一定程度上改善投影域移位的问题, 但在大型数据集上的识别正确率仍有待提高, 以词向量作为主要的语义信息, 其特征提取的质量直接决定模型的识别效果.因此, 今后主要研究并提高零样本模型在大型数据集上的性能表现.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="2" type="formula" href="images/MSSB201903004_00200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">林克正</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="4" type="formula" href="images/MSSB201903004_00400.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">李昊天</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="6" type="formula" href="images/MSSB201903004_00600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">白婧轩</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="8" type="formula" href="images/MSSB201903004_00800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">李骜</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="9">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061700310924&amp;v=MDA2MjNOcUk5Rlorb1BCWDQ5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZ3UWFoYz1OaWZPZmJLOEh0Zg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>SMIRNOV E A, TIMOSHENKO D M, ANDRIANOV S N.Comparison of Regularization Methods for Image Net Classification with Deep Convolutional Neural Networks.AASRI Procedia, 2014, 6:89-94.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attribute-Based Classification for Zero-Shot Visual Object Categorization">

                                <b>[2]</b>LAMPERT C H, NICKISCH H, HARMELING S.Attribute-Based Classification for Zero-Shot Visual Object Categorization.IEEETransactions on Pattern Analysis and Machine Intelligence, 2014, 36 (3) :453-465.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sharing features between objects and their attributes">

                                <b>[3]</b>HWANG S J, SHA F, GRAUMAN K.Sharing Features between Objects and Their Attributes//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2011:1761-1768.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Predicting multiple attributes via relative multi-task learning">

                                <b>[4]</b>CHEN L, ZHANG Q, LI B X.Predicting Multiple Attributes via Relative Multi-task Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2014:1027-1034.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Evaluation of output embeddings for fine-grained image classification">

                                <b>[5]</b>AKATA Z, REED S, WALTER D, et al.Evaluation of Output Embeddings for Fine-Grained Image Classification//Proc of the IEEEConference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:2927-2936.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Latent embeddings for zero-shot classification">

                                <b>[6]</b>XIAN Y Q, AKATA Z, SHARMA G, et al.Latent Embeddings for Zero-Shot Classification//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:69-77.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Predicting deep zero-shot convolutional neural networks using textual descriptions">

                                <b>[7]</b>BA J L, SWERSKY K, FIDLER S, et al.Predicting Deep ZeroShot Convolutional Neural Networks Using Textual Descriptions//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:4247-4255.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201707006&amp;v=MjQ2MzE0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpuVjcvTklUZlNkckc0SDliTXFJOUZZb1FLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>乔雪, 彭晨, 段贺, 等.基于共享特征相对属性的零样本图像分类.电子与信息学报, 2017, 39 (7) :1563-1570. (QIAO X, PENG C, DUAN H, et al.Shared Features Based Relative Attributes for Zero-Shot Image Classification.Journal of Electronics and Information Technology, 2017, 39 (7) :1563-1570.) 
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201706026&amp;v=MjQ0MDRSTE9lWmVSbkZ5em5WNy9OSVRmVGU3RzRIOWJNcVk5SFlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>程玉虎, 乔雪, 王雪松.基于混合属性的零样本图像分类.电子学报, 2017, 45 (6) :1462-1468. (CHENG Y H, QIAO X, WANG X S.Hybrid Attribute-Based ZeroShot Image Classification.Acta Electronica Sinica, 2017, 45 (6) :1462-1468.) 
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC9226795E95D84ED42F5C361E675ED1F&amp;v=MDk0MjJjemZyZmhNYnZwQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0xxN3dLMD1OaWZPZmNDeEhOUEtxSVpBRWVJS2VIUTl1bUlYNkVsNE8zemtyVw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>JI Z, YU Y L, PANG Y W, et al.Manifold Regularized Cross-Modal Embedding for Zero-Shot Learning.Information Sciences, 2017, 378:48-58.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot learning through cross-modal transfer">

                                <b>[11]</b>SOCHER R, GANJOO M, BASTANI O, et al.Zero-Shot Lear-ning through Cross-Modal Transfer//BURGES C J C, BOTTOU L, WELLING W, et al., eds.Advances in Neural Information Processing Systems 26.Cambridge, USA:The MIT Press, 2013:935-943.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transductive multi-view zero-shot learning">

                                <b>[12]</b>FU Y W, HOSPEDALES T M, XIANG T, et al.Transductive Multi-view Zero-Shot Learning.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (11) :2332-2345.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised Domain Adaptation for Zero-Shot Learning">

                                <b>[13]</b>KODIROV E, XIANG T, FU Z Y, et al.Unsupervised Domain Adaptation for Zero-Shot Learning//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2016:2452-2460.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a deep embedding model for zero-shot learning">

                                <b>[14]</b>ZHANG L, XIANG T, GONG S G.Learning a Deep Embedding Model for Zero-Shot Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:2021-2030.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic Autoencoder for Zero-Shot Learning">

                                <b>[15]</b>KODIROV E, XIANG T, GONG S G.Semantic Autoencoder for Zero-Shot Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:4447-4456.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature Generating Networks for Zero-Shot Learning">

                                <b>[16]</b>XIAN Y Q, LORENZ T, SCHIELE B, et al.Feature Generating Networks for Zero-Shot Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:5542-5551.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Glo Ve:Global Vectors for Word Representation[C/OL]">

                                <b>[17]</b>PENNINGTON J, SOCHER R, MANNING C.Glo Ve:Global Vectors for Word Representation[C/OL].[2018-07-01].https://nlp.stanford.edu/pubs/glove.pdf.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adapting Visual Category Models to New Domains">

                                <b>[18]</b>SAENKO K, KULIS B, FRITZ M, et al.Adapting Visual Category Models to New Domains//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2010:213-226.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000023944&amp;v=MTAwODFPa01CWGc5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZ3UWFoYz1OaWZJWTdLN0h0ak5yNDlGWg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b>BARTELS R H, STEWART G W.Algorithm 432:Solution of the Matrix Equation AX+XB=C.Communications of the ACM, 1972, 15 (9) :820-826.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to Compare:Relation Network for Few-Shot Learning">

                                <b>[20]</b>SUNG F, YANG Y X, ZHANG L, et al.Learning to Compare:Relation Network for Few-Shot Learning//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2018:1199-1208.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discover potential adverse drug reactions using the skip-gram model">

                                <b>[21]</b>ZHAO M Z, XU B, LIN H F, et al.Discover Potential Adverse Drug Reactions Using the Skip-Gram Model//Proc of the IEEEInternational Conference on Bioinformatics and Biomedicine.Washington, USA:IEEE, 2015:1765-1767.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_22" >
                                    <b>[22]</b>
                                VAN DER MAATEN L, HINTON G.Visualizing Data Using tSNE.Journal of Machine Learning Research, 2008, 9:2579-2605.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Embarrassingly Simple Approach to Zero-Shot Learning">

                                <b>[23]</b>ROMERA-PAREDES B, TORR P H S.An Embarrassingly Simple Approach to Zero-Shot Learning//FERIS R S, LAMPERT C, PARIKH D, eds.Visual Attributes.Berlin, Germany:Springer, 2017:11-30.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-Shot Learning via Joint Latent Similarity Embedding">

                                <b>[24]</b>ZHANG Z M, SALIGRAMA V.Zero-Shot Learning via Joint Latent Similarity Embedding//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2015:6034-6042.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-shot object recognition by semantic manifold distance">

                                <b>[25]</b>FU Z Y, XIANG T A, KODIROV E, et al.Zero-Shot Object Recognition by Semantic Manifold Distance//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:2635-2644.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Zero-Shot Learning by Convex Combination of Semantic Embeddings[C/OL]">

                                <b>[26]</b>NOROUZI M, MIKOLOV T, BENGIO S, et al.Zero-Shot Learning by Convex Combination of Semantic Embeddings[C/OL].[2018-07-01].https://arxiv.org/pdf/1312.5650.pdf.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201903004" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201903004&amp;v=MDAwMzdCdEdGckNVUkxPZVplUm5GeXpuVjcvTktEN1liTEc0SDlqTXJJOUZZSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
