<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131439409405000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201902005%26RESULT%3d1%26SIGN%3d0AGL5FlFHC6Lzrqp%252b530DwdtCDA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201902005&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201902005&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201902005&amp;v=MDMyMDVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpuVUx6SUtEN1liTEc0SDlqTXJZOUZZWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#73" data-title="1 基于远程监督的领域实体属性关系抽取的混合方法 ">1 基于远程监督的领域实体属性关系抽取的混合方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="&lt;b&gt;1.1&lt;/b&gt; 训练语料获取"><b>1.1</b> 训练语料获取</a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;1.2&lt;/b&gt; 训练语料优化"><b>1.2</b> 训练语料优化</a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;1.3&lt;/b&gt; 特征提取"><b>1.3</b> 特征提取</a></li>
                                                <li><a href="#148" data-title="&lt;b&gt;1.4&lt;/b&gt; 训练关系抽取模型"><b>1.4</b> 训练关系抽取模型</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#160" data-title="2 实验及结果分析 ">2 实验及结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#193" data-title="3 结 束 语 ">3 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#76" data-title="图1 本文方法原理框架图">图1 本文方法原理框架图</a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;表1&lt;/b&gt;&lt;b&gt;中文旅游领域知识库结构&lt;/b&gt;"><b>表1</b><b>中文旅游领域知识库结构</b></a></li>
                                                <li><a href="#96" data-title="图2 LDA主题模型层次结构">图2 LDA主题模型层次结构</a></li>
                                                <li><a href="#102" data-title="图3 关键词抽取流程图">图3 关键词抽取流程图</a></li>
                                                <li><a href="#104" data-title="图4 关键词抽取结果展示">图4 关键词抽取结果展示</a></li>
                                                <li><a href="#119" data-title="图5 先导词“景点”的同义词">图5 先导词“景点”的同义词</a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;表2&lt;/b&gt;&lt;b&gt;实体词性特征的模板&lt;/b&gt;"><b>表2</b><b>实体词性特征的模板</b></a></li>
                                                <li><a href="#138" data-title="图6 依存关系分析">图6 依存关系分析</a></li>
                                                <li><a href="#143" data-title="图7 最小完全句法树实例">图7 最小完全句法树实例</a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;表3&lt;/b&gt;&lt;b&gt;句法树特征的选择模板&lt;/b&gt;"><b>表3</b><b>句法树特征的选择模板</b></a></li>
                                                <li><a href="#166" data-title="&lt;b&gt;表4&lt;/b&gt;&lt;b&gt;去噪语料与未去噪语料的关系抽取结果对比&lt;/b&gt;"><b>表4</b><b>去噪语料与未去噪语料的关系抽取结果对比</b></a></li>
                                                <li><a href="#169" data-title="&lt;b&gt;表5&lt;/b&gt;&lt;b&gt;不同特征融合后的关系抽取效果对比&lt;/b&gt;"><b>表5</b><b>不同特征融合后的关系抽取效果对比</b></a></li>
                                                <li><a href="#173" data-title="&lt;b&gt;表6&lt;/b&gt;&lt;b&gt;Text-CNN分类器与SVM分类器的关系抽取结果 对比&lt;/b&gt;"><b>表6</b><b>Text-CNN分类器与SVM分类器的关系抽取结果 对比</b></a></li>
                                                <li><a href="#176" data-title="&lt;b&gt;表7&lt;/b&gt;&lt;b&gt;本文方法与Attention-Based LSTM模型的关系抽取 结果对比&lt;/b&gt;"><b>表7</b><b>本文方法与Attention-Based LSTM模型的关系抽取 结果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="15">


                                    <a id="bibliography_1" title=" CRAVEN M, KUMLIEN J. Constructing Biological Knowledge Bases by Extracting Information from Text Sources[C/OL]. [2018-08-25]. http://www.aaai.org/Papers/ISMB/1999/ISMB99-010.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Constructing biological knowledge bases by extracting information from text sources">
                                        <b>[1]</b>
                                         CRAVEN M, KUMLIEN J. Constructing Biological Knowledge Bases by Extracting Information from Text Sources[C/OL]. [2018-08-25]. http://www.aaai.org/Papers/ISMB/1999/ISMB99-010.pdf.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_2" title=" MINTZ M, BILLS S, SNOW R, &lt;i&gt;et al&lt;/i&gt;. Distant Supervision for Relation Extraction without Labeled Data // Proc of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Stroudsburg, USA: ACL, 2009: 1003-1011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distant supervision for relation extraction w ithout labeled data">
                                        <b>[2]</b>
                                         MINTZ M, BILLS S, SNOW R, &lt;i&gt;et al&lt;/i&gt;. Distant Supervision for Relation Extraction without Labeled Data // Proc of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Stroudsburg, USA: ACL, 2009: 1003-1011.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_3" title=" 欧阳丹彤, 瞿剑峰, 叶育鑫.关系抽取中基于本体的远监督样本扩充.软件学报, 2014, 25 (9) : 2088-2101. (OUYANG D T, QU J F, YE Y X. Extending Training Set in Distant Supervision by Ontology for Relation Extraction. Journal of Software, 2014, 25 (9) : 2088-2101.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201409015&amp;v=MTY3MjllUm5GeXpuVUx6SU55ZlRiTEc0SDlYTXBvOUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         欧阳丹彤, 瞿剑峰, 叶育鑫.关系抽取中基于本体的远监督样本扩充.软件学报, 2014, 25 (9) : 2088-2101. (OUYANG D T, QU J F, YE Y X. Extending Training Set in Distant Supervision by Ontology for Relation Extraction. Journal of Software, 2014, 25 (9) : 2088-2101.) 
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_4" title=" 贾真, 何大可, 杨燕, 等.基于弱监督学习的中文网络百科关系抽取.智能系统学报, 2015, 10 (1) : 113-119. (JIA Z, HE D K, YANG Y, &lt;i&gt;et al&lt;/i&gt;. Relation Extraction from Chinese Online Encyclopedia Based on Weakly Supervised Learning. CAAI Transactions on Intelligent Systems, 2015, 10 (1) : 113-119.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZNXT201501020&amp;v=MDc4NzNvOUhaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpuVUx6SVB5UFRlckc0SDlUTXI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         贾真, 何大可, 杨燕, 等.基于弱监督学习的中文网络百科关系抽取.智能系统学报, 2015, 10 (1) : 113-119. (JIA Z, HE D K, YANG Y, &lt;i&gt;et al&lt;/i&gt;. Relation Extraction from Chinese Online Encyclopedia Based on Weakly Supervised Learning. CAAI Transactions on Intelligent Systems, 2015, 10 (1) : 113-119.) 
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_5" title=" RIEDEL S, YAO L M, MCCALLUM A. Modeling Relations and Their Mentions without Labeled Text // Proc of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Berlin, Germany: Springer-Verlag, 2010: 148-163." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modeling Relations and Their Mentions without Labeled Text">
                                        <b>[5]</b>
                                         RIEDEL S, YAO L M, MCCALLUM A. Modeling Relations and Their Mentions without Labeled Text // Proc of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Berlin, Germany: Springer-Verlag, 2010: 148-163.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_6" title=" FAN M, ZAHO D L, ZHOU Q, &lt;i&gt;et al&lt;/i&gt;. Errata: Distant Supervision for Relation Extraction with Matrix Completion[C/OL]. [2018-08-25]. https://arxiv.org/pdf/1411.4455.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Errata: Distant Supervision for Relation Extraction with Matrix Completion[C/OL]">
                                        <b>[6]</b>
                                         FAN M, ZAHO D L, ZHOU Q, &lt;i&gt;et al&lt;/i&gt;. Errata: Distant Supervision for Relation Extraction with Matrix Completion[C/OL]. [2018-08-25]. https://arxiv.org/pdf/1411.4455.pdf.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_7" title=" TAKAMATSU S, SATO I, NAKAGAWA H. Reducing Wrong Labels in Distant Supervision for Relation Extraction // Proc of the 50th Annual Meeting of the Association for Computational Linguistics. Stroudsburg, USA: ACL, 2012: 721-729." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reducing Wrong Labels in Distant Supervision for Relation Extraction">
                                        <b>[7]</b>
                                         TAKAMATSU S, SATO I, NAKAGAWA H. Reducing Wrong Labels in Distant Supervision for Relation Extraction // Proc of the 50th Annual Meeting of the Association for Computational Linguistics. Stroudsburg, USA: ACL, 2012: 721-729.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_8" title=" QU J F, OUYANG D T, HUA W, &lt;i&gt;et al&lt;/i&gt;. Distant Supervision for Neural Relation Extraction Integrated with Word Attention and Pro-perty Features. Neural Networks, 2018, 100: 59-69." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distant supervision for neural relation extraction integrated with word attention and property features">
                                        <b>[8]</b>
                                         QU J F, OUYANG D T, HUA W, &lt;i&gt;et al&lt;/i&gt;. Distant Supervision for Neural Relation Extraction Integrated with Word Attention and Pro-perty Features. Neural Networks, 2018, 100: 59-69.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_9" title=" JI G L, LIU K, HE S Z, &lt;i&gt;et al&lt;/i&gt;. Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions // Proc of the 31st AAAI Conference on Artificial Intelligence. Palo Alto, USA: AAAI Press, 2017: 3060-3066." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions">
                                        <b>[9]</b>
                                         JI G L, LIU K, HE S Z, &lt;i&gt;et al&lt;/i&gt;. Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions // Proc of the 31st AAAI Conference on Artificial Intelligence. Palo Alto, USA: AAAI Press, 2017: 3060-3066.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_10" title=" 刘剑, 许洪波, 唐慧丰, 等.面向中文网络百科的语义知识库构建.系统仿真学报, 2016, 28 (3) : 542-548. (LIU J, XU H B, TANG H F, &lt;i&gt;et al&lt;/i&gt;. Semantic Knowledge Base Constructed from Chinese Online Encyclopedia. Journal of System Simulation, 2016, 28 (3) : 542-548.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTFZ201603006&amp;v=MDY5MjVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpuVUx6SVBUbk5kTEc0SDlmTXJJOUZZb1FLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         刘剑, 许洪波, 唐慧丰, 等.面向中文网络百科的语义知识库构建.系统仿真学报, 2016, 28 (3) : 542-548. (LIU J, XU H B, TANG H F, &lt;i&gt;et al&lt;/i&gt;. Semantic Knowledge Base Constructed from Chinese Online Encyclopedia. Journal of System Simulation, 2016, 28 (3) : 542-548.) 
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_11" title=" XU B, XU Y, LIANG J Q, &lt;i&gt;et al&lt;/i&gt;. CN-DBpedia: A Never-Ending Chinese Knowledge Extraction System // Proc of the International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems. Berlin, Germany: Springer, 2017: 428-438." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CN-DBpedia:A Never-Ending Chinese Knowledge Extraction System">
                                        <b>[11]</b>
                                         XU B, XU Y, LIANG J Q, &lt;i&gt;et al&lt;/i&gt;. CN-DBpedia: A Never-Ending Chinese Knowledge Extraction System // Proc of the International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems. Berlin, Germany: Springer, 2017: 428-438.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_12" title=" 张巧燕, 林民, 张树钧.基于维基百科的领域概念语义知识库的自动构建方法.计算机应用研究, 2018, 35 (1) : 130-134. (ZHANG Q Y, LIN M, ZHANG S J. Research on Automatic Construction of Domain Concepts on Wikipedia Semantic Knowledge Base. Application Research of Computers, 2018, 35 (1) : 130-134.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201801027&amp;v=MDQxMTJPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5em5VTHpJTHo3U1pMRzRIOW5Ncm85SFk0UUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         张巧燕, 林民, 张树钧.基于维基百科的领域概念语义知识库的自动构建方法.计算机应用研究, 2018, 35 (1) : 130-134. (ZHANG Q Y, LIN M, ZHANG S J. Research on Automatic Construction of Domain Concepts on Wikipedia Semantic Knowledge Base. Application Research of Computers, 2018, 35 (1) : 130-134.) 
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_13" title=" 王磊, 董玮, 董少林, 等.基于在线百科的知识库构建方法研究.信息系统工程, 2018 (1) : 110-111. (WANG L, DONG W, DONG S L, &lt;i&gt;et al&lt;/i&gt;. Research on the Construction Method of Knowledge Base Based on Online Encyclopedia. Information Systems Engineering, 2018 (1) : 110-111.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXXT201801074&amp;v=MTU2MjBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5em5VTHpJUFRYVGVyRzRIOW5Ncm85Q1lJUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         王磊, 董玮, 董少林, 等.基于在线百科的知识库构建方法研究.信息系统工程, 2018 (1) : 110-111. (WANG L, DONG W, DONG S L, &lt;i&gt;et al&lt;/i&gt;. Research on the Construction Method of Knowledge Base Based on Online Encyclopedia. Information Systems Engineering, 2018 (1) : 110-111.) 
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_14" title=" MIKOLOV T, CHEN K, CORRADO G, &lt;i&gt;et al&lt;/i&gt;. Efficient Estimation of Word Representations in Vector Space[C/OL]. [2018-08-25]. https://arxiv.org/pdf/1301.3781.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient estimation of word representations in vector space">
                                        <b>[14]</b>
                                         MIKOLOV T, CHEN K, CORRADO G, &lt;i&gt;et al&lt;/i&gt;. Efficient Estimation of Word Representations in Vector Space[C/OL]. [2018-08-25]. https://arxiv.org/pdf/1301.3781.pdf.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_15" title=" GOLDBERG Y, LEVY O. Word2vec Explained: Deriving Mikolov et al.′s Negative-Sampling Word-Embedding Method[C/OL]. [2018-08-25]. https://arxiv.org/pdf/1402.3722.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Word2vec Explained: Deriving Mikolov et al.′s Negative-Sampling Word-Embedding Method[C/OL]">
                                        <b>[15]</b>
                                         GOLDBERG Y, LEVY O. Word2vec Explained: Deriving Mikolov et al.′s Negative-Sampling Word-Embedding Method[C/OL]. [2018-08-25]. https://arxiv.org/pdf/1402.3722.pdf.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_16" >
                                        <b>[16]</b>
                                     BLEI D M, NG A Y, JORDAN M I. Latent Dirichlet Allocation. Journal of Machine Learning Research Archive, 2003, 3: 993-1022.</a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_17" title=" CHEN W H, ZHANG X. Research on Text Categorization Model Based on LDA-KNN // Proc of the 2nd IEEE Advanced Information Technology, Electronic and Automation Control Conference. Washington, USA: IEEE, 2017: 2719-2726." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research on Text Categorization Model Based on LDA-KNN">
                                        <b>[17]</b>
                                         CHEN W H, ZHANG X. Research on Text Categorization Model Based on LDA-KNN // Proc of the 2nd IEEE Advanced Information Technology, Electronic and Automation Control Conference. Washington, USA: IEEE, 2017: 2719-2726.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_18" title=" ZHU J R, WANG Q L, LIU Y, &lt;i&gt;et al&lt;/i&gt;. A Method of Optimizing LDA Result Purity Based on Semantic Similarity // Proc of the 32nd Youth Academic Annual Conference of Chinese Association of Automation. Washington, USA: IEEE, 2017: 361-365." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Method of Optimizing LDA Result Purity Based on Semantic Similarity">
                                        <b>[18]</b>
                                         ZHU J R, WANG Q L, LIU Y, &lt;i&gt;et al&lt;/i&gt;. A Method of Optimizing LDA Result Purity Based on Semantic Similarity // Proc of the 32nd Youth Academic Annual Conference of Chinese Association of Automation. Washington, USA: IEEE, 2017: 361-365.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_19" title=" KIM Y. Convolutional Neural Networks for Sentence Classification[C/OL]. [2018-08-25]. https://arxiv.org/pdf/1408.5882.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks for Sentence Classification[C/OL]">
                                        <b>[19]</b>
                                         KIM Y. Convolutional Neural Networks for Sentence Classification[C/OL]. [2018-08-25]. https://arxiv.org/pdf/1408.5882.pdf.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_20" title=" VAPNIK V N. The Nature of Statistical Learning Theory. New York, USA: Springer-Verlag, 1995." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Nature of Statistical Learning Theory">
                                        <b>[20]</b>
                                         VAPNIK V N. The Nature of Statistical Learning Theory. New York, USA: Springer-Verlag, 1995.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_21" title=" HOCHREITER S, SCHMIDHUBER J. Long Short-Term Memory. Neural Computation, 1997, 9 (8) : 1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=Mjk5ODZJPU5pZkpaYks5SHRqTXFvOUZaT29MRFhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGd1hhUg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         HOCHREITER S, SCHMIDHUBER J. Long Short-Term Memory. Neural Computation, 1997, 9 (8) : 1735-1780.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_22" title=" LUONG M T, PHAM H, MANNING C D. Effective Approaches to Attention Based Neural Machine Translation[C/OL]. [2018-08-25]. https://nlp.stanford.edu/pubs/emnlp15_attn.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Effective approaches to attention-based neural machine translation">
                                        <b>[22]</b>
                                         LUONG M T, PHAM H, MANNING C D. Effective Approaches to Attention Based Neural Machine Translation[C/OL]. [2018-08-25]. https://nlp.stanford.edu/pubs/emnlp15_attn.pdf.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(02),133-143 DOI:10.16451/j.cnki.issn1003-6059.201902005            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">融合多特征的基于远程监督的中文领域实体关系抽取</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%96%8C&amp;code=17389094&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王斌</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E5%89%91%E6%AF%85&amp;code=07895859&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭剑毅</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BA%BF%E5%B2%A9%E5%9B%A2&amp;code=23245672&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">线岩团</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E7%BA%A2%E6%96%8C&amp;code=21967787&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王红斌</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%99%E6%AD%A3%E6%B6%9B&amp;code=05982358&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">余正涛</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%98%86%E6%98%8E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=0242668&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">昆明理工大学信息工程与自动化学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%98%86%E6%98%8E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E6%99%BA%E8%83%BD%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">昆明理工大学智能信息处理重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对从未标记的文本中抽取中文领域实体关系的问题, 文中提出基于远程监督的领域实体属性关系抽取的混合方法, 利用知识库中已有结构化的关系三元组, 从自然语言文本中自动获取训练语料.针对远程监督方法标注数据存在大量噪声的问题, 采用隐含狄利克雷分布主题模型抽取主题关键词, 再与关系类型进行相似度计算和对关键词模式匹配进行去噪.最后提取词性特征、依存关系特征和短语句法树特征, 并进行融合, 训练关系抽取模型.实验表明, 3种特征融合的F值较高, 抽取性能较好.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%9C%E7%A8%8B%E7%9B%91%E7%9D%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">远程监督;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">实体关系抽取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E5%BA%93&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">领域知识库;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">隐含狄利克雷分布主题模型;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王斌, 硕士研究生, 主要研究方向为自然语言处理.E-mail:1105193825@qq.com.
;
                                </span>
                                <span>
                                    *郭剑毅, 硕士, 教授, 主要研究方向为模式识别、自然语言处理、信息抽取、知识获取.E-mail:giade86@hotmail.com.
;
                                </span>
                                <span>
                                    线岩团, 博士研究生, 讲师, 主要研究方向为机器翻译、信息检索、信息抽取.E-mail:yantuan.xian@gmail.com.
;
                                </span>
                                <span>
                                    王红斌, 博士研究生, 主要研究方向为智能信息系统、自然语言处理、信息检索.E-mail:wh bin2007@126.com.
;
                                </span>
                                <span>
                                    余正涛, 博士, 教授, 主要研究方向为机器翻译、自然语言处理、信息检索.E-mail:ztyu@hotmail.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.61562052, 61363044, 61462054) 资助;</span>
                    </p>
            </div>
                    <h1>Entity Relations Extraction in Chinese Domain</h1>
                    <h2>
                    <span>WANG Bin</span>
                    <span>GUO Jianyi</span>
                    <span>XIAN Yantuan</span>
                    <span>WANG Hongbin</span>
                    <span>YU Zhengtao</span>
            </h2>
                    <h2>
                    <span>Faculty of Information Engineering and Automation, Kunming University of Science and Technology</span>
                    <span>Key Laboratory of Intelligent Information Processing, Kunming University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the extraction of Chinese domain entity relationship from unlabeled text, a hybrid method of domain entity attribute extraction based on distant supervision is proposed. The structured relational three tuples in the knowledge base are applied to obtain the training corpus automatically from the natural language text. Due to the large amount of noise in the annotation data of distant supervision method, the latent Dirichlet allocation ( LDA) topic model for topic keyword extraction is adopted, and then the similarity calculation with relationship type and keyword pattern matching for denoising are performed. Finally, the part-of-speech feature, the dependency feature and the phrase syntax tree feature are extracted, and the relationship extraction model is trained. Experiments show that the method fusing three features produces higher F value and better extraction performance.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Distant%20Supervision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Distant Supervision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Entity%20Relation%20Extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Entity Relation Extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Domain%20Knowledge%20Base&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Domain Knowledge Base;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Feature%20Fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Feature Fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Latent%20Dirichlet%20Allocation%20Topic%20Model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Latent Dirichlet Allocation Topic Model;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Bin, master student. His research interests include natural language processing.
;
                                </span>
                                <span>
                                    GUO Jianyi, master, professor. Her research interests include pattern recognition, natural language processing, information extraction and knowledge acquisition.
;
                                </span>
                                <span>
                                    XIAN Yantuan, Ph. D. candidate, lecturer. His research interests include machine translation, information retrieval and information extraction.
;
                                </span>
                                <span>
                                    WANG Hongbin, Ph.D. candidate. His research interests include intelligent information system, natural language processing and information retrieval.
;
                                </span>
                                <span>
                                    YU Zhengtao, Ph. D., professor. His research interests include machine translation, natural language processing and information retrieval.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-15</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.61562052, 61363044, 61462054);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="67">关系抽取是指自动识别使用自然语言表达的两个实体之间的关联关系.关系抽取在信息抽取、自动问答系统、机器翻译和知识库构建等方面具有重要作用, 一直以来都是专家学者们致力研究的课题.</p>
                </div>
                <div class="p1">
                    <p id="68">传统实体关系抽取主流方法有基于模式匹配的方法和基于有监督的统计机器学习方法.随着关系抽取从限定关系类型转向开放领域的多种关系类型, 数据源从标准语料库转向海量的网络数据.传统的基于模式匹配的方法无法适应大量关系类型的多种表现形式, 难以人工定义所有模式.基于有监督的统计机器学习方法需要人工标注大规模的实验语料, 具有较高的准确率.然而, 由于所需的人工标注代价太大, 导致训练数据缺乏, 无法胜任海量数据背景下的关系抽取任务.所以需要研究监督最小化, 即在减少人工标注的情况下, 也能构建性能较高的关系抽取模型.而基于远程监督 (Distant Supervision) 学习的关系抽取方法可以在较少的人工干预下对训练语料进行自动获取, 受到广泛的关注.</p>
                </div>
                <div class="p1">
                    <p id="69">Craven等<citation id="229" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出远程监督, 基本思想是依赖已有的知识库, 从文本中获取包含知识库中实体对的文本作为训练语料, 运用统计文本分类的方法从学术文献的摘要中抽取蛋白质与基因之间的关系.Mintz等<citation id="232" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>假设两个实体在已知的知识库中具有关系, 所有提到这两个实体的句子都会以某种方式表达这种关系.并且通过次假设利用Freebase知识库中的实体对与从Wikipedia中获取的文本集对齐以进行大规模标注数据.然而, 根据该假设获取的标注数据包含大量噪声, 严重制约实体关系抽取性能的提高.欧阳丹彤等<citation id="231" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>基于Freebase知识库使用远程监督的方法为学习算法提供大规模样本数据, 利用本体知识库进行关系实例的扩充, 并借助本体推理增加关系实例, 最后通过对齐新增关系实例和文本集中的自然语句, 达到扩充样本的效果.贾真等<citation id="234" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>利用远程监督方法对中文网络百科进行关系抽取, 针对训练语料数量较少导致特征不足的问题, 采用基于朴素贝叶斯的句子分类器和基于自扩展的训练方法, 从未标注数据中获取更多的训练语料.Riedel等<citation id="233" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>假设已知实体对存在某种实体关系, 至少有一个包含该实体对的句子潜在表达这种实体关系, 此假设帮助其利用Freebase知识库中的实体对获取更准确的标注数据.Fan等<citation id="236" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>针对远程监督获得稀疏及有噪声的特征, 通过矩阵分解补全未知实体对的关系标签.Takamatsu等<citation id="235" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出模式相关性的方法, 用于减小远程监督生成错误标注的数量.Qu等<citation id="230" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出基于神经网络的词语注意力机制的关系抽取模型, 在降低噪声的同时也提高关系抽取的精准性.Ji等<citation id="228" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出句子层的注意力机制和实体描述, 既降低错误标签带来的噪声问题, 又获得不同句子中丰富的语义信息.</p>
                </div>
                <div class="p1">
                    <p id="70">上述基于远程监督方法的研究不管针对英文还是中文的实体关系抽取, 主要借助已有的知识库和海量网络文本进行数据标注, 获取大量的训练语料, 再通过各种方法去除噪声后进行关系抽取模型训练.对于特定领域的实体关系抽取, 不能将上述方法直接应用于特定领域.目前特定领域实体关系抽取具有如下问题:1) 对基于远程监督方法的特定领域关系抽取最大的问题在于缺乏相应的领域知识库;2) 训练语料存在大量噪声数据的问题.由于文本结构的不同, 去除噪声数据的方法也各不相同.</p>
                </div>
                <div class="p1">
                    <p id="71">在中文知识库构建方面, 刘剑等<citation id="238" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>以语义本体和超图理论为基础, 提出语义表达模型, 半自动构建通用领域中文语义知识库, 为面向语义的中文信息处理提供支持.Xu等<citation id="237" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>开发以通用百科知识沉淀为主线的大规模通用领域结构化知识库CN-DBpedia, 有助于实现机器语言认知.张巧燕等<citation id="240" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>通过维基百科中概念的层次关系, 将概念与其它概念的解释性相联系, 并将不同概念的解释性文本相联系, 建立概念语义知识库.王磊等<citation id="239" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>针对在线百科元数据结构特点, 根据自然语言处理基本步骤, 引入Web节点多路径相关性计算技术, 探索基于维基百科的知识库构建方法.上述中文知识库构建的研究大多基于通用领域知识, 可能有所涉及特定领域的知识, 但是大多停留在概念层的知识, 未涉及特定领域更细粒度的知识, 导致现有的中文知识库并不完备.</p>
                </div>
                <div class="p1">
                    <p id="72">因此, 本文在现有的通用领域知识库的基础上, 借鉴Freebase知识库的数据结构, 自行构建旅游领域知识库, 抽取旅游领域中的“景点”这个类型下的相关属性, 提出基于远程监督的领域实体属性关系抽取的混合方法.对文本中噪声数据的处理根据旅游领域文本结构特征, 采用隐含狄利克雷分布 (Latent Dirichlet Allocation, LDA) 主题模型, 抽取主题关键词后与关系类型进行相似度计算.根据相似度值的大小去除文本中的噪声数据.在此基础上, 对于已获取的上下位关系文本, 采用关键词模式匹配对齐, 并进行去噪处理.在获取较高质量的正例标注数据的同时, 把一部分噪声数据作为负例标注数据组成训练语料, 提取训练语料的词性特征、依存关系特征、短语句法树特征, 融合3种特征为一个语义信息更丰富的大特征, 进行关系抽取模型训练, 最终使用训练好的模型进行关系抽取.</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag">1 基于远程监督的领域实体属性关系抽取的混合方法</h3>
                <div class="p1">
                    <p id="74">本文方法主要思想是:首先通过已构建好的旅游领域知识库中的实体对在旅游领域文本集中获取关系实例文本集, 然后利用LDA主题模型抽取主题关键词, 通过计算关系向量与主题关键词向量的相似度, 设置阈值将相似度较大主题下的文本作为正例数据, 其余的作为负例数据.针对负例数据中存在可以作为正例数据的上下位关系文本, 采用关键词模式匹配, 二次获取负例数据中的正例数据, 使正例数据得到极大丰富.最后使用各个关系下的正例数据和少量负例数据组合成的训练语料, 提取训练语料的词性特征、依存关系特征和短语句法树特征, 使3种特征融合为一个语义信息更丰富的大特征, 进行关系抽取模型训练, 最终使用训练好的模型进行关系抽取.</p>
                </div>
                <div class="p1">
                    <p id="75">本文方法包括5个部分:训练语料获取、训练语料优化、特征提取并融合、关系抽取模型训练和关系抽取.原理框图如图1所示.</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902005_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文方法原理框架图" src="Detail/GetImg?filename=images/MSSB201902005_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文方法原理框架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902005_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Framework of the proposed method</p>

                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>1.1</b> 训练语料获取</h4>
                <h4 class="anchor-tag" id="78" name="78"><b>1.1.1</b> 构建旅游领域知识库</h4>
                <div class="p1">
                    <p id="79">本文针对旅游领域中的“景点”这个类型下的相关属性进行抽取研究, 定义4种属性关系:“地理位置”、“少数民族”、“特色产品”、 “气候类型”, 如表1所示.借鉴Freebase知识库的结构特点, 构建基于中文旅游景点的知识库, 包括4种关于景点属性的关系及每种属性关系对应的10 000个实例.</p>
                </div>
                <div class="area_img" id="80">
                    <p class="img_tit"><b>表1</b><b>中文旅游领域知识库结构</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Structure of Chinese tourism knowledge base</p>
                    <p class="img_note"></p>
                    <table id="80" border="1"><tr><td><br />关系名称</td><td>实例</td></tr><tr><td><br />地理位置</td><td>丽江古城, 丽江市</td></tr><tr><td><br />少数民族</td><td>泸沽湖, 摩梭人</td></tr><tr><td><br />特色产品</td><td>大理, 饵块</td></tr><tr><td><br />气候类型</td><td>丽江古城, 西南季风气候</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="81">例如关系实例句子“西双版纳位于云南省的南端, 属热带季风气候.”中的实例“西双版纳”和“云南省”之间的关系就是“地理位置”关系, 实例“西双版纳”和“热带季风气候”之间的关系就是“气候类型”关系.</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82"><b>1.1.2</b> 构建关系实例集</h4>
                <div class="p1">
                    <p id="83">远程监督式实体抽取中的核心假设——如果一个句子中出现具有关系的两个命名实体, 认为这个句子在描述这两个命名实体的关系.构建关系实例集就是根据自建的旅游领域知识库, 在海量文本集中找出在知识库中存在某种关系的实体对出现的句子.例如对于关系实例〈地理位置 (丽江古城, 丽江市) 〉, 从文本集中可以找到的包含“丽江古城”和“丽江市”的句子如下.</p>
                </div>
                <div class="p1">
                    <p id="84">1) [丽江古城]是[丽江市]的一处旅游景点, 又名“大研古镇”.</p>
                </div>
                <div class="p1">
                    <p id="85">2) [丽江古城]坐落在云南省[丽江市]大研镇, 玉龙雪山下丽江坝中部, 北依象山、金虹山, 西枕狮子山, 东南面临数十里的良田沃野.</p>
                </div>
                <div class="p1">
                    <p id="86">3) [丽江古城]是滇西北主要的商品集散地和手工艺品产地, 因此, 西南各地的特产在[丽江市]均可买到.</p>
                </div>
                <div class="p1">
                    <p id="87">4) [丽江古城]内的街道依山傍水修建, 铺的大多都是红色角砾岩, 雨季不会泥泞, 旱季也不会飞灰, 石上花纹图案自然雅致, 与整个古城环境相得益彰, 成为[丽江市]著名景点之一.</p>
                </div>
                <div class="p1">
                    <p id="88">上述就是用实例〈丽江古城, 丽江市〉抽取的句子, 这些句子组成一个关系实例数据集.上述4个文本都含有[丽江古城]、[丽江市]这2个实体, 可以看出1) 和2) 表达关系实例〈地理位置 (丽江古城, 丽江市) 〉属于正例数据, 而3) 和4) 属于负例数据.</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89"><b>1.2</b> 训练语料优化</h4>
                <div class="p1">
                    <p id="90">由1.1.2节中实例可以看出, 由于远程监督的假设问题, 构建的关系实例集有大量的噪声存在.因此, 本文采用LDA主题模型关键词对关系实例集进行去噪, 假设描述实体关系句子中以某个特定的关键词为核心, 例如:描述“地理位置”关系的表达可能为“处于”、“位于”、“坐落”和“属于”等;描述“特色产品”关系的可能表达为“特产”、“盛产”及“特有”等.利用LDA主题模型对关系实例集中的文本进行关键词抽取, 筛选后, 计算主题关键词与实体对在知识库中对应的预测关系的相关性, 设置阈值提取正例数据.但是, 在被归为负例数据的文本中可能包含上下位关系的句子, 例如, 关系实例“大理是云南省的一座城市.”中提取的关键词为“大理”、“云南省”和“城市”.经过LDA主题模型关键词去噪后, 这个句子会被归为负例数据, 导致训练正例数据的流失, 不利于关系抽取模型的训练.所以, 本文采用关键词模式匹配二次获取负例数据中的正例数据, 通过关键词定义一个模式对LDA主题模型关键词去噪后的负例数据进行二次去噪, 尽量提升正例数据的数量, 以便于后续关系抽取模型的训练.</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91"><b>1.2.1</b> 词向量模型训练</h4>
                <div class="p1">
                    <p id="92">在计算机中, 词的表示通常分为2种:离散表示 (One-Hot Representation) 和分布式表示 (Distribution Representation) .离散表示把每个词表示为一个长向量, 向量的维度为词表大小, 向量中只有一个维度的值为1, 其余维度为0, 这个维度表示当前的词, 导致这种表示方式不能展示词与词之间的关系.分布式表示将自然语言中的字词转换为计算机可以理解的稠密向量 (Dense Vector) , 优点在于相似的词在距离上更相近, 体现不同词之间的相关性, 反映词之间的依赖关系.</p>
                </div>
                <div class="p1">
                    <p id="93">Mikolov等<citation id="242" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出2个对数线性模型——Skip-gram和连续词袋模型 (Continuous Bag-of-Words, CBOW) , 都用于对大规模语料进行向量转化.在语义关系识别方面, Skip-gram模型效果较佳, 因此, 本文选择Google的开源工具包Word2vec<citation id="241" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 采用Skip-gram模型进行词向量模型的训练, 其中窗口大小为5, 向量维数为200.语料数据为维基百科的离线数据库中关于旅游领域的文本76 000条, 从中文百科网站和旅游网站上爬取的26 000条旅游领域文本, 总计为102 000条文本, 用于词向量模型训练.训练好的模型保存为<i>model vectors</i>.<i>bin</i>形式.本文使用知识库中的所有三元组进行关系向量模型训练, 窗口大小为5, 向量维数为200.训练好的模型保存为<i>r vectors</i>.<i>bin</i>形式.</p>
                </div>
                <h4 class="anchor-tag" id="94" name="94"><b>1.2.2</b><b>LDA</b>主题模型去噪</h4>
                <div class="p1">
                    <p id="95">LDA为Blei等<citation id="244" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出的一种对离散数据集 (如文档集) 建模的概率主题模型.基于如下前提假设:文档由若干个隐含主题构成, 而这些主题由文本中若干个特定词汇构成, 忽略文档中的句法结构和词语出现的先后顺序.Chen等<citation id="245" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出基于LDA主题模型和<i>K</i>最近邻 (<i>K</i>-Nearest Neighbor, KNN) 的语义相似度计算方法, 在文本分类中取得良好效果.Zhu等<citation id="243" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出基于语义相似的LDA主题模型的优化方法.基于LDA的思想, 可以知道LDA模型具有清晰的层次结构, 依次为文档集合层、主题层和关键词层, 结构如图2所示.</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902005_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 LDA主题模型层次结构" src="Detail/GetImg?filename=images/MSSB201902005_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 LDA主题模型层次结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902005_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Hierarchical structure of LDA topic model</p>

                </div>
                <div class="p1">
                    <p id="97">通过LDA主题模型层次结构, 可以看出每个主题下拥有若干关键词, 词语在主题上的概率分布:</p>
                </div>
                <div class="p1">
                    <p id="98"><i>P</i> (<i>k</i><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>C</mi><msub><mrow></mrow><mrow><mi>w</mi><mi>k</mi></mrow></msub><mo>+</mo><mi>β</mi></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>Κ</mi></munderover><mi>C</mi></mstyle><msub><mrow></mrow><mrow><mi>w</mi><mi>k</mi></mrow></msub><mo>+</mo><mi>Κ</mi><mi>β</mi></mrow></mfrac></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="100">其中, <i>K</i>为主题数, <i>C</i><sub><i>wk</i></sub>为词语<i>w</i>被赋予主题<i>k</i>的次数, <i>β</i>为Dirichlet参数.选择与主题概率最大的10个词作为主题关键词.</p>
                </div>
                <div class="p1">
                    <p id="101">图3为关键词抽取流程图.对文档进行预处理, 首先采用哈尔滨工业大学的语言技术平台工具包对关系实例集进行分句、分词和词性标注, 再加载停用词词典去除停用词, 最后把处理好的文档输入到LDA主题模型中, 通过概率分布计算, 得到文本关键词.</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902005_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 关键词抽取流程图" src="Detail/GetImg?filename=images/MSSB201902005_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 关键词抽取流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902005_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Keyword extraction process</p>

                </div>
                <div class="p1">
                    <p id="103">图4为对“地理位置”关系进行LDA主题模型关键词提取的部分效果展示.其中, 文字为主题关键词, 数字为该主题关键词的分布概率.由图可以看出, 大部分主题下的关键字都含有描述地理位置关系的关系描述词语“位于”.对于LDA主题模型关键词提取, 通过不断调整参数进行实验, 最终得出每种关系类型构建150个主题, 每个主题选用关键词个数<i>topNum</i>为10, 可以取得较好的实验效果.</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902005_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 关键词抽取结果展示" src="Detail/GetImg?filename=images/MSSB201902005_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 关键词抽取结果展示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902005_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Demonstration of keyword extraction result</p>

                </div>
                <div class="p1">
                    <p id="106">使用余弦相似度计算主题关键词与实体对在知识库中对应的预测关系的相关性.设同一主题下抽取的关键词集合为<i>S</i>, 抽取的关键词数量为<i>m</i>, 对于关键词集合<i>S</i>中的关键词<i>k</i>, 通过加载2.2.1节中训练好的词向量模型<i>model vectors</i>.<i>bin</i>, 得到向量的形式为<b><i>w</i></b><sub><i>k</i></sub>, <i>k</i>=1, 2, …, <i>m</i>.关系向量的获取通过加载训练好的关系向量模型<i>r vectors</i>.<i>bin</i>, 得到知识库中对应的预测关系向量形式为<b><i>r</i></b>, 预测关系向量<i>r</i>与主题关键词之间的相似性:</p>
                </div>
                <div class="p1">
                    <p id="107"><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mi>i</mi><mi>m</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">r</mi><mo>, </mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">r</mi><mo>⋅</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mo stretchy="false">|</mo><mi mathvariant="bold-italic">r</mi><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">|</mo></mrow></mfrac></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="109">本文是以主题关键词向量和关系向量的相似度表示该主题下的句子与预测关系的相关程度, 计算相似度后通过设置阈值分辨正例数据和负例数据, 如果相似度大于等于阈值, 认为该主题下的所有句子都为正例数据, 否则为负例数据.最后通过输出每个主题下的句子编号以获取正例数据.阈值 (Threshold) 的设置是通过累加所有的相似度后与主题数求平均值:</p>
                </div>
                <div class="p1">
                    <p id="110"><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mi>h</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>h</mi><mi>o</mi><mi>l</mi><mi>d</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mn>5</mn><mn>0</mn></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>s</mi></mstyle><mi>i</mi><mi>m</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">r</mi><mo>, </mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="112">本文不使用句子向量和关系向量计算相似度, 这是由于句子向量和关系向量计算的相似度较低, 难以设置一个合适的阈值以进行正例数据获取.</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113"><b>1.2.3</b> 关键词模式匹配去噪</h4>
                <div class="p1">
                    <p id="114">由1.1.2节中的例1) 可以看出, 对于关系实例〈地理位置 (丽江古城, 丽江市) 〉, 在基于LDA主题模型关键词去噪后是不可能把例1) 归为正例数据, 因为这是一个标准的上下位关系实例, 上下位关系词“是”在文本预处理时, 就作为停用词被去除.如果在基于LDA主题模型关键词去噪方法中不去除上下位关系词“是”等无用词, 一方面可能会增加无用关键词的数量, 替换掉有用的关键词, 降低关键词与实体对在知识库中对应的预测关系的相关性, 导致正例数据被误认为是负例数据.另一方面可能无法排除这些无用词, 通过相关性计算也无法使它的相似性高于阈值.所以, 对于这种上下位关系的句子, 需要使用模式匹配方法进一步处理.</p>
                </div>
                <div class="p1">
                    <p id="115">对于关系实例文本“大理是云南省的一处景点.”, 可以通过LDA主题模型获取关键词“大理”、“云南省”、“景点”.然后通过“景点”关键词确定这个句子描述的是“大理”和“云南省”存在着“地理位置”关系.</p>
                </div>
                <div class="p1">
                    <p id="116">通过对旅游领域上下位关系文本的分析, 得出能够泛化大多数上下位关系的一种模式:</p>
                </div>
                <div class="p1">
                    <p id="117"><i>Object</i> (<i>target</i>) /ns是 (有) /v* <i>target</i> (<i>Object</i>) /ns的/u*/[<i>keyword dict</i>] /n</p>
                </div>
                <div class="p1">
                    <p id="118">其中, <i>Object</i>表示关系客体, <i>target</i>表示关系主体, *表示匹配任意字符, <i>keyword dict</i>表示关键词经过同义词扩展的词典.本文采用同义词 (Synonyms) 工具包对关键词进行同义词扩展, 使用“景点”作为先导词, 不断地从先导词的同义词中选择相似度大于0.6的同义词作为下一个先导词进行同义词扩展.图5为先导词“景点”的同义词.</p>
                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902005_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 先导词“景点”的同义词" src="Detail/GetImg?filename=images/MSSB201902005_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 先导词“景点”的同义词  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902005_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Synonyms of leading word "scenic spot"</p>

                </div>
                <h4 class="anchor-tag" id="120" name="120"><b>1.3</b> 特征提取</h4>
                <div class="p1">
                    <p id="121">在基于远程监督方法抽取实体关系研究中, 英文方面的研究只是使用少量特征.由于中文与英文语法结构的不同, 另外在特定领域中抽取实体的属性关系, 特定领域有其独特的句子结构和语法特征.因此为了提高关系抽取性能, 本文选取词性特征、依存关系特征、短语句法树特征作为特征.词性特征和依存关系特征可以表达句子中词语之间的语义关系, 而短语句法树特征描述句子中概念之间的语义组合关系, 统一表达方式多样的句子表层结构, 揭示句子的深层语义结构.</p>
                </div>
                <h4 class="anchor-tag" id="122" name="122"><b>1.3.1</b> 词性特征</h4>
                <div class="p1">
                    <p id="123">如表2所示, 词性特征包括如下要素.</p>
                </div>
                <div class="p1">
                    <p id="124">1) 句子中的两个实体和实体的词性;</p>
                </div>
                <div class="p1">
                    <p id="125">2) 句子中两个实体的先后顺序;</p>
                </div>
                <div class="p1">
                    <p id="126">3) 左窗口:第1个实体左右两边<i>K</i>个词及这些词的词性;</p>
                </div>
                <div class="p1">
                    <p id="127">4) 右窗口:第2个实体左右两边<i>K</i>个词及这些词的词性.</p>
                </div>
                <div class="p1">
                    <p id="128">根据具体的实验需要, 本次采用左窗口、右窗口的大小分别为0、1、2, 即分别提取实体左右两边的词语和词语的词性.下面具体介绍抽取句子“丽江位于云南省”的特征.</p>
                </div>
                <div class="area_img" id="129">
                    <p class="img_tit"><b>表2</b><b>实体词性特征的模板</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Template of entity part-of-speech feature</p>
                    <p class="img_note"></p>
                    <table id="129" border="1"><tr><td><br />编号</td><td>模板形式</td><td>模板含义</td></tr><tr><td><br />1</td><td><i>W</i> (1) </td><td>实体1本身</td></tr><tr><td><br />2</td><td><i>W</i> (2) </td><td>实体2本身</td></tr><tr><td><br />3</td><td><i>W</i> (1) -<i>K</i></td><td>实体1左边第<i>K</i>个词</td></tr><tr><td><br />4</td><td><i>W</i> (1) +<i>K</i></td><td>实体1右边第<i>K</i>个词</td></tr><tr><td><br />5</td><td><i>W</i> (2) -<i>K</i></td><td>实体2左边第<i>K</i>个词</td></tr><tr><td><br />6</td><td><i>W</i> (2) +<i>K</i></td><td>实体2右边第<i>K</i>个词</td></tr><tr><td><br />7</td><td><i>P</i> (1) </td><td>实体1的词性</td></tr><tr><td><br />8</td><td><i>P</i> (2) </td><td>实体2的词性</td></tr><tr><td><br />9</td><td><i>P</i> (1) -<i>K</i></td><td>实体1左边第<i>K</i>个词的词性</td></tr><tr><td><br />10</td><td><i>P</i> (1) +<i>K</i></td><td>实体1右边第<i>K</i>个词的词性</td></tr><tr><td><br />11</td><td><i>P</i> (2) -<i>K</i></td><td>实体2左边第<i>K</i>个词的词性</td></tr><tr><td><br />12</td><td><i>P</i> (2) +<i>K</i></td><td>实体2右边第<i>K</i>个词的词性</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="130">经过预处理, 结果为</p>
                </div>
                <div class="p1">
                    <p id="131">“丽江ns/位于v/云南省ns/./wp”, </p>
                </div>
                <div class="p1">
                    <p id="132">该句子中“丽江”和“云南省”为2个实体.然而每个词会同时具有多个属性, 如词语本身, 还有词语的词性和该词所属实体类别等.上述例子中对于“丽江”一词的一元特征有</p>
                </div>
                <div class="p1">
                    <p id="133" class="code-formula">
                        <mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mo stretchy="false"> (</mo><mtext>文</mtext><mtext>本</mtext><mo>:</mo><mtext>丽</mtext><mtext>江</mtext><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mtext>词</mtext><mtext>性</mtext><mo>:</mo><mtext>n</mtext><mtext>s</mtext><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mtext>实</mtext><mtext>体</mtext><mtext>类</mtext><mtext>型</mtext><mo>:</mo><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext><mtext>a</mtext><mtext>t</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext><mo stretchy="false">) </mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="134">二元特征有</p>
                </div>
                <div class="p1">
                    <p id="135" class="code-formula">
                        <mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mo stretchy="false"> (</mo><mtext>文</mtext><mtext>本</mtext><mo>:</mo><mtext>丽</mtext><mtext>江</mtext><mo>_</mo><mtext>位</mtext><mtext>于</mtext><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mtext>词</mtext><mtext>性</mtext><mo>:</mo><mtext>n</mtext><mtext>s</mtext><mo>_</mo><mtext>v</mtext><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mtext>实</mtext><mtext>体</mtext><mtext>类</mtext><mtext>型</mtext><mo>:</mo><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext><mtext>a</mtext><mtext>t</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext><mo>_</mo><mtext>n</mtext><mtext>u</mtext><mtext>l</mtext><mtext>l</mtext><mo stretchy="false">) </mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="136" name="136"><b>1.3.2</b> 依存句法特征</h4>
                <div class="p1">
                    <p id="137">句法特征是使用句法解析器处理句子, 得到句子的句法依赖树.句法依赖树包含一系列词语或短语, 这些词语或短语直接由有方向的依赖关系相联系.对每个句法依赖树, 远程监督式关系抽取截取两个实体之间的一段子树.本次实验使用哈尔滨工业大学的LTP工具, 句子“丽江位于云南省.”的依存关系分析如图6所示.</p>
                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902005_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 依存关系分析" src="Detail/GetImg?filename=images/MSSB201902005_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 依存关系分析  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902005_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Dependency relation analysis</p>

                </div>
                <div class="p1">
                    <p id="139">在图6中, HED为核心, SVB为主谓关系, 主语为“丽江”, 谓语为“位于”, VOB为动宾关系, 宾语为“云南省”.选择包含实体对的最短路径, 从中提取一元特征例子, 包括</p>
                </div>
                <div class="p1">
                    <p id="140" class="code-formula">
                        <mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mo stretchy="false"> (</mo><mtext>依</mtext><mtext>存</mtext><mtext>弧</mtext><mo>∶</mo><mtext>S</mtext><mtext>B</mtext><mtext>V</mtext><mo>‚</mo><mtext>文</mtext><mtext>本</mtext><mo>∶</mo><mo>“</mo><mtext>丽</mtext><mtext>江</mtext><mo>_</mo><mtext>位</mtext><mtext>于</mtext><mo>”</mo><mo>‚</mo><mtext>词</mtext><mtext>性</mtext><mo>∶</mo><mtext>n</mtext><mtext>s</mtext><mo>_</mo><mtext>v</mtext><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mtext>依</mtext><mtext>存</mtext><mtext>弧</mtext><mo>∶</mo><mtext>V</mtext><mtext>Ο</mtext><mtext>B</mtext><mo>‚</mo><mtext>文</mtext><mtext>本</mtext><mo>∶</mo><mo>“</mo><mtext>位</mtext><mtext>于</mtext><mo>_</mo><mtext>云</mtext><mtext>南</mtext><mtext>省</mtext><mo>”</mo><mo>‚</mo><mtext>词</mtext><mtext>性</mtext><mo>∶</mo><mtext>v</mtext><mo>_</mo><mtext>n</mtext><mtext>s</mtext><mo stretchy="false">) </mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="141" name="141"><b>1.3.3</b> 短语句法树</h4>
                <div class="p1">
                    <p id="142">由于依存关系特征更多注重于每个词之间的依存关系, 而短语句法特征更多注重于多单词组块短语的依赖关系, 表达更多的语义关系, 因此提出短语句法特征.使用斯坦福 (Stanford) 的句法分析器, 将句子“西双版纳位于云南省的南端, 土地面积近2万平方公里.”进行句法分析, 形成最小完全句法树, 如图7所示.</p>
                </div>
                <div class="area_img" id="143">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902005_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 最小完全句法树实例" src="Detail/GetImg?filename=images/MSSB201902005_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 最小完全句法树实例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902005_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Instance of minimum complete syntax tree</p>

                </div>
                <div class="p1">
                    <p id="144">最小完全句法树是指两个实体的最近公共根节点作为根节点的结构树.由于最小完全句法树包含一定的上下文语义信息, 也去除一定的噪声干扰, 所以本文利用最小完全句法树进行特征提取.由于两个实体在句法树中的路径过于具体, 容易造成数据稀疏的问题, 因此为了避免这个问题, 选择2个实体路径中节点数目和2个实体根节点类型作为特征.由于句法树的结构信息十分具体, 为了解决数据稀疏带来的低召回率的问题, 采取细化句法树结构信息的方式, 如表3所示.</p>
                </div>
                <div class="area_img" id="145">
                    <p class="img_tit"><b>表3</b><b>句法树特征的选择模板</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Selection template of syntactic tree features</p>
                    <p class="img_note"></p>
                    <table id="145" border="1"><tr><td><br />句法树特征1</td><td>第一个实体到根节点的路径</td></tr><tr><td><br />句法树特征2</td><td>第二个实体到根节点的路径</td></tr><tr><td><br />句法树特征3</td><td>两个实体的公共根节点类别</td></tr><tr><td><br />句法树特征4</td><td>第一个实体到根节点的节点数目</td></tr><tr><td><br />句法树特征5</td><td>第二个实体到根节点的节点数目</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="146" name="146"><b>1.3.4</b> 特征融合</h4>
                <div class="p1">
                    <p id="147">特征融合的思想是:串行拼接每个文本中提取的词性特征、依存关系特征、短语句法树特征, 以文本的形式保存, 构成文本融合后的特征.</p>
                </div>
                <h4 class="anchor-tag" id="148" name="148"><b>1.4</b> 训练关系抽取模型</h4>
                <div class="p1">
                    <p id="149">关系抽取的核心思想是利用关系名称作为标签, 通过提取文本特征输入到分类模型中, 训练分类模型后对新的实体对进行关系抽取.传统的文本分类方法主要分为如下步骤:1) 训练及进行文本预处理, 即对文本进行分词、去除停用词等操作.2) 从处理好的文本中提取特征, 对提取特征进行文本表示.文本表示的目的是把文本在预处理后转换成计算机可理解的方式, 传统的文本表示方法常用词袋模型 (Bag of Words, BOW) 或向量空间模型 (Vector Space Model) , 其中最大的不足是忽略文本上下文关系, 每个词之间彼此独立, 无法表征语义信息.由于本文提取词性特征、依存关系特征、短语句法树特征, 这些特征大部分为词组、短句等文本, 经过特征融合后形成一个具有深层语义的大特征.运用传统的文本分类方法无法使特征得到充分利用.</p>
                </div>
                <div class="p1">
                    <p id="150">卷积神经网络文本分类器 (Convolutional Neural Networks for Text Classifier, Text-CNN) 为Kim<citation id="246" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出的基于卷积神经网络的文本分类算法, 把文本表示为词向量的形式, 经过不断迭代, 捕捉文本的局部相关性, 获取文本的深层语义信息, 实现对文本分类.由于本文提取的特征为词组、短句等文本, 所以为了可以充分利用特征, 选择Text-CNN作为分类模型, 通过输入关系实例及从该实例对应的句子集合中抽取的词性特征、依存关系特征、短语句法树特征融合的大特征, 进行分类模型的训练.特征是以向量形式进行输入, 通过加载2.2.1节中训练好的词向量模型<i>model vectors</i>.<i>bin</i>, 得到输入特征词语的向量.</p>
                </div>
                <div class="p1">
                    <p id="151">Text-CNN对融合后的特征的具体处理过程如下.</p>
                </div>
                <div class="p1">
                    <p id="152">1) 对融合后的特征进行分词和序列化.分词和序列化的主要目的是为了处理特征中存在的短句, 把分好的词和其它特征词语进行归一化处理, 最后利用列表存储得到词语序列.</p>
                </div>
                <div class="p1">
                    <p id="153">2) 将词语序列转换为以词语编号为元素的序列.</p>
                </div>
                <div class="p1">
                    <p id="154">3) 在训练时输入词语的向量.词向量是通过加载2.2.1节中训练好的词向量模型<i>model vectors</i>.<i>bin</i>, 得到输入特征词语的向量.</p>
                </div>
                <div class="p1">
                    <p id="155">4) 在经过上述处理的短句特征和其它特征中进</p>
                </div>
                <div class="p1">
                    <p id="156">行卷积, 提取不同的<i>n</i>-gram特征.</p>
                </div>
                <div class="p1">
                    <p id="157">5) 池化层使用最大池化, 对卷积后得到的若干个向量取最大值, 拼接后作为本层的输出值.</p>
                </div>
                <div class="p1">
                    <p id="158">6) 通过全连接层把高维变到低维, 同时保留有用信息.</p>
                </div>
                <div class="p1">
                    <p id="159">7) 使用softmax进行分类.</p>
                </div>
                <h3 id="160" name="160" class="anchor-tag">2 实验及结果分析</h3>
                <div class="p1">
                    <p id="161">本文使用的实验数据由两部分组成:旅游领域景点属性知识库, 从维基百科的旅游类文本、中文百科类网站和旅游类网站中获取的102 000条旅游领域文本集.知识库中的数据包含4种关系, 每种关系有10 000组实例, 从旅游网站中获取.使用远程监督的方法, 把知识库中的实体对和文本集进行实体对齐, 抽取训练语料.本次实验从旅游领域文本集中抽取经过去噪后的4种关系共计72 000条语料, 负例数据总计8 000条.其中训练集总计61 000条, 测试集总计14 000条, 验证集总计5 000条.</p>
                </div>
                <div class="p1">
                    <p id="162">实验过程包含旅游领域知识库的构建、训练语料的获取、LDA主题模型关键词语料优化、特征抽取、关系抽取模型的训练等过程, 其中训练语料的获取是使用远程监督的方法, 提取LDA主题模型关键词, Text-CNN配置的类别数<i>num</i>_<i>classes</i>为5, 卷积核个数<i>num</i>_<i>filters</i>为128, 总迭代轮次<i>num</i>_<i>epochs</i>为10.使用精确率、召回率和<i>F</i>值作为评价指标:</p>
                </div>
                <div class="p1">
                    <p id="163" class="code-formula">
                        <mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mtext>精</mtext><mtext>确</mtext><mtext>率</mtext><mo stretchy="false"> (</mo><mi>Ρ</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mtext>正</mtext><mtext>确</mtext><mtext>抽</mtext><mtext>取</mtext><mtext>的</mtext><mtext>关</mtext><mtext>系</mtext><mtext>总</mtext><mtext>数</mtext></mrow><mrow><mtext>抽</mtext><mtext>取</mtext><mtext>的</mtext><mtext>所</mtext><mtext>有</mtext><mtext>的</mtext><mtext>关</mtext><mtext>系</mtext><mtext>总</mtext><mtext>数</mtext></mrow></mfrac><mo>, </mo></mtd></mtr><mtr><mtd><mtext>召</mtext><mtext>回</mtext><mtext>率</mtext><mo stretchy="false"> (</mo><mi>R</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mtext>正</mtext><mtext>确</mtext><mtext>抽</mtext><mtext>取</mtext><mtext>的</mtext><mtext>关</mtext><mtext>系</mtext><mtext>总</mtext><mtext>数</mtext></mrow><mrow><mtext>标</mtext><mtext>准</mtext><mtext>结</mtext><mtext>果</mtext><mtext>中</mtext><mtext>的</mtext><mtext>关</mtext><mtext>系</mtext><mtext>总</mtext><mtext>数</mtext></mrow></mfrac><mo>, </mo></mtd></mtr><mtr><mtd><mi>F</mi><mo>=</mo><mfrac><mrow><mn>2</mn><mi>Ρ</mi><mi>R</mi></mrow><mrow><mi>Ρ</mi><mo>+</mo><mi>R</mi></mrow></mfrac><mspace width="0.25em" /><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="164">为了验证本文方法的有效性和全面性, 在4种关系类型的基础上增加负例数据进行评估, 设计如下4组实验.</p>
                </div>
                <div class="p1">
                    <p id="165">实验一. 验证去噪与否后的语料训练的关系抽取模型的抽取效果.对去噪后的训练语料和未去噪的训练语料进行特征提取, 在相同实验条件下分别进行关系抽取模型训练, 利用相同的测试语料对比两个模型的关系抽取效果, 结果如表4所示.</p>
                </div>
                <div class="area_img" id="166">
                    <p class="img_tit"><b>表4</b><b>去噪语料与未去噪语料的关系抽取结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Comparison of relation extraction results between denoised and undenoised corpus</p>
                    <p class="img_note"></p>
                    <table id="166" border="1"><tr><td rowspan="2"><br />关系名称</td><td colspan="3"><br />去噪语料</td><td colspan="3"><br />未去噪语料</td></tr><tr><td><br /><i>P</i></td><td><i>R</i></td><td><i>F</i></td><td><br /><i>P</i></td><td><i>R</i></td><td><i>F</i></td></tr><tr><td>地理位置</td><td>0.97</td><td>0.78</td><td>0.86</td><td>0.95</td><td>0.70</td><td>0.80</td></tr><tr><td><br />少数民族</td><td>0.91</td><td>0.87</td><td>0.89</td><td>0.87</td><td>0.84</td><td>0.85</td></tr><tr><td><br />特色产品</td><td>0.93</td><td>0.96</td><td>0.94</td><td>0.73</td><td>0.87</td><td>0.80</td></tr><tr><td><br />气候类型</td><td>0.87</td><td>0.97</td><td>0.92</td><td>0.79</td><td>0.81</td><td>0.80</td></tr><tr><td><br />负例数据</td><td>0.96</td><td>0.95</td><td>0.95</td><td>0.76</td><td>0.87</td><td>0.81</td></tr><tr><td><br />所有</td><td>0.93</td><td>0.90</td><td>0.91</td><td>0.82</td><td>0.82</td><td>0.81</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="167">由表4可以看出, 本文方法取得一定效果, 去噪后的训练语料更纯粹, 提取到的有效特征更具代表性, 所以训练的关系抽取模型抽取效果更好.而未去噪的语料提取的特征质量较差, 代表性不足, 含有很多无关紧要的信息, 所以训练的关系抽取模型抽取效果较差.</p>
                </div>
                <div class="p1">
                    <p id="168">实验二.验证本文方法优于使用单一特征、两种特征随机融合后的关系抽取效果.单一特征分别是词性特征、依存关系特征、句法树特征, 利用这三种单一特征在同等实验条件下分别训练关系抽取模型, 然后对比文中方法和使用这三种单一特征训练的模型的关系抽取效果.两种特征随机融合的组合有[词性特征, 依存句法特征], [词性特征, 短语句法树特征], [依存句法特征, 短语句法树特征]3种, 在相同的实验条件下使用这三组融合特征训练关系抽取模型, 再对比本文方法和使用这三组融合特征训练的模型的关系抽取效果.对于三组融合特征的抽取效果, 取效果最优的[依存句法特征, 短语句法树特征]的实验结果与本文方法进行对比, 结果如表5所示.</p>
                </div>
                <div class="area_img" id="169">
                    <p class="img_tit"><b>表5</b><b>不同特征融合后的关系抽取效果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Comparison of relation extraction results with fusion of different features</p>
                    <p class="img_note"></p>
                    <table id="169" border="1"><tr><td rowspan="2"><br />关系名称</td><td colspan="3"><br />本文方法</td><td colspan="3"><br />词性特征</td><td colspan="3"><br />依存句法特征</td><td colspan="3"><br />短语句法树特征</td><td colspan="3"><br />两种特征融合最优效果</td></tr><tr><td><br /><i>P</i></td><td><i>R</i></td><td><i>F</i></td><td><br /><i>P</i></td><td><i>R</i></td><td><i>F</i></td><td><br /><i>P</i></td><td><i>R</i></td><td><i>F</i></td><td><br /><i>P</i></td><td><i>R</i></td><td><i>F</i></td><td><br /><i>P</i></td><td><i>R</i></td><td><i>F</i></td></tr><tr><td>地理位置</td><td>0.97</td><td>0.78</td><td>0.86</td><td>0.78</td><td>0.76</td><td>0.77</td><td>0.82</td><td>0.71</td><td>0.76</td><td>0.84</td><td>0.73</td><td>0.78</td><td>0.95</td><td>0.77</td><td>0.85</td></tr><tr><td><br />少数民族</td><td>0.91</td><td>0.86</td><td>0.89</td><td>0.85</td><td>0.81</td><td>0.82</td><td>0.87</td><td>0.84</td><td>0.85</td><td>0.90</td><td>0.81</td><td>0.85</td><td>0.89</td><td>0.85</td><td>0.87</td></tr><tr><td><br />特色产品</td><td>0.93</td><td>0.96</td><td>0.94</td><td>0.87</td><td>0.80</td><td>0.83</td><td>0.84</td><td>0.85</td><td>0.84</td><td>0.89</td><td>0.85</td><td>0.87</td><td>0.88</td><td>0.94</td><td>0.91</td></tr><tr><td><br />气候类型</td><td>0.87</td><td>0.97</td><td>0.92</td><td>0.84</td><td>0.88</td><td>0.86</td><td>0.83</td><td>0.85</td><td>0.84</td><td>0.90</td><td>0.83</td><td>0.86</td><td>0.84</td><td>0.93</td><td>0.88</td></tr><tr><td><br />负例数据</td><td>0.96</td><td>0.95</td><td>0.95</td><td>0.81</td><td>0.84</td><td>0.82</td><td>0.84</td><td>0.86</td><td>0.85</td><td>0.88</td><td>0.85</td><td>0.86</td><td>0.90</td><td>0.88</td><td>0.89</td></tr><tr><td><br />所有</td><td>0.93</td><td>0.90</td><td>0.91</td><td>0.83</td><td>0.82</td><td>0.82</td><td>0.84</td><td>0.82</td><td>0.83</td><td>0.88</td><td>0.81</td><td>0.84</td><td>0.90</td><td>0.87</td><td>0.88</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="170">由表5可以看出, 不论是使用单一特征, 还是使用两种特征融合, 取得的抽取效果都不如本文使用三种特征融合后的抽取效果.当然, 两种特征融合后的抽取效果优于只使用单一特征的抽取效果.从上述单一特征的数据中还可以看出, 特征越复杂, 本文使用的Text-CNN分类器训练的模型抽取结果越优.由此可知, 单个词语作为句子特征存在表示性不强的问题, 复杂特征更能表示句子的语义信息.</p>
                </div>
                <div class="p1">
                    <p id="171">实验三.为了验证本文方法的有效性, 在使用相同融合特征的情况下, 验证使用Text-CNN分类器的分类效果优于使用支持向量机 (SVM) 分类器的分类效果.实验结果如表6所示.</p>
                </div>
                <div class="p1">
                    <p id="172">SVM是由Vapnik<citation id="247" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>提出的机器学习方法, 原理是将低维空间中的点映射到高维空间中, 使它们成为线性可分, 再使用线性划分的原理判断分类边界.在高维空间中, 它是一种线性划分, 而在原有的数据空间中, 它是一种非线性划分.本文把特征数据映射到高维空间进行分类操作.这种分类器具有结构简单、训练误差较低和泛化能力较好等优点, 广泛应用于分类学习问题中.</p>
                </div>
                <div class="area_img" id="173">
                    <p class="img_tit"><b>表6</b><b>Text-CNN分类器与SVM分类器的关系抽取结果 对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 6 Comparison of relation extraction results by Text- CNN classifier and SVM classifier</p>
                    <p class="img_note"></p>
                    <table id="173" border="1"><tr><td rowspan="2"><br />关系名称</td><td colspan="3"><br />Text-CNN分类器</td><td colspan="3"><br />SVM分类器</td></tr><tr><td><br /><i>P</i></td><td><i>R</i></td><td><i>F</i></td><td><br /><i>P</i></td><td><i>R</i></td><td><i>F</i></td></tr><tr><td>地理位置</td><td>0.97</td><td>0.78</td><td>0.86</td><td>0.96</td><td>0.77</td><td>0.85</td></tr><tr><td><br />少数民族</td><td>0.91</td><td>0.86</td><td>0.89</td><td>0.89</td><td>0.87</td><td>0.88</td></tr><tr><td><br />特色产品</td><td>0.93</td><td>0.96</td><td>0.94</td><td>0.90</td><td>0.92</td><td>0.91</td></tr><tr><td><br />气候类型</td><td>0.87</td><td>0.97</td><td>0.92</td><td>0.89</td><td>0.90</td><td>0.90</td></tr><tr><td><br />负例数据</td><td>0.96</td><td>0.95</td><td>0.95</td><td>0.92</td><td>0.93</td><td>0.92</td></tr><tr><td><br />所有</td><td>0.93</td><td>0.90</td><td>0.91</td><td>0.91</td><td>0.88</td><td>0.89</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="174">由表6可以看出, Text-CNN分类器的效果略优于SVM分类器.出现这种差别的原因主要是由于Text-CNN分类器在融合特征的处理方面优于SVM分类器, Text-CNN分类器在处理短句特征时, 可以有效解析短句特征的语义, 提取其中的关键信息, 用于模型训练.而SVM分类器训练模型时, 还需要对短句特征进行分词、关键信息提取等预处理.而处理过后的短句特征和已经提取的词性特征、依存句法特征大同小异, 不能较好体现特征融合后的优势.</p>
                </div>
                <div class="p1">
                    <p id="175">实验四.为了验证本文方法的有效性, 设计在相同语料下使用基于注意力机制的长短期记忆网络 (Attention-Based LSTM) 进行实验.Attention-Based LSTM的参数设置为隐藏层节点数为256, 层数为1, 学习率为0.01, <i>batch</i>_<i>size</i>为20, 优化方法采用随机梯度下降法.对比结果如表7所示.</p>
                </div>
                <div class="area_img" id="176">
                    <p class="img_tit"><b>表7</b><b>本文方法与Attention-Based LSTM模型的关系抽取 结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 7 Comparison of relationship extraction results between the proposed method and Attention-Based LSTM model</p>
                    <p class="img_note"></p>
                    <table id="176" border="1"><tr><td rowspan="2"><br />关系名称</td><td colspan="3"><br />本文方法</td><td colspan="3"><br />Attention-Based LSTM模型</td></tr><tr><td><br /><i>P</i></td><td><i>R</i></td><td><i>F</i></td><td><br /><i>P</i></td><td><i>R</i></td><td><i>F</i></td></tr><tr><td>地理位置</td><td>0.97</td><td>0.78</td><td>0.86</td><td>0.96</td><td>0.80</td><td>0.87</td></tr><tr><td><br />少数民族</td><td>0.91</td><td>0.86</td><td>0.89</td><td>0.90</td><td>0.86</td><td>0.88</td></tr><tr><td><br />特色产品</td><td>0.93</td><td>0.96</td><td>0.94</td><td>0.90</td><td>0.95</td><td>0.92</td></tr><tr><td><br />气候类型</td><td>0.87</td><td>0.97</td><td>0.92</td><td>0.88</td><td>0.92</td><td>0.90</td></tr><tr><td><br />负例数据</td><td>0.96</td><td>0.95</td><td>0.95</td><td>0.95</td><td>0.93</td><td>0.93</td></tr><tr><td><br />所有</td><td>0.93</td><td>0.90</td><td>0.91</td><td>0.92</td><td>0.89</td><td>0.90</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="177">长短期记忆网络 (Long Short Term Memory, LSTM) 是由Hochreiter等<citation id="248" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提出的循环神经网络 (Recurrent Neural Network, RNN) 的改进模型, 通过构建专门的记忆单元存储历史信息, 使每个时间状态都保存前面的输入信息, 其中包含遗忘门、输入门和输出门计算单元, 可以有效缓解RNN和CNN的长距离依赖问题, 对于句子的语义解析具有较好效果.</p>
                </div>
                <div class="p1">
                    <p id="178">注意力机制 (Attention) 是由Luong等<citation id="249" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>提出的模拟人脑注意力机制的模型, 本质是用于度量输入与输出相似性, 输入与输出越相似, 认为输入的重要程度越高.注意力机制的引入是为了计算注意力概率, 体现融合后的特征集合中每个特征词语对于特征集合的重要程度.假设融合后的特征集合<b><i>F</i></b>={<b><i>f</i></b><sub>1</sub>, <b><i>f</i></b><sub>2</sub>, …, <b><i>f</i></b><sub><i>n</i></sub>}, 集合<i>F</i>中的每个特征词语<b><i>f</i></b><sub><i>i</i></sub>都包含该句子是否表达关系<b><i>r</i></b>的信息.首先使用余弦相似度计算每个特征词语向量<b><i>f</i></b><sub><i>i</i></sub>与关系向量<b><i>r</i></b>的相似度:</p>
                </div>
                <div class="p1">
                    <p id="179"><mathml id="180"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>*</mo><mi mathvariant="bold-italic">r</mi></mrow><mrow><mo stretchy="false">|</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">r</mi><mo stretchy="false">|</mo></mrow></mfrac></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="181">其中, 特征词向量和关系向量通过分别加载2.2.1节中的<i>model vectors</i>.<i>bin</i>模型和<i>r vectors</i>.<i>bin</i>模型得到.</p>
                </div>
                <div class="p1">
                    <p id="182">然后通过softmax函数进行归一化, 使所有的权重值相加后的值为1, 得到该特征词在特征集合中的权重</p>
                </div>
                <div class="p1">
                    <p id="183"><mathml id="184"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="185">其中<i>k</i>表示特征集合的词语个数.</p>
                </div>
                <div class="p1">
                    <p id="186">再使用最大池化 (Maxpool) 策略得到权重值最大的词语作为一个新的特征序列:</p>
                </div>
                <div class="p1">
                    <p id="187"><i>S</i>=<i>Maxpool</i> (<i>β</i>) , </p>
                </div>
                <div class="p1">
                    <p id="188">其中, <i>β</i>表示权重值的集合, <i>S</i>为新的特征序列.</p>
                </div>
                <div class="p1">
                    <p id="189">最终利用新的特征序列组成的特征集对softmax分类器进行关系抽取模型的训练.</p>
                </div>
                <div class="p1">
                    <p id="190">本次实验使用LSTM网络模型对融合后的特征进行解析, 通过模型中的门计算对特征中存在的短句特征进行处理, 去除短句特征中无关紧要的字词, 然后输出余下的词语与其它特征词语组成特征集, 引入注意力机制优化特征集中的特征词语, 获取最具代表性的特征词语组成新的特征集.最后使用新的特征集训练Softmax分类器, 得到最终的关系抽取模型.</p>
                </div>
                <div class="p1">
                    <p id="191">从表7可以看出, 本文方法的效果优于Atten-tion-Based LSTM, 出现这种情况的原因在于本文使用已处理的特征作为输入, 而Attention-Based LSTM模型注重一个完整句子的时序性、句子中各个词语之间的联系、句子的整体语义表达等因素, 所以处理单独的词语序列特征的效果差于本文方法.</p>
                </div>
                <div class="p1">
                    <p id="192">总体上看, 对于“地理位置”和“少数民族”关系类型的抽取效果并不佳, 主要原因在于本文使用的语料中存在含有民族信息的地名, 如“红河哈尼族彝族自治州”这个地名, 对于分类器来说, 它既有可能分到“地理位置”, 也有可能分到“少数民族”.这个问题也是今后所要从事的工作.</p>
                </div>
                <h3 id="193" name="193" class="anchor-tag">3 结 束 语</h3>
                <div class="p1">
                    <p id="194">根据特定领域文本特点提出本文方法, 并非已有方法的简单组合.针对现有的远程监督关系抽取方法无法契合领域实体关系抽取, 提出融合多特征的基于远程监督的中文领域实体关系抽取方法, 使用远程监督方法获取大量训练语料.针对训练语料中大量的噪声数据, 运用LDA主题模型抽取主题关键词后与关系类型进行相似度计算和关键词模式匹配以实现去噪.对于特征提取问题, 从正例数据和少量负例数据中提取多种特征, 融合成为一个更丰富的大特征, 用于训练关系抽取模型.实验表明, 去噪后的数据在模型训练方面拥有更好优势, 而且使用三种特征融合进行实验取得的抽取效果明显优于使用两种特征组合的实验结果.由于在“地理位置”和“少数民族”这两个关系类型的分类上存在一些问题, 所有接下来的工作就是在解决这个问题的基础上, 扩展旅游领域的关系类型, 然后再更深入研究关系抽取.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="15">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Constructing biological knowledge bases by extracting information from text sources">

                                <b>[1]</b> CRAVEN M, KUMLIEN J. Constructing Biological Knowledge Bases by Extracting Information from Text Sources[C/OL]. [2018-08-25]. http://www.aaai.org/Papers/ISMB/1999/ISMB99-010.pdf.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distant supervision for relation extraction w ithout labeled data">

                                <b>[2]</b> MINTZ M, BILLS S, SNOW R, <i>et al</i>. Distant Supervision for Relation Extraction without Labeled Data // Proc of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Stroudsburg, USA: ACL, 2009: 1003-1011.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201409015&amp;v=MDk5ODdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6blVMeklOeWZUYkxHNEg5WE1wbzlFWVlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 欧阳丹彤, 瞿剑峰, 叶育鑫.关系抽取中基于本体的远监督样本扩充.软件学报, 2014, 25 (9) : 2088-2101. (OUYANG D T, QU J F, YE Y X. Extending Training Set in Distant Supervision by Ontology for Relation Extraction. Journal of Software, 2014, 25 (9) : 2088-2101.) 
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZNXT201501020&amp;v=MzIwMjllUm5GeXpuVUx6SVB5UFRlckc0SDlUTXJvOUhaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 贾真, 何大可, 杨燕, 等.基于弱监督学习的中文网络百科关系抽取.智能系统学报, 2015, 10 (1) : 113-119. (JIA Z, HE D K, YANG Y, <i>et al</i>. Relation Extraction from Chinese Online Encyclopedia Based on Weakly Supervised Learning. CAAI Transactions on Intelligent Systems, 2015, 10 (1) : 113-119.) 
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modeling Relations and Their Mentions without Labeled Text">

                                <b>[5]</b> RIEDEL S, YAO L M, MCCALLUM A. Modeling Relations and Their Mentions without Labeled Text // Proc of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Berlin, Germany: Springer-Verlag, 2010: 148-163.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Errata: Distant Supervision for Relation Extraction with Matrix Completion[C/OL]">

                                <b>[6]</b> FAN M, ZAHO D L, ZHOU Q, <i>et al</i>. Errata: Distant Supervision for Relation Extraction with Matrix Completion[C/OL]. [2018-08-25]. https://arxiv.org/pdf/1411.4455.pdf.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reducing Wrong Labels in Distant Supervision for Relation Extraction">

                                <b>[7]</b> TAKAMATSU S, SATO I, NAKAGAWA H. Reducing Wrong Labels in Distant Supervision for Relation Extraction // Proc of the 50th Annual Meeting of the Association for Computational Linguistics. Stroudsburg, USA: ACL, 2012: 721-729.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distant supervision for neural relation extraction integrated with word attention and property features">

                                <b>[8]</b> QU J F, OUYANG D T, HUA W, <i>et al</i>. Distant Supervision for Neural Relation Extraction Integrated with Word Attention and Pro-perty Features. Neural Networks, 2018, 100: 59-69.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions">

                                <b>[9]</b> JI G L, LIU K, HE S Z, <i>et al</i>. Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions // Proc of the 31st AAAI Conference on Artificial Intelligence. Palo Alto, USA: AAAI Press, 2017: 3060-3066.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTFZ201603006&amp;v=MDIxMjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6blVMeklQVG5OZExHNEg5Zk1ySTlGWW9RS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 刘剑, 许洪波, 唐慧丰, 等.面向中文网络百科的语义知识库构建.系统仿真学报, 2016, 28 (3) : 542-548. (LIU J, XU H B, TANG H F, <i>et al</i>. Semantic Knowledge Base Constructed from Chinese Online Encyclopedia. Journal of System Simulation, 2016, 28 (3) : 542-548.) 
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CN-DBpedia:A Never-Ending Chinese Knowledge Extraction System">

                                <b>[11]</b> XU B, XU Y, LIANG J Q, <i>et al</i>. CN-DBpedia: A Never-Ending Chinese Knowledge Extraction System // Proc of the International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems. Berlin, Germany: Springer, 2017: 428-438.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201801027&amp;v=MDMxMzVIWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6blVMeklMejdTWkxHNEg5bk1ybzk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 张巧燕, 林民, 张树钧.基于维基百科的领域概念语义知识库的自动构建方法.计算机应用研究, 2018, 35 (1) : 130-134. (ZHANG Q Y, LIN M, ZHANG S J. Research on Automatic Construction of Domain Concepts on Wikipedia Semantic Knowledge Base. Application Research of Computers, 2018, 35 (1) : 130-134.) 
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXXT201801074&amp;v=MjYxMzR6cXFCdEdGckNVUkxPZVplUm5GeXpuVUx6SVBUWFRlckc0SDluTXJvOUNZSVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 王磊, 董玮, 董少林, 等.基于在线百科的知识库构建方法研究.信息系统工程, 2018 (1) : 110-111. (WANG L, DONG W, DONG S L, <i>et al</i>. Research on the Construction Method of Knowledge Base Based on Online Encyclopedia. Information Systems Engineering, 2018 (1) : 110-111.) 
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient estimation of word representations in vector space">

                                <b>[14]</b> MIKOLOV T, CHEN K, CORRADO G, <i>et al</i>. Efficient Estimation of Word Representations in Vector Space[C/OL]. [2018-08-25]. https://arxiv.org/pdf/1301.3781.pdf.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Word2vec Explained: Deriving Mikolov et al.′s Negative-Sampling Word-Embedding Method[C/OL]">

                                <b>[15]</b> GOLDBERG Y, LEVY O. Word2vec Explained: Deriving Mikolov et al.′s Negative-Sampling Word-Embedding Method[C/OL]. [2018-08-25]. https://arxiv.org/pdf/1402.3722.pdf.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_16" >
                                    <b>[16]</b>
                                 BLEI D M, NG A Y, JORDAN M I. Latent Dirichlet Allocation. Journal of Machine Learning Research Archive, 2003, 3: 993-1022.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research on Text Categorization Model Based on LDA-KNN">

                                <b>[17]</b> CHEN W H, ZHANG X. Research on Text Categorization Model Based on LDA-KNN // Proc of the 2nd IEEE Advanced Information Technology, Electronic and Automation Control Conference. Washington, USA: IEEE, 2017: 2719-2726.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Method of Optimizing LDA Result Purity Based on Semantic Similarity">

                                <b>[18]</b> ZHU J R, WANG Q L, LIU Y, <i>et al</i>. A Method of Optimizing LDA Result Purity Based on Semantic Similarity // Proc of the 32nd Youth Academic Annual Conference of Chinese Association of Automation. Washington, USA: IEEE, 2017: 361-365.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks for Sentence Classification[C/OL]">

                                <b>[19]</b> KIM Y. Convolutional Neural Networks for Sentence Classification[C/OL]. [2018-08-25]. https://arxiv.org/pdf/1408.5882.pdf.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Nature of Statistical Learning Theory">

                                <b>[20]</b> VAPNIK V N. The Nature of Statistical Learning Theory. New York, USA: Springer-Verlag, 1995.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MjU4MzhJPU5pZkpaYks5SHRqTXFvOUZaT29MRFhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGd1hhUg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> HOCHREITER S, SCHMIDHUBER J. Long Short-Term Memory. Neural Computation, 1997, 9 (8) : 1735-1780.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Effective approaches to attention-based neural machine translation">

                                <b>[22]</b> LUONG M T, PHAM H, MANNING C D. Effective Approaches to Attention Based Neural Machine Translation[C/OL]. [2018-08-25]. https://nlp.stanford.edu/pubs/emnlp15_attn.pdf.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201902005" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201902005&amp;v=MDMyMDVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpuVUx6SUtEN1liTEc0SDlqTXJZOUZZWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
