<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131439517530000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201902010%26RESULT%3d1%26SIGN%3dytcPn7xQJ7Uv9KMwi0iTTCz7B%252fo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201902010&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201902010&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201902010&amp;v=MDY0NjZPZVplUm5GeXpuVUwzTEtEN1liTEc0SDlqTXJZOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#56" data-title="1 基于CNN-LSTM的微生物生长环境关系抽取 ">1 基于CNN-LSTM的微生物生长环境关系抽取</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="&lt;b&gt;1.1&lt;/b&gt; 研究方案"><b>1.1</b> 研究方案</a></li>
                                                <li><a href="#69" data-title="&lt;b&gt;1.2&lt;/b&gt; 语料预处理"><b>1.2</b> 语料预处理</a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;1.3&lt;/b&gt; 分布式文本表示"><b>1.3</b> 分布式文本表示</a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;1.4&lt;/b&gt; 位置特征选择"><b>1.4</b> 位置特征选择</a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;1.5&lt;/b&gt; 建立&lt;b&gt;CNN-LSTM&lt;/b&gt;模型"><b>1.5</b> 建立<b>CNN-LSTM</b>模型</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#125" data-title="2 实验及结果分析 ">2 实验及结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#138" data-title="3 结 束 语 ">3 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="图1 关系抽取示例">图1 关系抽取示例</a></li>
                                                <li><a href="#61" data-title="&lt;b&gt;表1&lt;/b&gt;&lt;b&gt;关系抽取结果&lt;/b&gt;"><b>表1</b><b>关系抽取结果</b></a></li>
                                                <li><a href="#68" data-title="图2 本文方法流程图">图2 本文方法流程图</a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;表2&lt;/b&gt;&lt;b&gt;数据集统计&lt;/b&gt;"><b>表2</b><b>数据集统计</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;表3&lt;/b&gt;&lt;b&gt;文档具体数量统计&lt;/b&gt;"><b>表3</b><b>文档具体数量统计</b></a></li>
                                                <li><a href="#103" data-title="图3 分布式词向量的表示方法">图3 分布式词向量的表示方法</a></li>
                                                <li><a href="#107" data-title="图4 位置特征构建示例">图4 位置特征构建示例</a></li>
                                                <li><a href="#124" data-title="图5 CNN-LSTM模型结构">图5 CNN-LSTM模型结构</a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;表4&lt;/b&gt;&lt;b&gt;3种模型的实验结果对比&lt;/b&gt;"><b>表4</b><b>3种模型的实验结果对比</b></a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;表5&lt;/b&gt;&lt;b&gt;4种方法的实验结果对比&lt;/b&gt;"><b>表5</b><b>4种方法的实验结果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="11">


                                    <a id="bibliography_1" title=" N&#201;DELLEC C, BOSSY R, KIM J D, &lt;i&gt;et al&lt;/i&gt;. Overview of BioNLP Shared Task 2013 // Proc of the BioNLP Shared Task 2013 Workshop. Berlin, Germany: Springer, 2013: 1-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Overview of BioNLP shared task 2013">
                                        <b>[1]</b>
                                         N&#201;DELLEC C, BOSSY R, KIM J D, &lt;i&gt;et al&lt;/i&gt;. Overview of BioNLP Shared Task 2013 // Proc of the BioNLP Shared Task 2013 Workshop. Berlin, Germany: Springer, 2013: 1-7.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_2" title=" 王健, 李虹磊, 林鸿飞, 等.基于神经网络的微生物生长环境关系抽取方法.华南理工大学学报 (自然科学版) , 2017, 45 (3) :76-81. (WANG J, LI H L, LIN H F, &lt;i&gt;et al&lt;/i&gt;. Bacteria Biotope Extraction on the Basis of Neural Network. Journal of South China University of Technology (Natural Science Edition) , 2017, 45 (3) : 76-81.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HNLG201703011&amp;v=MjExMzQ5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5em5VTDNMTFNQSGFiRzRIOWJNckk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         王健, 李虹磊, 林鸿飞, 等.基于神经网络的微生物生长环境关系抽取方法.华南理工大学学报 (自然科学版) , 2017, 45 (3) :76-81. (WANG J, LI H L, LIN H F, &lt;i&gt;et al&lt;/i&gt;. Bacteria Biotope Extraction on the Basis of Neural Network. Journal of South China University of Technology (Natural Science Edition) , 2017, 45 (3) : 76-81.) 
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_3" title=" BJ&#214;RNE J, HEIMONEN J, GINTER F, &lt;i&gt;et al&lt;/i&gt;. Extracting Complex Biological Events with Rich Graph-Based Feature Sets // Proc of the Workshop on BioNLP: Shared Task. Berlin, Germany: Springer, 2009: 10-18." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extracting Complex Biological Events with Rich Graph-based Feature Sets">
                                        <b>[3]</b>
                                         BJ&#214;RNE J, HEIMONEN J, GINTER F, &lt;i&gt;et al&lt;/i&gt;. Extracting Complex Biological Events with Rich Graph-Based Feature Sets // Proc of the Workshop on BioNLP: Shared Task. Berlin, Germany: Springer, 2009: 10-18.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_4" title=" RATKOVIC Z, GOLIK W, WARNIER P, &lt;i&gt;et al&lt;/i&gt;. BioNLP 2011 Task Bacteria Biotope: The Alvis System // Proc of the BioNLP Shared Task 2011 Workshop. Stroudsburg, USA: ACL, 2011: 102-111." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bio NLP2011 task bacteria biotope:the Alvis system">
                                        <b>[4]</b>
                                         RATKOVIC Z, GOLIK W, WARNIER P, &lt;i&gt;et al&lt;/i&gt;. BioNLP 2011 Task Bacteria Biotope: The Alvis System // Proc of the BioNLP Shared Task 2011 Workshop. Stroudsburg, USA: ACL, 2011: 102-111.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_5" title=" KARADENIZ I, &#214;ZG&#220;R A. Bacteria Biotope Detection, Ontology-Based Normalization, and Relation Extraction Using Syntactic Rules // Proc of the BioNLP Shared Task 2013 Workshop. Berlin, Germany: Springer, 2013: 170-177." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bacteria biotope detection,ontology-based normalization,and relation extraction using syntactic rules">
                                        <b>[5]</b>
                                         KARADENIZ I, &#214;ZG&#220;R A. Bacteria Biotope Detection, Ontology-Based Normalization, and Relation Extraction Using Syntactic Rules // Proc of the BioNLP Shared Task 2013 Workshop. Berlin, Germany: Springer, 2013: 170-177.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_6" title=" LI L S, LIU S S, QIN M Y, &lt;i&gt;et al&lt;/i&gt;. Extracting Biomedical Event with Dual Decomposition Integrating Word Embeddings. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2016, 13 (4) : 669-677." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extracting biomedical event with dual decomposition integrating word embeddings">
                                        <b>[6]</b>
                                         LI L S, LIU S S, QIN M Y, &lt;i&gt;et al&lt;/i&gt;. Extracting Biomedical Event with Dual Decomposition Integrating Word Embeddings. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2016, 13 (4) : 669-677.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_7" title=" LEVER J, JONES S J. VERSE: Event and Relation Extraction in the BioNLP 2016 Shared Task // Proc of the 4th BioNLP Shared Task Workshop. Berlin, Germany: Springer, 2016: 42-49." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=VERSE: Event and Relation Extraction in the BioNLP 2016 Shared Task">
                                        <b>[7]</b>
                                         LEVER J, JONES S J. VERSE: Event and Relation Extraction in the BioNLP 2016 Shared Task // Proc of the 4th BioNLP Shared Task Workshop. Berlin, Germany: Springer, 2016: 42-49.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_8" title=" LECUN Y, BENGIO Y, HINTON G. Deep Learning. Nature, 2015, 521 (7553) : 436-444." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning">
                                        <b>[8]</b>
                                         LECUN Y, BENGIO Y, HINTON G. Deep Learning. Nature, 2015, 521 (7553) : 436-444.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_9" title=" COLLOBERT R, WESTON J, KARLEN M, &lt;i&gt;et al&lt;/i&gt;. Natural Language Processing (Almost) from Scratch. Journal of Machine Lear-ning Research, 2011, 12: 2493-2537." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Natural language processing (almost) from scratch">
                                        <b>[9]</b>
                                         COLLOBERT R, WESTON J, KARLEN M, &lt;i&gt;et al&lt;/i&gt;. Natural Language Processing (Almost) from Scratch. Journal of Machine Lear-ning Research, 2011, 12: 2493-2537.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_10" title=" BOSSY R, GOLIK W, RATKOVIC Z, &lt;i&gt;et al&lt;/i&gt;. BioNLP Shared Task 2013-An Overview of the Bacteria Biotope Task // Proc of the Bionlp Shared Task 2013 Workshop. Berlin, Germany: Springer, 2013: 161-169." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bionlp shared task2013–an overview of the bacteria biotope task[C/OL]">
                                        <b>[10]</b>
                                         BOSSY R, GOLIK W, RATKOVIC Z, &lt;i&gt;et al&lt;/i&gt;. BioNLP Shared Task 2013-An Overview of the Bacteria Biotope Task // Proc of the Bionlp Shared Task 2013 Workshop. Berlin, Germany: Springer, 2013: 161-169.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_11" title=" MIKOLOV T, YIH W T, ZWEIG G. Linguistic Regularities in Continuous Space Word Representations // Proc of the Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies. Berlin, Germany: Springer, 2013: 746-751." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Linguistic regularities in continuous space w ord representations">
                                        <b>[11]</b>
                                         MIKOLOV T, YIH W T, ZWEIG G. Linguistic Regularities in Continuous Space Word Representations // Proc of the Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies. Berlin, Germany: Springer, 2013: 746-751.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_12" title=" GOODFELLOW I, BENGIO Y, COURVILLE A. Deep Learning. Cambridge, USA: The MIT Press, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Learning">
                                        <b>[12]</b>
                                         GOODFELLOW I, BENGIO Y, COURVILLE A. Deep Learning. Cambridge, USA: The MIT Press, 2016.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_13" title=" HOCHREITER S, SCHMIDHUBER J. Long Short-Term Memory. Neural Computation, 1997, 9 (8) : 1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTc4OTlGWk9vTERYVXhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRndYYUJFPU5pZkpaYks5SHRqTXFvOQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         HOCHREITER S, SCHMIDHUBER J. Long Short-Term Memory. Neural Computation, 1997, 9 (8) : 1735-1780.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_14" title=" RAVURI S, STOLOCKE A. A Comparative Study of Recurrent Neural Network Models for Lexical Domain Classification // Proc of the IEEE International Conference on Acoustics, Speech and Signal Processing. Washington, USA: IEEE, 2016: 6075-6079." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A comparative study of recurrent neural network models for lexical domain classification C]">
                                        <b>[14]</b>
                                         RAVURI S, STOLOCKE A. A Comparative Study of Recurrent Neural Network Models for Lexical Domain Classification // Proc of the IEEE International Conference on Acoustics, Speech and Signal Processing. Washington, USA: IEEE, 2016: 6075-6079.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(02),177-183 DOI:10.16451/j.cnki.issn1003-6059.201902010            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于融合式神经网络的微生物生长环境关系抽取</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%AD%9F%E9%A2%96&amp;code=41251071&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李孟颖</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%81%A5&amp;code=06522915&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王健</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E7%90%B0&amp;code=24028216&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王琰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9E%97%E9%B8%BF%E9%A3%9E&amp;code=06504899&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">林鸿飞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E5%BF%97%E8%B1%AA&amp;code=06523490&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨志豪</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A7%E8%BF%9E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0222286&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">大连理工大学计算机科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了构建完整的微生物生长环境关系数据库, 提出基于卷积神经网络-长短时记忆 (CNN-LSTM) 的关系抽取系统.结合卷积神经网络 (CNN) 和长短时记忆 (LSTM) , 实现对隐含特征的深度学习, 提取分布式词向量特征和实体位置特征作为模型的特征输入.对比实验验证加入特征后CNN-LSTM模型的优势, 并将CNN模型的特征输出作为LSTM模型的特征输入.在Bio-NLP 2016共享任务发布的BB-event语料集上得到目前最好的结果.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长短时记忆神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">关系抽取;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李孟颖, 硕士研究生, 主要研究方向为自然语言处理.E-mail:2326034523@qq.com.
;
                                </span>
                                <span>
                                    *王健, 博士, 教授, 主要研究方向为自然语言处理.E-mail:wangjian@dlut.edu.cn.
;
                                </span>
                                <span>
                                    王琰, 博士研究生, 主要研究方向为自然语言处理.E-mail:wy2266336@mail.dlut.edu.cn.
;
                                </span>
                                <span>
                                    林鸿飞, 博士, 教授, 主要研究方向为自然语言处理.E-mail:hflin@dlut.edu.cn.
;
                                </span>
                                <span>
                                    杨志豪, 博士, 教授, 主要研究方向为自然语言处理.E-mail:yangzh@dlut.edu.cn.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目 (No.2016YFB1001103);</span>
                                <span>国家自然科学基金项目 (No.61572098) 资助;</span>
                    </p>
            </div>
                    <h1>Bacteria Biotope Relation Extraction Based on a Fusion Neural Network</h1>
                    <h2>
                    <span>LI Mengying</span>
                    <span>WANG Jian</span>
                    <span>WANG Yan</span>
                    <span>LIN Hongfei</span>
                    <span>YANG Zhihao</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, Dalian University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To build a complete bacteria biotope relation database, a relation extraction system based on a convolutional neural network (CNN) -long short-term memory (LSTM) model is proposed. Combining CNN and LSTM, the deep learning of hidden features are realized, and the distributed word vector feature and entity position feature are extracted as feature input of the model.Comparative experiments verify the advantages of CNN-LSTM model after the addition of features.The feature output of the CNN model is taken as the feature input of the LSTM model, and the best result is obtained on the BB-event corpus published by the Bio-NLP 2016 shared task.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Long%20Short-Term%20Memory%20(LSTM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Long Short-Term Memory (LSTM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Relation%20Extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Relation Extraction;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LI Mengying, master student. Her research interests include natural language processing.
;
                                </span>
                                <span>
                                    WANG Jan, Ph.D., professor. Her research interests include natural language processing.
;
                                </span>
                                <span>
                                    WANG Yan, Ph. D. candidate. His research interests include natural language processing.
;
                                </span>
                                <span>
                                    LIN Hongfei, Ph. D., professor. His research interests include natural language processing.
;
                                </span>
                                <span>
                                    YANG Zhihao, Ph.D., professor. His research interests include natural language processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-20</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Key R&amp;D Program of China (No.2016YF B1001103);</span>
                                <span>National Natural Science Foundation of China (No.61572098);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="51">在生物医学领域内, 文献数量急剧增长, 有些生物医学相关的实体在文献中出现频繁和广泛<citation id="168" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 生物学家很难通过定位方法从文献中准确获取有用的信息, 这种情况也间接影响生物学家从文献中获取潜在有价值的信息.为了解决生物多样性的问题, 研究人员将问题具体化, 对具体的研究对象进行特定的关系抽取<citation id="167" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.为了构建完整的微生物及其栖息地的关系数据库, 满足生物医学发展需求, 生物医学关系抽取领域针对具体研究提出微生物生长环境关系抽取问题.</p>
                </div>
                <div class="p1">
                    <p id="52">目前针对微生物生长环境关系抽取技术还不成熟, 抽取结果的准确率普遍较低, 在BioNLP 2016评测的BB-event子任务中, 成绩位列前三名参赛队的<i>F</i>1值分别为55.8%、52.1%、48.5%, 均未达到60%.Björne等<citation id="169" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出关于生物医学事件抽取的生物医学自然语言处理 (Biomedical Natural Language Processing, BioNLP) 共享任务, 目的是抽取细粒度的生物实体之间的复杂关系.在BioNLP共享任务中典型的事件抽取系统为UturKu系统.该系统将事件抽取问题视为多分类问题处理, 利用机器学习的方法人工设计特征, 然后使用支持向量机 (SVM) 抽取事件.该系统的<i>F</i>1值为 51.95%, 位居当时第一名.Ratkovic等<citation id="170" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出Alvis系统, 通过人工设计模式进行关系匹配, 实现微生物生长环境的关系抽取.该系统的理论基础是词法学和语言学的相关知识.Karadniz等<citation id="171" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>进一步改善UturKu系统, 将微生物生长环境抽取任务视为二分类问题, 优化抽取特征, 利用机器学习进行关系抽取.</p>
                </div>
                <div class="p1">
                    <p id="53">传统的机器学习方法需要人工标注大量数据, 限制关系抽取任务的进一步发展.Li等<citation id="172" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出对偶分解法和词向量法, 词向量能够从大量未标注语料中自动学习, 得到丰富的语义信息, 利用未标注语料进行较准确的关系抽取, 在BioNLP2013 语料中<i>F</i>1值提高2%.Lever等<citation id="173" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出温哥华事件和关系抽取系统 (Vancouver Event and Relation System for Extraction, VERSE) , 在2016 BB-event任务比赛中取得第一 (<i>F</i>1值55.8%) , 结合随机优化策略, 允许特征选择, 减少大型稀疏特征向量并避免过拟合.</p>
                </div>
                <div class="p1">
                    <p id="54">由于深度学习的广泛应用, 卷积神经网络 (Convolutional Neural Network, CNN) 不仅在图像处理领域<citation id="174" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>取得卓越成绩, 在自然语言处理 (Natural Language Processing, NLP) <citation id="175" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>领域, 特别是文本和关系分类任务, 也取得不错效果.王健等<citation id="177" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出基于CNN和长短期记忆网络 (Long Short-Term Memory, LSTM) 结果融合的关系抽取方法, 将<i>F</i>1值提高到56.6%<citation id="176" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="55">由此可见, 关于微生物生长环境关系抽取的研究工作尚有较大的提升空间, 目前该任务采用的技术方法主要有3种:1) 基于传统规则的方法、2) 基于机器学习<citation id="178" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>的方法、3) 基于深度学习的方法.前两种方法都需要花费大量精力, 人工设计大量规则和特征, 存在局限性.本文提出基于CNN-LSTM的微生物生长环境关系抽取方法, 旨在利用深度学习两种模型结合的优点进行特征的自动学习, 自动准确地抽取有关系的微生物-栖息地实体对或微生物-地理位置实体对, 为分子生物和医学研究提供高质量的标注语料、工具和测评服务.</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag">1 基于CNN-LSTM的微生物生长环境关系抽取</h3>
                <h4 class="anchor-tag" id="57" name="57"><b>1.1</b> 研究方案</h4>
                <div class="p1">
                    <p id="58">基于深度学习的微生物生长环境关系抽取系统属于自然语言处理、文本挖掘领域的研究, 以BioNLP ST-2016评测任务中发布的BB3语料为数据集, 完成BB_event (即微生物与生长环境的关系) 抽取任务.抽取任务示例如图1所示, 在已知3种实体——微生物 (Bacteria) 、栖息地 (Habitat) 、地理位置 (Geographical Places) 的情况下, 判断微生物实体 (Bacteria) 与其它2个实体 (Habitat和Geographical places) 是否为生存 (Live in) 关系.</p>
                </div>
                <div class="area_img" id="59">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902010_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 关系抽取示例" src="Detail/GetImg?filename=images/MSSB201902010_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 关系抽取示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902010_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Relation extraction example</p>

                </div>
                <div class="p1">
                    <p id="60">图1样例抽取结果如表1所示, vibrio vulnificus实体与estuaries实体存在live in的关系, vibrio vulnificus实体与waterways不存在live in的关系, 所以该任务抽取的关系是单一的live in关系, 为二分类问题, 最终目的是识别有关系的细菌-栖息地实体对, 或有关系的细菌-地理位置实体对.</p>
                </div>
                <div class="area_img" id="61">
                    <p class="img_tit"><b>表1</b><b>关系抽取结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Relation extraction results</p>
                    <p class="img_note"></p>
                    <table id="61" border="1"><tr><td><br />Live in (yes or no) </td><td>实体1 (Bacteria) </td><td>实体2 (Location) </td></tr><tr><td><br />0</td><td>vibrio vulnificus</td><td>waterways</td></tr><tr><td><br />1</td><td>vibrio vulnificus</td><td>estuaries</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="62">本文方法如图2所示, 主要工作如下.</p>
                </div>
                <div class="p1">
                    <p id="63">1) 语料预处理.分句, 分词, 人工标注0、1标签, 构建最基本的样本正负例.</p>
                </div>
                <div class="p1">
                    <p id="64">2) 构建分布式词向量特征.从PubMed上下载大量文献摘要, 使用Word2Vec<citation id="179" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>训练词向量, 得到含有丰富语言特征的分布式表示.</p>
                </div>
                <div class="p1">
                    <p id="65">3) 构建位置特征.根据实体之间的距离构建位置向量, 将位置向量作为特征输入模型中.</p>
                </div>
                <div class="p1">
                    <p id="66">4) 建立相关神经网络模型.将分布式词向量特征和位置向量特征作为模型的输入.</p>
                </div>
                <div class="p1">
                    <p id="67">5) 选择分类器.使用sigmoid对模型输出结果分类, 得到关系抽取结果.</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902010_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 本文方法流程图" src="Detail/GetImg?filename=images/MSSB201902010_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 本文方法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902010_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Flowchart of the proposed method</p>

                </div>
                <h4 class="anchor-tag" id="69" name="69"><b>1.2</b> 语料预处理</h4>
                <div class="p1">
                    <p id="70">本文采用的数据集已进行一定处理, 包含3种类型文档, 分别是TXT、A1和A2文档.TXT文档内容是生物医学领域相关文献的摘要.A1文档内容是TXT文档中文献摘要涉及的所有实体, 形如“T16 Habitat 807 814 Patient”, 其中, T16表示实体序号, Habitat表示实体属性, 数字表示实体在TXT文档中位置偏移量, Patient表示实体具体名称.A2文档内容提供存在关系的实体对, 形如“R1 Lives_In Bacteria:T15 Location:T13”, 其中, R表示关系relation, Lives_In表示上述R的具体关系, Bacteria:T15表示细菌实体, Location:T13表示地理位置实体.</p>
                </div>
                <div class="p1">
                    <p id="71">实验数据集详细的信息统计如表2所示.表中文档数指A1、A2、TXT这3种类型文档的总数, 每种文档具体数量如表3所示.实体数量是指语料中存在的所有实体, 包括存在关系的实体对和不存在关系的实体对, 样本的正负例根据这些实体对构建.关系数量指所给语料中存在live in的关系数.</p>
                </div>
                <div class="p1">
                    <p id="72">由表3可知, 训练集中存在的关系数仅有338个, 由于数据集较小, 在训练深度学习模型时会存在一些问题.因此, 本文从PubMed语料库中下载大量文献摘要, 使用Word2Vec训练的词数为646 928, 覆盖大部分语料, 这样可充分利用未标注语料, 得到词的丰富语义特征, 解决小语料带来的部分问题.</p>
                </div>
                <div class="area_img" id="73">
                    <p class="img_tit"><b>表2</b><b>数据集统计</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Statistics of datasets</p>
                    <p class="img_note"></p>
                    <table id="73" border="1"><tr><td><br />统计项目</td><td>训练集</td><td>验证集</td><td>测试集</td><td>总数</td></tr><tr><td><br />文档数</td><td>183</td><td>102</td><td>102</td><td>387</td></tr><tr><td><br />实体数量</td><td>1102</td><td>748</td><td>1144</td><td>2994</td></tr><tr><td><br />关系数量</td><td>338</td><td>229</td><td>-</td><td>567</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="74">
                    <p class="img_tit"><b>表3</b><b>文档具体数量统计</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Statistics of documents</p>
                    <p class="img_note"></p>
                    <table id="74" border="1"><tr><td><br />文档类型</td><td>训练集</td><td>验证集</td><td>测试集</td><td>总数</td></tr><tr><td><br />A1</td><td>61</td><td>34</td><td>51</td><td>146</td></tr><tr><td><br />A2</td><td>61</td><td>34</td><td>0</td><td>95</td></tr><tr><td><br />TXT</td><td>61</td><td>34</td><td>51</td><td>146</td></tr><tr><td><br />总计</td><td>183</td><td>102</td><td>102</td><td>387</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="75">首先对文档进行分词处理, 通过3个文档的关联找到存在live in关系的实体对.再抽出实体及实体之间的单词作为特征输入实例.对训练集语料处理的具体算法如算法1所示.</p>
                </div>
                <div class="p1">
                    <p id="76"><b>算法</b> 正例构建算法</p>
                </div>
                <div class="p1">
                    <p id="77"><b>输入</b> 训练集语料中的所有文档</p>
                </div>
                <div class="p1">
                    <p id="78"><b>输出</b> 按行输出训练集语料中所有存在live in关系的实体及实体之间的单词</p>
                </div>
                <div class="p1">
                    <p id="79">for <i>i</i>=0, 3, 6, …, <i>m</i> do</p>
                </div>
                <div class="p1">
                    <p id="80">//3种类型文档, 批量处理, 步长为3</p>
                </div>
                <div class="p1">
                    <p id="81">按行读取文档;</p>
                </div>
                <div class="p1">
                    <p id="82">分词在A2文档中抽出存在live in关系的实体<i>T</i><sub>1</sub>, <i>T</i><sub>2</sub>;</p>
                </div>
                <div class="p1">
                    <p id="83">构建<i>T</i><sub>1</sub>, <i>T</i><sub>2</sub>的字典;</p>
                </div>
                <div class="p1">
                    <p id="84">for <i>i</i>=1, 4, 7, …, <i>m</i>+1 do</p>
                </div>
                <div class="p1">
                    <p id="85">按行读取文档;</p>
                </div>
                <div class="p1">
                    <p id="86">根据字典找到存在live in关系具体的实体名称;</p>
                </div>
                <div class="p1">
                    <p id="87">if 实体名称涉及到跨越单词</p>
                </div>
                <div class="p1">
                    <p id="88">then根据实体位置偏移量拼接实体;</p>
                </div>
                <div class="p1">
                    <p id="89">end if</p>
                </div>
                <div class="p1">
                    <p id="90">for <i>i</i>=2, 5, 8, …, <i>m</i>+2 do</p>
                </div>
                <div class="p1">
                    <p id="91">将整片文档内容写入一个list, 一个单词是一个元素;</p>
                </div>
                <div class="p1">
                    <p id="92">利用位置偏移量抽取实体之间的单词;</p>
                </div>
                <div class="p1">
                    <p id="93">if 偏移量 (<i>T</i><sub>1</sub>) ≥ 偏移量 (<i>T</i><sub>2</sub>) </p>
                </div>
                <div class="p1">
                    <p id="94">then print (<i>T</i><sub>2</sub>+<i>list</i>[偏移量 (<i>T</i><sub>2</sub>+1) :偏移量 (<i>T</i><sub>1</sub>+1) ] +<i>T</i><sub>1</sub>) ;</p>
                </div>
                <div class="p1">
                    <p id="95">else print (<i>T</i><sub>1</sub>+<i>list</i>[偏移量 (<i>T</i><sub>1</sub>+1) :偏移量 (<i>T</i><sub>2</sub>+1) ] +<i>T</i><sub>2</sub>) ;</p>
                </div>
                <div class="p1">
                    <p id="96">end if</p>
                </div>
                <div class="p1">
                    <p id="97">end for</p>
                </div>
                <div class="p1">
                    <p id="98">end for</p>
                </div>
                <div class="p1">
                    <p id="99">end for</p>
                </div>
                <div class="p1">
                    <p id="100">算法1实现按行输出训练集语料中所有存在live in关系的实体及实体之间的单词, 实现样本正例的构建.样本负例的构建算法与此类似, 不同的是在文档中抽取的实体不同.正例构建抽取存在关系的实体, 负例构建抽取一定不存在关系的实体.在经过算法处理后, 将正负样本写入一个文档中, 每行内容是实体对及实体对之间的单词, 表示一个实体对关系 (即存在live in关系和不存在live in关系) .人工赋予0、1标签, 标签1表示实体对之间存在live in关系, 标签0表示实体对之间不存在live in关系.</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101"><b>1.3</b> 分布式文本表示</h4>
                <div class="p1">
                    <p id="102">目前, 词向量的表示方法有传统的one-hot表示方法和分布式表示方法.传统表示方法是在高维向量中只使用一个维度描述词信息, 简单地将向量中相应元素设置为1, 其它元素设置为0.分布式表示方法是基于上下文相似的词语义也进行相似假设, 把文本信息分布式地存储在向量各个维度中, 具体如图3所示.</p>
                </div>
                <div class="area_img" id="103">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902010_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 分布式词向量的表示方法" src="Detail/GetImg?filename=images/MSSB201902010_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 分布式词向量的表示方法  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902010_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Distributed word vector representation</p>

                </div>
                <div class="p1">
                    <p id="104">文中利用Word2Vec工具对所得词进行词向量训练, 单词来源是PubMed数据库中的相关文献摘要.采用Word2Vec工具中的连续词袋模型 (Conti-nuous Bag of Word, CBOW) 进行词向量的训练.随机初始化, 将单词或字词映像到同个坐标系下, 使用神经网络模型进行特征学习, 得到连续的数值向量.该向量蕴含丰富的上下文语义信息.CBOW根据原始的语句预测目标字词或单词, 即给定上下文单词或字词, 使输出核心单词或字词的概率最大.实验时设定词向量维度大小为50.</p>
                </div>
                <h4 class="anchor-tag" id="105" name="105"><b>1.4</b> 位置特征选择</h4>
                <div class="p1">
                    <p id="106">因为位置信息可以反映实体之间的亲疏关系, 作为判断两实体之间是否存在关系的重要参数, 所以尝试在模型输入中加入位置特征.位置特征构建方案示例如图4所示, 人为规定从左侧开始计算时是正方向, 从右侧开始计算时是负方向.</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902010_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 位置特征构建示例" src="Detail/GetImg?filename=images/MSSB201902010_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 位置特征构建示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902010_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Location feature construction</p>

                </div>
                <div class="p1">
                    <p id="108">由图4可知, vulnificus这个词的位置特征可描述为 (1, -5) .从左侧开始计算, vulnificus的左侧有1个单词, 可得vulnificus的第1个参数为1.从右侧开始计算, 可得vulnificus的第2个参数是-5.同理可得very的位置特征可描述为 (4, -2) .</p>
                </div>
                <div class="p1">
                    <p id="109">在将位置特征输入模型之前, 进行特征映射, 即将每个方向的位置特征数值分别映射成高维的特征向量.最终将上述高维的位置特征向量输入模型中.实验表明, 太高的位置向量维度会使词向量的特征弱化, 实验结果不乐观, 太低的位置向量维度会使位置的特征弱化, 当位置向量特征维度约为词向量特征维度的1/10时, 实验结果最佳, 因此将该位置向量维度设置为5.</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110"><b>1.5</b> 建立<b>CNN-LSTM</b>模型</h4>
                <div class="p1">
                    <p id="111">卷积是数学中常用的一种运算, 运算如下:</p>
                </div>
                <div class="p1">
                    <p id="112"><i>S</i><sub> (<i>t</i>) </sub>= (<i>x</i>*<i>w</i>) (<i>t</i>) =∫<i>x</i> (<i>a</i>) <i>w</i> (<i>t</i>-<i>a</i>) d<i>a</i>, </p>
                </div>
                <div class="p1">
                    <p id="113">其中, <i>S</i><sub> (<i>t</i>) </sub>为函数<i>x</i>和函数<i>w</i>的卷积, <i>a</i>为积分变量, *为卷积操作.</p>
                </div>
                <div class="p1">
                    <p id="114">在卷积神经网络中, 卷积的第1个参数函数<i>x</i>称为模型输入, 第2个参数函数<i>w</i>称为核函数, 输出通常称为特征映射<citation id="180" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>.卷积操作可看成是对特征进行过滤的操作, 通过核函数<i>w</i>获得局部最优特征, 保留特征并组合形成新的特征, 这样每经过一层就会筛选这层较显著的特征并传到更高层, 即</p>
                </div>
                <div class="p1">
                    <p id="115"><i>S</i><sub> (<i>t</i>) </sub>=<i>σ</i> (<i>Wx</i><sub><i>t</i>:<i>t</i>+<i>w</i>-1</sub>+<i>b</i>) .</p>
                </div>
                <div class="p1">
                    <p id="116">其中:<i>W</i>表示卷积核函数;<i>x</i><sub><i>t</i>:<i>t</i>+<i>w</i>-1</sub>表示输入数据, 在本文中输入的数据是分布式词向量, 词向量形式为[<i>x</i><sub><i>t</i></sub>, <i>x</i><sub><i>t</i>+1</sub>, …, <i>x</i><sub><i>t</i>+<i>w</i>-2</sub>, <i>x</i><sub><i>t</i>+<i>w</i>-1</sub>];<i>w</i>表示输入窗口大小;<i>b</i>表示偏倚项;<i>σ</i>表示激活函数.通过卷积操作, 得到过滤后文本的特征值<i>S</i><sub> (<i>t</i>) </sub>.选取合适的滑动窗口大小, 在输入数据上依次滑动, 得到不同窗口下的特征值, 最后通过池化层筛选这组特征值, 获得最显著的特征.</p>
                </div>
                <div class="p1">
                    <p id="117">前馈神经网络隐含层之间的节点无连接.为了解决处理序列数据问题, 引入循环神经网络, 它的隐含层之间的节点有连接, 从而实现有记忆历史信息, 并实现把历史信息应用到当前信息的功能.</p>
                </div>
                <div class="p1">
                    <p id="118">循环神经网络不断改进, 产生很多模型, 其中较典型的模型是LSTM.LSTM能有效解决简单循环神经网络的梯度爆炸或梯度消失问题, 它的自循环权重不固定, 可根据上下文确定权重值, 更好地处理长距离依赖问题<citation id="181" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>.LSTM引入记忆单元 (Memory Unit) , 使神经网络可以学习遗忘历史信息的时间和使用新信息更新记忆单元的时间.</p>
                </div>
                <div class="p1">
                    <p id="119">记忆单元主要由3个门的控制组成:输入门、输出门和遗忘门.输入门主要控制每个内存单元加入新信息数量的多少, 输出门主要控制每个内存单元输出信息数量的多少, 遗忘门主要控制每个内存单元需要遗忘信息的多少.这3个门决定输入信息的重要与否、需不需要被记忆和能不能被输出<citation id="182" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>.3个门的元素值在0和1之间.在某一时刻<i>t</i>, 给定输入和隐含层节点状态的更新如下:</p>
                </div>
                <div class="p1">
                    <p id="120" class="code-formula">
                        <mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">i</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>f</mi></msub><mtext> </mtext><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>f</mi></msub><mtext> </mtext><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>f</mi></msub><mtext> </mtext><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">o</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>o</mi></msub><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>o</mi></msub><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>o</mi></msub><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>t</mi></msub><mo>*</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">i</mi><msub><mrow></mrow><mi>t</mi></msub><mo>*</mo><mi>tanh</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>c</mi></msub><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>c</mi></msub><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi mathvariant="bold-italic">o</mi><msub><mrow></mrow><mi>t</mi></msub><mo>*</mo><mi>tanh</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="121">其中, <b><i>i</i></b><sub><i>t</i></sub>表示输入门, <b><i>f</i></b><sub><i>t</i></sub>表示遗忘门, <b><i>o</i></b><sub><i>t</i></sub>表示输出门, <b><i>h</i></b><sub><i>t</i></sub>表示隐含层状态节点, <b><i>c</i></b><sub><i>t</i></sub>表示记忆单元, <b><i>x</i></b><sub><i>t</i></sub>表示<i>t</i>时刻的输入, <i>σ</i>表示logistic激活函数, <b><i>W</i>、<i>U</i>、<i>V</i></b>表示权重矩阵, 其中<b><i>V</i></b><sub><i>i</i></sub>、<b><i>V</i></b><sub><i>f</i></sub>、<b><i>V</i></b><sub><i>o</i></sub>为对角矩阵.</p>
                </div>
                <div class="p1">
                    <p id="122">考虑到CNN和LSTM神经网络具有各自的特点, 因此结合两种神经网络的优点搭建模型.将上述CNN学习到的特征输出作为LSTM的输入进行特征学习.CNN通过卷积核过滤筛选特征值, 又通过池化操作避免过拟合, 并选择最有价值的特征, 这些都是LSTM神经网络不具备的特性.因此把CNN已筛选出的有价值的特征输入到LSTM中, LSTM具有对原始输入记忆的功能, 此时的LSTM带着对CNN学习到的特征值的记忆再次进行特征学习, 使模型学习到的特征效果最佳.</p>
                </div>
                <div class="p1">
                    <p id="123">CNN-LSTM结构如图5 所示, 将位置向量放在单词分布式向量之后, 拼接作为模型输入, 最终模型输入实例的向量化表示为 (<b><i>e</i></b><sub>1</sub>, <b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>n</i></sub>, <b><i>e</i></b><sub>2</sub>, <b><i>m</i></b>, <b><i>n</i></b>) , 其中, <b><i>e</i></b><sub>1</sub>、<b><i>e</i></b><sub>2</sub>为实体对向量, <b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>n</i></sub>为两个实体之间的单词向量, <b><i>m</i></b>、<b><i>n</i></b>为正负两个方向的位置特征向量.因考虑到两个实体之间的单词个数不固定, 采用序列补齐策略, 设置一个固定的序列长度, 当列表元素 (<b><i>e</i></b><sub>1</sub>, <b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>n</i></sub>, <b><i>e</i></b><sub>2</sub>, <b><i>m</i></b>, <b><i>n</i></b>) 长度小于这个固定长度时, 在序列的末尾补零至该长度, 当列表元素的长度大于该长度时, 产生截断, 保留索引值的个数为该长度值.将模型输入数据处理好后使其经过卷积神经网络筛选出一部分特征, 再经过LSTM神经网络筛选特征.因为微生物生长环境关系抽取是二分类问题, 所以经过两个神经网络筛选得到的特征值最后经过sigmoid函数对其进行分类, 得到模型的分类结果, 即微生物与栖息地或地理位置之间是否存在live in的关系.</p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902010_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 CNN-LSTM模型结构" src="Detail/GetImg?filename=images/MSSB201902010_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 CNN-LSTM模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902010_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Architecure of CNN-LSTM model</p>

                </div>
                <h3 id="125" name="125" class="anchor-tag">2 实验及结果分析</h3>
                <div class="p1">
                    <p id="126">本文研究关系抽取技术, 领域是自然语言处理, 对于该系统性能可使用准确率 (<i>P</i>) 和召回率 (<i>R</i>) 进行评估, <i>F</i>1值为准确率和召回率的综合评估, 计算如下:</p>
                </div>
                <div class="p1">
                    <p id="127" class="code-formula">
                        <mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mo>‚</mo></mtd></mtr><mtr><mtd><mi>R</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mo>‚</mo></mtd></mtr><mtr><mtd><mi>F</mi><mn>1</mn><mo>=</mo><mfrac><mrow><mn>2</mn><mi>Ρ</mi><mi>R</mi></mrow><mrow><mi>Ρ</mi><mo>+</mo><mi>R</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mo>‚</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="128">其中, <i>TP</i>表示系统抽取结果正确的元素集合, <i>FP</i>表示系统抽取结果非正确的元素集合, <i>FN</i>表示抽取系统完全未抽取到的那一部分标准答案的元素个数.文中通过BioNLP ST-2016评测任务中发布的BB-event语料集中的验证集度量分类结果.</p>
                </div>
                <div class="p1">
                    <p id="129">本文主要构建3个神经网络模型, 分别为CNN、LSTM和CNN-LSTM.为了形成对比实验, 3种模型的输入及模型参数设置始终保持一致.</p>
                </div>
                <div class="p1">
                    <p id="130">首先, 随机初始化模型输入, 即模型输入为 (<b><i>a</i></b><sub>1</sub>, <b><i>b</i></b><sub>1</sub>, <b><i>b</i></b><sub>2</sub>, …, <b><i>b</i></b><sub><i>n</i></sub>, <b><i>a</i></b><sub>2</sub>) , 其中, <b><i>a</i></b><sub>1</sub>、<b><i>a</i></b><sub>2</sub>表示随机初始化后的实体单词向量, <b><i>b</i></b><sub>1</sub>, <b><i>b</i></b><sub>2</sub>, …, <b><i>b</i></b><sub><i>n</i></sub>表示随机初始化后实体对之间的单词向量.然后, 使用Word2Vec训练得到的词向量, 分别初始化CNN、LSTM、CNN-LSTM的输入, 模型的输入形式与随机初始化形式相同, 即 (<b><i>e</i></b><sub>1</sub>, <b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>n</i></sub>, <b><i>e</i></b><sub>2</sub>) , 其中, <b><i>e</i></b><sub>1</sub>、<b><i>e</i></b><sub>2</sub>为使用Word2Vec训练得到实体对向量, <b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>n</i></sub>为Word2Vec训练得到的两个实体之间的单词向量.最后, 使用Word2Vec训练得到的词向量与特征映射得到的位置特征向量进行拼接, 将拼接后的特征向量结果作为模型输入, 即 (<b><i>e</i></b><sub>1</sub>, <b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>n</i></sub>, <b><i>e</i></b><sub>2</sub>, <b><i>m</i></b>, <b><i>n</i></b>) , 其中, <b><i>m</i></b>表示正方向的位置相对数值映射成的5维特征向量, <b><i>n</i></b>表示反方向的位置相对数值映射成的5维特征向量.具体实验结果如表4所示, 表中的CNN-LSTM-P表示加入位置特征后CNN-LSTM结果.</p>
                </div>
                <div class="area_img" id="132">
                    <p class="img_tit"><b>表4</b><b>3种模型的实验结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Experiment result comparison of 3 models</p>
                    <p class="img_note"></p>
                    <table id="132" border="1"><tr><td><br />初始化方式</td><td>模型</td><td><i>P</i></td><td><i>R</i></td><td><i>F</i></td></tr><tr><td><br />随机</td><td>CNN<br />LSTM<br />CNN-LSTM</td><td>0.5528<br />0.4345<br />0.4327</td><td>0.4603<br />0.8439<br />0.6023</td><td>0.5023<br />0.5737<br />0.5036</td></tr><tr><td><br />分布式<br />词向量</td><td>CNN<br />LSTM<br />CNN-LSTM<br />CNN-P<br />LSTM-P<br />CNN-LSTM-P</td><td>0.4681<br />0.4313<br />0.4532<br />0.4513<br />0.4392<br />0.5690</td><td>0.6250<br />0.7841<br />0.6875<br />0.7102<br />0.7386<br />0.7071</td><td>0.5353<br />0.5565<br />0.5462<br />0.5519<br />0.5508<br />0.6306</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="133">由表4可知, 当使用分布式词向量初始化模型输入时, CNN和CNN-LSTM的<i>F</i>1值均有提高, 原因是通过Word2Vec训练大量单词使得到的分布式词向量具有丰富的语法语义信息.LSTM的<i>F</i>1值略有下降, 可能是由于从PubMed语料库中下载的文献摘要并不能完全覆盖文中语料涉及的所有单词, 导致部分语料信息缺失, 对LSTM的学习造成一定影响, 致使学习效果下降.当使用分布式词向量初始化并加入位置特征后, CNN与CNN-LSTM结果均有所提升, 但是CNN提升效果不明显, 而CNN-LSTM具有大幅提升, 与不加位置向量特征相比, <i>F</i>1值提高近10%.实验说明2个问题:1) 在本文关系抽取任务中, 分布式词向量与位置向量是重要的特征参数;2) 只有CNN-LSTM能充分利用上述两个特征, 使模型得到充分、自动、准确的学习.</p>
                </div>
                <div class="p1">
                    <p id="134">由表4数据还可看出:通过CNN训练得到的结果准确率较高;通过LSTM训练得到的结果召回率较高;将CNN与LSTM结合的CNN-LSTM得到的效果最佳, 更论证CNN-LSTM的正确性, 它实现将CNN模型与LSTM模型的特性结合, 使得最终结果既有CNN模型训练得到的较高准确率, 又有LSTM模型训练得到的较高召回率, 最终提高<i>F</i>1值, 使系统性能最佳.但是由于训练集数据较少等原因, 导致模型输出结果不稳定, 因此本文对最佳模型进行多次实验, 最终的实验结果<i>F</i>1的平均值为0.641 4, 范围为0.621 4～0.661 4.</p>
                </div>
                <div class="p1">
                    <p id="135">CNN-LSTM与其它性能较高的系统模型的对比结果如表5所示, 正文CNN-LSTM结果数据值为平均值, 表4与表5数据值为某次实验具体值.</p>
                </div>
                <div class="area_img" id="136">
                    <p class="img_tit"><b>表5</b><b>4种方法的实验结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Experiment result comparison of 4 methods</p>
                    <p class="img_note"></p>
                    <table id="136" border="1"><tr><td><br />方法</td><td><i>P</i></td><td><i>R</i></td><td><i>F</i></td></tr><tr><td><br />SVM<sup>[4]</sup></td><td>0.594</td><td>0.336</td><td>0.429</td></tr><tr><td><br />VERSE<sup>[7]</sup></td><td>0.510</td><td>0.615</td><td>0.558</td></tr><tr><td><br />神经网络<sup>[2]</sup></td><td>0.493</td><td>0.663</td><td>0.566</td></tr><tr><td><br />CNN-LSTM-P</td><td>0.569</td><td>0.707</td><td>0.631</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="137">通过对比实验可知, 相比其它较优系统, CNN-LSTM在加入位置特征后提高近7.5%, 说明CNN-LSTM在微生物生长环境关系抽取任务中性能较好.CNN-LSTM继承CNN和LSTM模型各自的优点, 通过Word2Vec训练大量单词得到的分布式词向量在一定程度上克服提供数据量较少的困难, 又通过加入位置特征, 使CNN-LSTM充分发挥模型结合的优势, 挖掘隐含层的特征, 关系抽取效果达到最好.</p>
                </div>
                <h3 id="138" name="138" class="anchor-tag">3 结 束 语</h3>
                <div class="p1">
                    <p id="139">本文着重研究微生物生长环境关系抽取方法, 借助PubMed数据库中未标注语料, 通过Word2Vec训练大量词向量, 解决小语料带来的困难.通过增加位置特征, 使CNN-LSTM得到准确学习.将CNN的特征输出作为LSTM的特征输入, 综合两种神经网络模型的优势以弥补单个神经网络模型的不足, 利用CNN-LSTM实现对隐含层特征的自动学习.在Bio-NLP 2016的BB-event语料集上进行实验, 获得64.14%的<i>F</i>1值, 该性能是目前微生物生长环境关系抽取任务的最好结果.实验表明, 结合卷积神经网络与长短时记忆神经网络, 并联合加入位置特征建立的CNN-LSTM关系抽取模型是有效的.</p>
                </div>
                <div class="p1">
                    <p id="140">虽然CNN-LSTM的实验结果可观, 但是目前关于微生物生长环境的关系抽取技术仍不够成熟, <i>F</i>1值普遍偏低, 不能满足实际需求.主要原因是微生物生长环境复杂多样导致关于微生物生长环境的语料不足, 深度学习不能充分发挥优势.今后将尝试进一步采用attention机制建立模型, 以期更有效地提高微生物生长环境关系抽取的性能.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="11">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Overview of BioNLP shared task 2013">

                                <b>[1]</b> NÉDELLEC C, BOSSY R, KIM J D, <i>et al</i>. Overview of BioNLP Shared Task 2013 // Proc of the BioNLP Shared Task 2013 Workshop. Berlin, Germany: Springer, 2013: 1-7.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HNLG201703011&amp;v=MjA5OTdMM0xMU1BIYWJHNEg5Yk1ySTlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6blU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 王健, 李虹磊, 林鸿飞, 等.基于神经网络的微生物生长环境关系抽取方法.华南理工大学学报 (自然科学版) , 2017, 45 (3) :76-81. (WANG J, LI H L, LIN H F, <i>et al</i>. Bacteria Biotope Extraction on the Basis of Neural Network. Journal of South China University of Technology (Natural Science Edition) , 2017, 45 (3) : 76-81.) 
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extracting Complex Biological Events with Rich Graph-based Feature Sets">

                                <b>[3]</b> BJÖRNE J, HEIMONEN J, GINTER F, <i>et al</i>. Extracting Complex Biological Events with Rich Graph-Based Feature Sets // Proc of the Workshop on BioNLP: Shared Task. Berlin, Germany: Springer, 2009: 10-18.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bio NLP2011 task bacteria biotope:the Alvis system">

                                <b>[4]</b> RATKOVIC Z, GOLIK W, WARNIER P, <i>et al</i>. BioNLP 2011 Task Bacteria Biotope: The Alvis System // Proc of the BioNLP Shared Task 2011 Workshop. Stroudsburg, USA: ACL, 2011: 102-111.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bacteria biotope detection,ontology-based normalization,and relation extraction using syntactic rules">

                                <b>[5]</b> KARADENIZ I, ÖZGÜR A. Bacteria Biotope Detection, Ontology-Based Normalization, and Relation Extraction Using Syntactic Rules // Proc of the BioNLP Shared Task 2013 Workshop. Berlin, Germany: Springer, 2013: 170-177.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extracting biomedical event with dual decomposition integrating word embeddings">

                                <b>[6]</b> LI L S, LIU S S, QIN M Y, <i>et al</i>. Extracting Biomedical Event with Dual Decomposition Integrating Word Embeddings. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2016, 13 (4) : 669-677.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=VERSE: Event and Relation Extraction in the BioNLP 2016 Shared Task">

                                <b>[7]</b> LEVER J, JONES S J. VERSE: Event and Relation Extraction in the BioNLP 2016 Shared Task // Proc of the 4th BioNLP Shared Task Workshop. Berlin, Germany: Springer, 2016: 42-49.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning">

                                <b>[8]</b> LECUN Y, BENGIO Y, HINTON G. Deep Learning. Nature, 2015, 521 (7553) : 436-444.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Natural language processing (almost) from scratch">

                                <b>[9]</b> COLLOBERT R, WESTON J, KARLEN M, <i>et al</i>. Natural Language Processing (Almost) from Scratch. Journal of Machine Lear-ning Research, 2011, 12: 2493-2537.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bionlp shared task2013–an overview of the bacteria biotope task[C/OL]">

                                <b>[10]</b> BOSSY R, GOLIK W, RATKOVIC Z, <i>et al</i>. BioNLP Shared Task 2013-An Overview of the Bacteria Biotope Task // Proc of the Bionlp Shared Task 2013 Workshop. Berlin, Germany: Springer, 2013: 161-169.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Linguistic regularities in continuous space w ord representations">

                                <b>[11]</b> MIKOLOV T, YIH W T, ZWEIG G. Linguistic Regularities in Continuous Space Word Representations // Proc of the Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies. Berlin, Germany: Springer, 2013: 746-751.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Learning">

                                <b>[12]</b> GOODFELLOW I, BENGIO Y, COURVILLE A. Deep Learning. Cambridge, USA: The MIT Press, 2016.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=Mjg1MDRxbzlGWk9vTERYVXhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRndYYUJFPU5pZkpaYks5SHRqTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> HOCHREITER S, SCHMIDHUBER J. Long Short-Term Memory. Neural Computation, 1997, 9 (8) : 1735-1780.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A comparative study of recurrent neural network models for lexical domain classification C]">

                                <b>[14]</b> RAVURI S, STOLOCKE A. A Comparative Study of Recurrent Neural Network Models for Lexical Domain Classification // Proc of the IEEE International Conference on Acoustics, Speech and Signal Processing. Washington, USA: IEEE, 2016: 6075-6079.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201902010" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201902010&amp;v=MDY0NjZPZVplUm5GeXpuVUwzTEtEN1liTEc0SDlqTXJZOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
