<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131453739717500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201907002%26RESULT%3d1%26SIGN%3dCdv91UxDLL8gN8dl6XpwEUBcLbo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201907002&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201907002&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201907002&amp;v=MDE3OTlNS0Q3WWJMRzRIOWpNcUk5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emhWTC8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#56" data-title="1 正则化回归学习算法 ">1 正则化回归学习算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#69" data-title="&lt;b&gt;1.1 最小二乘正则化回归学习算法&lt;/b&gt;"><b>1.1 最小二乘正则化回归学习算法</b></a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;1.2 基于多尺度核的正则化回归学习算法&lt;/b&gt;"><b>1.2 基于多尺度核的正则化回归学习算法</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;1.3 基于分布式学习的正则化回归学习算法&lt;/b&gt;"><b>1.3 基于分布式学习的正则化回归学习算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#109" data-title="2 基于多尺度核的分布式正则化回归学习算法 ">2 基于多尺度核的分布式正则化回归学习算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#161" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#168" data-title="&lt;b&gt;3.1 模拟数据实验&lt;/b&gt;"><b>3.1 模拟数据实验</b></a></li>
                                                <li><a href="#229" data-title="&lt;b&gt;3.2 真实数据实验&lt;/b&gt;"><b>3.2 真实数据实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#263" data-title="4 结 束 语 ">4 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#100" data-title="图1 分布式学习方法流程图">图1 分布式学习方法流程图</a></li>
                                                <li><a href="#203" data-title="图2 模拟数据集1的分布">图2 模拟数据集1的分布</a></li>
                                                <li><a href="#204" data-title="表1 4种算法在模拟数据1上的实验结果">表1 4种算法在模拟数据1上的实验结果</a></li>
                                                <li><a href="#295" data-title="图3 m不同时MKG-DRR在模拟数据1上的拟合误差">图3 m不同时MKG-DRR在模拟数据1上的拟合误差</a></li>
                                                <li><a href="#215" data-title="表2 m不同时MKG-DRR在模拟数据1上的运行时间">表2 m不同时MKG-DRR在模拟数据1上的运行时间</a></li>
                                                <li><a href="#218" data-title="表3 参数不同时MKG-DRR在模拟数据集2上的实验结果">表3 参数不同时MKG-DRR在模拟数据集2上的实验结果</a></li>
                                                <li><a href="#226" data-title="图4 参数不同时MKG-DRR在模拟数据集2上的拟合误差">图4 参数不同时MKG-DRR在模拟数据集2上的拟合误差</a></li>
                                                <li><a href="#233" data-title="表4 真实数据集描述">表4 真实数据集描述</a></li>
                                                <li><a href="#246" data-title="图5 Wind数据集的风速数据分布">图5 Wind数据集的风速数据分布</a></li>
                                                <li><a href="#247" data-title="表5 4种算法在真实数据集上的实验结果">表5 4种算法在真实数据集上的实验结果</a></li>
                                                <li><a href="#260" data-title="图6 4种算法在Wind数据集上的残差对比">图6 4种算法在Wind数据集上的残差对比</a></li>
                                                <li><a href="#260" data-title="图6 4种算法在Wind数据集上的残差对比">图6 4种算法在Wind数据集上的残差对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="296">


                                    <a id="bibliography_1" title=" CUCKER F, SMALE S.On the Mathematical Foundations of Lear-ning.Bulletin of the American Mathematical Society, 2001, 39 (1) :1-49." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJAM&amp;filename=SJAM120524050515&amp;v=MTgzMTc0OUFaTzRPQ1JNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3ZzVTd2TUoxNFFOaWZLWTdLNkh0VE9x&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         CUCKER F, SMALE S.On the Mathematical Foundations of Lear-ning.Bulletin of the American Mathematical Society, 2001, 39 (1) :1-49.
                                    </a>
                                </li>
                                <li id="298">


                                    <a id="bibliography_2" title=" ZHENG D N, WANG J X, ZHAO Y N.Nonflat Function Estimation with a Multi-scale Support Vector Regression.Neurocomputing, 2006, 70 (1/2/3) :420-429." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501913096&amp;v=Mjg4OTNGb1RhaFk9TmlmT2ZiSzdIdEROcW85RWJlb01ESFUvb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         ZHENG D N, WANG J X, ZHAO Y N.Nonflat Function Estimation with a Multi-scale Support Vector Regression.Neurocomputing, 2006, 70 (1/2/3) :420-429.
                                    </a>
                                </li>
                                <li id="300">


                                    <a id="bibliography_3" title=" SAUNDERS C, GAMMERMAN A, VOVK V.Ridge Regression Learning Algorithm in Dual Variables // Proc of the 15th International Conference on Machine Learning.New York, USA:ACM, 1998:515-521." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ridge regression learning algorithm in dual variables">
                                        <b>[3]</b>
                                         SAUNDERS C, GAMMERMAN A, VOVK V.Ridge Regression Learning Algorithm in Dual Variables // Proc of the 15th International Conference on Machine Learning.New York, USA:ACM, 1998:515-521.
                                    </a>
                                </li>
                                <li id="302">


                                    <a id="bibliography_4" title=" YANG H Q, XU Z L, YE J P, et al.Efficient Sparse Generalized Multiple Kernel Learning.IEEE Transactions on Neural Networks, 2011, 22 (3) :433-446." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient Sparse Generalized Multiple Kernel Learning">
                                        <b>[4]</b>
                                         YANG H Q, XU Z L, YE J P, et al.Efficient Sparse Generalized Multiple Kernel Learning.IEEE Transactions on Neural Networks, 2011, 22 (3) :433-446.
                                    </a>
                                </li>
                                <li id="304">


                                    <a id="bibliography_5" title=" 邵喜高.基于统计学习理论的多核预测模型研究及应用.博士学位论文.长沙:中南大学, 2013. (SHAO X G.The Research and Application of Multiple Kernel Prediction Model Based on Statistical Learning Theory.Ph.D.Dissertation.Changsha, China:Central South University, 2013.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1013310489.nh&amp;v=MDk0NTVxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emhWTC9NVkYyNkhiQzVIdFhFcHBFYlBJUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         邵喜高.基于统计学习理论的多核预测模型研究及应用.博士学位论文.长沙:中南大学, 2013. (SHAO X G.The Research and Application of Multiple Kernel Prediction Model Based on Statistical Learning Theory.Ph.D.Dissertation.Changsha, China:Central South University, 2013.) 
                                    </a>
                                </li>
                                <li id="306">


                                    <a id="bibliography_6" title=" KINGSBURY N, TAY D, PALANISWAMI M.Multi-scale Kernel Methods for Classification // Proc of the IEEE Workshop on Machine Learning for Signal Processing.Washington, USA:IEEE Press, 2005:43-48." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-scale kernel methods for classification">
                                        <b>[6]</b>
                                         KINGSBURY N, TAY D, PALANISWAMI M.Multi-scale Kernel Methods for Classification // Proc of the IEEE Workshop on Machine Learning for Signal Processing.Washington, USA:IEEE Press, 2005:43-48.
                                    </a>
                                </li>
                                <li id="308">


                                    <a id="bibliography_7" >
                                        <b>[7]</b>
                                     汪洪桥, 蔡艳宁, 孙富春, 等.多尺度核方法的自适应序列学习及应用.模式识别与人工智能, 2011, 24 (1) :72-81. (WANG H Q, CAI Y N, SUN F C.Adaptive Sequence Learning and Applications for Multi-scale Kernel Method.Pattern Recognition and Artificial Intelligence, 2011, 24 (1) :72-81.) </a>
                                </li>
                                <li id="310">


                                    <a id="bibliography_8" title=" XU Y L, CHEN D R, LI H X, et al.Least Square Regularized Regression in Sum Space.IEEE Transactions on Neural Networks and Learning Systems, 2013, 24 (4) :635-646." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Least Square Regularized Regression in Sum Space">
                                        <b>[8]</b>
                                         XU Y L, CHEN D R, LI H X, et al.Least Square Regularized Regression in Sum Space.IEEE Transactions on Neural Networks and Learning Systems, 2013, 24 (4) :635-646.
                                    </a>
                                </li>
                                <li id="312">


                                    <a id="bibliography_9" title=" SHAMIR O, SREBRO N.Distributed Stochastic Optimization and Learning[C/OL].[2018-11-12].https://arxiv.org/pdf/1408.5294.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributed Stochastic Optimization and Learning[C/OL]">
                                        <b>[9]</b>
                                         SHAMIR O, SREBRO N.Distributed Stochastic Optimization and Learning[C/OL].[2018-11-12].https://arxiv.org/pdf/1408.5294.pdf.
                                    </a>
                                </li>
                                <li id="314">


                                    <a id="bibliography_10" title=" ZHANG Y C, DUCHI J, WAINWRIGHT M.Divide and Conquer Kernel Ridge Regression:A Distributed Algorithm with Minimax Optimal Rates.Journal of Machine Learning Research, 2015, 16:3299-3340." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Divide and conquer kernel ridge regression:A distributed algorithm with minimax optimal rates">
                                        <b>[10]</b>
                                         ZHANG Y C, DUCHI J, WAINWRIGHT M.Divide and Conquer Kernel Ridge Regression:A Distributed Algorithm with Minimax Optimal Rates.Journal of Machine Learning Research, 2015, 16:3299-3340.
                                    </a>
                                </li>
                                <li id="316">


                                    <a id="bibliography_11" title=" M&#220;CKE N, BLANCHARD G.Parallelizing Spectral Algorithms for Kernel Learning.Journal of Machine Learning Research, 2016, 19:1-29." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=mlr:Machine Learning in R">
                                        <b>[11]</b>
                                         M&#220;CKE N, BLANCHARD G.Parallelizing Spectral Algorithms for Kernel Learning.Journal of Machine Learning Research, 2016, 19:1-29.
                                    </a>
                                </li>
                                <li id="318">


                                    <a id="bibliography_12" title=" LIN S B, GUO X, ZHOU D X.Distributed Learning with Regula-rized Least Squares.Journal of Machine Learning Research, 2017, 18:1-31." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributed Learning with Regula-rized Least Squares">
                                        <b>[12]</b>
                                         LIN S B, GUO X, ZHOU D X.Distributed Learning with Regula-rized Least Squares.Journal of Machine Learning Research, 2017, 18:1-31.
                                    </a>
                                </li>
                                <li id="320">


                                    <a id="bibliography_13" title=" GUO Z C, LIN S B, ZHOU D X.Learning Theory of Distributed Spectral Algorithm.Inverse Problems, 2017, 33 (7) .DOI:10.1088/1361-6420/aa72b2." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SIPD&amp;filename=SIPD2C33BC30127A6785CE48F34C0735CA34&amp;v=MjYyNzRpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3THk0d0t3PU5pVGJhckhMSGRLKzNJeEZaZWtJZlhvK3h4TmduenQxUG56bTN4SXllcmZuTkxtYkNPTnZGUw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         GUO Z C, LIN S B, ZHOU D X.Learning Theory of Distributed Spectral Algorithm.Inverse Problems, 2017, 33 (7) .DOI:10.1088/1361-6420/aa72b2.
                                    </a>
                                </li>
                                <li id="322">


                                    <a id="bibliography_14" title=" GUO Z C, SHI L, WU Q.Learning Theory of Distributed Regre-ssion with Bias Corrected Regularization Kernel Network.Journal of Machine Learning Research, 2017, 18:1-25." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Theory of Distributed Regre-ssion with Bias Corrected Regularization Kernel Network">
                                        <b>[14]</b>
                                         GUO Z C, SHI L, WU Q.Learning Theory of Distributed Regre-ssion with Bias Corrected Regularization Kernel Network.Journal of Machine Learning Research, 2017, 18:1-25.
                                    </a>
                                </li>
                                <li id="324">


                                    <a id="bibliography_15" title=" JOSHI B, IUTZELER F, AMINI M R.Large-Scale Asynchronous Distributed Learning Based on Parameter Exchanges[J/OL].[2018-11-12].https://arxiv.org/pdf/1705.07751.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-Scale Asynchronous Distributed Learning Based on Parameter Exchanges">
                                        <b>[15]</b>
                                         JOSHI B, IUTZELER F, AMINI M R.Large-Scale Asynchronous Distributed Learning Based on Parameter Exchanges[J/OL].[2018-11-12].https://arxiv.org/pdf/1705.07751.pdf.
                                    </a>
                                </li>
                                <li id="326">


                                    <a id="bibliography_16" title=" TIKHONOV A N.Regularization of Incorrectly Posed Problems.Soviet Mathematics Doklady, 1963, 4 (1) :1624-1627." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Regularization of incorrectly posed problems">
                                        <b>[16]</b>
                                         TIKHONOV A N.Regularization of Incorrectly Posed Problems.Soviet Mathematics Doklady, 1963, 4 (1) :1624-1627.
                                    </a>
                                </li>
                                <li id="328">


                                    <a id="bibliography_17" title=" ARONSZAJN N.Theory of Reproducing Kernels.Transactions of the American Mathematical Society, 1950, 68 (3) :337-404." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJAM&amp;filename=SJAM120524013474&amp;v=MzIyNjBZN0s2SHRUT3E0OUVaKzhJQ0JNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3ZzVTd2TUoxNFFOaWZL&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         ARONSZAJN N.Theory of Reproducing Kernels.Transactions of the American Mathematical Society, 1950, 68 (3) :337-404.
                                    </a>
                                </li>
                                <li id="330">


                                    <a id="bibliography_18" title=" CUCKER F, ZHOU D X.Learning Theory:An Approximation Theory Viewpoint.Cambridge, UK:Cambridge University Press, 2007." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Theory:An Approximation Theory Viewpoint">
                                        <b>[18]</b>
                                         CUCKER F, ZHOU D X.Learning Theory:An Approximation Theory Viewpoint.Cambridge, UK:Cambridge University Press, 2007.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(07),589-599 DOI:10.16451/j.cnki.issn1003-6059.201907002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于多尺度高斯核的分布式正则化回归学习算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%91%A3%E9%9B%AA%E6%A2%85&amp;code=34209386&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">董雪梅</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%B4%81%E5%BE%AE&amp;code=41924814&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王洁微</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%99%E6%B1%9F%E5%B7%A5%E5%95%86%E5%A4%A7%E5%AD%A6%E7%BB%9F%E8%AE%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0197743&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">浙江工商大学统计与数学学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对工业、信息等领域出现的基于较大规模、非平稳变化复杂数据的回归问题, 已有算法在计算成本及拟合效果方面无法同时满足要求.因此, 文中提出基于多尺度高斯核的分布式正则化回归学习算法.算法中的假设空间为多个具有不同尺度的高斯核生成的再生核Hilbert空间的和空间.考虑到整个数据集划分的不同互斥子集波动程度不同, 建立不同组合系数核函数逼近模型.利用最小二乘正则化方法同时独立求解各逼近模型.最后, 通过对所得的各个局部估计子加权合成得到整体逼近模型.在2个模拟数据集和4个真实数据集上的实验表明, 文中算法既能保证较优的拟合性能, 又能降低运行时间.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E6%A0%B8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度核;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%B8%E6%96%B9%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">核方法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分布式学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%AD%A3%E5%88%99%E5%8C%96%E5%9B%9E%E5%BD%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最小二乘正则化回归;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *董雪梅 (通讯作者) , 博士, 副研究员, 主要研究方向为机器学习、数据挖掘.E-mail:dongxuemei@zjgsu.edu.cn.;
                                </span>
                                <span>
                                    王洁微, 硕士, 工程师, 主要研究方向为机器学习.E-mail:wangjwhm@qq.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-14</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.11571031, 11701509);</span>
                                <span>浙江省一流学科A类 (浙江工商大学统计学) 资助;</span>
                    </p>
            </div>
                    <h1><b>Distributed Regularized Regression Learning Algorithm Based on Multi-scale Gaussian Kernels</b></h1>
                    <h2>
                    <span>DONG Xuemei</span>
                    <span>WANG Jiewei</span>
            </h2>
                    <h2>
                    <span>School of Statistics and Mathematics, Zhejiang Gongshang University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The existing algorithms cannot produce satisfactory results with both low calculation cost and good fitting effect, due to the regression problems based on the complex data with large scale and non-stationary variation in industry, information and other fields. Therefore, a distributed regularized regression learning algorithm based on multi-scale Gaussian kernels is proposed. The hypothesis space of the proposed algorithm is a sum space composed of reproducing kernel Hilbert spaces generated by multiple Gaussian kernels with different scales. Since each disjoint subset partitioned from the whole data set with different degree of fluctuation, kernel function approximation models with different combination coefficients are established. According to the least square regularized method, a local estimator is learned from each subset independently in the meantime. Finally, a global approximation model is obtained by weighting all the local estimators. The experimental results on two simulation datasets and four real datasets show that the proposed algorithm reduces the running time successfully with a strong fitting ability compared with the existing algorithms.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multi-scale%20Kernels&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multi-scale Kernels;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Kernel%20Method&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Kernel Method;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Distributed%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Distributed Learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Least%20Square%20Regularized%20Regression&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Least Square Regularized Regression;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    DONG Xuemei ( Corresponding author) , Ph. D. , associate researcher. Her research interests include machine learning and data mining.;
                                </span>
                                <span>
                                    WANG Jiewei, master, engineer. Her research interests includes machine learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-14</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No11571031, 11701509);</span>
                                <span>First Class Discipline of Zhejiang-A (Zhejiang Gongshang University-Statistics);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="50">人工智能和机器学习是当今大数据时代的热门话题.随着信息科技的日益发展, 迅速提高的计算能力可产生海量的数据, 同时数据也越来越复杂.在实际应用中, 当处理波动剧烈且规模较大的复杂数据时, 再生核Hilbert函数空间框架下的最小二乘正则化回归算法<citation id="332" type="reference"><link href="296" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>主要面临两方面的挑战:1) 算法中基于单个核函数的核方法已无法对其中的剧烈波动达到优良的拟合效果<citation id="333" type="reference"><link href="298" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>;2) 在算法实现上核矩阵逆运算会大幅增加时间成本和存储成本<citation id="334" type="reference"><link href="300" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="51">针对第一个挑战, 目前已有很多学者提出多核学习 (Multiple Kernels Learning) 方法.相对基于单个核函数的核学习方法, 多核学习将一些单核按照一定的方式组合构造成多核函数, 使基于该函数学习到的逼近模型具有更优的泛化性.Yang等<citation id="335" type="reference"><link href="302" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出将一些简单核函数线性组合成多核函数的核方法, 并应用于学习问题.邵喜高等<citation id="336" type="reference"><link href="304" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出基于多核的支持向量回归算法, 并应用于实证分析中, 以较低的计算成本取得期望的预测精度.此外, 随着多尺度分析理论的不断成熟和完善, 多尺度核方法<citation id="337" type="reference"><link href="306" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>也广泛应用于多种算法中, 目的是寻找一组具有多种尺度表示能力的核函数.汪洪桥等<citation id="338" type="reference"><link href="308" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出基于多尺度核方法的自适应序列的学习算法, 性能较优.Xu等<citation id="339" type="reference"><link href="310" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>将多尺度高斯核生成的再生核Hilbert空间的和空间作为假设空间, 提出基于和空间的最小二乘正则化回归学习算法.</p>
                </div>
                <div class="p1">
                    <p id="52">针对第二个挑战, Saunders等<citation id="340" type="reference"><link href="300" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>研究表明, 对于大规模的数据集, 核矩阵运算中矩阵求逆的实现需要<i>O</i> (<i>N</i><sup>3</sup>) 的时间成本和<i>O</i> (<i>N</i><sup>2</sup>) 的存储成本.为了克服这些困难, 学者们提出一些可扩展的逼近算法, 如近期较常采用的基于分而治之思想的分布式学习 (Distributed Learning) 方法, 降低算法的运行时间和计算存储成本, 并保证学习算法优良的表现性能.Shamir等<citation id="341" type="reference"><link href="312" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出分布式随机优化算法.Zhang等<citation id="342" type="reference"><link href="314" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出基于分布式学习方法的核岭回归算法.Mücke等<citation id="343" type="reference"><link href="316" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>对于有监督学习任务, 针对再生核Hilbert空间中的谱正则化算法, 采用并行化的分布式学习策略.此外, Lin等<citation id="344" type="reference"><link href="318" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>对基于分布式学习的最小二乘正则化回归学习算法展开理论研究.Guo等<citation id="345" type="reference"><link href="320" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>将分布式最小二乘正则化方法应用到谱算法.Guo等<citation id="346" type="reference"><link href="322" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>从分布式学习算法的偏差角度出发, 在分布式回归学习算法的基础上, 提出偏差调整正则化核网络算法.Joshi等<citation id="347" type="reference"><link href="324" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出异步的分布式学习方法, 估计平滑函数, 同时有效解决推荐系统和二分类中的矩阵分解问题.</p>
                </div>
                <div class="p1">
                    <p id="53">在上述文献中, 多核方法和分布式学习方法从不同角度改进最小二乘正则化回归算法, 分别单一地解决大数据背景下复杂数据波动剧烈和样本规模较大这两个问题, 但在实际应用中这两种问题共同存在.只利用多核学习方法的正则化学习算法依旧只能适用于一些规模较小的数据集, 一旦样本规模较大, 核矩阵的逆运算会耗费更多的运行时间和计算存储成本.只利用分布式学习方法的正则化学习算法在每个样本子集上依旧运用基于单个简单核的最小二乘正则化回归算法, 未能考虑到在每个子集上由于复杂波动程度不同而使用不同系数线性组合的多核方法进行学习, 不能完全保证优良的拟合性能.目前尚未见相关文献将核方法和分布式学习方法进行综合考虑.</p>
                </div>
                <div class="p1">
                    <p id="54">因此, 本文进一步改进最小二乘正则化回归算法, 提出基于多尺度高斯核的分布式正则化回归学习算法 (Distributed Regularized Regression Learning Algorithm Based on Multi-scale Gaussian Kernels, MKG-DRR) .该算法将样本数据划分成多个互斥的样本子集, 考虑到每个子集上的样本数据复杂波动程度不同, 在每个子集上采用不同系数线性组合成的多尺度高斯核函数, 并由多个不同尺度的高斯核生成的再生核Hilbert空间的和空间作为假设空间, 通过最小二乘正则化回归算法, 同时独立学习, 从而得到相应的局部估计, 最终将所有局部估计加权合成, 得到该函数的总体估计.在此基础上, 通过对2个模拟数据集和4个真实数据集的实验分析, 检验本文算法的表现性能.</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag">1 正则化回归学习算法</h3>
                <div class="p1">
                    <p id="57">设 (<font face="EU-HT">X</font>, <i>d</i>) 是一个紧致的度量空间, <font face="EU-HT">Y</font>∈<b>R</b>.假设<i>ρ</i>为定义在<font face="EU-HT">Z</font>∶<font face="EU-HT">X</font>×<font face="EU-HT">Y</font>上一个固定但未知的概率测度.在统计学习理论框架下, 对于一组给定且独立同分布抽取于<i>ρ</i>的样本集<i>z</i>∶={ (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) }<mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>, 回归问题的目标是寻找一个恰当的模型学习如下定义的回归函数<citation id="348" type="reference"><link href="296" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mi>ρ</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><mrow><msub><mo>∫</mo><mi mathvariant="script">Y</mi></msub><mi>y</mi></mrow></mstyle><mtext>d</mtext><mi>ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mi>x</mi><mo>∈</mo><mi mathvariant="script">X</mi><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">其中, <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ρ</mi><mo stretchy="false"> (</mo><mo>⋅</mo><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></math></mathml>为<i>ρ</i>在<i>x</i>∈<font face="EU-HT">X</font>处所诱导的条件概率测度.</p>
                </div>
                <div class="p1">
                    <p id="62">Tikhonov<citation id="349" type="reference"><link href="326" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>建立正则化思想, 为回归问题提供较好的解决方法.经典的正则化回归算法体系为</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>, </mo><mi>λ</mi></mrow></msub><mo>=</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>f</mi><mo>∈</mo><mi mathvariant="script">H</mi><msub><mrow></mrow><mi>Κ</mi></msub></mrow></munder><mo stretchy="false">{</mo><mi>ε</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi><mi>Ω</mi><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">其中, <i>λ</i>≥0为正则化参数, <i>Ω</i> (<i>f</i>) 为函数<i>f</i>的复杂度的泛函, </p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ε</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">为经验风险, <font face="EU-HT">H</font><sub><i>K</i></sub>指从<font face="EU-HT">X</font>到<font face="EU-HT">Y</font>的函数集合, 称为假设空间, 这里指文献<citation id="350" type="reference">[<a class="sup">17</a>]</citation>中定义的再生核Hilbert空间.核函数选取如下高斯核函数:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><msup><mi>x</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mfrac><mrow><mo>-</mo><mrow><mrow><mo>|</mo><mrow><mi>x</mi><mo>-</mo><msup><mi>x</mi><mo>′</mo></msup></mrow><mo>|</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">) </mo><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">其中<i>σ</i>为高斯核函数的尺度参数.</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69"><b>1.1 最小二乘正则化回归学习算法</b></h4>
                <div class="p1">
                    <p id="70">对于给定的样本集<i>z</i>, 在再生核Hilbert空间<font face="EU-HT">H</font><sub><i>K</i></sub>中, 经典的最小二乘正则化回归学习算法 (Least Square Regularized Regression, LSRR) 定义如下:</p>
                </div>
                <div class="area_img" id="71">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201907002_07100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="73">其中正则化参数<i>λ</i>&gt;0.</p>
                </div>
                <div class="p1">
                    <p id="74">根据文献<citation id="351" type="reference">[<a class="sup">18</a>]</citation>的表示定理 (Representation Theo-rem) , 式 (1) 的最优解为</p>
                </div>
                <div class="p1">
                    <p id="76"><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>Κ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>,       (2) </p>
                </div>
                <div class="p1">
                    <p id="78">其中, <i>α</i>= (<i>α</i><sub>1</sub>, <i>α</i><sub>2</sub>, …, <i>α</i><sub><i>N</i></sub>) <sup>T</sup>为适定的线性系统<b>R</b><sup><i>N</i></sup>上</p>
                </div>
                <div class="p1">
                    <p id="79"> (<i>λN</i><b><i>I</i></b>+<b><i>K</i></b>[<b><i>X</i></b>]) <i>α</i>=<b><i>y</i></b></p>
                </div>
                <div class="p1">
                    <p id="80">的唯一解, <b><i>K</i></b>[<b><i>X</i></b>]为<i>N</i>×<i>N</i>维的核矩阵, <b><i>I</i></b>为<i>N</i>×<i>N</i>维的单位矩阵, <b><i>y</i></b>= (<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>N</i></sub>) <sup>T</sup>.</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>1.2 基于多尺度核的正则化回归学习算法</b></h4>
                <div class="p1">
                    <p id="82">首先引入再生核Hilbert空间的和空间概念.</p>
                </div>
                <div class="p1">
                    <p id="83"><b>定义1</b> 如果有<i>l</i>个不同的Mercer核{<i>K</i><sub><i>t</i></sub>}, 相应生成的再生核Hilbert空间为<font face="EU-HT">H</font><sub><i>K</i><sub><i>t</i></sub></sub> (<i>t</i>=1, 2, …, <i>l</i>) , 则再生核Hilbert空间的和空间可定义为</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="script">H</mi><msub><mrow></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></msub><mo>∶</mo><mo>=</mo><mi mathvariant="script">H</mi><msub><mrow></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo>⊕</mo><mi mathvariant="script">H</mi><msub><mrow></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></msub><mrow><mo>⊕</mo><mo>⋯</mo><mo>⊕</mo><mi mathvariant="script">H</mi></mrow><msub><mrow></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mi>l</mi></msub></mrow></msub><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mo stretchy="false">{</mo><mi>f</mi><mo>∶</mo><mspace width="0.25em" /><mi>f</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mi>f</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mi>t</mi></msub><mo>∈</mo><mi mathvariant="script">H</mi><msub><mrow></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></msub><mo stretchy="false">}</mo><mo>.</mo><mspace width="0.25em" /></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">根据文献<citation id="352" type="reference">[<a class="sup">8</a>]</citation>可知, 不同的单核函数之间存在线性组合系数</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>ω</mi><mo>∈</mo><mi mathvariant="bold-italic">W</mi><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mo stretchy="false">{</mo><mrow><mo stretchy="false"> (</mo><mi>ω</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>ω</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>ω</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">|</mo><mrow><mi>min</mi></mrow><mo stretchy="false">{</mo><mi>ω</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>l</mi><mo stretchy="false">}</mo><mo>=</mo><mn>1</mn><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">使得和空间<font face="EU-HT">H</font><sub><i>K</i><sub>0</sub></sub>中的核函数<i>K</i><sub>0</sub> (<i>x</i>, <i>x</i>′) 与每个单核生成的<font face="EU-HT">H</font><sub><i>K</i><sub><i>t</i></sub></sub>中核函数<i>K</i><sub><i>t</i></sub> (<i>x</i>, <i>x</i>′) 满足</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><msup><mi>x</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mi>ω</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mi>Κ</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><msup><mi>x</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mi>x</mi><mo>∈</mo><mi>X</mi><mo>, </mo><mspace width="0.25em" /><msup><mi>x</mi><mo>′</mo></msup><mo>∈</mo><mi>X</mi><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">对于线性组合系数<i>ω</i>∈<b><i>W</i></b>, <i>f</i>∈<font face="EU-HT">H</font><sub><i>K</i><sub>0</sub></sub>, 在该和空间<font face="EU-HT">H</font><sub><i>K</i><sub>0</sub></sub>中定义范数:</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi>f</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mi>ω</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mrow><mi>min</mi></mrow><mo stretchy="false">{</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>ω</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mrow><mo>|</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mn>2</mn></msubsup><mo>∶</mo><mspace width="0.25em" /><mi>f</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mi>f</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mi>t</mi></msub><mo>∈</mo><mi mathvariant="script">H</mi><msub><mrow></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></msub><mo stretchy="false">}</mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">对于给定的样本数据集<i>z</i>, 在和空间<font face="EU-HT">H</font><sub><i>K</i><sub>0</sub></sub>中, 基于多尺度核的最小二乘正则化回归学习算法 (Least Square Regularized Regression in Sum Space, LSRR-SS) <citation id="353" type="reference"><link href="310" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>定义为</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>, </mo><mi>Κ</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></msub><mo>=</mo><mi>f</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>, </mo><mi>λ</mi><mo>, </mo><mi>Κ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mi>ω</mi></mrow></msub><mo>∶</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mspace width="0.25em" /><mtext> </mtext><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>f</mi><mo>∈</mo><mi mathvariant="script">H</mi><msub><mrow></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></msub></mrow></munder><mo stretchy="false">{</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>∈</mo><mi>z</mi></mrow></munder><mrow><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mi>λ</mi><mrow><mo>|</mo><mi>f</mi><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mi>ω</mi></mrow><mn>2</mn></msubsup><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">根据和空间及其范数的定义, 上述优化问题具有唯一解<citation id="354" type="reference"><link href="310" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="95">本文将多尺度高斯核函数中<i>l</i>个不同的尺度参数记为<i>σ</i><sub><i>t</i></sub> (<i>t</i>=1, 2, …, <i>l</i>) , 并假设<i>σ</i><sub>1</sub>&lt;<i>σ</i><sub>2</sub>&lt;…&lt;<i>σ</i><sub><i>l</i></sub>, 其对应的核函数为</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><msup><mi>x</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mfrac><mrow><mo>-</mo><mrow><mrow><mo>|</mo><mrow><mi>x</mi><mo>-</mo><msup><mi>x</mi><mo>′</mo></msup></mrow><mo>|</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mrow><mn>2</mn><mi>σ</mi></mrow><msubsup><mrow></mrow><mi>t</mi><mn>2</mn></msubsup></mrow></mfrac><mo stretchy="false">) </mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="97" name="97"><b>1.3 基于分布式学习的正则化回归学习算法</b></h4>
                <div class="p1">
                    <p id="98">Zhang等<citation id="355" type="reference"><link href="314" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出基于分布式学习的正则化回归学习算法 (Divide and Conquer Kernel Ridge Regre-ssion) (作者简称为Fast-KRR) .算法的基本思想是将数据集<i>z</i>随机划分成<i>m</i>个互斥的子集, 通过在每个子集上学习一个局部估计, 再经过加权得到总体估计函数.基本流程如图1所示.</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907002_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 分布式学习方法流程图" src="Detail/GetImg?filename=images/MSSB201907002_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 分布式学习方法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907002_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Flow chart of distributed learning method</p>

                </div>
                <div class="p1">
                    <p id="101">在Fast-KRR中, 每个样本子集<i>z</i><sub><i>s</i></sub>上的优化问题的求解方法与LSRR一致, 即在每个样本子集上的局部估计定义为</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></msub><mo>=</mo><mi>f</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub><mo>, </mo><mi>λ</mi></mrow></msub><mo>∶</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>f</mi><mo>∈</mo><mi mathvariant="script">H</mi><msub><mrow></mrow><mi>Κ</mi></msub></mrow></munder><mo stretchy="false">{</mo><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>∈</mo><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></munder><mrow><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mi>λ</mi><mrow><mo>|</mo><mi>f</mi><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mi>Κ</mi><mn>2</mn></msubsup><mo stretchy="false">}</mo><mo>, </mo><mspace width="0.25em" /></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">总体估计定义为</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>f</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>z</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mfrac><mrow><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mi>z</mi><mo>|</mo></mrow></mrow></mfrac></mrow></mstyle><mi>f</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></msub><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105">其中:<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi>z</mi><mo>|</mo></mrow></mrow></math></mathml>为样本数据集<i>z</i>的容量大小, 为<i>N</i>;<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow></mrow></math></mathml>为第<i>s</i>个样本子集<i>z</i><sub><i>s</i></sub>的容量大小, 一般设置每个样本子集的容量为大小相近, 在应用中通常取</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow><mo>=</mo><mi>Ν</mi><mo>/</mo><mi>m</mi><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <h3 id="109" name="109" class="anchor-tag">2 基于多尺度核的分布式正则化回归学习算法</h3>
                <div class="p1">
                    <p id="110">针对波动剧烈且样本规模较大的复杂数据, 本文将多尺度核方法和分布式学习方法进行有机结合, 提出基于多尺度核的分布式正则化回归学习算法 (MKG-DRR) .算法的核心思想在于:由于每个样本子集<i>z</i><sub><i>s</i></sub>中的样本均来源于同一样本总体<i>z</i>, 均处于同一度量空间中, 即输入空间, 所以在每个子集<i>z</i><sub><i>s</i></sub>上, 组成多核函数的单核函数类型及单核 参 数 需 设置一致, 使不同子集中的样本都通过核函数映射到同一特征空间中, 即由多个不同单核生成的再生核Hilbert空间的和空间中.考虑到在每个样本子集上的数据复杂波动程度有所不同, 对于本文采用的高斯核函数, 在每个样本子集上应用的多核函数是由多个不同尺度的高斯核函数、不同组合系数线性组合构造而成.在对样本数据集进行分布式学习的同时, 多尺度核之间的组合系数随不同样本子集进行动态变化.通过改进, 可以让多尺度核函数具有一定的自适应能力, 学习到数据的复杂变化趋势, 且并行式同步学习样本可切实地提高算法的运行效率和减少计算机执行所需的存储成本.</p>
                </div>
                <div class="p1">
                    <p id="111">对于给定的样本<i>z</i>, 在和空间<font face="EU-HT">H</font><sub><i>K</i><sub>0</sub></sub>中, 基于多尺度核的分布式最小二乘正则化回归学习算法, 在每个子集<i>z</i><sub><i>s</i></sub>上的局部估计定义如下:</p>
                </div>
                <div class="p1">
                    <p id="112"><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub><mo>, </mo><mi>Κ</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></msub><mo>∶</mo><mo>=</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>f</mi><msubsup><mrow></mrow><mi>s</mi><mi>t</mi></msubsup><mo>∈</mo><mi mathvariant="script">H</mi><msub><mrow></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></msub></mrow></mstyle><mrow><mi>t</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>l</mi></mrow></munder></mrow></munder><mo stretchy="false">{</mo><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>∈</mo><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></munder><mrow><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mi>f</mi></mstyle><msup><mrow></mrow><mspace width="0.25em" /></msup><mi>t</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>v</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mi>s</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mrow><mrow><mo>|</mo><mrow><mi>f</mi><mspace width="0.25em" /><msubsup><mrow></mrow><mi>s</mi><mi>t</mi></msubsup></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mn>2</mn></msubsup><mo stretchy="false">}</mo></mrow></math></mathml>,       (3) </p>
                </div>
                <div class="p1">
                    <p id="114">其中</p>
                </div>
                <div class="p1">
                    <p id="115"><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>×</mo><mi>m</mi></mrow></msub><mo>=</mo><mo stretchy="false">{</mo><mi>v</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mi>s</mi></mrow></msub><mo>≥</mo><mn>1</mn><mo stretchy="false">|</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>l</mi></mrow></math></mathml>;<i>s</i>=1, 2, …, <i>m</i>}</p>
                </div>
                <div class="p1">
                    <p id="117">为多尺度组合矩阵系数.总体估计定义为</p>
                </div>
                <div class="p1">
                    <p id="118"><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>f</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>z</mi><mo>, </mo><mi>Κ</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mfrac><mrow><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mi>z</mi><mo>|</mo></mrow></mrow></mfrac></mrow></mstyle><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub><mo>, </mo><mi>Κ</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></msub></mrow></math></mathml>.       (4) </p>
                </div>
                <div class="p1">
                    <p id="120">根据式 (2) 易知, 每个样本子集上的优化问题式 (3) 的最优解为</p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula">
                        <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi>f</mi><msup><mrow></mrow><mspace width="0.25em" /></msup><mi>t</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow></mrow></munderover><mi>α</mi></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>s</mi></mrow><mi>t</mi></msubsup><mi>Κ</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="122">其中</p>
                </div>
                <div class="p1">
                    <p id="123"><i>α</i><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>s</mi><mi>t</mi></msubsup></mrow></math></mathml>= (<i>α</i><sup><i>t</i></sup><sub>1, <i>s</i></sub>, <i>α</i><sup><i>t</i></sup><sub>2, <i>s</i></sub>, …, <i>α</i><sup><i>t</i></sup><sub>|<i>z</i><sub><i>s</i></sub>|, <i>s</i></sub>) <sup>T</sup></p>
                </div>
                <div class="p1">
                    <p id="125">为适定的线性系统<b>R</b><sup><i>lN</i></sup>上方程</p>
                </div>
                <div class="p1">
                    <p id="126" class="code-formula">
                        <mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>λ</mi><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow><mrow><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>, </mo><mi>s</mi></mrow></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mi>s</mi></msub><mo>+</mo><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">]</mo></mtd><mtd><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">]</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">]</mo></mtd><mtd><mi>λ</mi><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow><mrow><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mrow><mn>2</mn><mo>, </mo><mi>s</mi></mrow></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mi>s</mi></msub><mo>+</mo><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">]</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">]</mo></mtd><mtd><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">]</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>λ</mi><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow><mrow><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>s</mi></mrow></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mi>s</mi></msub><mo>+</mo><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">[</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">]</mo></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtable><mtr><mtd><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">α</mi><msubsup><mrow></mrow><mi>s</mi><mn>1</mn></msubsup></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">α</mi><msubsup><mrow></mrow><mi>s</mi><mn>2</mn></msubsup></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">α</mi><msubsup><mrow></mrow><mi>s</mi><mi>l</mi></msubsup></mtd></mtr></mtable><mo>]</mo></mrow></mtd><mtd></mtd><mtd></mtd></mtr></mtable><mo>=</mo><mtable><mtr><mtd><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mspace width="0.25em" /></mtd><mtd></mtd><mtd></mtd></mtr></mtable><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="127">的唯一解, <b><i>K</i></b><sub><i>t</i></sub>[<b><i>X</i></b><sub><i>s</i></sub>]为在第<i>s</i>个子集上关于第<i>t</i>个单核函数的核矩阵, <b><i>I</i></b><sub><i>s</i></sub>为<mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow><mo>×</mo><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow></mrow></math></mathml>维的单位矩阵, <mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub><mo>=</mo><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>y</mi><msub><mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></math></mathml>为第s个子集中<mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow><mo>×</mo><mn>1</mn></mrow></math></mathml>维的矩阵.</p>
                </div>
                <div class="p1">
                    <p id="131"><i>MKG</i>-<i>DRR</i>的总体最优解可表示如下:</p>
                </div>
                <div class="area_img" id="132">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201907002_13200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="293">MKG-DRR的具体实现过程描述如下．</p>
                </div>
                <div class="area_img" id="294">
                                <img alt="" src="Detail/GetImg?filename=images/MSSB201907002_29400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="294">
                                <img alt="" src="Detail/GetImg?filename=images/MSSB201907002_29401.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="294">
                                <img alt="" src="Detail/GetImg?filename=images/MSSB201907002_29402.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="159">MKG-DRR是对LSRR的一种推广, 汲取LSRR、LSRR-SS和Fast-KRR的特性和优势.对于规模较大且分布不平坦的复杂数据, MKG-DRR快速做出优良的估计或预测, 尽可能有效地拟合波动剧烈的非线性趋势.但是, 相比LSRR、LSRR-SS和Fast-KRR, </p>
                </div>
                <div class="p1">
                    <p id="160">MKG-DRR的参数数量明显增加, 主要是关于多个高斯核的尺度参数及在每个样本子集上多个单核函数之间的组合系数.在实际应用中, 为了找到一个最佳的函数估计, MKG-DRR在选择最优参数过程中需进行较多的调参工作.</p>
                </div>
                <h3 id="161" name="161" class="anchor-tag">3 实验及结果分析</h3>
                <div class="p1">
                    <p id="162">为了检验MKG-DRR的表现性能, 本文通过2组模拟数据集和4组真实数据集展开实验分析.计算机配置为:Intel (R) Core (TM) i5-7200CPU@ 2.50 GHz、4 GB内存.实验程序由Microsoft Windows 10操作系统上的R 3.4.3实现.实验选取均方误差 (Mean Square Error, MSE) 作为衡量模型拟合性能的评价指标:</p>
                </div>
                <div class="p1">
                    <p id="163" class="code-formula">
                        <mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>S</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="164">其中, <i>N</i>为训练集的样本量, <mathml id="165"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>y</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>为模型的预测值, <i>y</i><sub><i>i</i></sub>为样本的真实值.</p>
                </div>
                <div class="p1">
                    <p id="166">在训练过程中, 75%的样本作为训练集, 用于训练模型, 其余25%的样本作为测试集, 用于评估训练结果.在训练集上采用10-折交叉验证进行调参.在每次实验中由于样本划分存在一定随机性, 可能会导致实验结果之间产生略微偏差, 因此对每个训练模型均按上述步骤重复进行20次实验.最终, 将20次重复实验的训练误差、测试误差和运行时间的均值作为每组实验中每种算法的实验结果.</p>
                </div>
                <div class="p1">
                    <p id="167">本文涉及的算法运行时间是指进行1次实验的整个算法的运行时间, 单位为s.对于MKG-DRR, 其包含<i>m</i>个样本子集的划分、同时学习得到局部估计及合成得到总体估计的运行时间, 即包含step 2～step 12的运行时间.</p>
                </div>
                <h4 class="anchor-tag" id="168" name="168"><b>3.1 模拟数据实验</b></h4>
                <div class="p1">
                    <p id="169">本文首先设置2组模拟数据, 对比MKG-DRR、LSRR、LSRR-SS和Fast-KRR.2组模拟数据具体信息如下.</p>
                </div>
                <div class="p1">
                    <p id="170">1) 模拟数据1:</p>
                </div>
                <div class="p1">
                    <p id="171"><i>f</i><sup>*</sup><sub>1</sub> (<i>x</i>) =sin (5<i>x</i>) +<i>e</i><sup>-500<i>x</i><sup>2</sup></sup>+<i>x</i><sup>3</sup>, -1≤<i>x</i>≤1.</p>
                </div>
                <div class="p1">
                    <p id="172">模拟数据1的样本集为{ (<i>x</i><sub><i>i</i></sub>, <i>f</i><sup>*</sup><sub>1</sub> (<i>x</i><sub><i>i</i></sub>) +<i>ε</i><sub><i>i</i></sub>) }<mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>.其中:<i>x</i><sub><i>i</i></sub>为由[-1, 1]的均匀分布独立生成;<i>ε</i><sub><i>i</i></sub>为噪声, 是由[-0.01, 0.01]的均匀分布独立生成;样本容量<i>N</i>设置为{2<sup><i>i</i></sup>, <i>i</i>=10, 11, …, 14}.</p>
                </div>
                <div class="p1">
                    <p id="174">2) 模拟数据2:</p>
                </div>
                <div class="p1">
                    <p id="175"><i>f</i><sup>*</sup><sub>2</sub> (<i>x</i>, <i>y</i>) =0.4sin<i>x</i>+0.4cos<i>y</i>+0.2sin (20<i>xy</i>) , </p>
                </div>
                <div class="p1">
                    <p id="176">-π≤<i>x</i>≤π, π≤<i>y</i>≤π.</p>
                </div>
                <div class="p1">
                    <p id="177">模拟数据2的样本集为</p>
                </div>
                <div class="p1">
                    <p id="178">{ ( (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) , <i>f</i><sup>*</sup><sub>2</sub> (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) +<i>ε</i><sub><i>i</i></sub>) }<mathml id="179"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="180">其中:<i>x</i><sub><i>i</i></sub>和<i>y</i><sub><i>i</i></sub>均由[-π, π]的均匀分布独立生成;<i>ε</i><sub><i>i</i></sub>为噪声, 由[-0.1, 0.1]的均匀分布独立生成;样本容量<i>N</i>设置为2<sup>13</sup>.</p>
                </div>
                <div class="p1">
                    <p id="181">4种学习算法的参数选择的具体设置如下.</p>
                </div>
                <div class="p1">
                    <p id="182">1) LSRR.通过网格搜索 (Grid Search) 进行调参, 范围如下:</p>
                </div>
                <div class="p1">
                    <p id="183" class="code-formula">
                        <mathml id="183"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>σ</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>2</mn><msup><mrow></mrow><mi>i</mi></msup><mo>, </mo><mi>i</mi><mo>=</mo><mo>-</mo><mn>6</mn><mo>, </mo><mo>-</mo><mn>5</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>6</mn><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr><mtr><mtd><mi>λ</mi><mo>∈</mo><mo stretchy="false">{</mo><mrow><mn>1</mn><mn>0</mn></mrow><msup><mrow></mrow><mi>i</mi></msup><mo>, </mo><mi>i</mi><mo>=</mo><mo>-</mo><mn>7</mn><mo>, </mo><mo>-</mo><mn>6</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>3</mn><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="184">2) LSRR-SS.通过网格搜索进行调参, 范围如下:</p>
                </div>
                <div class="p1">
                    <p id="185" class="code-formula">
                        <mathml id="185"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>l</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mn>3</mn><mo>, </mo><mn>4</mn><mo>, </mo></mtd></mtr><mtr><mtd><mi>σ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>∈</mo><mo stretchy="false">{</mo><mn>2</mn><msup><mrow></mrow><mi>i</mi></msup><mo>, </mo><mi>i</mi><mo>=</mo><mo>-</mo><mn>6</mn><mo>, </mo><mo>-</mo><mn>5</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>6</mn><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr><mtr><mtd><mi>λ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>∈</mo><mo stretchy="false">{</mo><mrow><mn>1</mn><mn>0</mn></mrow><msup><mrow></mrow><mi>i</mi></msup><mo>, </mo><mi>i</mi><mo>=</mo><mo>-</mo><mn>7</mn><mo>, </mo><mo>-</mo><mn>6</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>3</mn><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="186">将优化问题中正则化项</p>
                </div>
                <div class="p1">
                    <p id="187" class="code-formula">
                        <mathml id="187"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mrow><mo stretchy="false"> (</mo><mi>ω</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mrow><mrow><mo>|</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mn>2</mn></msubsup></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="188">的参数合并, 记为正则化组合系数</p>
                </div>
                <div class="p1">
                    <p id="189">{<i>λ</i><sub><i>t</i></sub>∶=<i>λ</i> (<i>ω</i><sub><i>t</i></sub>) <sup>2</sup>, <i>t</i>=1, 2, …, <i>l</i>}, </p>
                </div>
                <div class="p1">
                    <p id="190">可以减少调参的工作量和计算的复杂度.另外, 考虑到计算机配置条件和计算复杂度等问题, <i>l</i>的取值范围设置较小.</p>
                </div>
                <div class="p1">
                    <p id="191">3) Fast-KRR.通过网格搜索进行调参, 范围如下:</p>
                </div>
                <div class="p1">
                    <p id="192" class="code-formula">
                        <mathml id="192"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>σ</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>2</mn><msup><mrow></mrow><mi>i</mi></msup><mo>, </mo><mi>i</mi><mo>=</mo><mo>-</mo><mn>6</mn><mo>, </mo><mo>-</mo><mn>7</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>6</mn><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr><mtr><mtd><mi>m</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>4</mn><msup><mrow></mrow><mi>i</mi></msup><mo>, </mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>1</mn><mo>.</mo><mn>5</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>3</mn><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr><mtr><mtd><mi>λ</mi><mo>∈</mo><mo stretchy="false">{</mo><mrow><mn>1</mn><mn>0</mn></mrow><msup><mrow></mrow><mi>i</mi></msup><mo>, </mo><mi>i</mi><mo>=</mo><mo>-</mo><mn>7</mn><mo>, </mo><mo>-</mo><mn>6</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>3</mn><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="193">每个样本子集<i>z</i><sub><i>j</i></sub>的容量大小通常取<i>N</i>/<i>m</i>.另外, 根据文献<citation id="356" type="reference">[<a class="sup">13</a>]</citation>的Corollary 4, 当分布式学习中样本划分个数<i>m</i>和样本量<i>N</i>之间尽量满足<i>m</i>≲<i>N</i><sup>1/3</sup>时, 算法的误差能达到最优的收敛性能, 这对设置参数取值范围具有重要的参考意义.</p>
                </div>
                <div class="p1">
                    <p id="194"> (4) MKG-DRR.采用网格搜索和随机搜索结合的参数搜索方法进行调参, 范围如下:</p>
                </div>
                <div class="p1">
                    <p id="195" class="code-formula">
                        <mathml id="195"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>l</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mn>3</mn><mo>, </mo><mn>4</mn><mo>, </mo></mtd></mtr><mtr><mtd><mi>σ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>∈</mo><mo stretchy="false">{</mo><mn>2</mn><msup><mrow></mrow><mi>i</mi></msup><mo>, </mo><mspace width="0.25em" /><mi>i</mi><mo>=</mo><mo>-</mo><mn>6</mn><mo>, </mo><mo>-</mo><mn>5</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>6</mn><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr><mtr><mtd><mi>m</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>4</mn><msup><mrow></mrow><mi>i</mi></msup><mo>, </mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>1</mn><mo>.</mo><mn>5</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>4</mn><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr><mtr><mtd><mi>λ</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∈</mo><mo stretchy="false">{</mo><mrow><mn>1</mn><mn>0</mn></mrow><msup><mrow></mrow><mi>i</mi></msup><mo>, </mo><mi>i</mi><mo>=</mo><mo>-</mo><mn>7</mn><mo>, </mo><mo>-</mo><mn>6</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>3</mn><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="196">将优化问题中正则化项</p>
                </div>
                <div class="p1">
                    <p id="197" class="code-formula">
                        <mathml id="197"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>v</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mi>s</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mrow><mrow><mo>|</mo><mrow><mi>f</mi><mspace width="0.25em" /><msubsup><mrow></mrow><mi>s</mi><mi>t</mi></msubsup></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mn>2</mn></msubsup></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="198">的参数合并, 记为正则化组合系数{<i>λ</i><sub><i>k</i></sub>}<mathml id="199"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>l</mi><mo>×</mo><mi>m</mi></mrow></msubsup></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="200">对于模拟数据1, 选取不同大小的样本, 对LSRR、LSRR-SS (<i>l</i>=2) 、Fast-KRR (<i>m</i>=8) 和MKG-DRR (<i>l</i>=2, <i>m</i>=8) 的表现性能进行对比分析.为了直观了解选取的模拟数据1, 图2为一维的模拟数据集1在[0, 1]上的分布.从图2中可看出, 该数据集分布不平坦, 既有平缓变化的光滑部分, 又有剧烈变化的陡峭部分.</p>
                </div>
                <div class="p1">
                    <p id="202">表1为4种算法对不同样本数量的测试误差和运行时间.</p>
                </div>
                <div class="area_img" id="203">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907002_203.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 模拟数据集1的分布" src="Detail/GetImg?filename=images/MSSB201907002_203.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 模拟数据集1的分布  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907002_203.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Distribution of simulated dataset 1</p>

                </div>
                <div class="area_img" id="204">
                    <p class="img_tit">表1 4种算法在模拟数据1上的实验结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Experimental results of 4 algorithms on simulated data 1</p>
                    <p class="img_note"></p>
                    <table id="204" border="1"><tr><td rowspan="2"><br /><i>N</i></td><td colspan="2"><br />LSRR</td><td colspan="2"><br />LSRR-SS</td><td colspan="2"><br />Fast-KRR</td><td colspan="2"><br />MKG-DRR</td></tr><tr><td><br />测试误差</td><td>运行时间/s</td><td><br />测试误差</td><td>运行时间/s</td><td><br />测试误差</td><td>运行时间/s</td><td><br />测试误差</td><td>运行时间/s</td></tr><tr><td>2<sup>10</sup></td><td>0.003357</td><td>0.790</td><td>0.000452</td><td>3.420</td><td>0.004935</td><td>0.041</td><td>0.000716</td><td>0.107</td></tr><tr><td>2<sup>11</sup></td><td>0.002825</td><td>2.201</td><td>0.000370</td><td>23.023</td><td>0.004024</td><td>0.106</td><td>0.000593</td><td>0.476</td></tr><tr><td><br />2<sup>12</sup></td><td>0.002446</td><td>11.494</td><td>0.000326</td><td>186.880</td><td>0.003666</td><td>0.485</td><td>0.000387</td><td>3.209</td></tr><tr><td><br />2<sup>13</sup></td><td>0.002220</td><td>80.840</td><td>0.000248</td><td>2673.840</td><td>0.003241</td><td>2.374</td><td>0.000277</td><td>22.672</td></tr><tr><td><br />2<sup>14</sup></td><td>0.001978</td><td>1235.320</td><td>Fail</td><td>Fail</td><td>0.002700</td><td>24.684</td><td>0.000239</td><td>181.360</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="205">由表1可见, LSRR-SS性能明显优于单核LSRR.但是, LSRR-SS由于多核矩阵的逆运算会耗费较大的运行成本, 运行时间明显长于LSRR, 尤其随样本量的增大, 两者的差距也越来越大.在引入分布式学习框架后, Fast-KRR训练误差和测试误差都略高于LSRR.在算法的运行时间上, Fast-KRR耗时均远少于LSRR.随着样本量的增大, 引入分布式学习方法增大的误差幅度越来越小, 而降低的时间成本也相应地越来越多.当<i>N</i>=2<sup>14</sup>时, Fast-KRR的运行时间仅需24.684 s, 为LSRR运行时间的1/5, 而Fast-KRR的测试误差只比LSRR增加0.000 7左右.相比发现运行效率大幅度提高.结合多核方法与分布式学习方法之后, MKG-DRR的训练误差和测试误差均大于LSRR-SS, 但随着样本量的增大, 两者误差的差距逐渐变小, MKG-DRR的拟合效果越来越优.同时, 相比LSRR-SS, MKG-DRR运行时间明显降低, 随着样本量增大, 降低幅度逐渐变大.当<i>N</i>=2<sup>13</sup>时, 两者的误差仅相差0.000 03左右, 拟合效果非常接近, 而MKG-DRR的运行时间仅占MK-LSRR的1/10, 只需22.672 s即可实现.</p>
                </div>
                <div class="p1">
                    <p id="206">由此可见, 对于分布不平坦且规模较大的复杂数据集, MKG-DRR在保证多核方法带来的优良拟合性的基础上, 分布式学习方法使运行效率得到有效提升.在实际应用中, MKG-DRR结合两种算法的优势, 更易于实现且具有较优的表现性能.</p>
                </div>
                <div class="p1">
                    <p id="207">另外, 通过模拟数据集1, 进一步分析MKG-DRR在不同样本量下, 当样本划分个数<i>m</i>不同时, 拟合误差和运行时间的实验结果, 如图3和表2所示.</p>
                </div>
                <div class="p1">
                    <p id="208">观察图3可知:当样本量<i>N</i>较小时, 随着<i>m</i>增大, MKG-DRR的训练误差和测试误差均逐渐增大;当<i>N</i>较大时, 随着<i>m</i>的增大, 拟合误差增大幅度并不明显;<i>N</i>=2<sup>14</sup>时, 随着<i>m</i>的变化拟合误差只出现小幅度的波动.</p>
                </div>
                <div class="p1">
                    <p id="209">观察表2可知, 通常<i>m</i>越大, 算法的运行时间越少, 样本量<i>N</i>增大时, 随着<i>m</i>增大, 运行时间成本降低的幅度增大.但是, 当<i>N</i>较小且<i>m</i>过大或<i>N</i>较大且<i>m</i>过小时, 样本划分不合理会增加一定的时间成本.例如当<i>N</i>=2<sup>11</sup>、<i>m</i>=16时, 运行时间小于<i>N</i>、<i>m</i>其它取值情况下的运行时间.由此可见, 对于波动剧烈且样本量较大的复杂数据, 当<i>m</i>取值较大时, MKG-DRR表现性能较佳, 在拟合能力和运行时间这两方面的综合优势较明显, 不仅保证拟合误差在可接受范围内, 还可大幅提高运行效率.</p>
                </div>
                <div class="area_img" id="295">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907002_29500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 m不同时MKG-DRR在模拟数据1上的拟合误差" src="Detail/GetImg?filename=images/MSSB201907002_29500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 m不同时MKG-DRR在模拟数据1上的拟合误差  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907002_29500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Fitting error of MKG-DRR with different values of m on simulated data 1</p>

                </div>
                <div class="area_img" id="215">
                    <p class="img_tit">表2 m不同时MKG-DRR在模拟数据1上的运行时间 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Running time of MKG-DRR with different values of m on simulated data 1</p>
                    <p class="img_note">s</p>
                    <table id="215" border="1"><tr><td><i>N</i></td><td><i>m</i>=4</td><td><i>m</i>=8</td><td><i>m</i>=16</td><td><i>m</i>=32</td><td><i>m</i>=64</td></tr><tr><td><br />2<sup>10</sup></td><td>0.237</td><td>0.107</td><td>0.122</td><td>0.188</td><td>0.361</td></tr><tr><td>2<sup>11</sup></td><td>1.670</td><td>0.476</td><td>0.216</td><td>0.224</td><td>0.376</td></tr><tr><td><br />2<sup>12</sup></td><td>13.163</td><td>3.209</td><td>0.938</td><td>0.455</td><td>0.452</td></tr><tr><td><br />2<sup>13</sup></td><td>93.313</td><td>22.672</td><td>6.823</td><td>1.862</td><td>0.951</td></tr><tr><td><br />2<sup>14</sup></td><td>547.117</td><td>181.360</td><td>51.250</td><td>14.280</td><td>4.262</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="216">对于样本量固定为2<sup>13</sup>、具有多维特征变量的模拟数据集2, 本文针对MKG-DRR的高斯核个数<i>l</i>和样本划分个数<i>m</i>, 分析两者在不同取值下对算法的拟合能力和运行时间的影响, 进一步验证MKG-DRR通过采用多核方法和分布式学习方法可使性能得到一定优化.MKG-DRR在模拟数据集2上的实验结果如表3所示.需注意的是, MKG-DRR在<i>m</i>=1时等同于LSRR-SS, 在<i>l</i>=1时等同于Fast-KRR, 在<i>m</i>=1, <i>l</i>=1时等同于LSRR.</p>
                </div>
                <div class="p1">
                    <p id="217">由表3可知, 对于规模较大且不平坦的模拟数据集2, 当<i>l</i>=2且<i>m</i>=1时, 即基于2个不同尺度高斯核的LSRR-SS, 测试误差为0.032 076, 拟合效果优于其它算法, 但需耗费大量的时间成本.而当<i>l</i>=3, <i>m</i>=4时, MKG-DRR拟合效果与之相当, 测试误差也仅为0.032 508.在保证达到较优拟合效果的前提下, 相比LSRR-SS, MKG-DRR大幅降低运行时间, 正常实现只需花费前者1/10左右的时间.</p>
                </div>
                <div class="area_img" id="218">
                    <p class="img_tit">表3 参数不同时MKG-DRR在模拟数据集2上的实验结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Experimental results of MKG-DRR with different values of parameters on simulated dataset 2</p>
                    <p class="img_note"></p>
                    <table id="218" border="1"><tr><td rowspan="2"><i>m</i></td><td colspan="3"><br /><i>l</i>=1</td><td colspan="4"><br /><i>l</i>=2</td><td colspan="3"><br /><i>l</i>=3</td><td colspan="3"><br /><i>l</i>=4</td></tr><tr><td><br />训练<br />误差</td><td>测试<br />误差</td><td>运行<br />时间/s</td><td><br />训练<br />误差</td><td colspan="2">测试<br />误差</td><td>运行<br />时间/s</td><td><br />训练<br />误差</td><td>测试<br />误差</td><td>运行<br />时间/s</td><td><br />训练<br />误差</td><td>测试<br />误差</td><td>运行<br />时间/s</td></tr><tr><td>1</td><td>0.047691</td><td>0.058871</td><td>86.610</td><td>0.021943</td><td>0.032076</td><td colspan="2">2691.352</td><td>Fail</td><td>Fail</td><td>Fail</td><td>Fail</td><td>Fail</td><td>Fail</td></tr><tr><td>4</td><td>0.048636</td><td>0.061260</td><td>7.566</td><td>0.023100</td><td>0.035159</td><td colspan="2">96.850</td><td>0.022096</td><td>0.032508</td><td>312.710</td><td>0.026580</td><td>0.038790</td><td>799.319</td></tr><tr><td><br />8</td><td>0.050982</td><td>0.063192</td><td>2.174</td><td>0.025257</td><td>0.037731</td><td colspan="2">25.854</td><td>0.024923</td><td>0.034014</td><td>77.740</td><td>0.028310</td><td>0.039480</td><td>185.502</td></tr><tr><td><br />16</td><td>0.062226</td><td>0.072209</td><td>0.736</td><td>0.034480</td><td>0.039683</td><td colspan="2">7.033</td><td>0.031892</td><td>0.036807</td><td>23.551</td><td>0.037039</td><td>0.045159</td><td>48.449</td></tr><tr><td><br />32</td><td>0.095414</td><td>0.100433</td><td>0.354</td><td>0.048757</td><td>0.051460</td><td colspan="2">2.086</td><td>0.038496</td><td>0.040242</td><td>5.153</td><td>0.053935</td><td>0.061935</td><td>13.455</td></tr><tr><td><br />64</td><td>0.133347</td><td>0.143747</td><td>0.298</td><td>0.060665</td><td>0.068699</td><td colspan="2">1.063</td><td>0.044889</td><td>0.051675</td><td>2.297</td><td>0.072730</td><td>0.087215</td><td>3.931</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="219">为了清晰直观地体现<i>l</i>和<i>m</i>对MKG-DRR拟合能力的影响, 图4给出算法在2个参数取不同值时的训练误差和测试误差.</p>
                </div>
                <div class="p1">
                    <p id="220">综合观察表3和图4可发现, 对于高斯核个数<i>l</i>, 当<i>l</i>=3时, 训练误差和测试误差均最低.对于样本划分个数<i>m</i>, 当<i>m</i>=4, 8时, 训练误差和测试误差均较低.倘若只基于多尺度核方法, 即<i>m</i>=1时, LSRR-SS的拟合效果完全优于<i>l</i>=1时的LSRR、Fast-KRR, 但LSRR-SS耗时过长, 尤其当<i>l</i>取值较大时, 本文计算机配置无法满足需要, <i>l</i>=3, 4时算法实现均失败.倘若只是基于分布式学习方法, 即<i>l</i>=1时, 随着<i>m</i>的增大, Fast-KRR可有效缩减时间成本, 但同时会带来拟合误差的增大, <i>m</i>=64时算法的训练误差和测试误差均远大于<i>m</i>=1时.</p>
                </div>
                <div class="area_img" id="226">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907002_22600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 参数不同时MKG-DRR在模拟数据集2上的拟合误差" src="Detail/GetImg?filename=images/MSSB201907002_22600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 参数不同时MKG-DRR在模拟数据集2上的拟合误差  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907002_22600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Fitting error of MKG-DRR with different values of parameters on simulated dataset 2</p>

                </div>
                <div class="p1">
                    <p id="228">综合分析表明, MKG-DRR汲取这两种算法的特点和优势, 在该实验中, 当<i>l</i>=3、<i>m</i>=16或<i>l</i>=2、<i>m</i>=8时, 相比其它3种算法, 整体表现性能均最佳.换言之, 对于样本规模较大且不平坦的数据集, 当MKG-DRR的<i>l</i>、<i>m</i>选取适当的值时:相比LSRR-SS, 在保证优良的拟合能力的基础上, 节省大量算法正常实现所需的时间成本;相比Fast-KRR, 在较短时间内达到更佳的拟合效果.</p>
                </div>
                <h4 class="anchor-tag" id="229" name="229"><b>3.2 真实数据实验</b></h4>
                <div class="p1">
                    <p id="230">本文进一步通过4组真实数据集, 对MKG-DRR展开实验分析, 并对比LSRR、LSRR-SS、Fast-KRR, 检验算法的综合表现性能.</p>
                </div>
                <div class="p1">
                    <p id="231">4组真实数据集分别为Concrete、CCPP、Protein、Wind.前三组真实数据集均来源于UCI机器学习公开数据集 (http://archive.ics.uci.edu/ml/index.php) , 第四组真实数据集来源于某风机叶片结冰预测大赛中的风速数据集 (http://www.industrial-bigdata .com/competition/competitionAction!showDetail.action?competition.competitionId=1) .4组真实数据集的基本信息如表4所示.</p>
                </div>
                <div class="area_img" id="233">
                    <p class="img_tit">表4 真实数据集描述 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Description of real datasets</p>
                    <p class="img_note"></p>
                    <table id="233" border="1"><tr><td><br />名称</td><td>样本量</td><td>特征变量个数</td><td>目标变量范围</td></tr><tr><td><br />Concrete</td><td>1030</td><td>8</td><td>[2.331, 82.599]</td></tr><tr><td><br />CCPP</td><td>9568</td><td>4</td><td>[420.26, 495.76]</td></tr><tr><td><br />Protein</td><td>15730</td><td>9</td><td>[0, 21]</td></tr><tr><td><br />Wind</td><td>20232</td><td>1</td><td>[-1.853, 5.135]</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="234">在实验过程中, 为了消除量纲影响, 对上述4组真实数据集的特征变量和目标变量都进行均值为0和标准差为1的标准化处理.对于每组数据集, 首先随机选取25%的样本作为模型的测试集, 考虑到样本量对学习算法表现性能的影响, 在剩余样本中依次选取总样本量的25%、50%、75%的样本数据作为模型的训练集.所以, 在每组数据集上分别选取3种不同大小的训练样本集用于训练模型, 再将训练得到的最优模型应用到测试集上, 通过对比4种算法在测试集上的拟合效果和运行时间, 评估<i>MKG</i>- <i>DRR</i>的表现性能.</p>
                </div>
                <div class="p1">
                    <p id="235">由于不同数据集之间样本量差距较大, 综合考虑4组真实数据集的特点和样本大小, 进一步扩大算法参数搜索范围.对于<i>LSRR</i>, 通过网格搜索进行调参, 范围如下:</p>
                </div>
                <div class="p1">
                    <p id="236" class="code-formula">
                        <mathml id="236"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mtext>σ</mtext><mo>∈</mo><mo stretchy="false">{</mo><mn>2</mn><msup><mrow></mrow><mtext>i</mtext></msup><mo>, </mo><mtext>i</mtext><mo>=</mo><mo>-</mo><mn>8</mn><mo>, </mo><mo>-</mo><mn>7</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>8</mn><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext>λ</mtext><mo>∈</mo><mo stretchy="false">{</mo><mrow><mn>1</mn><mn>0</mn></mrow><msup><mrow></mrow><mtext>i</mtext></msup><mo>, </mo><mtext>i</mtext><mo>=</mo><mo>-</mo><mn>9</mn><mo>, </mo><mo>-</mo><mn>8</mn><mo>.</mo><mn>5</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>3</mn><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="237">对于<i>LSRR</i>-<i>SS</i>, 通过网格搜索进行调参, 范围如下:</p>
                </div>
                <div class="p1">
                    <p id="238" class="code-formula">
                        <mathml id="238"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mtext>l</mtext><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mn>3</mn><mo>, </mo><mn>4</mn><mo>, </mo></mtd></mtr><mtr><mtd><mtext>σ</mtext><msub><mrow></mrow><mtext>t</mtext></msub><mo>∈</mo><mo stretchy="false">{</mo><mn>2</mn><msup><mrow></mrow><mtext>i</mtext></msup><mo>, </mo><mtext>i</mtext><mo>=</mo><mo>-</mo><mn>8</mn><mo>, </mo><mo>-</mo><mn>7</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>8</mn><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext>λ</mtext><msub><mrow></mrow><mtext>t</mtext></msub><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mn>0</mn><msup><mrow></mrow><mtext>i</mtext></msup><mo>, </mo><mtext>i</mtext><mo>=</mo><mo>-</mo><mn>9</mn><mo>, </mo><mo>-</mo><mn>8</mn><mo>.</mo><mn>5</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>3</mn><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="239">对于<i>Fast</i>-<i>KRR</i>, 通过网格搜索进行调参, 范围如下:</p>
                </div>
                <div class="p1">
                    <p id="240" class="code-formula">
                        <mathml id="240"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mtext>σ</mtext><mo>∈</mo><mo stretchy="false">{</mo><mn>2</mn><msup><mrow></mrow><mtext>i</mtext></msup><mo>, </mo><mtext>i</mtext><mo>=</mo><mo>-</mo><mn>8</mn><mo>, </mo><mo>-</mo><mn>7</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>8</mn><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext>m</mtext><mo>∈</mo><mo stretchy="false">{</mo><mn>4</mn><msup><mrow></mrow><mtext>i</mtext></msup><mo>, </mo><mtext>i</mtext><mo>=</mo><mn>1</mn><mo>, </mo><mn>1</mn><mo>.</mo><mn>5</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>4</mn><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext>λ</mtext><mo>∈</mo><mo stretchy="false">{</mo><mrow><mn>1</mn><mn>0</mn></mrow><msup><mrow></mrow><mtext>i</mtext></msup><mo>, </mo><mtext>i</mtext><mo>=</mo><mo>-</mo><mn>9</mn><mo>, </mo><mo>-</mo><mn>8</mn><mo>.</mo><mn>5</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>3</mn><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="241">对于<i>MKG</i>-<i>DRR</i>, 采用网格搜索和随机搜索结合的搜索方法进行调参, 范围如下:</p>
                </div>
                <div class="p1">
                    <p id="242" class="code-formula">
                        <mathml id="242"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mtext>l</mtext><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mn>3</mn><mo>, </mo><mn>4</mn><mo>, </mo></mtd></mtr><mtr><mtd><mtext>σ</mtext><msub><mrow></mrow><mtext>t</mtext></msub><mo>∈</mo><mo stretchy="false">{</mo><mn>2</mn><msup><mrow></mrow><mtext>i</mtext></msup><mo>, </mo><mtext>i</mtext><mo>=</mo><mo>-</mo><mn>8</mn><mo>, </mo><mo>-</mo><mn>7</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>8</mn><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext>m</mtext><mo>∈</mo><mo stretchy="false">{</mo><mn>4</mn><msup><mrow></mrow><mtext>i</mtext></msup><mo>, </mo><mtext>i</mtext><mo>=</mo><mn>1</mn><mo>, </mo><mn>1</mn><mo>.</mo><mn>5</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>4</mn><mo stretchy="false">}</mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext>λ</mtext><msub><mrow></mrow><mtext>k</mtext></msub><mo>∈</mo><mo stretchy="false">{</mo><mrow><mn>1</mn><mn>0</mn></mrow><msup><mrow></mrow><mtext>i</mtext></msup><mo>, </mo><mtext>i</mtext><mo>=</mo><mo>-</mo><mn>9</mn><mo>, </mo><mo>-</mo><mn>8</mn><mo>.</mo><mn>5</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>3</mn><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="243">本文将4种算法应用到4组不同样本量的真实数据集上, 对比分析表现性能.为了直观了解真实数据集波动剧烈的特点, 图5为其中的一维特征变量<i>Wind</i>数据集的分布.</p>
                </div>
                <div class="p1">
                    <p id="244">对于同组数据集, 在不同训练集大小的每种学习算法中, 根据相应的参数取值范围进行调参, 在尽可能短的时间内达到尽可能优的拟合效果, 以此选取最优的参数值.</p>
                </div>
                <div class="p1">
                    <p id="245">在每组数据集上, 以4种算法选取最优参数值时的表现性能作为最终实验结果, 如表5所示, 表中结果均为20次重复实验的测试误差的平均值和标准差.</p>
                </div>
                <div class="area_img" id="246">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907002_246.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 Wind数据集的风速数据分布" src="Detail/GetImg?filename=images/MSSB201907002_246.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 Wind数据集的风速数据分布  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907002_246.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Wind speed data distribution on Wind dataset</p>

                </div>
                <div class="area_img" id="247">
                    <p class="img_tit">表5 4种算法在真实数据集上的实验结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 Experimental results of 4 algorithms on real datasets</p>
                    <p class="img_note"></p>
                    <table id="247" border="1"><tr><td rowspan="2">数据集</td><td rowspan="2">训练集<br />样本量</td><td colspan="2"><br />LSRR</td><td colspan="2"><br />LSRR-SS</td><td colspan="2"><br />Fast-KRR</td><td colspan="2"><br />MKG-DRR</td></tr><tr><td><br /> 测试误差</td><td>运行时间/s</td><td><br /> 测试误差</td><td>运行时间/s</td><td><br /> 测试误差</td><td>运行时间/s</td><td><br /> 测试误差</td><td>运行时间/s</td></tr><tr><td rowspan="3"><br />Concrete<br /> (<i>N</i>=1030) </td><td>258</td><td>0.750 (0.062) </td><td>0.013</td><td>0.497 (0.049) </td><td>0.302</td><td>0.836 (0.081) </td><td>0.014</td><td>0.521 (0.077) </td><td>0.042</td></tr><tr><td>515</td><td>0.663 (0.070) </td><td>0.092</td><td>0.354 (0.058) </td><td>1.746</td><td>0.789 (0.075) </td><td>0.028</td><td>0.393 (0.064) </td><td>0.115</td></tr><tr><td>773</td><td>0.598 (0.043) </td><td>0.355</td><td>0.283 (0.040) </td><td>5.254</td><td>0.703 (0.065) </td><td>0.046</td><td>0.292 (0.041) </td><td>0.287</td></tr><tr><td rowspan="3"><br />CCPP<br /> (<i>N</i>=9568) </td><td>2392</td><td>0.086 (0.005) </td><td>6.051</td><td>0.028 (0.003) </td><td>126.290</td><td>0.210 (0.005) </td><td>0.190</td><td>0.032 (0.007) </td><td>1.462</td></tr><tr><td>4784</td><td>0.080 (0.007) </td><td>38.310</td><td>0.016 (0.004) </td><td>1241.270</td><td>0.172 (0.008) </td><td>0.948</td><td>0.019 (0.004) </td><td>5.416</td></tr><tr><td>7176</td><td>0.068 (0.006) </td><td>125.310</td><td>Fail</td><td>Fail</td><td>0.106 (0.007) </td><td>2.714</td><td>0.016 (0.006) </td><td>12.034</td></tr><tr><td rowspan="3"><br />Protein<br /> (<i>N</i>=15730) </td><td>3933</td><td>0.531 (0.028) </td><td>23.233</td><td>0.330 (0.002) </td><td>693.220</td><td>0.692 (0.025) </td><td>0.603</td><td>0.353 (0.019) </td><td>4.013</td></tr><tr><td>7865</td><td>0.480 (0.012) </td><td>285.420</td><td>Fail</td><td>Fail</td><td>0.603 (0.008) </td><td>3.550</td><td>0.312 (0.016) </td><td>16.853</td></tr><tr><td>11798</td><td>0.444 (0.019) </td><td>11798</td><td>Fail</td><td>Fail</td><td>0.548 (0.014) </td><td>8.833</td><td>0.279 (0.013) </td><td>25.337</td></tr><tr><td rowspan="3"><br />Wind<br /> (<i>N</i>=20232) </td><td>5058</td><td>0.689 (0.007) </td><td>45.312</td><td>0.288 (0.003) </td><td>1689.630</td><td>0.723 (0.005) </td><td>1.453</td><td>0.296 (0.002) </td><td>7.160</td></tr><tr><td>10116</td><td>0.679 (0.007) </td><td>401.565</td><td>Fail</td><td>Fail</td><td>0.706 (0.005) </td><td>5.921</td><td>0.282 (0.003) </td><td>21.270</td></tr><tr><td>15174</td><td>0.673 (0.006) </td><td>15174</td><td>Fail</td><td>Fail</td><td>0.691 (0.004) </td><td>12.957</td><td>0.254 (0.003) </td><td>38.770</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="248">从算法的拟合效果上看, 由表5可知, 本文选取的4组真实数据集均为分布不平坦的复杂数据, 基于多尺度核的LSRR-SS、MKG-DRR可更好地拟合剧烈的波动, 两者拟合误差较接近, 尤其对于CCPP、Wind数据集.当CCPP数据集的训练集样本量为4 784时, LSRR-SS和MKG-DRR的拟合误差分别为0.016和0.019, 均远小于利用单核的LSRR (0.080) 、Fast-KRR (0.172) , 并且前两者拟合误差的标准差也较小, 模型较稳定.当Wind数据集的训练集样本量为10 116和15 174时, MKG-DRR的误差分别为0.282和0.254, 均小于Fast-KRR的误差 (0.706和0.691) .不同尺度大小的高斯核组合的多核方法结合各个单核的特性, 明显提升对波动剧烈数据的拟合效果.</p>
                </div>
                <div class="p1">
                    <p id="249">从算法的运行时间上看, 由表5可知, 当训练集的样本量较大时, 基于分布式学习方法的Fast-KRR和MKG-DRR大幅降低运行时间.在Protein、Wind数据集上, 当训练集的样本量分别为11 798和15 174时, LSRR的实现分别需要934.740 s和2462.130 s, 耗时过多, LSRR-SS因核矩阵逆运算所需的存储成本过大而无法正常实现.而基于分布式学习方法的算法则相反, Fast-KRR在这两个训练集上运行只需8.833 s和12.957 s, MKG-DRR需25.337 s和38.770 s, 虽然MKG-DRR利用多核方法导致运行时间有所增加, 但相比LSRR、LSRR-SS, 时间在可接受范围之内.</p>
                </div>
                <div class="p1">
                    <p id="250">另外, 对于样本规模较小的Concrete数据集, 当训练集的样本量为258时, Fast-KRR的运行时间与LSRR相近, 分布式学习方法降低时间成本的优势并不明显.而从基于多核方法的算法可见, MKG-DRR运行时间小于LSRR-SS, 在多核算法中引入分布式学习可降低时间成本这一优势更明显.</p>
                </div>
                <div class="p1">
                    <p id="251">根据上述分析, 本文综合考虑拟合性能和计算成本, 尽量在达到较优的拟合效果的同时耗时较短, 分别对4组不同样本规模的真实数据集选择较适用的正则化回归学习算法, 用于估计和预测.</p>
                </div>
                <div class="p1">
                    <p id="252">对于规模较小的Concrete数据集, MKG-DRR和LSRR-SS在拟合能力和运行时间上均表现不错.当训练集较小时, LSRR-SS的运行时间在可接受范围内, 拟合性能更优.但当训练集较大时, MKG-DRR花费的时间成本明显更低, 拟合误差也更小, 更适用.对于规模中等的CCPP数据集, 多尺度核方法提升的拟合效果尤为突出, MKG-DRR和LSRR-SS的拟合误差大幅降低, 但LSRR-SS对计算机配置具有较高的要求, 可能会导致算法无法正常运行, 相比之下MKG-DRR的实用性更强, 整体表现性能更优.对于规模较大的Protein数据集, 综合对比4种学习算法, 在达到较低拟合误差的前提下, MKG-DRR可以保证算法正常运行, 缩短算法实现所需的计算时间和存储成本, 有效提高运行效率.</p>
                </div>
                <div class="p1">
                    <p id="253">对于规模较大的Wind数据集, 在图6中进一步给出4种算法的残差变化, 即预测值和真实值之差, 考虑到可视性和美观性, 给出前500个样本点的残差.从中发现, 相比LSRR和Fast-KRR, 基于多尺核的MKG-DRR和LSRR-SS的残差波动范围明显更小, 变化趋势更集中、平稳, 拟合效果更佳.相比LSRR-SS, MKG-DRR运行时间大幅降低, 在算法实现上具有更强的实用性.</p>
                </div>
                <div class="area_img" id="260">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907002_26000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 4种算法在Wind数据集上的残差对比" src="Detail/GetImg?filename=images/MSSB201907002_26000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 4种算法在Wind数据集上的残差对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907002_26000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Residual comparison of 4 algorithms on Wind dataset</p>

                </div>
                <div class="area_img" id="260">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201907002_26001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 4种算法在Wind数据集上的残差对比" src="Detail/GetImg?filename=images/MSSB201907002_26001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 4种算法在Wind数据集上的残差对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201907002_26001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Residual comparison of 4 algorithms on Wind dataset</p>

                </div>
                <div class="p1">
                    <p id="261">本文通过上述4个真实数据集体现MKG-DRR的优势.对于样本规模较大且不平坦的复杂数据, MKG-DRR在保证达到较佳拟合效果的前提下, 花费较低的计算成本.</p>
                </div>
                <div class="p1">
                    <p id="262">相比LSRR-SS, 虽然MKG-DRR由于分布式学习导致拟合误差略有增加, 但增加幅度很小, 在可接受范围之内, 并且节省大量的计算时间成本, 在实际应用中更易实现.相比Fast-KRR, 虽然MKG-DRR利用多核方法导致运行时间略有延长, 但延长的时间很短, 在可允许范围内, 并且达到更佳的拟合效果.所以, MKG-DRR的综合表现性能优于已有的正则化回归学习算法.</p>
                </div>
                <h3 id="263" name="263" class="anchor-tag">4 结 束 语</h3>
                <div class="p1">
                    <p id="264">在回归问题中, 对于波动剧烈且样本规模较大的复杂数据, 针对多核算法和分布式学习算法只解决单一问题的局限性, 本文提出基于多尺度核的分布式正则化回归学习算法 (MKG-DRR) , 同时有效解决剧烈波动的拟合性能和计算实现的时间成本.算法着重考虑分布式学习中样本数据集划分的每个互斥子集上的数据复杂波动程度不同, 采用不同系数线性组合的多尺度高斯核函数, 同时独立学习, 并使用网格搜索和随机搜索结合的参数搜索方法, 使其在拟合能力和计算实现上具有更强的实用性.本文通过2组模拟数据集和4组真实数据集实验验证MKG-DRR能在尽可能短的运行时间内达到尽可能优的拟合效果, 不但保证由多尺度高斯核方法带来的优良拟合能力, 而且由于分布式学习方法的引入成功降低算法的运行时间和存储成本.在实际应用中, 如在时间序列动态预测、工业设备故障预报、生物信息预测及信号波动解析等领域和场景中, 对于一些规模较大且波动剧烈的复杂数据, 可以快速做出优良的估计或预测.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="292" type="" href="images/MSSB201907002_29200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">王洁微</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="296">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJAM&amp;filename=SJAM120524050515&amp;v=MzI3NDAzeEU5ZmJ2bktyaWZadTl1RkN2c1U3dk1KMTRRTmlmS1k3SzZIdFRPcTQ5QVpPNE9DUk04enhVU21EZDlTSDdu&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> CUCKER F, SMALE S.On the Mathematical Foundations of Lear-ning.Bulletin of the American Mathematical Society, 2001, 39 (1) :1-49.
                            </a>
                        </p>
                        <p id="298">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501913096&amp;v=MTg5MTN0RE5xbzlFYmVvTURIVS9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRm9UYWhZPU5pZk9mYks3SA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> ZHENG D N, WANG J X, ZHAO Y N.Nonflat Function Estimation with a Multi-scale Support Vector Regression.Neurocomputing, 2006, 70 (1/2/3) :420-429.
                            </a>
                        </p>
                        <p id="300">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ridge regression learning algorithm in dual variables">

                                <b>[3]</b> SAUNDERS C, GAMMERMAN A, VOVK V.Ridge Regression Learning Algorithm in Dual Variables // Proc of the 15th International Conference on Machine Learning.New York, USA:ACM, 1998:515-521.
                            </a>
                        </p>
                        <p id="302">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient Sparse Generalized Multiple Kernel Learning">

                                <b>[4]</b> YANG H Q, XU Z L, YE J P, et al.Efficient Sparse Generalized Multiple Kernel Learning.IEEE Transactions on Neural Networks, 2011, 22 (3) :433-446.
                            </a>
                        </p>
                        <p id="304">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1013310489.nh&amp;v=MTc5MjBGckNVUkxPZVplUm5GeXpoVkwvTVZGMjZIYkM1SHRYRXBwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 邵喜高.基于统计学习理论的多核预测模型研究及应用.博士学位论文.长沙:中南大学, 2013. (SHAO X G.The Research and Application of Multiple Kernel Prediction Model Based on Statistical Learning Theory.Ph.D.Dissertation.Changsha, China:Central South University, 2013.) 
                            </a>
                        </p>
                        <p id="306">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-scale kernel methods for classification">

                                <b>[6]</b> KINGSBURY N, TAY D, PALANISWAMI M.Multi-scale Kernel Methods for Classification // Proc of the IEEE Workshop on Machine Learning for Signal Processing.Washington, USA:IEEE Press, 2005:43-48.
                            </a>
                        </p>
                        <p id="308">
                            <a id="bibliography_7" >
                                    <b>[7]</b>
                                 汪洪桥, 蔡艳宁, 孙富春, 等.多尺度核方法的自适应序列学习及应用.模式识别与人工智能, 2011, 24 (1) :72-81. (WANG H Q, CAI Y N, SUN F C.Adaptive Sequence Learning and Applications for Multi-scale Kernel Method.Pattern Recognition and Artificial Intelligence, 2011, 24 (1) :72-81.) 
                            </a>
                        </p>
                        <p id="310">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Least Square Regularized Regression in Sum Space">

                                <b>[8]</b> XU Y L, CHEN D R, LI H X, et al.Least Square Regularized Regression in Sum Space.IEEE Transactions on Neural Networks and Learning Systems, 2013, 24 (4) :635-646.
                            </a>
                        </p>
                        <p id="312">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributed Stochastic Optimization and Learning[C/OL]">

                                <b>[9]</b> SHAMIR O, SREBRO N.Distributed Stochastic Optimization and Learning[C/OL].[2018-11-12].https://arxiv.org/pdf/1408.5294.pdf.
                            </a>
                        </p>
                        <p id="314">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Divide and conquer kernel ridge regression:A distributed algorithm with minimax optimal rates">

                                <b>[10]</b> ZHANG Y C, DUCHI J, WAINWRIGHT M.Divide and Conquer Kernel Ridge Regression:A Distributed Algorithm with Minimax Optimal Rates.Journal of Machine Learning Research, 2015, 16:3299-3340.
                            </a>
                        </p>
                        <p id="316">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=mlr:Machine Learning in R">

                                <b>[11]</b> MÜCKE N, BLANCHARD G.Parallelizing Spectral Algorithms for Kernel Learning.Journal of Machine Learning Research, 2016, 19:1-29.
                            </a>
                        </p>
                        <p id="318">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributed Learning with Regula-rized Least Squares">

                                <b>[12]</b> LIN S B, GUO X, ZHOU D X.Distributed Learning with Regula-rized Least Squares.Journal of Machine Learning Research, 2017, 18:1-31.
                            </a>
                        </p>
                        <p id="320">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SIPD&amp;filename=SIPD2C33BC30127A6785CE48F34C0735CA34&amp;v=MTAzODFiUTM1TkZod0x5NHdLdz1OaVRiYXJITEhkSyszSXhGWmVrSWZYbyt4eE5nbnp0MVBuem0zeEl5ZXJmbk5MbWJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> GUO Z C, LIN S B, ZHOU D X.Learning Theory of Distributed Spectral Algorithm.Inverse Problems, 2017, 33 (7) .DOI:10.1088/1361-6420/aa72b2.
                            </a>
                        </p>
                        <p id="322">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Theory of Distributed Regre-ssion with Bias Corrected Regularization Kernel Network">

                                <b>[14]</b> GUO Z C, SHI L, WU Q.Learning Theory of Distributed Regre-ssion with Bias Corrected Regularization Kernel Network.Journal of Machine Learning Research, 2017, 18:1-25.
                            </a>
                        </p>
                        <p id="324">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-Scale Asynchronous Distributed Learning Based on Parameter Exchanges">

                                <b>[15]</b> JOSHI B, IUTZELER F, AMINI M R.Large-Scale Asynchronous Distributed Learning Based on Parameter Exchanges[J/OL].[2018-11-12].https://arxiv.org/pdf/1705.07751.pdf.
                            </a>
                        </p>
                        <p id="326">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Regularization of incorrectly posed problems">

                                <b>[16]</b> TIKHONOV A N.Regularization of Incorrectly Posed Problems.Soviet Mathematics Doklady, 1963, 4 (1) :1624-1627.
                            </a>
                        </p>
                        <p id="328">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJAM&amp;filename=SJAM120524013474&amp;v=MDMyMjR6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2c1U3dk1KMTRRTmlmS1k3SzZIdFRPcTQ5RVorOElDQk04&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> ARONSZAJN N.Theory of Reproducing Kernels.Transactions of the American Mathematical Society, 1950, 68 (3) :337-404.
                            </a>
                        </p>
                        <p id="330">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Theory:An Approximation Theory Viewpoint">

                                <b>[18]</b> CUCKER F, ZHOU D X.Learning Theory:An Approximation Theory Viewpoint.Cambridge, UK:Cambridge University Press, 2007.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201907002" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201907002&amp;v=MDE3OTlNS0Q3WWJMRzRIOWpNcUk5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5emhWTC8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
