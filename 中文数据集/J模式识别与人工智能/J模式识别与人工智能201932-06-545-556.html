<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131451173467500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201906008%26RESULT%3d1%26SIGN%3d5wdwZ9FFCRR0lHCCwibxn55olqw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201906008&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201906008&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201906008&amp;v=MTAyNjc0SDlqTXFZOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpoVTd2T0tEN1liTEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#93" data-title="1 安全样本筛选方法 ">1 安全样本筛选方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#94" data-title="&lt;b&gt;1.1&lt;/b&gt; 形式化描述"><b>1.1</b> 形式化描述</a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;1.2&lt;/b&gt; 安全样本筛选思路和规则"><b>1.2</b> 安全样本筛选思路和规则</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#146" data-title="2 基于安全样本筛选的不平衡数据抽样 ">2 基于安全样本筛选的不平衡数据抽样</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#147" data-title="&lt;b&gt;2.1&lt;/b&gt; 算法步骤"><b>2.1</b> 算法步骤</a></li>
                                                <li><a href="#180" data-title="&lt;b&gt;2.2&lt;/b&gt; 安全样本筛选对训练集的影响"><b>2.2</b> 安全样本筛选对训练集的影响</a></li>
                                                <li><a href="#184" data-title="&lt;b&gt;2.3&lt;/b&gt; 时间复杂度分析"><b>2.3</b> 时间复杂度分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#186" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#187" data-title="&lt;b&gt;3.1&lt;/b&gt; 实验数据集"><b>3.1</b> 实验数据集</a></li>
                                                <li><a href="#192" data-title="&lt;b&gt;3.2&lt;/b&gt; 度量方式"><b>3.2</b> 度量方式</a></li>
                                                <li><a href="#199" data-title="&lt;b&gt;3.3&lt;/b&gt; 安全样本筛选的实验结果"><b>3.3</b> 安全样本筛选的实验结果</a></li>
                                                <li><a href="#231" data-title="&lt;b&gt;3.4&lt;/b&gt; 基于抽样数据集的分类性能实验"><b>3.4</b> 基于抽样数据集的分类性能实验</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#250" data-title="4 结 束 语 ">4 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#123" data-title="图1 3种类型实例的示例">图1 3种类型实例的示例</a></li>
                                                <li><a href="#190" data-title="&lt;b&gt;表1 KEEL不平衡数据库&lt;/b&gt;"><b>表1 KEEL不平衡数据库</b></a></li>
                                                <li><a href="#191" data-title="&lt;b&gt;表2 LIBSVM不平衡数据库&lt;/b&gt;"><b>表2 LIBSVM不平衡数据库</b></a></li>
                                                <li><a href="#295" data-title="图2 λ值与对应的筛选数据集的不平衡比率">图2 λ值与对应的筛选数据集的不平衡比率</a></li>
                                                <li><a href="#295" data-title="图2 λ值与对应的筛选数据集的不平衡比率">图2 λ值与对应的筛选数据集的不平衡比率</a></li>
                                                <li><a href="#224" data-title="图3 5个不平衡数据集筛选前后不平衡比率对比">图3 5个不平衡数据集筛选前后不平衡比率对比</a></li>
                                                <li><a href="#230" data-title="&lt;b&gt;表3 LIBSVM原始数据集和筛选数据集的实例个数&lt;/b&gt;"><b>表3 LIBSVM原始数据集和筛选数据集的实例个数</b></a></li>
                                                <li><a href="#247" data-title="&lt;b&gt;表4 不同方法下SVM分类器的G-mean值&lt;/b&gt;"><b>表4 不同方法下SVM分类器的G-mean值</b></a></li>
                                                <li><a href="#248" data-title="&lt;b&gt;表5 不同方法下SVM分类器的AUC值&lt;/b&gt;"><b>表5 不同方法下SVM分类器的AUC值</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="296">


                                    <a id="bibliography_1" title="TSANG S, KOH Y S, DOBBIE G, et al.Detecting Online Auction Shilling Frauds Using Supervised Learning.Expert Systems with Applications, 2014, 41 (6) :3027-3040." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB04B37E0A5139AD2925C14D9B22F68A5&amp;v=MjgyODhUY3VhQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0x5L3hLND1OaWZPZmNHNEdxUFBxUHBGRmU0T0QzVkl1eFFhNkRvT1NYdVdwV0EzZThTUw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        TSANG S, KOH Y S, DOBBIE G, et al.Detecting Online Auction Shilling Frauds Using Supervised Learning.Expert Systems with Applications, 2014, 41 (6) :3027-3040.
                                    </a>
                                </li>
                                <li id="298">


                                    <a id="bibliography_2" title="HASSAN A K I, ABRAHAM A.Modeling Insurance Fraud Detection Using Imbalanced Data Classification//PILLAY N, ENGEL-BRECHT A P, ABRAHAM A, et al., eds.Advances in Nature and Biologically Inspired Computing.Berlin, Germany:Springer, 2016:117-127." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modeling Insurance Fraud Detection Using Imbalanced Data Classification">
                                        <b>[2]</b>
                                        HASSAN A K I, ABRAHAM A.Modeling Insurance Fraud Detection Using Imbalanced Data Classification//PILLAY N, ENGEL-BRECHT A P, ABRAHAM A, et al., eds.Advances in Nature and Biologically Inspired Computing.Berlin, Germany:Springer, 2016:117-127.
                                    </a>
                                </li>
                                <li id="300">


                                    <a id="bibliography_3" title="ALMENDRA V.Finding the Needle:A Risk-Based Ranking of Product Listings at Online Auction Sites for Non-delivery Fraud Prediction.Expert Systems with Applications, 2013, 40 (12) :4805-4811." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13050900030923&amp;v=MjM2MTZCWDQ2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZvVWJoUT1OaWZPZmJLN0h0VE1wbzlGWk9nUA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        ALMENDRA V.Finding the Needle:A Risk-Based Ranking of Product Listings at Online Auction Sites for Non-delivery Fraud Prediction.Expert Systems with Applications, 2013, 40 (12) :4805-4811.
                                    </a>
                                </li>
                                <li id="302">


                                    <a id="bibliography_4" title="YU L, ZHOU R T, TANG L, et al.A DBN-Based Resampling SVMEnsemble Learning Paradigm for Credit Classification with Imbalanced Data.Applied Soft Computing, 2018, 69 (8) :192-202." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES42FB27A851DF3C349F1A6D7A678AAB3F&amp;v=MDE0NDRhQnVIWWZPR1FsZkNwYlEzNU5GaHdMeS94SzQ9TmlmT2ZiZTZhS1BPcVA1TlllcDdlbjlLekJJYW5ENE1UZ3ZsM1JReWNjUGxON25wQ09OdkZTaVdXcjdKSUZwbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        YU L, ZHOU R T, TANG L, et al.A DBN-Based Resampling SVMEnsemble Learning Paradigm for Credit Classification with Imbalanced Data.Applied Soft Computing, 2018, 69 (8) :192-202.
                                    </a>
                                </li>
                                <li id="304">


                                    <a id="bibliography_5" title="TSAI C F, HSU Y F, LIN C Y, et al.Intrusion Detection by Machine Learning:A Review.Expert Systems with Applications, 2009, 36 (10) :11994-12000." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501641437&amp;v=MjA3OTZVTGJJSkZvVWJoUT1OaWZPZmJLN0h0RE5xbzlFWXU4T0NIOCtvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        TSAI C F, HSU Y F, LIN C Y, et al.Intrusion Detection by Machine Learning:A Review.Expert Systems with Applications, 2009, 36 (10) :11994-12000.
                                    </a>
                                </li>
                                <li id="306">


                                    <a id="bibliography_6" title="ZHOU C V, LECKIE C, KARUNASEKERA S.A Survey of Coordinated Attacks and Collaborative Intrusion Detection.Computer and Security, 2010, 29 (1) :124-140." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300518337&amp;v=MzA3NDV0RE9ySTlGWWVvSEQzOCtvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRm9VYmhRPU5pZk9mYks3SA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        ZHOU C V, LECKIE C, KARUNASEKERA S.A Survey of Coordinated Attacks and Collaborative Intrusion Detection.Computer and Security, 2010, 29 (1) :124-140.
                                    </a>
                                </li>
                                <li id="308">


                                    <a id="bibliography_7" title="REN F L, CAO P, LI W, et al.Ensemble Based Adaptive OverSampling Method for Imbalanced Data Learning in Computer Aided Detection of Microaneurysm.Computerized Medical Imaging and Graphics, 2017, 55 (1) :54-67." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB335562C4F961B56A6A821B5AD344068&amp;v=MDA5NjJhUVJieVhDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3THkveEs0PU5pZk9mY0c3SGRUSnFZMDJZSjBHQ24xTHloQmk3RTUxU242UXFXTkJlcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        REN F L, CAO P, LI W, et al.Ensemble Based Adaptive OverSampling Method for Imbalanced Data Learning in Computer Aided Detection of Microaneurysm.Computerized Medical Imaging and Graphics, 2017, 55 (1) :54-67.
                                    </a>
                                </li>
                                <li id="310">


                                    <a id="bibliography_8" title="BLASZCZYNSKI J, STEFANOWSKI J.Neighbourhood Sampling in Bagging for Imbalanced Data.Neurocomputing, 2015, 150:529-542." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700312790&amp;v=MTg1NTRiSUpGb1ViaFE9TmlmT2ZiSzhIOURNcUk5Rlorb05DM1U1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        BLASZCZYNSKI J, STEFANOWSKI J.Neighbourhood Sampling in Bagging for Imbalanced Data.Neurocomputing, 2015, 150:529-542.
                                    </a>
                                </li>
                                <li id="312">


                                    <a id="bibliography_9" title="ZHANG J P, MANI I.KNN Approach to Unbalanced Data Distributions:A Case Study Involving Information Extraction//Proc of the20th International Conference on Machine Learning.Washington, USA:IEEE, 2003:42-48." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=KNN Approach to Unbalanced Data Distributions A Case Study Involving Information Extraction">
                                        <b>[9]</b>
                                        ZHANG J P, MANI I.KNN Approach to Unbalanced Data Distributions:A Case Study Involving Information Extraction//Proc of the20th International Conference on Machine Learning.Washington, USA:IEEE, 2003:42-48.
                                    </a>
                                </li>
                                <li id="314">


                                    <a id="bibliography_10" title="YEN S J, LEE Y S.Cluster-Based Under-Sampling Approaches for Imbalanced Data Distributions.Expert Systems with Applications, 2009, 36 (3) :5718-5727." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501642306&amp;v=MTg5MjNORDN3L29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGb1ViaFE9TmlmT2ZiSzdIdEROcW85RVl1OA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        YEN S J, LEE Y S.Cluster-Based Under-Sampling Approaches for Imbalanced Data Distributions.Expert Systems with Applications, 2009, 36 (3) :5718-5727.
                                    </a>
                                </li>
                                <li id="316">


                                    <a id="bibliography_11" title="LIN W C, TSAI C F, HU Y H, et al.Clustering-Based Undersampling in Class-Imbalanced Data.Information Sciences, 2017, 409/410:17-26." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESDFB713E40B45707321DAC87E21B62038&amp;v=MDA2OTdTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0x5L3hLND1OaWZPZmNmT2JOYk5yUHBCWkprTENYczV5QlVSNjBzTU8zZmwyUkEwQzdTV1JibVhDT052Rg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        LIN W C, TSAI C F, HU Y H, et al.Clustering-Based Undersampling in Class-Imbalanced Data.Information Sciences, 2017, 409/410:17-26.
                                    </a>
                                </li>
                                <li id="318">


                                    <a id="bibliography_12" title="OFEK N, ROKACH L, STERN R, et al.Fast-CBUS:A Fast Clustering-Based Undersampling Method for Addressing the Class Imbalance Problem.Neurocomputing, 2017, 243:88-102." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6611992DA082AAA9525BC8A03505604A&amp;v=MjM4Mzc5REZwbzB4RmVzSERnMUl2aDhXNkRvUE8zZVRyQkV3ZWJlU1JiN3VDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3THkveEs0PU5pZk9mYlcrSA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        OFEK N, ROKACH L, STERN R, et al.Fast-CBUS:A Fast Clustering-Based Undersampling Method for Addressing the Class Imbalance Problem.Neurocomputing, 2017, 243:88-102.
                                    </a>
                                </li>
                                <li id="320">


                                    <a id="bibliography_13" title="TSAI C F, LIN W C, HU Y H, et al.Under-Sampling Class Imbalanced Datasets by Combining Clustering Analysis and Instance Selection.Information Sciences, 2019, 477:47-54." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES11EA71807A6AE33164E5469D16C88217&amp;v=MTkxOTVKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0x5L3hLND1OaWZPZmJLNWE2RExyb2RGWTVvSmZRazZ6QmNWN2twNFRIbnIyQk16Q3JxY1I3dVlDT052RlNpV1dyNw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        TSAI C F, LIN W C, HU Y H, et al.Under-Sampling Class Imbalanced Datasets by Combining Clustering Analysis and Instance Selection.Information Sciences, 2019, 477:47-54.
                                    </a>
                                </li>
                                <li id="322">


                                    <a id="bibliography_14" title="熊冰妍, 王国胤, 邓维斌.基于样本权重的不平衡数据欠抽样方法.计算机研究与发展, 2016, 53 (11) :2613-2622. (XIONG B Y, WANG G Y, DENG W B.Under-Sampling Method Based on Sample Weight for Imbalanced Data.Journal of Computer Research and Development, 2016, 53 (11) :2613-2622.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201611017&amp;v=MTAyMjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpoVTd2T0x5dlNkTEc0SDlmTnJvOUVZNFFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        熊冰妍, 王国胤, 邓维斌.基于样本权重的不平衡数据欠抽样方法.计算机研究与发展, 2016, 53 (11) :2613-2622. (XIONG B Y, WANG G Y, DENG W B.Under-Sampling Method Based on Sample Weight for Imbalanced Data.Journal of Computer Research and Development, 2016, 53 (11) :2613-2622.) 
                                    </a>
                                </li>
                                <li id="324">


                                    <a id="bibliography_15" title="TAO X M, LI Q, GUO W J, et al.Self-adaptive Cost WeightsBased Support Vector Machine Cost-Sensitive Ensemble for Imbalanced Data Classification.Information Sciences, 2019, 487:31-56." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Self-adaptive Cost WeightsBased Support Vector Machine Cost-Sensitive Ensemble for Imbalanced Data Classification">
                                        <b>[15]</b>
                                        TAO X M, LI Q, GUO W J, et al.Self-adaptive Cost WeightsBased Support Vector Machine Cost-Sensitive Ensemble for Imbalanced Data Classification.Information Sciences, 2019, 487:31-56.
                                    </a>
                                </li>
                                <li id="326">


                                    <a id="bibliography_16" title="BEYER K, GOLDSTEIN J, RAMAKRISHNAN R, et al.When is&quot;Nearest Neighbor&quot;Meaningful?//Proc of the International Conference on Database Theory.Berlin, Germany:Springer, 1999:217-235." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=When is &amp;quot;nearest neighbor&amp;quot; meaningful?">
                                        <b>[16]</b>
                                        BEYER K, GOLDSTEIN J, RAMAKRISHNAN R, et al.When is&quot;Nearest Neighbor&quot;Meaningful?//Proc of the International Conference on Database Theory.Berlin, Germany:Springer, 1999:217-235.
                                    </a>
                                </li>
                                <li id="328">


                                    <a id="bibliography_17" >
                                        <b>[17]</b>
                                    CHAWLA N V, BOWYER K W, HALL L O, et al.SMOTE:Synthetic Minority Over-Sampling Technique.Journal of Artificial Intelligence Research, 2002, 16:321-357.</a>
                                </li>
                                <li id="330">


                                    <a id="bibliography_18" title="MACIEJEWSKI T, STEFANOWSKI J.Local Neighbourhood Extension of SMOTE for Mining Imbalanced Data//Proc of the IEEESymposium on Computational Intelligence and Data Mining.Washington, USA:IEEE, 2011:104-111." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Local neighbourhood extension of SMOTE for mining imbalanced data">
                                        <b>[18]</b>
                                        MACIEJEWSKI T, STEFANOWSKI J.Local Neighbourhood Extension of SMOTE for Mining Imbalanced Data//Proc of the IEEESymposium on Computational Intelligence and Data Mining.Washington, USA:IEEE, 2011:104-111.
                                    </a>
                                </li>
                                <li id="332">


                                    <a id="bibliography_19" title="OGAWA K, SUZUKI Y, TAKEUCHI I.Safe Screening of Nonsupport Vectors in Pathwise SVM Computation[C/OL].[2018-12-15].http://proceedings.mlr.press/v28/ogawa13b.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Safe Screening of Nonsupport Vectors in Pathwise SVM Computation[C/OL]">
                                        <b>[19]</b>
                                        OGAWA K, SUZUKI Y, TAKEUCHI I.Safe Screening of Nonsupport Vectors in Pathwise SVM Computation[C/OL].[2018-12-15].http://proceedings.mlr.press/v28/ogawa13b.pdf.
                                    </a>
                                </li>
                                <li id="334">


                                    <a id="bibliography_20" title="OGAWA K, SUZUKI Y, SUZUMURA S, et al.Safe Sample Screening for Support Vector Machines[J/OL].[2018-12-15].https://arxiv.org/pdf/1401.6740.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Safe Sample Screening for Support Vector Machines">
                                        <b>[20]</b>
                                        OGAWA K, SUZUKI Y, SUZUMURA S, et al.Safe Sample Screening for Support Vector Machines[J/OL].[2018-12-15].https://arxiv.org/pdf/1401.6740.pdf
                                    </a>
                                </li>
                                <li id="336">


                                    <a id="bibliography_21" title="NAPIERALA K, STEFANOWSKI J.Types of Minority Class Examples and Their Influence on Learning Classifiers from Imbalanced Data.Journal of Intelligent Information Systems, 2016, 46 (3) :563-597." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Types of minority class examples and their influence on learning classifiers from imbalanced data">
                                        <b>[21]</b>
                                        NAPIERALA K, STEFANOWSKI J.Types of Minority Class Examples and Their Influence on Learning Classifiers from Imbalanced Data.Journal of Intelligent Information Systems, 2016, 46 (3) :563-597.
                                    </a>
                                </li>
                                <li id="338">


                                    <a id="bibliography_22" title="HAN H, WANG W Y, MAO B H.Borderline-SMOTE:A New Over-Sampling Method in Imbalanced Data Sets Learning//Proc of the International Conference on Intelligent Computing.Berlin, Germany:Springer, 2005:878-887." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Borderline-SMOTE:anew over-sampling method in imbalanced data sets learning">
                                        <b>[22]</b>
                                        HAN H, WANG W Y, MAO B H.Borderline-SMOTE:A New Over-Sampling Method in Imbalanced Data Sets Learning//Proc of the International Conference on Intelligent Computing.Berlin, Germany:Springer, 2005:878-887.
                                    </a>
                                </li>
                                <li id="340">


                                    <a id="bibliography_23" title="LI K W, FANG X H, ZHAI J P, et al.An Imbalanced Data Classification Method Driven by Boundary Samples-Boundary-Boost//Proc of the International Conference on Information Science and Control Engineering.Washington, USA:IEEE, 2016:194-199." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Imbalanced Data Classification Method Driven by Boundary Samples-Boundary-Boost">
                                        <b>[23]</b>
                                        LI K W, FANG X H, ZHAI J P, et al.An Imbalanced Data Classification Method Driven by Boundary Samples-Boundary-Boost//Proc of the International Conference on Information Science and Control Engineering.Washington, USA:IEEE, 2016:194-199.
                                    </a>
                                </li>
                                <li id="342">


                                    <a id="bibliography_24" title="WANG J, WONKA P, YE J P.Scaling SVM and Least Absolute Deviations via Exact Data Reduction[C/OL].[2018-12-15].http://export.arxiv.org/pdf/1310.7048." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scaling SVM and Least Absolute Deviations via Exact Data Reduction[C/OL]">
                                        <b>[24]</b>
                                        WANG J, WONKA P, YE J P.Scaling SVM and Least Absolute Deviations via Exact Data Reduction[C/OL].[2018-12-15].http://export.arxiv.org/pdf/1310.7048.
                                    </a>
                                </li>
                                <li id="344">


                                    <a id="bibliography_25" title="SHIBAGAKI A, KARASUYAMA M, HATANO K, et al.Simultaneous Safe Screening of Features and Samples in Doubly Sparse Modeling[C/OL].[2018-12-15].https://arxiv.org/pdf/1602.02485.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simultaneous Safe Screening of Features and Samples in Doubly Sparse Modeling[C/OL]">
                                        <b>[25]</b>
                                        SHIBAGAKI A, KARASUYAMA M, HATANO K, et al.Simultaneous Safe Screening of Features and Samples in Doubly Sparse Modeling[C/OL].[2018-12-15].https://arxiv.org/pdf/1602.02485.pdf.
                                    </a>
                                </li>
                                <li id="346">


                                    <a id="bibliography_26" title="ZHANG W Z, HONG B, LIU W, et al.Scaling Up Sparse Support Vector Machine by Simultaneous Feature and Sample Reduction[C/OL].[2018-12-15].https://arxiv.org/pdf/1607.06996.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scaling Up Sparse Support Vector Machine by Simultaneous Feature and Sample Reduction[C/OL]">
                                        <b>[26]</b>
                                        ZHANG W Z, HONG B, LIU W, et al.Scaling Up Sparse Support Vector Machine by Simultaneous Feature and Sample Reduction[C/OL].[2018-12-15].https://arxiv.org/pdf/1607.06996.pdf.
                                    </a>
                                </li>
                                <li id="348">


                                    <a id="bibliography_27" title="WU G, CHANG E Y.Class-Boundary Alignment for Imbalanced Dataset Learning[C/OL].[2018-12-15].https://sci2s.ugr.es/keel/pdf/specific/congreso/Wu-final.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Class-Boundary Alignment for Imbalanced Dataset Learning[C/OL]">
                                        <b>[27]</b>
                                        WU G, CHANG E Y.Class-Boundary Alignment for Imbalanced Dataset Learning[C/OL].[2018-12-15].https://sci2s.ugr.es/keel/pdf/specific/congreso/Wu-final.pdf.
                                    </a>
                                </li>
                                <li id="350">


                                    <a id="bibliography_28" title="廖士中, 王梅, 赵志辉.正定矩阵支持向量机正则化路径算法.计算机研究与发展, 2013, 50 (11) :2253-2261. (LIAO S Z, WANG M, ZHAO Z H.Regularization Path Algorithm of SVM via Positive Definite Matrix.Journal of Computer Research and Development, 2013, 50 (11) :2253-2261.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201311001&amp;v=MDExMTh0R0ZyQ1VSTE9lWmVSbkZ5emhVN3ZPTHl2U2RMRzRIOUxOcm85RlpZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                        廖士中, 王梅, 赵志辉.正定矩阵支持向量机正则化路径算法.计算机研究与发展, 2013, 50 (11) :2253-2261. (LIAO S Z, WANG M, ZHAO Z H.Regularization Path Algorithm of SVM via Positive Definite Matrix.Journal of Computer Research and Development, 2013, 50 (11) :2253-2261.) 
                                    </a>
                                </li>
                                <li id="352">


                                    <a id="bibliography_29" title="FARQUAD M A H, BOSE I.Preprocessing Unbalanced Data Using Support Vector Machine.Decision Support Systems, 2012, 53 (1) :226-233." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300403122&amp;v=MDgyOTNoUT1OaWZPZmJLN0h0RE9ySTlGWU9zTURYNDdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRm9VYg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[29]</b>
                                        FARQUAD M A H, BOSE I.Preprocessing Unbalanced Data Using Support Vector Machine.Decision Support Systems, 2012, 53 (1) :226-233.
                                    </a>
                                </li>
                                <li id="354">


                                    <a id="bibliography_30" title="ALCAL-FDEZ J, FERNNDEZ A, LUENGO J, et al.KEELData-Mining Software Tool:Data Set Repository, Integration of Algorithms and Experimental Analysis Framework.Journal of Multiple-Valued Logic and Soft Computing, 2011, 17 (2) :255-287." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=KEEL data-mining software tool: Data set repository, integration of algorithms and experimental analysis framework">
                                        <b>[30]</b>
                                        ALCAL-FDEZ J, FERNNDEZ A, LUENGO J, et al.KEELData-Mining Software Tool:Data Set Repository, Integration of Algorithms and Experimental Analysis Framework.Journal of Multiple-Valued Logic and Soft Computing, 2011, 17 (2) :255-287.
                                    </a>
                                </li>
                                <li id="356">


                                    <a id="bibliography_31" title="VANHOEYVELD J, MARTENS D.Imbalanced Classification in Sparse and Large Behaviour Datasets.Data Mining and Knowledge Discovery, 2018, 32 (1) :25-82." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imbalanced Classification in Sparse and Large Behaviour Datasets">
                                        <b>[31]</b>
                                        VANHOEYVELD J, MARTENS D.Imbalanced Classification in Sparse and Large Behaviour Datasets.Data Mining and Knowledge Discovery, 2018, 32 (1) :25-82.
                                    </a>
                                </li>
                                <li id="358">


                                    <a id="bibliography_32" title="METZ C E.Basic Principles of ROC Analysis.Seminars in Nuclear Medicine, 1978, 8 (4) :283-298." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Basic principles of ROC analysis">
                                        <b>[32]</b>
                                        METZ C E.Basic Principles of ROC Analysis.Seminars in Nuclear Medicine, 1978, 8 (4) :283-298.
                                    </a>
                                </li>
                                <li id="360">


                                    <a id="bibliography_33" title="KUBAT M, HOLTE R, MATWIN S.Learning When Negative Examples Abound//Proc of the 9th European Conference on Machine Learning.Berlin, Germany:Springer, 1997:146-153." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning when negative examples abound">
                                        <b>[33]</b>
                                        KUBAT M, HOLTE R, MATWIN S.Learning When Negative Examples Abound//Proc of the 9th European Conference on Machine Learning.Berlin, Germany:Springer, 1997:146-153.
                                    </a>
                                </li>
                                <li id="362">


                                    <a id="bibliography_34" title="HART P E.The Condensed Nearest Neighbour Rule.IEEE Transactions on Information Theory, 1968, 14 (5) :515-516." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The condensed nearest neighbor rule (Corresp.)">
                                        <b>[34]</b>
                                        HART P E.The Condensed Nearest Neighbour Rule.IEEE Transactions on Information Theory, 1968, 14 (5) :515-516.
                                    </a>
                                </li>
                                <li id="364">


                                    <a id="bibliography_35" title="BUNKHUMPORNPAT C, SINAPIROMSARAN K, LURSINSAPC.Safe-Level-SMOTE:Safe-Level-Synthetic Minority Over-Sampling Technique for Handling the Class Imbalanced Problem//Proc of the 13th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining.Berlin, Germany:Springer, 2009:475-482." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Safe-Level-SMOTE:Safe-Level-Synthetic Minority Over-Sampling TEchnique for Handling the Class Imbalanced Problem">
                                        <b>[35]</b>
                                        BUNKHUMPORNPAT C, SINAPIROMSARAN K, LURSINSAPC.Safe-Level-SMOTE:Safe-Level-Synthetic Minority Over-Sampling Technique for Handling the Class Imbalanced Problem//Proc of the 13th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining.Berlin, Germany:Springer, 2009:475-482.
                                    </a>
                                </li>
                                <li id="366">


                                    <a id="bibliography_36" title="BATISTA G E A P A, PRATI R C, MONARD M C.A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data.ACM SIGKDD Explorations Newsletter, 2004, 6 (1) :20-29." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM388195169BFE0E7494828A3B5E98AD35&amp;v=MjIyMDloZEFjTHJsTWJtYUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHdMeS94SzQ9TmlmSVk3Q3dGdERGcW81RGJabDVlWHhNeUJJYTdqZC9RQTdoMw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[36]</b>
                                        BATISTA G E A P A, PRATI R C, MONARD M C.A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data.ACM SIGKDD Explorations Newsletter, 2004, 6 (1) :20-29.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(06),545-556 DOI:10.16451/j.cnki.issn1003-6059.201906007            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于安全样本筛选的不平衡数据抽样方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%9F%B3%E6%B4%AA%E6%B3%A2&amp;code=08399757&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">石洪波</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E7%84%B1%E6%98%95&amp;code=40486720&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘焱昕</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%80%E7%B4%A0%E7%90%B4&amp;code=27069343&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冀素琴</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E8%A5%BF%E8%B4%A2%E7%BB%8F%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0073552&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山西财经大学信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对欠抽样可能导致有用信息的丢失, 以及合成小类的过抽样技术 (SMOTE) 可能使大类和小类间类重叠更严重的问题, 文中提出基于安全样本筛选的欠抽样和SMOTE结合的抽样方法 (Screening<sub>S</sub>MOTE) .利用安全筛选规则, 识别并丢弃大类中部分对确定决策边界无价值的实例和噪音实例, 采用SMOTE对筛选后数据集进行过抽样.基于安全样本筛选的欠抽样既避免原始数据中有价值信息的丢失, 又丢弃大类中的噪音实例, 缓减过抽样数据集类重叠的问题.实验表明在处理不平衡数据集, 特别是维数较高的不平衡数据集时Screening<sub>S</sub>MOTE的有效性.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">不平衡数据;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%89%E5%85%A8%E6%A0%B7%E6%9C%AC%E7%AD%9B%E9%80%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安全样本筛选;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AC%A0%E6%8A%BD%E6%A0%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">欠抽样;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%AF%94%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">不平衡比率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%90%88%E6%88%90%E5%B0%8F%E7%B1%BB%E7%9A%84%E8%BF%87%E6%8A%BD%E6%A0%B7%E6%8A%80%E6%9C%AF%20(SMOTE)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">合成小类的过抽样技术 (SMOTE) ;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *石洪波 (通讯作者) , 博士, 教授, 主要研究方向为机器学习、人工智能.E-mail:shb710@163.com.;
                                </span>
                                <span>
                                    刘焱昕, 硕士研究生, 主要研究方向为机器学习.E-mail:312464821@qq.com.;
                                </span>
                                <span>
                                    冀素琴, 硕士, 副教授, 主要研究方向为机器学习、数据挖掘.E-mail:jsq58@sina.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-29</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.61801279);</span>
                                <span>山西省自然科学基金项目 (No.2014011022-2, 201801D121115) 资助;</span>
                    </p>
            </div>
                    <h1><b>Safe Sample Screening Based Sampling Method for Imbalanced Data</b></h1>
                    <h2>
                    <span>SHI Hongbo</span>
                    <span>LIU Yanxin</span>
                    <span>JI Suqin</span>
            </h2>
                    <h2>
                    <span>College of Information, Shanxi University of Finance and Economics</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The loss of valuable information may be caused by undersampling, and the class overlapping between the majority class and the minority class may be aggravated by the synthetic minority oversampling technique (SMOTE) . A sampling method, Screening<sub>S</sub>MOTE, is proposed in this paper, combining safe sample screening based undersampling with SMOTE. Parts of non-informative instances and noise instances in the majority class are identified and discarded by the undersampling method using safe screening rules. Then, the minority class instances generated by SMOTE are added into the screened dataset. The loss of informative information is avoided and the noise instances in the majority class are discarded using safe sample screening based undersampling, relieving the class overlapping. The experimental results show that Screening<sub>S</sub>MOTE is an effective method of rebalancing imbalanced datasets, especially for high dimensional imbalanced datasets.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Imbalanced%20Data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Imbalanced Data;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Safe%20Sample%20Screening&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Safe Sample Screening;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Undersampling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Undersampling;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Imbalance%20Ratio&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Imbalance Ratio;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Synthetic%20Minority%20Oversampling%20Technique%20(SMOTE)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Synthetic Minority Oversampling Technique (SMOTE) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    SHI Hongbo (Corresponding author) , Ph.D., professor. Her research interests include machine learning and artificial intelligence.;
                                </span>
                                <span>
                                    LIU Yanxin, master student. Her research interests include machine learning.;
                                </span>
                                <span>
                                    JI Suqin, master, associate professor. Her research interests include machine learning and data mining.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-01-29</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.61801279);</span>
                                <span>Natural Science Foundation of Shanxi Province (No.2014011022-2, 201801D121115);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="84">近些年, 类不平衡问题, 如金融欺诈探测<citation id="369" type="reference"><link href="296" rel="bibliography" /><link href="298" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>、金融风险评估<citation id="370" type="reference"><link href="300" rel="bibliography" /><link href="302" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>、网络入侵检测<citation id="371" type="reference"><link href="304" rel="bibliography" /><link href="306" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>及疾病诊断<citation id="368" type="reference"><link href="308" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>等受到研究人员的关注.在这类问题中, 数据集的类分布不平衡, 小类数据集的实例个数远小于大类数据集的实例个数.类不平衡及类之间的数据重叠、小类数据的碎块化等问题的存在导致小类数据的预测结果变差.对于许多不平衡问题, 小类数据的预测性能是关注重点.</p>
                </div>
                <div class="p1">
                    <p id="85">针对不平衡问题, 已有许多解决方法, 主要分为2类.1) 从学习算法入手, 针对现有学习算法处理不平衡数据存在的问题, 对其进行修改, 适用于不平衡数据的学习.2) 从训练数据分布入手, 通过改变原始数据集的分布, 降低原始数据集的不平衡比率, 得到更平衡的数据集, 改进小类数据的预测性能.本文关注改变原始数据分布的方法.</p>
                </div>
                <div class="p1">
                    <p id="86">一般地, 改变原始数据分布的方法主要分为两大类:欠抽样方法和过抽样方法.</p>
                </div>
                <div class="p1">
                    <p id="87">在欠抽样方法中, 通过丢弃不平衡数据集的部分大类实例, 使数据集的类分布接近平衡.随机欠抽样源于Bootstrap技术, 是简单、易操作的欠抽样方法.该方法假设大类中的每个实例都同等重要, 没有任何特定的欠采样机制, 仅通过简单丢弃大类数据集的随机子集以减少大类实例的个数.这种方法的随机性使其可能会丢弃较多对分类有用的实例, 导致分类性能更糟.</p>
                </div>
                <div class="p1">
                    <p id="88">为了避免抽样方法的随机性, 研究人员尝试依据数据集的数据分布, 采用某种方法或技术, 选择并保留大类数据中有代表性的实例.基于距离的欠抽样方法<citation id="372" type="reference"><link href="310" rel="bibliography" /><link href="312" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>利用距离度量和不同的度量评估标准 (如离小类数据的最近距离、最远距离等) , 从大类数据的特殊区域中选取有代表性的实例.基于聚类的欠抽样方法<citation id="373" type="reference"><link href="314" rel="bibliography" /><link href="316" rel="bibliography" /><link href="318" rel="bibliography" /><link href="320" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>将聚类技术应用于大类或小类数据, 并从已聚类的数据中选取全部或有代表性的实例, 与另一类数据的实例组成较平衡的数据集.基于样本加权的欠抽样方法<citation id="374" type="reference"><link href="310" rel="bibliography" /><link href="322" rel="bibliography" /><link href="324" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>通过实例周围多数类实例的个数或通过多次聚类确定实例的权重, 使用权重表示每个实例所在的区域或重要程度, 然后依据实例的权重大小对大类数据进行欠抽样.</p>
                </div>
                <div class="p1">
                    <p id="89">上述方法旨在尽可能地选择有代表性、有价值的大类数据, 对类不平衡问题的处理有益.但是, 上述欠抽样方法或多或少都会丢弃部分潜在有价值的实例, 丢失数据集中部分潜在重要的信息.此外, 基于距离的欠抽样和基于聚类的欠抽样需要采用距离度量以度量实例间的距离, 对于高维数据集, 实例之间的距离趋近于相等<citation id="375" type="reference"><link href="326" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 使常用的距离度量失去意义.</p>
                </div>
                <div class="p1">
                    <p id="90">与欠抽样方法不同, 过抽样方法是从小类数据中抽取实例或采用某种机制生成额外实例, 再填入小类中.简单的过抽样是随机过抽样, 多次随机从小类中抽取实例并添至数据集中, 产生较平衡的数据集.随机过抽样的主要问题是可能会产生过拟合, 特别是添加的实例个数较多时.另一类过抽样方法通过生成合成数据以实现.合成小类的过抽样技术 (Synthetic Minority Oversampling Technique, SMOTE) <citation id="376" type="reference"><link href="328" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>在小类实例和它们的<i>k</i>个小类最近邻的连线上生成合成数据点, 通过生成合成数据, 可以避免过拟合, 明显改进分类准确率, 特别是对小类数据.但是, SMOTE可能会导致过泛化, 在生成合成数据点时, 未考虑小类实例附近可能会存在大类实例的情况, 这种过泛化可能导致类之间的重叠<citation id="377" type="reference"><link href="330" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="91">对于基于分类决策边界的分类模型, 如SVM, 训练数据集中每个实例对确定决策边界的作用不同, 有些作用很大, 有些作用很小, 有些对决策边界的确定完全没有影响.因此, 理想的欠抽样方法是丢弃原始大类数据集中对决策边界没有影响的实例, 只保留 (即选择) 对决策边界有影响的实例.从训练数据集中找出对决策边界没有影响的实例是其中的关键.</p>
                </div>
                <div class="p1">
                    <p id="92">安全样本筛选 (Safe Sample Screening) <citation id="378" type="reference"><link href="332" rel="bibliography" /><link href="334" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>利用安全筛选规则, 识别并丢弃训练集中对确定决策边界没有价值的部分实例和噪音实例, 保留所有有价值的实例.安全样本筛选方法可减少训练集的数据量, 提高分类模型的学习效率, 又可保留所有有价值的实例, 避免信息丢失引起的分类模型性能降低.本文提出基于安全样本筛选的欠抽样与SMOTE结合抽样方法 (Safe Sample Screening and SMOTE, Screening_SMOTE) , 实现不平衡数据的再平衡.与一般欠抽样不同, 基于安全样本筛选的欠抽样并不能保证欠抽样后的数据集平衡, 因此, 在安全样本筛选之后, 依据筛选后数据集的不平衡比率, 决定是否需要采用SMOTE对小类数据过抽样.由于安全样本筛选丢弃大类中的噪音实例, 而这些实例与小类实例较接近, 所以, 这在一定程度上减小SMOTE生成的小类实例与大类中实例重叠的可能性.</p>
                </div>
                <h3 id="93" name="93" class="anchor-tag">1 安全样本筛选方法</h3>
                <h4 class="anchor-tag" id="94" name="94"><b>1.1</b> 形式化描述</h4>
                <div class="p1">
                    <p id="95">样本筛选方法基于支持向量机, 形式化的描述如下.令<b><i>X</i></b>⊆<b>R</b><sup><i>d</i></sup>表示输入空间, <i>Y</i>表示输出空间, 对于二分类问题, <i>Y</i>={-1, 1}, 则训练数据集可表示为</p>
                </div>
                <div class="p1">
                    <p id="96"><i>S</i>={ (<b><i>x</i></b><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) }<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow></math></mathml>⊆<b><i>X</i></b>×<i>Y</i>, </p>
                </div>
                <div class="p1">
                    <p id="98">其中<b><i>x</i></b><sub><i>i</i></sub>∈<b><i>X</i></b>为输入实例, <i>y</i><sub><i>i</i></sub>∈<i>Y</i>为对应的类标签, <i>n</i>为训练集的实例个数.</p>
                </div>
                <div class="p1">
                    <p id="99">支持向量机为基于决策边界的分类模型, 利用决策边界<i>f</i> (<b><i>x</i></b>) =<b><i>w</i></b><sup>T</sup><i>Φ</i> (<b><i>x</i></b>) 划分两类数据, 其中<b><i>x</i></b>∈<b><i>X</i></b>为输入实例, <b><i>w</i></b>∈<b><i>F</i></b>为特征空间的系数向量, <i>Φ</i> (·) 为输入空间到特征空间的映射函数.为了构建最优分类决策边界, 可通过求解如下原问题, 得到最优解<b><i>w</i></b><sup>*</sup>, 即</p>
                </div>
                <div class="p1">
                    <p id="100"><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">w</mi><mo>∈</mo><mi mathvariant="bold-italic">F</mi></mrow></munder><mspace width="0.25em" /><mi>Ρ</mi><msub><mrow></mrow><mi>λ</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>ψ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>l</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>;<b><i>w</i></b>) .      (1) </p>
                </div>
                <div class="p1">
                    <p id="102">在式 (1) 表示的原问题中, <i>ψ</i> (·) 为惩罚项, <i>l</i> (·, ·) 为实例的损失函数, <i>λ</i>&gt;0为控制惩罚项和损失函数之间平衡的正则化参数.本文采用的惩罚项为<i>L</i><sub>2</sub>范数的平方:<i>ψ</i> (<b><i>w</i></b>) =‖<b><i>w</i></b>‖<mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>.损失函数采用hinge损失函数:</p>
                </div>
                <div class="p1">
                    <p id="104"><i>l</i> (<b><i>x</i></b><sub><i>i</i></sub>;<b><i>w</i></b>) =max{0, 1-<i>y</i><sub><i>i</i></sub><i>f</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>w</i></b>) }.</p>
                </div>
                <div class="p1">
                    <p id="105">原问题 (1) 的拉格朗日对偶问题可表示为</p>
                </div>
                <div class="area_img" id="106">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201906008_10600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="108">其中</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>Q</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>≜</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mi>Κ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mi>Κ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>≜</mo><mi>Φ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mi>Φ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">为由特征映射<i>Φ</i>定义的Mercer核函数.求解该对偶问题可得最优解<i>α</i><sup>*</sup>.</p>
                </div>
                <div class="p1">
                    <p id="111">由于原问题 (1) 的最优解<b><i>w</i></b><sup>*</sup>与正则化参数<i>λ</i>密切相关, 不同的<i>λ</i>可能得到不同的<b><i>w</i></b><sup>*</sup>, 因此, 使用<b><i>w</i></b><sup>*</sup><sub><i>λ</i></sub>表示正则化参数取值为<i>λ</i>时原问题的最优解.类似地, 对偶问题 (2) 的最优解也是与<i>λ</i>密切相关, 使用</p>
                </div>
                <div class="p1">
                    <p id="112"><i>α</i><sup>*</sup><sub><i>λ</i></sub>= (<i>α</i><sup>*</sup><sub><i>λ</i>, 1</sub>, <i>α</i><sup>*</sup><sub><i>λ</i>, 2</sub>, …, <i>α</i><sup>*</sup><sub><i>λ</i>, <i>i</i></sub>, …, <i>α</i><sup>*</sup><sub><i>λ</i>, <i>n</i></sub>) <sup>T</sup></p>
                </div>
                <div class="p1">
                    <p id="113">表示正则化参数取值为<i>λ</i>时对偶问题的最优解, 其中<i>α</i><sup>*</sup><sub><i>λ</i>, <i>i</i></sub>表示第<i>i</i>个实例对应的拉格朗日系数.</p>
                </div>
                <div class="p1">
                    <p id="114">如果将训练集划分成如下3个子集:</p>
                </div>
                <div class="area_img" id="115">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201906008_11500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="117">则问题 (1) 或 (2) 的最优性条件表示如下:</p>
                </div>
                <div class="area_img" id="118">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201906008_11800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="120">其中, <i>R</i>为由正确划分且位于间隔外的实例组成的集合, <i>E</i>由正确划分且位于间隔上的实例组成的集合, <i>L</i>为由错分实例和间隔内实例组成的集合.</p>
                </div>
                <div class="p1">
                    <p id="121">式 (3) 中训练集数据的划分与训练实例在样本空间的位置紧密相关, 不同位置的实例对决策边界的确定贡献不同.式 (3) 中3个子集<i>E</i>、<i>R</i>和<i>L</i>的实例分别称为边界实例、安全实例和噪音实例.边界实例是位于类间分类决策边界附近的实例, 能为决策边界参数的估计提供有价值的信息.安全实例是指远离类间分类决策边界的实例, 不参与决策边界参数的计算, 对决策边界的确定没有影响.噪音实例是指训练数据集中分类模型预测值与真实值不一致或间隔内的实例, 对决策边界的确定可能有一定的负面影响.</p>
                </div>
                <div class="p1">
                    <p id="122">图1显示三类实例的情况.在虚线上的圆点和五星为边界实例, 在右下方虚线之下的五星和左上方虚线之上的圆点为安全实例, 而在实线之上的五星和实线之下的圆点为噪音实例.</p>
                </div>
                <div class="area_img" id="123">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906008_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 3种类型实例的示例" src="Detail/GetImg?filename=images/MSSB201906008_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 3种类型实例的示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906008_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 An example of 3 types of instances</p>

                </div>
                <div class="p1">
                    <p id="124">文献<citation id="379" type="reference">[<a class="sup">21</a>]</citation>～文献<citation id="380" type="reference">[<a class="sup">23</a>]</citation>采用距离度量的方式确定不同类型的实例, 以便分别处理各种类型的数据.但是, 对于高维数据, 由于维数灾难问题, 采用欧氏距离等距离度量方式效果不佳.本文采用安全样本筛选规则判别实例类型.</p>
                </div>
                <h4 class="anchor-tag" id="125" name="125"><b>1.2</b> 安全样本筛选思路和规则</h4>
                <div class="p1">
                    <p id="126">安全样本筛选<citation id="381" type="reference"><link href="332" rel="bibliography" /><link href="334" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>利用安全筛选规则识别并丢弃训练集中对分类决策边界没有影响或有负面影响的实例, 减少训练集的实例个数.安全样本筛选可大幅降低在筛选后的训练集上求解原问题 (1) 的计算消耗, 得到的解与在原始训练集上求解原问题 (1) 得到的解相当.另外, 安全样本筛选不会筛选对确定分类决策边界有影响的边界实例, 有助于需要这些边界实例的其它任务的完成.</p>
                </div>
                <div class="p1">
                    <p id="127">安全样本筛选的基本思路<citation id="382" type="reference"><link href="332" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>是:考虑一个包含最优解<b><i>w</i></b><sup>*</sup><sub><i>λ</i></sub>的解空间域<i>Θ</i><sub><i>λ</i></sub>⊂<i>F</i>, 即使不知道最优解<b><i>w</i></b><sup>*</sup><sub><i>λ</i></sub>的准确值, 依据最优性条件 (4) , 也可筛选<i>R</i>和<i>L</i>的部分实例.基于此, Ogawa等<citation id="383" type="reference"><link href="332" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出支持向量机的安全样本筛选规则, 文献<citation id="384" type="reference">[<a class="sup">20</a>]</citation>、文献<citation id="385" type="reference">[<a class="sup">24</a>]</citation>～文献<citation id="386" type="reference">[<a class="sup">26</a>]</citation>采用不同方式构建包含最优解的解空间域, 提高筛选样本率和筛选速度.与文献<citation id="387" type="reference">[<a class="sup">25</a>]</citation>类似, 本文采用对偶间隙 (Duality Gap) 方式构建包含最优解的解空间域.</p>
                </div>
                <div class="p1">
                    <p id="128">具体地, 构建安全样本筛选规则的步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="129">1) 构建一个包含最优解的解空间域<i>Θ</i><sub><i>λ</i></sub>.给定任意的原可行解<mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover></math></mathml>∈dom <i>P</i><sub><i>λ</i></sub>和对偶可行解<mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover></math></mathml>∈dom <i>D</i><sub><i>λ</i></sub>, 构造包含原问题 (1) 最优解<b><i>w</i></b><sup>*</sup><sub><i>λ</i></sub>的球形区域<i>Θ</i><sub><i>λ</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="132"><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>λ</mi><mo>*</mo></msubsup><mo>∈</mo><mi>Θ</mi><msub><mrow></mrow><mi>λ</mi></msub><mo>=</mo><mo stretchy="false">{</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">|</mo><mrow><mo>|</mo><mrow><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover><mo>-</mo><mi mathvariant="bold-italic">w</mi></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mo stretchy="false"> (</mo><mn>2</mn><mi>G</mi><msub><mrow></mrow><mi>λ</mi></msub><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mi>λ</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mo stretchy="false">}</mo></mrow></math></mathml>,      (5) </p>
                </div>
                <div class="p1">
                    <p id="134">其中</p>
                </div>
                <div class="p1">
                    <p id="135" class="code-formula">
                        <mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><msub><mrow></mrow><mi>λ</mi></msub><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>∶</mo><mo>=</mo><mi>Ρ</mi><msub><mrow></mrow><mi>λ</mi></msub><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>λ</mi></msub><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="136">为由可行解<mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover></math></mathml>和<mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover></math></mathml>定义的对偶间隙.</p>
                </div>
                <div class="p1">
                    <p id="139">2) 在解空间域<i>Θ</i><sub><i>λ</i></sub>中计算<i>y</i><sub><i>i</i></sub><i>f</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>w</i></b><sup>*</sup><sub><i>λ</i></sub>) 的下、上界.由于解空间域<i>Θ</i><sub><i>λ</i></sub>为球形, <i>y</i><sub><i>i</i></sub><i>f</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>w</i></b><sup>*</sup><sub><i>λ</i></sub>) 的下界<i>LB</i> (<i>y</i><sub><i>i</i></sub><i>f</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>w</i></b><sup>*</sup><sub><i>λ</i></sub>) ) 和上界<i>UB</i> (<i>y</i><sub><i>i</i></sub><i>f</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>w</i></b><sup>*</sup><sub><i>λ</i></sub>) ) 可分别表示为</p>
                </div>
                <div class="area_img" id="140">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201906008_14000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="142">3) 给出安全样本筛选规则.依据式 (4) 的最优性条件和式 (6) 求解<i>y</i><sub><i>i</i></sub><i>f</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>w</i></b><sup>*</sup><sub><i>λ</i></sub>) 上、下界的计算式, 安全样本筛选规则如下:</p>
                </div>
                <div class="area_img" id="143">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201906008_14300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="145">根据式 (7) 的安全样本筛选规则, 对于某个实例 (<b><i>x</i></b><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) , 如果<i>y</i><sub><i>i</i></sub><i>f</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>w</i></b><sup>*</sup><sub><i>λ</i></sub>) 的下界大于1, 该实例必然是训练子集<i>R</i>中的实例, 即该实例是安全实例.如果<i>y</i><sub><i>i</i></sub><i>f</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>w</i></b><sup>*</sup><sub><i>λ</i></sub>) 的上界小于1, 该实例一定属于训练子集<i>L</i>, 即该实例属于噪音实例.安全筛选方法将从数据集中识别并丢弃满足这两条规则的实例, 保证不会将边界实例识别为安全实例<citation id="388" type="reference"><link href="344" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>, 因此, 式 (7) 的样本筛选规则是安全的.</p>
                </div>
                <h3 id="146" name="146" class="anchor-tag">2 基于安全样本筛选的不平衡数据抽样</h3>
                <h4 class="anchor-tag" id="147" name="147"><b>2.1</b> 算法步骤</h4>
                <div class="p1">
                    <p id="148">本文提出基于安全样本筛选的欠抽样和SMOTE过抽样结合的抽样方法, 旨在从不平衡数据集中筛选有价值的实例, 使原始数据集中两类数据达到基本平衡.安全样本筛选能筛选大类中对决策边界有价值的实例, 避免一般欠抽样方法可能丢失有价值信息的问题, 同时丢弃大类数据中的噪音实例, 缓减SMOTE生成的小类实例与大类实例重叠的问题.</p>
                </div>
                <div class="p1">
                    <p id="149">安全样本筛选是筛选训练集的全部实例, 在样本筛选后, 大类和小类实例集都只保留全部边界实例、部分安全实例和噪音实例.但是, 由于小类实例个数较少, 有限的小类实例可能并不是总位于“理想的决策边界”附近<citation id="389" type="reference"><link href="348" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>, 筛选远离决策边界的小类实例后, 小类的实例个数更少, 即使采用基于插值法的SMOTE扩展筛选后的小类实例, 也难以得到足够的描述决策边界的特征信息.因此, 在安全样本筛选后, 本文将原始小类数据替换为筛选后的小类数据, 并采用SMOTE对原始小类数据进行过抽样, 实现训练数据集的再平衡.</p>
                </div>
                <div class="p1">
                    <p id="150">Screening _SMOTE步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="151"><b>算法</b> Screening_SMOTE</p>
                </div>
                <div class="area_img" id="294">
                                <img alt="" src="Detail/GetImg?filename=images/MSSB201906008_29400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="173" class="code-formula">
                        <mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mrow><mtext>b</mtext><mtext>e</mtext><mtext>s</mtext><mtext>t</mtext></mrow></msub><mo>=</mo><mrow><mi>arg</mi></mrow><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>S</mi><msub><mrow></mrow><mrow><mi>λ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mspace width="0.25em" /></mrow></munder></mrow></mstyle><mrow><mi>i</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>2</mn><mo>, </mo><mn>3</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Τ</mi><mo stretchy="false">}</mo></mrow></munder><mi>A</mi><mi>U</mi><mi>C</mi><msub><mrow></mrow><mrow><mi>λ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="174">算法对原始不平衡数据进行抽样, 得到基本平衡的最优数据集.算法主要解决如下问题.</p>
                </div>
                <div class="p1">
                    <p id="175">1) 如何得到筛选数据集.本文采用正则化路径算法的求解思路.正则化路径算法是数值求解支持向量机分类问题的有效方法, 可在相当于一次SVM求解的时间复杂度内得到所有正则化参数及对应SVM的解<citation id="390" type="reference"><link href="350" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>.本文采用热启动方法, 从较小<i>λ</i>到较大<i>λ</i>, 构造一系列安全筛选规则, 得到一系列筛选数据集.当正则化参数<i>λ</i>较小时, SVM的训练更容易收敛<citation id="391" type="reference"><link href="334" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>.因此, 从较小<i>λ</i>开始求解具有计算上的优势.此外, 热启动方法使每个<i>λ</i><sub><i>i</i>-1</sub>对应的解<b><i>w</i></b><sup>*</sup><sub><i>λ</i><sub><i>i</i>-1</sub></sub>和<i>α</i><sup>*</sup><sub><i>λ</i><sub><i>i</i>-1</sub></sub>可作为求解<i>λ</i><sub><i>i</i></sub>的可行解<mathml id="176"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover></math></mathml>和<mathml id="177"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">α</mi><mo>^</mo></mover></math></mathml>, 得到更强筛选规则.</p>
                </div>
                <div class="p1">
                    <p id="178">2) 如何再平衡筛选后的数据集.本文算法采用SMOTE使筛选后数据集达到基本平衡.在安全样本筛选数据后, 使用原始小类数据替换筛选数据集中的小类数据, 再使用SMOTE实现过抽样.事实上, 不平衡数据集的再平衡并不是必须达到完全平衡.过度的SMOTE过抽样会导致过泛化, 因此, 本文算法并不要求SMOTE使数据完全达到平衡, 通过设置参数, 使两类数据的比例达到一定比例 (如1.2∶1) 即可终止SMOTE.</p>
                </div>
                <div class="p1">
                    <p id="179">3) 如何选取最优的抽样数据集.为了选取最优的抽样数据集, 在抽样数据集上构建一个基于决策边界的分类器, 评估该分类器, 依据分类器的接收者操作特征曲线下面积 (Area Under the Curve, AUC) 选择最优抽样数据集.在安全样本筛选中, 随着<i>λ</i>的增大, 分类决策边界朝大类数据集方向移动<citation id="392" type="reference"><link href="352" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>, 可使更多的小类实例正确分类, 提高小类实例的判别正确率.因此, 从较大<i>λ</i>对应的抽样数据集中, 依据AUC值选择最优抽样数据集.</p>
                </div>
                <h4 class="anchor-tag" id="180" name="180"><b>2.2</b> 安全样本筛选对训练集的影响</h4>
                <div class="p1">
                    <p id="181">1) 对训练集不平衡率的影响.不平衡比率为数据集的大类和小类实例个数的比率, 在一定程度上反映数据集上构建分类模型的难易程度.给定不平衡数据集<b><i>S</i></b>, 直接统计<b><i>S</i></b>中大类实例个数<i>Num</i><sup>-</sup>和小类实例个数<i>Num</i><sup>+</sup>, 计算<i>Num</i><sup>-</sup>/<i>Num</i><sup>+</sup>, 得到训练数据集的不平衡比率<i>IR</i> (<b><i>S</i></b>) .数据集<b><i>S</i></b>中每个实例都会参与不平衡率的计算.</p>
                </div>
                <div class="p1">
                    <p id="182">对于SVM分类模型, 仅依据数据集<i>S</i>中所有边界实例就可确定分类决策边界, 因此, 边界实例集<b><i>S</i></b><sup>*</sup>的不平衡比率<i>IR</i> (<b><i>S</i></b><sup>*</sup>) 才是原始数据集<b><i>S</i></b>的真实不平衡比率, 能更好地反映构建分类模型的难易程度.利用安全样本筛选得到的筛选数据集<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>保留所有边界实例及部分安全实例和噪音实例, 统计筛选数据集<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>的大类和小类实例个数, 可得到<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>的不平衡比率<i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) .相比<i>IR</i> (<b><i>S</i></b>) , <i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) 更接近原始数据集的真实不平衡比率<i>IR</i> (<b><i>S</i></b><sup>*</sup>) .依据<i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) 决定SMOTE应添加的新实例个数是合理的.</p>
                </div>
                <div class="p1">
                    <p id="183">2) 对数据集大小的影响.安全样本筛选的主要目的是在不影响分类模型分类性能的前提下, 丢弃对分类决策边界无影响的实例.影响分类决策边界的实例仅是训练集的部分实例, 因此, 筛选后可大幅降低数据集的实例个数, 有利于高效构建分类模型.</p>
                </div>
                <h4 class="anchor-tag" id="184" name="184"><b>2.3</b> 时间复杂度分析</h4>
                <div class="p1">
                    <p id="185">算法主要包括两部分.1) 安全样本筛选.构建筛选规则的时间复杂度为<i>O</i> (<i>n</i><sup>2</sup>) , 实验筛选后数据集的实例个数减少80%以上, 利用SVM求解 (<b><i>w</i></b><sup>*</sup><sub><i>λ</i><sub><i>i</i></sub></sub>, <i>α</i><sup>*</sup><sub><i>λ</i><sub><i>i</i></sub></sub>) 将缩短时间, 可在一次SVM求解的时间复杂度内得到所有参数<i>λ</i>的解路径, 则安全样本筛选的时间复杂度为<i>O</i> (<i>n</i><sup>2</sup>) .2) SMOTE过抽样和对抽样数据集的评估.得到基本平衡的抽样数据集耗时与<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>的不平衡比率<i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) 有关, <i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) 较小时, 过抽样的时间也较短, 否则, 过抽样时间较长.选择最优抽样数据集最差情况下的时间复杂度为<i>O</i> (<i>Tn</i><sup>2</sup>) , 对于大部分数据集, 抽样数据集的实例个数远小于原始数据集的实例个数<i>n</i>, 减少算法所需时间, 通过一些技巧可减小<i>T</i>值.</p>
                </div>
                <h3 id="186" name="186" class="anchor-tag">3 实验及结果分析</h3>
                <h4 class="anchor-tag" id="187" name="187"><b>3.1</b> 实验数据集</h4>
                <div class="p1">
                    <p id="188">KEEL数据库<citation id="393" type="reference"><link href="354" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>包含许多不平衡数据集, 常用于检验不平衡数据处理方法.本文算法适用于数值型数据, 从KEEL数据库中选取16个不平衡数据集, 每个数据集的所有变量 (类变量除外) 都是数值型, 用于验证Screening_SMOTE的有效性.表1按照数据集不平衡比率从小到大列出每个数据集.</p>
                </div>
                <div class="p1">
                    <p id="189">为了在较大规模的数据集上验证Screening_ SMOTE的有效性, 从LIBSVM (https://www.csie.ntu. edu.tw/～cjlin/libsvmtools/datasets/) 选取5个数据集.相比KEEL数据库, 这组数据集包含的实例个数和变量个数较多, 每个数据集由训练集和测试集组成.本文关注两类不平衡问题, 对于原始数据集为多类的usps数据集, 采用与文献<citation id="394" type="reference">[<a class="sup">31</a>]</citation>类似的方法, 将usps的第4类作为小类, 其余类别的数据合并为大类.为了避免不同变量取值范围对算法的影响, 本文对数据集进行标准化处理, 保证所有数据集中每个变量 (类变量除外) 的取值范围为[0, 1].表2列出每个数据集的具体情况.</p>
                </div>
                <div class="area_img" id="190">
                    <p class="img_tit"><b>表1 KEEL不平衡数据库</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 KEEL imbalanced datasets</p>
                    <p class="img_note"></p>
                    <table id="190" border="1"><tr><td>名称</td><td>变量<br />个数</td><td>实例<br />总数</td><td>小类<br />实例个数</td><td>大类<br />实例个数</td><td>不平衡<br />比率</td><td>名称</td><td>变量<br />个数</td><td>实例<br />总数</td><td>小类<br />实例个数</td><td>大类<br />实例个数</td><td>不平衡<br />比率</td></tr><tr><td><br />wisconsin</td><td>9</td><td>683</td><td>239</td><td>444</td><td>1.86</td><td>vowel0</td><td>13</td><td>988</td><td>90</td><td>898</td><td>9.98</td></tr><tr><td><br />glass0</td><td>9</td><td>214</td><td>70</td><td>144</td><td>2.06</td><td>shuttle0vs4</td><td>9</td><td>1829</td><td>123</td><td>1706</td><td>13.87</td></tr><tr><td><br />heberman</td><td>3</td><td>306</td><td>81</td><td>225</td><td>2.78</td><td>glass4</td><td>9</td><td>214</td><td>13</td><td>201</td><td>15.46</td></tr><tr><td><br />vehicle1</td><td>18</td><td>846</td><td>217</td><td>629</td><td>2.9</td><td>pageblocks13vs4</td><td>10</td><td>472</td><td>28</td><td>444</td><td>15.86</td></tr><tr><td><br />vehicle3</td><td>18</td><td>846</td><td>212</td><td>634</td><td>2.99</td><td>glass5</td><td>9</td><td>214</td><td>9</td><td>205</td><td>22.78</td></tr><tr><td><br />vehicle0</td><td>18</td><td>846</td><td>199</td><td>647</td><td>3.25</td><td>yeast4</td><td>8</td><td>1484</td><td>51</td><td>1433</td><td>28.1</td></tr><tr><td><br />newthyroid1</td><td>5</td><td>215</td><td>35</td><td>180</td><td>5.14</td><td>yeast5</td><td>8</td><td>1484</td><td>44</td><td>1440</td><td>32.73</td></tr><tr><td><br />glass6</td><td>9</td><td>214</td><td>29</td><td>185</td><td>6.38</td><td>yeast6</td><td>8</td><td>1484</td><td>35</td><td>1449</td><td>41.4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="191">
                    <p class="img_tit"><b>表2 LIBSVM不平衡数据库</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 LIBSVM imbalanced datasets</p>
                    <p class="img_note"></p>
                    <table id="191" border="1"><tr><td rowspan="2"><br />名称</td><td rowspan="2">变量个数</td><td colspan="4"><br />训练集</td><td colspan="4"><br />测试集</td></tr><tr><td><br />实例<br />总数</td><td>小类<br />实例个数</td><td>大类<br />实例个数</td><td>不平衡<br />比率</td><td><br />实例<br />总数</td><td>小类<br />实例个数</td><td>大类<br />实例个数</td><td>不平衡<br />比率</td></tr><tr><td>a1a</td><td>123</td><td>1605</td><td>395</td><td>1210</td><td>3.06</td><td>30956</td><td>7446</td><td>23510</td><td>3.16</td></tr><tr><td><br />a8a</td><td>123</td><td>22696</td><td>5506</td><td>17190</td><td>3.09</td><td>9865</td><td>2335</td><td>7530</td><td>3.22</td></tr><tr><td><br />usps</td><td>256</td><td>7291</td><td>658</td><td>6633</td><td>10.08</td><td>2007</td><td>166</td><td>1841</td><td>11.09</td></tr><tr><td><br />w1a</td><td>300</td><td>2477</td><td>72</td><td>2405</td><td>33.4</td><td>47272</td><td>1407</td><td>45865</td><td>32.59</td></tr><tr><td><br />w3a</td><td>300</td><td>4912</td><td>143</td><td>4777</td><td>33.41</td><td>44272</td><td>1336</td><td>43501</td><td>32.56</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="192" name="192"><b>3.2</b> 度量方式</h4>
                <div class="p1">
                    <p id="193">在不平衡问题的应用领域中, 小类实例的预测性能是关注重点.常用的分类准确率 (Accuracy) 受大类数据预测性能的影响较大, 大类实例的预测结果会淹没小类实例的预测结果, 因此, 不适合不平衡问题的性能度量.本文采用AUC和G-mean两种度量方式度量分类性能.</p>
                </div>
                <div class="p1">
                    <p id="194">AUC是不平衡问题分类性能的通用度量方式, 度量结果既不会受大类实例的支配, 也不会特别偏向小类实例<citation id="395" type="reference"><link href="358" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>.AUC的取值范围为[0, 1], 理想的AUC值等于1.</p>
                </div>
                <div class="p1">
                    <p id="195">G-mean (Geometric Mean) 同等考虑小类和大类, 计算两类数据的预测准确率的几何平均值, 可以度量数据集的整体分类性能, 只有当小类和大类的分类正确率都较高时, 才能得到较高的G-mean值<sup></sup><citation id="396" type="reference"><link href="360" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>.G-mean的计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="196"><mathml id="197"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mo>-</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>=</mo><mroot><mrow><mi>S</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo>⋅</mo><mi>S</mi><mi>p</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>f</mi><mi>i</mi><mi>c</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><mtext> </mtext></mroot></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="198">其中, <i>Sensitivity</i>=<i>TP</i>/ (<i>TP</i>+<i>FN</i>) 为小类数据的预测准确率, <i>Specificity</i>=<i>TN</i>/ (<i>TN</i>+<i>FP</i>) 为大类数据的预测准确率, <i>TP</i>和<i>TN</i>分别为正确分类的小类实例和大类实例的个数, <i>FP</i>和<i>FN</i>分别为误分类的小类实例和大类实例的个数.</p>
                </div>
                <h4 class="anchor-tag" id="199" name="199"><b>3.3</b> 安全样本筛选的实验结果</h4>
                <h4 class="anchor-tag" id="200" name="200"><b>3.3.1</b> 筛选数据集的不平衡比率与<i>λ</i>的关系</h4>
                <div class="p1">
                    <p id="201">给定原始数据集<b><i>S</i></b>, Screening_SMOTE生成<i>T</i>个<i>λ</i><sub><i>i</i></sub> (1≤<i>i</i>≤<i>T</i>) 对应的筛选数据集<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>, 每个<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>具有各自的不平衡比率<i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) .为了观察不同筛选数据集的不平衡比率变化, 本文设定在Screening_SMOTE中<i>T</i>=100, 算法生成<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>输出, 统计得到每个<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>的不平衡比率<i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) .</p>
                </div>
                <div class="p1">
                    <p id="202">图2显示每个原始数据集的不平衡比率, 以及不同<i>λ</i>与对应的筛选数据集不平衡比率<i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) .在每幅图中, 横坐标表示正则化参数<i>λ</i>的序号, 序号范围为[1, 100], 相应的<i>λ</i>取值为[<i>λ</i><sub>1</sub>, <i>λ</i><sub>100</sub>], 每个数据集的<i>λ</i><sub><i>i</i></sub>取值方式如下.</p>
                </div>
                <div class="p1">
                    <p id="203"><i>λ</i><sub>1</sub>由原始数据集计算得到, 即</p>
                </div>
                <div class="p1">
                    <p id="204" class="code-formula">
                        <mathml id="204"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mo stretchy="false"> (</mo><mrow><mo>|</mo><mrow><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mn>2</mn></msub><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><msub><mrow></mrow><mi>∞</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="205">其余<i>λ</i>在<i>λ</i><sub>1</sub>基础上按比例逐渐增加, </p>
                </div>
                <div class="p1">
                    <p id="206"><i>λ</i><sub><i>i</i></sub>=<i>λ</i><sub><i>i</i>-1</sub>+<i>Δλ</i>, <i>i</i>=2, 3, …, <i>T</i>, </p>
                </div>
                <div class="p1">
                    <p id="207">本文设置<i>Δλ</i>=10<sup>2</sup><i>λ</i><sub>1</sub>.</p>
                </div>
                <div class="p1">
                    <p id="208">图2中虚线表示KEEL原始数据库或LIBSVM训练数据库的不平衡比率, 实线表示筛选数据集<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>的不平衡比率<i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) 随<i>λ</i>增加的变化情况.部分<i>λ</i><sub><i>i</i></sub>对应的筛选数据集<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>仅留下大类数据, 因此图中未画出<i>λ</i><sub><i>i</i></sub>对应的不平衡比率.此外, 在安全样本筛选后, yeast4、yeast5、yeast6数据集的筛选数据集或完全没有小类实例或仅有极少小类实例, 所以未给出这3个数据集不平衡比率随<i>λ</i>变化的关系图.</p>
                </div>
                <div class="area_img" id="295">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906008_29500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 λ值与对应的筛选数据集的不平衡比率" src="Detail/GetImg?filename=images/MSSB201906008_29500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 λ值与对应的筛选数据集的不平衡比率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906008_29500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2λand imbalance ratio of corresponding screening datasets</p>

                </div>
                <div class="area_img" id="295">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906008_29501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 λ值与对应的筛选数据集的不平衡比率" src="Detail/GetImg?filename=images/MSSB201906008_29501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 λ值与对应的筛选数据集的不平衡比率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906008_29501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2λand imbalance ratio of corresponding screening datasets</p>

                </div>
                <div class="p1">
                    <p id="220">由图2可见, 在每个数据集正则化路径求解的前期 (即图中横坐标靠左的位置) , 绝大多数筛选数据集<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>的<i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) 较高.随着<i>λ</i><sub><i>i</i></sub>的增加, 筛选数据集的<i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) 逐渐减少, 当<i>λ</i><sub><i>i</i></sub>增大到一定程度时, 部分筛选数据集的<i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) 低于原始数据集的不平衡比率<i>IR</i> (<b><i>S</i></b>) , 逐渐趋于平衡.但是, wisconsin数据集例外, 在正则化路径求解的前期, 筛选数据集的<i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) 低于原始数据集的<i>IR</i> (<b><i>S</i></b>) , 但是随着<i>λ</i>的增加, <i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) 远高于原始训练集的<i>IR</i> (<b><i>S</i></b>) .</p>
                </div>
                <div class="p1">
                    <p id="221">上述实验结果表明, <i>λ</i><sub><i>i</i></sub>为安全样本筛选中重要参数, 给定正则化参数<i>λ</i><sub><i>i</i></sub>, 绝大多数数据集的<i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) 与<i>IR</i> (<b><i>S</i></b>) 不同, 不同<i>λ</i><sub><i>i</i></sub>的<i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) 也可能不同.对于大部分数据集, 采用具有较大<i>λ</i><sub><i>i</i></sub>进行安全样本筛选之后, 筛选数据集的<i>IR</i> (<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>) 有所减小.此外, 较大的<i>λ</i><sub><i>i</i></sub>意味着决策边界更靠近大类方向, 更多的小类实例能正确分类.对于wisconsin数据集, 并不适合取较大的<i>λ</i><sub><i>i</i></sub>值.</p>
                </div>
                <div class="p1">
                    <p id="222">为了进一步观察筛选数据集与原始数据集不平衡比率, 本文在筛选数据集上构建分类器, 使用分类性能 (如AUC) 作为度量指标, 从多个筛选数据集中选择最优筛选数据集.由于选择最优筛选数据集需要有确定的训练数据集, 因此, 实验采用训练集和测试集组成的5个LIBSVM数据库.首先, 采用安全样本筛选从训练集得到每个<i>λ</i><sub><i>i</i></sub>对应的筛选数据集<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>, 然后在每个筛选数据集<b><i>S</i></b><sub><i>λ</i><sub><i>i</i></sub></sub>上构建SVM分类器, 在相应测试集上得到分类器的AUC值, 并依据AUC值选择最优筛选数据集.</p>
                </div>
                <div class="p1">
                    <p id="223">图3显示LIBSVM的5个原始训练集和相应的最优筛选数据集的不平衡比率.由图可看出, 5个最优筛选数据集的不平衡比率都低于原始训练集的不平衡比率, 其中, usps、w1a、w3a数据集的不平衡比率相差较大.</p>
                </div>
                <div class="area_img" id="224">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201906008_224.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 5个不平衡数据集筛选前后不平衡比率对比" src="Detail/GetImg?filename=images/MSSB201906008_224.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 5个不平衡数据集筛选前后不平衡比率对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201906008_224.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Comparison of imbalance ratios of LIBSVM datasets before and after screening</p>

                </div>
                <div class="p1">
                    <p id="226">由图2可见, 大部分数据集在筛选之后, 不平衡比率都有所改变.由图3可见, 最优筛选数据集的不平衡比率低于原始数据集的不平衡比率.因此, 采用安全样本筛选作为欠抽样, 可降低不平衡数据集的不平衡比率, 使得安全样本筛选之后采用SMOTE添加小类数据时, 不必增加过多实例, 缓解可能出现的过泛化问题.</p>
                </div>
                <h4 class="anchor-tag" id="227" name="227"><b>3.3.2</b> 最优筛选数据集的实例个数</h4>
                <div class="p1">
                    <p id="228">在安全样本筛选之后, 删除大类和小类中的部分安全实例和噪音实例, 减少训练集中的剩余实例个数.表3列出LIBSVM的5个原始训练集和最优筛选数据集的实例个数.在安全样本筛选之后, 丢弃5个原始训练集中大部分实例, 丢弃的实例总个数都在81%以上.其中, usps数据集丢弃的实例个数最多, 达到94%.丢弃的大类实例个数也都在83%以上, 丢弃大类实例最多的数据集还是usps数据集, 多达95%.</p>
                </div>
                <div class="p1">
                    <p id="229">由上述结果可看出, 对于LIBSVM的绝大部分数据集, 最优筛选数据集保留所有对决策边界确定有影响的边界实例, 相比原始训练集, 大幅减少实例总个数和大类实例个数.因此, 在最优筛选数据集上构建SVM分类器的效率得以提高.</p>
                </div>
                <div class="area_img" id="230">
                    <p class="img_tit"><b>表3 LIBSVM原始数据集和筛选数据集的实例个数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Number of instances on original datasets and screened datasets of LIBSVM</p>
                    <p class="img_note"></p>
                    <table id="230" border="1"><tr><td>数据集</td><td>原始数据集<br />的实例总数</td><td>最优筛选<br />数据集的<br />实例总数</td><td>原始数据集<br />的大类实例<br />个数</td><td>最优筛选数<br />据集的大类<br />实例个数</td></tr><tr><td><br />a1a</td><td>1605</td><td>297</td><td>1210</td><td>202</td></tr><tr><td><br />a8a</td><td>22696</td><td>3902</td><td>17190</td><td>2733</td></tr><tr><td><br />usps</td><td>7291</td><td>431</td><td>6633</td><td>312</td></tr><tr><td><br />w1a</td><td>2477</td><td>290</td><td>2405</td><td>261</td></tr><tr><td><br />w3a</td><td>4912</td><td>481</td><td>4777</td><td>434</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="231" name="231"><b>3.4</b> 基于抽样数据集的分类性能实验</h4>
                <div class="p1">
                    <p id="232">本文在KEEL、LIBSVM数据集上测试Screening_SMOTE的有效性.对于LIBSVM的每个数据集, 首先在训练集上应用Screening_SMOTE, 得到基本平衡的抽样数据集, 然后, 在基本平衡的抽样数据集上构建分类器, 并使用测试集进行评估, 得到分类器的G-mean值和AUC值.对于KEEL中每个数据集, 采用五折交叉验证方法评估16个不平衡数据集的分类效果.在交叉验证的每折计算中, 采用与LIBSVM数据集同样的建模和评估方式, 得到每折训练集上分类器的G-mean值和AUC值, 最后对5组G-mean值和AUC值求平均值, 得到最终的五折交叉验证结果.</p>
                </div>
                <div class="p1">
                    <p id="233">本文选取如下9种重抽样方法用于原始数据集的抽样, 并对比抽样数据集上构建的分类器的分类性能.</p>
                </div>
                <div class="p1">
                    <p id="235">1) 随机过抽样 (Random Oversampling, R_Over_Sampling) .采用有放回的随机抽样方法, 从小类中抽取实例, 添加到小类中, 实现原始数据集的过抽样.</p>
                </div>
                <div class="p1">
                    <p id="236">2) 随机欠抽样 (Ramdom Undersampling, R_Under_Sampling) .采用随机抽样方法, 从大类中抽取部分实例并丢弃, 实现原始数据集的欠抽样.</p>
                </div>
                <div class="p1">
                    <p id="237">3) SMOTE<citation id="398" type="reference"><link href="328" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>.通过插值法在小类数据中添加实例, 实现原始数据集的过抽样.</p>
                </div>
                <div class="p1">
                    <p id="238">4) 边缘合成小类过抽样 (Borderline-Synthetic Minority Oversampling Technique, Bordline_SMO-TE) <citation id="399" type="reference"><link href="338" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>.采用距离度量的方式, 选出小类数据中的边界实例, 采用SMOTE扩展小类中的边界实例.</p>
                </div>
                <div class="p1">
                    <p id="239">5) 凝聚最近邻规则 (Condensed Nearest Neighbor Rule, CNN) <citation id="400" type="reference"><link href="362" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>.通过寻找与原始数据集一致的数据子集, 实现原始数据的欠抽样.寻找一致的数据子集实质上是移走大类中远离决策边界的实例, 留下大类中的安全实例和所有的小类实例.</p>
                </div>
                <div class="p1">
                    <p id="240">6) 安全级别合成小类过抽样 (Safe Level Synt-hetic Minority Oversampling Technique, Safe_Level_SMOTE) <citation id="401" type="reference"><link href="364" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>.通过距离度量计算每个小类实例的安全级别, 采用SMOTE生成靠近高安全级别实例的实例.</p>
                </div>
                <div class="p1">
                    <p id="241">7) 合成小类过抽样和编辑最近邻 (Synthetic Minority Oversampling Technique+Edited Nearest Neighbor, SMOTE_ENN) <citation id="402" type="reference"><link href="366" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>.采用SMOTE过抽样之后, 使用ENN清洗过抽样数据集的两类数据.比Tomek Links移出更多的实例.</p>
                </div>
                <div class="p1">
                    <p id="242">8) 合成小类过抽样和Tomek链接 (Synthetic Minority Oversampling Technique+Tomek Links, SMOTE_TomekLinks) <citation id="403" type="reference"><link href="366" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>.采用SMOTE实现过抽样, 使用Tomek Links对过抽样后的全体数据集进行数据清洗, 生成具有较好类簇的平衡数据集.</p>
                </div>
                <div class="p1">
                    <p id="243">9) 基于聚类的快速欠抽样 (Fast Clustering_Based Undersampling, Fast_CBUS) <citation id="404" type="reference"><link href="318" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>.聚类小类样本集为若干簇, 对小类各簇附近的大类样本进行欠采样, 与小类各簇合并形成若干平衡的样本数据集.</p>
                </div>
                <div class="p1">
                    <p id="244">在安全样本筛选之后, 采用SMOTE对筛选的数据集进行再平衡.在SMOTE过抽样中, 最近邻实例数设定为5, 增加的实例个数依据筛选后数据集的大类实例个数确定.</p>
                </div>
                <div class="p1">
                    <p id="245">表4列出在原始数据集上10种方法构建的SVM分类器的G-mean值, 表5列出相应的AUC值.由全部数据集的平均性能可见, Screening_SMOTE的平均G-mean值为0.888, 平均AUC值为0.89, 均高于其它方法的平均值.由单个数据集可见, Screening_SMOTE在12个数据集上的G-mean值和9个数据集上的AUC值最高.实验表明, Screening _SMOTE是再平衡不平衡数据集的有效方法.</p>
                </div>
                <div class="p1">
                    <p id="246">一方面, 数据集中误分的大类实例位于分类决策边界的小类数据一侧, 可能与小类实例产生重叠.安全样本筛选方法丢弃大类数据中的误分实例, 相当于缩小两类数据的重叠区域, 决策边界向大类方向移动, 正确分类更多的小类实例.另一方面, 安全样本筛选保留大类中的所有边界实例, 尽可能多地删除安全实例和噪音实例, 使大类数据的实例总个数减少 (减少量的实验结果见表3) , 相应地, SMOTE过抽样方法向小类数据集添加实例时, 不会增加过多实例, 部分缓解过泛化问题.更重要地, 在安全样本筛选之后, 没有丢弃大类数据中的边界实例, 不存在一般欠抽样方法可能引起的信息丢失问题.</p>
                </div>
                <div class="area_img" id="247">
                    <p class="img_tit"><b>表4 不同方法下SVM分类器的G-mean值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 G-mean values of SVM classifiers by different methods</p>
                    <p class="img_note"></p>
                    <table id="247" border="1"><tr><td>数据集</td><td>原始<br />数据集</td><td>R_Over_<br />Sampling</td><td>R_Under_<br />Sampling</td><td>SMOTE</td><td>Borderline<br />_SMOTE</td><td>CNN</td><td>Safe_Level<br />_SMOTE</td><td>SMOTE<br />_ENN</td><td>SMOTE_<br />TomekLinks</td><td>Fast_<br />CBUS</td><td>Screening<br />_SMOTE</td></tr><tr><td><br />a1a</td><td>0.707</td><td>0.706</td><td>0.744</td><td>0.703</td><td>0.706</td><td>0.710</td><td>0.726</td><td>0.758</td><td>0.737</td><td>0.690</td><td>0.811</td></tr><tr><td><br />a8a</td><td>0.739</td><td>0.778</td><td>0.783</td><td>0.783</td><td>0.745</td><td>0.765</td><td>0.793</td><td>0.795</td><td>0.784</td><td>0.462</td><td>0.819</td></tr><tr><td><br />usps</td><td>0.947</td><td>0.947</td><td>0.956</td><td>0.947</td><td>0.947</td><td>0.928</td><td>0.928</td><td>0.956</td><td>0.956</td><td>0.881</td><td>0.966</td></tr><tr><td><br />w1a</td><td>0.683</td><td>0.726</td><td>0.728</td><td>0.710</td><td>0.697</td><td>0.701</td><td>0.723</td><td>0.764</td><td>0.727</td><td>0.670</td><td>0.782</td></tr><tr><td><br />w3a</td><td>0.746</td><td>0.773</td><td>0.789</td><td>0.773</td><td>0.764</td><td>0.780</td><td>0.775</td><td>0.804</td><td>0.789</td><td>0.420</td><td>0.830</td></tr><tr><td><br />glass0</td><td>0.580</td><td>0.657</td><td>0.627</td><td>0.729</td><td>0.796</td><td>0.763</td><td>0.776</td><td>0.744</td><td>0.751</td><td>0.766</td><td>0.796</td></tr><tr><td><br />glass4</td><td>0.537</td><td>0.536</td><td>0.848</td><td>0.536</td><td>0.763</td><td>0.738</td><td>0.732</td><td>0.912</td><td>0.872</td><td>0.758</td><td>0.894</td></tr><tr><td><br />glass5</td><td>0.936</td><td>0.936</td><td>0.575</td><td>0.936</td><td>0.915</td><td>0.908</td><td>0.759</td><td>0.965</td><td>0.965</td><td>0.722</td><td>0.977</td></tr><tr><td><br />glass6</td><td>0.858</td><td>0.858</td><td>0.758</td><td>0.855</td><td>0.896</td><td>0.933</td><td>0.869</td><td>0.908</td><td>0.891</td><td>0.905</td><td>0.958</td></tr><tr><td><br />Haberman</td><td>0.445</td><td>0.624</td><td>0.610</td><td>0.635</td><td>0.589</td><td>0.558</td><td>0.573</td><td>0.643</td><td>0.619</td><td>0.615</td><td>0.632</td></tr><tr><td><br />new-thyroid1</td><td>0.979</td><td>0.979</td><td>0.973</td><td>0.979</td><td>0.934</td><td>0.948</td><td>0.986</td><td>0.991</td><td>0.983</td><td>0.921</td><td>0.979</td></tr><tr><td><br />Page</td><td>0.873</td><td>0.874</td><td>0.928</td><td>0.874</td><td>0.944</td><td>0.918</td><td>0.940</td><td>0.996</td><td>0.996</td><td>0.933</td><td>0.983</td></tr><tr><td><br />shuttle-c0-vs-c4</td><td>0.959</td><td>0.959</td><td>0.951</td><td>0.959</td><td>0.941</td><td>0.941</td><td>0.914</td><td>0.995</td><td>0.991</td><td>0.704</td><td>0.988</td></tr><tr><td><br />vehicle0</td><td>0.756</td><td>0.784</td><td>0.797</td><td>0.778</td><td>0.969</td><td>0.973</td><td>0.973</td><td>0.962</td><td>0.967</td><td>0.941</td><td>0.963</td></tr><tr><td><br />vehicle1</td><td>0.652</td><td>0.559</td><td>0.529</td><td>0.618</td><td>0.860</td><td>0.847</td><td>0.849</td><td>0.827</td><td>0.825</td><td>0.746</td><td>0.780</td></tr><tr><td><br />vehicle3</td><td>0.597</td><td>0.622</td><td>0.643</td><td>0.634</td><td>0.830</td><td>0.812</td><td>0.828</td><td>0.814</td><td>0.802</td><td>0.702</td><td>0.775</td></tr><tr><td><br />vowel0</td><td>0.993</td><td>0.993</td><td>0.977</td><td>0.993</td><td>1</td><td>0.997</td><td>0.982</td><td>0.996</td><td>0.998</td><td>0.943</td><td>0.997</td></tr><tr><td><br />wisconsin</td><td>0.933</td><td>0.943</td><td>0.948</td><td>0.929</td><td>0.932</td><td>0.924</td><td>0.949</td><td>0.974</td><td>0.963</td><td>0.948</td><td>0.974</td></tr><tr><td><br />yeast4</td><td>0.399</td><td>0.783</td><td>0.809</td><td>0.773</td><td>0.653</td><td>0.377</td><td>0.779</td><td>0.795</td><td>0.782</td><td>0.815</td><td>0.840</td></tr><tr><td><br />yeast5</td><td>0.779</td><td>0.950</td><td>0.953</td><td>0.928</td><td>0.917</td><td>0.813</td><td>0.929</td><td>0.952</td><td>0.954</td><td>0.935</td><td>0.981</td></tr><tr><td><br />yeast6</td><td>0.577</td><td>0.838</td><td>0.879</td><td>0.849</td><td>0.816</td><td>0.695</td><td>0.835</td><td>0.846</td><td>0.837</td><td>0.796</td><td>0.916</td></tr><tr><td><br />平均值</td><td>0.746</td><td>0.801</td><td>0.8</td><td>0.806</td><td>0.839</td><td>0.811</td><td>0.839</td><td>0.876</td><td>0.866</td><td>0.775</td><td>0.888</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="248">
                    <p class="img_tit"><b>表5 不同方法下SVM分类器的AUC值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 AUC values of SVM classifiers by different methods</p>
                    <p class="img_note"></p>
                    <table id="248" border="1"><tr><td>数据集</td><td>原始<br />数据集</td><td>R_Over_<br />Sampling</td><td>R_Under_<br />Sampling</td><td>SMOTE</td><td>Borderline<br />_SMOTE</td><td>CNN</td><td>Safe_Level<br />_SMOTE</td><td>SMOTE<br />_ENN</td><td>SMOTE_<br />TomekLinks</td><td>Fast_<br />CBUS</td><td>Screening<br />_SMOTE</td></tr><tr><td><br />a1a</td><td>0.718</td><td>0.716</td><td>0.744</td><td>0.715</td><td>0.717</td><td>0.715</td><td>0.727</td><td>0.758</td><td>0.740</td><td>0.830</td><td>0.811</td></tr><tr><td><br />a8a</td><td>0.752</td><td>0.778</td><td>0.784</td><td>0.763</td><td>0.756</td><td>0.768</td><td>0.793</td><td>0.795</td><td>0.785</td><td>0.860</td><td>0.821</td></tr><tr><td><br />usps</td><td>0.948</td><td>0.947</td><td>0.956</td><td>0.947</td><td>0.948</td><td>0.930</td><td>0.930</td><td>0.956</td><td>0.957</td><td>0.926</td><td>0.967</td></tr><tr><td><br />w1a</td><td>0.729</td><td>0.726</td><td>0.728</td><td>0.710</td><td>0.736</td><td>0.738</td><td>0.731</td><td>0.771</td><td>0.755</td><td>0.658</td><td>0.783</td></tr><tr><td><br />w3a</td><td>0.777</td><td>0.773</td><td>0.788</td><td>0.773</td><td>0.789</td><td>0.801</td><td>0.777</td><td>0.808</td><td>0.807</td><td>0.693</td><td>0.831</td></tr><tr><td><br />glass0</td><td>0.656</td><td>0.693</td><td>0.686</td><td>0.751</td><td>0.800</td><td>0.772</td><td>0.778</td><td>0.754</td><td>0.762</td><td>0.810</td><td>0.815</td></tr><tr><td><br />glass4</td><td>0.699</td><td>0.699</td><td>0.864</td><td>0.699</td><td>0.806</td><td>0.779</td><td>0.754</td><td>0.916</td><td>0.884</td><td>0.853</td><td>0.905</td></tr><tr><td><br />glass5</td><td>0.942</td><td>0.942</td><td>0.702</td><td>0.942</td><td>0.925</td><td>0.915</td><td>0.858</td><td>0.965</td><td>0.965</td><td>0.831</td><td>0.978</td></tr><tr><td><br />glass6</td><td>0.861</td><td>0.861</td><td>0.789</td><td>0.859</td><td>0.900</td><td>0.934</td><td>0.870</td><td>0.909</td><td>0.892</td><td>0.921</td><td>0.959</td></tr><tr><td><br />Haberman</td><td>0.559</td><td>0.629</td><td>0.612</td><td>0.639</td><td>0.601</td><td>0.607</td><td>0.587</td><td>0.646</td><td>0.624</td><td>0.634</td><td>0.651</td></tr><tr><td><br />new-thyroid1</td><td>0.980</td><td>0.980</td><td>0.974</td><td>0.980</td><td>0.937</td><td>0.951</td><td>0.986</td><td>0.991</td><td>0.983</td><td>0.947</td><td>0.980</td></tr><tr><td><br />Page</td><td>0.883</td><td>0.884</td><td>0.929</td><td>0.884</td><td>0.946</td><td>0.922</td><td>0.941</td><td>0.996</td><td>0.996</td><td>0.954</td><td>0.974</td></tr><tr><td><br />shuttle-c0-vs-c4</td><td>0.961</td><td>0.962</td><td>0.954</td><td>0.961</td><td>0.950</td><td>0.950</td><td>0.918</td><td>0.995</td><td>0.991</td><td>0.814</td><td>0.979</td></tr><tr><td><br />vehicle0</td><td>0.791</td><td>0.811</td><td>0.817</td><td>0.809</td><td>0.969</td><td>0.973</td><td>0.973</td><td>0.962</td><td>0.968</td><td>0.965</td><td>0.964</td></tr><tr><td><br />vehicle1</td><td>0.714</td><td>0.686</td><td>0.661</td><td>0.699</td><td>0.860</td><td>0.847</td><td>0.852</td><td>0.836</td><td>0.831</td><td>0.759</td><td>0.787</td></tr><tr><td><br />vehicle3</td><td>0.667</td><td>0.679</td><td>0.693</td><td>0.692</td><td>0.831</td><td>0.814</td><td>0.834</td><td>0.823</td><td>0.812</td><td>0.765</td><td>0.777</td></tr><tr><td><br />vowel0</td><td>0.993</td><td>0.993</td><td>0.978</td><td>0.993</td><td>1</td><td>0.997</td><td>0.982</td><td>0.996</td><td>0.998</td><td>0.980</td><td>0.997</td></tr><tr><td><br />wisconsin</td><td>0.933</td><td>0.943</td><td>0.948</td><td>0.930</td><td>0.932</td><td>0.925</td><td>0.949</td><td>0.974</td><td>0.963</td><td>0.980</td><td>0.974</td></tr><tr><td><br />yeast4</td><td>0.581</td><td>0.793</td><td>0.814</td><td>0.787</td><td>0.717</td><td>0.559</td><td>0.786</td><td>0.797</td><td>0.788</td><td>0.859</td><td>0.842</td></tr><tr><td><br />yeast5</td><td>0.802</td><td>0.951</td><td>0.954</td><td>0.930</td><td>0.921</td><td>0.831</td><td>0.930</td><td>0.952</td><td>0.954</td><td>0.968</td><td>0.981</td></tr><tr><td><br />yeast6</td><td>0.683</td><td>0.845</td><td>0.882</td><td>0.857</td><td>0.832</td><td>0.740</td><td>0.839</td><td>0.850</td><td>0.841</td><td>0.880</td><td>0.918</td></tr><tr><td><br />平均值</td><td>0.792</td><td>0.823</td><td>0.822</td><td>0.825</td><td>0.851</td><td>0.832</td><td>0.847</td><td>0.879</td><td>0.871</td><td>0.852</td><td>0.890</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="249">由表4和表5可见, 无论是G-mean值还是AUC值, Screening_SMOTE在LIBSVM中5个数据集上全部得到最高值, 优于其它方法.没有采用基于距离度量的方法选择边界实例是一个主要原因.在实验中表现相对较好的Bordline_SMOTE、Safe_Level _SMOTE、SMOTE_TomekLinks和SMOTE_ENN都使用距离度量.对于维数较低的数据集, 通过距离度量选取边界实例是合理的方式, 但是, 随着数据集维数的增加, 每个实例与其最近实例和最远实例之间的距离差值逐渐减小, 使利用最近距离选取边界实例的可行性降低.而Screening_SMOTE依据安全筛选规则确定边界实例, 当数据集具有较高维数时, 边界实例的选取不受影响.相比KEEL数据集, LIBSVM中5个数据集具有较高的维数, 因此, Screening_SMOTE是更适合的抽样方法.</p>
                </div>
                <h3 id="250" name="250" class="anchor-tag">4 结 束 语</h3>
                <div class="p1">
                    <p id="251">本文提出基于安全样本筛选的欠抽样方法, 并与SMOTE结合, 实现不平衡数据集的再平衡.本文方法首先利用安全样本筛选规则, 丢弃数据集大类数据的部分安全实例和噪音实例, 保留对确定分类决策边界有价值的大类边界实例, 避免一般欠抽样方法可能产生的信息丢失问题.然后, 采用SMOTE对筛选后数据集进行过抽样, 使数据集达到基本平衡.在KEEL、LIBSVM数据库上的实验表明, 本文方法是一种有效的不平衡数据抽样方法, 特别是对维数较高的数据集, 表现效果较好.</p>
                </div>
                <div class="p1">
                    <p id="252">本文采用正则化路径算法求解安全样本筛选后, 得到一组筛选数据集, 在求解过程中, 正则化参数的取值范围很重要:如果范围过大, 生成太多的筛选数据集和平衡数据集, 降低获取最优平衡数据集的效率;如果范围过小, 可能排除最优筛选数据集和平衡数据集, 无法得到最佳平衡数据集.今后需要继续研究如何得到恰当的参数取值范围, 提高获取最优平衡数据集的效率.此外, 如何从生成的平衡数据集中快速选择最优平衡数据集也是今后需要研究的方向.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="291" type="formula" href="images/MSSB201906008_29100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">石洪波</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="292" type="formula" href="images/MSSB201906008_29200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">刘焱昕</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="293" type="formula" href="images/MSSB201906008_29300.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">冀素琴</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="296">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB04B37E0A5139AD2925C14D9B22F68A5&amp;v=MzA5ODVsZkNwYlEzNU5GaHdMeS94SzQ9TmlmT2ZjRzRHcVBQcVBwRkZlNE9EM1ZJdXhRYTZEb09TWHVXcFdBM2U4U1NUY3VhQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>TSANG S, KOH Y S, DOBBIE G, et al.Detecting Online Auction Shilling Frauds Using Supervised Learning.Expert Systems with Applications, 2014, 41 (6) :3027-3040.
                            </a>
                        </p>
                        <p id="298">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modeling Insurance Fraud Detection Using Imbalanced Data Classification">

                                <b>[2]</b>HASSAN A K I, ABRAHAM A.Modeling Insurance Fraud Detection Using Imbalanced Data Classification//PILLAY N, ENGEL-BRECHT A P, ABRAHAM A, et al., eds.Advances in Nature and Biologically Inspired Computing.Berlin, Germany:Springer, 2016:117-127.
                            </a>
                        </p>
                        <p id="300">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13050900030923&amp;v=MjE1MTdUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZvVWJoUT1OaWZPZmJLN0h0VE1wbzlGWk9nUEJYNDZvQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>ALMENDRA V.Finding the Needle:A Risk-Based Ranking of Product Listings at Online Auction Sites for Non-delivery Fraud Prediction.Expert Systems with Applications, 2013, 40 (12) :4805-4811.
                            </a>
                        </p>
                        <p id="302">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES42FB27A851DF3C349F1A6D7A678AAB3F&amp;v=MDE5NDdOWWVwN2VuOUt6QklhbkQ0TVRndmwzUlF5Y2NQbE43bnBDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3THkveEs0PU5pZk9mYmU2YUtQT3FQNQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>YU L, ZHOU R T, TANG L, et al.A DBN-Based Resampling SVMEnsemble Learning Paradigm for Credit Classification with Imbalanced Data.Applied Soft Computing, 2018, 69 (8) :192-202.
                            </a>
                        </p>
                        <p id="304">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501641437&amp;v=MTY1MzE3SHRETnFvOUVZdThPQ0g4K29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGb1ViaFE9TmlmT2ZiSw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>TSAI C F, HSU Y F, LIN C Y, et al.Intrusion Detection by Machine Learning:A Review.Expert Systems with Applications, 2009, 36 (10) :11994-12000.
                            </a>
                        </p>
                        <p id="306">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300518337&amp;v=MzAzMjFGb1ViaFE9TmlmT2ZiSzdIdERPckk5Rlllb0hEMzgrb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>ZHOU C V, LECKIE C, KARUNASEKERA S.A Survey of Coordinated Attacks and Collaborative Intrusion Detection.Computer and Security, 2010, 29 (1) :124-140.
                            </a>
                        </p>
                        <p id="308">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB335562C4F961B56A6A821B5AD344068&amp;v=MjQ0MzZXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZod0x5L3hLND1OaWZPZmNHN0hkVEpxWTAyWUowR0NuMUx5aEJpN0U1MVNuNlFxV05CZXJhUVJieVhDT052RlNpVw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>REN F L, CAO P, LI W, et al.Ensemble Based Adaptive OverSampling Method for Imbalanced Data Learning in Computer Aided Detection of Microaneurysm.Computerized Medical Imaging and Graphics, 2017, 55 (1) :54-67.
                            </a>
                        </p>
                        <p id="310">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700312790&amp;v=MTU4MTVHZXJxUVRNbndaZVp1SHlqbVVMYklKRm9VYmhRPU5pZk9mYks4SDlETXFJOUZaK29OQzNVNW9CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>BLASZCZYNSKI J, STEFANOWSKI J.Neighbourhood Sampling in Bagging for Imbalanced Data.Neurocomputing, 2015, 150:529-542.
                            </a>
                        </p>
                        <p id="312">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=KNN Approach to Unbalanced Data Distributions A Case Study Involving Information Extraction">

                                <b>[9]</b>ZHANG J P, MANI I.KNN Approach to Unbalanced Data Distributions:A Case Study Involving Information Extraction//Proc of the20th International Conference on Machine Learning.Washington, USA:IEEE, 2003:42-48.
                            </a>
                        </p>
                        <p id="314">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501642306&amp;v=Mjc0MzlpclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZvVWJoUT1OaWZPZmJLN0h0RE5xbzlFWXU4TkQzdy9vQk1UNlQ0UFFILw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>YEN S J, LEE Y S.Cluster-Based Under-Sampling Approaches for Imbalanced Data Distributions.Expert Systems with Applications, 2009, 36 (3) :5718-5727.
                            </a>
                        </p>
                        <p id="316">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESDFB713E40B45707321DAC87E21B62038&amp;v=MDMyMDNDcGJRMzVORmh3THkveEs0PU5pZk9mY2ZPYk5iTnJQcEJaSmtMQ1hzNXlCVVI2MHNNTzNmbDJSQTBDN1NXUmJtWENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>LIN W C, TSAI C F, HU Y H, et al.Clustering-Based Undersampling in Class-Imbalanced Data.Information Sciences, 2017, 409/410:17-26.
                            </a>
                        </p>
                        <p id="318">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6611992DA082AAA9525BC8A03505604A&amp;v=MjU1NTB1SFlmT0dRbGZDcGJRMzVORmh3THkveEs0PU5pZk9mYlcrSDlERnBvMHhGZXNIRGcxSXZoOFc2RG9QTzNlVHJCRXdlYmVTUmI3dUNPTnZGU2lXV3I3SklGcG1hQg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>OFEK N, ROKACH L, STERN R, et al.Fast-CBUS:A Fast Clustering-Based Undersampling Method for Addressing the Class Imbalance Problem.Neurocomputing, 2017, 243:88-102.
                            </a>
                        </p>
                        <p id="320">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES11EA71807A6AE33164E5469D16C88217&amp;v=MTc0Nzg0PU5pZk9mYks1YTZETHJvZEZZNW9KZlFrNnpCY1Y3a3A0VEhucjJCTXpDcnFjUjd1WUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHdMeS94Sw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>TSAI C F, LIN W C, HU Y H, et al.Under-Sampling Class Imbalanced Datasets by Combining Clustering Analysis and Instance Selection.Information Sciences, 2019, 477:47-54.
                            </a>
                        </p>
                        <p id="322">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201611017&amp;v=MjY0Mzh0R0ZyQ1VSTE9lWmVSbkZ5emhVN3ZPTHl2U2RMRzRIOWZOcm85RVk0UUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>熊冰妍, 王国胤, 邓维斌.基于样本权重的不平衡数据欠抽样方法.计算机研究与发展, 2016, 53 (11) :2613-2622. (XIONG B Y, WANG G Y, DENG W B.Under-Sampling Method Based on Sample Weight for Imbalanced Data.Journal of Computer Research and Development, 2016, 53 (11) :2613-2622.) 
                            </a>
                        </p>
                        <p id="324">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Self-adaptive Cost WeightsBased Support Vector Machine Cost-Sensitive Ensemble for Imbalanced Data Classification">

                                <b>[15]</b>TAO X M, LI Q, GUO W J, et al.Self-adaptive Cost WeightsBased Support Vector Machine Cost-Sensitive Ensemble for Imbalanced Data Classification.Information Sciences, 2019, 487:31-56.
                            </a>
                        </p>
                        <p id="326">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=When is &amp;quot;nearest neighbor&amp;quot; meaningful?">

                                <b>[16]</b>BEYER K, GOLDSTEIN J, RAMAKRISHNAN R, et al.When is"Nearest Neighbor"Meaningful?//Proc of the International Conference on Database Theory.Berlin, Germany:Springer, 1999:217-235.
                            </a>
                        </p>
                        <p id="328">
                            <a id="bibliography_17" >
                                    <b>[17]</b>
                                CHAWLA N V, BOWYER K W, HALL L O, et al.SMOTE:Synthetic Minority Over-Sampling Technique.Journal of Artificial Intelligence Research, 2002, 16:321-357.
                            </a>
                        </p>
                        <p id="330">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Local neighbourhood extension of SMOTE for mining imbalanced data">

                                <b>[18]</b>MACIEJEWSKI T, STEFANOWSKI J.Local Neighbourhood Extension of SMOTE for Mining Imbalanced Data//Proc of the IEEESymposium on Computational Intelligence and Data Mining.Washington, USA:IEEE, 2011:104-111.
                            </a>
                        </p>
                        <p id="332">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Safe Screening of Nonsupport Vectors in Pathwise SVM Computation[C/OL]">

                                <b>[19]</b>OGAWA K, SUZUKI Y, TAKEUCHI I.Safe Screening of Nonsupport Vectors in Pathwise SVM Computation[C/OL].[2018-12-15].http://proceedings.mlr.press/v28/ogawa13b.pdf.
                            </a>
                        </p>
                        <p id="334">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Safe Sample Screening for Support Vector Machines">

                                <b>[20]</b>OGAWA K, SUZUKI Y, SUZUMURA S, et al.Safe Sample Screening for Support Vector Machines[J/OL].[2018-12-15].https://arxiv.org/pdf/1401.6740.pdf
                            </a>
                        </p>
                        <p id="336">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Types of minority class examples and their influence on learning classifiers from imbalanced data">

                                <b>[21]</b>NAPIERALA K, STEFANOWSKI J.Types of Minority Class Examples and Their Influence on Learning Classifiers from Imbalanced Data.Journal of Intelligent Information Systems, 2016, 46 (3) :563-597.
                            </a>
                        </p>
                        <p id="338">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Borderline-SMOTE:anew over-sampling method in imbalanced data sets learning">

                                <b>[22]</b>HAN H, WANG W Y, MAO B H.Borderline-SMOTE:A New Over-Sampling Method in Imbalanced Data Sets Learning//Proc of the International Conference on Intelligent Computing.Berlin, Germany:Springer, 2005:878-887.
                            </a>
                        </p>
                        <p id="340">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Imbalanced Data Classification Method Driven by Boundary Samples-Boundary-Boost">

                                <b>[23]</b>LI K W, FANG X H, ZHAI J P, et al.An Imbalanced Data Classification Method Driven by Boundary Samples-Boundary-Boost//Proc of the International Conference on Information Science and Control Engineering.Washington, USA:IEEE, 2016:194-199.
                            </a>
                        </p>
                        <p id="342">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scaling SVM and Least Absolute Deviations via Exact Data Reduction[C/OL]">

                                <b>[24]</b>WANG J, WONKA P, YE J P.Scaling SVM and Least Absolute Deviations via Exact Data Reduction[C/OL].[2018-12-15].http://export.arxiv.org/pdf/1310.7048.
                            </a>
                        </p>
                        <p id="344">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simultaneous Safe Screening of Features and Samples in Doubly Sparse Modeling[C/OL]">

                                <b>[25]</b>SHIBAGAKI A, KARASUYAMA M, HATANO K, et al.Simultaneous Safe Screening of Features and Samples in Doubly Sparse Modeling[C/OL].[2018-12-15].https://arxiv.org/pdf/1602.02485.pdf.
                            </a>
                        </p>
                        <p id="346">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scaling Up Sparse Support Vector Machine by Simultaneous Feature and Sample Reduction[C/OL]">

                                <b>[26]</b>ZHANG W Z, HONG B, LIU W, et al.Scaling Up Sparse Support Vector Machine by Simultaneous Feature and Sample Reduction[C/OL].[2018-12-15].https://arxiv.org/pdf/1607.06996.pdf.
                            </a>
                        </p>
                        <p id="348">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Class-Boundary Alignment for Imbalanced Dataset Learning[C/OL]">

                                <b>[27]</b>WU G, CHANG E Y.Class-Boundary Alignment for Imbalanced Dataset Learning[C/OL].[2018-12-15].https://sci2s.ugr.es/keel/pdf/specific/congreso/Wu-final.pdf.
                            </a>
                        </p>
                        <p id="350">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201311001&amp;v=MDI3NDJDVVJMT2VaZVJuRnl6aFU3dk9MeXZTZExHNEg5TE5ybzlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b>廖士中, 王梅, 赵志辉.正定矩阵支持向量机正则化路径算法.计算机研究与发展, 2013, 50 (11) :2253-2261. (LIAO S Z, WANG M, ZHAO Z H.Regularization Path Algorithm of SVM via Positive Definite Matrix.Journal of Computer Research and Development, 2013, 50 (11) :2253-2261.) 
                            </a>
                        </p>
                        <p id="352">
                            <a id="bibliography_29" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300403122&amp;v=MTE3ODNNRFg0N29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUpGb1ViaFE9TmlmT2ZiSzdIdERPckk5RllPcw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[29]</b>FARQUAD M A H, BOSE I.Preprocessing Unbalanced Data Using Support Vector Machine.Decision Support Systems, 2012, 53 (1) :226-233.
                            </a>
                        </p>
                        <p id="354">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=KEEL data-mining software tool: Data set repository, integration of algorithms and experimental analysis framework">

                                <b>[30]</b>ALCAL-FDEZ J, FERNNDEZ A, LUENGO J, et al.KEELData-Mining Software Tool:Data Set Repository, Integration of Algorithms and Experimental Analysis Framework.Journal of Multiple-Valued Logic and Soft Computing, 2011, 17 (2) :255-287.
                            </a>
                        </p>
                        <p id="356">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imbalanced Classification in Sparse and Large Behaviour Datasets">

                                <b>[31]</b>VANHOEYVELD J, MARTENS D.Imbalanced Classification in Sparse and Large Behaviour Datasets.Data Mining and Knowledge Discovery, 2018, 32 (1) :25-82.
                            </a>
                        </p>
                        <p id="358">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Basic principles of ROC analysis">

                                <b>[32]</b>METZ C E.Basic Principles of ROC Analysis.Seminars in Nuclear Medicine, 1978, 8 (4) :283-298.
                            </a>
                        </p>
                        <p id="360">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning when negative examples abound">

                                <b>[33]</b>KUBAT M, HOLTE R, MATWIN S.Learning When Negative Examples Abound//Proc of the 9th European Conference on Machine Learning.Berlin, Germany:Springer, 1997:146-153.
                            </a>
                        </p>
                        <p id="362">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The condensed nearest neighbor rule (Corresp.)">

                                <b>[34]</b>HART P E.The Condensed Nearest Neighbour Rule.IEEE Transactions on Information Theory, 1968, 14 (5) :515-516.
                            </a>
                        </p>
                        <p id="364">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Safe-Level-SMOTE:Safe-Level-Synthetic Minority Over-Sampling TEchnique for Handling the Class Imbalanced Problem">

                                <b>[35]</b>BUNKHUMPORNPAT C, SINAPIROMSARAN K, LURSINSAPC.Safe-Level-SMOTE:Safe-Level-Synthetic Minority Over-Sampling Technique for Handling the Class Imbalanced Problem//Proc of the 13th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining.Berlin, Germany:Springer, 2009:475-482.
                            </a>
                        </p>
                        <p id="366">
                            <a id="bibliography_36" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM388195169BFE0E7494828A3B5E98AD35&amp;v=MTY5OThPR1FsZkNwYlEzNU5GaHdMeS94SzQ9TmlmSVk3Q3dGdERGcW81RGJabDVlWHhNeUJJYTdqZC9RQTdoM2hkQWNMcmxNYm1hQ09OdkZTaVdXcjdKSUZwbWFCdUhZZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[36]</b>BATISTA G E A P A, PRATI R C, MONARD M C.A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data.ACM SIGKDD Explorations Newsletter, 2004, 6 (1) :20-29.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201906008" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201906008&amp;v=MTAyNjc0SDlqTXFZOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXpoVTd2T0tEN1liTEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
