<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131442502842500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201903006%26RESULT%3d1%26SIGN%3d81g62OXUoH9CbLTwOSpxW5JNQrk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201903006&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201903006&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201903006&amp;v=MTMwNzN5em5WYjNKS0Q3WWJMRzRIOWpNckk5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#208" data-title="1 特征图切分网络架构 ">1 特征图切分网络架构</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#209" data-title="1.1 常规卷积结构">1.1 常规卷积结构</a></li>
                                                <li><a href="#215" data-title="1.2 多分支卷积">1.2 多分支卷积</a></li>
                                                <li><a href="#233" data-title="1.3 深度可分离卷积">1.3 深度可分离卷积</a></li>
                                                <li><a href="#240" data-title="1.4 残差连接">1.4 残差连接</a></li>
                                                <li><a href="#245" data-title="1.5 切分结构">1.5 切分结构</a></li>
                                                <li><a href="#291" data-title="1.6 模型架构">1.6 模型架构</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#299" data-title="2 实验及结果分析 ">2 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#301" data-title="2.1 实验设置">2.1 实验设置</a></li>
                                                <li><a href="#304" data-title="2.2 在Cifar10数据集上实验结果">2.2 在Cifar10数据集上实验结果</a></li>
                                                <li><a href="#311" data-title="2.3 在SVHN数据集上实验结果">2.3 在SVHN数据集上实验结果</a></li>
                                                <li><a href="#315" data-title="2.4 消融实验">2.4 消融实验</a></li>
                                                <li><a href="#320" data-title="2.5 Friedman检验与Nemenyi后续检验">2.5 Friedman检验与Nemenyi后续检验</a></li>
                                                <li><a href="#331" data-title="2.6 目标检测">2.6 目标检测</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#335" data-title="3 结束语 ">3 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#232" data-title="图1 本文使用的4种卷积核Fig.1 Four proposed convolution kernels">图1 本文使用的4种卷积核Fig.1 Four proposed convolution kerne......</a></li>
                                                <li><a href="#248" data-title="图2 切分模块结构Fig.2 Structure of slice block">图2 切分模块结构Fig.2 Structure of slice block</a></li>
                                                <li><a href="#289" data-title="表1 通道数不同时单个网络模块的参数和计算量Table 1Parameters and floating-point operations of a single network module with different number of channels">表1 通道数不同时单个网络模块的参数和计算量Table 1Parameters and floati......</a></li>
                                                <li><a href="#293" data-title="图3 切分模块和标准卷积中使用Batch Normalization和ReLU的位置Fig.3 Location of batch normalization and ReLU used in the slice block and standard convolution">图3 切分模块和标准卷积中使用Batch Normalization和ReLU的位置Fig.3 Lo......</a></li>
                                                <li><a href="#293" data-title="图3 切分模块和标准卷积中使用Batch Normalization和ReLU的位置Fig.3 Location of batch normalization and ReLU used in the slice block and standard convolution">图3 切分模块和标准卷积中使用Batch Normalization和ReLU的位置Fig.3 Lo......</a></li>
                                                <li><a href="#297" data-title="表2 SFNet具体结构Table 2SFNet architecture">表2 SFNet具体结构Table 2SFNet architecture</a></li>
                                                <li><a href="#308" data-title="表3 各网络在Cifar10数据集上的分类正确率Table 3Classification accuracy of each network on Cifar10 dataset">表3 各网络在Cifar10数据集上的分类正确率Table 3Classification accu......</a></li>
                                                <li><a href="#314" data-title="表4 各网络在SVHN数据集上的分类正确率Table 4Classification accuracy of each network on SVHN dataset">表4 各网络在SVHN数据集上的分类正确率Table 4Classification accurac......</a></li>
                                                <li><a href="#319" data-title="表5 2个网络在2个数据集上的分类正确率Table 5 Classification accuracy of 2 networks on 2 datasets">表5 2个网络在2个数据集上的分类正确率Table 5 Classification accurac......</a></li>
                                                <li><a href="#326" data-title="表6 算法的比较序值Table 6 Comparative order value of algorithm">表6 算法的比较序值Table 6 Comparative order value of algor......</a></li>
                                                <li><a href="#333" data-title="表7 各网络在Pascal VOC数据集上的平均精度均值Table 7Mean of average precision of each network on the Pascal VOC dataset">表7 各网络在Pascal VOC数据集上的平均精度均值Table 7Mean of average......</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="109">


                                    <a id="bibliography_1" title="KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net Classification with Deep Convolutional Neural Networks//PEREIRA F, BURGES C J C, BOTTOU L, et al., eds.Advances in Neural Information Processing Systems 25.Cambridge, USA:The MIT Press, 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep conv olutional neural networks">
                                        <b>[1]</b>
                                        KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net Classification with Deep Convolutional Neural Networks//PEREIRA F, BURGES C J C, BOTTOU L, et al., eds.Advances in Neural Information Processing Systems 25.Cambridge, USA:The MIT Press, 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="111">


                                    <a id="bibliography_2" title="REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks.IEEETransactions on Pattern Analysis and Machine Intelligence, 2016, 39 (6) :1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towarks real-time object detection with region proposal networks">
                                        <b>[2]</b>
                                        REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks.IEEETransactions on Pattern Analysis and Machine Intelligence, 2016, 39 (6) :1137-1149.
                                    </a>
                                </li>
                                <li id="113">


                                    <a id="bibliography_3" title="LONG J, SHELHAMER E, DARRELL T.Fully Convolutional Networks for Semantic Segmentation//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[3]</b>
                                        LONG J, SHELHAMER E, DARRELL T.Fully Convolutional Networks for Semantic Segmentation//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:3431-3440.
                                    </a>
                                </li>
                                <li id="115">


                                    <a id="bibliography_4" title="SIMONYAN K, ZISSERMAN A.Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL].[2018-10-24].https://arxiv.org/pdf/1409.1556.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL]">
                                        <b>[4]</b>
                                        SIMONYAN K, ZISSERMAN A.Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL].[2018-10-24].https://arxiv.org/pdf/1409.1556.pdf.
                                    </a>
                                </li>
                                <li id="117">


                                    <a id="bibliography_5" title="DENG J, DONG W, SOCHER R, et al.Image Net:A Large-Scale Hie-rarchical Image Database//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2009:248-255." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet:A large-scale hierarchical image database">
                                        <b>[5]</b>
                                        DENG J, DONG W, SOCHER R, et al.Image Net:A Large-Scale Hie-rarchical Image Database//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2009:248-255.
                                    </a>
                                </li>
                                <li id="119">


                                    <a id="bibliography_6" title="SZEGEDY C, LIU W, JIA Y Q, et al.Going Deeper with Convolutions//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015.DOI:10.1109/CVPR.2015.7298594." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[6]</b>
                                        SZEGEDY C, LIU W, JIA Y Q, et al.Going Deeper with Convolutions//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015.DOI:10.1109/CVPR.2015.7298594.
                                    </a>
                                </li>
                                <li id="121">


                                    <a id="bibliography_7" title="HE K M, ZHANG X Y, REN S Q, et al.Deep Residual Learning for Image Recognition//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[7]</b>
                                        HE K M, ZHANG X Y, REN S Q, et al.Deep Residual Learning for Image Recognition//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:770-778.
                                    </a>
                                </li>
                                <li id="123">


                                    <a id="bibliography_8" title="HUANG G, LIU Z, VAN DER LAURENS M, et al.Densely Connected Convolutional Networks//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:2261-2269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DenselyConnected Convolutional Networks">
                                        <b>[8]</b>
                                        HUANG G, LIU Z, VAN DER LAURENS M, et al.Densely Connected Convolutional Networks//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:2261-2269.
                                    </a>
                                </li>
                                <li id="125">


                                    <a id="bibliography_9" title="HOWARD A G, ZHU M L, CHEN B, et al.Mobile Nets:Efficient Convolutional Neural Networks for Mobile Vision Applications[C/OL].[2018-10-24].https://arxiv.org/pdf/1704.04861.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mobile Nets:Efficient convolutional neural networks for mobile vision applications">
                                        <b>[9]</b>
                                        HOWARD A G, ZHU M L, CHEN B, et al.Mobile Nets:Efficient Convolutional Neural Networks for Mobile Vision Applications[C/OL].[2018-10-24].https://arxiv.org/pdf/1704.04861.pdf.
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_10" title="HAN S, POOL J, TRAN J, et al.Learning Both Weights and Connections for Efficient Neural Network//CORTES C, LAWRENCEN D, LEE D D, et al., eds.Advances in Neural Information Processing Systems 28.Cambridge, USA:The MIT Press, 2015:1135-1143." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning both Weights and Connections for Efficient Neural Network">
                                        <b>[10]</b>
                                        HAN S, POOL J, TRAN J, et al.Learning Both Weights and Connections for Efficient Neural Network//CORTES C, LAWRENCEN D, LEE D D, et al., eds.Advances in Neural Information Processing Systems 28.Cambridge, USA:The MIT Press, 2015:1135-1143.
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_11" title="NGUYEN H V, ZHOU K, VEMULAPALLI R.Cross-Domain Synthesis of Medical Images Using Efficient Location-Sensitive Deep Network//Proc of the International Conference on Medical Image Computing and Computer-Assisted Intervention.Berlin, Germany:Springer, 2015:677-684." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-domain synthesis of medical images using efficient location-sensitive deep network">
                                        <b>[11]</b>
                                        NGUYEN H V, ZHOU K, VEMULAPALLI R.Cross-Domain Synthesis of Medical Images Using Efficient Location-Sensitive Deep Network//Proc of the International Conference on Medical Image Computing and Computer-Assisted Intervention.Berlin, Germany:Springer, 2015:677-684.
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_12" title="LI H, KADAV A, DURDANOVIC I, et al.Pruning Filters for Efficient ConvNets[C/OL].[2018-10-24].https://arxiv.org/pdf/1608.08710.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pruning Filters for Efficient ConvNets[C/OL]">
                                        <b>[12]</b>
                                        LI H, KADAV A, DURDANOVIC I, et al.Pruning Filters for Efficient ConvNets[C/OL].[2018-10-24].https://arxiv.org/pdf/1608.08710.pdf.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_13" title="HAN S, MAO H Z, DALLY W J.Deep Compression:Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding[C/OL].[2018-10-24].https://arxiv.org/pdf/1510.00149.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep compression:compressing deep neural networks with pruning trained quantization and huffman coding">
                                        <b>[13]</b>
                                        HAN S, MAO H Z, DALLY W J.Deep Compression:Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding[C/OL].[2018-10-24].https://arxiv.org/pdf/1510.00149.pdf.
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_14" title="CHEN W L, WILSON J T, TYREE S, et al.Compressing Neural Networks with the Hashing Trick//Proc of the 32nd International Conference on Machine Learning.Berlin, Germany:Springer, 2015:2285-2294." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Compressing neural networks with the hashing trick">
                                        <b>[14]</b>
                                        CHEN W L, WILSON J T, TYREE S, et al.Compressing Neural Networks with the Hashing Trick//Proc of the 32nd International Conference on Machine Learning.Berlin, Germany:Springer, 2015:2285-2294.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_15" title="DENTON E, ZAREMBA W, BRUNA J, et al.Exploiting Linear Structure within Convolutional Networks for Efficient Evaluation//Proc of the 27th International Conference on Neural Information Processing Systems.Cambridge, USA:The MIT Press, 2014:1269-1277." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting linear structure within convolutional networks for efficient evaluation">
                                        <b>[15]</b>
                                        DENTON E, ZAREMBA W, BRUNA J, et al.Exploiting Linear Structure within Convolutional Networks for Efficient Evaluation//Proc of the 27th International Conference on Neural Information Processing Systems.Cambridge, USA:The MIT Press, 2014:1269-1277.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_16" title="SIRONI A, TEKIN B, RIGAMONTI R, et al.Learning Separable Filters.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (1) :94-116." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning separable filters">
                                        <b>[16]</b>
                                        SIRONI A, TEKIN B, RIGAMONTI R, et al.Learning Separable Filters.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (1) :94-116.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_17" title="JADERBERG M, VEDALDI A, ZISSERMAN A.Speeding up Convolutional Neural Networks with Low Rank Expansions[C/OL].[2018-10-24].https://arxiv.org/pdf/1405.3866.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speeding up Convolutional Neural Networks with Low Rank Expansions[C/OL]">
                                        <b>[17]</b>
                                        JADERBERG M, VEDALDI A, ZISSERMAN A.Speeding up Convolutional Neural Networks with Low Rank Expansions[C/OL].[2018-10-24].https://arxiv.org/pdf/1405.3866.pdf.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_18" title="SOTOUDEH M, BAGHSORKHI S S.DeepThin:A Self-Compressing Library for Deep Neural Networks[C/OL].[2018-10-24].https://arxiv.org/pdf/1802.06944.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepThin:A Self-Compressing Library for Deep Neural Networks[C/OL]">
                                        <b>[18]</b>
                                        SOTOUDEH M, BAGHSORKHI S S.DeepThin:A Self-Compressing Library for Deep Neural Networks[C/OL].[2018-10-24].https://arxiv.org/pdf/1802.06944.pdf.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_19" title="VANHOUCKE V, SENIOR A, MAO M Z.Improving the Speed of Neural Networks on CPUs//Proc of the NIPS Workshop on Deep Learning and Unsupervised Feature Learning.Berlin, Germany:Springer, 2011, I:611-620." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving the Speed of Neural Networks on CPUs">
                                        <b>[19]</b>
                                        VANHOUCKE V, SENIOR A, MAO M Z.Improving the Speed of Neural Networks on CPUs//Proc of the NIPS Workshop on Deep Learning and Unsupervised Feature Learning.Berlin, Germany:Springer, 2011, I:611-620.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_20" title="ARORA S, BHASKARA A, GE R, et al.Provable Bounds for Learning Some Deep Representations[C/OL].[2018-10-24].https://arxiv.org/pdf/1310.6343.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Provable Bounds for Learning Some Deep Representations[C/OL]">
                                        <b>[20]</b>
                                        ARORA S, BHASKARA A, GE R, et al.Provable Bounds for Learning Some Deep Representations[C/OL].[2018-10-24].https://arxiv.org/pdf/1310.6343.pdf.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_21" title="HWANG K, SUNG W.Fixed-Point Feedforward Deep Neural Network Design Using Weights+1, 0, and-1//Proc of the IEEEWorkshop on Signal Processing Systems.Washington, USA:IEEE, 2014.DOI:10.1109/SiPS.2014.6986082." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fixed-point feedforward deep neural network design using weights+1,0,and-1">
                                        <b>[21]</b>
                                        HWANG K, SUNG W.Fixed-Point Feedforward Deep Neural Network Design Using Weights+1, 0, and-1//Proc of the IEEEWorkshop on Signal Processing Systems.Washington, USA:IEEE, 2014.DOI:10.1109/SiPS.2014.6986082.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_22" title="COURBARIAUX M, BENGIO Y, DAVID J P.BinaryConnect:Training Deep Neural Networks with Binary Weights During Propagations//CORTES C, LAWRENCE N D, LEE D D, et al., eds.Advances in Neural Information Processing Systems 28.Cambridge, USA:The MIT Press, 2015:3123-3131." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=BinaryConnect:training deep neural networks with binary weights during propagations">
                                        <b>[22]</b>
                                        COURBARIAUX M, BENGIO Y, DAVID J P.BinaryConnect:Training Deep Neural Networks with Binary Weights During Propagations//CORTES C, LAWRENCE N D, LEE D D, et al., eds.Advances in Neural Information Processing Systems 28.Cambridge, USA:The MIT Press, 2015:3123-3131.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_23" title="COURBARIAUX M, BENGIO Y.Binarized Neural Networks:Training Deep Neural Networks with Weights and Activations Constrained to+1 or-1[C/OL].[2018-10-24].https://arxiv.org/pdf/1602.02830.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Binarized Neural Networks:Training Deep Neural Networks with Weights and Activations Constrained to+1 or-1[C/OL]">
                                        <b>[23]</b>
                                        COURBARIAUX M, BENGIO Y.Binarized Neural Networks:Training Deep Neural Networks with Weights and Activations Constrained to+1 or-1[C/OL].[2018-10-24].https://arxiv.org/pdf/1602.02830.pdf.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_24" title="RASTEGAN M, ORDONEZ V, REDMON J, et al.XNOR-Net:Image Net Classification Using Binary Convolutional Neural Networks//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:525-542." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Xnor-net:Imagenet classification using binary convolutional neural networks">
                                        <b>[24]</b>
                                        RASTEGAN M, ORDONEZ V, REDMON J, et al.XNOR-Net:Image Net Classification Using Binary Convolutional Neural Networks//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:525-542.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_25" title="BUCILUA‘C, CARUANA R, NICULESCU-MIZIL A.Model Compression//Proc of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2006:535-541." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Model Compression">
                                        <b>[25]</b>
                                        BUCILUA‘C, CARUANA R, NICULESCU-MIZIL A.Model Compression//Proc of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2006:535-541.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_26" title="BA L J, CARUANA R.Do Deep Nets Really Need to be Deep?[C/OL].[2018-10-24].https://arxiv.org/pdf/1312.6184v5.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Do Deep Nets Really Need to be Deep?[C/OL]">
                                        <b>[26]</b>
                                        BA L J, CARUANA R.Do Deep Nets Really Need to be Deep?[C/OL].[2018-10-24].https://arxiv.org/pdf/1312.6184v5.pdf.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_27" title="HINTON G, VINYALS O, DEAN J.Distilling the Knowledge in a Neural Network[C/OL].[2018-10-24].https://arxiv.org/pdf/1503.02531.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distilling the Knowledge in a Neural Network[C/OL]">
                                        <b>[27]</b>
                                        HINTON G, VINYALS O, DEAN J.Distilling the Knowledge in a Neural Network[C/OL].[2018-10-24].https://arxiv.org/pdf/1503.02531.pdf.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_28" title="ZEILER M D, FERGUS R.Visualizing and Understanding Convolutional Networks//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2013:818-833." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing and Understanding Convolutional Networks">
                                        <b>[28]</b>
                                        ZEILER M D, FERGUS R.Visualizing and Understanding Convolutional Networks//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2013:818-833.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_29" title="SZEGEDY C, VANHOUCKE V, IOFFE S, et al.Rethinking the Inception Architecture for Computer Vision//Proc of the IEEEConference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:2818-2826." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rethinking the inception architecture for computer vision">
                                        <b>[29]</b>
                                        SZEGEDY C, VANHOUCKE V, IOFFE S, et al.Rethinking the Inception Architecture for Computer Vision//Proc of the IEEEConference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:2818-2826.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_30" title="LIN M, CHEN Q, YAN S C.Network in Network[C/OL].[2018-10-24].https://arxiv.org/pdf/1312.4400.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Network in Network[C/OL]">
                                        <b>[30]</b>
                                        LIN M, CHEN Q, YAN S C.Network in Network[C/OL].[2018-10-24].https://arxiv.org/pdf/1312.4400.pdf.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_31" title="SZEGEDY C, IOFFE S, VANHOUCKE V, et al.Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning[C/OL].[2018-10-24].https://arxiv.org/pdf/1602.07261.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-ResNet and the Impact of Residual Connections on Learning[C/OL]">
                                        <b>[31]</b>
                                        SZEGEDY C, IOFFE S, VANHOUCKE V, et al.Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning[C/OL].[2018-10-24].https://arxiv.org/pdf/1602.07261.pdf.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_32" title="IANDOLA F N, HAN S, MOSKEWICZ M W, et al.SqueezeNet:Alex Net-Level Accuracy with 50x Fewer Parameters and&amp;lt;0.5MBModel Size[C/OL].[2018-10-24].https://arxiv.org/pdf/1602.07360.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SqueezeNet:Alex Net-Level Accuracy with 50x Fewer Parameters and&amp;lt;0.5MBModel Size[C/OL]">
                                        <b>[32]</b>
                                        IANDOLA F N, HAN S, MOSKEWICZ M W, et al.SqueezeNet:Alex Net-Level Accuracy with 50x Fewer Parameters and&amp;lt;0.5MBModel Size[C/OL].[2018-10-24].https://arxiv.org/pdf/1602.07360.pdf.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_33" title="IOANNOU Y, ROBERTSON D, SHOTTON J, et al.Training CN-Ns with Low-Rank Filters for Efficient Image Classification[C/OL].[2018-10-24].https://arxiv.org/pdf/1511.06744.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Training CN-Ns with Low-Rank Filters for Efficient Image Classification[C/OL]">
                                        <b>[33]</b>
                                        IOANNOU Y, ROBERTSON D, SHOTTON J, et al.Training CN-Ns with Low-Rank Filters for Efficient Image Classification[C/OL].[2018-10-24].https://arxiv.org/pdf/1511.06744.pdf.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_34" title="IOANNOU Y, ROBERTSON D, CIPOLLA R, et al.Deep Roots:Improving CNN Efficiency with Hierarchical Filter Groups//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:5977-5986." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep roots:Improving CNN efficiency with hierarchical filter groups">
                                        <b>[34]</b>
                                        IOANNOU Y, ROBERTSON D, CIPOLLA R, et al.Deep Roots:Improving CNN Efficiency with Hierarchical Filter Groups//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:5977-5986.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_35" title="XIE S N, GIRSHICK R, DOLLAR P, et al.Aggregated Residual Transformations for Deep Neural Networks//Proc of the IEEEConference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:5987-5995." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Aggregated Residual Transformations for Deep Neural Networks">
                                        <b>[35]</b>
                                        XIE S N, GIRSHICK R, DOLLAR P, et al.Aggregated Residual Transformations for Deep Neural Networks//Proc of the IEEEConference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:5987-5995.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_36" title="CHOLLET F.Xception:Deep Learning with Depthwise Separable Convolutions//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:1800-1807." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Xception:Deep learning with depthwise separable convolutions">
                                        <b>[36]</b>
                                        CHOLLET F.Xception:Deep Learning with Depthwise Separable Convolutions//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:1800-1807.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_37" title="ZHANG X Y, ZHOU X Y, LIN M X, et al.ShuffleNet:An Extremely Efficient Convolutional Neural Network for Mobile Devices[C/OL].[2018-10-24].https://arxiv.org/pdf/1707.01083.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ShuffleNet:An Extremely Efficient Convolutional Neural Network for Mobile Devices[C/OL]">
                                        <b>[37]</b>
                                        ZHANG X Y, ZHOU X Y, LIN M X, et al.ShuffleNet:An Extremely Efficient Convolutional Neural Network for Mobile Devices[C/OL].[2018-10-24].https://arxiv.org/pdf/1707.01083.pdf.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_38" title="ZHANG T, QI G J, XIAO B, et al.Interleaved Group Convolutions for Deep Neural Networks//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2017:4383-4392." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interleaved group convolutions for deep neural networks">
                                        <b>[38]</b>
                                        ZHANG T, QI G J, XIAO B, et al.Interleaved Group Convolutions for Deep Neural Networks//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2017:4383-4392.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_39" title="SANDLER M, HOWARD A, ZHU M L, et al.Mobile Net V2:Inverted Residuals and Linear Bottlenecks[C/OL].[2018-10-24].https://arxiv.org/pdf/1801.04381v3.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mobile Net V2:Inverted Residuals and Linear Bottlenecks[C/OL]">
                                        <b>[39]</b>
                                        SANDLER M, HOWARD A, ZHU M L, et al.Mobile Net V2:Inverted Residuals and Linear Bottlenecks[C/OL].[2018-10-24].https://arxiv.org/pdf/1801.04381v3.pdf.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_40" title="IOFFE S, SZEGEDY C.Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift//Proc of the 32nd International Conference on Machine Learning.New York, USA:Springer, 2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[40]</b>
                                        IOFFE S, SZEGEDY C.Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift//Proc of the 32nd International Conference on Machine Learning.New York, USA:Springer, 2015:448-456.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_41" title="JIA Y Q, SHELHAMER E, DONAHUE J, et al.Caffe:Convolutional Architecture for Fast Feature Embedding[C/OL].[2018-10-24].https://arxiv.org/pdf/1408.5093.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffe:Convolutional Architecture for Fast Feature Embedding[C/OL]">
                                        <b>[41]</b>
                                        JIA Y Q, SHELHAMER E, DONAHUE J, et al.Caffe:Convolutional Architecture for Fast Feature Embedding[C/OL].[2018-10-24].https://arxiv.org/pdf/1408.5093.pdf.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_42" title="KRIZHEVSKY A.Learning Multiple Layers of Features from Tiny Images[C/OL].[2018-10-24].https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Multiple Layers of Features from Tiny Images[C/OL]">
                                        <b>[42]</b>
                                        KRIZHEVSKY A.Learning Multiple Layers of Features from Tiny Images[C/OL].[2018-10-24].https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_43" title="NETZER Y, WANG T, COATES A, et al.Reading Digits in Natural Images with Unsupervised Feature Learning//Proc of the NIPSWorkshop on Deep Learning and Unsupervised Feature Learning.Berlin, Germany:Springer, 2011.DOI:10.2118/18761-MS." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reading digits in natural images with unsupervised feature learning">
                                        <b>[43]</b>
                                        NETZER Y, WANG T, COATES A, et al.Reading Digits in Natural Images with Unsupervised Feature Learning//Proc of the NIPSWorkshop on Deep Learning and Unsupervised Feature Learning.Berlin, Germany:Springer, 2011.DOI:10.2118/18761-MS.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_44" title="FREEMAN I, ROESE-KOERNER L, KUMMERT A.Effnet:An Efficient Structure For Convolutional Neural Networks//Proc of the 25th IEEE International Conference on Image Processing.Washington, USA:IEEE, 2018:6-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Effnet:An Efficient Structure For Convolutional Neural Networks">
                                        <b>[44]</b>
                                        FREEMAN I, ROESE-KOERNER L, KUMMERT A.Effnet:An Efficient Structure For Convolutional Neural Networks//Proc of the 25th IEEE International Conference on Image Processing.Washington, USA:IEEE, 2018:6-10.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_45" title="EVERINGHAM M, VAN GOOL L, WILLIAMS C K I, et al.The PASCAL Visual Object Classes (VOC) Challenge.International Journal of Computer Vision, 2010, 88 (2) :303-338.." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MDgzODlyTzRIdEhQcVlkSFkrSUxZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDSGxWNzNPSWw4PU5qN0Jh&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[45]</b>
                                        EVERINGHAM M, VAN GOOL L, WILLIAMS C K I, et al.The PASCAL Visual Object Classes (VOC) Challenge.International Journal of Computer Vision, 2010, 88 (2) :303-338..
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_46" title="LIU W, ANGUELOV D, ERHAN D, et al.SSD:Single Shot Multi Box Detector//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot Multi Box Detector">
                                        <b>[46]</b>
                                        LIU W, ANGUELOV D, ERHAN D, et al.SSD:Single Shot Multi Box Detector//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:21-37.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(03),237-246 DOI:10.16451/j.cnki.issn1003-6059.201903005            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于特征图切分的轻量级卷积神经网络</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E9%9B%A8%E4%B8%B0&amp;code=41420907&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张雨丰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%91%E5%BF%A0%E9%BE%99&amp;code=11547617&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郑忠龙</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%8D%8E%E6%96%87&amp;code=26833864&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘华文</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%91%E9%81%93%E7%BA%A2&amp;code=41420908&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">向道红</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%95%E5%B0%8F%E5%8D%AB&amp;code=09470976&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">何小卫</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E7%9F%A5%E8%8F%B2&amp;code=14587135&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李知菲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%95%E4%BE%9D%E7%84%B6&amp;code=41420909&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">何依然</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=KHODJA%20Abd%20Erraouf&amp;code=41420910&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">KHODJA Abd Erraouf</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%99%E6%B1%9F%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%B3%BB&amp;code=0075331&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">浙江师范大学计算机科学与工程系</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%99%E6%B1%9F%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E6%95%B0%E5%AD%A6%E7%B3%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">浙江师范大学数学系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>卷积神经网络模型所需的存储容量和计算资源远超出移动和嵌入式设备的承载量, 因此文中提出轻量级卷积神经网络架构 (SFNet) .SFNet架构引入切分模块的概念, 通过将网络的输出特征图进行“切分”处理, 每个特征图片段分别输送给不同大小的卷积核进行卷积运算, 将运算得到的特征图拼接后由大小为1×1的卷积核进行通道融合.实验表明, 相比目前通用的轻量级卷积神经网络, 在卷积核数目及输入特征图通道数相同时, SFNet的参数和计算量更少, 分类正确率更高.相比标准卷积, 在网络复杂度大幅降低的情况下, 切分模块的分类正确率持平甚至更高.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">轻量级网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%87%E5%88%86%E6%A8%A1%E5%9D%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">切分模块;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E5%9B%BE%E5%88%87%E5%88%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征图切分;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%84%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">组卷积;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张雨丰, 硕士研究生, 主要研究方向为深度学习.E-mail:527353785@qq.com.;
                                </span>
                                <span>
                                    *郑忠龙 (通讯作者) , 博士, 教授, 主要研究方向为模式识别、机器学习、图像处理等.E-mail:zhonglong@zjnu.edu.cn.;
                                </span>
                                <span>
                                    刘华文, 博士, 教授, 主要研究方向为机器学习、数据挖掘等.E-mail:hwliu@zjnu.edu.cn.;
                                </span>
                                <span>
                                    向道红, 博士, 副教授, 主要研究方向为统计机器学习、稳健统计、深度学习等.E-mail:daohongxiang@zjnu.cn.;
                                </span>
                                <span>
                                    何小卫, 硕士, 教授, 主要研究方向为机器学习、图像视频处理等.E-mail:jhhxw@zjnu.edu.cn.;
                                </span>
                                <span>
                                    李知菲, 硕士, 讲师, 主要研究方向为模式识别、深度学习、虚拟现实等.E-mail:zjnulzf@163.com.;
                                </span>
                                <span>
                                    何依然, 硕士研究生, 主要研究方向为机器学习.E-mail:583587931@qq.com.;
                                </span>
                                <span>
                                    KHODJA Abd Erraouf, 博士研究生, 主要研究方向为计算机视觉、图像处理等.E-mail:201710800010@zjnu.edu.cn.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.61672467, 61572443, 11871438) 资助;</span>
                    </p>
            </div>
                    <h1>A Lightweight Convolutional Neural Network Architecture with Slice Feature Map</h1>
                    <h2>
                    <span>ZHANG Yufeng</span>
                    <span>ZHENG Zhonglong</span>
                    <span>LIU Huawen</span>
                    <span>XIANG Daohong</span>
                    <span>HE Xiaowei</span>
                    <span>LI Zhifei</span>
                    <span>HE Yiran</span>
                    <span>KHODJA Abd Erraouf</span>
            </h2>
                    <h2>
                    <span>Department of Computer Science, Zhejiang Normal University</span>
                    <span>Department of Mathematics, Zhejiang Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The capacities of mobile and embedded devices are quite inadequate for the requirement of the storage capacity and computational resources of convolutional neural network models. Therefore, a lightweight convolutional neural network architecture, network with slice feature map, named SFNet, is proposed. The concept of slice block is introduced. By performing the “slice”processing on the output feature map of the network, each feature map segment is respectively sent to a convolution kernel of different sizes for convolution operation, and then the obtained feature map is concatenated. A simple 1×1 convolution is utilized to fuse the channels of the feature map. The experiments show that compared with the state-of-the-art lightweight convolutional neural networks, SFNet has fewer parameters and floatingpoint operations, and higher classification accuracy with the same number of convolution kernels and input feature map channels. Compared with the standard convolution, in the case of a significant reduction in network complexity, the classification accuracy is same or higher.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Lightweight%20Network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Lightweight Network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Slice%20Block&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Slice Block;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Feature%20Slice%20Map&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Feature Slice Map;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Group%20Convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Group Convolution;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Yufeng, master student. His research interests include deep learning.;
                                </span>
                                <span>
                                    ZHENG Zhonglong ( Corresponding author) . Ph. D., professor. His research interests include pattern recognition, machine learning and image processing.;
                                </span>
                                <span>
                                    LIU Huawen, Ph. D., professor. His research interests include machine learning and data mining.;
                                </span>
                                <span>
                                    XIANG Daohong, Ph.D., associate professor. Her research interests include statistical machine learning, robust statistics and deep learning.;
                                </span>
                                <span>
                                    HE Xiaowei, master, professor. His research interests include machine learning, image and video processing.;
                                </span>
                                <span>
                                    LI Zhifei, master, lecturer. His research interests include pattern recognition, deep learning and virtual reality.;
                                </span>
                                <span>
                                    HE Yiran, master student. Her research interests include machine learning.;
                                </span>
                                <span>
                                    KHODJA Abd Erraouf, Ph.D. candidate. His research interests include computer vision and image processing .;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-11-15</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.61672467, 61572443, 11871438);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="201">自从2012年AlexNet<citation id="348" type="reference"><link href="109" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>被提出以来, 卷积神经网络已经普遍应用于多个领域并取得较好效果, 如目标检测<citation id="349" type="reference"><link href="111" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、图像分割<citation id="350" type="reference"><link href="113" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、图像分类<citation id="351" type="reference"><link href="115" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等.许多研究致力于提高网络在ImageNet<citation id="352" type="reference"><link href="117" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等公共数据集上的准确率, 因此设计更深更复杂的网络<citation id="353" type="reference"><link href="119" rel="bibliography" /><link href="121" rel="bibliography" /><link href="123" rel="bibliography" /><link href="125" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="202">由于卷积神经网络模型过于庞大, 训练和预测时对机器的性能要求较高, 当普通的卷积神经网络模型应用于机器人和自动驾驶等实际应用领域时困难较多.因为这些领域通常只拥有非常有限的存储资源和计算能力, 因此, 在实时性要求较高的场景中, 人们更关注轻量级的网络架构.</p>
                </div>
                <div class="p1">
                    <p id="203">针对卷积神经网络的轻量级实现, 目前主要有参数修剪<citation id="354" type="reference"><link href="127" rel="bibliography" /><link href="129" rel="bibliography" /><link href="131" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>与共享<citation id="355" type="reference"><link href="133" rel="bibliography" /><link href="135" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>、权重矩阵分解<citation id="356" type="reference"><link href="137" rel="bibliography" /><link href="139" rel="bibliography" /><link href="141" rel="bibliography" /><link href="143" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>、参数量化<citation id="357" type="reference"><link href="145" rel="bibliography" /><link href="147" rel="bibliography" /><link href="149" rel="bibliography" /><link href="151" rel="bibliography" /><link href="153" rel="bibliography" /><link href="155" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>,<a class="sup">23</a>,<a class="sup">24</a>]</sup></citation>、知识精炼<citation id="358" type="reference"><link href="157" rel="bibliography" /><link href="159" rel="bibliography" /><link href="161" rel="bibliography" /><sup>[<a class="sup">25</a>,<a class="sup">26</a>,<a class="sup">27</a>]</sup></citation>和设计高效的网络结构等方法.前面4种方法大多都是对预训练网络进行处理, 存在一些问题.首先大部分对预训练网络进行压缩的方法需要繁琐的反复训练.其次对预训练网络的近似只能提高速度和减少内存, 并不能提高正确率.因此, 设计高效的网络结构也成为卷积网络轻量级实现的重要方向.</p>
                </div>
                <div class="p1">
                    <p id="204">从最初的AlexNet<citation id="359" type="reference"><link href="109" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>中使用的11×11大小的卷积核到ZFNet<citation id="360" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>使用的7×7大小的卷积核, 人们渐渐发现将小卷积核堆叠使用可以完全代替大的卷积核, 并减少大量的参数和计算量.Szegedy等<citation id="361" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>使用1×3和3×1这样更小的卷积核构建网络.Lin等<citation id="362" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>使用全局平均池化 (Global Average Pooling) 层代替卷积神经网络中的全连接层, 在减少参数的同时获得更好效果.He等<citation id="363" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>在提出的残差网络 (Residual Network, ResNet) 中使用瓶颈结构 (BottleNeck) , 减少参数并加速网络.</p>
                </div>
                <div class="p1">
                    <p id="205">最早的多分支结构 (Multi-branch) 来自于Szegedy等<citation id="364" type="reference"><link href="119" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出的Inception模块, 使用4种不同大小的卷积核对输入进行操作, 并把各部分输出在通道维度上进行拼接.一种特殊的多分支结构是在ResNet<citation id="365" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>中使用的恒等连接 (Identity Connection) , 能在不增加参数的情况下提升网络的训练速度和正确率, 并使网络免受退化现象的影响.随后Szegedy等<citation id="368" type="reference"><link href="165" rel="bibliography" /><link href="169" rel="bibliography" /><sup>[<a class="sup">29</a>,<a class="sup">31</a>]</sup></citation>又改进初始的Inception模块, 提出多个改进版本.文献<citation id="366" type="reference">[<a class="sup">32</a>]</citation>和文献<citation id="367" type="reference">[<a class="sup">33</a>]</citation>中方法同样在网络结构中使用多分支结构, 并取得减少参数和加速网络的效果.</p>
                </div>
                <div class="p1">
                    <p id="206">组卷积 (Group Convolution) 最早是为了应对图像处理器 (Graphics Processing Unit, GPU) 显存不足而在AlexNet<citation id="369" type="reference"><link href="109" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>中使用.Ioannou等<citation id="370" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>发现组卷积既能减少复杂度, 也能减少学习到的卷积核与前层之间的依赖, 从而提高泛化能力, 于是提出采用根结构进行组卷积, 即随着深度的增加, 使用的组数逐渐减少.Xie等<citation id="371" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>提出的聚合残差变换网络 (Aggregated Residual Transformation Networks, ResNeXt) 中的“NeXt”暗示网络的另一个维度, 称为Cardinality, 即组数.在相同参数情况下, 增加组数比增加深度和宽度对网络性能的提升更大.Chollet等<citation id="372" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>提出深度可分离卷积 (Depthwise Separable Convolution) .随后Howard等<citation id="373" type="reference"><link href="125" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出的MobileNet也沿用深度可分离卷积的思想.Zhang等<citation id="374" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">37</a>]</sup></citation>提出在组卷积之后增加随机重排代替1×1卷积, 可以减少参数并达到通道融合的效果.Zhang等<citation id="375" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>提出交错组卷积网络 (Interleaved Group Convolutional Neural Network, IGCNet) , 将卷积分成两个阶段组卷积, 第一阶段进行空间组卷积, 第二阶段进行1×1组卷积.在同样参数和计算复杂度下, IGCNet比常规卷积更宽, 效果更好.Sandler等<citation id="376" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">39</a>]</sup></citation>提出MobileNet V2结构, 继续沿用深度可分离卷积的思想, 并且加入导致残差和线性瓶颈结构.</p>
                </div>
                <div class="p1">
                    <p id="207">受到多分支结构和组卷积结构的启发, 本文提出特征图切分网络 (Network with Slice Feature Map, SFNet) .多分支结构是在同一输入特征图上应用不同的卷积核进行卷积操作, 组卷积是将特征图进行分组, 在不同的组上使用相同的卷积核进行卷积操作.SFNet构架引入切分模块 (Slice Block) 的概念, 在特征图的不同片段上分别使用大小形状各不相同的多种形式的卷积核进行卷积操作, 增加特征提取的多样性.实验表明, 相比目前通用的轻量级卷积神经网络, 在卷积核数目及输入特征图通道数相同时, SFNet的参数和计算量更少, 分类正确率更高.相比标准卷积, 在网络复杂度大幅降低的情况下, 切分模块的分类正确率持平甚至更高.</p>
                </div>
                <h3 id="208" name="208" class="anchor-tag">1 特征图切分网络架构</h3>
                <h4 class="anchor-tag" id="209" name="209">1.1 常规卷积结构</h4>
                <div class="p1">
                    <p id="210">使用适当尺寸的卷积核是减少卷积神经网络参数量和计算量的有效方法之一.在最早的AlexNet<citation id="377" type="reference"><link href="109" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>中, 作者使用11×11大小的卷积核, 虽然在视野上更宽广, 但是带来的参数量和计算量也过于庞大.之后人们发现, 堆叠使用小尺寸的卷积核可以代替大尺寸卷积核的使用, 并且拥有更少的参数和计算量, 如使用两个3×3的卷积核代替一个5×5的卷积核, 在拥有相同通道和卷积核个数的情况下前者的参数量和计算量都是后者的<image id="337" type="formula" href="images/MSSB201903006_33700.jpg" display="inline" placement="inline"><alt></alt></image>.目前主流的网络几乎都使用尺寸为3×3的卷积核.</p>
                </div>
                <h4 class="anchor-tag" id="215" name="215">1.2 多分支卷积</h4>
                <div class="p1">
                    <p id="216">多分支卷积结构是在同层使用不同形状的卷积核对特征图进行特征提取.这种结构在许多网络中都使用到, 如在经典的Inception<citation id="378" type="reference"><link href="119" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>模块中, 它使用1×1, 3×3和5×5的卷积核及3×3的最大池化操作对输入特征图分别进行运算, 再把运算得到的特征图按通道维度进行串联, 作为下一层的输入.</p>
                </div>
                <div class="p1">
                    <p id="217">Ioannou等<citation id="379" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>提出使用低秩卷积核, 使网络更高效, 将常规的多分支卷积分解成两个步骤, 第一步使用不同大小的卷积核 (大小分别为1×3, 3×1) 对特征图进行卷积操作, 第二步使用1×1的卷积核对第一步输出的特征图进行卷积运算.</p>
                </div>
                <div class="p1">
                    <p id="218">如果假设上述结构中卷积核大小为D<sub>k</sub>×1, 1×D<sub>k</sub>, 个数为<image id="338" type="formula" href="images/MSSB201903006_33800.jpg" display="inline" placement="inline"><alt></alt></image>.特征图大小为D<sub>f</sub>×D<sub>f</sub>, 通道数为M, 使用上述结构的参数量为</p>
                </div>
                <div class="area_img" id="221">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_22100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="222">计算量为</p>
                </div>
                <div class="area_img" id="223">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_22300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="224">在卷积神经网络中, 通常M和N取相同的值, 卷积核尺寸D<sub>k</sub>一般取值为3, 所以上述结构的计算量和参数量是常规卷积的<image id="339" type="formula" href="images/MSSB201903006_33900.jpg" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <div class="p1">
                    <p id="228">本文使用4种不同尺寸的卷积核, 如图1所示.卷积核的空间大小分别为1×3, 3×1, 3×3, 1×1.个数均为<image id="340" type="formula" href="images/MSSB201903006_34000.jpg" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <div class="area_img" id="232">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903006_23200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文使用的4种卷积核Fig.1 Four proposed convolution kernels" src="Detail/GetImg?filename=images/MSSB201903006_23200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文使用的4种卷积核Fig.1 Four proposed convolution kernels  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903006_23200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="233" name="233">1.3 深度可分离卷积</h4>
                <div class="p1">
                    <p id="234">深度可分离卷积是MobileNet网络<citation id="380" type="reference"><link href="125" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>的组成核心, 将常规卷积分成深度卷积和逐点卷积两层.第一层深度卷积通过在输入特征图的每个通道上单独使用单通道卷积核进行滤波, 相对标准卷积, 深度卷积非常高效.但是, 它只是对单个输入通道进行操作, 而不会将它们结合.因此, 需要一个额外的层将各通道的输出进行线性组合.所以在第二层使用1×1的逐点卷积对第一步输出特征图的各个通道进行组合.这样的操作可以大幅减少模型参数和计算量.本文也使用深度可分离卷积, 进一步减少网络的参数和计算量.</p>
                </div>
                <div class="p1">
                    <p id="235">作为常规卷积的替代, 可使用深度可分离卷积, 在显著减少参数和计算量的前提下, 精度下降不多.深度可分离卷积的参数为</p>
                </div>
                <div class="area_img" id="236">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_23600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="237">计算量为</p>
                </div>
                <div class="area_img" id="238">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_23800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="239">两者均为标准卷积的<image id="341" type="formula" href="images/MSSB201903006_34100.jpg" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <h4 class="anchor-tag" id="240" name="240">1.4 残差连接</h4>
                <div class="p1">
                    <p id="241">为了构建更深的网络, 消除网络的退化现象, ResNet<citation id="381" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出残差连接.每个残差块包含两个分支, 一条分支使用普通卷积进行操作, 另一条分支为恒等映射.具体公式如下:</p>
                </div>
                <div class="area_img" id="242">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_24200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="243">其中, x<sub>i</sub>表示第i个残差块的输入, x<sub>i+1</sub>表示第i个残差块的输出, F<sub>i</sub> (·) 表示卷积层需要学习的变换.</p>
                </div>
                <div class="p1">
                    <p id="244">残差结构可以看作是多分支结构的一种变体.由于本文的切分结构是将一层常规卷积拆分成两层, 因此深度有所增加, 所以本文也使用残差连接结构, 使网络不至于出现退化现象.</p>
                </div>
                <h4 class="anchor-tag" id="245" name="245">1.5 切分结构</h4>
                <div class="p1">
                    <p id="246">不同于传统的构建网络方式, 本文不是只使用组卷积结构或只使用多分支结构, 而是结合二者构建网络.多分支结构能够提取图像多尺度特征, 并且在各分支上使用低秩卷积核能大幅降低网络的复杂度, 而组卷积不仅能够减少层级卷积核之间的依赖性, 同时可以减少网络的参数和计算量.本文结合此两种方法, 提出切分模块 (Slice Block) 的概念.SFNet在切分模块的第一层使用4种形状均不相同的卷积核, 也就意味着该层拥有4种不同的感受野, 能同时提取4种不同尺度的特征信息.在第二层进行通道融合则是将不同尺度的特征进行融合.对比MobileNet及MobileNet V2中使用的深度可分离卷积, 每幅特征图均只使用3×3的卷积核进行卷积运算, 提取的特征在一定程度上具有尺度单一性.</p>
                </div>
                <div class="p1">
                    <p id="247">对输入特征图进行切分处理是SFNet网络的核心思想.如图2所示, 切分结构分为两层, 一共三个步骤.第一层进行两步操作———切分特征图和卷积操作, 第二层只进行通道融合.这样分解可以大幅减少计算量和模型大小.</p>
                </div>
                <div class="area_img" id="248">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903006_24800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 切分模块结构Fig.2 Structure of slice block" src="Detail/GetImg?filename=images/MSSB201903006_24800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 切分模块结构Fig.2 Structure of slice block  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903006_24800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="249">标准卷积层以D<sub>f</sub>×D<sub>f</sub>×M的特征图F作为输入, 产生D<sub>g</sub>×D<sub>g</sub>×N的输出特征图G.一般情况下, 卷积层输入和输出的特征图都是方形, 所以D<sub>f</sub>表示输入特征图的宽和高, M表示输入特征图的通道数, 即深度.在具有合适大小的0填充的情况下, 卷积层的输入和输出特征图具有相同大小, 所以D<sub>g</sub>=D<sub>f</sub>, N表示输出特征图的通道数.</p>
                </div>
                <div class="p1">
                    <p id="250">标准卷积层通过卷积核对输入特征图进行操作, 卷积核K的尺寸为D<sub>k</sub>×D<sub>k</sub>×M×N, 标准卷积层一般使用方形的卷积核.D<sub>k</sub>为卷积核的大小, M为卷积核的通道数, N为卷积核的个数, 即输出通道数.</p>
                </div>
                <div class="p1">
                    <p id="251">这样, 当步长为1, 并添加合适的0填充的情况下, 一个标准卷积层的参数量为</p>
                </div>
                <div class="area_img" id="252">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_25200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="253">卷积层的计算量为</p>
                </div>
                <div class="area_img" id="254">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_25400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="255">其中卷积层参数量与卷积核大小D<sub>k</sub>、输入通道数M和卷积核个数N有关, 而卷积层计算量与卷积核参数量及特征图大小D<sub>f</sub>有关.所以可通过切分特征图打破输入特征图通道数与卷积核通道数的相等关系, 并通过使用低秩卷积核减少每个卷积核参数量, 从而减少卷积层的参数和计算量.</p>
                </div>
                <div class="p1">
                    <p id="256">标准卷积操作可以通过使用切分结构将滤波和组合步骤分成三个步骤, 降低计算成本:1) 将上一层输出的特征图沿着通道这一维度进行切分 (在本文中将特征图切分为四部分.2) 将每部分分别输送给大小形状不一的四组卷积核, 分别进行卷积运算, 可以额外使用深度卷积代替常规卷积, 进一步降低网络的参数和计算量.3) 拼接深度卷积得到输出特征图, 第二层使用1×1的卷积融合特征图各个通道算法步骤如下所示.</p>
                </div>
                <div class="p1">
                    <p id="257">算法1切分模块基本算法</p>
                </div>
                <div class="p1">
                    <p id="342">输入上一模块输出特征图, 大小为D<sub>f</sub>×D<sub>f</sub>×M</p>
                </div>
                <div class="p1">
                    <p id="343">输出本模块输出特征图, 大小为D<sub>f</sub>×D<sub>f</sub>×M</p>
                </div>
                <div class="p1">
                    <p id="259">step 1将输入沿通道维度平均切分成4部分</p>
                </div>
                <div class="p1">
                    <p id="260">step 2第一部分使用1×D<sub>k</sub>×1卷积核, 第二部分使用D<sub>k</sub>×1×1卷积核, 第三部分使用D<sub>k</sub>×D<sub>k</sub>×1卷积核, 第四部分使用1×1×1卷积核, 分别进行卷积操作.</p>
                </div>
                <div class="p1">
                    <p id="261">step 3将step 2中得到的4部分特征图沿通道维度进行拼接.</p>
                </div>
                <div class="p1">
                    <p id="262">step 4将step 3中得到的特征图使用1×1×M的卷积核进行通道融合, 得到本模块输出.</p>
                </div>
                <div class="p1">
                    <p id="344">假设输入特征图大小为D<sub>f</sub>×D<sub>f</sub>×M, 第一层将输入特征图切分成4部分, 每部分单独拥有<image id="345" type="formula" href="images/MSSB201903006_34500.jpg" display="inline" placement="inline"><alt></alt></image><image id="345" type="formula" href="images/MSSB201903006_34501.jpg" display="inline" placement="inline"><alt></alt></image>大小的特征图, 本文使用4种形状各不相同的卷积核分别对每份特征图进行卷积操作, 则这4个卷积核大小分别为</p>
                </div>
                <div class="p1">
                    <p id="267">积核分别对每份特征图进行卷积操作, 则这4个卷积核大小分别为</p>
                </div>
                <div class="area_img" id="268">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_26800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="269">本文希望通过使用不同的卷积核让该网络学习到不同的特征, 于是第一层卷积的参数量为</p>
                </div>
                <div class="area_img" id="270">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_27000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="271">计算量为</p>
                </div>
                <div class="area_img" id="272">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_27200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="273">如果使用额外的深度卷积, 则这4种卷积核大小分别为</p>
                </div>
                <div class="area_img" id="274">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_27400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="275">该层的参数量为</p>
                </div>
                <div class="area_img" id="276">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_27600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="277">计算量为</p>
                </div>
                <div class="area_img" id="278">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_27800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="279">经过上述操作, 特征图在空间维度上得到较好的滤波, 但是还未在通道维度上进行任何处理, 于是使用1×1的卷积, 对其特征进行融合.</p>
                </div>
                <div class="p1">
                    <p id="280">将上一层各部分输出进行拼接, 并输入到1×1卷积层, 则该输入大小依旧为D<sub>f</sub>×D<sub>f</sub>×M, 在这一层卷积核的大小为1×1×M×N.于是该层参数量为M×N, 计算量为D<sub>f</sub>×D<sub>f</sub>×M×N.</p>
                </div>
                <div class="p1">
                    <p id="281">所以切分结构的总参数量为</p>
                </div>
                <div class="area_img" id="282">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_28200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="283">总计算量为</p>
                </div>
                <div class="area_img" id="284">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_28400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="285">相比标准卷积, SFNet计算量和参数量只是原来的</p>
                </div>
                <div class="area_img" id="286">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_28600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="287">取D<sub>k</sub>=3, 则参数和计算量是标准卷积的<image id="346" type="formula" href="images/MSSB201903006_34600.jpg" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <div class="p1">
                    <p id="288">相比MobileNet提出的深度可分离卷积, SFNet的参数和计算量均减少<image id="347" type="formula" href="images/MSSB201903006_34700.jpg" display="inline" placement="inline"><alt></alt></image>, 这在构建小网络时至关重要, 因为小网络通常N取值较小.表1显示在构建小网络时, 通道数取不同值时不同结构每层网络所需的参数和计算量.</p>
                </div>
                <div class="area_img" id="289">
                                            <p class="img_tit">
                                                表1 通道数不同时单个网络模块的参数和计算量Table 1Parameters and floating-point operations of a single network module with different number of channels
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903006_28900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201903006_28900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903006_28900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 通道数不同时单个网络模块的参数和计算量Table 1Parameters and floating-point operations of a single network module with different number of channels" src="Detail/GetImg?filename=images/MSSB201903006_28900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="290">假设输入通道数M等于输出通道数N, 卷积核大小D<sub>k</sub>=3, 特征图大小D<sub>f</sub>=14, 取正常卷积的参数量为基准, 系数表示与正常卷积相比参数量的比例系数.由表1可以看出, N取值越小, SFNet比MobileNet减少的参数和计算量越多.</p>
                </div>
                <h4 class="anchor-tag" id="291" name="291">1.6 模型架构</h4>
                <div class="p1">
                    <p id="292">基于1.5节提出的切分模块构建网络SFNet, 在网络的第一层, 由于输入为RGB图像, 只有3个通道, 所以在第一层依然使用常规卷积, 因为这样并不会让网络拥有过多的参数和计算量.在第二层与全局平均池化之间的所有卷积层上, 都使用切分模块构建网络, 并且在切分模块中的每个卷积层后都加上批归一化 (Batch Normalization, BN) <citation id="382" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">40</a>]</sup></citation>和线性整流函数 (Rectified Linear Unit, ReLU) .图3对比常规卷积与切分结构的差异.</p>
                </div>
                <div class="area_img" id="293">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903006_29300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 切分模块和标准卷积中使用Batch Normalization和ReLU的位置Fig.3 Location of batch normalization and ReLU used in the slice block and standard convolution" src="Detail/GetImg?filename=images/MSSB201903006_29300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 切分模块和标准卷积中使用Batch Normalization和ReLU的位置Fig.3 Location of batch normalization and ReLU used in the slice block and standard convolution  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903006_29300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="293">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903006_29301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 切分模块和标准卷积中使用Batch Normalization和ReLU的位置Fig.3 Location of batch normalization and ReLU used in the slice block and standard convolution" src="Detail/GetImg?filename=images/MSSB201903006_29301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 切分模块和标准卷积中使用Batch Normalization和ReLU的位置Fig.3 Location of batch normalization and ReLU used in the slice block and standard convolution  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903006_29301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="296">本文使用步长为2的卷积层代替池化层进行下采样处理, 并在该层之后将卷积核个数翻倍以保证网络的表征能力.在网络的最后使用全局平均池化, 将输出特征图的空间分辨率降为1.在全局平均池化之后使用1×1卷积层代替全连接层, 用于构建全卷积网络.整个SFNet网络架构如表2所示.</p>
                </div>
                <div class="area_img" id="297">
                                            <p class="img_tit">
                                                表2 SFNet具体结构Table 2SFNet architecture
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903006_29700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201903006_29700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903006_29700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 SFNet具体结构Table 2SFNet architecture" src="Detail/GetImg?filename=images/MSSB201903006_29700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="298">由于使用残差连接, 所以在步长为2的层中, 在残差分支上额外增加1个步长为2的1×1卷积层, 保证特征图大小和通道数量得以匹配.如果将切分模块单独计算为两层, 则SFNet具有38层.</p>
                </div>
                <h3 id="299" name="299" class="anchor-tag">2 实验及结果分析</h3>
                <div class="p1">
                    <p id="300">本节评估SFNet在Cifar10及其它数据集上的正确率.在实验中, 每隔1 000次迭代对整个测试集进行1次测试, 实验结果取历次测试的最高值, 并与目前最通用的网络进行对比.结果显示, 在显著降低网络复杂度的情况下, 预测正确率并未出现破坏性的下降.</p>
                </div>
                <h4 class="anchor-tag" id="301" name="301">2.1 实验设置</h4>
                <div class="p1">
                    <p id="302">SFNet模型是在caffe<citation id="383" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">41</a>]</sup></citation>上使用Nesterov加速梯度下降 (Nesterov Accelerated Gradient, NAG) 进行训练, 相比训练大网络, 不存在过拟合的问题, 所以只使用少量的数据增强, 也未对原始数据集进行任何处理, 只是在训练时随机裁剪28×28的图像送入网络进行训练.这样能在一定程度上提升网络精度.</p>
                </div>
                <div class="p1">
                    <p id="303">设置训练时批大小为128, 测试时批大小为100, 一共迭代64 000次.初始学习率设置为0.1, 并在32 000次和48 000次迭代时将学习率减少为之前的1/10.此外, 由于网络参数量相对其它网络来说非常少, 所以只需要使用很小的权重衰减 (L<sub>2</sub>正则化) , 就可以有效训练网络.</p>
                </div>
                <h4 class="anchor-tag" id="304" name="304">2.2 在Cifar10数据集上实验结果</h4>
                <div class="p1">
                    <p id="305">Cifar10数据集<citation id="384" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">42</a>]</sup></citation>包含6万幅彩色图像, 每幅图像的像素大小为32×32, 其中5万幅训练图像, 1万幅测试图像.数据集共有10个类别, 每个类别大约5000幅训练图像和1 000幅测试图像.Cifar10数据集非常适合训练小型网络.</p>
                </div>
                <div class="p1">
                    <p id="306">首先构造一个和SFNet同样是38层的MobileNet, 并且使用相同的训练超参数对它们分别进行训练.从理论上分析, SFNet使用4种大小各不相同的卷积核, 因此可以提取原始图像多种不同尺度的特征, 此外SFNet还使用残差连接, 用于加速训练并且防止网络退化现象.而MobileNet及MobileNet V2只使用3×3的卷积核进行卷积操作, 只能提取单一化的特征.</p>
                </div>
                <div class="p1">
                    <p id="307">表3为各网络在Cifar10数据集上的实验结果对比.从表3中可以看出, SFNet在计算量和参数量均低于MobileNet的情况下, 准确率远超MobileNet, 并且在计算量略低于MobileNet V2的情况下, SFNet正确率比MobileNet V2高出1.3%.本文同时还构建一个较浅的网络, 取名为Shallow＿SFNet, 该网络只有22层, 接近MobileNet的一半, 计算量只有MobileNet的58%.从实验结果可以看出, 正确率依然超过MobileNet.</p>
                </div>
                <div class="area_img" id="308">
                                            <p class="img_tit">
                                                表3 各网络在Cifar10数据集上的分类正确率Table 3Classification accuracy of each network on Cifar10 dataset
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903006_30800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201903006_30800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903006_30800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 各网络在Cifar10数据集上的分类正确率Table 3Classification accuracy of each network on Cifar10 dataset" src="Detail/GetImg?filename=images/MSSB201903006_30800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="309">为了对比目前通用的一些网络, 如38层的IGCNet (IGC-38) <citation id="386" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>、16层的深度根网络 (ROOT-16) <citation id="387" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>、内嵌网络 (Network in Network, NIN) <citation id="388" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>、ResNet-20<citation id="385" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 本文构建卷积核数目加倍的网络, 命名为SFNet2.0, 并且在步长为2的残差分支上将原先的1×1卷积层的卷积核大小改为2×2, 增加其表征能力.将SFNet2.0与ResNet-20对比目前通用的网络, 结果如表3所示, 在显著减少参数和计算量的前提下, 本文网络性能降低并不明显.</p>
                </div>
                <div class="p1">
                    <p id="310">表3中ResNet-20为文献<citation id="389" type="reference">[<a class="sup">7</a>]</citation>中所给的正确率, 文章中使用数据增强操作.在本文中, 不对原数据集进行数据增强, 而是在训练时随机裁剪28×28的图像送入网络进行训练, 这样不仅可以减少计算量, 而且ResNet-20 (本文) 还取得比原来更优的效果.</p>
                </div>
                <h4 class="anchor-tag" id="311" name="311">2.3 在SVHN数据集上实验结果</h4>
                <div class="p1">
                    <p id="312">SVHN数据集<citation id="390" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">43</a>]</sup></citation>是从Google街景图像的门牌号码中获得, 包含73 257幅训练图像, 26 032幅测试图像, 每幅图像大小为32×32.该数据集同样适用小网络的评估.</p>
                </div>
                <div class="p1">
                    <p id="313">表4为各网络在SVHN数据集上的结果.从表中可以看出, 在网络复杂度更低的情况下, 本文构建的网络正确率超过之前的网络.</p>
                </div>
                <div class="area_img" id="314">
                                            <p class="img_tit">
                                                表4 各网络在SVHN数据集上的分类正确率Table 4Classification accuracy of each network on SVHN dataset
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903006_31400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201903006_31400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903006_31400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表4 各网络在SVHN数据集上的分类正确率Table 4Classification accuracy of each network on SVHN dataset" src="Detail/GetImg?filename=images/MSSB201903006_31400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h4 class="anchor-tag" id="315" name="315">2.4 消融实验</h4>
                <div class="p1">
                    <p id="316">为了进一步证明将特征图进行切分并使用不同的卷积核进行卷积运算这一操作能有效减少网络复杂度并且不会造成网络精确度的大幅下降, 本节将SFNet与常规的卷积网络进行对比.</p>
                </div>
                <div class="p1">
                    <p id="317">设计一个常规的卷积神经网络Conv, 使用的卷积核均为3×3的标准卷积核, 一共有20层.</p>
                </div>
                <div class="p1">
                    <p id="318">另外调整SFNet, 除了在网络中不使用深度卷积, 其它操作均与SFNet一致, 命名为Slice＿mult网络, 2个网络的分类正确率如表5所示.在参数量和计算量均为标准网络23%的情况下, 切分操作在Cifar10数据集上只下降0.4%的正确率, 而在SVHN数据集上提高2.1%.</p>
                </div>
                <div class="area_img" id="319">
                                            <p class="img_tit">
                                                表5 2个网络在2个数据集上的分类正确率Table 5 Classification accuracy of 2 networks on 2 datasets
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903006_31900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201903006_31900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903006_31900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表5 2个网络在2个数据集上的分类正确率Table 5 Classification accuracy of 2 networks on 2 datasets" src="Detail/GetImg?filename=images/MSSB201903006_31900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h4 class="anchor-tag" id="320" name="320">2.5 Friedman检验与Nemenyi后续检验</h4>
                <div class="p1">
                    <p id="321">为了进一步对比SFNet和其它轻量级网络在多个数据集上的性能, 本节加入基于算法排序的Friedman检验.根据测试性能由好到坏排序, 并赋予序值.通过对每列序值求平均, 得到平均序值, 结果如表6所示.</p>
                </div>
                <div class="p1">
                    <p id="322">假设在N个数据集上对比k种算法, 令r<sub>i</sub>表示第i种算法的平均序值, 则变量</p>
                </div>
                <div class="area_img" id="323">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_32300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="324">服从自由度为k-1的χ<sup>2</sup>分布.上述“原始Friedman检验”过于保守, 现在通常使用变量</p>
                </div>
                <div class="area_img" id="325">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_32500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="326">
                                            <p class="img_tit">
                                                表6 算法的比较序值Table 6 Comparative order value of algorithm
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903006_32600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201903006_32600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903006_32600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表6 算法的比较序值Table 6 Comparative order value of algorithm" src="Detail/GetImg?filename=images/MSSB201903006_32600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="327">由表6得出τ<sub>F</sub>的值为9, 查表可知, 它大于α=0.1时的F检验临界值5.391, 因此拒绝“所有算法性能相同”这个假设.然后使用Nemenyi后续检验, 使用</p>
                </div>
                <div class="area_img" id="328">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201903006_32800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="329">计算平均序值差别的临界值域.若两种算法平均序值之差超出临界值域CD, 则以相应的置信度拒绝“算法性能相同”这一假设.</p>
                </div>
                <div class="p1">
                    <p id="330">当k=4时, q<sub>0.1</sub>=2.291, 计算临界值域CD=2.96.从表6中可知, SFNet与MobileNet性能显著不同.SFNet、MobileNetV2、SqueezeNet、MobileNet、Mobile-Net V2、SqueezeNet性能无显著差别.</p>
                </div>
                <h4 class="anchor-tag" id="331" name="331">2.6 目标检测</h4>
                <div class="p1">
                    <p id="332">SFNet也能应用于目标检测领域, 在Pascal VOC0712数据集<citation id="391" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">45</a>]</sup></citation>上进行实验.VOC0712训练集包含VOC2007和VOC2012的训练集, 共16 551幅图像, 测试集为VOC2007的测试集, 共4 952幅图像.本次实验中使用SSD (Single Shot Multibox Detector) <citation id="392" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">46</a>]</sup></citation>进行目标检测, 输入分辨率为300×300.分别训练构建的3个网络不同于之前的一些工作, 不使用在ImageNet数据集上预训练的网络, 而是从零开始训练网络.由于只是为了证实SFNet网络在目标检测领域也同样适用, 故对超参数只进行简单调试.实验结果如表7所示.</p>
                </div>
                <div class="area_img" id="333">
                                            <p class="img_tit">
                                                表7 各网络在Pascal VOC数据集上的平均精度均值Table 7Mean of average precision of each network on the Pascal VOC dataset
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201903006_33300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201903006_33300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201903006_33300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表7 各网络在Pascal VOC数据集上的平均精度均值Table 7Mean of average precision of each network on the Pascal VOC dataset" src="Detail/GetImg?filename=images/MSSB201903006_33300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="334">因为未对网络超参数进行仔细调优, 故SFNet-SSD目前在准确率上稍逊于MobileNet-SSD, 但也高于SqueezeNet-SSD.并且SFNet-SSD在参数量和计算量方面有所降低, 更适合于在计算能力和存储能力受限的移动设备部署.</p>
                </div>
                <h3 id="335" name="335" class="anchor-tag">3 结束语</h3>
                <div class="p1">
                    <p id="336">本文提出比MobileNet参数和计算量更少的轻量级网络构建模块, 称为切分模块.切分模块将输入的特征图沿通道维度进行切分, 切分出的各部分分别使用不同大小的卷积核进行卷积运算, 再使用1×1的卷积层进行通道融合.本文将使用切分模块构建的网络取名为SFNet, 该网络具有在显著减少网络复杂度的前提下, 精确度丢失不多的特性.Cifar10数据集上与MobileNet进行对比, 在略低于MobileNet的网络复杂度情况下, SFNet准确率提高5.42%.SFNet2.0在相差仅0.03%的精确度前提下, 网络参数量为ResNet网络的58%.此外, 本文认为深度网络还存在很大的改进空间, 在第一层卷积层中使用每组特征图为可变通道数的组卷积以代替深度网络, 希望进一步降低参数和计算量, 并对正确率不会造成太大的影响.下一步还会尝试将SFNet结构应用于深度学习的其它领域, 如图像分割及视频跟踪, 期望获得较优的表现.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="2" type="formula" href="images/MSSB201903006_00200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">张雨丰</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="4" type="formula" href="images/MSSB201903006_00400.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">郑忠龙</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="6" type="formula" href="images/MSSB201903006_00600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">刘华文</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="8" type="formula" href="images/MSSB201903006_00800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">向道红</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="10" type="formula" href="images/MSSB201903006_01000.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">何小卫</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="12" type="formula" href="images/MSSB201903006_01200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">李知菲</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="14" type="formula" href="images/MSSB201903006_01400.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">何依然</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="16" type="formula" href="images/MSSB201903006_01600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">KHODJA Abd Erraouf</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="109">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep conv olutional neural networks">

                                <b>[1]</b>KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net Classification with Deep Convolutional Neural Networks//PEREIRA F, BURGES C J C, BOTTOU L, et al., eds.Advances in Neural Information Processing Systems 25.Cambridge, USA:The MIT Press, 2012:1097-1105.
                            </a>
                        </p>
                        <p id="111">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towarks real-time object detection with region proposal networks">

                                <b>[2]</b>REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks.IEEETransactions on Pattern Analysis and Machine Intelligence, 2016, 39 (6) :1137-1149.
                            </a>
                        </p>
                        <p id="113">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[3]</b>LONG J, SHELHAMER E, DARRELL T.Fully Convolutional Networks for Semantic Segmentation//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015:3431-3440.
                            </a>
                        </p>
                        <p id="115">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL]">

                                <b>[4]</b>SIMONYAN K, ZISSERMAN A.Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL].[2018-10-24].https://arxiv.org/pdf/1409.1556.pdf.
                            </a>
                        </p>
                        <p id="117">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet:A large-scale hierarchical image database">

                                <b>[5]</b>DENG J, DONG W, SOCHER R, et al.Image Net:A Large-Scale Hie-rarchical Image Database//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2009:248-255.
                            </a>
                        </p>
                        <p id="119">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[6]</b>SZEGEDY C, LIU W, JIA Y Q, et al.Going Deeper with Convolutions//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2015.DOI:10.1109/CVPR.2015.7298594.
                            </a>
                        </p>
                        <p id="121">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[7]</b>HE K M, ZHANG X Y, REN S Q, et al.Deep Residual Learning for Image Recognition//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="123">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DenselyConnected Convolutional Networks">

                                <b>[8]</b>HUANG G, LIU Z, VAN DER LAURENS M, et al.Densely Connected Convolutional Networks//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:2261-2269.
                            </a>
                        </p>
                        <p id="125">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mobile Nets:Efficient convolutional neural networks for mobile vision applications">

                                <b>[9]</b>HOWARD A G, ZHU M L, CHEN B, et al.Mobile Nets:Efficient Convolutional Neural Networks for Mobile Vision Applications[C/OL].[2018-10-24].https://arxiv.org/pdf/1704.04861.pdf.
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning both Weights and Connections for Efficient Neural Network">

                                <b>[10]</b>HAN S, POOL J, TRAN J, et al.Learning Both Weights and Connections for Efficient Neural Network//CORTES C, LAWRENCEN D, LEE D D, et al., eds.Advances in Neural Information Processing Systems 28.Cambridge, USA:The MIT Press, 2015:1135-1143.
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-domain synthesis of medical images using efficient location-sensitive deep network">

                                <b>[11]</b>NGUYEN H V, ZHOU K, VEMULAPALLI R.Cross-Domain Synthesis of Medical Images Using Efficient Location-Sensitive Deep Network//Proc of the International Conference on Medical Image Computing and Computer-Assisted Intervention.Berlin, Germany:Springer, 2015:677-684.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pruning Filters for Efficient ConvNets[C/OL]">

                                <b>[12]</b>LI H, KADAV A, DURDANOVIC I, et al.Pruning Filters for Efficient ConvNets[C/OL].[2018-10-24].https://arxiv.org/pdf/1608.08710.pdf.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep compression:compressing deep neural networks with pruning trained quantization and huffman coding">

                                <b>[13]</b>HAN S, MAO H Z, DALLY W J.Deep Compression:Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding[C/OL].[2018-10-24].https://arxiv.org/pdf/1510.00149.pdf.
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Compressing neural networks with the hashing trick">

                                <b>[14]</b>CHEN W L, WILSON J T, TYREE S, et al.Compressing Neural Networks with the Hashing Trick//Proc of the 32nd International Conference on Machine Learning.Berlin, Germany:Springer, 2015:2285-2294.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting linear structure within convolutional networks for efficient evaluation">

                                <b>[15]</b>DENTON E, ZAREMBA W, BRUNA J, et al.Exploiting Linear Structure within Convolutional Networks for Efficient Evaluation//Proc of the 27th International Conference on Neural Information Processing Systems.Cambridge, USA:The MIT Press, 2014:1269-1277.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning separable filters">

                                <b>[16]</b>SIRONI A, TEKIN B, RIGAMONTI R, et al.Learning Separable Filters.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (1) :94-116.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speeding up Convolutional Neural Networks with Low Rank Expansions[C/OL]">

                                <b>[17]</b>JADERBERG M, VEDALDI A, ZISSERMAN A.Speeding up Convolutional Neural Networks with Low Rank Expansions[C/OL].[2018-10-24].https://arxiv.org/pdf/1405.3866.pdf.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepThin:A Self-Compressing Library for Deep Neural Networks[C/OL]">

                                <b>[18]</b>SOTOUDEH M, BAGHSORKHI S S.DeepThin:A Self-Compressing Library for Deep Neural Networks[C/OL].[2018-10-24].https://arxiv.org/pdf/1802.06944.pdf.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving the Speed of Neural Networks on CPUs">

                                <b>[19]</b>VANHOUCKE V, SENIOR A, MAO M Z.Improving the Speed of Neural Networks on CPUs//Proc of the NIPS Workshop on Deep Learning and Unsupervised Feature Learning.Berlin, Germany:Springer, 2011, I:611-620.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Provable Bounds for Learning Some Deep Representations[C/OL]">

                                <b>[20]</b>ARORA S, BHASKARA A, GE R, et al.Provable Bounds for Learning Some Deep Representations[C/OL].[2018-10-24].https://arxiv.org/pdf/1310.6343.pdf.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fixed-point feedforward deep neural network design using weights+1,0,and-1">

                                <b>[21]</b>HWANG K, SUNG W.Fixed-Point Feedforward Deep Neural Network Design Using Weights+1, 0, and-1//Proc of the IEEEWorkshop on Signal Processing Systems.Washington, USA:IEEE, 2014.DOI:10.1109/SiPS.2014.6986082.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=BinaryConnect:training deep neural networks with binary weights during propagations">

                                <b>[22]</b>COURBARIAUX M, BENGIO Y, DAVID J P.BinaryConnect:Training Deep Neural Networks with Binary Weights During Propagations//CORTES C, LAWRENCE N D, LEE D D, et al., eds.Advances in Neural Information Processing Systems 28.Cambridge, USA:The MIT Press, 2015:3123-3131.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Binarized Neural Networks:Training Deep Neural Networks with Weights and Activations Constrained to+1 or-1[C/OL]">

                                <b>[23]</b>COURBARIAUX M, BENGIO Y.Binarized Neural Networks:Training Deep Neural Networks with Weights and Activations Constrained to+1 or-1[C/OL].[2018-10-24].https://arxiv.org/pdf/1602.02830.pdf.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Xnor-net:Imagenet classification using binary convolutional neural networks">

                                <b>[24]</b>RASTEGAN M, ORDONEZ V, REDMON J, et al.XNOR-Net:Image Net Classification Using Binary Convolutional Neural Networks//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:525-542.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Model Compression">

                                <b>[25]</b>BUCILUA‘C, CARUANA R, NICULESCU-MIZIL A.Model Compression//Proc of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2006:535-541.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Do Deep Nets Really Need to be Deep?[C/OL]">

                                <b>[26]</b>BA L J, CARUANA R.Do Deep Nets Really Need to be Deep?[C/OL].[2018-10-24].https://arxiv.org/pdf/1312.6184v5.pdf.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distilling the Knowledge in a Neural Network[C/OL]">

                                <b>[27]</b>HINTON G, VINYALS O, DEAN J.Distilling the Knowledge in a Neural Network[C/OL].[2018-10-24].https://arxiv.org/pdf/1503.02531.pdf.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing and Understanding Convolutional Networks">

                                <b>[28]</b>ZEILER M D, FERGUS R.Visualizing and Understanding Convolutional Networks//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2013:818-833.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rethinking the inception architecture for computer vision">

                                <b>[29]</b>SZEGEDY C, VANHOUCKE V, IOFFE S, et al.Rethinking the Inception Architecture for Computer Vision//Proc of the IEEEConference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:2818-2826.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Network in Network[C/OL]">

                                <b>[30]</b>LIN M, CHEN Q, YAN S C.Network in Network[C/OL].[2018-10-24].https://arxiv.org/pdf/1312.4400.pdf.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-ResNet and the Impact of Residual Connections on Learning[C/OL]">

                                <b>[31]</b>SZEGEDY C, IOFFE S, VANHOUCKE V, et al.Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning[C/OL].[2018-10-24].https://arxiv.org/pdf/1602.07261.pdf.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SqueezeNet:Alex Net-Level Accuracy with 50x Fewer Parameters and&amp;lt;0.5MBModel Size[C/OL]">

                                <b>[32]</b>IANDOLA F N, HAN S, MOSKEWICZ M W, et al.SqueezeNet:Alex Net-Level Accuracy with 50x Fewer Parameters and&lt;0.5MBModel Size[C/OL].[2018-10-24].https://arxiv.org/pdf/1602.07360.pdf.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Training CN-Ns with Low-Rank Filters for Efficient Image Classification[C/OL]">

                                <b>[33]</b>IOANNOU Y, ROBERTSON D, SHOTTON J, et al.Training CN-Ns with Low-Rank Filters for Efficient Image Classification[C/OL].[2018-10-24].https://arxiv.org/pdf/1511.06744.pdf.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep roots:Improving CNN efficiency with hierarchical filter groups">

                                <b>[34]</b>IOANNOU Y, ROBERTSON D, CIPOLLA R, et al.Deep Roots:Improving CNN Efficiency with Hierarchical Filter Groups//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:5977-5986.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Aggregated Residual Transformations for Deep Neural Networks">

                                <b>[35]</b>XIE S N, GIRSHICK R, DOLLAR P, et al.Aggregated Residual Transformations for Deep Neural Networks//Proc of the IEEEConference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:5987-5995.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Xception:Deep learning with depthwise separable convolutions">

                                <b>[36]</b>CHOLLET F.Xception:Deep Learning with Depthwise Separable Convolutions//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2017:1800-1807.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ShuffleNet:An Extremely Efficient Convolutional Neural Network for Mobile Devices[C/OL]">

                                <b>[37]</b>ZHANG X Y, ZHOU X Y, LIN M X, et al.ShuffleNet:An Extremely Efficient Convolutional Neural Network for Mobile Devices[C/OL].[2018-10-24].https://arxiv.org/pdf/1707.01083.pdf.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interleaved group convolutions for deep neural networks">

                                <b>[38]</b>ZHANG T, QI G J, XIAO B, et al.Interleaved Group Convolutions for Deep Neural Networks//Proc of the IEEE International Conference on Computer Vision.Washington, USA:IEEE, 2017:4383-4392.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mobile Net V2:Inverted Residuals and Linear Bottlenecks[C/OL]">

                                <b>[39]</b>SANDLER M, HOWARD A, ZHU M L, et al.Mobile Net V2:Inverted Residuals and Linear Bottlenecks[C/OL].[2018-10-24].https://arxiv.org/pdf/1801.04381v3.pdf.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_40" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[40]</b>IOFFE S, SZEGEDY C.Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift//Proc of the 32nd International Conference on Machine Learning.New York, USA:Springer, 2015:448-456.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_41" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffe:Convolutional Architecture for Fast Feature Embedding[C/OL]">

                                <b>[41]</b>JIA Y Q, SHELHAMER E, DONAHUE J, et al.Caffe:Convolutional Architecture for Fast Feature Embedding[C/OL].[2018-10-24].https://arxiv.org/pdf/1408.5093.pdf.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_42" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Multiple Layers of Features from Tiny Images[C/OL]">

                                <b>[42]</b>KRIZHEVSKY A.Learning Multiple Layers of Features from Tiny Images[C/OL].[2018-10-24].https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_43" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reading digits in natural images with unsupervised feature learning">

                                <b>[43]</b>NETZER Y, WANG T, COATES A, et al.Reading Digits in Natural Images with Unsupervised Feature Learning//Proc of the NIPSWorkshop on Deep Learning and Unsupervised Feature Learning.Berlin, Germany:Springer, 2011.DOI:10.2118/18761-MS.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_44" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Effnet:An Efficient Structure For Convolutional Neural Networks">

                                <b>[44]</b>FREEMAN I, ROESE-KOERNER L, KUMMERT A.Effnet:An Efficient Structure For Convolutional Neural Networks//Proc of the 25th IEEE International Conference on Image Processing.Washington, USA:IEEE, 2018:6-10.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_45" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MjM2MTJJbDg9Tmo3QmFyTzRIdEhQcVlkSFkrSUxZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDSGxWNzNP&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[45]</b>EVERINGHAM M, VAN GOOL L, WILLIAMS C K I, et al.The PASCAL Visual Object Classes (VOC) Challenge.International Journal of Computer Vision, 2010, 88 (2) :303-338..
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_46" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot Multi Box Detector">

                                <b>[46]</b>LIU W, ANGUELOV D, ERHAN D, et al.SSD:Single Shot Multi Box Detector//Proc of the European Conference on Computer Vision.Berlin, Germany:Springer, 2016:21-37.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201903006" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201903006&amp;v=MTMwNzN5em5WYjNKS0Q3WWJMRzRIOWpNckk5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
