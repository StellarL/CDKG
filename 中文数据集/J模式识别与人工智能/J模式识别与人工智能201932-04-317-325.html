<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131443976436250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dMSSB201904005%26RESULT%3d1%26SIGN%3dGLOvK3KtzgD54Fh80hkaNBJ0iNA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201904005&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201904005&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201904005&amp;v=MzEzMjBLRDdZYkxHNEg5ak1xNDlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6blc3dk8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#88" data-title="1 可扩展的半监督深度网络表示学习模型 ">1 可扩展的半监督深度网络表示学习模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#90" data-title="&lt;b&gt;1.1&lt;/b&gt; 图卷积神经网络"><b>1.1</b> 图卷积神经网络</a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;1.2&lt;/b&gt; 自编码器"><b>1.2</b> 自编码器</a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;1.3&lt;/b&gt; 模型结构"><b>1.3</b> 模型结构</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="2 实验及结果分析 ">2 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#136" data-title="&lt;b&gt;2.1&lt;/b&gt; 实验数据集"><b>2.1</b> 实验数据集</a></li>
                                                <li><a href="#144" data-title="&lt;b&gt;2.2&lt;/b&gt; 基准网络表示学习方法"><b>2.2</b> 基准网络表示学习方法</a></li>
                                                <li><a href="#155" data-title="&lt;b&gt;2.3&lt;/b&gt; 评价标准"><b>2.3</b> 评价标准</a></li>
                                                <li><a href="#173" data-title="&lt;b&gt;2.4&lt;/b&gt; 实验结果"><b>2.4</b> 实验结果</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#201" data-title="3 结 束 语 ">3 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#133" data-title="图1 Semi-GCNAE模型结构图">图1 Semi-GCNAE模型结构图</a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;表1 网络数据集&lt;/b&gt;"><b>表1 网络数据集</b></a></li>
                                                <li><a href="#360" data-title="图2 6种方法在4个数据集上的节点分类结果">图2 6种方法在4个数据集上的节点分类结果</a></li>
                                                <li><a href="#360" data-title="图2 6种方法在4个数据集上的节点分类结果">图2 6种方法在4个数据集上的节点分类结果</a></li>
                                                <li><a href="#361" data-title="图3 6种方法在2个数据集上的可视化结果">图3 6种方法在2个数据集上的可视化结果</a></li>
                                                <li><a href="#197" data-title="&lt;b&gt;表2 6种方法在Cora数据集上的网络重构结果&lt;/b&gt;"><b>表2 6种方法在Cora数据集上的网络重构结果</b></a></li>
                                                <li><a href="#198" data-title="&lt;b&gt;表3 6种方法在Pubmed数据集上的网络重构结果&lt;/b&gt;"><b>表3 6种方法在Pubmed数据集上的网络重构结果</b></a></li>
                                                <li><a href="#199" data-title="&lt;b&gt;表4 6种方法在Wikipedia数据集上的网络重构结果&lt;/b&gt;"><b>表4 6种方法在Wikipedia数据集上的网络重构结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="244">


                                    <a id="bibliography_1" title="HOFF P D, RAFTERY A E, HANDCOCK M S.Latent Space Approaches to Social Network Analysis.Technical Report, 399.Seattle, USA:University of Washington, 2001." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Latent Space Approaches to Social Network Analysis">
                                        <b>[1]</b>
                                        HOFF P D, RAFTERY A E, HANDCOCK M S.Latent Space Approaches to Social Network Analysis.Technical Report, 399.Seattle, USA:University of Washington, 2001.
                                    </a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_2" title="ZHANG D K, YIN J, ZHU X Q, et al.Network Representation Learning[C/OL].[2018-11-25].https://arxiv.org/pdf/1801.05852.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Network Representation Learning[C/OL]">
                                        <b>[2]</b>
                                        ZHANG D K, YIN J, ZHU X Q, et al.Network Representation Learning[C/OL].[2018-11-25].https://arxiv.org/pdf/1801.05852.pdf.
                                    </a>
                                </li>
                                <li id="248">


                                    <a id="bibliography_3" title="BENGIO Y, COURVILLE A, VINCENT P.Representation Learning:A Review and New Perspectives.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 35 (8) :1798-1828." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Representation learning:A review and new perspectives">
                                        <b>[3]</b>
                                        BENGIO Y, COURVILLE A, VINCENT P.Representation Learning:A Review and New Perspectives.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 35 (8) :1798-1828.
                                    </a>
                                </li>
                                <li id="250">


                                    <a id="bibliography_4" title="BHAGAT S, CORMODE G, MUTHUKRISHNAN S.Node Classification in Social Networks//AGGARWAL C C, ed.Social Network Data Analytics.Berlin, Germany:Springer, 2011:115-148." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Node classification in social networks">
                                        <b>[4]</b>
                                        BHAGAT S, CORMODE G, MUTHUKRISHNAN S.Node Classification in Social Networks//AGGARWAL C C, ed.Social Network Data Analytics.Berlin, Germany:Springer, 2011:115-148.
                                    </a>
                                </li>
                                <li id="252">


                                    <a id="bibliography_5" title="VAN DER MAATEN L, HINTON G.Visualizing Data Using t-SNE.Journal of Machine Learning Research, 2008, 9:2579-2605." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing data using t-SNE">
                                        <b>[5]</b>
                                        VAN DER MAATEN L, HINTON G.Visualizing Data Using t-SNE.Journal of Machine Learning Research, 2008, 9:2579-2605.
                                    </a>
                                </li>
                                <li id="254">


                                    <a id="bibliography_6" title="TANG J, LIU J Z, ZHANG M, et al.Visualizing Large-Scale and High-Dimensional Data//Proc of the 25th International Conference on World Wide Web.New York, USA:ACM, 2016:287-297." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing large-scale and high-dimensional data">
                                        <b>[6]</b>
                                        TANG J, LIU J Z, ZHANG M, et al.Visualizing Large-Scale and High-Dimensional Data//Proc of the 25th International Conference on World Wide Web.New York, USA:ACM, 2016:287-297.
                                    </a>
                                </li>
                                <li id="256">


                                    <a id="bibliography_7" title="DING C H Q, HE X F, ZHA H Y, et al.Spectral Min-Max Cut for Graph Partitioning and Data Clustering[C/OL].[2018-11-25].https://escholarship.org/uc/item/0g18c027." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spectral Min-Max Cut for Graph Partitioning and Data Clustering[C/OL]">
                                        <b>[7]</b>
                                        DING C H Q, HE X F, ZHA H Y, et al.Spectral Min-Max Cut for Graph Partitioning and Data Clustering[C/OL].[2018-11-25].https://escholarship.org/uc/item/0g18c027.
                                    </a>
                                </li>
                                <li id="258">


                                    <a id="bibliography_8" title="LIBEN-NOWELL D, KLEINBERG J.The Link-Prediction Problem for Social Networks//Proc of the 20th Annual ACM International Conference on Information and Knowledge Management.New York, USA:ACM, 2003:556-559." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The link prediction problem for social networks">
                                        <b>[8]</b>
                                        LIBEN-NOWELL D, KLEINBERG J.The Link-Prediction Problem for Social Networks//Proc of the 20th Annual ACM International Conference on Information and Knowledge Management.New York, USA:ACM, 2003:556-559.
                                    </a>
                                </li>
                                <li id="260">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                    HE K M, ZHANG X Y, REN S Q, et al.Deep Residual Learning for Image Recognition//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:770-778.</a>
                                </li>
                                <li id="262">


                                    <a id="bibliography_10" title="LEVY O, GOLDBERG Y.Neural Word Embedding as Implicit Matrix Factorization//GHAHARMANI Z, WELLING M, CORTESC, et al., eds.Advances in Neural Information Processing Systems27.Cambridge, USA:The MIT Press, 2014:2177-2185." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural Word Embedding as Implicit Matrix Factorization">
                                        <b>[10]</b>
                                        LEVY O, GOLDBERG Y.Neural Word Embedding as Implicit Matrix Factorization//GHAHARMANI Z, WELLING M, CORTESC, et al., eds.Advances in Neural Information Processing Systems27.Cambridge, USA:The MIT Press, 2014:2177-2185.
                                    </a>
                                </li>
                                <li id="264">


                                    <a id="bibliography_11" title="PEROZZI B, AL-RFOU R, SKIENA S.DeepWalk:Online Learning of Social Representations//Proc of the 20th ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2014:701-710." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deepwalk:Online learning of social representations">
                                        <b>[11]</b>
                                        PEROZZI B, AL-RFOU R, SKIENA S.DeepWalk:Online Learning of Social Representations//Proc of the 20th ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2014:701-710.
                                    </a>
                                </li>
                                <li id="266">


                                    <a id="bibliography_12" title="GROVER A, LESKOVEC J.Node2vec:Scalable Feature Learning for Networks//Proc of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2016:855-864." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Node2vec:Scalable Feature Learning for Networks">
                                        <b>[12]</b>
                                        GROVER A, LESKOVEC J.Node2vec:Scalable Feature Learning for Networks//Proc of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2016:855-864.
                                    </a>
                                </li>
                                <li id="268">


                                    <a id="bibliography_13" title="TANG J, QU M, WANG M Z, et al.LINE:Large-Scale Information Network Embedding//Proc of the 24th International Conference on World Wide Web.New York, USA:ACM, 2015:1067-1077." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LINE:large-scale information network embedding">
                                        <b>[13]</b>
                                        TANG J, QU M, WANG M Z, et al.LINE:Large-Scale Information Network Embedding//Proc of the 24th International Conference on World Wide Web.New York, USA:ACM, 2015:1067-1077.
                                    </a>
                                </li>
                                <li id="270">


                                    <a id="bibliography_14" title="BRUNA J, ZAREMBA W, SZLAM A, et al.Spectral Networks and Locally Connected Networks on Graphs[C/OL].[2018-11-25].https://arxiv.org/pdf/1312.6203.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spectral Networks and Locally Connected Networks on Graphs[C/OL]">
                                        <b>[14]</b>
                                        BRUNA J, ZAREMBA W, SZLAM A, et al.Spectral Networks and Locally Connected Networks on Graphs[C/OL].[2018-11-25].https://arxiv.org/pdf/1312.6203.pdf.
                                    </a>
                                </li>
                                <li id="272">


                                    <a id="bibliography_15" title="CHUNG F R K.Spectral Graph Theory.Providence, USA:American Mathematical Society, 1997." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spectra Graph Theory">
                                        <b>[15]</b>
                                        CHUNG F R K.Spectral Graph Theory.Providence, USA:American Mathematical Society, 1997.
                                    </a>
                                </li>
                                <li id="274">


                                    <a id="bibliography_16" title="LECUN Y, BOTTOU L, BENGIO Y, et al.Gradient-Based Learning Applied to Document Recognition.Proceedings of the IEEE, 1998, 86 (11) :2278-2324." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gradient-based learning applied to document recognition">
                                        <b>[16]</b>
                                        LECUN Y, BOTTOU L, BENGIO Y, et al.Gradient-Based Learning Applied to Document Recognition.Proceedings of the IEEE, 1998, 86 (11) :2278-2324.
                                    </a>
                                </li>
                                <li id="276">


                                    <a id="bibliography_17" title="DEFFERRARD M, BRESSON X, VANDERGHEYNST P.Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering[C/OL].[2018-11-25].https://arxiv.org/pdf/1606.09375.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering[C/OL]">
                                        <b>[17]</b>
                                        DEFFERRARD M, BRESSON X, VANDERGHEYNST P.Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering[C/OL].[2018-11-25].https://arxiv.org/pdf/1606.09375.pdf.
                                    </a>
                                </li>
                                <li id="278">


                                    <a id="bibliography_18" title="HAMMOND D K, VANDERGHEYNST P, GRIBONVAL R.Wavelets on Graphs via Spectral Graph Theory.Applied and Computational Harmonic Analysis, 2011, 30 (2) :129-150." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501147058&amp;v=MjczMjNtVUxiSUpGd2NiaFE9TmlmT2ZiSzdIdEROcW85RVplOElESGt4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5ag==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                        HAMMOND D K, VANDERGHEYNST P, GRIBONVAL R.Wavelets on Graphs via Spectral Graph Theory.Applied and Computational Harmonic Analysis, 2011, 30 (2) :129-150.
                                    </a>
                                </li>
                                <li id="280">


                                    <a id="bibliography_19" title="KIPF T N, WELLING M.Semi-supervised Classification with Graph Convolutional Networks[C/OL].[2018-11-25].https://arxiv.org/pdf/1609.02907.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised Classification with Graph Convolutional Networks[C/OL]">
                                        <b>[19]</b>
                                        KIPF T N, WELLING M.Semi-supervised Classification with Graph Convolutional Networks[C/OL].[2018-11-25].https://arxiv.org/pdf/1609.02907.pdf.
                                    </a>
                                </li>
                                <li id="282">


                                    <a id="bibliography_20" title="ZHU X J, GHAHRAMANI Z, LAFFERTY J.Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions//Proc of the 20th International Conference on Machine Learning.Palo Alto, USA:AAAI Press, 2003:912-919." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning using Gaussian fields and harmonic functions">
                                        <b>[20]</b>
                                        ZHU X J, GHAHRAMANI Z, LAFFERTY J.Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions//Proc of the 20th International Conference on Machine Learning.Palo Alto, USA:AAAI Press, 2003:912-919.
                                    </a>
                                </li>
                                <li id="284">


                                    <a id="bibliography_21" title="WANG D X, CUI P, ZHU W.Structural Deep Network Embedding//Proc of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2016:1225-1234." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structural deep network embedding">
                                        <b>[21]</b>
                                        WANG D X, CUI P, ZHU W.Structural Deep Network Embedding//Proc of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2016:1225-1234.
                                    </a>
                                </li>
                                <li id="286">


                                    <a id="bibliography_22" title="BREIMAN L.Stacked Regression.Machine Learning, 1996, 24 (1) :49-64." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339457&amp;v=MzA4NjNNWU80SVkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkNIbFY3M0FKRmc9Tmo3QmFyTzRIdEhOckl4&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                        BREIMAN L.Stacked Regression.Machine Learning, 1996, 24 (1) :49-64.
                                    </a>
                                </li>
                                <li id="288">


                                    <a id="bibliography_23" title="WOLPERT D H.Stacked Generalization.Neural Networks, 2017, 5 (2) :241-259." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked generalization">
                                        <b>[23]</b>
                                        WOLPERT D H.Stacked Generalization.Neural Networks, 2017, 5 (2) :241-259.
                                    </a>
                                </li>
                                <li id="290">


                                    <a id="bibliography_24" title="BELKIN M, NIYOGI P.Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.Neural Computation, 2003, 15 (6) :1373-1396." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012194&amp;v=MDAwNzNiSzlIdGpNcW85RlpPb05EWFU5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSkZ3Y2JoUT1OaWZKWg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                        BELKIN M, NIYOGI P.Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.Neural Computation, 2003, 15 (6) :1373-1396.
                                    </a>
                                </li>
                                <li id="292">


                                    <a id="bibliography_25" title="BENGION Y.Learning Deep Architectures for AI.Foundations and Trends in Machine Learning, 2009, 2 (1) :1-127." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep architectures for AI">
                                        <b>[25]</b>
                                        BENGION Y.Learning Deep Architectures for AI.Foundations and Trends in Machine Learning, 2009, 2 (1) :1-127.
                                    </a>
                                </li>
                                <li id="294">


                                    <a id="bibliography_26" title="ZHOU D Y, BOUSQUET O, LAL T N, et al.Learning with Local and Global Consistency//Proc of the 16th International Conference on Neural Information Processing Systems.Cambridge, USA:The MIT Press, 2003:321-328" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning with Local and Global Consistency">
                                        <b>[26]</b>
                                        ZHOU D Y, BOUSQUET O, LAL T N, et al.Learning with Local and Global Consistency//Proc of the 16th International Conference on Neural Information Processing Systems.Cambridge, USA:The MIT Press, 2003:321-328
                                    </a>
                                </li>
                                <li id="296">


                                    <a id="bibliography_27" title="BELKIN M, NIYOGI P, SINDHWANI V.Manifold Regularization:A Geometric Framework for Learning from Labeled and Unlabeled Examples.Journal of Machine Learning Research, 2006, 7:2399-2434." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Manifold regularization: a geometric framework for learning from labeled and unlabeled examples">
                                        <b>[27]</b>
                                        BELKIN M, NIYOGI P, SINDHWANI V.Manifold Regularization:A Geometric Framework for Learning from Labeled and Unlabeled Examples.Journal of Machine Learning Research, 2006, 7:2399-2434.
                                    </a>
                                </li>
                                <li id="298">


                                    <a id="bibliography_28" title="WESTON J, RATLE F, MOBAHI U, et al.Deep Learning via Semi-supervised Embedding//MONTAVON G, ORR G B, MLLER K R, eds.Neural Networks:Tricks of the Trade.Berlin, Germany:Springer, 2012:639-665." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Learning via Semi-Supervised Embedding">
                                        <b>[28]</b>
                                        WESTON J, RATLE F, MOBAHI U, et al.Deep Learning via Semi-supervised Embedding//MONTAVON G, ORR G B, MLLER K R, eds.Neural Networks:Tricks of the Trade.Berlin, Germany:Springer, 2012:639-665.
                                    </a>
                                </li>
                                <li id="300">


                                    <a id="bibliography_29" title="ABADI M, AGARWAL A, BARHAM P, et al.Tensor Flow:LargeScale Machine Learning on Heterogeneous Distributed Systems[C/OL].[2018-11-25].https://arxiv.org/pdf/1603.04467.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tensor Flow:LargeScale Machine Learning on Heterogeneous Distributed Systems[C/OL]">
                                        <b>[29]</b>
                                        ABADI M, AGARWAL A, BARHAM P, et al.Tensor Flow:LargeScale Machine Learning on Heterogeneous Distributed Systems[C/OL].[2018-11-25].https://arxiv.org/pdf/1603.04467.pdf.
                                    </a>
                                </li>
                                <li id="302">


                                    <a id="bibliography_30" title="VELICˇKOVI CP, CUCURULL G, CASANOVA A, et al.Graph Attention Networks[C/OL].[2018-11-25].https://arxiv.org/pdf/1710.10903.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Graph Attention Networks[C/OL]">
                                        <b>[30]</b>
                                        VELICˇKOVI CP, CUCURULL G, CASANOVA A, et al.Graph Attention Networks[C/OL].[2018-11-25].https://arxiv.org/pdf/1710.10903.pdf.
                                    </a>
                                </li>
                                <li id="304">


                                    <a id="bibliography_31" title="OU M D, CUI P, PEI J, et al.Asymmetric Transitivity Preserving Graph Embedding//Proc of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2016:1105-1114" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Asymmetric transitivity preserving graph embedding">
                                        <b>[31]</b>
                                        OU M D, CUI P, PEI J, et al.Asymmetric Transitivity Preserving Graph Embedding//Proc of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2016:1105-1114
                                    </a>
                                </li>
                                <li id="306">


                                    <a id="bibliography_32" title="VAN LOAN C F.Generalizing the Singular Value Decomposition.SIAM Journal on Numerical Analysis, 1976, 13 (1) :76-83." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14120403044306&amp;v=MTA5ODRyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRndjYmhRPU5pZlllcks4SDlQTXE0OUdaTzhMRDN3L29CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[32]</b>
                                        VAN LOAN C F.Generalizing the Singular Value Decomposition.SIAM Journal on Numerical Analysis, 1976, 13 (1) :76-83.
                                    </a>
                                </li>
                                <li id="308">


                                    <a id="bibliography_33" title="AHMED A, SHERVASHIDZE N, NARAYANAMURTHY S, et al.Distributed Large-Scale Natural Graph Factorization//Proc of the22nd International Conference on World Wide Web.New York, USA:ACM, 2013:37-48." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributed large-scale natural graph factorization">
                                        <b>[33]</b>
                                        AHMED A, SHERVASHIDZE N, NARAYANAMURTHY S, et al.Distributed Large-Scale Natural Graph Factorization//Proc of the22nd International Conference on World Wide Web.New York, USA:ACM, 2013:37-48.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(04),317-325 DOI:10.16451/j.cnki.issn1003-6059.201904004            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于图卷积网络和自编码器的半监督网络表示学习模型</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%9D%B0&amp;code=10608684&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王杰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%9B%A6%E7%85%8C&amp;code=07770640&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张曦煌</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E5%8D%97%E5%A4%A7%E5%AD%A6%E7%89%A9%E8%81%94%E7%BD%91%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0074200&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江南大学物联网工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了保留网络结构信息和节点特征信息, 结合图卷积神经网络 (GCN) 和自编码器 (AE) , 提出可扩展的半监督深度网络表示学习模型 (Semi-GCNAE) .利用GCN捕获节点的<i>K</i>阶邻域中所有节点的结构和特征信息, 并将捕获的信息作为AE的输入.AE对GCN捕获的<i>K</i>阶邻域信息进行特征提取和非线性降维, 并结合Laplacian特征映射保留节点的团簇结构信息.引入集成学习方法联合训练GCN和AE, 使模型习得的节点低维向量表示能同时保留网络结构信息和节点特征信息.在5个真实数据集上的广泛评估表明, 文中模型习得的节点低维向量表示可以有效保留网络的结构和节点特征信息, 并在节点分类、可视化和网络重构任务上性能较优.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">网络表示学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20(GCN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图卷积神经网络 (GCN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%20(AE)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自编码器 (AE) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Laplacian%E7%89%B9%E5%BE%81%E6%98%A0%E5%B0%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Laplacian特征映射;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王杰, 硕士研究生, 主要研究方向为网络表示学习.E-mail:1473355215@qq.com.;
                                </span>
                                <span>
                                    *张曦煌 (通讯作者) , 博士, 教授, 主要研究方向为计算机网络、分布式系统与应用等.E-mail:619449188@qq.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-17</p>

                    <p>

                            <b>基金：</b>
                                                        <span>江苏省产学研合作基金项目 (No.BY2015019-30) 资助;</span>
                    </p>
            </div>
                    <h1><b>Semi-supervised Network Representation Learning Model Based on Graph Convolutional Networks and Auto Encoder</b></h1>
                    <h2>
                    <span>WANG Jie</span>
                    <span>ZHANG Xihuang</span>
            </h2>
                    <h2>
                    <span>School of Internet of Things Engineering, Jiangnan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Combining graph convolutional networks (GCN) and auto encoder (AE) , a scalable semi-supervised network representation learning model, Semi-GCNAE, is proposed to preserve the network structure information and node feature information. GCN is utilized to capture the structure and feature information of all nodes in <i>K</i>-order neighborhood of the node. The captured information is utilized as the input of AE. The <i>K</i>-order neighborhood information captured by GCN is extracted and the dimension is reduced nonlinearly by AE. The cluster structure information of nodes is preserved by combining Laplacian feature mapping. The ensemble learning method is introduced to train GCN and AE jointly. Therefore, the learned low-dimensional vector representation of nodes can retain both network structure information and node feature information. Extensive evaluation on five real datasets shows that the low-dimensional vector representation of nodes acquired by the proposed model preserves the structure and characteristics of the network effectively. And it generates better performance in node classification, visualization and network reconstruction tasks.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Network%20Representation%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Network Representation Learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Graph%20Convolutional%20Networks%20(GCN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Graph Convolutional Networks (GCN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Auto%20Encoder%20(AE)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Auto Encoder (AE) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Laplacian%20Eigenmap&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Laplacian Eigenmap;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Jie, master student. His research interests include network representation learning.;
                                </span>
                                <span>
                                    ZHANG Xihuang ( Corresponding author) , Ph. D., professor. His research interests include computer network, distributed system and application.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-17</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by Jiangsu Province Production and Research Cooperation Project (No.BY2015019-30);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="81">近些年, 许多现实世界的应用需要挖掘和分析复杂信息网络中的信息, 如对社交网络、引文网络的可视化建模、网络广告投放.因此, 挖掘网络中的信息的实用价值很高.</p>
                </div>
                <div class="p1">
                    <p id="82">在网络分析<citation id="310" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>或挖掘时, 首先要能表示网络.网络表示学习 (Network Representation Learning, NRL) <citation id="314" type="reference"><link href="246" rel="bibliography" /><link href="248" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>或图嵌入 (Graph Embedding) 目标就是学习生成低维稠密向量表示网络中的节点, 使节点分类<citation id="311" type="reference"><link href="250" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、节点可视化<citation id="315" type="reference"><link href="252" rel="bibliography" /><link href="254" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>、社区发现<citation id="312" type="reference"><link href="256" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、链路预测<citation id="313" type="reference"><link href="258" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等数据挖掘和机器学习应用任务都在习得的低维向量空间中直接进行.</p>
                </div>
                <div class="p1">
                    <p id="83">近些年, 受深度神经网络在图像处理<citation id="316" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>和NLP<citation id="317" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>领域取得显著成功的启发, 学者们提出许多基于神经网络的NRL<citation id="321" type="reference"><link href="246" rel="bibliography" /><link href="248" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>.Perozzi等<citation id="318" type="reference"><link href="264" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出基于社会表征的在线学习 (Online Learning of Social Representation) , 借助随机游走算法采样获得节点序列, 并借助Skip-Gram实现节点表示学习.Grover等<citation id="319" type="reference"><link href="266" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出可扩展的网络特征学习 (Scalable Feature Learning for Networks, node2vec) .不同之处在于node2vec通过设置超参数控制随机游走的策略, 平衡广度优先采样和深度优先采样.Tang等<citation id="320" type="reference"><link href="268" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>基于边采样提出大规模信息网络嵌入 (Large-Scale Information Network Embedding, LINE) , 定义节点一阶和二阶相似函数, 联合优化, 有效提取网络中局部和全局结构信息.</p>
                </div>
                <div class="p1">
                    <p id="84">然而, 很少有人应用深度神经网络处理复杂网络.Bruna等<citation id="322" type="reference"><link href="270" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>结合图谱理论<citation id="323" type="reference"><link href="272" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 成功地将卷积神经网络 (Convolution Neural Network, CNN) <citation id="324" type="reference"><link href="274" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>应用到NRL<citation id="331" type="reference"><link href="246" rel="bibliography" /><link href="248" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>中.Defferrard等<citation id="325" type="reference"><link href="276" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>使用<i>K</i>阶Chebyshev多项式<citation id="326" type="reference"><link href="278" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>作为卷积核, 抽取节点的<i>K</i>阶邻域特征, 将算法复杂度从<i>O</i> (<i>N</i><sup>2</sup>) 降到<mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mi>Κ</mi><mo stretchy="false">|</mo><mi>E</mi><mo stretchy="false">|</mo><mo stretchy="false">) </mo></mrow></math></mathml>, 其中, <i>N</i>表示节点个数, <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>E</mi><mo stretchy="false">|</mo></mrow></math></mathml>表示网络所含边数.之后, <i>Kipf</i>等<citation id="327" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>使用一阶<i>Chebyshev</i>多项式, 借助堆叠多层卷积层的方法实现K阶邻域特征卷积滤波器的相同功能, 并结合<i>Zhu</i>等<citation id="328" type="reference"><link href="282" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>基于图的标签传播算法 (<i>Label Propagation Algorithm</i>, <i>LPA</i>) , 提出图卷积网络 (<i>Graph Convolu</i>-<i>tional Network</i>, <i>GCN</i>) <citation id="329" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, 有效利用图中节点属性信息和标签信息, 降低时间复杂度.<i>Wang</i>等<citation id="330" type="reference"><link href="284" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>引入节点二阶相似性, 提出结构深度网络嵌入模型 (<i>Structural Deep Network Embedding</i>, <i>SDNE</i>) , 习得的节点低维向量表示同时保留网络的局部和全局结构信息.</p>
                </div>
                <div class="p1">
                    <p id="87"><i>GCN</i><citation id="332" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>和<i>SDNE</i><citation id="333" type="reference"><link href="284" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>都从某一方面解决网络表示的问题.然而, 这些模型大多数只保留网络的部分信息, 并不能同时保留网络的结构和特征信息.为了解决这一问题, 本文提出基于图卷积网络和自编码器的半监督网络表示学习模型 (<i>Semi</i>-<i>supervised Net</i>-<i>work Representation Learning Model Based on Graph Convolutional Networks and Auto Encoder</i>, <i>Semi</i>-<i>GCNAE</i>) , 习得的节点低维向量表示能同时保留网络的结构信息和节点的特征信息.引入集成学习<citation id="339" type="reference"><link href="286" rel="bibliography" /><link href="288" rel="bibliography" /><sup>[<a class="sup">22</a>,<a class="sup">23</a>]</sup></citation>的思想, 提出有区分性的半监督深度模型.<i>GCN</i><citation id="334" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>组件和拉普拉斯特征映射 (<i>Laplacian Eigenmaps</i>, <i>LE</i>) <citation id="335" type="reference"><link href="290" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>显式正则化作为有监督学习部分, 自编码器 (<i>Auto Encoder</i>, <i>AE</i>) <citation id="336" type="reference"><link href="292" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>组件作为无监督部分, 最后联合两个组件组成半监督模型.使用<i>GCN</i><citation id="337" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>组件采样K阶邻域特征, 由<i>AE</i>组件对采样特征进行非线性降维, 大幅降低模型运行时的计算量和内存, 使模型可以有效处理大规模复杂信息网络.通过在五个真实数据集上实验评估, 表明本文模型习得的节点低维向量在可视化<citation id="340" type="reference"><link href="252" rel="bibliography" /><link href="254" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>、节点分类<citation id="338" type="reference"><link href="250" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>和网络重构任务中性能较优.</p>
                </div>
                <h3 id="88" name="88" class="anchor-tag">1 可扩展的半监督深度网络表示学习模型</h3>
                <div class="p1">
                    <p id="89">在很多复杂信息网络或图中, 每个节点通常会附带特征信息.本文聚焦无向网络或无向图, 即<i>G</i>= (<i>V</i>, <i>E</i>) .<i>V</i>表示节点集合, 即∀<i>v</i><sub><i>i</i></sub>∈<i>V</i> (<i>i</i>=1, 2, …) ;<i>E</i>表示边集合, 即∀<i>e</i><sub><i>ij</i></sub>∈<i>E</i> (<i>j</i>=1, 2, …) .Semi-GCNAE模型由两部分组成:1) 负责采样<i>K</i>阶邻域中所有节点信息的GCN组件;2) AE组件, 用于提取由GCN组件学习到的激活值矩阵<b><i>A</i></b>的隐藏特征, 并结合LE保留节点团簇结构.</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90"><b>1.1</b> 图卷积神经网络</h4>
                <div class="p1">
                    <p id="91">GCN从谱图卷积的框架出发, 以一阶Chebyshev多项式作为卷积核, 进一步简化基于<i>K</i>阶Chebyshev多项式的平滑滤波器.在分类问题中GCN将问题定义为基于图的半监督学习, 标签传播部分使用基于图的显式正则化项<citation id="341" type="reference"><link href="282" rel="bibliography" /><link href="294" rel="bibliography" /><link href="296" rel="bibliography" /><link href="298" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">26</a>,<a class="sup">27</a>,<a class="sup">28</a>]</sup></citation>, 使标签能在图上平滑过渡.GCN损失函数定义为</p>
                </div>
                <div class="area_img" id="92">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201904005_09200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="93">其中:<i>L</i><sub>0</sub>表示模型有监督部分, 即这部分节点有标签; <i>f</i> (·) 表示以一阶Chebyshev多项式作为卷积核的滤波器;<i>λ</i>为加权因子;<b><i>X</i></b>为所有节点的特征值向量组成的矩阵;<i>Δ</i>=<b><i>D</i></b>-<b><i>S</i></b>为图<i>G</i>= (<i>V</i>, <i>E</i>) 的未归一化的Laplacian矩阵, <b><i>S</i></b>∈<b>R</b><sup>|<i>V</i> |╳|<i>V</i> |</sup>为网络或图的邻接矩阵, 包含各个节点之间的连接关系, 对于∀<i>s</i><sub><i>ij</i></sub>∈<b><i>S</i></b>都是非负的, 若<i>s</i><sub><i>ij</i></sub>&gt;0, 表示节点<i>v</i><sub><i>i</i></sub>和节点<i>v</i><sub><i>j</i></sub>之间存在联系, 若<i>s</i><sub><i>ij</i></sub>=0, 表示节点<i>v</i><sub><i>i</i></sub>和节点<i>v</i><sub><i>j</i></sub>之间不存在联系, <b><i>D</i></b>∈<b>R</b><sup>|<i>V</i> |╳|<i>V</i> |</sup>为网络或图的度矩阵, <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>s</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>.</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="95">本文利用GCN对图<i>G</i>= (<i>V</i>, <i>E</i>) 进行结构和特征采样, 为了保留更多网络或图的结构信息和节点的特征信息, 增加GCN的卷积层数至<i>K</i>层, 使其能采样<i>K</i>步邻域的所有节点信息.为了充分利用信息网络或图中的标签信息, 使用交叉熵函数定义Semi-GCNAE节点采样组件的损失函数:</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>c</mtext><mtext>n</mtext></mrow></msub><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mrow><mi>log</mi></mrow><msub><mrow></mrow><mn>2</mn></msub><mrow><mover accent="true"><mi>Y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mrow><mi>log</mi></mrow><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mrow><mover accent="true"><mi>Y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">其中, <i>y</i><sub><i>i</i></sub>为节点真实的标签, <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>为GCN组件预测的标签.</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>1.2</b> 自编码器</h4>
                <div class="p1">
                    <p id="100">AE是一种无监督的神经网络模型, 经过训练可以学习到输入数据的隐含特征.自编码器由两部分组成:负责编码的编码器 (Encoder) 和负责解码的解码器 (Decoder) .编码操作就是将数据从输入层映射到隐藏层<i>H</i>的过程, 而解码操作是以隐藏层<i>H</i>获得的编码作为输入从隐藏层映射到输出层的过程.</p>
                </div>
                <div class="p1">
                    <p id="101">自编码器的学习目的是将输入层数据<b><i>X</i></b>通过转换得到隐藏层<i>H</i>的编码表示, 然后由隐藏层重构, 还原新的输入数据<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">X</mi></mstyle><mo>︿</mo></mover><mo>.</mo></mrow></math></mathml><i>AE</i>的训练目标是使重构后的数据<mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">X</mi></mstyle><mo>︿</mo></mover></mrow></math></mathml>尽可能地还原输入层数据<b><i>X</i></b>, 因此可将AE的损失函数定义为</p>
                </div>
                <div class="p1">
                    <p id="104"><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>u</mtext><mtext>t</mtext><mtext>o</mtext><mo>_</mo><mtext>e</mtext><mtext>n</mtext><mtext>c</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>r</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mrow><mover accent="true"><mi>X</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo stretchy="false">) </mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="106">为了能学习到由GCN采样的邻域节点的隐含特征, 本文采用隐层<i>H</i>的维度比<b><i>X</i></b>小的欠完备 (Undercomplete) AE<citation id="342" type="reference"><link href="292" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>.欠完备的AE能捕捉训练数据中最显著的特征, 将节点的主要显著特征嵌入到低维向量空间.因此, 将Semi-GCNAE自编码器组件的损失函数定义为</p>
                </div>
                <div class="area_img" id="362">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201904005_36200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="109">其中, <b><i>A</i></b><sup> (<i>K</i>) </sup>={<b><i>a</i></b><sup> (<i>K</i>) </sup><sub><i>i</i></sub>}<mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>表示一个激活值矩阵, <i>K</i>表示节点<i>v</i><sub><i>i</i></sub>的邻域阶数, <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">A</mi></mstyle><mo>︿</mo></mover></mrow><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>Κ</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mrow><mo stretchy="false">{</mo><mover accent="true"><mi mathvariant="bold-italic">A</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>Κ</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">}</mo></mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>表示重构的激活值矩阵.为了网络中节点在嵌入到低维向量空间中保持原有的数据结构, 受到SNDE的启发, 本文在AE组件中也引入LE, 使原始网络中的两个相近节点在嵌入到低维向量空间后, 两个节点仍然接近.因此, 将LE的目标函数定义为</p>
                </div>
                <div class="area_img" id="363">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201904005_36300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="114">将LE的目标函数纳入到自编码器的损失函数中便能达到本文的期望, 捕获GCN组件采集的<i>K</i>阶邻域节点的信息, 同时保持原有的数据结构的目标.本文利用超参数<i>γ</i>控制LE, 由此将Semi-GCNAE的自编码器组件的损失函数修改为</p>
                </div>
                <div class="area_img" id="115">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201904005_11500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="117">其中, <b><i>A</i></b><sup> (<i>k</i>) </sup>={<b><i>a</i></b><sup> (<i>k</i>) </sup><sub><i>i</i></sub>}<mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>表示一个激活值矩阵, <i>K</i>表示节点<i>v</i><sub><i>i</i></sub>的邻域阶数, <image id="119" type="formula" href="images/MSSB201904005_11900.jpg" display="inline" placement="inline"><alt></alt></image>表示重构的激活值矩阵, <b><i>H</i></b><sup> (<i>l</i>) </sup>={<b><i>h</i></b><sup> (<i>l</i>) </sup><sub><i>i</i></sub>}<mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>表示AE第<i>l</i>层的隐层表示, <i>L</i>表示AE的隐层层数.</p>
                </div>
                <h4 class="anchor-tag" id="121" name="121"><b>1.3</b> 模型结构</h4>
                <div class="p1">
                    <p id="122">在Semi-GCNAE模型中GCN组件利用图卷积神经网络以节点∀<i>v</i><sub><i>i</i></sub>∈<i>V</i> (<i>i</i>=1, 2, …) 为中心采样<i>K</i>步的所有节点的结构和特征信息, 即编码<i>K</i>阶邻域信息, 并结合节点的标签训练生成作为自编码器组件输入的激活值矩阵<b><i>A</i></b>.GCN通过基于节点标签的有监督学习, 可同时编码网络的局部结构和特征信息, 略去<i>K</i>阶邻域外对生成节点的低维向量影响较小的次要结构信息, 大幅降低运算量和空间复杂度.利用GCN习得的激活值矩阵<b><i>A</i></b>作为AE的输入, 自编码器通过无监督学习的方式对<b><i>A</i></b>进一步提取特征信息, 并结合LE, 将原网络映射到一个较低维的空间.</p>
                </div>
                <div class="p1">
                    <p id="123">为了保证模型可以有效训练, 引入集成学习 (Ensemble Learning) <citation id="343" type="reference"><link href="288" rel="bibliography" /><link href="290" rel="bibliography" /><sup>[<a class="sup">23</a>,<a class="sup">24</a>]</sup></citation>的思想.本文使用集成学习中的堆叠法 (Stacking) 将两个组件线性组合并联合训练, 组成Semi-GCNAE.这样整个模型习得的节点低维向量表示既能保留节点的特征信息又能保留结构信息.Semi-GCNAE结构图如图1所示.</p>
                </div>
                <div class="p1">
                    <p id="124">借助Stacking, 线性组合GCN组件和AE组件, 并使用两个超参数<i>α</i>和<i>β</i>控制这两个组件损失函数 (即式 (1) 和式 (2) ) 的权重, 最终将Semi-GCNAE的损失函数定义为</p>
                </div>
                <div class="area_img" id="125">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201904005_12500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="125">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201904005_12501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="356">其中, y<sub>i</sub>为节点真实的标签, <image id="357" type="formula" href="images/MSSB201904005_35700.jpg" display="inline" placement="inline"><alt></alt></image>为GCN预测的标签, A<sup> (k) </sup>={a<sup> (k) </sup><sub>i</sub>}<sup>N</sup><sub>i=1</sub>为一个激活值矩阵, K为节点v<sub>i</sub>的邻域阶数, <image id="358" type="formula" href="images/MSSB201904005_35800.jpg" display="inline" placement="inline"><alt></alt></image>为重构的激活值矩阵, H<sup> (l) </sup>={h<sup> (l) </sup><sub>i</sub>}<sup>N</sup><sub>i=1</sub>为AE第l层的隐层表示, L为AE的隐层层数.</p>
                </div>
                <div class="p1">
                    <p id="132">使用TensorFlow<citation id="344" type="reference"><link href="300" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>框架借助GPU加速模型训练.模型优化部分使用TensorFlow提供的Adam-Optimizer优化器更新模型参数, 通过使用动量 (即参数的移动平均数) 改善传统梯度下降, 促进超参数动态调整, 使模型可以快速有效的训练.为了能更好地处理大规模网络, 将网络数据集随机划分为若干个等大的批处理样本 (Batch) .每次只在一个batch上更新模型参数, 进一步降低模型训练时的内存占用.</p>
                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904005_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Semi-GCNAE模型结构图" src="Detail/GetImg?filename=images/MSSB201904005_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Semi-GCNAE模型结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904005_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Architecture of semi-GCNAE model</p>

                </div>
                <h3 id="134" name="134" class="anchor-tag">2 实验及结果分析</h3>
                <div class="p1">
                    <p id="135">本节从节点分类、可视化和网络重构3个方面考查Semi-GCNAE的性能以及Semi-GCNAE学习到的节点低维向量表示在真实数据集上3种应用的表现, 用于评估模型的性能.</p>
                </div>
                <h4 class="anchor-tag" id="136" name="136"><b>2.1</b> 实验数据集</h4>
                <div class="p1">
                    <p id="137">为了广泛客观地评估Semi-GCNAE的性能, 使用真实数据集作为基准数据集.选取3个引文网络、1个网页网络及1个电力网络共5个数据集, 详细的数据集数据如表1所示.</p>
                </div>
                <div class="area_img" id="138">
                    <p class="img_tit"><b>表1 网络数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Network datasets</p>
                    <p class="img_note"></p>
                    <table id="138" border="1"><tr><td>类别</td><td>名称</td><td>节点数</td><td>边数</td><td>标签数</td><td>节点属性</td></tr><tr><td><br />引文网络</td><td>Cora<br />Citeseer<br />Pubmed</td><td>2708<br />3312<br />19717</td><td>5429<br />4732<br />44338</td><td>7<br />6<br />3</td><td>是<br />是<br />是</td></tr><tr><td><br />网页网络</td><td>Wikipedia</td><td>2405</td><td>17981</td><td>20</td><td>是</td></tr><tr><td><br />电力网络</td><td>USPowerGrid</td><td>4941</td><td>6594</td><td>7</td><td>否</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="139">基准数据集的详细介绍如下.</p>
                </div>
                <div class="p1">
                    <p id="140">1) 引文网络.由文章之间的相互引用关系或作者之间的相互引用关系形成的复杂信息网络.本文使用的Cora、Citeseer及Pubmed是二值型论文引文网络, 网络中的每个节点都包含节点特征信息, 每个节点都有表示其所属类别的标签信息.不同于社交网络的用户配置信息, 引文网络中的节点特征信息以主题为中心, 包含的信息丰富, 能作为网络结构的补充, 以便学习有效的节点低维向量表示.</p>
                </div>
                <div class="p1">
                    <p id="141">2) 网页网络.由网页之间的超链接关系形成, 网页网络中的每个节点都表示一个网页, 每个边都表示连接到其它网页的超链接.本文在网页网络这一类选用Wikipedia作为实验的基准数据集, 在Wikipedia中每个网页的文本内容作为该网页的特征, 按照网页类别赋予不同的标签.</p>
                </div>
                <div class="p1">
                    <p id="142">3) 电力网络.由发电机、变压器或变电站之间电力运输线路形成的网络.本文选用美国西部各州的电网USPowerGrid作为基准数据集, 网络中的每个节点表示发电、变压器或变电站, 每条边表示电力运输的线路.同样, 网络中的每个节点都赋予类别标签.</p>
                </div>
                <div class="p1">
                    <p id="143">综上所述, 在实验中所用的数据集涵盖稀疏、非稀疏网络等不同类型的复杂信息网络.因此, 这些数据集能从不同的视角反映Semi-GCNAE的特性.</p>
                </div>
                <h4 class="anchor-tag" id="144" name="144"><b>2.2</b> 基准网络表示学习方法</h4>
                <div class="p1">
                    <p id="145">本文选用5种NRL方法作为基准, 具体介绍如下.</p>
                </div>
                <div class="p1">
                    <p id="146">1) GCN<citation id="345" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>.从谱图卷积的框架出发, 以一阶Chebyshev多项式作为卷积核, 利用层次化结构实现的半监督模型.设置GCN的参数如下:初始学习率为0.01, 模型训练轮数为200, 卷积层输出单元个数为16, 失活率为0.5, 权重正则化参数为5×10<sup>-4</sup>, 学习到低维向量维数设置为128.</p>
                </div>
                <div class="p1">
                    <p id="147">2) SDNE<citation id="346" type="reference"><link href="284" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>.使用深度自编码器, 捕获网络中节点的一阶相似性和二阶相似性, 损失函数由无监督部分、有监督部分及防止过拟合的正则化项组成.无监督的自编码器负责维护网络的二阶相似性, 基于LE的有监督组件负责维护网络的一阶相似性.SDNE的一阶、二阶相似性通过两个超参数<i>γ</i>和<i>α</i>控制, 正则化项便由超参数<i>reg</i>控制.参数设置如下:<i>α</i>=100, <i>γ</i>=<i>reg</i>=1, 训练的批处理样本容量设置为64, 初始学习率为0.01, 学习到的低维向量维数设置为128.</p>
                </div>
                <div class="p1">
                    <p id="148">3) 图关注网络 (Graph Attention Networks, GAT) <citation id="347" type="reference"><link href="302" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>.将attention机制应用到图卷积中, 根据每个节点在其邻节点上的attention更新节点表示.设置参数如下:学习率为6 , L2正则化系数为0.000 5, 每个attention head的隐层神经元个数为16, 整个模型设置8个attention head, 最终学习到的低维向量维数为128.</p>
                </div>
                <div class="p1">
                    <p id="149">4) 高阶近似保留嵌入 (High-Order Proximity Preserved Embedding, HOPE) <citation id="348" type="reference"><link href="304" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>.通过最小化<mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><mo>-</mo><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mi>s</mi></msub><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mi>t</mi><mtext>Τ</mtext></msubsup></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mrow></math></mathml>保留1-K<sup>th</sup>阶相似性, 其中<b><i>S</i></b>表示复杂信息网络或图的邻接矩阵, <b><i>Z</i></b>表示获得的节点低维向量表示.HOPE使用广义奇异值分解 (Generalized Singular Value Decomposition, SVD) <citation id="349" type="reference"><link href="306" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>高效获得网络嵌入表示.设置参数如下:初始学习率<i>β</i>=0.01, 节点嵌入到的向量空间维数<i>d</i>=128.</p>
                </div>
                <div class="p1">
                    <p id="151">5) 图分解 (Graph Factorization, GF) <citation id="350" type="reference"><link href="308" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>.通过分解复杂信息网络或图邻接矩阵方法, 获得结点的低维向量表示.损失函数定义为</p>
                </div>
                <div class="area_img" id="359">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201904005_35900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="154">其中, <i>λ</i>为正则化系数, <b><i>Z</i></b>表示获得的节点低维向量表示.设置参数如下:学习率为10<sup>-4</sup>, 正则化率<i>λ</i>=1.0, 节点嵌入到向量空间的维数为128.</p>
                </div>
                <h4 class="anchor-tag" id="155" name="155"><b>2.3</b> 评价标准</h4>
                <div class="p1">
                    <p id="156">本文使用的指标具体定义如下.</p>
                </div>
                <div class="p1">
                    <p id="157">1) <i>Pr</i>@<i>k</i> (Precision at <i>k</i>) .表示前<i>k</i>个预测中正确预测的个数与<i>k</i>比值, 定义为</p>
                </div>
                <div class="p1">
                    <p id="158"><mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>Ρ</mi><mi>r</mi></mrow><mo>@</mo><mi>k</mi><mo>=</mo><mfrac><mrow><mo stretchy="false">|</mo><mi>E</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>r</mtext><mo>_</mo><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false">|</mo></mrow><mi>k</mi></mfrac></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="160">其中<mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>E</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>r</mtext><mo>_</mo><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false">|</mo></mrow></math></mathml>表示正确预测的边的个数.</p>
                </div>
                <div class="p1">
                    <p id="162">2) macro-F1.定义为在所有类别的平均<i>F</i>1:</p>
                </div>
                <div class="p1">
                    <p id="163"><mathml id="164"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><mi>a</mi><mi>c</mi><mi>r</mi><mi>o</mi><mo>-</mo><mi>F</mi><mn>1</mn><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>Y</mi><mo stretchy="false">|</mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><mi>F</mi></mstyle><mn>1</mn><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="165">其中, <i>F</i>1为标签<i>y</i>的<mathml id="166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mn>1</mn><mo>-</mo><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>, </mo><mo stretchy="false">|</mo><mi>Y</mi><mo stretchy="false">|</mo></mrow></math></mathml>为标签的类别总数.</p>
                </div>
                <div class="p1">
                    <p id="167">3) micro-F1.对每个实体赋予相同的权重, 计算全部的真阳性 (True Postive, TP) 、假阳性 (False Positive, FP) 和假阴性 (False Negative, FN) 的个数, 具体定义为</p>
                </div>
                <div class="p1">
                    <p id="168"><mathml id="169"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><mi>i</mi><mi>c</mi><mi>r</mi><mi>o</mi><mo>-</mo><mi>F</mi><mn>1</mn><mo>=</mo><mfrac><mrow><mn>2</mn><mi>Ρ</mi><mi>r</mi><mi>R</mi></mrow><mrow><mi>Ρ</mi><mi>r</mi><mo>+</mo><mi>R</mi></mrow></mfrac></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="170">其中</p>
                </div>
                <div class="p1">
                    <p id="171" class="code-formula">
                        <mathml id="171"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mi>r</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><mi>Τ</mi></mstyle><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><mo stretchy="false"> (</mo></mstyle><mi>Τ</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>F</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac><mo>, </mo></mtd></mtr><mtr><mtd><mi>R</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><mi>Τ</mi></mstyle><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><mo stretchy="false"> (</mo></mstyle><mi>Τ</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>F</mi><mi>Ν</mi><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac><mo>, </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="172"><i>TP</i>为预测为正、实际也为正;<i>FP</i>为预测为正、实际为负;<i>FN</i>为预测为负、实际为正.</p>
                </div>
                <h4 class="anchor-tag" id="173" name="173"><b>2.4</b> 实验结果</h4>
                <div class="p1">
                    <p id="174">先使用两个典型的数据挖掘和机器学习应用考察Semi-GCNAE的表现, 即节点分类和可视化, 最后考察Semi-GCNAE对复杂信息网络或图的结构保留情况, 即网络重构性能.</p>
                </div>
                <h4 class="anchor-tag" id="175" name="175"><b>2.4.1</b> 节点分类</h4>
                <div class="p1">
                    <p id="176">节点分类是一种重要的NRL下游应用, 本节分类实验中使用Semi-GCNAE及各对比方法学习到的节点低维向量表示作为训练样本训练分类器.将模型学习到节点向量表示按照10%～90%划分成训练样本集和测试样本集, 分别测试在不同容量训练集上的分类效果.在Cora、Citeseer、USPowerGrid、Wiki-pedia数据集上进行节点分类实验, 结果如图2所示.</p>
                </div>
                <div class="p1">
                    <p id="177">由图2可见, Semi-GCNAE在节点多标签分类任务上, 性能明显优于其它对比方法, 在USPowerGrid数据集上更佳.对于Wikipedia这样拥有众多标签的数据集, 虽然在只训练10%～20%样本时, macro-F1指标低于SDNE, 但当训练样本数达到80%以上时, Semi-GCNAE能保持较好的稳定性, 性能更优.总之在Wikipebia数据集上Semi-GCNAE节点分类性能最优.由此说明, Semi-GCNAE学习到网络节点低维向量表示, 相比其它方法, 在节点多标签分类上的表现更好.</p>
                </div>
                <div class="area_img" id="360">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904005_36000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 6种方法在4个数据集上的节点分类结果" src="Detail/GetImg?filename=images/MSSB201904005_36000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 6种方法在4个数据集上的节点分类结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904005_36000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Node classification results of 6 methods on 4 datasets</p>

                </div>
                <div class="area_img" id="360">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904005_36001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 6种方法在4个数据集上的节点分类结果" src="Detail/GetImg?filename=images/MSSB201904005_36001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 6种方法在4个数据集上的节点分类结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904005_36001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Node classification results of 6 methods on 4 datasets</p>

                </div>
                <h4 class="anchor-tag" id="188" name="188"><b>2.4.2</b> 可视化</h4>
                <div class="p1">
                    <p id="189">对习得的节点低维向量表示可视化, 能直观评估NRL模型的性能.本文用<i>t</i>分布随机嵌入 (<i>t</i>-Dis-tributed Stochastic Neighbor Embedding, t-SNE) 将习得的向量表示降至2维, 并在图中绘制, 以便直接观察原始网络的群落结构.图中每个点都代表原网络中的一个节点, 并按类别使用不同颜色标注<citation id="351" type="reference"><link href="254" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>.另外, 在图中相似节点彼此靠近, 不同节点彼此分离.在Cora、Citeseer数据集上可视化结果显示, Semi-GCNAE可以较好反映网络的群落结构, 结果如图3所示.</p>
                </div>
                <div class="p1">
                    <p id="190">图3可视化结果表明, Semi-GCNAE在Cora、Citeseer数据集上, 显著提升网络可视化的表现, 有效反映网络中节点的团簇关系.GCN和GAT可视化结果表明, 学习的节点向量表示在一定程度上捕获网络的群落结构.与之相比, SDNE、HOPE、GF可视化性能较弱, 在对Cora、Citeseer数据集的可视化结果中, 不同类别的点相互交错, 并不能反映节点的群落结构.这也说明Semi-GCNAE习得的网络节点低维向量表示有效捕获原始网络中节点的群落结构.</p>
                </div>
                <div class="area_img" id="361">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201904005_36100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 6种方法在2个数据集上的可视化结果" src="Detail/GetImg?filename=images/MSSB201904005_36100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 6种方法在2个数据集上的可视化结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201904005_36100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Visualization results of 6 methods on 2 datasets</p>

                </div>
                <h4 class="anchor-tag" id="195" name="195"><b>2.4.3</b> 网络重构</h4>
                <div class="p1">
                    <p id="196">网络重构就是利用学习到节点低维向量表示重新构建原始网络, 达到简化网络的目的.网络的重构效果好坏可通过计算基于节点的内积或相似性以预测节点的连接, 使用正确预测的连接占原始网络中连接的比例评估模型的重构表现, 本文使用<i>Pr</i>@<i>k</i>指标评估模型的网络重构性能.选用Cora、Pubmed、Wikipedia数据集进行实验, 具体实验结果如表2～表4所示.</p>
                </div>
                <div class="area_img" id="197">
                    <p class="img_tit"><b>表2 6种方法在Cora数据集上的网络重构结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Results of network reconstruction of 6 methods on Cora dataset</p>
                    <p class="img_note"></p>
                    <table id="197" border="1"><tr><td>指标</td><td>Semi-<br />GCNAE</td><td>GCN</td><td>SDNE</td><td>GAT</td><td>HOPE</td><td>GF</td></tr><tr><td><br /><i>Pr</i>@5</td><td>1.0</td><td>1.0</td><td>0.2</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td><br /><i>Pr</i>@10</td><td>1.0</td><td>1.0</td><td>0.2</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td><br /><i>Pr</i>@100</td><td>0.56</td><td>0.27</td><td>0.08</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td><br /><i>Pr</i>@200</td><td>0.45</td><td>0.32</td><td>0.08</td><td>0.01</td><td>0.0</td><td>0.0</td></tr><tr><td><br /><i>Pr</i>@300</td><td>0.41</td><td>0.29</td><td>0.073</td><td>0.04</td><td>0.0</td><td>0.0</td></tr><tr><td><br /><i>Pr</i>@500</td><td>0.33</td><td>0.26</td><td>0.04</td><td>0.03</td><td>0.0</td><td>0.0</td></tr><tr><td><br /><i>Pr</i>@1000</td><td>0.26</td><td>0.191</td><td>0.04</td><td>0.03</td><td>0.0</td><td>0.0</td></tr><tr><td><br /><i>Pr</i>@3000</td><td>0.14</td><td>0.13</td><td>0.03</td><td>0.03</td><td>0.0003</td><td>0.08</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="198">
                    <p class="img_tit"><b>表3 6种方法在Pubmed数据集上的网络重构结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Results of network reconstruction of 6 methods on Pubmed dataset</p>
                    <p class="img_note"></p>
                    <table id="198" border="1"><tr><td>指标</td><td>Semi-<br />GCNAE</td><td>GCN</td><td>SDNE</td><td>GAT</td><td>HOPE</td><td>GF</td></tr><tr><td><br /><i>Pr</i>@5</td><td>0.5</td><td>0.2</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td><br /><i>Pr</i>@10</td><td>0.5</td><td>0.2</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td><br /><i>Pr</i>@100</td><td>0.17</td><td>0.1</td><td>0.0</td><td>0.02</td><td>0.0</td><td>0.0</td></tr><tr><td><br /><i>Pr</i>@200</td><td>0.15</td><td>0.09</td><td>0.0</td><td>0.02</td><td>0.0</td><td>0.0</td></tr><tr><td><br /><i>Pr</i>@300</td><td>0.13</td><td>0.09</td><td>0.03</td><td>0.01</td><td>0.0</td><td>0.0</td></tr><tr><td><br /><i>Pr</i>@500</td><td>0.13</td><td>0.086</td><td>0.04</td><td>0.01</td><td>0.0</td><td>0.0</td></tr><tr><td><br /><i>Pr</i>@1000</td><td>0.11</td><td>0.065</td><td>0.05</td><td>0.01</td><td>0.0</td><td>0.0</td></tr><tr><td><br /><i>Pr</i>@3000</td><td>0.08</td><td>0.05</td><td>0.05</td><td>0.01</td><td>0.0001</td><td>0.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="199">
                    <p class="img_tit"><b>表4 6种方法在Wikipedia数据集上的网络重构结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Results of network reconstruction of 6 methods on Wikipedia dataset</p>
                    <p class="img_note"></p>
                    <table id="199" border="1"><tr><td>指标</td><td>Semi-<br />GCNAE</td><td>GCN</td><td>SDNE</td><td>GAT</td><td>HOPE</td><td>GF</td></tr><tr><td><br /><i>Pr</i>@5</td><td>1.0</td><td>0.2</td><td>0.0</td><td>0.4</td><td>0.4</td><td>0.2</td></tr><tr><td><br /><i>Pr</i>@10</td><td>0.8</td><td>0.2</td><td>0.0</td><td>0.2</td><td>0.2</td><td>0.2</td></tr><tr><td><br /><i>Pr</i>@100</td><td>0.43</td><td>0.13</td><td>0.0</td><td>0.02</td><td>0.2</td><td>0.03</td></tr><tr><td><br /><i>Pr</i>@200</td><td>0.33</td><td>0.12</td><td>0.01</td><td>0.08</td><td>0.13</td><td>0.02</td></tr><tr><td><br /><i>Pr</i>@300</td><td>0.29</td><td>0.12</td><td>0.12</td><td>0.07</td><td>0.1</td><td>0.012</td></tr><tr><td><br /><i>Pr</i>@500</td><td>0.26</td><td>0.11</td><td>0.18</td><td>0.09</td><td>0.07</td><td>0.02</td></tr><tr><td><br /><i>Pr</i>@1000</td><td>0.25</td><td>0.09</td><td>0.16</td><td>0.10</td><td>0.06</td><td>0.01</td></tr><tr><td><br /><i>Pr</i>@3000</td><td>0.18</td><td>0.07</td><td>0.14</td><td>0.08</td><td>0.04</td><td>0.01</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="200">由表2～表4可见, 在3个数据集上, Semi-GCNAE在网络重构任务中的表现均明显优于其它方法.另外, 从实验结果可见, HOPE、GF在Cora、Pubmed数据集上的表现很不理想, 几乎没有保留网络的结构.由此表明, Semi-GCNAE能有效保留复杂信息网络或图的结构信息.</p>
                </div>
                <h3 id="201" name="201" class="anchor-tag">3 结 束 语</h3>
                <div class="p1">
                    <p id="202">本文提出可扩展的半监督深度NRL模型 (Semi-GCNAE) .利用多个卷积层的卷积操作, 获得原始网络以节点为中心的<i>K</i>阶邻域内所有节点结构和特征信息.为了进一步处理获得的节点结构和特征信息, 联合AE和LE并借助集成学习的思想, 对GCN组件习得的邻接向量进行特征提取和非线性降维, 最终生成节点的低维向量表示.通过一系列的对比实验验证Semi-GCNAE能有效捕获大规模复杂信息网络中结构和特征信息, 并在下游的应用中表现较优.今后将针对大规模动态复杂网络进行网络表示建模, 以实现对动态复杂信息网络的实时网络数据挖掘.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="354" type="formula" href="images/MSSB201904005_35400.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">王杰</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="355" type="formula" href="images/MSSB201904005_35500.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">张曦煌</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="244">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Latent Space Approaches to Social Network Analysis">

                                <b>[1]</b>HOFF P D, RAFTERY A E, HANDCOCK M S.Latent Space Approaches to Social Network Analysis.Technical Report, 399.Seattle, USA:University of Washington, 2001.
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Network Representation Learning[C/OL]">

                                <b>[2]</b>ZHANG D K, YIN J, ZHU X Q, et al.Network Representation Learning[C/OL].[2018-11-25].https://arxiv.org/pdf/1801.05852.pdf.
                            </a>
                        </p>
                        <p id="248">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Representation learning:A review and new perspectives">

                                <b>[3]</b>BENGIO Y, COURVILLE A, VINCENT P.Representation Learning:A Review and New Perspectives.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 35 (8) :1798-1828.
                            </a>
                        </p>
                        <p id="250">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Node classification in social networks">

                                <b>[4]</b>BHAGAT S, CORMODE G, MUTHUKRISHNAN S.Node Classification in Social Networks//AGGARWAL C C, ed.Social Network Data Analytics.Berlin, Germany:Springer, 2011:115-148.
                            </a>
                        </p>
                        <p id="252">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing data using t-SNE">

                                <b>[5]</b>VAN DER MAATEN L, HINTON G.Visualizing Data Using t-SNE.Journal of Machine Learning Research, 2008, 9:2579-2605.
                            </a>
                        </p>
                        <p id="254">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing large-scale and high-dimensional data">

                                <b>[6]</b>TANG J, LIU J Z, ZHANG M, et al.Visualizing Large-Scale and High-Dimensional Data//Proc of the 25th International Conference on World Wide Web.New York, USA:ACM, 2016:287-297.
                            </a>
                        </p>
                        <p id="256">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spectral Min-Max Cut for Graph Partitioning and Data Clustering[C/OL]">

                                <b>[7]</b>DING C H Q, HE X F, ZHA H Y, et al.Spectral Min-Max Cut for Graph Partitioning and Data Clustering[C/OL].[2018-11-25].https://escholarship.org/uc/item/0g18c027.
                            </a>
                        </p>
                        <p id="258">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The link prediction problem for social networks">

                                <b>[8]</b>LIBEN-NOWELL D, KLEINBERG J.The Link-Prediction Problem for Social Networks//Proc of the 20th Annual ACM International Conference on Information and Knowledge Management.New York, USA:ACM, 2003:556-559.
                            </a>
                        </p>
                        <p id="260">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                HE K M, ZHANG X Y, REN S Q, et al.Deep Residual Learning for Image Recognition//Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington, USA:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="262">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural Word Embedding as Implicit Matrix Factorization">

                                <b>[10]</b>LEVY O, GOLDBERG Y.Neural Word Embedding as Implicit Matrix Factorization//GHAHARMANI Z, WELLING M, CORTESC, et al., eds.Advances in Neural Information Processing Systems27.Cambridge, USA:The MIT Press, 2014:2177-2185.
                            </a>
                        </p>
                        <p id="264">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deepwalk:Online learning of social representations">

                                <b>[11]</b>PEROZZI B, AL-RFOU R, SKIENA S.DeepWalk:Online Learning of Social Representations//Proc of the 20th ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2014:701-710.
                            </a>
                        </p>
                        <p id="266">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Node2vec:Scalable Feature Learning for Networks">

                                <b>[12]</b>GROVER A, LESKOVEC J.Node2vec:Scalable Feature Learning for Networks//Proc of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2016:855-864.
                            </a>
                        </p>
                        <p id="268">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LINE:large-scale information network embedding">

                                <b>[13]</b>TANG J, QU M, WANG M Z, et al.LINE:Large-Scale Information Network Embedding//Proc of the 24th International Conference on World Wide Web.New York, USA:ACM, 2015:1067-1077.
                            </a>
                        </p>
                        <p id="270">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spectral Networks and Locally Connected Networks on Graphs[C/OL]">

                                <b>[14]</b>BRUNA J, ZAREMBA W, SZLAM A, et al.Spectral Networks and Locally Connected Networks on Graphs[C/OL].[2018-11-25].https://arxiv.org/pdf/1312.6203.pdf.
                            </a>
                        </p>
                        <p id="272">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spectra Graph Theory">

                                <b>[15]</b>CHUNG F R K.Spectral Graph Theory.Providence, USA:American Mathematical Society, 1997.
                            </a>
                        </p>
                        <p id="274">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gradient-based learning applied to document recognition">

                                <b>[16]</b>LECUN Y, BOTTOU L, BENGIO Y, et al.Gradient-Based Learning Applied to Document Recognition.Proceedings of the IEEE, 1998, 86 (11) :2278-2324.
                            </a>
                        </p>
                        <p id="276">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering[C/OL]">

                                <b>[17]</b>DEFFERRARD M, BRESSON X, VANDERGHEYNST P.Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering[C/OL].[2018-11-25].https://arxiv.org/pdf/1606.09375.pdf.
                            </a>
                        </p>
                        <p id="278">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501147058&amp;v=MDk1NTlHZXJxUVRNbndaZVp1SHlqbVVMYklKRndjYmhRPU5pZk9mYks3SHRETnFvOUVaZThJREhreG9CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b>HAMMOND D K, VANDERGHEYNST P, GRIBONVAL R.Wavelets on Graphs via Spectral Graph Theory.Applied and Computational Harmonic Analysis, 2011, 30 (2) :129-150.
                            </a>
                        </p>
                        <p id="280">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised Classification with Graph Convolutional Networks[C/OL]">

                                <b>[19]</b>KIPF T N, WELLING M.Semi-supervised Classification with Graph Convolutional Networks[C/OL].[2018-11-25].https://arxiv.org/pdf/1609.02907.pdf.
                            </a>
                        </p>
                        <p id="282">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning using Gaussian fields and harmonic functions">

                                <b>[20]</b>ZHU X J, GHAHRAMANI Z, LAFFERTY J.Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions//Proc of the 20th International Conference on Machine Learning.Palo Alto, USA:AAAI Press, 2003:912-919.
                            </a>
                        </p>
                        <p id="284">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structural deep network embedding">

                                <b>[21]</b>WANG D X, CUI P, ZHU W.Structural Deep Network Embedding//Proc of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2016:1225-1234.
                            </a>
                        </p>
                        <p id="286">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339457&amp;v=MTg0NTU1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDSGxWNzNBSkZnPU5qN0Jhck80SHRITnJJeE1ZTzRJWTNr&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b>BREIMAN L.Stacked Regression.Machine Learning, 1996, 24 (1) :49-64.
                            </a>
                        </p>
                        <p id="288">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked generalization">

                                <b>[23]</b>WOLPERT D H.Stacked Generalization.Neural Networks, 2017, 5 (2) :241-259.
                            </a>
                        </p>
                        <p id="290">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012194&amp;v=MjYzMzRvTkRYVTlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRndjYmhRPU5pZkpaYks5SHRqTXFvOUZaTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b>BELKIN M, NIYOGI P.Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.Neural Computation, 2003, 15 (6) :1373-1396.
                            </a>
                        </p>
                        <p id="292">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep architectures for AI">

                                <b>[25]</b>BENGION Y.Learning Deep Architectures for AI.Foundations and Trends in Machine Learning, 2009, 2 (1) :1-127.
                            </a>
                        </p>
                        <p id="294">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning with Local and Global Consistency">

                                <b>[26]</b>ZHOU D Y, BOUSQUET O, LAL T N, et al.Learning with Local and Global Consistency//Proc of the 16th International Conference on Neural Information Processing Systems.Cambridge, USA:The MIT Press, 2003:321-328
                            </a>
                        </p>
                        <p id="296">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Manifold regularization: a geometric framework for learning from labeled and unlabeled examples">

                                <b>[27]</b>BELKIN M, NIYOGI P, SINDHWANI V.Manifold Regularization:A Geometric Framework for Learning from Labeled and Unlabeled Examples.Journal of Machine Learning Research, 2006, 7:2399-2434.
                            </a>
                        </p>
                        <p id="298">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Learning via Semi-Supervised Embedding">

                                <b>[28]</b>WESTON J, RATLE F, MOBAHI U, et al.Deep Learning via Semi-supervised Embedding//MONTAVON G, ORR G B, MLLER K R, eds.Neural Networks:Tricks of the Trade.Berlin, Germany:Springer, 2012:639-665.
                            </a>
                        </p>
                        <p id="300">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tensor Flow:LargeScale Machine Learning on Heterogeneous Distributed Systems[C/OL]">

                                <b>[29]</b>ABADI M, AGARWAL A, BARHAM P, et al.Tensor Flow:LargeScale Machine Learning on Heterogeneous Distributed Systems[C/OL].[2018-11-25].https://arxiv.org/pdf/1603.04467.pdf.
                            </a>
                        </p>
                        <p id="302">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Graph Attention Networks[C/OL]">

                                <b>[30]</b>VELICˇKOVI CP, CUCURULL G, CASANOVA A, et al.Graph Attention Networks[C/OL].[2018-11-25].https://arxiv.org/pdf/1710.10903.pdf.
                            </a>
                        </p>
                        <p id="304">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Asymmetric transitivity preserving graph embedding">

                                <b>[31]</b>OU M D, CUI P, PEI J, et al.Asymmetric Transitivity Preserving Graph Embedding//Proc of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM, 2016:1105-1114
                            </a>
                        </p>
                        <p id="306">
                            <a id="bibliography_32" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14120403044306&amp;v=MzIyMjZ3Y2JoUT1OaWZZZXJLOEg5UE1xNDlHWk84TEQzdy9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKRg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[32]</b>VAN LOAN C F.Generalizing the Singular Value Decomposition.SIAM Journal on Numerical Analysis, 1976, 13 (1) :76-83.
                            </a>
                        </p>
                        <p id="308">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributed large-scale natural graph factorization">

                                <b>[33]</b>AHMED A, SHERVASHIDZE N, NARAYANAMURTHY S, et al.Distributed Large-Scale Natural Graph Factorization//Proc of the22nd International Conference on World Wide Web.New York, USA:ACM, 2013:37-48.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201904005" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201904005&amp;v=MzEzMjBLRDdZYkxHNEg5ak1xNDlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnl6blc3dk8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
