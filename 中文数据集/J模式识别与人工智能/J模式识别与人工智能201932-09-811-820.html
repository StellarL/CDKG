<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131458885811250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dMSSB201909006%26RESULT%3d1%26SIGN%3dU6nu3LK%252bqci6t%252fmUMTplau5i%252foA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201909006&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201909006&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201909006&amp;v=MjExMjNZYkxHNEg5ak1wbzlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1ZyckJLRDc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#85" data-title="1 相关知识 ">1 相关知识</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#86" data-title="&lt;b&gt;1.1&lt;/b&gt; 类别的层次结构"><b>1.1</b> 类别的层次结构</a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;1.2&lt;/b&gt; 邻域粗糙集"><b>1.2</b> 邻域粗糙集</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#111" data-title="2 基于邻域粗糙集的层次分类在线流特征选择 ">2 基于邻域粗糙集的层次分类在线流特征选择</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#112" data-title="&lt;b&gt;2.1&lt;/b&gt; 层次分类的兄弟策略"><b>2.1</b> 层次分类的兄弟策略</a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;2.2&lt;/b&gt; 面向层次结构化数据的邻域粗糙集模型"><b>2.2</b> 面向层次结构化数据的邻域粗糙集模型</a></li>
                                                <li><a href="#139" data-title="&lt;b&gt;2.3 基于邻域粗糙集的层次分类在线流特征选择算法模型&lt;/b&gt;"><b>2.3 基于邻域粗糙集的层次分类在线流特征选择算法模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#181" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#182" data-title="&lt;b&gt;3.1 实验数据&lt;/b&gt;"><b>3.1 实验数据</b></a></li>
                                                <li><a href="#185" data-title="&lt;b&gt;3.2 实验设置&lt;/b&gt;"><b>3.2 实验设置</b></a></li>
                                                <li><a href="#201" data-title="&lt;b&gt;3.3 参数&lt;/b&gt;&lt;i&gt;δ&lt;/i&gt;&lt;b&gt;分析&lt;/b&gt;"><b>3.3 参数</b><i>δ</i><b>分析</b></a></li>
                                                <li><a href="#224" data-title="&lt;b&gt;3.4 有效性分析&lt;/b&gt;"><b>3.4 有效性分析</b></a></li>
                                                <li><a href="#252" data-title="&lt;b&gt;3.5 时间复杂度分析&lt;/b&gt;"><b>3.5 时间复杂度分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#255" data-title="4 结 束 语 ">4 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="&lt;b&gt;表1 符号说明&lt;/b&gt;"><b>表1 符号说明</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;表2 定义同类样本和异类样本的3种策略&lt;/b&gt;"><b>表2 定义同类样本和异类样本的3种策略</b></a></li>
                                                <li><a href="#184" data-title="&lt;b&gt;表3 实验数据集&lt;/b&gt;"><b>表3 实验数据集</b></a></li>
                                                <li><a href="#318" data-title="图1 δ不同时OHFS在LSVM分类器上的性能对比">图1 δ不同时OHFS在LSVM分类器上的性能对比</a></li>
                                                <li><a href="#318" data-title="图1 δ不同时OHFS在LSVM分类器上的性能对比">图1 δ不同时OHFS在LSVM分类器上的性能对比</a></li>
                                                <li><a href="#319" data-title="图2 δ不同时OHFS在KNN分类器上的性能对比">图2 δ不同时OHFS在KNN分类器上的性能对比</a></li>
                                                <li><a href="#319" data-title="图2 δ不同时OHFS在KNN分类器上的性能对比">图2 δ不同时OHFS在KNN分类器上的性能对比</a></li>
                                                <li><a href="#227" data-title="&lt;b&gt;表4 各算法在LSVM分类器上的分类精度&lt;/b&gt;"><b>表4 各算法在LSVM分类器上的分类精度</b></a></li>
                                                <li><a href="#228" data-title="&lt;b&gt;表5 各算法在LSVM分类器上的LCA-F1值&lt;/b&gt;"><b>表5 各算法在LSVM分类器上的LCA-F1值</b></a></li>
                                                <li><a href="#229" data-title="&lt;b&gt;表6 各算法在LSVM分类器上的TIE值&lt;/b&gt;"><b>表6 各算法在LSVM分类器上的TIE值</b></a></li>
                                                <li><a href="#230" data-title="&lt;b&gt;表7 各算法在KNN分类器上的分类精度&lt;/b&gt;"><b>表7 各算法在KNN分类器上的分类精度</b></a></li>
                                                <li><a href="#231" data-title="&lt;b&gt;表8 各算法在KNN分类器上的LCA-F1值&lt;/b&gt;"><b>表8 各算法在KNN分类器上的LCA-F1值</b></a></li>
                                                <li><a href="#232" data-title="&lt;b&gt;表9 各算法在KNN分类器上的TIE值&lt;/b&gt;"><b>表9 各算法在KNN分类器上的TIE值</b></a></li>
                                                <li><a href="#320" data-title="图3 各算法在LSVM分类器上的盒形图对比">图3 各算法在LSVM分类器上的盒形图对比</a></li>
                                                <li><a href="#321" data-title="图4 各算法在KNN分类器上的盒形图对比">图4 各算法在KNN分类器上的盒形图对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="322">


                                    <a id="bibliography_1" title=" 胡清华,王煜,周玉灿,等.大规模分类任务的分层学习方法综述.中国科学:信息科学,2018,48(5):487-500.(HU Q H,WANG Y,ZHOU Y C,et al.A Review on Hierarchical Learning Methods for Large-Scale Classification Task.Scientia Sinica(Informationis),2018,48(5):487-500.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=PZKX201805002&amp;v=MjAxNzdvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tWcnJCTlRmQWRyRzRIOW5NcW85Rlo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         胡清华,王煜,周玉灿,等.大规模分类任务的分层学习方法综述.中国科学:信息科学,2018,48(5):487-500.(HU Q H,WANG Y,ZHOU Y C,et al.A Review on Hierarchical Learning Methods for Large-Scale Classification Task.Scientia Sinica(Informationis),2018,48(5):487-500.)
                                    </a>
                                </li>
                                <li id="324">


                                    <a id="bibliography_2" title=" GOPAL S,YANG Y M.Hierarchical Bayesian Inference and Recursive Regularization for Large-Scale Classification.ACM Transactions on Knowledge Discovery from Data,2015,9(3).DOI:10.1145/2629585." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM499518FDD3BC5750C7297CCB0C57EA62&amp;v=MjgyMzEzNU5GaHc3bTZ4YUU9TmlmSVk3ZXhGOVROcC9reEVPaDlmM2sreWhaZzdUMTBUd3lSM2hKR2ZMWGhOTHlkQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         GOPAL S,YANG Y M.Hierarchical Bayesian Inference and Recursive Regularization for Large-Scale Classification.ACM Transactions on Knowledge Discovery from Data,2015,9(3).DOI:10.1145/2629585.
                                    </a>
                                </li>
                                <li id="326">


                                    <a id="bibliography_3" title=" ASHBURNER M,BALL C A,BLAKE J A,et al.Gene Ontology:Tool for the Unification of Biology.Nature Genetics,2000,25(1):25-29." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gene ontology: tool for the unification of biology">
                                        <b>[3]</b>
                                         ASHBURNER M,BALL C A,BLAKE J A,et al.Gene Ontology:Tool for the Unification of Biology.Nature Genetics,2000,25(1):25-29.
                                    </a>
                                </li>
                                <li id="328">


                                    <a id="bibliography_4" title=" NOURANI-VATANI N,L&#211;PEZ-SASTRE R,WILLIAM S.Structured Output Prediction with Hierarchical Loss Functions for Seafloor Imagery Taxonomic Categorization // Proc of the Iberian Conference on Pattern Recognition and Image Analysis.Berlin,Germany:Springer,2015:173-183." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structured Output Prediction with Hierarchical Loss Functions for Seafloor Imagery Taxonomic Categorization">
                                        <b>[4]</b>
                                         NOURANI-VATANI N,L&#211;PEZ-SASTRE R,WILLIAM S.Structured Output Prediction with Hierarchical Loss Functions for Seafloor Imagery Taxonomic Categorization // Proc of the Iberian Conference on Pattern Recognition and Image Analysis.Berlin,Germany:Springer,2015:173-183.
                                    </a>
                                </li>
                                <li id="330">


                                    <a id="bibliography_5" title=" SUN A X,LIM E P.Hierarchical Text Classification and Evaluation // Proc of the 1st IEEE International Conference on Data Mining.Washington,USA:IEEE,2001:521-528." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical text classification and evaluation">
                                        <b>[5]</b>
                                         SUN A X,LIM E P.Hierarchical Text Classification and Evaluation // Proc of the 1st IEEE International Conference on Data Mining.Washington,USA:IEEE,2001:521-528.
                                    </a>
                                </li>
                                <li id="332">


                                    <a id="bibliography_6" title=" 王晨曦,林耀进,唐莉,等.基于信息粒化的多标记特征选择算法.模式识别与人工智能,2018,31(2):123-131.(WANG C X,LIN Y J,TANG L,et al.Multi-label Feature Selection Based on Information Granulation.Pattern Recognition and Artificial Intelligence,2018,31(2):123-131.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201802003&amp;v=MDA0OTVGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1ZyckJLRDdZYkxHNEg5bk1yWTk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         王晨曦,林耀进,唐莉,等.基于信息粒化的多标记特征选择算法.模式识别与人工智能,2018,31(2):123-131.(WANG C X,LIN Y J,TANG L,et al.Multi-label Feature Selection Based on Information Granulation.Pattern Recognition and Artificial Intelligence,2018,31(2):123-131.)
                                    </a>
                                </li>
                                <li id="334">


                                    <a id="bibliography_7" title=" FREEMAN C,KULI■.Joint Feature Selection and Hierarchical Classifier Design // Proc of the IEEE International Conference on Systems,Man,and Cybernetics.Washington,USA:IEEE,2011:1728-1734." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint Feature Selection and Hierarchical Classifier Design">
                                        <b>[7]</b>
                                         FREEMAN C,KULI■.Joint Feature Selection and Hierarchical Classifier Design // Proc of the IEEE International Conference on Systems,Man,and Cybernetics.Washington,USA:IEEE,2011:1728-1734.
                                    </a>
                                </li>
                                <li id="336">


                                    <a id="bibliography_8" title=" SONG J,ZHANG P Z,QIN S J,et al.A Method of the Feature Selection in Hierarchical Text Classification Based on the Category Discrimination and Position Information.IEEE Transactions on Engineering Management,2015,53(4):555-569." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Method of the Feature Selection in Hierarchical Text Classification Based on the Category Discrimination and Position Information">
                                        <b>[8]</b>
                                         SONG J,ZHANG P Z,QIN S J,et al.A Method of the Feature Selection in Hierarchical Text Classification Based on the Category Discrimination and Position Information.IEEE Transactions on Engineering Management,2015,53(4):555-569.
                                    </a>
                                </li>
                                <li id="338">


                                    <a id="bibliography_9" title=" ZHAO H,ZHU P F,WANG P,et al.Hierarchical Feature Selection with Recursive Regularization // Proc of the 26th International Joint Conference on Artificial Intelligence.New York,USA:ACM,2017:3483-3489." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical feature selection with recursive regularization">
                                        <b>[9]</b>
                                         ZHAO H,ZHU P F,WANG P,et al.Hierarchical Feature Selection with Recursive Regularization // Proc of the 26th International Joint Conference on Artificial Intelligence.New York,USA:ACM,2017:3483-3489.
                                    </a>
                                </li>
                                <li id="340">


                                    <a id="bibliography_10" title=" WU X D,YU K,DING W,et al.Online Feature Selection with Streaming Features.IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(5):1178-1192." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online feature selection with streaming features">
                                        <b>[10]</b>
                                         WU X D,YU K,DING W,et al.Online Feature Selection with Streaming Features.IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(5):1178-1192.
                                    </a>
                                </li>
                                <li id="342">


                                    <a id="bibliography_11" title=" ZHOU P,HU X G,LI P P,et al.OFS-Density:A Novel Online Streaming Feature Selection Method.Pattern Recognition,2019,86:48-61." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES5DC777A05054E013A29479C15F09A214&amp;v=MDkwMzNlc0tDQWs1emhWaTZEWjVUM2FSclJkRGVidmxSN3ViQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZodzdtNnhhRT1OaWZPZmJiTWJkYkxxUDVGWQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         ZHOU P,HU X G,LI P P,et al.OFS-Density:A Novel Online Streaming Feature Selection Method.Pattern Recognition,2019,86:48-61.
                                    </a>
                                </li>
                                <li id="344">


                                    <a id="bibliography_12" title=" 刘景华,林梦雷,王晨曦,等.基于局部子空间的多标记特征选择算法.模式识别与人工智能,2016,29(3):240-251.(LIU J H,LIN M L,WANG C X,et al.Multi-label Feature Selection Algorithm Based on Local Subspace.Pattern Recognition and Artificial Intelligence,2016,29(3):240-251.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201603006&amp;v=MjEyMzdyckJLRDdZYkxHNEg5Zk1ySTlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         刘景华,林梦雷,王晨曦,等.基于局部子空间的多标记特征选择算法.模式识别与人工智能,2016,29(3):240-251.(LIU J H,LIN M L,WANG C X,et al.Multi-label Feature Selection Algorithm Based on Local Subspace.Pattern Recognition and Artificial Intelligence,2016,29(3):240-251.)
                                    </a>
                                </li>
                                <li id="346">


                                    <a id="bibliography_13" title=" LIN Y J,HU Q H,LIU J H,et al.Streaming Feature Selection for Multilabel Learning Based on Fuzzy Mutual Information.IEEE Transactions on Fuzzy Systems,2017,25(6):1491-1507." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Streaming Feature Selection for Multilabel Learning Based on Fuzzy Mutual Information">
                                        <b>[13]</b>
                                         LIN Y J,HU Q H,LIU J H,et al.Streaming Feature Selection for Multilabel Learning Based on Fuzzy Mutual Information.IEEE Transactions on Fuzzy Systems,2017,25(6):1491-1507.
                                    </a>
                                </li>
                                <li id="348">


                                    <a id="bibliography_14" title=" LIU J H,LIN Y J,LI Y W,et al.Online Multi-label Streaming Feature Selection Based on Neighborhood Rough Set.Pattern Reco-gnition,2018,84:273-287." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC0F4FC48B60B9DCF6FF0C3ACEA015564&amp;v=MjAxNTJmY0M0YU5XNjNJdE5GdTBQZm5WTnZHQVZuRWw5TzN5VDMyZEVlYk9SUUx5YkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHc3bTZ4YUU9TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         LIU J H,LIN Y J,LI Y W,et al.Online Multi-label Streaming Feature Selection Based on Neighborhood Rough Set.Pattern Reco-gnition,2018,84:273-287.
                                    </a>
                                </li>
                                <li id="350">


                                    <a id="bibliography_15" title=" 介飞,谢飞,李磊,等.社交网络中隐式事件突发性检测.自动化学报,2018,44(4):730-742.(JIE F,XIE F,LI L,et al.Latent Event-Related Burst in Social Networks.Acta Automatica Sinica,2018,44(4):730-742.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201804014&amp;v=MTA1NzM0OUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVnJyQktDTGZZYkc0SDluTXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         介飞,谢飞,李磊,等.社交网络中隐式事件突发性检测.自动化学报,2018,44(4):730-742.(JIE F,XIE F,LI L,et al.Latent Event-Related Burst in Social Networks.Acta Automatica Sinica,2018,44(4):730-742.)
                                    </a>
                                </li>
                                <li id="352">


                                    <a id="bibliography_16" title=" WU F H,ZHANG J,HONAVAR V.Learning Classifiers Using Hierarchically Structured Class Taxonomies // Proc of the International Symposium on Abstraction,Reformulation and Approximation.Berlin,Germany:Springer,2005:313-320." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Classifiers Using Hierarchically Structured Class Taxonomies">
                                        <b>[16]</b>
                                         WU F H,ZHANG J,HONAVAR V.Learning Classifiers Using Hierarchically Structured Class Taxonomies // Proc of the International Symposium on Abstraction,Reformulation and Approximation.Berlin,Germany:Springer,2005:313-320.
                                    </a>
                                </li>
                                <li id="354">


                                    <a id="bibliography_17" title=" JR SILLA C N,FREITAS A A.A Survey of Hierarchical Classification across Different Application Domains.Data Mining and Knowledge Discovery,2011,22(1/2):31-72." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A survey of hierarchical classification across different application domains">
                                        <b>[17]</b>
                                         JR SILLA C N,FREITAS A A.A Survey of Hierarchical Classification across Different Application Domains.Data Mining and Knowledge Discovery,2011,22(1/2):31-72.
                                    </a>
                                </li>
                                <li id="356">


                                    <a id="bibliography_18" title=" 胡清华,于达仁,谢宗霞.基于邻域粒化和粗糙逼近的数值属性约简.软件学报,2008,19(3):640-649.(HU Q H,YU D R,XIE Z X.Numerical Attribute Reduction Based on Neighborhood Granulation and Rough Approximation.Journal of Software,2008,19(3):640-649.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200803018&amp;v=MDk1NzE0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVnJyQk55ZlRiTEc0SHRuTXJJOUViSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         胡清华,于达仁,谢宗霞.基于邻域粒化和粗糙逼近的数值属性约简.软件学报,2008,19(3):640-649.(HU Q H,YU D R,XIE Z X.Numerical Attribute Reduction Based on Neighborhood Granulation and Rough Approximation.Journal of Software,2008,19(3):640-649.)
                                    </a>
                                </li>
                                <li id="358">


                                    <a id="bibliography_19" title=" EISNER R,POULIN B,SZAFRON D,et al.Improving Protein Function Prediction Using the Hierarchical Structure of the Gene Ontology // Proc of the IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology.Washington,USA:IEEE,2005.DOI:10.1109/CIBCB.2005.1594940." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving protein function prediction using the hierarchical structure of the gene ontology">
                                        <b>[19]</b>
                                         EISNER R,POULIN B,SZAFRON D,et al.Improving Protein Function Prediction Using the Hierarchical Structure of the Gene Ontology // Proc of the IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology.Washington,USA:IEEE,2005.DOI:10.1109/CIBCB.2005.1594940.
                                    </a>
                                </li>
                                <li id="360">


                                    <a id="bibliography_20" title=" CECI M,MALERBA D.Classifying Web Documents in a Hierarchy of Categories:A Comprehensive Study.Journal of Intelligent Information Systems(Integrating Artificial,Intelligence and Database Technologies),2007,28(1):37-78." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001510155&amp;v=Mjc5ODhZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDSGxWTDdOSlZjPU5qN0Jhck80SHRITnFvNUZaZTRL&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         CECI M,MALERBA D.Classifying Web Documents in a Hierarchy of Categories:A Comprehensive Study.Journal of Intelligent Information Systems(Integrating Artificial,Intelligence and Database Technologies),2007,28(1):37-78.
                                    </a>
                                </li>
                                <li id="362">


                                    <a id="bibliography_21" title=" KRIZHEVSKY A,HINTON G.Learning Multiple Layers of Features from Tiny Images[C/OL].[2019-04-25].http://www.cs.toronto.edu/～kriz/learning-features-2009-TR.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning multiple layers of features from tiny images">
                                        <b>[21]</b>
                                         KRIZHEVSKY A,HINTON G.Learning Multiple Layers of Features from Tiny Images[C/OL].[2019-04-25].http://www.cs.toronto.edu/～kriz/learning-features-2009-TR.pdf.
                                    </a>
                                </li>
                                <li id="364">


                                    <a id="bibliography_22" title=" LAMPERT C H,NICKISCH H,HARMELING S.Learning to Detect Unseen Object Classes by Between-Class Attribute Transfer // Proc of the IEEE Conference on Computer Vision and Pattern Reco-gnition.Washington,USA:IEEE,2009:951-958." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to detect unseen object classes by between-class attribute transfer">
                                        <b>[22]</b>
                                         LAMPERT C H,NICKISCH H,HARMELING S.Learning to Detect Unseen Object Classes by Between-Class Attribute Transfer // Proc of the IEEE Conference on Computer Vision and Pattern Reco-gnition.Washington,USA:IEEE,2009:951-958.
                                    </a>
                                </li>
                                <li id="366">


                                    <a id="bibliography_23" title=" EVERINGHAM M,VAN GOOL L,WILLIAMS C K,et al.The Pascal Visual Object Classes(voc) Challenge.International Journal of Computer Vision,2010,88(2):303-338." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MDA0NzBrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQ0hsVkw3TkpWYz1OajdCYXJPNEh0SFBxWWRIWStJTFkz&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         EVERINGHAM M,VAN GOOL L,WILLIAMS C K,et al.The Pascal Visual Object Classes(voc) Challenge.International Journal of Computer Vision,2010,88(2):303-338.
                                    </a>
                                </li>
                                <li id="368">


                                    <a id="bibliography_24" title=" WEI L Y,LIAO M H,GAO X,et al.An Improved Protein Structural Classes Prediction Method by Incorporating both Sequence and Structure Information.IEEE Transactions on NanoBioscience,2015,14(4):339-349." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Improved Protein Structural Prediction Method By Incorporating Both Sequence And Structure Information.">
                                        <b>[24]</b>
                                         WEI L Y,LIAO M H,GAO X,et al.An Improved Protein Structural Classes Prediction Method by Incorporating both Sequence and Structure Information.IEEE Transactions on NanoBioscience,2015,14(4):339-349.
                                    </a>
                                </li>
                                <li id="370">


                                    <a id="bibliography_25" title=" DING C H,DUBCHAK I.Multi-class Protein Fold Recognition Using Support Vector Machines and Neural Networks.Bioinforma-tics,2001,17(4):349-358." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJOX&amp;filename=SJOX14010900014144&amp;v=MjM2NzlGWk9vTERYZzlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklKMThSYnhzPU5pZkVkcks4SHRETXBvOQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         DING C H,DUBCHAK I.Multi-class Protein Fold Recognition Using Support Vector Machines and Neural Networks.Bioinforma-tics,2001,17(4):349-358.
                                    </a>
                                </li>
                                <li id="372">


                                    <a id="bibliography_26" title=" DEKEL O,KESHET J,SINGER Y.Large Margin Hierarchical Classification // Proc of the 21th ACM International Conference on Machine Learning.New York,USA:ACM,2004.DOI:10.1145/1015330.1015374." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large Margin Hierarchical Classification">
                                        <b>[26]</b>
                                         DEKEL O,KESHET J,SINGER Y.Large Margin Hierarchical Classification // Proc of the 21th ACM International Conference on Machine Learning.New York,USA:ACM,2004.DOI:10.1145/1015330.1015374.
                                    </a>
                                </li>
                                <li id="374">


                                    <a id="bibliography_27" title=" YU K,WU X D,DING W,et al.Scalable and Accurate Online Feature Selection for Big Data.ACM Transactions on Knowledge Discovery from Data,2016,11(2).DOI:10.1145/2976744." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM390FFF33858E8979717A5E1CF83FE812&amp;v=MTEzNjZOdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZodzdtNnhhRT1OaWZJWTdDeEhxZTYyWXhHYk80SGVYUXd5QjhVNnpnTVRRcmozMlE5ZXNUaFRidWRDTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         YU K,WU X D,DING W,et al.Scalable and Accurate Online Feature Selection for Big Data.ACM Transactions on Knowledge Discovery from Data,2016,11(2).DOI:10.1145/2976744.
                                    </a>
                                </li>
                                <li id="376">


                                    <a id="bibliography_28" title=" ZHOU P,HU X G,LI P P.A New Online Feature Selection Method Using Neighborhood Rough Set // Proc of the 8th IEEE Internatio-nal Conference on Big Knowledge.Washington,USA:IEEE,2017:135-142." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A New Online Feature Selection Method Using Neighborhood Rough Set">
                                        <b>[28]</b>
                                         ZHOU P,HU X G,LI P P.A New Online Feature Selection Method Using Neighborhood Rough Set // Proc of the 8th IEEE Internatio-nal Conference on Big Knowledge.Washington,USA:IEEE,2017:135-142.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(09),811-820 DOI:10.16451/j.cnki.issn1003-6059.201909005            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于邻域粗糙集的大规模层次分类在线流特征选择</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%99%BD%E7%9B%9B%E5%85%B4&amp;code=42911709&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">白盛兴</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9E%97%E8%80%80%E8%BF%9B&amp;code=33194400&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">林耀进</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%99%A8%E6%9B%A6&amp;code=37906622&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王晨曦</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E6%99%9F%E7%85%9C&amp;code=42911712&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈晟煜</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%97%BD%E5%8D%97%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=1698703&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">闽南师范大学计算机学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%BB%BA%E7%9C%81%E7%B2%92%E8%AE%A1%E7%AE%97%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0041326&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福建省粒计算及其应用重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=School%20of%20Informatics%2CComputing%20%26%20Engineering%2CIndiana%20University%20Bloomington&amp;code=0088150&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">School of Informatics,Computing &amp; Engineering,Indiana University Bloomington</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在分类学习任务中,数据的类标记空间存在层次化结构,特征空间伴随着未知性和演化性.因此,文中提出面向大规模层次分类学习的在线流特征选择框架.定义面向层次化结构数据的邻域粗糙模型,基于特征相关性进行重要特征动态选择.最后,基于特征冗余性进行鉴别冗余动态特征.实验验证文中算法的有效性.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%A8%E7%BA%BF%E6%B5%81%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">在线流特征选择;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B1%82%E6%AC%A1%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">层次分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%82%BB%E5%9F%9F%E7%B2%97%E7%B3%99%E9%9B%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邻域粗糙集;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%84%E5%BC%9F%E7%AD%96%E7%95%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">兄弟策略;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    白盛兴，硕士研究生，主要研究方向为数据挖掘．E-mail:bsxing813@gmail.com.&lt;image id="313" type="formula" href="images/MSSB201909006_31300.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    *林耀进(通讯作者)，博士，教授，主要研究方向为数据挖掘、粒计算．E-mail:zzlinyaojin@163.com.&lt;image id="314" type="formula" href="images/MSSB201909006_31400.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    王晨曦，硕士，讲师，主要研究方向为数据挖掘．E-mail:wangcx5@sina.com.&lt;image id="315" type="formula" href="images/MSSB201909006_31500.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    陈晟煜，硕士研究生，主要研究方向为数据挖掘．E-mail:chensy0715@gmail.com.&lt;image id="316" type="formula" href="images/MSSB201909006_31600.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-07</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(No.61672272);</span>
                                <span>福建省自然科学基金项目(No.2018J01548,2018J01547);</span>
                                <span>福建省教育厅科技项目(No.JT180318)资助;</span>
                    </p>
            </div>
                    <h1><b>Large-Scale Hierarchical Classification Online Streaming Feature Selection Based on Neighborhood Rough Set</b></h1>
                    <h2>
                    <span>BAI Shengxing</span>
                    <span>LIN Yaojin</span>
                    <span>WANG Chenxi</span>
                    <span>CHEN Shengyu</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Engineering,Minnan Normal University</span>
                    <span>Fujian Key Laboratory of Granular Computing and Application</span>
                    <span>School of Informatics,Computing &amp; Engineering,Indiana University Bloomington</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Label space of data possesses a hierarchical structure, and feature space is unknown and evolutionary in many classification learning tasks. An online streaming feature selection framework for large-scale hierarchical classification task is proposed. Firstly, a neighborhood rough model is defined for hierarchical structure data. Important features are dynamically selected based on feature correlation. Finally, the redundant dynamic features are identified based on feature redundancy. Experiments are conducted to verify the effectiveness of the proposed algorithm.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Online%20Streaming%20Feature%20Selection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Online Streaming Feature Selection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Hierarchical%20Classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Hierarchical Classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Neighborhood%20Rough%20Set&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Neighborhood Rough Set;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Sibling%20Strategy&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Sibling Strategy;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    BAI Shengxing,master student. His research interests include data mining.;
                                </span>
                                <span>
                                    LIN Yaojing(Corresponding author),Ph. D., professor. His research interests include data mining and granular computing.;
                                </span>
                                <span>
                                    WANG Chenxi,master,lecturer. Her research interests include data mining.;
                                </span>
                                <span>
                                    CHEN Shengyu,master student. His research interests include data mining.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-07</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China(No.61672272);</span>
                                <span>Natural Science Foundation of Fujian Province(No.2018J01548,2018J01547);</span>
                                <span>Department of Education of Fujian Province(No.JT180318);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="72">在大数据时代,在许多实际应用领域中,数据特征空间呈现特征高维性及动态性,类标记空间存在层次化结构.例如,卡尔冯林奈提出树状界、门、纲、目、科、属、种的物种分类,用于处理大量动植物类别.类似的还有ImageNet、维基百科、网页数据、生物数据、地理数据、文本数据等<citation id="378" type="reference"><link href="324" rel="bibliography" /><link href="326" rel="bibliography" /><link href="328" rel="bibliography" /><link href="330" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>应 用场景, 都体现出对于大规模学习问题，人类往往利用分层结构处理．</p>
                </div>
                <div class="p1">
                    <p id="78">在层次化分类建模过程中,该类数据通常伴随样本数量超大、特征空间超高维及演化性<citation id="379" type="reference"><link href="332" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、类别层次化的学习问题.为了降低该类数据所需的存储空间与分类模型训练时间,提高预测精度,研究人员利用特征选择以有效降低该类数据特征空间的高维性.</p>
                </div>
                <div class="p1">
                    <p id="79">目前,学者们已提出相当数量的层次化结构特征选择算法.Freeman等<citation id="380" type="reference"><link href="334" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>通过联合特征选择采用遗传算法设计的分层分类器,提高分类器精度.Song等<citation id="381" type="reference"><link href="336" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出分层文本分类的特征选择算法.Zhao等<citation id="382" type="reference"><link href="338" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出递归正则化的层次分类特征选择框架.</p>
                </div>
                <div class="p1">
                    <p id="80">现有的层次分类学习算法都是假设特征空间固定已知,忽略实际应用中特征的动态性和未知性,即数据特征逐个获取或流入,特征空间的大小未知.例如:在疾病诊断中,通过持续跟踪随访,病人在不同时间段表现出不同的症状;在信息检索中,众包环境下图像随时序的推进逐渐赋予不同的语义描述.学者们提出一些流特征选择算法,针对传统的单标记数据,Wu等<citation id="383" type="reference"><link href="340" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出在线流特征选择框架.Zhou等<citation id="384" type="reference"><link href="342" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出基于邻域密度的在线流特征选择算法.针对多标记数据<citation id="385" type="reference"><link href="344" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,Lin等<citation id="386" type="reference"><link href="346" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出基于模糊互信息的多标记在线流特征选择算法.Liu等<citation id="387" type="reference"><link href="348" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出基于邻域粗糙集的多标记在线流特征选择算法.上述算法都默认类别间相互独立,忽略层次分类数据中类别之间存在的层次结构.</p>
                </div>
                <div class="p1">
                    <p id="81">在层次分类学习中,由于类别之间复杂的语义关联,描述样本的特征并非一次性全部获得,而是随着实际需求不断提取产生,形成流特征表现形式.例如,在图顶点分类中,图顶点数量及顶点特征随时间逐渐增多<citation id="388" type="reference"><link href="350" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="82">于是,如何针对层次化结构数据的分类学习进行在线流特征选择具有重要意义.目前已有的层次化特征选择或在线流特征选择都并未涉及上述问题.</p>
                </div>
                <div class="p1">
                    <p id="83">为此,本文提出基于邻域粗糙集的层次分类在线流特征选择算法(Online Streaming Hierarchical Feature Selection Based on Neighborhood Rough Set, OHFS).首先,利用层次化数据中节点之间存在的兄弟策略定义面向层次化结构数据的邻域粗糙模型.然后,基于邻域依赖度定义特征在线重要性选择和在线冗余更新等策略,构建层次分类在线流特征选择框架,并设计相关算法.最后,通过实验验证本文算法的有效性.</p>
                </div>
                <h3 id="85" name="85" class="anchor-tag">1 相关知识</h3>
                <h4 class="anchor-tag" id="86" name="86"><b>1.1</b> 类别的层次结构</h4>
                <div class="p1">
                    <p id="87">在层次化结构数据中,类别的层次结构一般存在树结构和有向无环图结构<citation id="389" type="reference"><link href="352" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>.为了方便,本文只关注树结构的关系,树结构的“从属”关系可归纳为:不可逆性、反自反性和传递性<citation id="390" type="reference"><link href="354" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>等.使用序对(<i>D</i>,∈)表示层次结构,其中,<i>D</i>为标签的集合,∈为从属关系,则3个特性的形式化描述如下:</p>
                </div>
                <div class="p1">
                    <p id="88">1)不 可 逆 性. 若 <i>d</i><sub><i>i</i></sub>∈<i>d</i><sub><i>j</i></sub>, ∀<i>d</i><sub><i>i</i></sub>∈<i>D</i>, <i>d</i><sub><i>j</i></sub>∈<i>D</i>, 则<i>d</i><sub><i>j</i></sub>≮<i>d</i><sub><i>i</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="90">2)反自反性.∀<i>d</i><sub><i>i</i></sub>∈<i>D</i>,有<i>d</i><sub><i>i</i></sub>≮<i>d</i><sub><i>i</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="91">3)传递性.若<i>d</i><sub><i>i</i></sub>∈<i>d</i><sub><i>k</i></sub>且<i>d</i><sub><i>k</i></sub>∈<i>d</i><sub><i>j</i></sub>,对∀<i>d</i><sub><i>i</i></sub>∈<i>D</i>,<i>d</i><sub><i>j</i></sub>∈<i>D</i>,<i>d</i><sub><i>k</i></sub>∈<i>D</i>,则<i>d</i><sub><i>i</i></sub>∈<i>d</i><sub><i>j</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="92">为了清楚地表达节点之间的常用关系,表1列出与类别<i>d</i><sub><i>i</i></sub>与<i>d</i><sub><i>k</i></sub>相关的语义信息.</p>
                </div>
                <div class="area_img" id="93">
                    <p class="img_tit"><b>表1 符号说明</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Description of symbols</p>
                    <p class="img_note"></p>
                    <table id="93" border="1"><tr><td><br />符号表示</td><td>表达意义</td></tr><tr><td><br /><i>Anc</i>(<i>d</i><sub><i>i</i></sub>)</td><td>类别<i>d</i><sub><i>i</i></sub>的所有祖先节点集合</td></tr><tr><td><br /><i>Des</i>(<i>d</i><sub><i>i</i></sub>)</td><td>类别<i>d</i><sub><i>i</i></sub>的所有子孙节点集合</td></tr><tr><td><br /><i>Sib</i>(<i>d</i><sub><i>i</i></sub>)</td><td>类别<i>d</i><sub><i>i</i></sub>的所有兄弟节点集合</td></tr><tr><td><br /><i>LCA</i>(<i>d</i><sub><i>i</i></sub>,<i>d</i><sub><i>k</i></sub>)</td><td>类别<i>d</i><sub><i>i</i></sub>和<i>d</i><sub><i>k</i></sub>的最近公共祖先节点集合</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="94" name="94"><b>1.2</b> 邻域粗糙集</h4>
                <div class="p1">
                    <p id="95">给定邻域决策系统NDS=&lt;U,C,D&gt;,其中U={x<sub>1</sub>,x<sub>2</sub>,…,x<sub>n</sub>}表示样本集合,C={a<sub>1</sub>,a<sub>2</sub>,…,a<sub>m</sub>}表示U的实数型特征集合,D表示决策属性.</p>
                </div>
                <div class="p1">
                    <p id="96"><b>定义1</b><citation id="391" type="reference"><link href="356" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation> 设U表示非空集合,对∀x<sub>i</sub>∈U,x<sub>j</sub>∈U,x<sub>k</sub>∈U,存在唯一确定的实函数Δ与之对应,Δ满足:</p>
                </div>
                <div class="p1">
                    <p id="97">1)Δ(x<sub>i</sub>,x<sub>j</sub>)≥0,当且仅当x<sub>i</sub>=x<sub>j</sub>,Δ(x<sub>i</sub>,x<sub>j</sub>)=0;</p>
                </div>
                <div class="p1">
                    <p id="98">2)Δ(x<sub>i</sub>,x<sub>j</sub>)=Δ(x<sub>j</sub>,x<sub>i</sub>);</p>
                </div>
                <div class="p1">
                    <p id="99">3)Δ(x<sub>i</sub>,x<sub>k</sub>)≤Δ(x<sub>i</sub>,x<sub>j</sub>)+Δ(x<sub>j</sub>,x<sub>k</sub>).</p>
                </div>
                <div class="p1">
                    <p id="100"><b>定义2</b><citation id="392" type="reference"><link href="356" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation><sup></sup> 设&lt;U,Δ&gt;表示非空度量空间,x∈U,δ≥0,称点集</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">{</mo><mi>y</mi><mo stretchy="false">|</mo><mi>Δ</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>≤</mo><mi>δ</mi><mo>,</mo><mi>y</mi><mo>∈</mo><mi>U</mi><mo stretchy="false">}</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">是以x为中心、δ为半径的闭球,又称为x的邻域.若Δ为欧氏距离,则样本x的邻域是以x为中心、δ为半径的超球体.</p>
                </div>
                <div class="p1">
                    <p id="103"><b>定义3</b><citation id="393" type="reference"><link href="356" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation><sup></sup> 给定NDS=&lt;U,C,D&gt;,D将U划分为N个邻域类:X<sub>1</sub>,X<sub>2</sub>,…,X<sub>N</sub>,B⊆C生成U上的邻域关系R<sub>B</sub>,决策D关于B的邻域下近似和邻域上近似分别为</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><munder accentunder="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></munder><mi>D</mi><mo>=</mo><mo stretchy="false">{</mo><munder accentunder="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></munder><mi>X</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><munder accentunder="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></munder><mi>X</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><munder accentunder="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></munder><mi>X</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">}</mo><mo>,</mo></mtd></mtr><mtr><mtd><mover accent="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></mover><mi>D</mi><mo>=</mo><mo stretchy="false">{</mo><mover accent="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></mover><mi>X</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mover accent="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></mover><mi>X</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mover accent="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></mover><mi>X</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">}</mo><mo>,</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105">其中</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><munder accentunder="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></munder><mi>X</mi><mo>=</mo><mo stretchy="false">{</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>δ</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>⊆</mo><mi>X</mi><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>U</mi><mo stretchy="false">}</mo><mo>,</mo></mtd></mtr><mtr><mtd><mover accent="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></mover><mi>X</mi><mo>=</mo><mo stretchy="false">{</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>δ</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mstyle displaystyle="true"><mo>∩</mo><mi>X</mi></mstyle><mo>≠</mo><mo>∅</mo><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>U</mi><mo stretchy="false">}</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107"><b>定义4</b><citation id="394" type="reference"><link href="356" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation><sup></sup> 给定NDS=&lt;U,C,D&gt;,决策属性D对于条件属性子集B⊆C的依赖度为</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>C</mi><mi>a</mi><mi>r</mi><mi>d</mi><mo stretchy="false">(</mo><munder accentunder="true"><mrow><mi>R</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mo stretchy="true">¯</mo></munder><mi>D</mi><mo stretchy="false">)</mo></mrow><mrow><mi>C</mi><mi>a</mi><mi>r</mi><mi>d</mi><mo stretchy="false">(</mo><mi>U</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109"><b>定义5</b><citation id="395" type="reference"><link href="356" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation><sup></sup> 给定NDS=&lt;U,C,D&gt;,B⊆C,∀a∉B,属性a的重要度为</p>
                </div>
                <div class="p1">
                    <p id="110">Sig(a,B,D)=γ<sub>B∪a</sub>(D)-γ<sub>B</sub>(D).</p>
                </div>
                <h3 id="111" name="111" class="anchor-tag">2 基于邻域粗糙集的层次分类在线流特征选择</h3>
                <h4 class="anchor-tag" id="112" name="112"><b>2.1</b> 层次分类的兄弟策略</h4>
                <div class="p1">
                    <p id="113">单标记特征选择算法未考虑不同类别之间的关系,而是直接使用排斥策略<citation id="396" type="reference"><link href="358" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,如表2中第1行所示,即同类为x,异类为非x.同时,在层次分类学习中,可利用父子关系或兄弟关系区分目标样本的同类和异类.基于父子关系的称为包含策略<citation id="397" type="reference"><link href="358" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,基于兄弟关系的称为兄弟策略<citation id="398" type="reference"><link href="360" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>,如表2中第2行与第3行所示.</p>
                </div>
                <div class="area_img" id="114">
                    <p class="img_tit"><b>表2 定义同类样本和异类样本的3种策略</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 Three strategies for defining positive and negative samples</p>
                    <p class="img_note"></p>
                    <table id="114" border="1"><tr><td><br />搜索策略</td><td>同类</td><td>异类</td></tr><tr><td><br />排斥策略</td><td>x</td><td>非x</td></tr><tr><td><br />包含策略</td><td>x+Des(x)</td><td>非[x+Des(x)]</td></tr><tr><td><br />兄弟策略</td><td>x</td><td>Sib(x)</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="115" name="115"><b>2.2</b> 面向层次结构化数据的邻域粗糙集模型</h4>
                <div class="p1">
                    <p id="116">从认知与决策角度出发,排斥策略与包含策略分别表示悲观与乐观的决策关系,兄弟策略表示中立决策,这较符合实际应用场景,能够避免极端化.本节采用兄弟策略构建面向层次结构化数据的邻域粗糙集模型.</p>
                </div>
                <div class="p1">
                    <p id="117"><b>定义6</b> 设&lt;U,Δ&gt;表示非空度量空间,样本集U的决策属性类别存在层次关系S.∀x∈U,称x的兄弟策略最大近邻点集为</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><msup><mrow></mrow><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">{</mo><mi>y</mi><mo stretchy="false">|</mo><mi>Δ</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>≤</mo><mi>d</mi><msup><mrow></mrow><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>,</mo><mi>y</mi><mo>∈</mo><mi>U</mi><mo stretchy="false">}</mo><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">其中</p>
                </div>
                <div class="p1">
                    <p id="120" class="code-formula">
                        <mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>d</mi><msup><mrow></mrow><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo stretchy="false">(</mo><mi>d</mi><msup><mrow></mrow><mspace width="0.25em" /></msup><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>,</mo><mi>d</mi><msup><mrow></mrow><mspace width="0.25em" /></msup><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mi>d</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Δ</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>Ν</mi><mi>Τ</mi><msup><mrow></mrow><mspace width="0.25em" /></msup><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mi>d</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Δ</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>Ν</mi><mi>Μ</mi><msup><mrow></mrow><mspace width="0.25em" /></msup><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="121">NM<sup><i>sib</i></sup>(x)表示利用兄弟策略在样本空间中找到与x最近的异类样本,NT<sup><i>sib</i></sup>(x)表示利用兄弟策略在样本空间中找到与x最近的同类样本.</p>
                </div>
                <div class="p1">
                    <p id="122"><b>定义7</b> 给定层次邻域决策系统HNDS=&lt;U,C,D,S&gt;,其中,U表示非空有限样本集合,C表示描述U的实数型特征集合,D表示决策属性,S表示决策属性D类别的层次关系.D将U划分为L个邻域类:Z<sub>1</sub>,Z<sub>2</sub>,…,Z<sub>L</sub>.A⊆C生成U上的兄弟策略邻域关系R<sup><i>sib</i></sup><sub>A</sub>,决策属性D关于A的兄弟策略邻域下近似和兄弟策略邻域上近似分别定义为</p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><munder accentunder="true"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup></mrow><mo stretchy="true">¯</mo></munder><mi>D</mi><mo>=</mo><mo stretchy="false">{</mo><munder accentunder="true"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup></mrow><mo stretchy="true">¯</mo></munder><mi>Ζ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><munder accentunder="true"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup></mrow><mo stretchy="true">¯</mo></munder><mi>Ζ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><munder accentunder="true"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup></mrow><mo stretchy="true">¯</mo></munder><mi>Ζ</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">}</mo><mo>,</mo></mtd></mtr><mtr><mtd><mover accent="true"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup></mrow><mo stretchy="true">¯</mo></mover><mi>D</mi><mo>=</mo><mo stretchy="false">{</mo><mover accent="true"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup></mrow><mo stretchy="true">¯</mo></mover><mi>Ζ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mover accent="true"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup></mrow><mo stretchy="true">¯</mo></mover><mi>Ζ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mover accent="true"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup></mrow><mo stretchy="true">¯</mo></mover><mi>Ζ</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">}</mo><mo>,</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="124">其中</p>
                </div>
                <div class="p1">
                    <p id="125" class="code-formula">
                        <mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><munder accentunder="true"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup></mrow><mo stretchy="true">¯</mo></munder><mi>Ζ</mi><mo>=</mo><mo stretchy="false">{</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>δ</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>⊆</mo><mi>Ζ</mi><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>U</mi><mo stretchy="false">}</mo><mo>,</mo></mtd></mtr><mtr><mtd><mover accent="true"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup></mrow><mo stretchy="true">¯</mo></mover><mi>Ζ</mi><mo>=</mo><mo stretchy="false">{</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>δ</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mstyle displaystyle="true"><mo>∩</mo><mi>Ζ</mi></mstyle><mo>≠</mo><mo>∅</mo><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>U</mi><mo stretchy="false">}</mo><mo>,</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="126">δ<sup><i>sib</i></sup><sub><sup>A</sup></sub>(x)为利用兄弟策略在属性空间A、度量Δ下,以x为中心的最大近邻信息粒子.</p>
                </div>
                <div class="p1">
                    <p id="127"><b>定义8</b> 给定&lt;U,Δ&gt;非空度量空间,样本集U的决策属性类别存在层次关系S,样本x∈U,则x的兄弟策略分类间隔定义为</p>
                </div>
                <div class="p1">
                    <p id="128">m<sup><i>sib</i></sup>(x)=d<sup><i>sib</i></sup><sub>2</sub>(x)-d<sup><i>sib</i></sup><sub>1</sub>(x).</p>
                </div>
                <div class="p1">
                    <p id="129"><b>定理1</b> 给定HNDS=&lt;U,C,D,S&gt;,Δ为U上的度量,A⊆C,决策属性D关于A的兄弟策略邻域下近似为</p>
                </div>
                <div class="p1">
                    <p id="130"><mathml id="257"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup></mrow><mo stretchy="true">¯</mo></munder><mi>D</mi><mo>=</mo><mo stretchy="false">{</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>m</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>&gt;</mo><mn>0</mn><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>U</mi><mo stretchy="false">}</mo></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="131"><b>证明</b> 若m<sup><i>sib</i></sup><sub>A</sub>(x<sub>j</sub>)&gt;0,则有</p>
                </div>
                <div class="p1">
                    <p id="132">d<sup><i>sib</i></sup><sub>2</sub>(x<sub>j</sub>)&gt;d<sup><i>sib</i></sup><sub>1</sub>(x<sub>j</sub>),</p>
                </div>
                <div class="p1">
                    <p id="133">于是Δ(x<sub>j</sub>,y)&lt;d<sup><i>sib</i></sup><sub>2</sub>(x<sub>j</sub>),所以对于决策类D,存在δ<sup><i>sib</i></sup><sub>A</sub>(x<sub>j</sub>)⊆D,所以<mathml id="258"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><munder accentunder="true"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup></mrow><mo stretchy="true">¯</mo></munder><mi>D</mi></mrow></math></mathml>成立.</p>
                </div>
                <div class="p1">
                    <p id="134">反之,若<mathml id="259"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><munder accentunder="true"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup></mrow><mo stretchy="true">¯</mo></munder><mi>D</mi></mrow></math></mathml>,则在决策类D上有δ<sup><i>sib</i></sup><sub>A</sub>(x<sub>j</sub>)⊆D,即d<sup><i>sib</i></sup><sub>2</sub>(x<sub>j</sub>)&gt;d<sup><i>sib</i></sup><sub>1</sub>(x<sub>j</sub>),从而m<sup><i>sib</i></sup><sub>A</sub>(x<sub>j</sub>)&gt;0成立.证毕.</p>
                </div>
                <div class="p1">
                    <p id="135"><b>定义9</b> 给定HNDS=&lt;U,C,D,S&gt;,决策属性D对条件属性子集A⊆C的兄弟策略依赖度为</p>
                </div>
                <div class="p1">
                    <p id="136"><mathml id="260"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>C</mi><mi>a</mi><mi>r</mi><mi>d</mi><mo stretchy="false">(</mo><munder accentunder="true"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>A</mi><mrow><mtext>s</mtext><mtext>i</mtext><mtext>b</mtext></mrow></msubsup></mrow><mo stretchy="true">¯</mo></munder><mi>D</mi><mo stretchy="false">)</mo></mrow><mrow><mi>C</mi><mi>a</mi><mi>r</mi><mi>d</mi><mo stretchy="false">(</mo><mi>U</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></math></mathml>.      (1)</p>
                </div>
                <div class="p1">
                    <p id="137"><b>定义10</b> 给定HNDS=&lt;U,C,D,S&gt;,A⊆C为已选的特征子集,∀a∉A,属性a的兄弟策略重要度为</p>
                </div>
                <div class="p1">
                    <p id="138">HSig(a,A,D)=γ<sup><i>sib</i></sup><sub>A∪a</sub>(D)-γ<sup><i>sib</i></sup><sub>A</sub>(D).</p>
                </div>
                <h4 class="anchor-tag" id="139" name="139"><b>2.3 基于邻域粗糙集的层次分类在线流特征选择算法模型</b></h4>
                <div class="p1">
                    <p id="140">本节针对层次化结构数据的分类学习中在线流特征选择问题,提出基于面向层次结构化数据的邻域粗糙集模型的两种在线特征评估准则.</p>
                </div>
                <h4 class="anchor-tag" id="141" name="141"><b>2.3.1 在线重要性选择</b></h4>
                <div class="p1">
                    <p id="142"><b>定义11</b> 给定在线流特征的层次邻域决策系统HDST=&lt;U,C,D,S,T&gt;,其中,U表示非空有限样本集合,C表示U的实数型特征集合,D表示决策属性,S表示决策属性D类别的层次关系,T表示时间序列.R<sub>t-1</sub>表示t-1时刻已选的特征子集, f<sub>t</sub>表示t时刻新到的特征,则f<sub>t</sub>相对于D的兄弟策略重要度为</p>
                </div>
                <div class="p1">
                    <p id="143">HSig<sub>t</sub>(f<sub>t</sub>,R<sub>t-1</sub>,D)=γ<sup><i>sib</i></sup><sub>Rt-1∪ft</sub>(D)-γ<sup><i>sib</i></sup><sub>Rt-1</sub>(D),      (2)</p>
                </div>
                <div class="p1">
                    <p id="144">因为</p>
                </div>
                <div class="p1">
                    <p id="145">γ<sup><i>sib</i></sup><sub>Rt-1∪ft</sub>(D)∈[0,1], γ<sup><i>sib</i></sup><sub>Rt-1∪ft</sub>(D)≥γ<sup><i>sib</i></sup><sub>Rt-1</sub>(D),</p>
                </div>
                <div class="p1">
                    <p id="146">显然HSig<sub>t</sub>(f<sub>t</sub>,R<sub>t-1</sub>,D)∈[0,1].HSig<sub>t</sub>(f<sub>t</sub>,R<sub>t-1</sub>,D)越大,所选特征就越重要.当HSig<sub>t</sub>(f<sub>t</sub>,R<sub>t-1</sub>,D)=0时,特征子集R<sub>t-1</sub>中存在与f<sub>t</sub>相互冗余的特征;否则f<sub>t</sub>对于已选特征子集R<sub>t-1</sub>是有意义的非冗余特征.</p>
                </div>
                <h4 class="anchor-tag" id="147" name="147"><b>2.3.2 在线冗余更新</b></h4>
                <div class="p1">
                    <p id="148">依据定义11,HSig<sub>t</sub>(f<sub>t</sub>,R<sub>t-1</sub>,D)=0作为识别方法调用在线冗余判断与更新.通过修改定义11得到定义12,可建立评价标准删除与更新特征子集.</p>
                </div>
                <div class="p1">
                    <p id="149"><b>定义12</b> 给定在线流特征的层次邻域决策系统HDST=&lt;U,C,D,S,T&gt;,其中,U表示非空有限样本集合,C表示U的实数型特征集合,D表示决策属性,S表示决策属性D类别的层次关系,T表示时间序列.R<sub>t</sub>表示t时刻已选的特征子集, f⊆R<sub>t</sub>, f相对于决策属性D的兄弟策略冗余重要度为</p>
                </div>
                <div class="p1">
                    <p id="150">HRSig<sub>t</sub>(f,R<sub>t</sub>,D)=γ<sup><i>sib</i></sup><sub>Rt</sub>(D)-γ<sup><i>sib</i></sup><sub>{Rt-f}</sub>(D).      (3)</p>
                </div>
                <div class="p1">
                    <p id="151">经过在线冗余判断与更新,可以删除冗余特征,从而保持一个较优的特征子集.并通过与在线重要性选择结合,得到基于邻域粗糙集的层次分类在线流特征选择模型框架.</p>
                </div>
                <h4 class="anchor-tag" id="152" name="152"><b>2.3.3 基于邻域粗糙集的层次分类在线流特征选择算法</b></h4>
                <div class="p1">
                    <p id="153">根据定义11与定义12,提出基于邻域粗糙集的层次分类在线流特征选择算法,算法步骤如下.</p>
                </div>
                <div class="area_img" id="317">
                                <img alt="" src="Detail/GetImg?filename=images/MSSB201909006_31700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="178">算法主要包含两个阶段:在线重要性选择和在线冗余更新.在在线重要性选择阶段中,当新特征f<sub>t</sub>到达时,执行第3步计算特征依赖度γ<sup><i>sib</i></sup><sub>ft</sub>(D).当γ<sup><i>sib</i></sup><sub>ft</sub>(D)&lt;δ时,直接删除特征f<sub>t</sub>;否则执行第5步计算f<sub>t</sub>的相对已选特征子集R<sub>t-1</sub>的HSig<sub>t</sub>(f<sub>t</sub>,R<sub>t-1</sub>,D).当HSig<sub>t</sub>(f<sub>t</sub>,R<sub>t-1</sub>,D)&gt;0时,执行第7步,将特征加入特征子集中,否则执行第9～16步.在在线冗余更新阶段中,先将特征加入特征子集中,得到R<sub>t</sub>.然后对R<sub>t</sub>中每个特征执行第11～15步.在第11步时随机从中选取一个特征f,在第12步中通过式(3)计算冗余度HRSig<sub>t</sub>(f,R<sub>t</sub>,D).当HRSig<sub>t</sub>(f,R<sub>t</sub>,D)=0时,可删除特征f.</p>
                </div>
                <div class="p1">
                    <p id="179">假设<mathml id="261"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>U</mi><mo stretchy="false">|</mo></mrow></math></mathml>为论域U的样本个数,<mathml id="262"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo></mrow></math></mathml>为条件属性C的属性个数,计算重要度HSig<sub>t</sub>的时间复杂度为<mathml id="263"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false">(</mo><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo><mrow><mi>lg</mi></mrow><mo stretchy="false">|</mo><mi>U</mi><mo stretchy="false">|</mo><mo stretchy="false">)</mo></mrow></math></mathml>.在在线重要性选择阶段,遍历所有样本所需时间复杂度为<mathml id="264"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false">(</mo><mo stretchy="false">|</mo><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>U</mi><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mrow><mi>lg</mi></mrow><mo stretchy="false">|</mo><mi>U</mi><mo stretchy="false">|</mo><mo stretchy="false">)</mo></mrow></math></mathml>.当执行在线冗余更新阶段时,若当前所选特征子集数量为<mathml id="265"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo></mrow></math></mathml>,<i>OHFS</i>的最差时间复杂度为</p>
                </div>
                <div class="p1">
                    <p id="180"><mathml id="266"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false">(</mo><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>U</mi><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mrow><mi>lg</mi></mrow><mo stretchy="false">|</mo><mi>U</mi><mo stretchy="false">|</mo><mo stretchy="false">)</mo></mrow></math></mathml>.</p>
                </div>
                <h3 id="181" name="181" class="anchor-tag">3 实验及结果分析</h3>
                <h4 class="anchor-tag" id="182" name="182"><b>3.1 实验数据</b></h4>
                <div class="p1">
                    <p id="183">本节选取6个类别包含层次结构的蛋白质数据集(<i>F</i>194,<i>DD</i>)和图像数据集(<i>AWA</i>,<i>Bridges</i>,<i>Cifar</i>,<i>VOC</i>)用于验证<i>OHFS</i>性能<citation id="399" type="reference"><link href="362" rel="bibliography" /><link href="364" rel="bibliography" /><link href="366" rel="bibliography" /><link href="368" rel="bibliography" /><link href="370" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>,<a class="sup">23</a>,<a class="sup">24</a>,<a class="sup">25</a>]</sup></citation>.表3给出数据集的相关描述信息.</p>
                </div>
                <div class="area_img" id="184">
                    <p class="img_tit"><b>表3 实验数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Table</i> 3 <i>Experimental datasets</i></p>
                    <p class="img_note"></p>
                    <table id="184" border="1"><tr><td>名称</td><td>样本数</td><td>特征数</td><td>内部节点</td><td>叶子节点</td><td>树高</td></tr><tr><td><br /><i>AWA</i></td><td>6405</td><td>252</td><td>17</td><td>10</td><td>4</td></tr><tr><td><br /><i>Bridges</i></td><td>108</td><td>12</td><td>8</td><td>6</td><td>3</td></tr><tr><td><br /><i>Cifar</i></td><td>50000</td><td>512</td><td>121</td><td>100</td><td>3</td></tr><tr><td><br /><i>DD</i></td><td>3625</td><td>473</td><td>32</td><td>27</td><td>3</td></tr><tr><td><br /><i>F</i>194</td><td>8525</td><td>473</td><td>202</td><td>194</td><td>3</td></tr><tr><td><br /><i>VOC</i></td><td>7178</td><td>1000</td><td>30</td><td>88</td><td>5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="185" name="185"><b>3.2 实验设置</b></h4>
                <div class="p1">
                    <p id="186">为了评价层次在线流特征选择效果的优劣,除了采用传统的预测精度,针对分层结构中错分程度的描述,额外引入两种层次分类评价指标:树诱导损失(<i>Tree Induced Error</i>, <i>TIE</i>)<citation id="400" type="reference"><link href="372" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>,最低共同祖先<i>F</i><sub>1</sub>(<i>Lowest Common Ancestor</i>-<i>F</i>1, <i>LCA</i>-<i>F</i>1)<citation id="401" type="reference"><link href="376" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>,用于评价算法性能.</p>
                </div>
                <div class="p1">
                    <p id="187">令D表示样本标记,<mathml id="267"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover></mrow></math></mathml>表示预测标记,它们层次分类扩展标记分别表示为</p>
                </div>
                <div class="p1">
                    <p id="188" class="code-formula">
                        <mathml id="188"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>u</mtext><mtext>g</mtext></mrow></msub><mo>=</mo><mi>D</mi><mstyle displaystyle="true"><mo>∪</mo><mi>A</mi></mstyle><mi>n</mi><mi>c</mi><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>,</mo><mspace width="0.25em" /><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>u</mtext><mtext>g</mtext></mrow></msub><mo>=</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><mstyle displaystyle="true"><mo>∪</mo><mi>A</mi></mstyle><mi>n</mi><mi>c</mi><mo stretchy="false">(</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><mo stretchy="false">)</mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="189">最小公共祖先节点层次分类扩展标记表示为</p>
                </div>
                <div class="p1">
                    <p id="190" class="code-formula">
                        <mathml id="190"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>u</mtext><mtext>g</mtext></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>A</mtext></mrow></msubsup><mo>=</mo><mi>D</mi><mstyle displaystyle="true"><mo>∪</mo><mi>L</mi></mstyle><mi>C</mi><mi>A</mi><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><mo stretchy="false">)</mo><mo>,</mo><mspace width="0.25em" /><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>u</mtext><mtext>g</mtext></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>A</mtext></mrow></msubsup><mo>=</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><mstyle displaystyle="true"><mo>∪</mo><mi>L</mi></mstyle><mi>C</mi><mi>A</mi><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><mo stretchy="false">)</mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="191"><i>TIE</i>表示样本的预测标记<mathml id="268"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover></mrow></math></mathml>和样本标记D在层次结构中节点之间的总边数:</p>
                </div>
                <div class="p1">
                    <p id="192" class="code-formula">
                        <mathml id="192"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mi>Ι</mi><mi>E</mi><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>E</mi></munder><mo stretchy="false">(</mo></mstyle><mi>D</mi><mo>,</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><mo stretchy="false">)</mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="193">为了方便观察,实验统一取平均值</p>
                </div>
                <div class="area_img" id="194">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201909006_19400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="195">表示样本的预测标记<mathml id="269"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover></mrow></math></mathml>和样本标记<i>D</i>的最小公共祖先:</p>
                </div>
                <div class="p1">
                    <p id="196" class="code-formula">
                        <mathml id="196"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mi>C</mi><mi>A</mi><mo>-</mo><mi>F</mi><mn>1</mn><mo>=</mo><mfrac><mrow><mn>2</mn><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>A</mtext><mtext>Η</mtext></mrow></msub><mi>R</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>A</mtext><mtext>Η</mtext></mrow></msub></mrow><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>A</mtext><mtext>Η</mtext></mrow></msub><mo>+</mo><mi>R</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>A</mtext><mtext>Η</mtext></mrow></msub></mrow></mfrac><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="197">其中</p>
                </div>
                <div class="p1">
                    <p id="198" class="code-formula">
                        <mathml id="198"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>A</mtext><mtext>Η</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">|</mo><mi>D</mi><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>u</mtext><mtext>g</mtext></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>A</mtext></mrow></msubsup><mstyle displaystyle="true"><mo>∩</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover></mrow></mstyle><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>u</mtext><mtext>g</mtext></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>A</mtext></mrow></msubsup><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>u</mtext><mtext>g</mtext></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>A</mtext></mrow></msubsup><mo stretchy="false">|</mo></mrow></mfrac><mspace width="0.25em" /><mo>,</mo></mtd></mtr><mtr><mtd><mi>R</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>A</mtext><mtext>Η</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">|</mo><mi>D</mi><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>u</mtext><mtext>g</mtext></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>A</mtext></mrow></msubsup><mstyle displaystyle="true"><mo>∩</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>D</mi></mstyle><mo>︿</mo></mover></mrow></mstyle><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>u</mtext><mtext>g</mtext></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>A</mtext></mrow></msubsup><mo stretchy="false">|</mo></mrow><mrow><mrow><mo>|</mo><mrow><mi>D</mi><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>u</mtext><mtext>g</mtext></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>A</mtext></mrow></msubsup></mrow><mo>|</mo></mrow></mrow></mfrac><mspace width="0.25em" /><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="199">对于这2个性能评价指标,TIE指标取值越小越好,而LCA-F1取值越大越好.</p>
                </div>
                <div class="p1">
                    <p id="200">为了有效评价OHFS的性能,选择5种不同类型在线流特征选择算法作为对比算法:在线流特征选择(Online Streaming Feature Selection, OSFS)<citation id="402" type="reference"><link href="340" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,快速在线特征选择(Faster Online Streaming Feature Selection, FOSFS)<citation id="403" type="reference"><link href="340" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,一种新的在线特征选择方法(A Novel Online Streaming Feature Selection Method, OFSD)<citation id="404" type="reference"><link href="346" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>,可扩展和准确的在线特征选择算法(Scalable and Accurate Online Selection Approach, SAOLA)<citation id="405" type="reference"><link href="376" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>,一种基于邻域粗糙集的在线特征选择方法(A New Online Feature Selection Method Using Neighborhood Rough Set, A3M)<sup><a class="sup">[29]</a></sup>.</p>
                </div>
                <h4 class="anchor-tag" id="201" name="201"><b>3.3 参数</b><i>δ</i><b>分析</b></h4>
                <div class="p1">
                    <p id="202">为了分析属性依赖度<i>δ</i>取值对OHFS的影响,本节选择<i>δ</i>=0,0.03,0.06,0.09,OHFS针对在3个数据集上筛选的特征子集的3个评价指标与特征选择所需时间的性能表现如图1和图2所示.</p>
                </div>
                <div class="area_img" id="318">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909006_31800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 δ不同时OHFS在LSVM分类器上的性能对比" src="Detail/GetImg?filename=images/MSSB201909006_31800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 δ不同时OHFS在LSVM分类器上的性能对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909006_31800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Performance comparison of OHFS using LSVMclassifier with differentδvalues</p>

                </div>
                <div class="area_img" id="318">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909006_31801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 δ不同时OHFS在LSVM分类器上的性能对比" src="Detail/GetImg?filename=images/MSSB201909006_31801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 δ不同时OHFS在LSVM分类器上的性能对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909006_31801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Performance comparison of OHFS using LSVMclassifier with differentδvalues</p>

                </div>
                <div class="area_img" id="319">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909006_31900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 δ不同时OHFS在KNN分类器上的性能对比" src="Detail/GetImg?filename=images/MSSB201909006_31900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 δ不同时OHFS在KNN分类器上的性能对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909006_31900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Performance comparison of OHFS using KNN classifier with differentδvalues</p>

                </div>
                <div class="area_img" id="319">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909006_31901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 δ不同时OHFS在KNN分类器上的性能对比" src="Detail/GetImg?filename=images/MSSB201909006_31901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 δ不同时OHFS在KNN分类器上的性能对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909006_31901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Performance comparison of OHFS using KNN classifier with differentδvalues</p>

                </div>
                <div class="p1">
                    <p id="221">当使用LSVM分类器时,由图1可看出,在预测精度和LCA-F1指标上,<i>δ</i>=0.03时OHFS在所有数据集上的表现都高于<i>δ</i>=0.06,0.09时的情况,低于<i>δ</i>=0时的情况.在TIE评价指标中,<i>δ</i>=0.03时OHFS同样在所有数据集上的表现都低于<i>δ</i>=0.06,0.09的情况,相对高于<i>δ</i>=0时的情况.在特征选择所需运行时间上,<i>δ</i>=0.03时OHFS在大部分数据集上明显低于<i>δ</i>=0时的情况,相对高于<i>δ</i>=0.06,0.09时的情况.</p>
                </div>
                <div class="p1">
                    <p id="222">当使用KNN(<i>k</i>=10)分类器时,由图2可看出,在预测精度和LCA-F1指标上,<i>δ</i>=0.03时,OHFS在所有数据集上的表现最高.在TIE评价指标上,<i>δ</i>=0.03时,OHFS同样在所有数据集上的表现最低.在特征选择所需运行时间上,全部低于<i>δ</i>=0时的情况,相对高于<i>δ</i>=0.06,0.09时的情况.</p>
                </div>
                <div class="p1">
                    <p id="223">从上述分析结果可知,综合4个性能评价指标和运行时间,在考虑不同分类器的情况下,OHFS在<i>δ</i>=0.03时,表现最佳.所以在下面实验中,属性依赖度 <i>δ</i>=0.03.</p>
                </div>
                <h4 class="anchor-tag" id="224" name="224"><b>3.4 有效性分析</b></h4>
                <div class="p1">
                    <p id="225">为了验证OHFS的有效性,对比各种特征选择算法在传统分类评价指标和三种层次分类评价指标上的效果,进一步使用盒形图直观分析各算法在3个分类评价指标上的性能.基分类器采用KNN和LSVM.</p>
                </div>
                <div class="p1">
                    <p id="226">不同特征选择算法在3个评价指标上的LSVM和KNN分类结果见表4～表9,黑体数字表示不同评价指标中算法取得最优效果.</p>
                </div>
                <div class="area_img" id="227">
                    <p class="img_tit"><b>表4 各算法在LSVM分类器上的分类精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Predictive accuracy of algorithms with LSVM classifier</p>
                    <p class="img_note"></p>
                    <table id="227" border="1"><tr><td><br />数据集</td><td>OHFS</td><td>OSFS</td><td>FOSFS</td><td>SAOLA</td><td>A3M</td><td>OFSD</td></tr><tr><td><br />AWA</td><td><b>0.3188</b></td><td>0.2080</td><td>0.2195</td><td>0.1781</td><td>0.2512</td><td>0.1867</td></tr><tr><td><br />Bridges</td><td>0.6000</td><td><b>0.6300</b></td><td>0.6300</td><td>0.6300</td><td>0.6256</td><td>0.5644</td></tr><tr><td><br />Cifar</td><td>0.2341</td><td>0.0674</td><td>0.0747</td><td>0.0201</td><td><b>0.2710</b></td><td>0.1289</td></tr><tr><td><br />DD</td><td><b>0.7118</b></td><td>0.3704</td><td>0.3707</td><td>0.2929</td><td>0.7054</td><td>0.3079</td></tr><tr><td><br />F194</td><td><b>0.4960</b></td><td>0.2197</td><td>0.2537</td><td>0.2252</td><td>0.0879</td><td>0.1010</td></tr><tr><td><br />VOC</td><td><b>0.3608</b></td><td>0.2594</td><td>0.2671</td><td>0.2568</td><td>0.3165</td><td>0.2898</td></tr><tr><td><br />平均值</td><td><b>0.4536</b></td><td>0.2925</td><td>0.3026</td><td>0.2672</td><td>0.3763</td><td>0.2631</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="228">
                    <p class="img_tit"><b>表5 各算法在LSVM分类器上的LCA-F1值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 LCA-F1 score of algorithms with LSVM classifier</p>
                    <p class="img_note"></p>
                    <table id="228" border="1"><tr><td><br />数据集</td><td>OHFS</td><td>OSFS</td><td>FOSFS</td><td>SAOLA</td><td>A3M</td><td>OFSD</td></tr><tr><td><br />AWA</td><td><b>0.5393</b></td><td>0.4643</td><td>0.4712</td><td>0.4484</td><td>0.4948</td><td>0.4482</td></tr><tr><td><br />Bridges</td><td>0.7704</td><td>0.7852</td><td>0.7852</td><td>0.7852</td><td><b>0.7861</b></td><td>0.7528</td></tr><tr><td><br />Cifar</td><td>0.5070</td><td>0.3908</td><td>0.3967</td><td>0.3559</td><td><b>0.5328</b></td><td>0.4352</td></tr><tr><td><br />DD</td><td><b>0.8396</b></td><td>0.6338</td><td>0.6339</td><td>0.5786</td><td>0.8370</td><td>0.5743</td></tr><tr><td><br />F194</td><td><b>0.7171</b></td><td>0.5553</td><td>0.5809</td><td>0.5635</td><td>0.4517</td><td>0.4526</td></tr><tr><td><br />VOC</td><td><b>0.5898</b></td><td>0.5184</td><td>0.5236</td><td>0.5165</td><td>0.5605</td><td>0.5411</td></tr><tr><td><br />平均值</td><td><b>0.6605</b></td><td>0.5580</td><td>0.5652</td><td>0.5414</td><td>0.6105</td><td>0.5340</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="229">
                    <p class="img_tit"><b>表6 各算法在LSVM分类器上的TIE值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 6 TIE score of algorithms with LSVM classifier</p>
                    <p class="img_note"></p>
                    <table id="229" border="1"><tr><td><br />数据集</td><td>OHFS</td><td>OSFS</td><td>FOSFS</td><td>SAOLA</td><td>A3M</td><td>OFSD</td></tr><tr><td><br />AWA</td><td><b>3.1641</b></td><td>3.6868</td><td>3.6434</td><td>3.7627</td><td>3.4626</td><td>3.8151</td></tr><tr><td><br />Bridges</td><td>1.0741</td><td><b>1.0093</b></td><td>1.0093</td><td>1.0093</td><td>1.0000</td><td>1.1389</td></tr><tr><td><br />Cifar</td><td>2.8528</td><td>3.5803</td><td>3.5390</td><td>3.8094</td><td><b>2.6907</b></td><td>3.2936</td></tr><tr><td><br />DD</td><td><b>0.7719</b></td><td>1.8759</td><td>1.8759</td><td>2.2284</td><td>0.7779</td><td>2.3399</td></tr><tr><td><br />F194</td><td><b>1.3752</b></td><td>2.2133</td><td>2.0418</td><td>2.1370</td><td>2.9300</td><td>2.9713</td></tr><tr><td><br />VOC</td><td><b>2.3906</b></td><td>2.8175</td><td>2.7877</td><td>2.8268</td><td>2.5632</td><td>2.6800</td></tr><tr><td><br />平均值</td><td><b>1.9381</b></td><td>2.5305</td><td>2.4828</td><td>2.6289</td><td>2.2374</td><td>2.7065<br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="230">
                    <p class="img_tit"><b>表7 各算法在KNN分类器上的分类精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 7 Predictive accuracy of algorithms with KNN classifier</p>
                    <p class="img_note"></p>
                    <table id="230" border="1"><tr><td><br />数据集</td><td>OHFS</td><td>OSFS</td><td>FOSFS</td><td>SAOLA</td><td>A3M</td><td>OFSD</td></tr><tr><td><br />AWA</td><td>0.1925</td><td><b>0.1991</b></td><td>0.1939</td><td>0.1717</td><td>0.2190</td><td>0.1802</td></tr><tr><td><br />Bridges</td><td><b>0.6300</b></td><td>0.6300</td><td>0.4178</td><td>0.6300</td><td>0.5489</td><td>0.5078</td></tr><tr><td><br />Cifar</td><td>0.2386</td><td>0.0673</td><td>0.0754</td><td>0.0151</td><td><b>0.2585</b></td><td>0.1491</td></tr><tr><td><br />DD</td><td>0.6242</td><td>0.4058</td><td>0.4058</td><td>0.3172</td><td><b>0.6487</b></td><td>0.5584</td></tr><tr><td><br />F194</td><td><b>0.4693</b></td><td>0.2429</td><td>0.2893</td><td>0.2596</td><td>0.1919</td><td>0.2217</td></tr><tr><td><br />VOC</td><td><b>0.2841</b></td><td>0.1923</td><td>0.2026</td><td>0.1768</td><td>0.2537</td><td>0.2239</td></tr><tr><td><br />平均值</td><td><b>0.4064</b></td><td>0.2896</td><td>0.2641</td><td>0.2617</td><td>0.3535</td><td>0.3068</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="231">
                    <p class="img_tit"><b>表8 各算法在KNN分类器上的LCA-F1值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 8 LCA-F1 score of algorithms with KNN classifier</p>
                    <p class="img_note"></p>
                    <table id="231" border="1"><tr><td><br />数据集</td><td>OHFS</td><td>OSFS</td><td>FOSFS</td><td>SAOLA</td><td>A3M</td><td>OFSD</td></tr><tr><td><br />AWA</td><td>0.4491</td><td><b>0.4525</b></td><td>0.4520</td><td>0.4348</td><td>0.4679</td><td>0.4439</td></tr><tr><td><br />Bridges</td><td><b>0.7880</b></td><td>0.7852</td><td>0.6741</td><td>0.7852</td><td>0.7407</td><td>0.7157</td></tr><tr><td><br />Cifar</td><td>0.5091</td><td>0.3899</td><td>0.3961</td><td>0.3514</td><td><b>0.5234</b></td><td>0.4474</td></tr><tr><td><br />DD</td><td>0.7924</td><td>0.6598</td><td>0.6598</td><td>0.5969</td><td><b>0.8060</b></td><td>0.7499</td></tr><tr><td><br />F194</td><td><b>0.6981</b></td><td>0.5664</td><td>0.6005</td><td>0.5830</td><td>0.5346</td><td>0.5630</td></tr><tr><td><br />VOC</td><td><b>0.5376</b></td><td>0.4714</td><td>0.4793</td><td>0.4607</td><td>0.5164</td><td>0.4974</td></tr><tr><td><br />平均值</td><td><b>0.6290</b></td><td>0.5542</td><td>0.5436</td><td>0.5353</td><td>0.5982</td><td>0.5696</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="232">
                    <p class="img_tit"><b>表9 各算法在KNN分类器上的TIE值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 9 TIE score of algorithms with KNN classifier</p>
                    <p class="img_note"></p>
                    <table id="232" border="1"><tr><td><br />数据集</td><td>OHFS</td><td>OSFS</td><td>FOSFS</td><td>SAOLA</td><td>A3M</td><td>OFSD</td></tr><tr><td><br />AWA</td><td>3.8373</td><td>3.8258</td><td><b>3.7955</b></td><td>3.9326</td><td>3.6971</td><td>3.8329</td></tr><tr><td><br />Bridges</td><td><b>0.9815</b></td><td>1.0093</td><td>1.4815</td><td>1.0093</td><td>1.2037</td><td>1.3148</td></tr><tr><td><br />Cifar</td><td>2.8453</td><td>3.5906</td><td>3.5485</td><td>3.8438</td><td><b>2.7536</b></td><td>3.2277</td></tr><tr><td><br />DD</td><td>0.9881</td><td>1.7054</td><td>1.7054</td><td>2.1065</td><td><b>0.9230</b></td><td>1.2342</td></tr><tr><td><br />F194</td><td><b>1.4977</b></td><td>2.1752</td><td>1.9510</td><td>2.0420</td><td>2.3510</td><td>2.1309</td></tr><tr><td><br />VOC</td><td><b>2.7354</b></td><td>3.2176</td><td>3.1549</td><td>3.2923</td><td>2.8846</td><td>2.9958</td></tr><tr><td><br />平均值</td><td><b>2.1476</b></td><td>2.5873</td><td>2.6061</td><td>2.7044</td><td>2.3022</td><td>2.4561</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="233">根据表4～表9的结果可得如下结论.</p>
                </div>
                <div class="p1">
                    <p id="234">1)在总体上,OHFS在6个数据集上3个评价指标的平均性能都排第一.在3个评价指标上,至少在一半以上的数据集上的性能最优,另外的数据集上也达到次优水平.所以从总体性能上看,OHFS在各个评价指标上性能更优.</p>
                </div>
                <div class="p1">
                    <p id="235">2)OHFS在LSVM、KNN两个分类器中,在F194、VOC数据集上的分类性能在所有指标上都最优,而在其它数据集上的分类性能与最优值相差不大.在每个数据集上的分类性能都较稳定.</p>
                </div>
                <div class="p1">
                    <p id="236">为了更直观地对比6种算法在不同分类器及评价指标上的分类性能的差异, 采 用 盒 形图对实验结果进行分析,结果见图3和图4.由图3、图4的结果可得如下结论.</p>
                </div>
                <div class="p1">
                    <p id="237">1)在总体上,OHFS在6个数据集上的3个评价指标中的中位数(平均性能)都排在第一.</p>
                </div>
                <div class="p1">
                    <p id="238">2)OHFS在LSVM、KNN两个分类器中,虽然上四分位数与下四分位数的分布较宽松,但大都集中分布在较高的分类性能区域.</p>
                </div>
                <div class="p1">
                    <p id="239">由此可得OHFS分类精度优于其它算法,更稳定.</p>
                </div>
                <div class="area_img" id="320">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909006_32000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 各算法在LSVM分类器上的盒形图对比" src="Detail/GetImg?filename=images/MSSB201909006_32000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 各算法在LSVM分类器上的盒形图对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909006_32000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Box plot comparison of algorithms with LSVM classifier</p>

                </div>
                <div class="area_img" id="321">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909006_32100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 各算法在KNN分类器上的盒形图对比" src="Detail/GetImg?filename=images/MSSB201909006_32100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 各算法在KNN分类器上的盒形图对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909006_32100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Box plot comparison of algorithms with KNN classifier</p>

                </div>
                <h4 class="anchor-tag" id="252" name="252"><b>3.5 时间复杂度分析</b></h4>
                <div class="p1">
                    <p id="253">为了分析各算法在时间性能上的表现,本节对比各算法(OSFS、FOSFS、SAOLA、OFSD、A3M)的时间复杂度.OSFS最坏的时间复杂度为<mathml id="270"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false">(</mo><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi>k</mi><msup><mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>|</mo></mrow></mrow></msup><mo stretchy="false">)</mo></mrow></math></mathml>,其中,<mathml id="271"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo></mrow></math></mathml>表示目前阶段已到达的特征总数,<mathml id="272"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo></mrow></math></mathml>表示<i>t</i>时刻已选的特征总数,<i>k</i>表示条件子集<mathml id="273"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo></mrow></math></mathml>最多可包含的特征总数<mathml id="274"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>≤</mo><mi>k</mi><mo>≤</mo><mo stretchy="false">|</mo><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mo stretchy="false">)</mo><mo>,</mo><mi>k</mi><msup><mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>|</mo></mrow></mrow></msup></mrow></math></mathml>表示<mathml id="275"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo></mrow></math></mathml>需要测试的特征总数. FOSFS为OSFS的改进,最坏的时间复杂度改进到OSFS的平均时间复杂度<mathml id="276"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false">(</mo><mo stretchy="false">|</mo><mi>R</mi><mi>C</mi><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi>k</mi><msup><mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>|</mo></mrow></mrow></msup><mo stretchy="false">)</mo></mrow></math></mathml>,其中<mathml id="277"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>S</mi><mi>R</mi><mo stretchy="false">|</mo></mrow></math></mathml>表示<i>R</i>中相关的特征总数. SAOLA最坏的时间复杂度为<mathml id="278"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false">(</mo><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mo stretchy="false">)</mo></mrow></math></mathml>. OFSD与A3M最坏的时间复杂度都为<mathml id="279"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false">(</mo><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>R</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>U</mi><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mrow><mi>lg</mi></mrow><mo stretchy="false">|</mo><mi>U</mi><mo stretchy="false">|</mo><mo stretchy="false">)</mo></mrow></math></mathml>,其中<mathml id="280"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>U</mi><mo stretchy="false">|</mo></mrow></math></mathml>表示样本总数.</p>
                </div>
                <div class="p1">
                    <p id="254">从上述时间复杂度的理论分析可看出, A3M、OSFD和OHFS的时间复杂度相同,相比其它对比算法,在计算过程中需考虑样本总数的影响,所以需要较多的计算时间.FOSFS、OSFS的时间复杂度相对较优.SAOLA的时间复杂度在理论上最优.</p>
                </div>
                <h3 id="255" name="255" class="anchor-tag">4 结 束 语</h3>
                <div class="p1">
                    <p id="256">本文提出基于邻域粗糙集的层次分类在线流特征选择算法.首先,利用分层结构的兄弟策略定义邻域类.同时,定义面向层次化结构数据的邻域粗糙模型.再将流特征选择过程划分为在线重要性选择和在线冗余更新两个阶段.在实验部分,对比各种在线流特征选择策略,OHFS在不同分类器上的各个评价指标都取得较优结果.今后,将设计针对更广泛存在的图类型层次结构的在线流特征选择算法.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="322">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=PZKX201805002&amp;v=MjU1MzA1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1ZyckJOVGZBZHJHNEg5bk1xbzlGWm9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 胡清华,王煜,周玉灿,等.大规模分类任务的分层学习方法综述.中国科学:信息科学,2018,48(5):487-500.(HU Q H,WANG Y,ZHOU Y C,et al.A Review on Hierarchical Learning Methods for Large-Scale Classification Task.Scientia Sinica(Informationis),2018,48(5):487-500.)
                            </a>
                        </p>
                        <p id="324">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM499518FDD3BC5750C7297CCB0C57EA62&amp;v=MTM0NTZ5aFpnN1QxMFR3eVIzaEpHZkxYaE5MeWRDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3N202eGFFPU5pZklZN2V4RjlUTnAva3hFT2g5ZjNrKw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> GOPAL S,YANG Y M.Hierarchical Bayesian Inference and Recursive Regularization for Large-Scale Classification.ACM Transactions on Knowledge Discovery from Data,2015,9(3).DOI:10.1145/2629585.
                            </a>
                        </p>
                        <p id="326">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gene ontology: tool for the unification of biology">

                                <b>[3]</b> ASHBURNER M,BALL C A,BLAKE J A,et al.Gene Ontology:Tool for the Unification of Biology.Nature Genetics,2000,25(1):25-29.
                            </a>
                        </p>
                        <p id="328">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structured Output Prediction with Hierarchical Loss Functions for Seafloor Imagery Taxonomic Categorization">

                                <b>[4]</b> NOURANI-VATANI N,LÓPEZ-SASTRE R,WILLIAM S.Structured Output Prediction with Hierarchical Loss Functions for Seafloor Imagery Taxonomic Categorization // Proc of the Iberian Conference on Pattern Recognition and Image Analysis.Berlin,Germany:Springer,2015:173-183.
                            </a>
                        </p>
                        <p id="330">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical text classification and evaluation">

                                <b>[5]</b> SUN A X,LIM E P.Hierarchical Text Classification and Evaluation // Proc of the 1st IEEE International Conference on Data Mining.Washington,USA:IEEE,2001:521-528.
                            </a>
                        </p>
                        <p id="332">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201802003&amp;v=MjI5ODhIOW5Nclk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tWcnJCS0Q3WWJMRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 王晨曦,林耀进,唐莉,等.基于信息粒化的多标记特征选择算法.模式识别与人工智能,2018,31(2):123-131.(WANG C X,LIN Y J,TANG L,et al.Multi-label Feature Selection Based on Information Granulation.Pattern Recognition and Artificial Intelligence,2018,31(2):123-131.)
                            </a>
                        </p>
                        <p id="334">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint Feature Selection and Hierarchical Classifier Design">

                                <b>[7]</b> FREEMAN C,KULI■.Joint Feature Selection and Hierarchical Classifier Design // Proc of the IEEE International Conference on Systems,Man,and Cybernetics.Washington,USA:IEEE,2011:1728-1734.
                            </a>
                        </p>
                        <p id="336">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Method of the Feature Selection in Hierarchical Text Classification Based on the Category Discrimination and Position Information">

                                <b>[8]</b> SONG J,ZHANG P Z,QIN S J,et al.A Method of the Feature Selection in Hierarchical Text Classification Based on the Category Discrimination and Position Information.IEEE Transactions on Engineering Management,2015,53(4):555-569.
                            </a>
                        </p>
                        <p id="338">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical feature selection with recursive regularization">

                                <b>[9]</b> ZHAO H,ZHU P F,WANG P,et al.Hierarchical Feature Selection with Recursive Regularization // Proc of the 26th International Joint Conference on Artificial Intelligence.New York,USA:ACM,2017:3483-3489.
                            </a>
                        </p>
                        <p id="340">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online feature selection with streaming features">

                                <b>[10]</b> WU X D,YU K,DING W,et al.Online Feature Selection with Streaming Features.IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(5):1178-1192.
                            </a>
                        </p>
                        <p id="342">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES5DC777A05054E013A29479C15F09A214&amp;v=MjgyNDJFPU5pZk9mYmJNYmRiTHFQNUZZZXNLQ0FrNXpoVmk2RFo1VDNhUnJSZERlYnZsUjd1YkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHc3bTZ4YQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> ZHOU P,HU X G,LI P P,et al.OFS-Density:A Novel Online Streaming Feature Selection Method.Pattern Recognition,2019,86:48-61.
                            </a>
                        </p>
                        <p id="344">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201603006&amp;v=MDA0NDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tWcnJCS0Q3WWJMRzRIOWZNckk5RllvUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 刘景华,林梦雷,王晨曦,等.基于局部子空间的多标记特征选择算法.模式识别与人工智能,2016,29(3):240-251.(LIU J H,LIN M L,WANG C X,et al.Multi-label Feature Selection Algorithm Based on Local Subspace.Pattern Recognition and Artificial Intelligence,2016,29(3):240-251.)
                            </a>
                        </p>
                        <p id="346">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Streaming Feature Selection for Multilabel Learning Based on Fuzzy Mutual Information">

                                <b>[13]</b> LIN Y J,HU Q H,LIU J H,et al.Streaming Feature Selection for Multilabel Learning Based on Fuzzy Mutual Information.IEEE Transactions on Fuzzy Systems,2017,25(6):1491-1507.
                            </a>
                        </p>
                        <p id="348">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC0F4FC48B60B9DCF6FF0C3ACEA015564&amp;v=MzE2MDRDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh3N202eGFFPU5pZk9mY0M0YU5XNjNJdE5GdTBQZm5WTnZHQVZuRWw5TzN5VDMyZEVlYk9SUUx5Yg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> LIU J H,LIN Y J,LI Y W,et al.Online Multi-label Streaming Feature Selection Based on Neighborhood Rough Set.Pattern Reco-gnition,2018,84:273-287.
                            </a>
                        </p>
                        <p id="350">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201804014&amp;v=MTU3NjdlWmVSbkZ5L2tWcnJCS0NMZlliRzRIOW5NcTQ5RVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 介飞,谢飞,李磊,等.社交网络中隐式事件突发性检测.自动化学报,2018,44(4):730-742.(JIE F,XIE F,LI L,et al.Latent Event-Related Burst in Social Networks.Acta Automatica Sinica,2018,44(4):730-742.)
                            </a>
                        </p>
                        <p id="352">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Classifiers Using Hierarchically Structured Class Taxonomies">

                                <b>[16]</b> WU F H,ZHANG J,HONAVAR V.Learning Classifiers Using Hierarchically Structured Class Taxonomies // Proc of the International Symposium on Abstraction,Reformulation and Approximation.Berlin,Germany:Springer,2005:313-320.
                            </a>
                        </p>
                        <p id="354">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A survey of hierarchical classification across different application domains">

                                <b>[17]</b> JR SILLA C N,FREITAS A A.A Survey of Hierarchical Classification across Different Application Domains.Data Mining and Knowledge Discovery,2011,22(1/2):31-72.
                            </a>
                        </p>
                        <p id="356">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200803018&amp;v=MTgzODM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tWcnJCTnlmVGJMRzRIdG5Nckk5RWJJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 胡清华,于达仁,谢宗霞.基于邻域粒化和粗糙逼近的数值属性约简.软件学报,2008,19(3):640-649.(HU Q H,YU D R,XIE Z X.Numerical Attribute Reduction Based on Neighborhood Granulation and Rough Approximation.Journal of Software,2008,19(3):640-649.)
                            </a>
                        </p>
                        <p id="358">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving protein function prediction using the hierarchical structure of the gene ontology">

                                <b>[19]</b> EISNER R,POULIN B,SZAFRON D,et al.Improving Protein Function Prediction Using the Hierarchical Structure of the Gene Ontology // Proc of the IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology.Washington,USA:IEEE,2005.DOI:10.1109/CIBCB.2005.1594940.
                            </a>
                        </p>
                        <p id="360">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001510155&amp;v=MjM1MzhkdEZDSGxWTDdOSlZjPU5qN0Jhck80SHRITnFvNUZaZTRLWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> CECI M,MALERBA D.Classifying Web Documents in a Hierarchy of Categories:A Comprehensive Study.Journal of Intelligent Information Systems(Integrating Artificial,Intelligence and Database Technologies),2007,28(1):37-78.
                            </a>
                        </p>
                        <p id="362">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning multiple layers of features from tiny images">

                                <b>[21]</b> KRIZHEVSKY A,HINTON G.Learning Multiple Layers of Features from Tiny Images[C/OL].[2019-04-25].http://www.cs.toronto.edu/～kriz/learning-features-2009-TR.pdf.
                            </a>
                        </p>
                        <p id="364">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to detect unseen object classes by between-class attribute transfer">

                                <b>[22]</b> LAMPERT C H,NICKISCH H,HARMELING S.Learning to Detect Unseen Object Classes by Between-Class Attribute Transfer // Proc of the IEEE Conference on Computer Vision and Pattern Reco-gnition.Washington,USA:IEEE,2009:951-958.
                            </a>
                        </p>
                        <p id="366">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MjI4MDc1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDSGxWTDdOSlZjPU5qN0Jhck80SHRIUHFZZEhZK0lMWTNr&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> EVERINGHAM M,VAN GOOL L,WILLIAMS C K,et al.The Pascal Visual Object Classes(voc) Challenge.International Journal of Computer Vision,2010,88(2):303-338.
                            </a>
                        </p>
                        <p id="368">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Improved Protein Structural Prediction Method By Incorporating Both Sequence And Structure Information.">

                                <b>[24]</b> WEI L Y,LIAO M H,GAO X,et al.An Improved Protein Structural Classes Prediction Method by Incorporating both Sequence and Structure Information.IEEE Transactions on NanoBioscience,2015,14(4):339-349.
                            </a>
                        </p>
                        <p id="370">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJOX&amp;filename=SJOX14010900014144&amp;v=MzI1NjFMYklKMThSYnhzPU5pZkVkcks4SHRETXBvOUZaT29MRFhnOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> DING C H,DUBCHAK I.Multi-class Protein Fold Recognition Using Support Vector Machines and Neural Networks.Bioinforma-tics,2001,17(4):349-358.
                            </a>
                        </p>
                        <p id="372">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large Margin Hierarchical Classification">

                                <b>[26]</b> DEKEL O,KESHET J,SINGER Y.Large Margin Hierarchical Classification // Proc of the 21th ACM International Conference on Machine Learning.New York,USA:ACM,2004.DOI:10.1145/1015330.1015374.
                            </a>
                        </p>
                        <p id="374">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM390FFF33858E8979717A5E1CF83FE812&amp;v=MDU1NThROWVzVGhUYnVkQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZodzdtNnhhRT1OaWZJWTdDeEhxZTYyWXhHYk80SGVYUXd5QjhVNnpnTVRRcmozMg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> YU K,WU X D,DING W,et al.Scalable and Accurate Online Feature Selection for Big Data.ACM Transactions on Knowledge Discovery from Data,2016,11(2).DOI:10.1145/2976744.
                            </a>
                        </p>
                        <p id="376">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A New Online Feature Selection Method Using Neighborhood Rough Set">

                                <b>[28]</b> ZHOU P,HU X G,LI P P.A New Online Feature Selection Method Using Neighborhood Rough Set // Proc of the 8th IEEE Internatio-nal Conference on Big Knowledge.Washington,USA:IEEE,2017:135-142.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201909006" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201909006&amp;v=MjExMjNZYkxHNEg5ak1wbzlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1ZyckJLRDc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
