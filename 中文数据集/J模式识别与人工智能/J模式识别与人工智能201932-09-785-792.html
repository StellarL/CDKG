<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131458825498750%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dMSSB201909002%26RESULT%3d1%26SIGN%3dYwskuJpMAZI216g8oTFPjgNBjmk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201909002&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201909002&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201909002&amp;v=MjIzMjBLRDdZYkxHNEg5ak1wbzlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1Zycks=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#69" data-title="1 本文模型框架 ">1 本文模型框架</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="&lt;b&gt;1.1 问题定义&lt;/b&gt;"><b>1.1 问题定义</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;1.2 主题模型&lt;/b&gt;"><b>1.2 主题模型</b></a></li>
                                                <li><a href="#83" data-title="&lt;b&gt;1.3 长短时记忆网络&lt;/b&gt;"><b>1.3 长短时记忆网络</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;1.4 深度主题特征提取&lt;/b&gt;"><b>1.4 深度主题特征提取</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#108" data-title="2 实验及结果分析 ">2 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#109" data-title="&lt;b&gt;2.1 实验数据&lt;/b&gt;"><b>2.1 实验数据</b></a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;2.2 评价指标&lt;/b&gt;"><b>2.2 评价指标</b></a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;2.3 模型的超参&lt;/b&gt;"><b>2.3 模型的超参</b></a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;2.4 对比模型&lt;/b&gt;"><b>2.4 对比模型</b></a></li>
                                                <li><a href="#150" data-title="&lt;b&gt;2.5 实验结果&lt;/b&gt;"><b>2.5 实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#158" data-title="3 结 束 语 ">3 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#81" data-title="图1 LDA主题模型框图">图1 LDA主题模型框图</a></li>
                                                <li><a href="#90" data-title="图2 长短时记忆网络框图">图2 长短时记忆网络框图</a></li>
                                                <li><a href="#99" data-title="图3 深度主题特征提取模型框图">图3 深度主题特征提取模型框图</a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;表1 实验数据集&lt;/b&gt;"><b>表1 实验数据集</b></a></li>
                                                <li><a href="#155" data-title="&lt;b&gt;表2 各模型在Reuters&lt;/b&gt;-&lt;b&gt;21578数据集上的实验结果&lt;/b&gt;"><b>表2 各模型在Reuters</b>-<b>21578数据集上的实验结果</b></a></li>
                                                <li><a href="#156" data-title="&lt;b&gt;表3 各模型在IMDB数据集上的实验结果&lt;/b&gt;"><b>表3 各模型在IMDB数据集上的实验结果</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;表4 各模型在AAPD数据集上的实验结果&lt;/b&gt;"><b>表4 各模型在AAPD数据集上的实验结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="200">


                                    <a id="bibliography_1" title=" WANG J,YANG Y,MAO J H,et al.CNN-RNN:A Unified Framework for Multi-label Image Classification // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:2285-2294." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CNN-RNN:A Unif ied Framework for Multi-label Image Classification">
                                        <b>[1]</b>
                                         WANG J,YANG Y,MAO J H,et al.CNN-RNN:A Unified Framework for Multi-label Image Classification // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:2285-2294.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_2" title=" CHEN G B,YE D H,XING Z C,et al.Ensemble Application of Convolutional and Recurrent Neural Networks for Multi-label Text Categorization // Proc of the International Joint Conference on Neural Networks.Washington,USA:IEEE,2017:2377-2383." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ensemble application of convolutional and recurrent neural networks for multi-label text categorization">
                                        <b>[2]</b>
                                         CHEN G B,YE D H,XING Z C,et al.Ensemble Application of Convolutional and Recurrent Neural Networks for Multi-label Text Categorization // Proc of the International Joint Conference on Neural Networks.Washington,USA:IEEE,2017:2377-2383.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_3" title=" SALTON G,BUCKLEY C.Term-Weighting Approaches in Automatic Text Retrieval.Information Processing and Management,1988,24(5):513-523." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Term-weighting approaches in automatic text retrieval">
                                        <b>[3]</b>
                                         SALTON G,BUCKLEY C.Term-Weighting Approaches in Automatic Text Retrieval.Information Processing and Management,1988,24(5):513-523.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_4" title=" BERGER A,LAFFERTY J.Information Retrieval as Statistical Translation.ACM SIGIR Forum,2017,51(2):219-226." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Information Retrieval as Statistical Translation">
                                        <b>[4]</b>
                                         BERGER A,LAFFERTY J.Information Retrieval as Statistical Translation.ACM SIGIR Forum,2017,51(2):219-226.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_5" title=" CAVNAR W B,TRENKLE J M.N-Gram-Based Text Categorization[C/OL].[2019-04-22].http://www.let.rug.nl/vannoord/TextCat/textcat.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=N-Gram-Based Text Categorization[C/OL]">
                                        <b>[5]</b>
                                         CAVNAR W B,TRENKLE J M.N-Gram-Based Text Categorization[C/OL].[2019-04-22].http://www.let.rug.nl/vannoord/TextCat/textcat.pdf.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                     BLEI D M,ANDREW Y N,JORDAN M I.Latent Dirichlet Allocation.Journal of Machine Learning Research,2003,3:993-1022.</a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_7" title=" 陈培新,郭武.融合潜在主题信息和卷积语义特征的文本主题分类.信号处理,2017,33(8):1090-1096.) (CHEN P X,GUO W.Document Topic Categorization Combining Latent Topic Information and Convolutional Semantic Features.Journal of Signal Processing,2017,33(8):1090-1096.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXCN201708009&amp;v=MDE0MzQva1ZycktQVFhJWUxHNEg5Yk1wNDlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         陈培新,郭武.融合潜在主题信息和卷积语义特征的文本主题分类.信号处理,2017,33(8):1090-1096.) (CHEN P X,GUO W.Document Topic Categorization Combining Latent Topic Information and Convolutional Semantic Features.Journal of Signal Processing,2017,33(8):1090-1096.)
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_8" title=" HARRIS Z S.Distributional Structure [C/OL].[2019-04-22].http://link.springer.com/chapter/10.1007%2F978-94-009-8467-7_1." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributional Structure [C/OL]">
                                        <b>[8]</b>
                                         HARRIS Z S.Distributional Structure [C/OL].[2019-04-22].http://link.springer.com/chapter/10.1007%2F978-94-009-8467-7_1.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_9" title=" FIRTH J R.A Synopsis of Linguistic Theory,1930-1955 // LANGENDOEN D T,ed.Studies in Linguistic Analysis.Oxford,UK:Philological Society,1957:1-32." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Synopsis of Linguistic Theory, 1930-1955">
                                        <b>[9]</b>
                                         FIRTH J R.A Synopsis of Linguistic Theory,1930-1955 // LANGENDOEN D T,ed.Studies in Linguistic Analysis.Oxford,UK:Philological Society,1957:1-32.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_10" title=" BENGIO Y,DUCHARME R,VINCENT P,et al.A Neural Pro-babilistic Language Model.Journal of Machine Learning Research,2003,3:1137-1155." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Neural Probabilistic Language Model">
                                        <b>[10]</b>
                                         BENGIO Y,DUCHARME R,VINCENT P,et al.A Neural Pro-babilistic Language Model.Journal of Machine Learning Research,2003,3:1137-1155.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_11" title=" MIKOLOV T,CHEN K,CORRADO G,et al.Efficient Estimation of Word Representations in Vector Space[C/OL].[2019-04-22].https://arxiv.org/pdf/1301.3781.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient estimation of word representations in vector space">
                                        <b>[11]</b>
                                         MIKOLOV T,CHEN K,CORRADO G,et al.Efficient Estimation of Word Representations in Vector Space[C/OL].[2019-04-22].https://arxiv.org/pdf/1301.3781.pdf.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_12" title=" PENNINGTON J,SOCHER R,MANNING C D.Glove:Global Vectors for Word Representation // Proc of the Conference on Empirical Methods in Natural Language Processing.Stroudsburg,USA:ACL,2014:1532-1543." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Glove:Global Vectors for Word Representation">
                                        <b>[12]</b>
                                         PENNINGTON J,SOCHER R,MANNING C D.Glove:Global Vectors for Word Representation // Proc of the Conference on Empirical Methods in Natural Language Processing.Stroudsburg,USA:ACL,2014:1532-1543.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     LE Q V,MIKOLOV T.Distributed Representations of Sentences and Documents // Proc of the 31th International Conference on Machine Learning.Berlin,Germany:Springer,2014:1188-1196.</a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_14" title=" HOCHREITER S,SCHMIDHUBER J.Long Short-Term Memory.Neural Computation,1997,9(8):1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MzE5NjRaYks5SHRqTXFvOUZaT29MRFhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUoxOFJieEE9TmlmSg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         HOCHREITER S,SCHMIDHUBER J.Long Short-Term Memory.Neural Computation,1997,9(8):1735-1780.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_15" title=" GRAVES A,MOHAMED A,HINTON G.Speech Recognition with Deep Recurrent Neural Networks // Proc of the IEEE International Conference on Acoustics,Speech and Signal Processing.Washington,USA:IEEE,2013:6645-6649." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech recognition with deep recurrent neural networks">
                                        <b>[15]</b>
                                         GRAVES A,MOHAMED A,HINTON G.Speech Recognition with Deep Recurrent Neural Networks // Proc of the IEEE International Conference on Acoustics,Speech and Signal Processing.Washington,USA:IEEE,2013:6645-6649.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_16" title=" CHO K,VAN MERRIENBOER B,GULCEHRE C,et al.Lear-ning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation[C/OL].[2019-04-22].https://arxiv.org/pdf/1406.1078.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lear-ning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation[C/OL]">
                                        <b>[16]</b>
                                         CHO K,VAN MERRIENBOER B,GULCEHRE C,et al.Lear-ning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation[C/OL].[2019-04-22].https://arxiv.org/pdf/1406.1078.pdf.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_17" title=" MIWA M,SASAKI Y.Modeling Joint Entity and Relation Extraction with Table Representation // Proc of the Conference on Empirical Methods in Natural Language Processing.Stroudsburg,USA:ACL,2014:1858-1869." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modeling Joint Entity and Relation Extraction with Table Representation">
                                        <b>[17]</b>
                                         MIWA M,SASAKI Y.Modeling Joint Entity and Relation Extraction with Table Representation // Proc of the Conference on Empirical Methods in Natural Language Processing.Stroudsburg,USA:ACL,2014:1858-1869.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_18" title=" TANG D Y,QIN B,LIU T,et al.Document Modeling with Gated Recurrent Neural Network for Sentiment Classification // Proc of the Conference on Empirical Methods in Natural Language Processing.Stroudsburg,USA:ACL,2015:1422-1432." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Document Modeling with Gated Recurrent Neural Network for Sentiment Classification">
                                        <b>[18]</b>
                                         TANG D Y,QIN B,LIU T,et al.Document Modeling with Gated Recurrent Neural Network for Sentiment Classification // Proc of the Conference on Empirical Methods in Natural Language Processing.Stroudsburg,USA:ACL,2015:1422-1432.
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_19" title=" ZHANG M L,ZHOU Z H.A Review on Multi-label Learning Algorithms.IEEE Transactions on Knowledge and Data Engineering,2014,26(8):1819-1837." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Review on Multi-label Learning Algorithms">
                                        <b>[19]</b>
                                         ZHANG M L,ZHOU Z H.A Review on Multi-label Learning Algorithms.IEEE Transactions on Knowledge and Data Engineering,2014,26(8):1819-1837.
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_20" title=" LUACES O,DIEZ J,BARRANQUERO J,et al.Binary Relevance Efficacy for Multilabel Classification.Progress in Artificial Intelligence,2012,1(4):303-313." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD121113000996&amp;v=MTQ2NjRIOUROckk5RlpPSUdDaE04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWnU5dUZDdnNVN2pKSlZzV05qN0Jhcks2&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         LUACES O,DIEZ J,BARRANQUERO J,et al.Binary Relevance Efficacy for Multilabel Classification.Progress in Artificial Intelligence,2012,1(4):303-313.
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_21" title=" TSOUMAKAS G,KATAKIS I,VLAHAVAS I.Random k-labelsets for Multilabel Classification.IEEE Transactions on Knowledge and Data Engineering,2011,23(7):1079-1089." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Random k-Labelsets for Multilabel Classification">
                                        <b>[21]</b>
                                         TSOUMAKAS G,KATAKIS I,VLAHAVAS I.Random k-labelsets for Multilabel Classification.IEEE Transactions on Knowledge and Data Engineering,2011,23(7):1079-1089.
                                    </a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_22" title=" READ J,PFAHRINGER B,HOLMES G,et al.Classifier Chains for Multi-label Classification // Proc of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases.Berlin,Germany:Springer,2011:254-269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Classifier Chains for Multi-label Classification">
                                        <b>[22]</b>
                                         READ J,PFAHRINGER B,HOLMES G,et al.Classifier Chains for Multi-label Classification // Proc of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases.Berlin,Germany:Springer,2011:254-269.
                                    </a>
                                </li>
                                <li id="244">


                                    <a id="bibliography_23" title=" ELISSEEFF A,WESTON J.A Kernel Method for Multi-labelled Classification // DIETTERICH T G,BECKER S,GHAHRAMANI Z,eds.Advances in Neural Information Processing Systems 14.Cambridge,USA:The MIT Press,2002:681-687." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Kernel Method for Multi-Labelled Classification">
                                        <b>[23]</b>
                                         ELISSEEFF A,WESTON J.A Kernel Method for Multi-labelled Classification // DIETTERICH T G,BECKER S,GHAHRAMANI Z,eds.Advances in Neural Information Processing Systems 14.Cambridge,USA:The MIT Press,2002:681-687.
                                    </a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_24" title=" ZHANG M L,ZHOU Z H.ML-KNN:A Lazy Learning Approach to Multi-label Learning.Pattern Recognition,2007,40(7):2038-2048." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739502&amp;v=MjQ2NjRaZVp1SHlqbVVMYklKMThSYnhBPU5pZk9mYks3SHRETnFZOUZZK2dHQ1h3N29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                         ZHANG M L,ZHOU Z H.ML-KNN:A Lazy Learning Approach to Multi-label Learning.Pattern Recognition,2007,40(7):2038-2048.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(09),785-792 DOI:10.16451/j.cnki.issn1003-6059.201909002            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>面向多标签文本分类的深度主题特征提取</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E6%96%87%E5%AE%9E&amp;code=06518795&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈文实</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%BF%83%E6%83%A0&amp;code=42001098&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘心惠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%B2%81%E6%98%8E%E7%BE%BD&amp;code=06499193&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">鲁明羽</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A7%E8%BF%9E%E6%B5%B7%E4%BA%8B%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0106810&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">大连海事大学信息科学技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对单标签特征提取方法不能有效解决多标签文本分类的问题,文中提出融合主题模型(LDA)与长短时记忆网络(LSTM)的双通道深度主题特征提取模型(DTFEM).LDA与LSTM分别作为两个通道,通过LDA为文本的全局特征建模,利用LSTM为文本的局部特征建模,使模型能同时表达文本的全局特征和局部特征,实现有监督学习与无监督学习的有效结合,得到文本不同层次的特征提取.实验表明,相比文本特征提取模型,文中模型在多标签分类结果上的多项指标均有明显提升.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A0%87%E7%AD%BE%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多标签文本分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E4%B8%BB%E9%A2%98%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度主题特征提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">主题模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长短时记忆网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    陈文实，博士研究生，副教授，主要研究方向为自然语言处理、文本分类．E-mail:lnjzcws@sohu.com.&lt;image id="197" type="formula" href="images/MSSB201909002_19700.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    刘心惠，硕士研究生，主要研究方向为自然语言处理、文本分类．E-mail:xinhuil-iu2017@163.com.&lt;image id="198" type="formula" href="images/MSSB201909002_19800.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    *鲁明羽(通讯作者)，博士，教授，主要研究方向为数据挖掘、模式识别、机器学习、自然语言处理．E-mail:lumingyu@dl-mu.edu.cn.&lt;image id="199" type="formula" href="images/MSSB201909002_19900.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(No.61073133,61272369)资助;</span>
                    </p>
            </div>
                    <h1><b>Feature Extraction of Deep Topic Model for Multi-label Text Classification</b></h1>
                    <h2>
                    <span>CHEN Wenshi</span>
                    <span>LIU Xinhui</span>
                    <span>LU Mingyu</span>
            </h2>
                    <h2>
                    <span>College of Information Science and Technology,Dalian Maritime University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Traditional single-label feature extraction methods cannot effectively solve the problem of multi-label text classification. Aiming at this problem, a dual model of latent dirichlet allocation(LDA) and long short-term memory(LSTM), deep topic feature extraction model(DTFEM), is proposed in this paper. LDA and LSTM are employed as two channels, respectively. LDA is used to model global features of the text, and LSTM is used to model local features of the text. DTFEM can express the global and local features of the text simultaneously and combine supervised learning and unsupervised learning effectively to realize the feature extraction of different levels of text. Experimental results show that DTFEM is superior to other traditional text feature extraction models and obviously improves the indicators of multi-label text classification tasks.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multi-label%20Text%20Classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multi-label Text Classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Topic%20Feature%20Extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Topic Feature Extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Topic%20Model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Topic Model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Long%20Short-Term%20Memory%20Network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Long Short-Term Memory Network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    CHEN Wenshi,Ph. D. candidate,associate professor. His research interests include natural language processing and text classification.;
                                </span>
                                <span>
                                    LIU Xinhui,master student. Her research interests include natural language processing and text classification.;
                                </span>
                                <span>
                                    LU Mingyu(Corresponding author),Ph. D.,professor. His research interests include data mining, pattern recognition, machine learning and natural language processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-15</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China(No.61073133,61272369);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="62">多标签文本分类是自然语言处理(Natural Language Processing, NLP)领域的一个重要研究方向,学者们提出一系列多标签文本分类算法.然而文本的高维数据中存在大量不相关、冗余的特征,直接导致分类器性能降低<citation id="248" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="63">作为一种高效的数据降维技术,特征提取已广泛应用于多标签文本分类任务中.传统的文本特征提取是人工设计的特征,特征的设计会极大影响最后的效果,特征设计和提取过程也非常耗费人力,同时还可能带来积累误差和噪音.文本特征提取大都依据简单的数理统计思想,认为特征词之间相互独立,忽略文档结构和语义对于特征词选取的重要性,导致语义因素无法在提取特征词的过程中发挥作用,从而影响多标签文本分类的准确性<citation id="249" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="64">词袋(Bags of  Words, BoW)模型是常用的文本特征提取方法,将文本视为若干个单词的集合,忽略单词顺序、语法和句法等要素,只关注单词在文本中的出现频率,如词频(Term Frequency, TF)<citation id="250" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和词频-逆文档频率(Term Frequency-Inverse Document Frequency, TF-IDF)<citation id="251" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>.为了进一步提取更多的特征,采用n-gram语言模型<citation id="252" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,为了不让特征集合过于庞大而拖累分类速度,采用<i>n</i>为1至3的一元Unigram、二元Bigram和三元Trigram特征提取模型.</p>
                </div>
                <div class="p1">
                    <p id="65">Blei等<citation id="253" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出隐含狄利克雷分布(Latent Diri-chlet Allocation, LDA)主题模型,通过词与词在文档中的共现情况,挖掘文本的隐含语义信息,将文本映射为一个低维向量,实现文本数据的有效分析.概率主题模型利用词与词之间的关联性,但未利用相邻词构成的上下文信息<citation id="254" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>.Harris<citation id="255" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出分布式假说(Distributional Hypothesis):“上下文相似的词,其语义也相似”.Firth<citation id="256" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>对分布式假说进行阐述和论证,指出词的语义由其上下文确定,因此利用词的相邻上下文信息可更好地挖掘词的语义信息.基于分布式假说和神经网络语言模型( Neural Network Language Model, NNLM)<citation id="257" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>等,Mikolov等<citation id="258" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出词向量化算法(word2vec)、连续词袋模型(Continuous Bag-of-Words, CBOW)和连续跳跃元语法模型(Skip-gram),用于学习词的分布式表示.CBOW和Skip-gram基于局部的上下文信息,Pennington等<citation id="259" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>在文献<citation id="260" type="reference">[<a class="sup">11</a>]</citation>基础上加入全局的统计信息,提出全局词向量(Global Vectors, GloVe)模型.Word2vec和GloVe是目前常用的两种词向量学习模型.受word2vec模型的启发,Le等<citation id="261" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出文档向量化算法(Doc2vec).Doc2vec是无监督学习算法,可实现使用一个向量表示不同的文档的功能,结构可潜在克服词袋模型的缺点.</p>
                </div>
                <div class="p1">
                    <p id="66">近些年,随着深度学习相关技术的发展,深度学习在特征的自动学习和表达中的作用越来越重要.循环神经网络(Recurrent Neural Network, RNN)在对时间序列数据的分析中表现出更好的特性.基于RNN模型出现较多变体模型,Hochreiter等<citation id="262" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出长短时记忆单元(Long-Short Term Memory, LSTM),对序列信息中的上下文结构具有较好的建模能力,可以捕获序列中长远的上下文信息,适合于对文本这种由单词组成的序列信息进行建模,广泛应用于语音识别、机器翻译、情感分类等领域.Graves等<citation id="263" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>将LSTM应用于语音识别系统.Google使用基于多层LSTM的机器翻译模型<citation id="264" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>.Miwa等<citation id="265" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>将LSTM用于实体和关系抽取.Tang等<citation id="266" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>将LSTM应用于情感分类任务,均取得不错效果.</p>
                </div>
                <div class="p1">
                    <p id="67">随着学术研究的推进,研究人员利用LSTM神经网络模型进行有监督的学习,整合特征提取过程与分类器训练过程,实现端到端的机器学习.可以将多特征输入到模型中进行联合训练,同时提取多种特征进行分类器训练,实现文本的特征互补,降低单一特征缺陷的影响,提高多标签文本分类的性能.</p>
                </div>
                <div class="p1">
                    <p id="68">为了有效提升高维多标签文本数据的低维表达性能,本文在现有的文本特征提取方法的基础上,借助深度学习的特征提取能力,结合深度学习中的处理序列数据的LSTM神经网络与LDA主题模型,提出深度主题特征提取方法(Deep Topic Feature Extrac-tion, DTFE),由LSTM神经网络得出文档内部的上下文信息特征表示,由主题模型捕获文档集的全局信息特征表示,实现文档的局部特征表示与全局特征表示结合,实现多标签文本分类的特征提取.将方法应用到多标签文本分类算法中,在3个经典的数据集上进行测试,多标签分类结果的多项指标均有明显提升.</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag">1 本文模型框架</h3>
                <h4 class="anchor-tag" id="70" name="70"><b>1.1 问题定义</b></h4>
                <div class="p1">
                    <p id="71">多标签分类的目的是为数据集中的每个样本分配多个标签.假设<i><b>X</b></i>=<b>R</b><sup><i>d</i></sup>表示在实数域<b>R</b>上<i>d</i>维的样本空间,<i>Y</i>={<i>y</i><sub>1</sub>,<i>y</i><sub>2</sub>,…,<i>y</i><sub><i>q</i></sub>}表示包含<i>q</i>个标签空间的类别标签集合,多标签分类是指通过训练数据集</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>m</mi><mo stretchy="false">}</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">得到一个映射函数 <i>f</i>∶<i><b>X</b></i>→2<sup><i>Y</i></sup>,其中<i><b>x</b></i><sub><i>i</i></sub>∈<i><b>X</b></i>,即<i><b>x</b></i><sub><i>i</i></sub>为输入空间<i><b>X</b></i>中的一个训练数据,<i>y</i><sub><i>i</i></sub>∈<i>Y</i>,即<i>y</i><sub><i>i</i></sub>为样本<i><b>x</b></i><sub><i>i</i></sub>的标签集合<citation id="267" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>.输入一个待分类样本<i><b>x</b></i>∈<i><b>X</b></i>时,通过映射函数<i>f</i> 得到<i><b>x</b></i>的预测标签集合<i>P</i><sub><i><b>x</b></i></sub>⊂<i>Y</i>,使<i>P</i><sub><i><b>x</b></i></sub>与样本<i><b>x</b></i>的真实标签集合<i>Y</i><sub><i><b>x</b></i></sub>最接近.单标签分类问题是多标签分类问题的特例,当<mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mo>=</mo><mi>q</mi><mo>=</mo><mn>1</mn></mrow></math></mathml>时,多标签分类转化为单标签分类.</p>
                </div>
                <div class="p1">
                    <p id="74">多标签分类算法主要包括问题转换方法和算法适应方法.问题转换方法将每个标签作为单独的分类进行处理,如二元关联算法(Binary Relevance)<citation id="268" type="reference"><link href="238" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、随机k标签算法(Random k-Labels)<citation id="269" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>和分类器链算法(Classifier Chains)<citation id="270" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>．算法自适应方法改造传统的分类算法，适应多标签数据，如基于排序的支持向量机算法(Ranking Based SVM,RankSVM)<citation id="271" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>，多标签k近邻算法(Multi-label k-Nearest Neighbor,ML-KNN)<citation id="272" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>等．</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>1.2 主题模型</b></h4>
                <div class="p1">
                    <p id="78">主题模型(Topic Models)是一种可以从离散数据集中自动提取隐含语义主题的生成概率模型,是一种包含隐含变量的层次贝叶斯混合成员身份模型,可以从无标记数据中挖掘隐含的语义信息.Blei等<citation id="273" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出隐含狄利克雷分布(LDA)主题模型,使用一个服从Dirichlet分布的<i>K</i>维隐含随机变量表示文本的主题概率分布.</p>
                </div>
                <div class="p1">
                    <p id="79">LDA主题模型的基本思想是:假设存在<i>K</i>个主题,每个主题是语料库中所有词项的概率分布,每篇文档由这<i>K</i>个主题按照不同概率随机混合产生.</p>
                </div>
                <div class="p1">
                    <p id="80">LDA主题模型如图1所示,<i>α</i>为文档-主题分布的先验参数,<i>β</i>为主题-单词分布的先验参数,<i>W</i><sub><i>m</i></sub><sub>,</sub><sub><i>n</i></sub>表示第<i>m</i>篇文档中的第<i>n</i>个单词,<i>Z</i><sub><i>m</i></sub><sub>,</sub><sub><i>n</i></sub>表示第<i>m</i>篇文档中第<i>n</i>个单词对应的主题,通过概率推断方法学习得到每个主题对应的词项分布<i>φ</i><sub><i>k</i></sub>及每篇文档对应的主题概率分布<i>θ</i><sub><i>m</i></sub>.</p>
                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909002_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 LDA主题模型框图" src="Detail/GetImg?filename=images/MSSB201909002_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 LDA主题模型框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909002_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Framework of LDA topic model</p>

                </div>
                <div class="p1">
                    <p id="82">LDA主题模型由文档、主题、单词三个层次的概率分布组成,在文档-单词特征层次中加入主题,捕获文本的全局底层语义结构,实现文本在主题潜在空间上特征的良好表达.LDA模型根据文档主题分布和主题词语分布的狄利克雷先验假设,结合词语样本信息计算文本后验主题分布的贝叶斯估算过程.模型可以对语料库<i>D</i>中的任意文本<i>m</i>建模,生成对应的主题概率分布<i>θ</i><sub><i>m</i></sub>=(<i>z</i><sub><i>m</i></sub><sub>,1</sub>,<i>z</i><sub><i>m</i></sub><sub>,2</sub>,…,<i>z</i><sub><i>m</i></sub><sub>,</sub><sub><i>n</i></sub>),构建基于主题分布的文本特征向量表示.</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83"><b>1.3 长短时记忆网络</b></h4>
                <div class="p1">
                    <p id="84">提取全局语义特征的LDA模型通过概率模型构建文本的主题分布,主题分布体现文本的整体语义信息,但忽略词义特征的相互关系,不能捕捉到词语的上下文语义关系.神经网络方法常使用RNN解决这一问题.RNN善于处理序列类型的文本数据,通过循环递归的结构有效利用序列自身携带的信息.然而,RNN由于结构缺陷,导致无法学习输入数据中距离较远的逻辑关系,长短期记忆模型将循环神经网络的循环结构替换成长短期记忆单元,解决远距离逻辑关系的学习问题.</p>
                </div>
                <div class="p1">
                    <p id="85">本文采用LSTM提取文档的局部特征信息,特征提取方法如图2所示.假设输入序列</p>
                </div>
                <div class="p1">
                    <p id="86"><i><b>X</b></i>=[<i><b>x</b></i><sub>1</sub>,<i><b>x</b></i><sub>2</sub>,…,<i><b>x</b></i><sub><i>n</i></sub>]</p>
                </div>
                <div class="p1">
                    <p id="87">为每篇文本的单词信息,表示每篇文本中包含<i>n</i>个单词,通过单词嵌入将每篇文本进行向量化表示,形成词嵌入向量.LSTM神经网络按照时间序列展开,每个时间点表示一层网络,每个时间点输入文本序列中的一个数据,通过计算当前输入和上一时间点中隐藏状态的值更新当前时刻的隐藏值.通过控制门的状态实现序列信息在时间上的传递.最终生成语义编码向量</p>
                </div>
                <div class="p1">
                    <p id="88"><i><b>H</b></i>=[<i><b>h</b></i><sub>1</sub>,<i><b>h</b></i><sub>2</sub>,…,<i><b>h</b></i><sub><i>n</i></sub>],</p>
                </div>
                <div class="p1">
                    <p id="89">表示文本局部特征信息.</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909002_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 长短时记忆网络框图" src="Detail/GetImg?filename=images/MSSB201909002_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 长短时记忆网络框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909002_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Framework of long short-term memory network</p>

                </div>
                <h4 class="anchor-tag" id="91" name="91"><b>1.4 深度主题特征提取</b></h4>
                <div class="p1">
                    <p id="92">随着深度学习理论的发展,基于深度学习理论的特征融合也得到广泛应用,取得较好效果.本文在提取样本多特征的基础上提出特征融合的方法,将特征融合的思想引入到深度学习理论中,在深度神经网络模型中加入融合过程,优化网络模型,实现特征互补,降低单一特征固有缺陷的影响.</p>
                </div>
                <div class="p1">
                    <p id="93">本文将深度主题特征提取模型应用于多标签文本分类任务中,验证模型的有效性,模型结构如图3所示.</p>
                </div>
                <div class="p1">
                    <p id="94">1)文本输入层.将输入文本序列转化为向量化表示,得到的文本表示向量作为下一层的输入.</p>
                </div>
                <div class="p1">
                    <p id="95">2)特征提取层.分别使用LSTM神经网络和LDA主题模型对输入序列进行局部特征提取和全局特征提取,提取不同层次、具有不同含义的特征向量.</p>
                </div>
                <div class="p1">
                    <p id="96">3)特征融合层. 实现对全局特征信息和局部特征信息的融合,实现多层次提取文本信息,有效减少文本信息丢失.</p>
                </div>
                <div class="p1">
                    <p id="97">4)多标签分类层.将特征融合后的向量信息输入多标签文本分类器中,进行分类操作.</p>
                </div>
                <div class="p1">
                    <p id="98">5)输出层.最终输出模型预测的类别标签.</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909002_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 深度主题特征提取模型框图" src="Detail/GetImg?filename=images/MSSB201909002_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 深度主题特征提取模型框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909002_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Framework of deep topic feature extraction model</p>

                </div>
                <div class="p1">
                    <p id="100">LDA是基于BOW的一种浅层特征表示方法,忽略词的顺序信息和上下文单词之间的信息,将词映射到主题空间,通过计算每个单词的权重选取特征,包含文本、主题和词三层结构的概率.LDA主题模型直接捕获文本中与单词相关的全局语义,获取文本的全局特征表示.基于词袋模型的假设,即认为每个文本是单词的无序集合,文本中的每个单词标记的产生过程互相独立,不会考虑文本中词之间的顺序.处理序列结构的神经网络LSTM可以捕捉到句子中的上下文信息,自动提取特征信息,捕获单词序列的局部结构,获取文本的局部特征表示,捕捉单词之间深层次的隐藏信息,但神经网络难以记住远程依赖关系,会产生信息丢失问题.因此,本文提出通过深度主题特征提取的特征融合方式,从词粒度和文本粒度两个层面表示文本的特征矩阵,实现多特征的优势互补,更全面细致地提取文本不同层次的特征信息.</p>
                </div>
                <div class="p1">
                    <p id="101">文本特征表示是对文本数据的向量化建模,以体现文本中表征性较强、计算价值较高的文本特征.LSTM从词粒度层面,挖掘词义对文本进行精细语义表达,LDA从文本粒度层面,通过概率模型构建文本的主题分布,着重文本语义整体语义的表达.两者通过向量拼接方式,构建包含语义和词义的特征矩阵,从两个层面保证文本特征的完整性.通过深度主题特征提取的方式结合文本的局部特征和全局特征,深入挖掘不同层次的文本特征.</p>
                </div>
                <div class="p1">
                    <p id="102">经过LSTM神经网络生成文本的局部特征表示</p>
                </div>
                <div class="p1">
                    <p id="103"><i><b>H</b></i>=[<i><b>h</b></i><sub>1</sub>,<i><b>h</b></i><sub>2</sub>,…,<i><b>h</b></i><sub><i>n</i></sub>],</p>
                </div>
                <div class="p1">
                    <p id="104">主题模型提取文本全局特征表示</p>
                </div>
                <div class="p1">
                    <p id="105"><i><b>D</b></i>=[<i><b>d</b></i><sub>1</sub>,<i><b>d</b></i><sub>2</sub>,…,<i><b>d</b></i><sub><i>n</i></sub>],</p>
                </div>
                <div class="p1">
                    <p id="106">通过融合层实现特征输出的融合<i><b>M</b></i>=[<i><b>H</b></i>,<i><b>D</b></i>].融合层通过合并拼接神经网络的输出向量和主题模型的输出向量,拼接融合提取的文本特征向量,实现不同层次的文本特征融合.神经网络和主题模型的输出特征均为一维数据,在融合时无需对输出向量进行维度统一,最大程度地避免数据信息损失.通过多标签文本分类器得到文本分类标签</p>
                </div>
                <div class="p1">
                    <p id="107"><i><b>y</b></i>=[<i>y</i><sub>1</sub>,<i>y</i><sub>2</sub>,…,<i>y</i><sub><i>m</i></sub>].</p>
                </div>
                <h3 id="108" name="108" class="anchor-tag">2 实验及结果分析</h3>
                <h4 class="anchor-tag" id="109" name="109"><b>2.1 实验数据</b></h4>
                <div class="p1">
                    <p id="110">本文采用如下3个数据集作为实验数据集.</p>
                </div>
                <div class="p1">
                    <p id="111">1)Reuters-21578.分布在22个文件中,从reut2-</p>
                </div>
                <div class="p1">
                    <p id="112">000.sgm到reut2-020.sgm,每个文件包含1 000个文本,reut2-021.sgm包含578个文本,总计21 578个文本,共有672个类别.本次实验选取文章数最多的20个类别标签.</p>
                </div>
                <div class="p1">
                    <p id="113">2)IMDB.包含117 352篇电影简介,其中英文的电影简介有117 196篇,每部电影可与27个类别相关.</p>
                </div>
                <div class="p1">
                    <p id="114">3)AAPD.arXiv网站包含计算机科学领域55 840篇论文的摘要和主题,每篇论文可能涉及多个科目,共有54个科目,可根据摘要内容预测学术论文对应的科目.</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115"><b>2.2 评价指标</b></h4>
                <div class="p1">
                    <p id="116">多标签文本分类的评价指标通常包括精确率(Precision, P)、召回率(Recall, R)、F1值(F1 Score, F1)、子集准确率(Subset Accuracy, SA)、汉明损失(Hamming Loss, HL),各个指标定义的说明和公式如下.</p>
                </div>
                <div class="p1">
                    <p id="117">精确率(P)反映被正确预测的标签与预测标签和实际标签之和的比例关系:</p>
                </div>
                <div class="p1">
                    <p id="118"><mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi></mrow></math></mathml>,</p>
                </div>
                <div class="p1">
                    <p id="119">值越大,被正确预测的标签数越多.</p>
                </div>
                <div class="p1">
                    <p id="120">召回率(R)反映正确预测的标签与实际标签总数的比例:</p>
                </div>
                <div class="p1">
                    <p id="121"><mathml id="162"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi></mrow></math></mathml>,</p>
                </div>
                <div class="p1">
                    <p id="122">值越大,系统性能越好.</p>
                </div>
                <div class="p1">
                    <p id="123"><i>F</i>1值(F1)是一个综合指标,结合精确率和召回率:</p>
                </div>
                <div class="p1">
                    <p id="124"><mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mn>1</mn><mo>=</mo><mfrac><mrow><mn>2</mn><mi>Ρ</mi><mi>R</mi></mrow><mrow><mi>Ρ</mi><mo>+</mo><mi>R</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi></mrow></math></mathml>,</p>
                </div>
                <div class="p1">
                    <p id="125">值越大,系统性能越好.</p>
                </div>
                <div class="p1">
                    <p id="126">子集准确率(SA)反映分类器对所有样本的判定能力,表示为分类器正确分类的样本数与总样本数之比:</p>
                </div>
                <div class="p1">
                    <p id="127"><mathml id="164"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>A</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi></mrow></math></mathml>,</p>
                </div>
                <div class="p1">
                    <p id="128">其中,<i>TP</i>(True Positives)表示正例中判断正确的样本数,<i>FP</i>(False Positives)表示正例中判断错误的样本数,<i>TN</i>(True Negative)表示负例中判断正确的样本数,<i>FN</i>(False Negatives)表示负例中判断错误的样本数.</p>
                </div>
                <div class="p1">
                    <p id="129">汉明损失(HL)反映样本中真实结果与预测结果间的异或,表示样本标签对被错分类的次数:</p>
                </div>
                <div class="p1">
                    <p id="130"><mathml id="165"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mi>D</mi><mo>|</mo></mrow></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mi>D</mi><mo>|</mo></mrow></mrow></munderover><mrow><mfrac><mrow><mtext>x</mtext><mtext>o</mtext><mtext>r</mtext><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mrow><mo>|</mo><mi>L</mi><mo>|</mo></mrow></mrow></mfrac></mrow></mstyle></mrow></math></mathml>,</p>
                </div>
                <div class="p1">
                    <p id="131">其中,<mathml id="166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi>D</mi><mo>|</mo></mrow></mrow></math></mathml>表示样本总数,<mathml id="167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi>L</mi><mo>|</mo></mrow></mrow></math></mathml>表示标签总数,<i>x</i><sub><i>i</i></sub>和<i>y</i><sub><i>i</i></sub>分别表示预测的标签结果和真实标签,xor表示异或运算.值越小,模型性能越好.</p>
                </div>
                <h4 class="anchor-tag" id="132" name="132"><b>2.3 模型的超参</b></h4>
                <div class="p1">
                    <p id="133">由于原始文本中存在不规范的字符(HTML标记、xml标记等)、多余的标点符号、无意义的停用词等,因此应去掉文本中的这些噪声数据,形成纯英文文本语料.每个文本包含单词数长短不一,单词数过短会导致无法准确判断文本所属类别,单词数过长会导致空间浪费,因此去除单词数少于20个单词的文本,选取70%文本的最大长度作为输入长度,少于该单词数的文本使用〈pad〉进行填充,大于该长度则将文本截断.</p>
                </div>
                <div class="p1">
                    <p id="134">为了评估模型,将数据集划分为两个部分.2/3的数据用于训练,1/3的数据用于验证分类模型的性能.</p>
                </div>
                <div class="p1">
                    <p id="135">本文模型采用谷歌开源的Keras神经网络库,后台采用Tensorflow搭建LSTM神经网络,LSTM隐藏层单元数目设置为128,损失函数采用binary_cross-entropy,迭代次数(epoch)设置为10,激活函数采用Sigmoid,采用自适应矩估计算法(Adaptive Moment Estimation, Adam)作为优化器函数,用于优化神经网络.</p>
                </div>
                <div class="p1">
                    <p id="137">表1为各个数据集的概况,<i>T</i>表示经过处理后的数据集文本总数,<i>L</i>表示标签类别数量,<i>V</i>表示每个数据集的词典中单词数目,<i>A</i>表示平均每个样本的包含标签类别数目.</p>
                </div>
                <div class="area_img" id="138">
                    <p class="img_tit"><b>表1 实验数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Experimental datasets</p>
                    <p class="img_note"></p>
                    <table id="138" border="1"><tr><td><br />名称</td><td><i>T</i></td><td><i>L</i></td><td><i>V</i></td><td><i>A</i></td></tr><tr><td><br />Reuters-21578</td><td>13306</td><td>20</td><td>36035</td><td>1.34</td></tr><tr><td><br />IMDB</td><td>93458</td><td>27</td><td>141045</td><td>2.41</td></tr><tr><td><br />AAPD</td><td>44081</td><td>54</td><td>57067</td><td>2.15</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="139"><i>Reuters</i>-21578中最大文本输入长度为915,70%文本的最大长度为83,文本的最大标签数为8,70%文本的标签数为2.</p>
                </div>
                <div class="p1">
                    <p id="140"><i>IMDB</i>中最大文本输入长度为1 106,70%文本的最大长度为65,文本的最大标签数为12,70%文本的标签数为3.</p>
                </div>
                <div class="p1">
                    <p id="141"><i>AAPD</i>中最大文本输入长度为336,70%文本的最大长度为111,文本的最大标签数为8,70%文本的标签数为3.</p>
                </div>
                <h4 class="anchor-tag" id="142" name="142"><b>2.4 对比模型</b></h4>
                <div class="p1">
                    <p id="143">选择5种方式进行文本的特征提取,分别为<i>TF</i>-<i>IDF</i>、<i>n</i>-<i>gram</i>、<i>LDA</i>主题模型、<i>Doc</i>2<i>vec</i>、<i>LSTM</i>深度学习模型.多标签文本分类器选择<i>Binary Relevance</i>分类器(记为<i>BR</i>)、<i>Classifier Chains</i>分类器(记为<i>CC</i>)、<i>ML</i>_<i>KNN</i>分类器(K=10)(记为<i>ML</i>_<i>KNN</i>).</p>
                </div>
                <div class="p1">
                    <p id="144"><i>TF</i>-<i>IDF</i>.一种常用的统计方法,根据单词在文本中出现的次数和在整个语料中出现的文档频率计算一个单词在整个语料中的重要程度.优点是可以过滤一些常见的却无关紧要的词语,同时保留影响整个文本的重要单词.</p>
                </div>
                <div class="p1">
                    <p id="145"><i>n</i>-<i>gram</i>.将文本中的内容按照单词大小为n的滑动窗口进行操作,统计所有<i>gram</i>出现的频率,本文实验中n=2.</p>
                </div>
                <div class="p1">
                    <p id="146"><i>LDA</i>主题模型.对文档中隐含的主题进行建模,考虑上下文之间的语义关系,提取的特征不依赖单词是否出现在该文档中,本文实验中主题数目K=128.</p>
                </div>
                <div class="p1">
                    <p id="148"><i>Doc</i>2<i>vec</i>.采用<i>python</i>中的<i>gensim</i>软件包调用<i>Doc</i>2<i>vec</i>获得文档的统一向量表示,本文实验中文档的向量表示长度设置为256.</p>
                </div>
                <div class="p1">
                    <p id="149">深度学习模型.采用<i>LSTM</i>神经网络进行文档的特征提取,得到文档的统一向量表示,本文实验中文档的特征向量长度设置为128.</p>
                </div>
                <h4 class="anchor-tag" id="150" name="150"><b>2.5 实验结果</b></h4>
                <div class="p1">
                    <p id="151">本节使用3组实验验证模型的可行性和有效性.本文将传统用于多标签文本分类的特征提取算法与本文的深度主题特征提取模型在<i>SA</i>、<i>P</i>、<i>R</i>、<i>F</i>1、<i>HL</i>指标上进行对比,各数据集实验结果如表2～表4所示,↑表示值越大,模型性能越好,↓表示值越小,模型性能越好.</p>
                </div>
                <div class="p1">
                    <p id="152">在3个经典的英文文本数据集上,本文的深度主题特征提取模型无论使用<i>BR</i>分类器、<i>CC</i>分类器还是<i>ML</i>_<i>KNN</i>(K=10)分类器,绝大多数的评价指标均为最优值.其中<i>F</i>1作为重要的评价指标,相比特征提取方法都有一定提升.</p>
                </div>
                <div class="p1">
                    <p id="153">实验表明<i>LSTM</i>的语义特征和潜在主题分布特征的融合能提高多标签文本分类的性能,在<i>LSTM</i>语义特征的基础上融合文本的潜在主题分布信息,新的文本特征表示对文本的表达能力更好.无论使用<i>BR</i>分类器、<i>CC</i>分类器还是<i>ML</i>_<i>KNN</i>(K=10)分类器,融合后的特征提取算法大都优于仅单独考虑<i>LSTM</i>局部语义特征信息和主题模型的全局语义特征信息.</p>
                </div>
                <div class="p1">
                    <p id="154">本文的特征提取算法对分类效果具有较大改善,可以挖掘文本隐藏信息,实现对文本的深层次理解,在多标签文本分类中产生较好的分类性能.</p>
                </div>
                <div class="area_img" id="155">
                                            <p class="img_tit">
                                                <b>表2 各模型在Reuters</b>-<b>21578数据集上的实验结果</b>
                                                    <br />
                                                Table 2 Experimental results of different models on Reuters- 21578 dataset
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909002_15500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201909002_15500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909002_15500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 各模型在Reuters-21578数据集上的实验结果" src="Detail/GetImg?filename=images/MSSB201909002_15500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="156">
                                            <p class="img_tit">
                                                <b>表3 各模型在IMDB数据集上的实验结果</b>
                                                    <br />
                                                Table 3 Experimental results of different models on IMDB dataset
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909002_15600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201909002_15600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909002_15600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 各模型在IMDB数据集上的实验结果" src="Detail/GetImg?filename=images/MSSB201909002_15600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="157">
                                            <p class="img_tit">
                                                <b>表4 各模型在AAPD数据集上的实验结果</b>
                                                    <br />
                                                Table 4 Experimental results of different models on AAPD dataset
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909002_15700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/MSSB201909002_15700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909002_15700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表4 各模型在AAPD数据集上的实验结果" src="Detail/GetImg?filename=images/MSSB201909002_15700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h3 id="158" name="158" class="anchor-tag">3 结 束 语</h3>
                <div class="p1">
                    <p id="159">本文提出面向多标签文本分类的深度主题特征提取模型,LSTM神经网络模型获取文本的局部特征表示,LDA主题模型获取文本的全局特征表示.通过融合深度神经网络获取的文本局部特征和主题模型获取的文本全局特征,实现文本不同层次的特征提取,更全面地挖掘文本特征.实验表明,相比传统的特征提取方法,在不同的数据集中,本文的深度主题特征提取模型的分类性能具有明显提高.下一步工作考虑将提出的深度主题特征提取模型用于序列模型,以此捕获类别标签之间的相关性.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="200">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CNN-RNN:A Unif ied Framework for Multi-label Image Classification">

                                <b>[1]</b> WANG J,YANG Y,MAO J H,et al.CNN-RNN:A Unified Framework for Multi-label Image Classification // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:2285-2294.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ensemble application of convolutional and recurrent neural networks for multi-label text categorization">

                                <b>[2]</b> CHEN G B,YE D H,XING Z C,et al.Ensemble Application of Convolutional and Recurrent Neural Networks for Multi-label Text Categorization // Proc of the International Joint Conference on Neural Networks.Washington,USA:IEEE,2017:2377-2383.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Term-weighting approaches in automatic text retrieval">

                                <b>[3]</b> SALTON G,BUCKLEY C.Term-Weighting Approaches in Automatic Text Retrieval.Information Processing and Management,1988,24(5):513-523.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Information Retrieval as Statistical Translation">

                                <b>[4]</b> BERGER A,LAFFERTY J.Information Retrieval as Statistical Translation.ACM SIGIR Forum,2017,51(2):219-226.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=N-Gram-Based Text Categorization[C/OL]">

                                <b>[5]</b> CAVNAR W B,TRENKLE J M.N-Gram-Based Text Categorization[C/OL].[2019-04-22].http://www.let.rug.nl/vannoord/TextCat/textcat.pdf.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                 BLEI D M,ANDREW Y N,JORDAN M I.Latent Dirichlet Allocation.Journal of Machine Learning Research,2003,3:993-1022.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXCN201708009&amp;v=MTk4MDRZTEc0SDliTXA0OUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVnJyS1BUWEk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 陈培新,郭武.融合潜在主题信息和卷积语义特征的文本主题分类.信号处理,2017,33(8):1090-1096.) (CHEN P X,GUO W.Document Topic Categorization Combining Latent Topic Information and Convolutional Semantic Features.Journal of Signal Processing,2017,33(8):1090-1096.)
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributional Structure [C/OL]">

                                <b>[8]</b> HARRIS Z S.Distributional Structure [C/OL].[2019-04-22].http://link.springer.com/chapter/10.1007%2F978-94-009-8467-7_1.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Synopsis of Linguistic Theory, 1930-1955">

                                <b>[9]</b> FIRTH J R.A Synopsis of Linguistic Theory,1930-1955 // LANGENDOEN D T,ed.Studies in Linguistic Analysis.Oxford,UK:Philological Society,1957:1-32.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Neural Probabilistic Language Model">

                                <b>[10]</b> BENGIO Y,DUCHARME R,VINCENT P,et al.A Neural Pro-babilistic Language Model.Journal of Machine Learning Research,2003,3:1137-1155.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient estimation of word representations in vector space">

                                <b>[11]</b> MIKOLOV T,CHEN K,CORRADO G,et al.Efficient Estimation of Word Representations in Vector Space[C/OL].[2019-04-22].https://arxiv.org/pdf/1301.3781.pdf.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Glove:Global Vectors for Word Representation">

                                <b>[12]</b> PENNINGTON J,SOCHER R,MANNING C D.Glove:Global Vectors for Word Representation // Proc of the Conference on Empirical Methods in Natural Language Processing.Stroudsburg,USA:ACL,2014:1532-1543.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 LE Q V,MIKOLOV T.Distributed Representations of Sentences and Documents // Proc of the 31th International Conference on Machine Learning.Berlin,Germany:Springer,2014:1188-1196.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTQ4ODRKMThSYnhBPU5pZkpaYks5SHRqTXFvOUZaT29MRFhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> HOCHREITER S,SCHMIDHUBER J.Long Short-Term Memory.Neural Computation,1997,9(8):1735-1780.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech recognition with deep recurrent neural networks">

                                <b>[15]</b> GRAVES A,MOHAMED A,HINTON G.Speech Recognition with Deep Recurrent Neural Networks // Proc of the IEEE International Conference on Acoustics,Speech and Signal Processing.Washington,USA:IEEE,2013:6645-6649.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lear-ning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation[C/OL]">

                                <b>[16]</b> CHO K,VAN MERRIENBOER B,GULCEHRE C,et al.Lear-ning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation[C/OL].[2019-04-22].https://arxiv.org/pdf/1406.1078.pdf.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modeling Joint Entity and Relation Extraction with Table Representation">

                                <b>[17]</b> MIWA M,SASAKI Y.Modeling Joint Entity and Relation Extraction with Table Representation // Proc of the Conference on Empirical Methods in Natural Language Processing.Stroudsburg,USA:ACL,2014:1858-1869.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Document Modeling with Gated Recurrent Neural Network for Sentiment Classification">

                                <b>[18]</b> TANG D Y,QIN B,LIU T,et al.Document Modeling with Gated Recurrent Neural Network for Sentiment Classification // Proc of the Conference on Empirical Methods in Natural Language Processing.Stroudsburg,USA:ACL,2015:1422-1432.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Review on Multi-label Learning Algorithms">

                                <b>[19]</b> ZHANG M L,ZHOU Z H.A Review on Multi-label Learning Algorithms.IEEE Transactions on Knowledge and Data Engineering,2014,26(8):1819-1837.
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD121113000996&amp;v=MDIzMDNXTmo3QmFySzZIOUROckk5RlpPSUdDaE04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWnU5dUZDdnNVN2pKSlZz&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> LUACES O,DIEZ J,BARRANQUERO J,et al.Binary Relevance Efficacy for Multilabel Classification.Progress in Artificial Intelligence,2012,1(4):303-313.
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Random k-Labelsets for Multilabel Classification">

                                <b>[21]</b> TSOUMAKAS G,KATAKIS I,VLAHAVAS I.Random k-labelsets for Multilabel Classification.IEEE Transactions on Knowledge and Data Engineering,2011,23(7):1079-1089.
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Classifier Chains for Multi-label Classification">

                                <b>[22]</b> READ J,PFAHRINGER B,HOLMES G,et al.Classifier Chains for Multi-label Classification // Proc of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases.Berlin,Germany:Springer,2011:254-269.
                            </a>
                        </p>
                        <p id="244">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Kernel Method for Multi-Labelled Classification">

                                <b>[23]</b> ELISSEEFF A,WESTON J.A Kernel Method for Multi-labelled Classification // DIETTERICH T G,BECKER S,GHAHRAMANI Z,eds.Advances in Neural Information Processing Systems 14.Cambridge,USA:The MIT Press,2002:681-687.
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739502&amp;v=MDI3NTdSZEdlcnFRVE1ud1plWnVIeWptVUxiSUoxOFJieEE9TmlmT2ZiSzdIdEROcVk5RlkrZ0dDWHc3b0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b> ZHANG M L,ZHOU Z H.ML-KNN:A Lazy Learning Approach to Multi-label Learning.Pattern Recognition,2007,40(7):2038-2048.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201909002" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201909002&amp;v=MjIzMjBLRDdZYkxHNEg5ak1wbzlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1Zycks=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
