<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131439292530000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMSSB201902001%26RESULT%3d1%26SIGN%3dcNs6EFra5w3RXQPan9o%252fYeQoHNg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201902001&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201902001&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201902001&amp;v=MjM3ODM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5em5VTDdBS0Q3WWJMRzRIOWpNclk5RlpZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#69" data-title="1 基于深度卷积神经网络的混合关节肢体模型 ">1 基于深度卷积神经网络的混合关节肢体模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#78" data-title="&lt;b&gt;1.1&lt;/b&gt; 变量定义"><b>1.1</b> 变量定义</a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;1.2&lt;/b&gt; 节点外观模型"><b>1.2</b> 节点外观模型</a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;1.3&lt;/b&gt; 相邻节点间的空间关系模型"><b>1.3</b> 相邻节点间的空间关系模型</a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;1.4&lt;/b&gt; 融合得分函数模型"><b>1.4</b> 融合得分函数模型</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#110" data-title="2 数据预处理与网络结构 ">2 数据预处理与网络结构</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#111" data-title="&lt;b&gt;2.1&lt;/b&gt; 数据预处理"><b>2.1</b> 数据预处理</a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;2.2&lt;/b&gt; 节点间混合空间关系类型的预训练"><b>2.2</b> 节点间混合空间关系类型的预训练</a></li>
                                                <li><a href="#123" data-title="&lt;b&gt;2.3&lt;/b&gt; 深度卷积神经网络架构"><b>2.3</b> 深度卷积神经网络架构</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#129" data-title="3 混合关节肢体模型流程 ">3 混合关节肢体模型流程</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#137" data-title="4 实验及结果分析 ">4 实验及结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#171" data-title="5 结 束 语 ">5 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#71" data-title="图1 不同角度下或者视角缩短情况下的传统关节肢体模型">图1 不同角度下或者视角缩短情况下的传统关节肢体模型</a></li>
                                                <li><a href="#74" data-title="图2 手肘节点与其相邻节点的部分空间关系">图2 手肘节点与其相邻节点的部分空间关系</a></li>
                                                <li><a href="#118" data-title="图3 右手肘关节正负图像块样本">图3 右手肘关节正负图像块样本</a></li>
                                                <li><a href="#126" data-title="图4 DCNN架构示意图">图4 DCNN架构示意图</a></li>
                                                <li><a href="#136" data-title="图5 基于DCNN的混合关节肢体模型流程图">图5 基于DCNN的混合关节肢体模型流程图</a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;表1&lt;/b&gt;&lt;b&gt;3个基准数据集的信息&lt;/b&gt;"><b>表1</b><b>3个基准数据集的信息</b></a></li>
                                                <li><a href="#150" data-title="图6 不同阈值尺度下不同节点在2个数据集上的PDJ结果">图6 不同阈值尺度下不同节点在2个数据集上的PDJ结果</a></li>
                                                <li><a href="#150" data-title="图6 不同阈值尺度下不同节点在2个数据集上的PDJ结果">图6 不同阈值尺度下不同节点在2个数据集上的PDJ结果</a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;表2&lt;/b&gt;&lt;b&gt;阈值为0.2时关节点的PDJ结果 (LSP数据集&lt;/b&gt;) "><b>表2</b><b>阈值为0.2时关节点的PDJ结果 (LSP数据集</b>) </a></li>
                                                <li><a href="#155" data-title="图7 LSP数据集上各方法PCP结果对比">图7 LSP数据集上各方法PCP结果对比</a></li>
                                                <li><a href="#158" data-title="图8 FLIC数据集上各方法PCP结果对比">图8 FLIC数据集上各方法PCP结果对比</a></li>
                                                <li><a href="#161" data-title="图9 Image Parse数据集上各方法PCP结果对比">图9 Image Parse数据集上各方法PCP结果对比</a></li>
                                                <li><a href="#208" data-title="图1 0 本文方法在不同数据集上的估计结果">图1 0 本文方法在不同数据集上的估计结果</a></li>
                                                <li><a href="#208" data-title="图1 0 本文方法在不同数据集上的估计结果">图1 0 本文方法在不同数据集上的估计结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="10">


                                    <a id="bibliography_1" title=" BARADEL F, WOLF C, MILLE J. Human Action Recognition: Pose-Based Attention Draws Focus to Hands // Proc of the IEEE International Conference on Computer Vision Workshop. Washington, USA: IEEE, 2017: 604-613." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human Action Recognition: Pose-Based Attention Draws Focus to Hands">
                                        <b>[1]</b>
                                         BARADEL F, WOLF C, MILLE J. Human Action Recognition: Pose-Based Attention Draws Focus to Hands // Proc of the IEEE International Conference on Computer Vision Workshop. Washington, USA: IEEE, 2017: 604-613.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_2" title=" VASILEIADIS M, MALASSIOTIS S, GIAKOUMIS D, &lt;i&gt;et al&lt;/i&gt;. Robust Human Pose Tracking for Realistic Service Robot Applications // Proc of the IEEE International Conference on Computer Vision Workshop. Washington, USA: IEEE, 2017: 1363-1372." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust Human Pose Tracking for Realistic Service Robot Applications">
                                        <b>[2]</b>
                                         VASILEIADIS M, MALASSIOTIS S, GIAKOUMIS D, &lt;i&gt;et al&lt;/i&gt;. Robust Human Pose Tracking for Realistic Service Robot Applications // Proc of the IEEE International Conference on Computer Vision Workshop. Washington, USA: IEEE, 2017: 1363-1372.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_3" title=" GONG K, LIANG X D, ZHANG D Y, &lt;i&gt;et al&lt;/i&gt;. Look into Person: Self-supervised Structure-Sensitive Learning and a New Benchmark for Human Parsing // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2017: 932-940." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Look into Person: Self-supervised Structure-Sensitive Learning and a New Benchmark for Human Parsing">
                                        <b>[3]</b>
                                         GONG K, LIANG X D, ZHANG D Y, &lt;i&gt;et al&lt;/i&gt;. Look into Person: Self-supervised Structure-Sensitive Learning and a New Benchmark for Human Parsing // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2017: 932-940.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_4" title=" YANG Y, RAMANAN D. Articulated Pose Estimation with Flexible Mixtures-of-Parts // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2011: 1385-1392." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Articulated pose estimation with flexible mixtures-of-parts">
                                        <b>[4]</b>
                                         YANG Y, RAMANAN D. Articulated Pose Estimation with Flexible Mixtures-of-Parts // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2011: 1385-1392.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_5" title=" CHEN X J, YUILLE A. Articulated Pose Estimation by a Gra-phical Model with Image Dependent Pairwise Relations // GHAHRAMANI Z, WELLING M, GORTES C, &lt;i&gt;et al&lt;/i&gt;., eds. Advances in Neural Information Processing Systems 27. Cambridge, USA: The MIT Press, 2014: 1736-1744." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Articulated pose estimation by agraphical model with image dependent pairwise relations">
                                        <b>[5]</b>
                                         CHEN X J, YUILLE A. Articulated Pose Estimation by a Gra-phical Model with Image Dependent Pairwise Relations // GHAHRAMANI Z, WELLING M, GORTES C, &lt;i&gt;et al&lt;/i&gt;., eds. Advances in Neural Information Processing Systems 27. Cambridge, USA: The MIT Press, 2014: 1736-1744.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_6" title=" FISCHLER M A, ELSCHLAGER R A. The Representation and Matching of Pictorial Structures. IEEE Transactions on Computers, 1973, 22 (1) : 67-92." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The representation and matching of pictorial structure">
                                        <b>[6]</b>
                                         FISCHLER M A, ELSCHLAGER R A. The Representation and Matching of Pictorial Structures. IEEE Transactions on Computers, 1973, 22 (1) : 67-92.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_7" title=" JOHNSON S, EVERINGHAM M. Learning Effective Human Pose Estimation from Inaccurate Annotation // Proc of the 24th IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2011: 1465-1472." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning effective human pose estimation from inaccurate annotation">
                                        <b>[7]</b>
                                         JOHNSON S, EVERINGHAM M. Learning Effective Human Pose Estimation from Inaccurate Annotation // Proc of the 24th IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2011: 1465-1472.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_8" title=" TSOCHANTARIDIS I, HOFMANN T, JOACHIMS T, &lt;i&gt;et al&lt;/i&gt;. Su-pport Vector Machine Learning for Interdependent and Structured Output Spaces // Proc of the 21st International Conference on Machine Learning. New York, USA: ACM, 2004. DOI:10.1145/1015330.1015341." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Support Vector Machine Learning for Interdependent and Structured Output Spaces">
                                        <b>[8]</b>
                                         TSOCHANTARIDIS I, HOFMANN T, JOACHIMS T, &lt;i&gt;et al&lt;/i&gt;. Su-pport Vector Machine Learning for Interdependent and Structured Output Spaces // Proc of the 21st International Conference on Machine Learning. New York, USA: ACM, 2004. DOI:10.1145/1015330.1015341.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_9" title=" CHEN X J, YUILLE A L. Parsing Occluded People by Flexible Compositions // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2015: 3945-3954." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parsing occluded people by flexible compositions">
                                        <b>[9]</b>
                                         CHEN X J, YUILLE A L. Parsing Occluded People by Flexible Compositions // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2015: 3945-3954.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_10" title=" CHU X, OUYANG W L, LI H S, &lt;i&gt;et al&lt;/i&gt;. Structured Feature Lear-ning for Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2016: 4715-4723." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structured Feature Learning for Pose Estimation">
                                        <b>[10]</b>
                                         CHU X, OUYANG W L, LI H S, &lt;i&gt;et al&lt;/i&gt;. Structured Feature Lear-ning for Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2016: 4715-4723.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_11" title=" FAN X C, ZHENG K, LIN Y W, &lt;i&gt;et al&lt;/i&gt;. Combining Local Appea-rance and Holistic View: Dual-Source Deep Neural Networks for Human Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2015: 1347-1355." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Combining Local Appearance and Holistic View:Dual-Source Deep Neural Networks for Human Pose Estimation[C/OL]">
                                        <b>[11]</b>
                                         FAN X C, ZHENG K, LIN Y W, &lt;i&gt;et al&lt;/i&gt;. Combining Local Appea-rance and Holistic View: Dual-Source Deep Neural Networks for Human Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2015: 1347-1355.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_12" title=" OUYANG W L, CHU X, WANG X G. Multi-source Deep Lear-ning for Human Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2014: 2337-2344." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-source deep learning for human pose estimation">
                                        <b>[12]</b>
                                         OUYANG W L, CHU X, WANG X G. Multi-source Deep Lear-ning for Human Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2014: 2337-2344.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_13" title=" TOSHEV A, SZEGEDY C. DeepPose: Human Pose Estimation via Deep Neural Networks // Proc of the IEEE Conference on Compu-ter Vision and Pattern Recognition. Washington, USA: IEEE, 2014: 1653-1660." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepPose:Human Pose Estimation via Deep Neural Networks">
                                        <b>[13]</b>
                                         TOSHEV A, SZEGEDY C. DeepPose: Human Pose Estimation via Deep Neural Networks // Proc of the IEEE Conference on Compu-ter Vision and Pattern Recognition. Washington, USA: IEEE, 2014: 1653-1660.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_14" title=" JOHNSON S, EVERINGHAM M. Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation // Proc of the British Machine Vision Conference. Dundee, Britain: BMVA, 2010: 12.1-12.11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation">
                                        <b>[14]</b>
                                         JOHNSON S, EVERINGHAM M. Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation // Proc of the British Machine Vision Conference. Dundee, Britain: BMVA, 2010: 12.1-12.11.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_15" title=" SAPP B, TASKAR B. MODEC: Multimodal Decomposable Models for Human Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2013: 3674-3681." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MODEC:Multimodal Decomposable Models for Human Pose Estimation">
                                        <b>[15]</b>
                                         SAPP B, TASKAR B. MODEC: Multimodal Decomposable Models for Human Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2013: 3674-3681.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_16" title=" RAMANAN D. Learning to Parse Images of Articulated Bodies // SCH&#214;LKOPF B, PLATT J C, HOFFMAN T, eds. Advances in Neural Information Processing Systems 19. Cambridge, USA: The MIT Press, 2006: 1129-1136." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to parse images of articulated bodies">
                                        <b>[16]</b>
                                         RAMANAN D. Learning to Parse Images of Articulated Bodies // SCH&#214;LKOPF B, PLATT J C, HOFFMAN T, eds. Advances in Neural Information Processing Systems 19. Cambridge, USA: The MIT Press, 2006: 1129-1136.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_17" title=" PISHCHULIN L, ANDRILUKA M, GEHLER P, &lt;i&gt;et al&lt;/i&gt;. Poselet Conditioned Pictorial Structures // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2013: 588-595." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Poselet conditioned pictorial structures">
                                        <b>[17]</b>
                                         PISHCHULIN L, ANDRILUKA M, GEHLER P, &lt;i&gt;et al&lt;/i&gt;. Poselet Conditioned Pictorial Structures // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2013: 588-595.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_18" title=" RAMAKRISHNA V, MUNOZ D, HEBERT M, &lt;i&gt;et al&lt;/i&gt;. Pose Machines: Articulated Pose Estimation via Inference Machines // Proc of the 13th European Conference on Computer Vision. Berlin, Germany: Springer, 2014, II: 33-47." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pose Machines:Articulated Pose Estimation via Inference Machines">
                                        <b>[18]</b>
                                         RAMAKRISHNA V, MUNOZ D, HEBERT M, &lt;i&gt;et al&lt;/i&gt;. Pose Machines: Articulated Pose Estimation via Inference Machines // Proc of the 13th European Conference on Computer Vision. Berlin, Germany: Springer, 2014, II: 33-47.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_19" title=" YANG W, OUYANG W L, LI H S, &lt;i&gt;et al&lt;/i&gt;. End-to-End Learning of Deformable Mixture of Parts and Deep Convolutional Neural Networks for Human Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2016: 3073-3082." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-End Learning of Deformable Mixture of Parts and Deep Convolutional Neural Networks for Human Pose Estimation">
                                        <b>[19]</b>
                                         YANG W, OUYANG W L, LI H S, &lt;i&gt;et al&lt;/i&gt;. End-to-End Learning of Deformable Mixture of Parts and Deep Convolutional Neural Networks for Human Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2016: 3073-3082.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_20" title=" TOMPSON J J, JAIN A, LECUN Y, &lt;i&gt;et al&lt;/i&gt;. Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation // GHAHRAMANI Z, WELLING M, CORTES C, &lt;i&gt;et al&lt;/i&gt;., eds. Advances in Neural Information Processing Systems 27. Cambridge, USA: The MIT Press, 2014: 1799-1807." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint training of a convolutional network and a graphical model for human pose estimation">
                                        <b>[20]</b>
                                         TOMPSON J J, JAIN A, LECUN Y, &lt;i&gt;et al&lt;/i&gt;. Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation // GHAHRAMANI Z, WELLING M, CORTES C, &lt;i&gt;et al&lt;/i&gt;., eds. Advances in Neural Information Processing Systems 27. Cambridge, USA: The MIT Press, 2014: 1799-1807.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(02),97-107 DOI:10.16451/j.cnki.issn1003-6059.201902001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于混合关节肢体模型的深度人体姿态估计方法</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E7%A7%89%E7%80%9A&amp;code=06681419&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘秉瀚</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%8C%AF%E8%BE%BE&amp;code=41251070&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李振达</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9F%AF%E9%80%8D&amp;code=26357091&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">柯逍</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6%E6%95%B0%E5%AD%A6%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0094575&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福州大学数学与计算机科学学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6%E7%A6%8F%E5%BB%BA%E7%9C%81%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97%E4%B8%8E%E6%99%BA%E8%83%BD%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福州大学福建省网络计算与智能信息处理重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出基于关节外观和关节间空间关系的模型与深层神经网络结构 (DCNN) 相结合的混合模型, 解决人体姿态估计问题.首先, 对人体构建图像模型以表示人体关节与肢体.然后, 根据标注信息将图像分解为以关节为中心的若干图像块, 作为训练输入数据.最后, 得到一个可以解决多个分类的DCNN网络, 用于人体姿态估计.文中方法对人体表示更灵活, 有效提升关节点的检测率及正确检测的比率.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人体姿态估计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像模型;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    刘秉瀚, 硕士, 教授, 主要研究方向为智能视频处理与分析.E-mail:lbh@fzu.edu.cn.
;
                                </span>
                                <span>
                                    李振达, 硕士研究生, 主要研究方向为计算机视觉.E-mail:877468204@qq.com.
;
                                </span>
                                <span>
                                    *柯逍, 博士, 副教授, 主要研究方向为计算机视觉、模式识别.E-mail:kex@fzu.edu.cn.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-03</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (No.61502105, 61672159);</span>
                                <span>福建省科技引导性项目 (No.2017H0015);</span>
                                <span>福建省高校产学合作项目 (No.2017H6008) 资助;</span>
                    </p>
            </div>
                    <h1>Deep Human Pose Estimation Method Based on Mixture Articulated Limb Model</h1>
                    <h2>
                    <span>LIU Binghan</span>
                    <span>LI Zhenda</span>
                    <span>KE Xiao</span>
            </h2>
                    <h2>
                    <span>College of Mathematics and Computer Science, Fuzhou University</span>
                    <span>Fujian Provincial Key Laboratory of Networking Computing and Intelligent Information Processing, Fuzhou University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A flexible mixture model is proposed to solve the problems of human pose estimation. The model is composed of joint appearance and inner-joint relationship models, and it is trained through a deep convolutional neural network (DCNN) . Firstly, a graphical model is constructed to represent joints and limbs of human body. Secondly, images are decomposed into several image blocks centered on the joints and used as training input data. Finally, a multiple classification DCNN network is obtained to perform human pose estimation.The proposed method is more flexible for human body representation, and the detection rate of joint points and the correct detection rate are effectively improved.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Convolutional%20Neural%20Network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Convolutional Neural Network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Human%20Pose%20Estimation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Human Pose Estimation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Graphical%20Model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Graphical Model;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LIU Binghan, master, professor. Her research interests include intelligent video processing and analysis.
;
                                </span>
                                <span>
                                    LI Zhenda, master student. His research interests include computer vision.
;
                                </span>
                                <span>
                                    KE Xiao , Ph. D., associate professor. His research interests include computer vision and pattern recognition.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-07-03</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China (No.61502105, 61672159);</span>
                                <span>Technology Guided Project of Fujian Province (No.2017H0015);</span>
                                <span>College-University Cooperation Project of Fujian Province (No.2017H6008);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="63">人体姿态估计在计算机视觉领域中是一项有意义的研究任务, 因为在计算机视觉更高层次的进展, 如动作识别<citation id="211" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、行人追踪<citation id="210" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、行人衣着属性解析<citation id="209" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>及视频分析等方面, 人体的姿态估计都起到关键作用.可以说, 这些进展及应用都是以人体姿态估计为基础.</p>
                </div>
                <div class="p1">
                    <p id="64">近些年, 关节姿态估计是人体姿态估计的主流方式, 以人体关节为中心出发点, 预估人体姿态, 利用图像模型<citation id="212" type="reference"><link href="16" rel="bibliography" /><link href="18" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>对人体关节及肢体进行建模, 图形节点表示人体的关节点或身体部分, 联结图形节点的边表示节点之间的空间关系.树型结构模型<citation id="215" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>是目前在人体姿态估计领域适配图像模型较优的结构模型, 通过将图像模型转化为树形结构模型, 实现处理计算的高效性, Yang等<citation id="214" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出灵活的混合模型, 捕捉部件之间的上语境共现关系.Johnson等<citation id="213" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>使用一系列身体部位探测器, 获得完整模型的混合模型.</p>
                </div>
                <div class="p1">
                    <p id="65">目前关节姿态估计的主流方法大致可分为传统机器学习方法与深度学习方法.</p>
                </div>
                <div class="p1">
                    <p id="66">进行关节姿态估计的传统机器学习代表方法之一为支持向量机 (SVM) .SVM<citation id="216" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>是目前机器学习方法中最常用、效果最好的分类器之一, 这是因为SVM本身的优化目标的结构化风险最小, 因此对于训练过程中过拟合的情况可采取有效措施, 通过几何间隔的概念, 得到数据分布的结构化描述, 减低对数据分布的要求.</p>
                </div>
                <div class="p1">
                    <p id="67">但是姿态分类是一个明显的非线性多分类问题, 在姿态估计方法中, 因为考虑到从数据集中学习关节点的外观及相邻节点间的空间关系, 即使针对某种关节, 该关节具有的外观及相邻节点间的空间关系的表示类型都不为单一种类.其次, 本文数据集每类对应的样本数量也存在差异, 对应的特征分布也不规则, 整个多分类的性质是复杂的.这对于SVM是个较困难的任务, 面对如此多特征类别时, 即使加入核函数及松弛变量映射到高维空间, 特征分布界限依然无明确的规则界限, 导致SVM在得到对应的支持向量时, 还是会存在较多错误归类的情况, 而且在映射过程中, 核函数的定义十分苛刻.如果采用多SVM结合的方式进行多分类, 假设其中一种分类器做出错误分类, 会导致最终结果也错误, 因此偏差值较大.此外, 本文实验中使用的训练样本数量规模也较大, 在大规模样本数量下, SVM的训练时间较长, 计算代价较大, 在训练后期容易出现梯度消失的情况, 对数据缺失较敏感.</p>
                </div>
                <div class="p1">
                    <p id="68">针对SVM出现的因数据量庞大而导致训练无法收敛的问题及需要训练多个二分类器才能进行多分类预测的问题, 本文提出基于混合关节肢体模型的深度人体姿态估计方法.通过构建深度卷积神经网络, 进行姿态类型分类预测训练, 构建多分类深层卷积神经网络 (Deep Convolutional Neural Network, DCNN) <citation id="217" type="reference"><link href="26" rel="bibliography" /><link href="28" rel="bibliography" /><link href="30" rel="bibliography" /><link href="32" rel="bibliography" /><link href="34" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>.相比SVM, DCNN具有如下优势:1) 采用依靠局部连接性及权值共享性的策略, 构建深层模型, 在样本特征多且复杂的情况下, 依旧具有良好的表达能力, 能够表示复杂的分类模型逼近复杂非线性关系.2) 在特征提取方面, 通过卷积及池化操作, 不断学习特征, 最终得到表达能力较强的高级深度特征, 对于非线性多分类情况下的样本特征具有较强的表示能力, 卷积层的稀疏连接也大幅减小训练过程的参数数量.3) 通过激活层、批标准化 (Batch Normalization) 等操作, 在大规模数据情况下, 在训练后期, 依旧具有良好的收敛能力.</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag">1 基于深度卷积神经网络的混合关节肢体模型</h3>
                <div class="p1">
                    <p id="70">传统的关节姿态估计模型通常考虑直接对全身进行整体的姿态估计, 但是对于直接估计全身姿态的情况, 因视角变化或其它一些干扰因素, 全身姿态估计模型会出现太多的可能性角度变化而难以预测.而且, 因为衣着及身体形态的多变性导致外形的种类具有太多的变化, 导致每种变化模型都需要对其进行建模, 如因为拍摄视角导致的平面呈现的方向及缩短等.图1所示为传统关节姿态估计中训练使用到的肢体在不同角度下或视角缩短情况下的一种表现.</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902001_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 不同角度下或者视角缩短情况下的传统关节肢体模型" src="Detail/GetImg?filename=images/MSSB201902001_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 不同角度下或者视角缩短情况下的传统关节肢体模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902001_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Traditional articulated limb models from different perspectives or shortened perspective</p>

                </div>
                <div class="p1">
                    <p id="73">本文针对传统关节肢体模型存在的问题, 提出灵活的混合关节肢体模型 (Mixture Articulated Limb Model, MALM) , 以多分类的形式解决姿态估计问题, 从局部的关节点进行回归, 得到对应的姿态模型, 进而组合形成全身姿态估计模型.对于关节肢体的建模不采用如图1所示由肢体方向进行固定的方式, 而是对每个关节点通过成对相邻节点间的空间关系考虑一个混合集 (即对每对相邻的节点, 每种不同类型空间关系都表示一类姿态实例, 对这些实例抽象中心关节与其相邻关节间的空间关系, 组合构成混合姿态类型表示该节点的姿态类型) , 具体如图2所示.这样的定义允许只通过对某一节点周围的局部图像块进行观察学习, 就能依据这些相对空间关系推断得到该节点相邻节点的相对位置.</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902001_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 手肘节点与其相邻节点的部分空间关系" src="Detail/GetImg?filename=images/MSSB201902001_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 手肘节点与其相邻节点的部分空间关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902001_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Partial spatial relationships between elbows and their neighbor parts</p>

                </div>
                <div class="p1">
                    <p id="76">当关节姿态受到衣着或视角变化的影响时, 直观体现在某个肢体以不同的姿态呈现在观察者的视野.例如:当观察者站在被观察对象不同方位——正前方或侧方, 被观察对象的右臂肢体以图像的形式呈现时将有不同结果;而衣着的影响体现在人体的肢体被衣物遮挡.本文提出融合关节外观及关节间空间关系的混合模型, 通过关节外观的置信度及节点间空间关系的适配性, 预测关节及其相应姿态.节点外观模型描述的是给定一个像素坐标中心位于<i>l</i><sub><i>i</i></sub>的图像块, 节点外观模型可以直观通过该节点的混合姿态类型集<i>t</i><sub><i>i</i></sub>度量<i>i</i>节点局部外观的置信度, 而置信度由DCNN最终得到的损失输出表示, 即通过置信度可以首先判断该关节点的类型, 关节间空间关系模型度量两个节点之间的空间适配性, 即两个关节位于同个肢体的置信度大小.通过某个关节点可以依靠局部图像块, 判断相邻关节的相对位置.因此, 在给出如图2所示以右手肘关节为中心的局部图像块时, 首先通过外观模型, 对右手肘关节做出预测, 再结合关节间的空间关系模型, 判断右肩及右手腕关节相对位置, 就可以对整个右手臂肢体做出预测, 得到对应的姿态结果.当肢体存在被衣着小部分遮挡时, 通过某个关节的混合姿态类型度量得到的置信度结果依旧可以预测该关节的类型, 而被遮挡较多时, 对未被遮挡的关节做出预测, 再通过关节间空间关系模型可以预测其相邻节点的相对空间位置.同样在视角变化时, 依靠同样的机制做出预测.</p>
                </div>
                <div class="p1">
                    <p id="77">本文定义一个融合节点外观及相邻节点间空间关系的混合关节肢体模型, 对输入图像进行姿态预估.人体姿态估计实质上可以看作多分类问题, DCNN通过前向传播得到的softmax函数结果, 对比原始图像数据映射到不同类目的得分情况, 同时将得分结果的指数域映射到总和为1的概率域, 大小顺序表示所属类别的得分排序, 得分最高即为预测结果.而且DCNN的softmax函数得到的结果绝对值大小还能表示对应每个类别的概率, 因此可以量化模型预测结果和实际结果之间的吻合度, 实现多分类预测.</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78"><b>1.1</b> 变量定义</h4>
                <div class="p1">
                    <p id="79">构建图像模型<i>G</i>= (<i>V</i>, <i>E</i>) 直观表示一个人体模型, 其中, <i>V</i>表示人体的关节点或身体某部位, <i>E</i>表示节点之间联结的边, <i>E</i>∈<i>V</i>×<i>V</i>, 表示相邻节点之间的空间关系, 令<i>K</i>=<i>V</i>表示关节数量.定义<i>I</i>表示图像, <i>i</i>表示该图像中第<i>i</i>个节点, <i>l</i>表示节点的像素坐标, <i>t</i>表示节点的混合空间关系 (由不同姿态实例聚类抽象并组合得到) .根据图像模型中的定义, 有<i>i</i>=1, 2, …, <i>K</i>, 定义<i>l</i><sub><i>i</i></sub>=1, 2, …, <i>L</i>, <i>t</i><sub><i>i</i></sub>=1, 2, …, <i>T</i>, <i>l</i><sub><i>i</i></sub>表示节点<i>i</i>的像素坐标{ (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) }, <i>t</i><sub><i>i</i></sub>表示节点<i>i</i>与其相邻节点的空间关系类型集 (即该节点处的<i>T</i>个姿态类型) .</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80"><b>1.2</b> 节点外观模型</h4>
                <div class="p1">
                    <p id="81">给定一个像素坐标中心位于<i>l</i><sub><i>i</i></sub>的图像块, 节点外观模型可以直观通过该节点的空间关系类型集<i>t</i><sub><i>i</i></sub>度量<i>i</i>节点局部外观的置信度, 其中<i>t</i><sub><i>i</i></sub>为节点<i>i</i>的姿态类型聚类集合, 用于表示该节点的混合姿态类型以适应不同姿态场景, 节点外观得分函数模型定义如下:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>F</mi><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>Ι</mi><mo>;</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>V</mi></mrow></munder><mtext>ϕ</mtext></mstyle><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>Ι</mi><mo>;</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>‚</mo></mtd></mtr><mtr><mtd><mtext>ϕ</mtext><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>Ι</mi><mo>;</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mi>ln</mi><mspace width="0.25em" /><mi>p</mi><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>Ι</mi><mo>;</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">通过DCNN训练外观模型ϕ (<i>l</i><sub><i>i</i></sub>, <i>t</i><sub><i>i</i></sub><i>I</i>;<i>θ</i>) , 其中<i>p</i> (<i>l</i><sub><i>i</i></sub>, <i>t</i><sub><i>i</i></sub><i>I</i>;<i>θ</i>) 为通过DCNN中前向传播softmax函数最终计算得到的得分结果映射得到的概率域, 用于预测在图像<i>I</i>中节点部分<i>i</i>的混合姿态类型为<i>t</i><sub><i>i</i></sub>并且像素坐标位于<i>l</i><sub><i>i</i></sub>的概率, <i>θ</i>为模型的参数.</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84"><b>1.3</b> 相邻节点间的空间关系模型</h4>
                <div class="p1">
                    <p id="85">空间关系模型描述2个不同的节点<i>i</i>和<i>j</i>在空间上的适配性, 即这2个相邻节点处于同一肢体的置信度大小, 同时描述相邻节点间的混合空间关系类型 (即中心节点<i>i</i>的姿态类型) .该模型定义如下:</p>
                </div>
                <div class="p1">
                    <p id="86"><i>F</i> (<i>l</i><sub><i>i</i></sub>, <i>l</i><sub><i>j</i></sub>, <i>t</i><sub><i>i</i></sub>, <i>t</i><sub><i>j</i></sub><i>I</i>;<mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ω</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msubsup><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mtable columnalign="left"><mtr><mtd><mi>i</mi><mo>∈</mo><mi>E</mi></mtd></mtr><mtr><mtd><mi>j</mi><mo>∈</mo><mi>E</mi></mtd></mtr></mtable></munder><mrow></mrow></mstyle><mi>ω</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msubsup><mi>φ</mi><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>l</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml><i>I</i>) ,      (1) </p>
                </div>
                <div class="p1">
                    <p id="88"><i>φ</i> (<i>l</i><sub><i>i</i></sub>, <i>l</i><sub><i>j</i></sub><i>I</i>) =〈<i>d</i> (<i>l</i><sub><i>i</i></sub>-<i>l</i><sub><i>j</i></sub>) 〉.      (2) </p>
                </div>
                <div class="p1">
                    <p id="89">对空间关系模型加入标准二次变化, 定义</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>〈</mo><mi>d</mi><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>l</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>〉</mo><mo>=</mo><mo stretchy="false">[</mo><mi>d</mi><mi>x</mi><mtext> </mtext><mi>d</mi><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mi>d</mi><mi>y</mi><mtext> </mtext><mi>d</mi><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">]</mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>‚</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">而<i>dx</i>=<i>x</i><sub><i>i</i></sub>-<i>x</i><sub><i>j</i></sub>, <i>dy</i>=<i>y</i><sub><i>i</i></sub>-<i>y</i><sub><i>j</i></sub>表示节点<i>i</i>关于节点<i>j</i>的相对像素定位.因为本文的姿态估计针对二维平面图像, 而二维平面图像以矩阵的形式存储, 因此图像中的像素点在矩阵中都有坐标定位 (<i>x</i>, <i>y</i>) .对于任意一个肢体部分, 都可以简单使用肢体两端的关节点的相对坐标位置表示, 可以直观表示两个关节间肢体的状态, 式 (1) 中相邻节点间的空间关系是关于权重及相邻节点间的相对空间位置的函数, 其中相对空间位置是关于肢体两端像素坐标的二次型函数, 此处加入标准二次形变进行约束.式 (2) 表示标准二次形变后关于该二次函数的特征矩阵, 在处理过程中, 相比原始函数, 特征矩阵的形式更易于存储.通过特征矩阵, 可较易转化为原始二次型进行计算.<i>ω</i><mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msubsup></mrow></math></mathml>为对2个节点的混合类型 (<i>t</i><sub><i>i</i></sub>, <i>t</i><sub><i>j</i></sub>) 进行编码的权重参数.</p>
                </div>
                <div class="p1">
                    <p id="93">训练相邻节点空间关系模型时利用直观语义关系.通常在我们的认知中, 如果关节点相邻, 如手肘节点和肩部节点及手腕节点, 可以较易想象这些节点的空间关系, 相邻部分<i>i</i>∈<i>E</i>, <i>j</i>∈<i>E</i>可以仅使用局部信息预测它们的相对空间位置.如图2所示, 给定一个以手肘关节为中心的局部图像块, 可以可靠预测相邻肩关节及手腕关节的相对位置.</p>
                </div>
                <h4 class="anchor-tag" id="94" name="94"><b>1.4</b> 融合得分函数模型</h4>
                <div class="p1">
                    <p id="95">通过融合节点外观得分函数模型及相邻节点间空间关系的得分函数模型, 得到一个完整的得分函数模型, 达到对关节点类型及相邻节点间姿态类型的预测, 完整的得分函数定义如下:</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>F</mi><mo stretchy="false"> (</mo><mi>l</mi><mo>, </mo><mi>t</mi><mo stretchy="false">|</mo><mi>Ι</mi><mo>;</mo><mi>θ</mi><mo>, </mo><mi>ω</mi><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>V</mi></mrow></munder><mtext>ϕ</mtext></mstyle><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97"><i>I</i>;<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><mo stretchy="false">) </mo><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>∈</mo><mi>E</mi></mrow></munder><mi>φ</mi></mstyle><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>l</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml><i>I</i>, <i>ω</i><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msubsup></mrow></math></mathml>) .</p>
                </div>
                <div class="p1">
                    <p id="100">模型优化的最终目的应该是融合得分函数有最大值, 即找到最优情况下的各节点的像素定位<i>l</i><sup>*</sup>及该节点的姿态类型集<i>t</i><sup>*</sup>, 定义如下:</p>
                </div>
                <div class="p1">
                    <p id="101"><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi>l</mi><msup><mrow></mrow><mo>*</mo></msup><mo>, </mo><mi>t</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>l</mi><mo>, </mo><mi>t</mi></mrow></munder><mspace width="0.25em" /><mi>F</mi><mo stretchy="false"> (</mo><mi>l</mi><mo>, </mo><mi>t</mi></mrow></math></mathml><i>I</i>;<i>θ</i>, <i>ω</i>) .</p>
                </div>
                <div class="p1">
                    <p id="103">采用最大和算法 (Max-Sum Algorithm) 计算得分函数的最大值.最大和算法已广泛用于推断图形模型中的最佳配置, 在不同配置情况下, 即遍历节点在不同的<i>l</i><sub><i>i</i></sub>、<i>t</i><sub><i>i</i></sub>情况下, 通过最大和计算该配置下消息传递到根节点的最终得分.尽管最大和算法仅仅是一个近似值, 并且在循环结构上不能保证收敛性, 但它仍然提供较好的实验结果.</p>
                </div>
                <div class="p1">
                    <p id="104">树型图像结构在最大和算法中是一个特别的例子, 将消息从叶节点传递到选择的根节点.当消息传递到根节点时, 即得到某种有最佳得分的姿态配置结果, 因为通常检测结果是多个有重叠区域的边界框 (Bounding Boxes) , 可以使用非极大值抑制 (Non-maximum Suppression, NMS) , 迭代消除冗余的盒区域, 最终得到的结果互相之间不相交.对于能够取得最大得分的下标arg max <i>l</i><sup>*</sup><sub><i>k</i></sub>, <i>t</i><sup>*</sup><sub><i>k</i></sub>进行追踪, 即在该最佳配置的情况下可以通过回溯的方法 (从根节点出发到叶节点) 找到这个下标对应的像素定位及类型.</p>
                </div>
                <div class="p1">
                    <p id="105">因为引入树型图像模型, 可以使用动态规划的方法有效计算整个最优化过程, 定义<i>K</i> (<i>i</i>) 表示节点<i>i</i>的子节点集, <i>M</i><sub><i>ij</i></sub> (<i>l</i><sub><i>j</i></sub>, <i>t</i><sub><i>j</i></sub>) 表示从节点<i>i</i>传递到节点<i>j</i>的信息, <i>C</i><sub><i>i</i></sub> (<i>l</i><sub><i>i</i></sub>, <i>t</i><sub><i>i</i></sub>) 表示节点<i>i</i>的一个置信度 (通过得分函数结果衡量) , <i>α</i><sub><i>M</i></sub>、<i>α</i><sub><i>C</i></sub>表示计算softmax函数结果过程的正则化参数, 则利用最大和寻找最优配置的方式为如下形式:</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>Μ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mi>α</mi><msub><mrow></mrow><mi>Μ</mi></msub><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mo stretchy="false"> (</mo><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>φ</mi><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>l</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>Ι</mi><mo>;</mo><mi>ω</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msubsup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>α</mi><msub><mrow></mrow><mi>C</mi></msub><mo stretchy="false"> (</mo><mtext>ϕ</mtext><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>Ι</mi><mo>;</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>∈</mo><mi>Κ</mi><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></munder><mi>Μ</mi></mstyle><msub><mrow></mrow><mrow><mi>k</mi><mi>i</mi></mrow></msub><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">假设动态规划已经进行到达到结束条件的最后一次, 此时每个节点的姿态配置都是最优解的近似值, 那么此时最优情况可以通过</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi>l</mi><msup><mrow></mrow><mo>*</mo></msup><mo>, </mo><mi>t</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mspace width="0.25em" /><mi>C</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mo>∀</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Κ</mi></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">得到, 即得到要求解的度量函数达到最大值时的姿态配置.</p>
                </div>
                <h3 id="110" name="110" class="anchor-tag">2 数据预处理与网络结构</h3>
                <h4 class="anchor-tag" id="111" name="111"><b>2.1</b> 数据预处理</h4>
                <div class="p1">
                    <p id="112">本文采用监督学习的方式进行训练, 模型参数的学习过程中输入的正样例为带有标注信息的, 即样例为{<i>I</i><sup><i>n</i></sup>, <i>l</i><sup><i>n</i></sup>, <i>t</i><sup><i>n</i></sup>}, 这些标注包括节点的像素定位及节点类型, 而负样例无任何标注信息, 即{<i>I</i><sup><i>n</i></sup>}.</p>
                </div>
                <div class="p1">
                    <p id="113">在DCNN的训练过程中, 首先对图像进行预处理, 根据输入图像的标注信息, 以每个节点的像素定位为中心, 产生一系列带有不同空间关系的局部图像块 (每个图像块的姿态标签对应标注好的标签信息) 作为正样本, 而作为负样本的图像块只包含背景.正负图像块样本如图3所示, 并进行去均值处理.</p>
                </div>
                <div class="p1">
                    <p id="114">为了降低训练过程中的过拟合现象, 对训练样例进行360°旋转及水平的镜像操作, 增加样本多样性.对于LSP及FLIC数据集, 在原有的关节点标注的基础上, 在处于同一肢体的相邻节点之间增加一个中间节点 (LSP得到26个节点, FLIC得到18个节点) , 发现对于视角变换的场景及性能提升具有帮助.随即使用这些局部图像块对DCNN进行训练各姿态类型的分类器.现有的数据集, 如LSP (Leeds Sports Poses) 数据集及FLIC (Frames Labeled in Cinema) 数据集, 通常带有单个人体关节点的位置信息及类型标签.由于本文方法考虑某个节点形成的姿态类型集, 因此考虑采用聚类方式得到混合姿态类型集群.</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902001_11800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 右手肘关节正负图像块样本" src="Detail/GetImg?filename=images/MSSB201902001_11800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 右手肘关节正负图像块样本  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902001_11800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Positive and negative image patch samples of right elbow joint</p>
                                <p class="img_note"> (b) 负样本 (b) Negative samples</p>

                </div>
                <h4 class="anchor-tag" id="120" name="120"><b>2.2</b> 节点间混合空间关系类型的预训练</h4>
                <div class="p1">
                    <p id="121">本文考虑各个关节点都有混合姿态类型集合以达到在不同姿态情况下关节点都能准确预测的目的, 关节点间不同的空间关系可以表示为不同姿态, 因此, 这个类型集合可以定义为该关节点与其相邻关节点不同相对位置的一个混合姿态类型集群.如图2所示, 手肘节点与相邻的手腕及肩部节点的不同空间关系构成一个混合姿态集.本文使用<i>K</i>-means聚类的方式, 得到<i>T</i>个聚类, 实验中对每个关节点的聚类结果数量都定义为<i>T</i>=6.通过在聚类数量为不同值时进行对比, 当<i>T</i>=6时取得的效果最好.</p>
                </div>
                <div class="p1">
                    <p id="122">令<i>r</i><sub><i>ij</i></sub>为从关节点<i>i</i>到其相邻节点部分<i>j</i>的相对坐标位置, 其中关节点<i>i</i>为聚类中心, 将训练集上的相对位置<i>r</i><sub><i>ij</i></sub>聚类为<i>T</i><sub><i>i</i></sub>个集群, 每个聚类集群对应于一组具有相似相对坐标位置的关节点部分实例.基于这些聚类得到的集群实例, 可以通过对应的关节坐标标注信息得到每个关节点对应的混合姿态类型集合, 形成该关节点的混合姿态类型.从像素定位获得的混合姿态类型与关节点的外观密切相关.</p>
                </div>
                <h4 class="anchor-tag" id="123" name="123"><b>2.3</b> 深度卷积神经网络架构</h4>
                <div class="p1">
                    <p id="124">DCNN主要包含5个卷积层, 2个最大池化层, 3个全连接层, 其中还有正则化层 (Normalization) 、丢弃层 (Dropout) 及激活层 (Activation) , 用于辅助训练过程的收敛.具体来说可以切分为12层, 分别为数据层, 2个卷积层 (包含最大池化层及局部响应正则化层, 对特征进行池化及归一化操作, 加快训练过程) , 3个纯卷积层, 2个全连接层 (包含dropout层) , 1个全连接层组成, 最终输出<i>S</i>维度的softmax结果.整个网络的优化方法采用随机梯度下降, 计算损失的方法选择softmax损失 (Softmax Loss) , 作为输入的图像块 (给定带标注的局部图像块作为正样本和背景图像块作为负样本) , 定义大小为36×36, 每批的数据批尺寸 (Batch Size) 设置为512, 初始的学习率为0.005, 学习率的衰减系数设置为0.1, 丢弃比率 (Dropout) 为0.5, 卷积层采用的激活函数是目前防止过拟合现象效果较强的Relu函数, 卷积的步长都是1.DCNN架构图概要如图4所示.</p>
                </div>
                <div class="p1">
                    <p id="125">DCNN具体结构及配置如下:第1层为3通道的36×36的原始图像输入层, ;第2层为卷积层, 卷积核大小为5×5, 步长为1;第3层为最大池化层, 卷积核大小为3×3, 步长为2;第4层为正则化层, local_size定义为5;第5层为卷积层, 卷积核大小为3×3, 步长为1;第6层为最大池化层, 卷积核大小为3×3, 步长为2;第7层为正则化层, local_size定义为5;第8层为卷积层, 卷积核大小为3×3, 步长为1;第9层为卷积层, 卷积核大小为3×3, 步长为1;第10层为卷积层, 卷积核大小为3×3, 步长为1;第11层为全连接层, 输出维度为4 096;第12层为全连接层, 输出维度为4 096;第13层为全连接层, 输出维度为9 699;最后一层为softmax输出层.</p>
                </div>
                <div class="area_img" id="126">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902001_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 DCNN架构示意图" src="Detail/GetImg?filename=images/MSSB201902001_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 DCNN架构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902001_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Architecture of DCNN</p>

                </div>
                <div class="p1">
                    <p id="127">训练时的配置信息如下:设定训练时大小为每批256, 测试时大小为每批100, 初始学习率为0.003, 最大迭代次数为60 000, 动量为0.9, 权值衰减率为0.000 5, 优化方式为随机梯度下降 (Stochastic Gradient Descent, SGD) .</p>
                </div>
                <div class="p1">
                    <p id="128">选取DCNN网络结构的原因如下.1) 该网络结构以Alexnet为基础, 减小卷积过程中卷积核的大小, 因此, 相比原始架构, 卷积后得到的特征图在维度上得到减小, 训练过程中的代价也就相应减小, Alexnet使用多种机制使训练性能及在通用性上都具有较强的表现.例如使用Relu激活函数, Relu函数不仅在训练速度上具有良好的表现, 在一定程度上可以解决梯度消失的问题.2) 在全连接层之后引入的Dropout层, 减小模型过拟合的问题.3) 通过数据增强的操作, 提升网络模型的泛化性.虽然视觉几何组网络 (Visual Geo-metry Group Network, VGG) 采用多个小尺寸卷积核连续计算, 在一定程度上减小参数庞大的问题, 可是由于层数更多, 对于得到的特征图进行堆叠操作, 通道数增加, 计算量远大于Alexnet.Googlenet因为网络更深, 训练后期梯度容易消失, 而且这些更深层次的网络通常需要更大规模的训练数据集, 在有限训练集的情况下, 因为训练数量的不足, 容易出现过拟合的现象, 导致训练效果不佳.并且网络庞大也会导致计算量同样旁大的问题.而本文采用改进后的Alexnet, 在参数规模及计算量规模方面, 具有一定程度的优势, 针对本文使用的公共数据集, 在训练时不会因为训练集样本的限制而出现网络过拟合的情况.</p>
                </div>
                <h3 id="129" name="129" class="anchor-tag">3 混合关节肢体模型流程</h3>
                <div class="p1">
                    <p id="130">混合关节肢体模型的训练测试过程大致描述如下.</p>
                </div>
                <div class="p1">
                    <p id="131">1) 以图像模型的形式对训练测试数据集中的图像构建关节肢体模型表示姿态, 其中, 点表示关节, 联结的边表示节点间的空间关系, 并且引入树形结构模型, 将图像模型的图结构转化为树型结构表示, 以便计算上的高效性.</p>
                </div>
                <div class="p1">
                    <p id="132">2) 对已经以树形结构模型表示的图像进行预处理, 即对关节间的空间关系模型进行预训练, 以每个关节为中心, 产生一系列表示节点间空间关系的局部图像块.为了防止训练过程中出现过拟合的现象, 通过镜像旋转等操作, 增加训练样本的泛化行.</p>
                </div>
                <div class="p1">
                    <p id="133">3) 因为人体姿态估计可以看作一种分类问题, 因此通过<i>K</i>-means聚类方法, 根据中心关节与相邻关节的空间相对位置, 对预处理得到的局部图像块进行聚类操作, 每个关节得到<i>T</i>个聚类结果, 每个集群表示一类空间关系类型实例.这些实例具有相似的相对空间关系, 通过这些实例结合标注的姿态信息, 得到<i>T</i>个聚类簇分别对应的姿态类型.这些实例组合用于表示节点的混合姿态类型, 以这些混合姿态类型表示某个关节的姿态类型, 得到混合关节肢体模型.</p>
                </div>
                <div class="p1">
                    <p id="134">4) 将这些聚类得到的关节局部图像块作为正样例组合, 建立混合关节肢体模型, 同时将INRIA数据集中得到的局部图像块作为负样例输入网络.利用DCNN网络进行训练, 通过得分函数模型将图像映射到不同的姿态类型, 根据标注信息, 通过损失函数调整权重参数, 使映射的得分结果与实际类别吻合, 完成对姿态类型的分类, 由此得到DCNN多分类模型.</p>
                </div>
                <div class="p1">
                    <p id="135">整个流程框架如图5所示.</p>
                </div>
                <div class="area_img" id="136">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902001_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 基于DCNN的混合关节肢体模型流程图" src="Detail/GetImg?filename=images/MSSB201902001_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 基于DCNN的混合关节肢体模型流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902001_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Training and testing process of DCNN based mixture-of-parts-limbs model</p>

                </div>
                <h3 id="137" name="137" class="anchor-tag">4 实验及结果分析</h3>
                <div class="p1">
                    <p id="138">实验使用的硬件环境与软件环境如下.实验在服务器上进行, 服务器的配置如下:CPU为4块6核Intel Xeon e5-2620 2.40 GHz, 显卡配置为8块12 GB的NVIDIA Tesla K80显卡, 操作系统为Ubuntu 16.04.5 LTS版本, 使用的编译器为Matlab R2018a.</p>
                </div>
                <div class="p1">
                    <p id="139">本文在3个公开、常用的姿态估计基准数据集上评估姿态估计方法, 它们是Leeds Sports Poses (LSP) 数据集<citation id="220" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、Frames Labeled In Cinema (FLIC) 数据集<citation id="218" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>和Image Parse数据集<citation id="219" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>.数据集情况如表1所示.</p>
                </div>
                <div class="area_img" id="140">
                    <p class="img_tit"><b>表1</b><b>3个基准数据集的信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Information of 3 benchmark datasets</p>
                    <p class="img_note"></p>
                    <table id="140" border="1"><tr><td><br />数据集</td><td>图像数</td><td>训练<br />图像数</td><td>测试<br />图像数</td><td>标注<br />关节数</td></tr><tr><td><br />LSP</td><td>2000</td><td>1000</td><td>1000</td><td>14</td></tr><tr><td><br />FLIC</td><td>5003</td><td>3987</td><td>1016</td><td>10</td></tr><tr><td><br />Image Parse</td><td>305</td><td>100</td><td>205</td><td>14</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="141">LSP数据集包含一些在体育活动中捕获的图像, 包含1 000幅训练样本和1 000幅测试样本.每个人身高大约150像素, 带有14个关节点的全身注释.因为在运动过程中身体幅度一般较大, 因此具有一定挑战性.</p>
                </div>
                <div class="p1">
                    <p id="142">FLIC数据集包含3 987幅训练样本和1 016幅测试样本.数据集收集好莱坞经典电影中的一些画面镜头, 因为电影拍摄的缘故, 大多数样本只有上半身, 因此每人只带有10个上半身关节注释.</p>
                </div>
                <div class="p1">
                    <p id="143">Image Parse数据集包含305幅能够良好表示人体姿势的图像, 并附有全身标注.由于Image Parse数据集仅用于在不同数据集间进行交叉验证, 故将训练在LSP数据集上的模型直接应用于Image Parse数据集的测试图像.在训练过程中, 随机在INRIA Person数据集中选取一些图像作为负样例 (图像中只包含背景而未包含人体) .</p>
                </div>
                <div class="p1">
                    <p id="144">本文使用2个在人体姿态估计中广泛使用的评估指标:关节肢体正确检测率 (Percentage of Correct Parts, PCP) , 关节检测率 (Percentage of Detected Joints, PDJ) .PCP衡量被正确检测到的关节点或肢体部分的百分比.PCP定义为:如果被检测的肢体终端与真实肢体的终端之间的距离在肢体长度的一半范围之内, 那么这个肢体被正确检测.本文采用PCP其中的一种类型strict PCP进行不同数据集上实验结果的对比.PDJ衡量关节点的检测率, 其定义为:如果被检测的关节点与真实关节点之间的距离小于躯干直径的其中一部分, 那么这个关键点被检测到, 躯干直径指每个真实的姿态结果中左肩部节点与右髋部节点之间的距离.</p>
                </div>
                <div class="p1">
                    <p id="145">本文针对传统关节肢体模型存在的容易因视觉变化导致模型多变等问题, 提出可以灵活表示关节点间空间关系的混合模型, 在一定程度上解决传统模型的多变性问题.针对SVM在样本特征不规则情况下出现的难以对特征分类的情况, 提出基于DCNN的学习方法.</p>
                </div>
                <div class="p1">
                    <p id="146">本文模型首先对人体构建图像模型以表示人体关节及肢体, 再根据标注信息将图像分解为以关节为中心的若干图像块作为训练输入数据, 最后通过训练, 得到一个分类DCNN网络用于预测人体姿态, 通过得分函数衡量预测结果的正确性.对于计算的复杂度来说, 因为节点的像素坐标有<i>L</i>种可能, 每个节点的混合姿态类型有<i>T</i>种可能, 因此, 总的时间复杂度为<i>O</i> (<i>L</i><sup>2</sup><i>T</i><sup>2</sup>) , 但是, 由于节点间空间关系模型是关于像素坐标<i>l</i><sub><i>i</i></sub>和<i>l</i><sub><i>j</i></sub>的二次函数, 那么可以通过利用广义距离变化以加速<i>l</i><sub><i>i</i></sub>上的最大化, 将时间复杂度降低为<i>O</i> (<i>LT</i><sup>2</sup>) .</p>
                </div>
                <div class="p1">
                    <p id="147">本文在LSP、FLIC数据集上进行PDJ的评估, 具体数据及曲线图如图6所示.观察发现, 对PDJ阈值参数设置不同值, 随着阈值的增大, 关节的检测率也随之上升.总体来说, 在检测的几个关节部分, 本文方法对头部、肩部及髋部关节的检测率远高于其它关节.考虑到手腕关节姿态变化最大, 而且由于手持物品或鞋子袜子遮挡的因素, 因此手腕关节及脚踝关节检测率相对较低.考虑到FLIC数据集的图像只有上半身, 因此手腕关节及手肘关节在FLIC数据集上的PDJ评估结果优于LSP.</p>
                </div>
                <div class="area_img" id="150">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902001_15000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同阈值尺度下不同节点在2个数据集上的PDJ结果" src="Detail/GetImg?filename=images/MSSB201902001_15000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同阈值尺度下不同节点在2个数据集上的PDJ结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902001_15000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 PDJ results for different parts on 2 datasets under different threshold conditions</p>
                                <p class="img_note"> (b) FLIC</p>

                </div>
                <div class="area_img" id="150">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902001_15001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同阈值尺度下不同节点在2个数据集上的PDJ结果" src="Detail/GetImg?filename=images/MSSB201902001_15001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同阈值尺度下不同节点在2个数据集上的PDJ结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902001_15001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 PDJ results for different parts on 2 datasets under different threshold conditions</p>
                                <p class="img_note"> (b) FLIC</p>

                </div>
                <div class="p1">
                    <p id="152">对比其它方法发现, 本文方法的性能表现更优.表2给出当阈值尺度取值为0.2时, 各方法在手肘、手腕、膝盖、脚踝这4个节点的PDJ评估结果.对比方法包括文献<citation id="226" type="reference">[<a class="sup">5</a>]</citation>方法、文献<citation id="223" type="reference">[<a class="sup">12</a>]</citation>方法、文献<citation id="224" type="reference">[<a class="sup">17</a>]</citation>方法和文献<citation id="221" type="reference">[<a class="sup">18</a>]</citation>方法.通过表中数据对比可知, 本文方法在手肘节点处的PDJ结果比文献<citation id="222" type="reference">[<a class="sup">17</a>]</citation>方法提升17.9%, 比文献<citation id="227" type="reference">[<a class="sup">5</a>]</citation>方法提升3%.本文方法在膝盖处的PDJ结果相比文献<citation id="225" type="reference">[<a class="sup">12</a>]</citation>方法具有11.7%的提升.</p>
                </div>
                <div class="area_img" id="153">
                    <p class="img_tit"><b>表2</b><b>阈值为0.2时关节点的PDJ结果 (LSP数据集</b>)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2 PDJ results of joints on LSP dataset with a threshold of 0.2</p>
                    <p class="img_note"></p>
                    <table id="153" border="1"><tr><td><br />方法</td><td>手肘</td><td>手腕</td><td>膝盖</td><td>脚踝</td></tr><tr><td><br />文献[12]方法</td><td>61.7</td><td>49.3</td><td>70.0</td><td>67.6</td></tr><tr><td><br />文献[5]方法</td><td>70.3</td><td>63.2</td><td>78.0</td><td>72.0</td></tr><tr><td><br />文献[17]方法</td><td>61.4</td><td>47.7</td><td>75.2</td><td>68.4</td></tr><tr><td><br />文献[18]方法</td><td>61.4</td><td>47.2</td><td>69.1</td><td>68.8</td></tr><tr><td><br />本文方法</td><td>72.6</td><td>64.5</td><td>78.9</td><td>70.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="154">在LSP数据集上, 对比各方法对躯干、头部、上臂、下臂、大腿、小腿正确检测的比率, 使用PCP指标衡量.对比其它方法, 本文方法除了头部的正确检测率略低于文献<citation id="229" type="reference">[<a class="sup">5</a>]</citation>方法之外, 其它部位的PCP评估结果均高于其它方法, 例如上臂的PCP结果比文献<citation id="228" type="reference">[<a class="sup">5</a>]</citation>方法提升2.3%, 比文献<citation id="231" type="reference">[<a class="sup">12</a>]</citation>方法提升14.3%, 比文献<citation id="230" type="reference">[<a class="sup">13</a>]</citation>方法提升26.4%, 具体结果如图7所示.</p>
                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902001_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 LSP数据集上各方法PCP结果对比" src="Detail/GetImg?filename=images/MSSB201902001_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 LSP数据集上各方法PCP结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902001_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Comparison of PCP results of different methods on LSP dataset</p>

                </div>
                <div class="p1">
                    <p id="157">在FLIC数据集上, 因为大部分图像只有上半身, 标注信息也是针对上半身, 因此选取上臂、下臂进行结果评估.本文方法在两个部位的正确检测率均最优.上臂与下臂的PCP结果, 对比文献<citation id="233" type="reference">[<a class="sup">15</a>]</citation>方法, 分别提升15.9%、67.2%, 对比文献<citation id="232" type="reference">[<a class="sup">5</a>]</citation>方法, 分别提升0.8%、0.3%.具体结果如图8所示.</p>
                </div>
                <div class="area_img" id="158">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902001_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 FLIC数据集上各方法PCP结果对比" src="Detail/GetImg?filename=images/MSSB201902001_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 FLIC数据集上各方法PCP结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902001_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Comparison of PCP results of different methods on FLIC dataset</p>

                </div>
                <div class="p1">
                    <p id="160">为了验证本文方法的泛化能力, 在Image Parse数据集上进行实验. Image Parse数据集并未经过训练, 而是直接使用在LSP数据集训练好的模型进行交叉验证, 因此进行评估的部位也与LSP数据集相同.在性能表现上, 本文方法除了头部的正确检测率略低于文献<citation id="237" type="reference">[<a class="sup">12</a>]</citation>方法之外, 其它部位的PCP结果均高于其它方法.本文方法躯干的PCP结果比文献<citation id="236" type="reference">[<a class="sup">17</a>]</citation>方法提升2.1%, 比文献<citation id="235" type="reference">[<a class="sup">12</a>]</citation>方法提升5.4%, 比文献<citation id="234" type="reference">[<a class="sup">4</a>]</citation>方法提升13.5%, 具体结果如图9所示.</p>
                </div>
                <div class="area_img" id="161">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902001_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 Image Parse数据集上各方法PCP结果对比" src="Detail/GetImg?filename=images/MSSB201902001_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 Image Parse数据集上各方法PCP结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902001_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Comparison of PCP results of different methods on Image Parse dataset</p>

                </div>
                <div class="p1">
                    <p id="163">图10给出本文方法在LSP数据集、FLIC数据集及Image Parse数据集的一些预测样例.将图像中的行人的关节点以点的形式可视化, 而节点部分之间的联系使用连线表示, 最终展示为一个树型图像结构.最后一行为一些因干扰因素而预测偏差的样例.可以看到, 即使对于在运动中出现的肢体幅度较大情况下的人体姿态, 本文方法也可以进行较准确的姿态估计, 其中Image Parse数据集只是用于交叉数据集评估, 并未用于训练.由此表明本文方法即使在未经过训练的数据集上依旧表现出良好的预测能力.而在一些极端情况下, 如 (d) 中出现的身体角度太极端导致的大部分身体被遮挡、人物之间重叠较多、背景与人体边界模糊或身体肢体幅度达到常人难以做到的情况, 会出现姿态估计的结果与实际存在偏差的情况.</p>
                </div>
                <div class="area_img" id="208">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902001_20800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 0 本文方法在不同数据集上的估计结果" src="Detail/GetImg?filename=images/MSSB201902001_20800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 0 本文方法在不同数据集上的估计结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902001_20800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 Estimation results of the proposed method on different datasets</p>

                </div>
                <div class="area_img" id="208">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201902001_20801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 0 本文方法在不同数据集上的估计结果" src="Detail/GetImg?filename=images/MSSB201902001_20801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 0 本文方法在不同数据集上的估计结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201902001_20801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 Estimation results of the proposed method on different datasets</p>

                </div>
                <h3 id="171" name="171" class="anchor-tag">5 结 束 语</h3>
                <div class="p1">
                    <p id="172">本文结合DCNN与树型结构的图像模型, 以DCNN为基础, 提出一个灵活的混合关节肢体模型, 通过局部图像块学习关节点存在的条件概率及不同节点部分之间的空间关系 (每个节点都通过不同姿态对应空间关系的集群组合形成该节点的一个混合姿态类型) .总体上, 整个模型不仅具备DCNN的稳定性, 还具有图像模型的灵活性及计算方面的高效性.在一些广泛使用的基准数据集上的实验表明, 本文方法体现出良好性能, 在未经过训练的数据集上, 同样也表现出良好的泛化能力.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="10">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human Action Recognition: Pose-Based Attention Draws Focus to Hands">

                                <b>[1]</b> BARADEL F, WOLF C, MILLE J. Human Action Recognition: Pose-Based Attention Draws Focus to Hands // Proc of the IEEE International Conference on Computer Vision Workshop. Washington, USA: IEEE, 2017: 604-613.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust Human Pose Tracking for Realistic Service Robot Applications">

                                <b>[2]</b> VASILEIADIS M, MALASSIOTIS S, GIAKOUMIS D, <i>et al</i>. Robust Human Pose Tracking for Realistic Service Robot Applications // Proc of the IEEE International Conference on Computer Vision Workshop. Washington, USA: IEEE, 2017: 1363-1372.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Look into Person: Self-supervised Structure-Sensitive Learning and a New Benchmark for Human Parsing">

                                <b>[3]</b> GONG K, LIANG X D, ZHANG D Y, <i>et al</i>. Look into Person: Self-supervised Structure-Sensitive Learning and a New Benchmark for Human Parsing // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2017: 932-940.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Articulated pose estimation with flexible mixtures-of-parts">

                                <b>[4]</b> YANG Y, RAMANAN D. Articulated Pose Estimation with Flexible Mixtures-of-Parts // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2011: 1385-1392.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Articulated pose estimation by agraphical model with image dependent pairwise relations">

                                <b>[5]</b> CHEN X J, YUILLE A. Articulated Pose Estimation by a Gra-phical Model with Image Dependent Pairwise Relations // GHAHRAMANI Z, WELLING M, GORTES C, <i>et al</i>., eds. Advances in Neural Information Processing Systems 27. Cambridge, USA: The MIT Press, 2014: 1736-1744.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The representation and matching of pictorial structure">

                                <b>[6]</b> FISCHLER M A, ELSCHLAGER R A. The Representation and Matching of Pictorial Structures. IEEE Transactions on Computers, 1973, 22 (1) : 67-92.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning effective human pose estimation from inaccurate annotation">

                                <b>[7]</b> JOHNSON S, EVERINGHAM M. Learning Effective Human Pose Estimation from Inaccurate Annotation // Proc of the 24th IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2011: 1465-1472.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Support Vector Machine Learning for Interdependent and Structured Output Spaces">

                                <b>[8]</b> TSOCHANTARIDIS I, HOFMANN T, JOACHIMS T, <i>et al</i>. Su-pport Vector Machine Learning for Interdependent and Structured Output Spaces // Proc of the 21st International Conference on Machine Learning. New York, USA: ACM, 2004. DOI:10.1145/1015330.1015341.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parsing occluded people by flexible compositions">

                                <b>[9]</b> CHEN X J, YUILLE A L. Parsing Occluded People by Flexible Compositions // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2015: 3945-3954.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structured Feature Learning for Pose Estimation">

                                <b>[10]</b> CHU X, OUYANG W L, LI H S, <i>et al</i>. Structured Feature Lear-ning for Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2016: 4715-4723.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Combining Local Appearance and Holistic View:Dual-Source Deep Neural Networks for Human Pose Estimation[C/OL]">

                                <b>[11]</b> FAN X C, ZHENG K, LIN Y W, <i>et al</i>. Combining Local Appea-rance and Holistic View: Dual-Source Deep Neural Networks for Human Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2015: 1347-1355.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-source deep learning for human pose estimation">

                                <b>[12]</b> OUYANG W L, CHU X, WANG X G. Multi-source Deep Lear-ning for Human Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2014: 2337-2344.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepPose:Human Pose Estimation via Deep Neural Networks">

                                <b>[13]</b> TOSHEV A, SZEGEDY C. DeepPose: Human Pose Estimation via Deep Neural Networks // Proc of the IEEE Conference on Compu-ter Vision and Pattern Recognition. Washington, USA: IEEE, 2014: 1653-1660.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation">

                                <b>[14]</b> JOHNSON S, EVERINGHAM M. Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation // Proc of the British Machine Vision Conference. Dundee, Britain: BMVA, 2010: 12.1-12.11.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MODEC:Multimodal Decomposable Models for Human Pose Estimation">

                                <b>[15]</b> SAPP B, TASKAR B. MODEC: Multimodal Decomposable Models for Human Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2013: 3674-3681.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to parse images of articulated bodies">

                                <b>[16]</b> RAMANAN D. Learning to Parse Images of Articulated Bodies // SCHÖLKOPF B, PLATT J C, HOFFMAN T, eds. Advances in Neural Information Processing Systems 19. Cambridge, USA: The MIT Press, 2006: 1129-1136.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Poselet conditioned pictorial structures">

                                <b>[17]</b> PISHCHULIN L, ANDRILUKA M, GEHLER P, <i>et al</i>. Poselet Conditioned Pictorial Structures // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2013: 588-595.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pose Machines:Articulated Pose Estimation via Inference Machines">

                                <b>[18]</b> RAMAKRISHNA V, MUNOZ D, HEBERT M, <i>et al</i>. Pose Machines: Articulated Pose Estimation via Inference Machines // Proc of the 13th European Conference on Computer Vision. Berlin, Germany: Springer, 2014, II: 33-47.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-End Learning of Deformable Mixture of Parts and Deep Convolutional Neural Networks for Human Pose Estimation">

                                <b>[19]</b> YANG W, OUYANG W L, LI H S, <i>et al</i>. End-to-End Learning of Deformable Mixture of Parts and Deep Convolutional Neural Networks for Human Pose Estimation // Proc of the IEEE Conference on Computer Vision and Pattern Recognition. Washington, USA: IEEE, 2016: 3073-3082.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint training of a convolutional network and a graphical model for human pose estimation">

                                <b>[20]</b> TOMPSON J J, JAIN A, LECUN Y, <i>et al</i>. Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation // GHAHRAMANI Z, WELLING M, CORTES C, <i>et al</i>., eds. Advances in Neural Information Processing Systems 27. Cambridge, USA: The MIT Press, 2014: 1799-1807.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201902001" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201902001&amp;v=MjM3ODM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5em5VTDdBS0Q3WWJMRzRIOWpNclk5RlpZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
