<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131458996436250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dMSSB201909010%26RESULT%3d1%26SIGN%3dVSCOCWx0kj%252bxHeGKcSDrR1WhUvM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201909010&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MSSB201909010&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201909010&amp;v=MTk5NjNVUkxPZVplUm5GeS9rVnJ2QUtEN1liTEc0SDlqTXBvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#55" data-title="1 本文算法原理及流程 ">1 本文算法原理及流程</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="&lt;b&gt;1.1 本文算法流程&lt;/b&gt;"><b>1.1 本文算法流程</b></a></li>
                                                <li><a href="#60" data-title="&lt;b&gt;1.2 MSFF-CGAN网络原理分析&lt;/b&gt;"><b>1.2 MSFF-CGAN网络原理分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#71" data-title="2 MSFF-CGAN结构设计与分析 ">2 MSFF-CGAN结构设计与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="&lt;b&gt;2.1 UC-Net的结构设计&lt;/b&gt;"><b>2.1 UC-Net的结构设计</b></a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;2.2 鉴别器结构设计&lt;/b&gt;"><b>2.2 鉴别器结构设计</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;2.3 MemNet超分辨网络&lt;/b&gt;"><b>2.3 MemNet超分辨网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#89" data-title="3 MSFF-CGAN网络训练数据生成方法 ">3 MSFF-CGAN网络训练数据生成方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#90" data-title="&lt;b&gt;3.1 手绘山水画训练数据的收集和处理&lt;/b&gt;"><b>3.1 手绘山水画训练数据的收集和处理</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;3.2 布局标签图的设计与聚类生成&lt;/b&gt;"><b>3.2 布局标签图的设计与聚类生成</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#121" data-title="4 实验及结果分析 ">4 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#122" data-title="&lt;b&gt;4.1 实验环境配置&lt;/b&gt;"><b>4.1 实验环境配置</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;4.2 山水画仿真学习能力的效果&lt;/b&gt;"><b>4.2 山水画仿真学习能力的效果</b></a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;4.3 MSFF-CGAN与MSFF-CGAN+MemNet 实 验效果对比&lt;/b&gt;"><b>4.3 MSFF-CGAN与MSFF-CGAN+MemNet 实 验效果对比</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;4.4 UC-Net与U-Net生成器的对比分析&lt;/b&gt;"><b>4.4 UC-Net与U-Net生成器的对比分析</b></a></li>
                                                <li><a href="#168" data-title="&lt;b&gt;4.5 L1范数和L2范数的实验效果&lt;/b&gt;"><b>4.5 L1范数和L2范数的实验效果</b></a></li>
                                                <li><a href="#179" data-title="&lt;b&gt;4.6 与Pix2pixHR的生成效果对比&lt;/b&gt;"><b>4.6 与Pix2pixHR的生成效果对比</b></a></li>
                                                <li><a href="#191" data-title="&lt;b&gt;4.7 布局可调的山水画仿真实验结果&lt;/b&gt;"><b>4.7 布局可调的山水画仿真实验结果</b></a></li>
                                                <li><a href="#197" data-title="&lt;b&gt;4.8 涂鸦式仿真实验结果&lt;/b&gt;"><b>4.8 涂鸦式仿真实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#203" data-title="5 结 束 语 ">5 结 束 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="图1 本文算法流程图">图1 本文算法流程图</a></li>
                                                <li><a href="#65" data-title="图2 MSFF-CGAN网络的生成器和鉴别器原理">图2 MSFF-CGAN网络的生成器和鉴别器原理</a></li>
                                                <li><a href="#79" data-title="图3 面向山水画布局的MSFF-CGAN网络示意图">图3 面向山水画布局的MSFF-CGAN网络示意图</a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;表1 生成器参数&lt;/b&gt;"><b>表1 生成器参数</b></a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;表2 鉴别器参数&lt;/b&gt;"><b>表2 鉴别器参数</b></a></li>
                                                <li><a href="#88" data-title="图4 MemNet网络结构">图4 MemNet网络结构</a></li>
                                                <li><a href="#98" data-title="图5 山水画训练数据对">图5 山水画训练数据对</a></li>
                                                <li><a href="#98" data-title="图5 山水画训练数据对">图5 山水画训练数据对</a></li>
                                                <li><a href="#256" data-title="图6 本文网络山水画仿真效果">图6 本文网络山水画仿真效果</a></li>
                                                <li><a href="#256" data-title="图6 本文网络山水画仿真效果">图6 本文网络山水画仿真效果</a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;表3 原作与仿真图在3类对象的粗糙度对比&lt;/b&gt;"><b>表3 原作与仿真图在3类对象的粗糙度对比</b></a></li>
                                                <li><a href="#156" data-title="&lt;b&gt;表4 两种网络结构清晰度对比&lt;/b&gt;"><b>表4 两种网络结构清晰度对比</b></a></li>
                                                <li><a href="#257" data-title="图7 U-Net和UC-Net的生成效果对比">图7 U-Net和UC-Net的生成效果对比</a></li>
                                                <li><a href="#167" data-title="&lt;b&gt;表5 U-Net与UC-Net的PSNR对比&lt;/b&gt;"><b>表5 U-Net与UC-Net的PSNR对比</b></a></li>
                                                <li><a href="#258" data-title="图8 两种范数的生成效果对比">图8 两种范数的生成效果对比</a></li>
                                                <li><a href="#178" data-title="&lt;b&gt;表6 两种范数生成图的清晰度对比&lt;/b&gt;"><b>表6 两种范数生成图的清晰度对比</b></a></li>
                                                <li><a href="#259" data-title="图9 Pix2pixHR和本文方法的生成效果对比">图9 Pix2pixHR和本文方法的生成效果对比</a></li>
                                                <li><a href="#190" data-title="&lt;b&gt;表7 Pix2pixHR和本文方法的PSNR对比&lt;/b&gt;"><b>表7 Pix2pixHR和本文方法的PSNR对比</b></a></li>
                                                <li><a href="#260" data-title="图10 编辑布局后的仿真效果展示">图10 编辑布局后的仿真效果展示</a></li>
                                                <li><a href="#260" data-title="图10 编辑布局后的仿真效果展示">图10 编辑布局后的仿真效果展示</a></li>
                                                <li><a href="#261" data-title="图11 涂鸦式仿真效果">图11 涂鸦式仿真效果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="262">


                                    <a id="bibliography_1" title=" LEE J.Simulating Oriental Black-Ink Painting.IEEE Computer Graphics and Applications,1999,19(3):74-81." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simulating oriental black-ink painting">
                                        <b>[1]</b>
                                         LEE J.Simulating Oriental Black-Ink Painting.IEEE Computer Graphics and Applications,1999,19(3):74-81.
                                    </a>
                                </li>
                                <li id="264">


                                    <a id="bibliography_2" title=" WAY D L,SHIH Z C.The Synthesis of Rock Textures in Chinese Landscape Painting.Computer Graphics Forum,2010,20(3):123-131." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Synthesis of Rock Textures in Chinese Landscape Painting">
                                        <b>[2]</b>
                                         WAY D L,SHIH Z C.The Synthesis of Rock Textures in Chinese Landscape Painting.Computer Graphics Forum,2010,20(3):123-131.
                                    </a>
                                </li>
                                <li id="266">


                                    <a id="bibliography_3" title=" 徐添辰,吴恩华.动态模拟中国水墨画中的笔画绘制.计算机辅助设计与图形学学报,2016,28(5):742-749.(XU T C,WU E H.Animating Strokes in Drawing Process of Chinese Ink Painting.Journal of Computer-Aided Design and Computer Graphics,2016,28(5):742-749.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201605006&amp;v=MjE4ODhaZVJuRnkva1ZydkFMejdCYUxHNEg5Zk1xbzlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         徐添辰,吴恩华.动态模拟中国水墨画中的笔画绘制.计算机辅助设计与图形学学报,2016,28(5):742-749.(XU T C,WU E H.Animating Strokes in Drawing Process of Chinese Ink Painting.Journal of Computer-Aided Design and Computer Graphics,2016,28(5):742-749.)
                                    </a>
                                </li>
                                <li id="268">


                                    <a id="bibliography_4" title=" TANG F,DONG W M,MENG Y P,et al.Animated Construction of Chinese Brush Paintings.IEEE Transactions on Visualization and Computer Graphics,2018,24(12):3019-3031." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Animated Construction of Chinese Brush Paintings">
                                        <b>[4]</b>
                                         TANG F,DONG W M,MENG Y P,et al.Animated Construction of Chinese Brush Paintings.IEEE Transactions on Visualization and Computer Graphics,2018,24(12):3019-3031.
                                    </a>
                                </li>
                                <li id="270">


                                    <a id="bibliography_5" title=" YU J H,LUO G M,PENG Q S.Image-Based Synthesis of Chinese Landscape Painting.Journal of Computer Science and Technology,2003,18(1):22-28." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003430836&amp;v=MTYyMDhiT2dKWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQ0hsVkw3TkpGWT1OajdCYXJPNEh0SFBxNHhG&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         YU J H,LUO G M,PENG Q S.Image-Based Synthesis of Chinese Landscape Painting.Journal of Computer Science and Technology,2003,18(1):22-28.
                                    </a>
                                </li>
                                <li id="272">


                                    <a id="bibliography_6" title=" 张海嵩,尹小勤,于金辉.实时绘制3D中国画效果.计算机辅助设计与图形学学报,2004,16(11):1485-1489.(ZHANG H S,YIN X Q,YU J H.Real-Time Rendering of 3D Chinese Painting Effects.Journal of Computer-Aided Design and Computer Graphics,2004,16(11):1485-1489.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF200411002&amp;v=MjgyNzQ5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tWcnZBTHo3QmFMRzRIdFhOcm8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         张海嵩,尹小勤,于金辉.实时绘制3D中国画效果.计算机辅助设计与图形学学报,2004,16(11):1485-1489.(ZHANG H S,YIN X Q,YU J H.Real-Time Rendering of 3D Chinese Painting Effects.Journal of Computer-Aided Design and Computer Graphics,2004,16(11):1485-1489.)
                                    </a>
                                </li>
                                <li id="274">


                                    <a id="bibliography_7" title=" WANG Z,SUN M J,SUN J Z,et al.Neural Network-Based Chinese Ink-Painting Art Style Learning // Proc of the IEEE International Conference on Intelligent Computing and Intelligent Systems.Washington,USA:IEEE,2010:462-466." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural network-based Chinese ink-painting art style learning">
                                        <b>[7]</b>
                                         WANG Z,SUN M J,SUN J Z,et al.Neural Network-Based Chinese Ink-Painting Art Style Learning // Proc of the IEEE International Conference on Intelligent Computing and Intelligent Systems.Washington,USA:IEEE,2010:462-466.
                                    </a>
                                </li>
                                <li id="276">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                     GATYS L A,ECKER A S,BETHGE M.Image Style Transfer Using Convolutional Neural Networks // Proc of the IEEE Confe-rence on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:2414-2423.</a>
                                </li>
                                <li id="278">


                                    <a id="bibliography_9" title=" WANG T C,LIU M Y,ZHU J Y,et al.High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs // Proc of the IEEE Conference on Computer Vision and Pattern Re-cognition.Washington,USA:IEEE,2018:8798-8807." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-resolution image synthesis and semantic manipulation with conditional gans">
                                        <b>[9]</b>
                                         WANG T C,LIU M Y,ZHU J Y,et al.High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs // Proc of the IEEE Conference on Computer Vision and Pattern Re-cognition.Washington,USA:IEEE,2018:8798-8807.
                                    </a>
                                </li>
                                <li id="280">


                                    <a id="bibliography_10" title=" ISOLA P,ZHU J Y,ZHOU T H,et al.Image-to-Image Translation with Conditional Adversarial Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2017:5967-5976." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image-to-Image Translation with Conditional Adversarial Networks">
                                        <b>[10]</b>
                                         ISOLA P,ZHU J Y,ZHOU T H,et al.Image-to-Image Translation with Conditional Adversarial Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2017:5967-5976.
                                    </a>
                                </li>
                                <li id="282">


                                    <a id="bibliography_11" title=" PARK T,LIU M Y,WANG T C,et al.Semantic Image Synthesis with Spatially-Adaptive Normalization[C/OL].[2019-03-21].https://arxiv.org/pdf/1903.07291.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic Image Synthesis with Spatially-Adaptive Normalization[C/OL]">
                                        <b>[11]</b>
                                         PARK T,LIU M Y,WANG T C,et al.Semantic Image Synthesis with Spatially-Adaptive Normalization[C/OL].[2019-03-21].https://arxiv.org/pdf/1903.07291.pdf.
                                    </a>
                                </li>
                                <li id="284">


                                    <a id="bibliography_12" title=" TAI Y,YANG J,LIU X M,et al.MemNet:A Persistent Memory Networks for Image Restoration // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2017:4549-4557." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Memnet A persistent memory network for image restoration">
                                        <b>[12]</b>
                                         TAI Y,YANG J,LIU X M,et al.MemNet:A Persistent Memory Networks for Image Restoration // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2017:4549-4557.
                                    </a>
                                </li>
                                <li id="286">


                                    <a id="bibliography_13" title=" GOODFELLOW I,POUGET-ABADIE J,MIRZA M,et al.Gene-rative Adversarial Networks // Proc of the 27th International Conference on Neural Information Processing Systems.Cambridge,USA:The MIT Press,2014:2672-2680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial networks">
                                        <b>[13]</b>
                                         GOODFELLOW I,POUGET-ABADIE J,MIRZA M,et al.Gene-rative Adversarial Networks // Proc of the 27th International Conference on Neural Information Processing Systems.Cambridge,USA:The MIT Press,2014:2672-2680.
                                    </a>
                                </li>
                                <li id="288">


                                    <a id="bibliography_14" title=" 王坤峰,苟超,段艳杰,等.生成式对抗网络GAN的研究进展与展望.自动化学报,2017,43(3):321-332.(WANG K F,GOU C,DUAN Y J.Generative Adversarial Networks:The State of the Art and Beyond.Acta Automatica Sinica,2017,43(3):321-332.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201703001&amp;v=MDQ2ODFDTGZZYkc0SDliTXJJOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVnJ2QUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         王坤峰,苟超,段艳杰,等.生成式对抗网络GAN的研究进展与展望.自动化学报,2017,43(3):321-332.(WANG K F,GOU C,DUAN Y J.Generative Adversarial Networks:The State of the Art and Beyond.Acta Automatica Sinica,2017,43(3):321-332.)
                                    </a>
                                </li>
                                <li id="290">


                                    <a id="bibliography_15" title=" 蔡雨婷,陈昭炯,叶东毅.基于双层级联GAN的草图到真实感图像的异质转换.模式识别与人工智能,2018,31(10):877-886.(CAI Y T,CHEN Z J,YE D Y.Bi-level Cascading GAN-Based Heterogeneous Conversion of Sketch-to-Realistic Images.Pattern Recognition and Artificial Intelligence,2018,31(10):877-886.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201810002&amp;v=MjgyMjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVnJ2QUtEN1liTEc0SDluTnI0OUZab1FLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         蔡雨婷,陈昭炯,叶东毅.基于双层级联GAN的草图到真实感图像的异质转换.模式识别与人工智能,2018,31(10):877-886.(CAI Y T,CHEN Z J,YE D Y.Bi-level Cascading GAN-Based Heterogeneous Conversion of Sketch-to-Realistic Images.Pattern Recognition and Artificial Intelligence,2018,31(10):877-886.)
                                    </a>
                                </li>
                                <li id="292">


                                    <a id="bibliography_16" title=" 唐贤伦,杜一铭,刘雨微,等.基于条件深度卷积生成对抗网络的图像识别方法.自动化学报,2018,44(5):855-864.(TANG X L,DU Y M,LIU Y W,et al.Image Recognition with Conditional Deep Convolutional Generative Adversarial Networks.Acta Automatica Sinica,2018,44(5):855-864.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805009&amp;v=MTI4OTlBS0NMZlliRzRIOW5NcW85RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tWcnY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         唐贤伦,杜一铭,刘雨微,等.基于条件深度卷积生成对抗网络的图像识别方法.自动化学报,2018,44(5):855-864.(TANG X L,DU Y M,LIU Y W,et al.Image Recognition with Conditional Deep Convolutional Generative Adversarial Networks.Acta Automatica Sinica,2018,44(5):855-864.)
                                    </a>
                                </li>
                                <li id="294">


                                    <a id="bibliography_17" title=" JOHNSON J,ALAHI A,LI F F.Perceptual Losses for Real-Time Style Transfer and Super-Resolution // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2016:694-711." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceptual losses for real-time style transfer and superresolution">
                                        <b>[17]</b>
                                         JOHNSON J,ALAHI A,LI F F.Perceptual Losses for Real-Time Style Transfer and Super-Resolution // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2016:694-711.
                                    </a>
                                </li>
                                <li id="296">


                                    <a id="bibliography_18" title=" PATHAK D,KRAHENBUHL P,DONAHUE J,et al.Context Encoders:Feature Learning by Inpainting // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:2536-2544." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Context Encoders:Feature Learning by Inpainting">
                                        <b>[18]</b>
                                         PATHAK D,KRAHENBUHL P,DONAHUE J,et al.Context Encoders:Feature Learning by Inpainting // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:2536-2544.
                                    </a>
                                </li>
                                <li id="298">


                                    <a id="bibliography_19" title=" YOO D,KIM N,PARK S,et al.Pixel-Level Domain Transfer // Proc of the European Conference on Computer Vision.Berlin,Ger-many:Springer,2016:517-532." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pixel-Level Domain Transfer">
                                        <b>[19]</b>
                                         YOO D,KIM N,PARK S,et al.Pixel-Level Domain Transfer // Proc of the European Conference on Computer Vision.Berlin,Ger-many:Springer,2016:517-532.
                                    </a>
                                </li>
                                <li id="300">


                                    <a id="bibliography_20" title=" RONNERBERGER O,FISCHER P,BROX T.U-Net:Convolutional Networks for Biomedical Image Segmentation // Proc of the International Conference on Medical Image Computing and Compu-ter-Assisted Intervention.Berlin,Germany:Springer,2015:234-241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">
                                        <b>[20]</b>
                                         RONNERBERGER O,FISCHER P,BROX T.U-Net:Convolutional Networks for Biomedical Image Segmentation // Proc of the International Conference on Medical Image Computing and Compu-ter-Assisted Intervention.Berlin,Germany:Springer,2015:234-241.
                                    </a>
                                </li>
                                <li id="302">


                                    <a id="bibliography_21" title=" VEDALDI A,LENC K.MatConvNet:Convolutional Neural Networks for Matlab // Proc of the 23rd ACM International Conference on Multimedia.New York,USA:ACM,2015:689-692." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Matconvnet:Convolutional neural networks for MATLAB">
                                        <b>[21]</b>
                                         VEDALDI A,LENC K.MatConvNet:Convolutional Neural Networks for Matlab // Proc of the 23rd ACM International Conference on Multimedia.New York,USA:ACM,2015:689-692.
                                    </a>
                                </li>
                                <li id="304">


                                    <a id="bibliography_22" title=" DENG J,DONG W,SOCHER R,et al.Image-Net:A Large-Scale Hierarchical Image Database // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2009:248-255." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet:A largescale hierarchical image database">
                                        <b>[22]</b>
                                         DENG J,DONG W,SOCHER R,et al.Image-Net:A Large-Scale Hierarchical Image Database // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2009:248-255.
                                    </a>
                                </li>
                                <li id="306">


                                    <a id="bibliography_23" title=" VICENTE S,CARREIRA J,AGAPITO L,et al.Reconstructing PASCAL VOC // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2014:41-48." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reconstructing pascal voc">
                                        <b>[23]</b>
                                         VICENTE S,CARREIRA J,AGAPITO L,et al.Reconstructing PASCAL VOC // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2014:41-48.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MSSB" target="_blank">模式识别与人工智能</a>
                2019,32(09),844-854 DOI:10.16451/j.cnki.issn1003-6059.201909009            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于CGAN的中国山水画布局可调的仿真生成方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A1%BE%E6%9D%A8&amp;code=42910505&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">顾杨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E6%98%AD%E7%82%AF&amp;code=06680030&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈昭炯</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E7%81%BF&amp;code=42910506&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈灿</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B6%E4%B8%9C%E6%AF%85&amp;code=06682886&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">叶东毅</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6%E6%95%B0%E5%AD%A6%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0094575&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福州大学数学与计算机科学学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6%E7%A9%BA%E9%97%B4%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%85%B1%E4%BA%AB%E6%95%99%E8%82%B2%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福州大学空间数据挖掘与信息共享教育部重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>以往的山水画计算机仿真由于未从山水画整体布局的角度进行研究,难以实现完整的画作生成.针对上述问题,文中提出布局引导、可实现完整画作生成的中国山水画仿真方法.基于山水画的绘制特点设计可行的布局标签图结构,用于表达山水画的构图形态和要素.借鉴条件生成对抗网络(CGAN)的思想,针对山水画的布局和笔触特点,设计并训练多尺度特征融合的网络结构(MSFF-CGAN),实现布局标签图到仿真山水画这一异质生成过程.同时针对网络训练过程中布局标签图数据稀缺的问题,采用语义关联的颜色像素聚类算法快速生成标签图.为了提高生成图的艺术真实感,引入MemNet超分辨网络增强生成图的纹理细节.实验表明,文中方法生成的仿真山水画具有较好的完整性和艺术真实感,不仅可以应对简单的手绘涂鸦式草图,还可以通过在布局空间的编辑操作,达到对画作空间进行编辑的效果.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%AD%E5%9B%BD%E5%B1%B1%E6%B0%B4%E7%94%BB%E4%BB%BF%E7%9C%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国山水画仿真;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B8%83%E5%B1%80%E5%8F%AF%E8%B0%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">布局可调;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B8%83%E5%B1%80%E6%A0%87%E7%AD%BE%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">布局标签图;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9D%A1%E4%BB%B6%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C(CGAN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">条件生成对抗网络(CGAN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D%E8%B6%85%E5%88%86%E8%BE%A8%E7%BD%91%E7%BB%9C(MemNet)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像修复超分辨网络(MemNet);</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    顾杨，硕士研究生，主要研究方向为智能图像处理．E-mail:573711345@qq.com.&lt;image id="252" type="formula" href="images/MSSB201909010_25200.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    *陈昭炯(通讯作者)，硕士，教授，主要研究方向为智能图像处理、计算智能．E-mail:chenzj@fzu.edu.cn.&lt;image id="253" type="formula" href="images/MSSB201909010_25300.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    陈灿，硕士研究生，主要研究方向为图像处理．E-mail:756154017@qq.com.&lt;image id="254" type="formula" href="images/MSSB201909010_25400.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    叶东毅，博士，教授，主要研究方向为计算智能、数据挖掘．E-mail:yiedy@fzu.edu.cn.&lt;image id="255" type="formula" href="images/MSSB201909010_25500.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(No.61672158);</span>
                                <span>福建省自然科学基金项目(No.2018J01798)资助;</span>
                    </p>
            </div>
                    <h1><b>Layout Adjustable Simulated Generation Method for Chinese Landscape Paintings Based on CGAN</b></h1>
                    <h2>
                    <span>GU Yang</span>
                    <span>CHEN Zhaojiong</span>
                    <span>CHEN Can</span>
                    <span>YE Dongyi</span>
            </h2>
                    <h2>
                    <span>College of Mathematics and Computer Science,Fuzhou University</span>
                    <span>Key Laboratory of Spatial Data Mining and Information Sharing,Ministry of Education,Fuzhou University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Creating a complete landscape painting via computer simulation is difficult without studying from global layout viewpoint. To address this issue, a layout-guided Chinese landscape painting simulation method for a complete painting generation is proposed. The characteristics of landscape paintings are taken into account in the design of feasible structures of layout label maps. Composition forms and elements of landscape paintings can be depicted using those structures. On the basis of condition generative adversarial network(CGAN) approach, a multi-scale feature fusion CGAN(MSFF-CGAN) is designed based on layouts and touches of landscape paintings. The proposed network is trained to accomplish heterogeneous transfer from a layout label map to a simulated landscape painting. To deal with rare availability of layout label maps for network training, a color pixel clustering algorithm with semantic correlation is used. In order to enhance the artistic reality of the generated landscape painting, a super resolution network named MemNet is incorporated to refine the texture details. Experimental results show that the proposed method is superior to existing methods in both integrity and artistic reality. Moreover, the proposed method can be used to handle simple graffiti sketches and modify simulated landscape paintings by editing label maps.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Chinese%20Landscape%20Painting%20Simulation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Chinese Landscape Painting Simulation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Layout%20Adjustable&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Layout Adjustable;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Layout%20Label%20Map&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Layout Label Map;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Condition%20Generative%20Adversarial%20Network(CGAN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Condition Generative Adversarial Network(CGAN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=MemNet%20Network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">MemNet Network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    GU Yang,master student. Her research interests include intelligent image processing.;
                                </span>
                                <span>
                                    CHEN Zhaojiong(Corresponding author), master,professor. Her research interests include intelligent image processing and computational intelligence.;
                                </span>
                                <span>
                                    CHEN Can,master student. His research interests include image processing.;
                                </span>
                                <span>
                                    YE Dongyi,Ph. D. ,professor. His research interests include computational intelligence and data mining.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-11</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Supported by National Natural Science Foundation of China(No.61672158);</span>
                                <span>Natural Science Foundation of Fujian Province(No.2018J01798);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="49">中国画简称“国画”,是我国传统的绘画形式.山水画是国画的一个重要分支,以自然景观为绘制主体.如何对传统山水画进行仿真生成一直是计算机图形图像领域极具挑战性的课题之一.</p>
                </div>
                <div class="p1">
                    <p id="50">对山水画的仿真研究始于20世纪90年代,发展至今,主要可分为三类.第一类方法通过笔触模拟,仿真山水画特有的笔法.Lee<citation id="308" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>和Way等<citation id="309" type="reference"><link href="264" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>通过对笔触建模以模仿水墨画和山石披麻皴法的绘制技巧.徐添辰等<citation id="310" type="reference"><link href="266" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和Tang等<citation id="311" type="reference"><link href="268" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出可以重建水墨画中绘画轨迹的工具,动态模拟画作原有的绘制过程.这类方法对山水画中特定对象的绘制效果良好,但未涉及一幅完整的山水画生成问题.</p>
                </div>
                <div class="p1">
                    <p id="51">第二类方法将山水画中的山石、云雾、流水等对象视为纹理,通过纹理分析、建模、合成技术对山水画进行仿真.Yu等<citation id="312" type="reference"><link href="270" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>通过纹理模型的构建和映射绘制山峦和云雾.张海嵩等<citation id="313" type="reference"><link href="272" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出实时绘制3D效果山峦模型的系统.这类方法对山水画中的山石等常见对象能产生一定的仿真效果,但视觉效果上不够自然,同时也难以完成一幅构图完整的山水画绘制.</p>
                </div>
                <div class="p1">
                    <p id="52">第三类方法采用艺术风格传递的思想,通过训练神经网络,将山水画风格赋予用户输入的任意一幅图像,从而实现山水画的仿真<citation id="314" type="reference"><link href="274" rel="bibliography" /><link href="276" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>.这类方法的主要思路是借助深度网络的特征提取优势,利用大量的标注样本,训练艺术风格网络,通过风格传递实现山水画仿真.但并不能创作山水画,只是对给定图像进行风格转换,赋予风格的同时难免会出现传递不合理、与艺术创作规律不符的情况.</p>
                </div>
                <div class="p1">
                    <p id="53">Wang等<citation id="315" type="reference"><link href="278" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>基于条件生成对抗网络<citation id="316" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>(Condition-Generative Adversarial Network, CGAN)将像素级的语义标签映射到高分辨率图像(Pix to Pix High-Resolu-tion Images from Sematic Label Maps, Pix2pixHR),通过全局和局部生成器及多尺度鉴别器加强对图像纹理细节的学习.Park等<citation id="317" type="reference"><link href="282" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>设计高斯条件生成对抗网络结构(Gaussian Generative Adversarial Network, GauGAN),可根据给定的标签类别构建涂鸦式草图,生成真实感图像.上述两种方法都可完成图像的异质转换,但设计目标并不是针对山水画仿真.</p>
                </div>
                <div class="p1">
                    <p id="54">综上所述,以往的山水画仿真方法或图像异质转换方法尚未考虑山水画整体的布局设计和对象生成与组合的自然性、合理性等方面.因此,本文提出考虑布局和笔触因素、可实现完整画作生成的中国山水画仿真方法.分析表明,山水画的绘制过程讲究胸中有布局,继而下笔成画,整个过程首先具有布局的引导,然后以特定的笔触完成画作.考虑到这种绘画规律,本文设计可反映布局意图的布局标签图结构,并借鉴CGAN的思想,针对山水画布局和笔触的仿真问题,设计并训练多尺度特征融合的网络结构(Multi-scale Feature Fusion CGAN, MSFF-CGAN),解决布局标签图到山水画仿真生成这一图像的异质转换问题,并通过不同尺度的采样方式,加强网络对笔触特征的学习.本文设计采用语义关联的颜色像素聚类算法快速生成标签图,不仅可用作训练数据,也可为用户提供布局编辑的参考.在上述工作基础上,为了提高仿真画的艺术真实感,引入MemNet<citation id="318" type="reference"><link href="284" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>超分辨网增强和优化生成图的纹理细节.实验表明,本文方法生成的仿真山水画更具完整性和艺术真实感,不仅可应对简单的手绘涂鸦式草图,还可通过在布局空间上的编辑操作,达到对画作空间进行编辑的效果.</p>
                </div>
                <h3 id="55" name="55" class="anchor-tag">1 本文算法原理及流程</h3>
                <h4 class="anchor-tag" id="56" name="56"><b>1.1 本文算法流程</b></h4>
                <div class="p1">
                    <p id="57">本文算法具体流程如图1所示.</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909010_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文算法流程图" src="Detail/GetImg?filename=images/MSSB201909010_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909010_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Flow chart of the proposed algorithm</p>

                </div>
                <div class="p1">
                    <p id="59">本文算法基本过程是用户输入任意一幅山水画的布局标签图,利用MSFF-CGAN网络生成一幅具有相同布局的第一层仿真山水画.再将生成的山水画经由 MemNet超分辨网络增强细节纹理,使山水画更具艺术真实感,最终完成山水画仿真.</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60"><b>1.2 MSFF-CGAN网络原理分析</b></h4>
                <div class="p1">
                    <p id="61">由于GAN采用无监督学习的方式,生成模型的不可控性导致在图像生成过程中可能产生不符合常理的结果图,无法锁定生成特定的目标<citation id="319" type="reference"><link href="286" rel="bibliography" /><link href="288" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="62">本文的首要目标是完成山水画布局标签图到具有相同布局的仿真山水画的转换,是一种有限制的图像异质转换问题,因此需要对GAN加以约束,使生成结果具有指向性.本文采用CGAN,不同于GAN,CGAN在生成模型和判别模型上分别引入一个条件变量作为约束,该条件变量提供样本数据的类别信息,如样本数据的类标签<citation id="320" type="reference"><link href="290" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>.这种可控性也正是本文带有布局约束的山水画仿真生成所需的,其中的类标签就是山水画的布局标签图.</p>
                </div>
                <div class="p1">
                    <p id="63">山水画具有特定的笔触,注重轮廓线条的勾勒皴擦这一特性,除了保证仿真画不仅在绘制风格上与真实山水画一致,还要保持绘制内容的轮廓和纹理与真实山水画具有相似的线条感和粒度感.因此,基于CGAN,本文提出针对山水画仿真的多尺度特征融合的条件生成对抗网(MSFF-CGAN).</p>
                </div>
                <div class="p1">
                    <p id="64">网络首先将成对的“手绘山水画(<i>x</i>)-布局标签图(<i>y</i>)”作为训练数据输入,如图2所示.</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909010_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 MSFF-CGAN网络的生成器和鉴别器原理" src="Detail/GetImg?filename=images/MSSB201909010_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 MSFF-CGAN网络的生成器和鉴别器原理  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909010_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Principle of generator and discriminator of MSFF-CGAN network</p>

                </div>
                <div class="p1">
                    <p id="67">模型包含随机噪声<i>z</i>,它一般采样于某个先验分布<i>p</i><sub><i>z</i></sub>(<i>z</i>),如高斯分布<citation id="321" type="reference"><link href="292" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>.生成器<i>G</i>将输入图像(<i>z</i>,<i>y</i>)转换为生成图像<i>G</i>(<i>z</i>,<i>y</i>).鉴别器<i>D</i>鉴别生成图像<i>G</i>(<i>z</i>,<i>y</i>)的真假.当鉴别器<i>D</i>无法区分生成样本与真实样本时,认为生成器<i>G</i>达到最优.</p>
                </div>
                <div class="p1">
                    <p id="68">研究表明,仅靠鉴别器的特征损失信息作为模型的调整依赖仍会产生模糊结果,而加入传统范数作为损失项的一部分(如<i>L</i>1或<i>L</i>2范数),有助于减少模糊,提高生成图的真实感<citation id="322" type="reference"><link href="294" rel="bibliography" /><link href="296" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>.因此,本文最终的网络训练目标函数设计如下:</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable><mtr><mtd columnalign="left"><mi>L</mi><mi>a</mi><mi>y</mi><mi>o</mi><mi>u</mi><mi>t</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>S</mtext><mtext>F</mtext><mtext>F</mtext><mo>-</mo><mtext>C</mtext><mtext>G</mtext><mtext>A</mtext><mtext>Ν</mtext></mrow></msub><mo>=</mo></mtd></mtr><mtr><mtd columnalign="left"><mtext> </mtext><mtext> </mtext><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mspace width="0.25em" /><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∈</mo><mi>G</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi><mo>-</mo><mi>t</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi><mo>,</mo><mi>y</mi><mo>∈</mo><mi>G</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi><mo>-</mo><mi>t</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi></mrow></msub><mo stretchy="false">[</mo><mi>ln</mi><mspace width="0.25em" /><mi>D</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd columnalign="left"><mtext> </mtext><mtext> </mtext><mi>E</mi><msub><mrow></mrow><mrow><mi>y</mi><mo>∈</mo><mi>G</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi><mo>-</mo><mi>t</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi><mo>,</mo><mi>z</mi><mo>∈</mo><mi>p</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>ln</mi><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">(</mo><mi>z</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd columnalign="left"><mtext> </mtext><mtext> </mtext><mi>λ</mi><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∈</mo><mi>G</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi><mo>-</mo><mi>t</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi><mo>,</mo><mi>y</mi><mo>∈</mo><mi>G</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi><mo>-</mo><mi>t</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi><mo>,</mo><mi>z</mi><mo>∈</mo><mi>p</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mrow><mo stretchy="false">∥</mo><mi>x</mi><mo>-</mo><mi>G</mi><mo stretchy="false">(</mo><mi>z</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">]</mo><mo>,</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">其中<i>λ</i>为正则系数.</p>
                </div>
                <h3 id="71" name="71" class="anchor-tag">2 MSFF-CGAN结构设计与分析</h3>
                <h4 class="anchor-tag" id="72" name="72"><b>2.1 UC-Net的结构设计</b></h4>
                <div class="p1">
                    <p id="73">在布局标签图到仿真山水画的转换问题中,生成器的设计需考虑如下三方面:1)仿真画与标签图应具有布局结构的基本一致性;2)仿真画与标签图色块对应的位置,语义内容具有基本一致性;3)仿真画应具有中国山水画线条笔触的特征.</p>
                </div>
                <div class="p1">
                    <p id="74">传统的生成模型大多采用编码-解码(Enconder-Decorder)网络<citation id="323" type="reference"><link href="298" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>，使用这种网络生成的仿真山水画可以保证输入与输出图像大小一致，但在图像特征提取到生成的过程中容易丢失图像的上下文信息，无法保证布局图与生成图的语义内容一致性，同时生成图的清晰度较低，尤其是景物轮廓和细节纹理易呈马赛克状．</p>
                </div>
                <div class="p1">
                    <p id="76">针对上述问题,本文首先采用对称式的U形长连接(简记U-Net)网络模型<citation id="324" type="reference"><link href="300" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>作为生成器的结构模型,在编解码网络中添加长连接,如图3生成器(<i>G</i>)原理图中蓝色箭头所示,将网络编码部分提取的上下文特征信息复用于解码层,加强特征信息的传递.</p>
                </div>
                <div class="p1">
                    <p id="77">采用U-Net能保证生成图与标签图语义内容和布局结构的基本一致性,但随着网络深度的增加,神经网络的卷积过程会丢失部分高频信息,使模型在生成内容的线条和纹理细节上模糊不清.针对山水画特定笔触线条的仿真问题,本文对生成器进行增强设计,在编码层和解码层中分别添加C形的短连接,称为C-Net,如图3紫色箭头和黄色箭头所示.当前层的输入融合前两层不同尺度采样的结果及上一层的输出,加强山水画笔触线条这类高频特征在层与层之间的传递和学习.如此形成的生成器模型,称为UC-Net.采用UC-Net的网络结构有利于特征的重利用,并且不同尺度的特征融合进一步加强上下文信息的传递,使网络可以更多地学习山水画风格的笔触和景物纹理细节.</p>
                </div>
                <div class="p1">
                    <p id="78">由于生成器网络层数较多,无法全部画出,图3只展示生成器的设计原理.生成器网络一共16层,采用标准4×4的卷积核,各层具体参数见表1.</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909010_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 面向山水画布局的MSFF-CGAN网络示意图" src="Detail/GetImg?filename=images/MSSB201909010_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 面向山水画布局的MSFF-CGAN网络示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909010_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Sketch map of landscape painting layout of MSFF-CGAN network</p>

                </div>
                <div class="area_img" id="80">
                    <p class="img_tit"><b>表1 生成器参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1 Generator parameters</p>
                    <p class="img_note"></p>
                    <table id="80" border="1"><tr><td><br /></td><td>层数</td><td>结构</td><td>卷积核<br />个数</td><td>步长</td></tr><tr><td rowspan="4"><br />编<br />码<br />器</td><td><br />Layer 1</td><td>Conv2d</td><td>64</td><td>1</td></tr><tr><td><br />Layer 2</td><td>Lrelu(Leak0.2),Conv2d,<br />Batchnorm</td><td>128</td><td>1</td></tr><tr><td><br />Layer 3</td><td>Lrelu(Leak0.2),Conv2d,<br />Batchnorm</td><td>256</td><td>1</td></tr><tr><td><br />Layer 4～<br />Layer 8</td><td>Lrelu(Leak0.2),Conv2d,<br />Batchnorm</td><td>512</td><td>1</td></tr><tr><td rowspan="5"><br />解<br />码<br />器</td><td><br />Layer 9～<br />Layer 12</td><td>Relu,Conv2d,Batchnorm</td><td>512</td><td>1</td></tr><tr><td><br />Layer 13</td><td>Relu,Conv2d,Batchnorm</td><td>256</td><td>1</td></tr><tr><td><br />Layer 14</td><td>Relu,Conv2d,Batchnorm</td><td>128</td><td>1</td></tr><tr><td><br />Layer 15</td><td>Relu,Conv2d,Batchnorm</td><td>64</td><td>1</td></tr><tr><td><br />Layer 16</td><td>Relu,Conv2d,Tanh</td><td>3</td><td>1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>2.2 鉴别器结构设计</b></h4>
                <div class="p1">
                    <p id="82">传统模型的鉴别器,如以视觉几何组(<i>Visual Geometry Group</i>, <i>VGG</i>)网络<citation id="325" type="reference"><link href="302" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>为基础的鉴别器,最终的判别依据是网络提取的高度抽象的语义信息.但针对山水画仿真的问题,对于生成仿真山水画的真假判定不仅需要依据语义信息,如画中某一区域是否为山,还需考虑如纹理这类细粒度信息,如仿真画中山的纹理是否符合山水画的风格.为了加强这种粒度信息和抽象的语义信息之间的相关性,可对图像进行分块鉴别,通过缩减图像粒度的方式,使判别网络更倾向于从图像细粒度特征的相似性以鉴别图像.因此,本文借鉴块生成对抗网(<i>Patch Genera</i>-tive Adversarial Network,PatchGAN)<citation id="326" type="reference"><link href="278" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>思想．在鉴别时，将图像分割为70×70的图像块，对于每个图像块进行全卷积操作，并使用函数激活得到当前区域块的鉴定值，最终统计所有图像块的鉴定值求均值作为当前图像的全局鉴定值．在提高生成图质量的同时，减少网络参数，加快运行速度．鉴别器结构如图3实线框所示，本文的鉴别器网络共5层，采用标准4×4的卷积核，各层的具体参数如表2所示．</p>
                </div>
                <div class="area_img" id="84">
                    <p class="img_tit"><b>表2 鉴别器参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Table</i> 2 <i>Discriminator parameters</i></p>
                    <p class="img_note"></p>
                    <table id="84" border="1"><tr><td><br />层数</td><td>结构</td><td>卷积核<br />个数</td><td>步长</td></tr><tr><td><br /><i>Laye</i> 1</td><td><i>Conv</i>2<i>d</i>,<i>Lrelu</i></td><td>64</td><td>2</td></tr><tr><td><br /><i>Laye</i> 2</td><td><i>Conv</i>2<i>d</i>,<i>Batchnorm</i><br /><i>Lrelu</i>(<i>Leak</i>0.2)</td><td>128</td><td>2</td></tr><tr><td><br /><i>Laye</i> 3</td><td><i>Conv</i>2<i>d</i>,<i>Batchnorm</i><br /><i>Lrelu</i>(<i>Leak</i>0.2)</td><td>256</td><td>2</td></tr><tr><td><br /><i>Laye</i> 4</td><td><i>Conv</i>2<i>d</i>,<i>Batchnorm</i><br /><i>Lrelu</i>(<i>Leak</i>0.2)</td><td>384</td><td>2</td></tr><tr><td><br /><i>Laye</i> 5</td><td><i>Conv</i>2<i>d</i>,<i>Batchnorm</i>,<i>Sigmoid</i></td><td>1</td><td>1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="85" name="85"><b>2.3 MemNet超分辨网络</b></h4>
                <div class="p1">
                    <p id="86">通过对生成器与鉴别器的改进,在一定程度上提高生成图质量,但随着网络深度的增加,部分细节、纹理信息仍可能丢失.因此引入MemNet超分辨网络<citation id="327" type="reference"><link href="284" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>对生成图像进行进一步的细节增强和优化.</p>
                </div>
                <div class="p1">
                    <p id="87">MemNet超分辨网络为长期记忆网络,设置多个记忆单元(Memory Block),用于保存在特征提取过程中的信息,每块单元(Block)的输入都取决于前一块的输出,有利于特征信息的共享与传递.将每块block保存的特征信息映射到最后一层重建网络(Reconstruction Network, ReconNet)中输出,通过加权求和的方式实现对图像的增强.MemNet网络结构如图4所示.</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909010_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 MemNet网络结构" src="Detail/GetImg?filename=images/MSSB201909010_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 MemNet网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909010_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Network structure of MemNet</p>

                </div>
                <h3 id="89" name="89" class="anchor-tag">3 MSFF-CGAN网络训练数据生成方法</h3>
                <h4 class="anchor-tag" id="90" name="90"><b>3.1 手绘山水画训练数据的收集和处理</b></h4>
                <div class="p1">
                    <p id="91">训练面向山水画布局的MSFF-CGAN网络需要大量的“手绘山水画-布局标签图”数据对,而现有的公开网络数据集,如ImageNet<citation id="328" type="reference"><link href="304" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、Pascal Voc<citation id="329" type="reference"><link href="306" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>等均无这种类型的数据.</p>
                </div>
                <div class="p1">
                    <p id="92">同时,为了使网络的生成效果较好,对手绘山水画数据集的清晰度和画作内容也有一定要求,对画作内容的挑选主要以山水类自然景观为主体.</p>
                </div>
                <div class="p1">
                    <p id="93">因此,本文的数据集主要通过从国内字画网上获取,并筛除如下情况的画作:1)明显水印遮挡的画作;2)分辨率较低,内容不清晰的画作;3)主体非山水类自然景观的画作.对符合要求的手绘山水画数据集使用图像处理工具去除非自然景观的物体及各类题款,以降低后期布局标签图制作的复杂性.最终获得景物山水画1 000幅.</p>
                </div>
                <div class="p1">
                    <p id="94">由于山水画数据集没有统一的尺寸,不便于模型的训练,因此本文在训练前将所有数据集均调整至256×256,选择丢弃尺寸过小的数据,山水画的部分数据展示如图5(a)所示.</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909010_09800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 山水画训练数据对" src="Detail/GetImg?filename=images/MSSB201909010_09800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 山水画训练数据对  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909010_09800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Training data pairs of landscape painting</p>

                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909010_09801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 山水画训练数据对" src="Detail/GetImg?filename=images/MSSB201909010_09801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 山水画训练数据对  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909010_09801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Training data pairs of landscape painting</p>

                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>3.2 布局标签图的设计与聚类生成</b></h4>
                <div class="p1">
                    <p id="100">由于时间、样本数量等条件约束,人工生成并标注大量的山水画布局标签图并不现实,因此,针对手绘山水画设计并提取对应的布局标签图.</p>
                </div>
                <div class="p1">
                    <p id="101">布局标签图是指使用不同色块指代山水画中不同的语义内容,同时每个色块外形还应具有内容的大致轮廓信息.因此,首要任务是确定布局标签图的类别和对应的语义.分析发现,以自然景观为主体的山水画具有如下2个绘制特点:1)自然景观山水画主要以云水、山石和植被作为绘制的主体内容,景物间的色相、对比度差异较明显.例如,偏灰褐色的景物大多为山石,留白部分多指代云、水,植被的颜色多为青绿、墨绿;2)山水画强调虚实结合、情景交融,云水、山石和植被在绘制上通常具有相似的线条结构和相融的空间位置.基于上述特点,本文将布局标签图的类别分为3类:云水、山石、植被.这种分类方式虽然较粗略但又基本胜任,能节省网络训练的时间成本.</p>
                </div>
                <div class="p1">
                    <p id="102">确定布局标签图的类别后,需对手绘山水画中的对应语义内容进行分割聚类,形成“色块-内容”对 应 的 山 水 画 布 局 标签图.根据特点2)可知,山水画在景物对象间无需清晰的边界区分,因此,在获取布局标签图时只需提取三类对象的大致轮廓,并且在每类内容的边界上允许有一定的误差.综上所述,本文采用语义关联的颜色像素聚类分割方法,旨在快速、高效地提取山水画的布局标签图.</p>
                </div>
                <div class="p1">
                    <p id="103">首先,为了减少噪声对布局标签图提取的影响,采用5×5的中值滤波器预处理山水画数据集.其次,针对上述山水画的绘制特点,采用像素聚类的方式,随机选定3个像素点作为3个聚类中心,分别计算每个像素点的<i>R</i>、<i>G</i>、<i>B</i>分量与3个聚类中心的<i>R</i>、<i>G</i>、<i>B</i>分量的差值并求模,按就近原则划分像素点所在类簇.由于初始聚类中心的随机性,难以保证所有的山水画“色块-内容”标签一致.为了统一布局标签图的标签,根据山水画的绘制内容在颜色着墨上差异较大的特点,对输入的山水画分别求三类分割内容的平均灰度值,依据平均灰度值的大小分配“色块-内容”标签,最终完成布局标签图的提取,如图5(b)所示.</p>
                </div>
                <div class="p1">
                    <p id="104"><i>X</i><sub><i>i</i></sub>表示第<i>i</i>个像素点的RGB值,<i>i</i>=1,2,…,<i>m</i>×<i>n</i>,<i>I</i>(<i>X</i><sub><i>i</i></sub>)表示像素点<i>X</i><sub><i>i</i></sub>对应的聚类中心,<i>C</i><mathml id="206"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>t</mi></msubsup></mrow></math></mathml>表示<i>t</i>时刻第<i>j</i>类聚类中心, <i>j</i>=1,2,3,<i>B</i><mathml id="207"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>t</mi></msubsup></mrow></math></mathml>表示<i>t</i>时刻第<i>j</i>类像素点集.像素点与聚类中心的距离公式及像素点的归类方式如下:</p>
                </div>
                <div class="area_img" id="105">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/MSSB201909010_10500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="107">对聚类中心的更新采用加权求和取均值的方式:</p>
                </div>
                <div class="p1">
                    <p id="108"><mathml id="208"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msubsup><mrow></mrow><mi>j</mi><mi>t</mi></msubsup><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><mi>B</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow><mo>|</mo></mrow></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>X</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mspace width="0.25em" /><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>B</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>.      (2)</p>
                </div>
                <div class="p1">
                    <p id="109">山水画布局标签图的聚类分割算法步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="110"><b>算法 1</b> 山水画布局标签图的聚类分割算法</p>
                </div>
                <div class="p1">
                    <p id="111"><b>输入</b> 手绘山水画数据集</p>
                </div>
                <div class="p1">
                    <p id="112"><b>输出</b> 手绘山水画对应的布局标签图<i>Layout</i></p>
                </div>
                <div class="p1">
                    <p id="113">step 1 记当前的输入图像为 <i>A</i><sub>1</sub>,大小为<i>m</i>×<i>n</i>.</p>
                </div>
                <div class="p1">
                    <p id="114">step 2 使用5×5的中值滤波器对图像<i>A</i><sub>1</sub>滤波得到图像<i>A</i><sub>2</sub>.</p>
                </div>
                <div class="p1">
                    <p id="115">step 3 分解图像<i>A</i><sub>2</sub>的<i>RGB</i>三通道,由<i><b>R</b></i>、<i><b>G</b></i>、<i><b>B</b></i>这3个向量组成矩阵<i><b>M</b></i>=[<i><b>R</b></i>,<i><b>G</b></i>,<i><b>B</b></i>],大小<i>m</i>×<i>n</i>×3.</p>
                </div>
                <div class="p1">
                    <p id="116">step 4 建立3个像素点集合<i>B</i><mathml id="209"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mn>0</mn></msubsup></mrow></math></mathml>,<i>B</i><mathml id="210"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>0</mn></msubsup></mrow></math></mathml>,<i>B</i><mathml id="211"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>3</mn><mn>0</mn></msubsup></mrow></math></mathml>,从<i><b>M</b></i>中随机选择3个像素点作为初始聚类中心,记为<i>C</i><mathml id="212"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mn>0</mn></msubsup></mrow></math></mathml>,<i>C</i><mathml id="213"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>0</mn></msubsup></mrow></math></mathml>,<i>C</i><mathml id="214"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>3</mn><mn>0</mn></msubsup></mrow></math></mathml>,分别保存到对应的像素点集合中.</p>
                </div>
                <div class="p1">
                    <p id="117">step 5 遍历<i><b>M</b></i>.对于当前像素点<i>X</i><sub><i>i</i></sub>,按式(1)分别计算与3个聚类中心的RGB分量差值并求模,选择模值最小的聚类中心所在的集合保存当前像素点.</p>
                </div>
                <div class="p1">
                    <p id="118">step 6 按式(2)的更新方式重新计算聚类中心<i>C</i><mathml id="215"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>t</mi></msubsup></mrow></math></mathml>,<i>C</i><mathml id="216"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>t</mi></msubsup></mrow></math></mathml>,<i>C</i><mathml id="217"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>3</mn><mi>t</mi></msubsup></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="119">step 7 判断<i>t</i>时刻第<i>j</i>类聚类中心<i>C</i><mathml id="218"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>t</mi></msubsup></mrow></math></mathml>与<i>t</i>-1时刻第<i>j</i>类聚类中心<i>C</i><mathml id="219"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>是否满足<i>C</i><mathml id="220"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>t</mi></msubsup></mrow></math></mathml>=<i>C</i><mathml id="221"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>,否则重复step 5和step 6.</p>
                </div>
                <div class="p1">
                    <p id="120">step 8 分别计算像素点集合<i>B</i><mathml id="222"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>t</mi></msubsup></mrow></math></mathml>、<i>B</i><mathml id="223"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>t</mi></msubsup></mrow></math></mathml>、<i>B</i><mathml id="224"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>3</mn><mi>t</mi></msubsup></mrow></math></mathml>的平均灰度值并排序,从大到小赋予集合颜色标签.记生成的布局标签图为<i>Layout</i>.</p>
                </div>
                <h3 id="121" name="121" class="anchor-tag">4 实验及结果分析</h3>
                <h4 class="anchor-tag" id="122" name="122"><b>4.1 实验环境配置</b></h4>
                <div class="p1">
                    <p id="123">本文的训练数据处理和布局标签图的生成均在Windows和Matlab平台下实现.在Tensorflow 环境中实现山水画布局标签图转换成仿真山水画,在caffee环境下实现山水画的超分辨, 配置GPU Tesla P100.使用adam优化器,MSFF-CGAN的学习率设置为0.000 2,正则系数<i>λ</i>=100,迭代次数为300.</p>
                </div>
                <div class="p1">
                    <p id="124">本文共收集和生成1 000对“手绘山水画-布局标签图”实验数据集,其中960对作为训练样本,其余40对作为测试样本.为了验证本文方法可适用于编辑后和涂鸦式的布局标签图的转换,也展示以非训练集的布局标签图为底本编辑的生成效果和手绘涂鸦式布局标签图作为输入的生成效果.</p>
                </div>
                <h4 class="anchor-tag" id="125" name="125"><b>4.2 山水画仿真学习能力的效果</b></h4>
                <div class="p1">
                    <p id="126">图6为本文单层和双层两种结构的仿真效果对比.从视觉效果上看,MSFF-CGAN网络基本保证生成图与标签图在布局结构和色块对应语义内容上的一致性,较好地模拟云水、山石和植被的轮廓线条和纹理,在不同的构图布局下,生成的仿真山水画色调合理,内容完整.</p>
                </div>
                <div class="area_img" id="256">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909010_25600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 本文网络山水画仿真效果" src="Detail/GetImg?filename=images/MSSB201909010_25600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 本文网络山水画仿真效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909010_25600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Landscape painting simulation results</p>

                </div>
                <div class="area_img" id="256">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909010_25601.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 本文网络山水画仿真效果" src="Detail/GetImg?filename=images/MSSB201909010_25601.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 本文网络山水画仿真效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909010_25601.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Landscape painting simulation results</p>

                </div>
                <div class="p1">
                    <p id="136">为了进一步评估仿真效果,本文采用图像纹理的粗糙度指标衡量三类绘制内容在仿真山水画与原作上的差异.粗糙度目标是获取使每个像素点垂直和水平方向灰度变化最大的邻域,则这个邻域的均值差值也最能反映图像的纹理信息.由于早期山水画的仿真工作通常将山石、流水等景物对象视作纹理,利用纹理合成技术对特定对象仿真,因此可将山水画的不同对象视作一种纹理.其次,本文分类的山水画标签分别为云水、山石和植被,类内结构和笔法相近,而类间差异较大,具有较明显的统计区分性,因此采用经典的刻画纹理特征的统计量——粗糙度衡量仿真效果具有一定的合理性.</p>
                </div>
                <div class="p1">
                    <p id="137">下面以山石类为例,给出粗糙度的计算步骤如下.</p>
                </div>
                <div class="p1">
                    <p id="138">首先在仿真山水画中提取对应布局标签图中浅蓝色分量的绘制部分.下面计算山石类的粗糙度.记山石类的绘制图大小为<i>m</i>×<i>n</i>,计算在2<sup><i>k</i></sup>×2<sup><i>k</i></sup>的滑动窗口中当前像素点(<i>x</i>,<i>y</i>)的平均灰度值:</p>
                </div>
                <div class="p1">
                    <p id="139" class="code-formula">
                        <mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><msup><mrow></mrow><mrow><mn>2</mn><mi>k</mi></mrow></msup></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>x</mi><mo>-</mo><mn>2</mn><msup><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow><mrow><mi>x</mi><mo>+</mo><mn>2</mn><msup><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>-</mo><mn>1</mn></mrow></munderover><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>y</mi><mo>-</mo><mn>2</mn><msup><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow><mrow><mi>y</mi><mo>+</mo><mn>2</mn><msup><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>-</mo><mn>1</mn></mrow></munderover><mi>g</mi></mstyle><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo><mo>,</mo><mspace width="0.25em" /><mn>1</mn><mo>≤</mo><mi>k</mi><mo>≤</mo><mn>8</mn><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="140">其中<i>g</i>(<i>i</i>,<i>j</i>)表示在当前窗口中像素点的灰度值.</p>
                </div>
                <div class="p1">
                    <p id="141">分别计算每个像素点在水平方向和竖直方向上不重叠窗口间的差值:</p>
                </div>
                <div class="p1">
                    <p id="142" class="code-formula">
                        <mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>E</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>,</mo><mi>h</mi><mi>o</mi><mi>r</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">|</mo><mi>A</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mn>2</mn><msup><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mo>-</mo><mo stretchy="false">|</mo><mi>A</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>-</mo><mn>2</mn><msup><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mo>,</mo></mtd></mtr><mtr><mtd><mi>E</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>,</mo><mi>v</mi><mi>e</mi><mi>r</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">|</mo><mi>A</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>+</mo><mn>2</mn><msup><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mo>-</mo><mo stretchy="false">|</mo><mi>A</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>-</mo><mn>2</mn><msup><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mo>.</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="143">对于当前像素点(<i>x</i>,<i>y</i>),找到</p>
                </div>
                <div class="p1">
                    <p id="144" class="code-formula">
                        <mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>k</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mn>1</mn><mo>≤</mo><mi>k</mi><mo>≤</mo><mn>8</mn></mrow></munder><mo stretchy="false">{</mo><mi>E</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">}</mo><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="145">使得</p>
                </div>
                <div class="p1">
                    <p id="146"><i>E</i><sub><i>k</i></sub>(<i>x</i>,<i>y</i>)=<i>E</i><sub>max</sub>(<i>x</i>,<i>y</i>)=max(<i>E</i><sub><i>k</i></sub><sub>,</sub><sub><i>hor</i></sub>(<i>x</i>,<i>y</i>),<i>E</i><sub><i>k</i></sub><sub>,</sub><sub><i>ver</i></sub>(<i>x</i>,<i>y</i>)).</p>
                </div>
                <div class="p1">
                    <p id="147">则当前像素点的最优窗口大小<i>S</i><sub>best</sub>(<i>x</i>,<i>y</i>)=2<sup><i>k</i></sup><sub><sup>0</sup></sub><sup>(</sup><sup><i>x</i></sup><sup>,</sup><sup><i>y</i></sup><sup>)</sup>.</p>
                </div>
                <div class="p1">
                    <p id="148">对所有的<i>S</i><sub>best</sub>加权求和取均值,求得山石的粗糙度:</p>
                </div>
                <div class="p1">
                    <p id="149" class="code-formula">
                        <mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>r</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>m</mi><mi>n</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>i</mi><mi>m</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>j</mi><mi>n</mi></munderover><mi>S</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mtext>b</mtext><mtext>e</mtext><mtext>s</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="150">表3为手绘山水画与仿真山水画分别在云水、山石和植被上的粗糙度对比,可以看出本文方法的仿真山水画在纹理的粗糙度上与原图的差异较小,表明本文方法在三类绘制内容上与原作的笔触和纹理结构类似.</p>
                </div>
                <div class="area_img" id="151">
                    <p class="img_tit"><b>表3 原作与仿真图在3类对象的粗糙度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 Roughness comparison between original paintings and simulation paintings for 3 objects</p>
                    <p class="img_note"></p>
                    <table id="151" border="1"><tr><td rowspan="2"><br />图6(a)</td><td colspan="2"><br />云水类</td><td colspan="2"><br />山石类</td><td colspan="2"><br />植被类</td></tr><tr><td><br />原作</td><td>仿真图</td><td><br />原作</td><td>仿真图</td><td><br />原作</td><td>仿真图</td></tr><tr><td>第1幅</td><td>19.2</td><td>19.3</td><td>14.9</td><td>15.2</td><td>16.2</td><td>15.9</td></tr><tr><td>第2幅</td><td>20.4</td><td>21.0</td><td>14.5</td><td>14.7</td><td>13.8</td><td>13.7</td></tr><tr><td><br />第3幅</td><td>21.1</td><td>21.7</td><td>16.9</td><td>16.9</td><td>18.1</td><td>17.3</td></tr><tr><td><br />第4幅</td><td>20.4</td><td>20.7</td><td>16.3</td><td>16.4</td><td>17.1</td><td>16.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="152" name="152"><b>4.3 MSFF-CGAN与MSFF-CGAN+MemNet 实 验效果对比</b></h4>
                <div class="p1">
                    <p id="153">在测试集上进行实验,评估单层MSFF-CGAN和双层MSFF-CGAN+MemNet生成图的差异性,效果如图6所示.MSFF-CGAN能够较好的完成布局标签图到仿真山水画的转换任务,但在真实感和细节上还有所欠缺.而MSFF-CGAN+MemNet的设计进一步优化纹理细节,使仿真山水画在线条感和艺术真实感上都有较大提升.</p>
                </div>
                <div class="p1">
                    <p id="154">为了进一步量化两种方法在生成效果上的差异,本文采用基于图像平均梯度的清晰度指标对比生成效果.图像平均梯度越大,表明图像的层次越丰富、越清晰.</p>
                </div>
                <div class="p1">
                    <p id="155">如表4所示,MSFF-CGAN+MemNet 的清晰度明显优于MSFF-CGAN.</p>
                </div>
                <div class="area_img" id="156">
                    <p class="img_tit"><b>表4 两种网络结构清晰度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4 Sharpness comparison of 2 network structures</p>
                    <p class="img_note"></p>
                    <table id="156" border="1"><tr><td><br />图6(a)</td><td>MSFF-CGAN</td><td>MSFF-CGAN+MemNet</td></tr><tr><td><br />第1幅</td><td>10.7</td><td>18.3</td></tr><tr><td><br />第2幅</td><td>8.5</td><td>14.5</td></tr><tr><td><br />第3幅</td><td>9.2</td><td>16.5</td></tr><tr><td><br />第4幅</td><td>9.3</td><td>16.4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="157" name="157"><b>4.4 UC-Net与U-Net生成器的对比分析</b></h4>
                <div class="p1">
                    <p id="158">为了验证UC-Net比U-Net生成器在景物的线条笔触模拟上效果更好,本文选取两组具有典型线条细节的布局标签图,分别使用不同的生成器生成仿真山水画,结果对比如图7所示,生成图像的右侧为虚线框处的内容放大图.</p>
                </div>
                <div class="area_img" id="257">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909010_25700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 U-Net和UC-Net的生成效果对比" src="Detail/GetImg?filename=images/MSSB201909010_25700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 U-Net和UC-Net的生成效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909010_25700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Results comparison of U-Net and UC-Net</p>

                </div>
                <div class="p1">
                    <p id="165">由图7可看出,U-Net生成器在景物的轮廓和线条生成上不太理想,图中的山石和树叶均存在边缘轮廓和细节纹理模糊不清的现象.而使用UC-Net生成的仿真山水画在山石和树叶的轮廓线条模拟上更清晰.相比U-Net,UC-Net的生成效果更具艺术真实感和线条感.</p>
                </div>
                <div class="p1">
                    <p id="166">为了进一步评估UC-Net生成器的有效性,本文采用图像的峰值信噪比(Peak Signal to Noise Ratio,PSNR)对U-Net和UC-Net的生成图进行定量对比,分别计算总体及三类绘制内容的PSNR,如表5所示.由表可看出,采用UC-Net生成器的生成效果优于U-Net.</p>
                </div>
                <div class="area_img" id="167">
                    <p class="img_tit"><b>表5 U-Net与UC-Net的PSNR对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 5 PSNR comparison between U-Net and UC-Net</p>
                    <p class="img_note"></p>
                    <table id="167" border="1"><tr><td rowspan="2"><br /></td><td rowspan="2">方法</td><td colspan="2"><br />图7(a)</td></tr><tr><td><br />第1幅</td><td>第2幅</td></tr><tr><td><br />云水类</td><td>U-Net<br />UC-Net</td><td>22.9<br />24.3</td><td>25.4<br />26.5</td></tr><tr><td><br />山石类</td><td>U-Net<br />UC-Net</td><td>24.1<br />26.7</td><td>26.5<br />29.5</td></tr><tr><td><br />植被类</td><td>U-Net<br />UC-Net</td><td>25.8<br />26.4</td><td>29.7<br />30.8</td></tr><tr><td><br />总体</td><td>U-Net<br />UC-Net</td><td>19.3<br />20.9</td><td>22.2<br />23.6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="168" name="168"><b>4.5 L1范数和L2范数的实验效果</b></h4>
                <div class="p1">
                    <p id="169">不同范数的损失函数会一定程度影响网络的整体仿真性能,L1、L2范数通常用于控制生成图的清晰度.因此,本文设置涂鸦图(图8(a)第1幅)和非涂鸦图(图8(a)第2幅和第3幅)两种输入方式.针对损失函数,对比两种不同范数类型生成的结果.实验结果如图8所示.由图可看出,使用L1范数生成的仿真山水画纹理细节更清晰、线条感更好.</p>
                </div>
                <div class="area_img" id="258">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909010_25800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 两种范数的生成效果对比" src="Detail/GetImg?filename=images/MSSB201909010_25800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 两种范数的生成效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909010_25800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Comparison of results generated by 2 norms</p>

                </div>
                <div class="p1">
                    <p id="177">为 了 更 客 观地对比L1和L2范数的生成效果,本文采用基于图像平均梯度的清晰度指标进行定量对比.从表6可看出,使用L1范数的生成图清晰度优于L2范数.</p>
                </div>
                <div class="area_img" id="178">
                    <p class="img_tit"><b>表6 两种范数生成图的清晰度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 6 Sharpness comparison of 2 norm-generated graphs</p>
                    <p class="img_note"></p>
                    <table id="178" border="1"><tr><td><br />图8(a)</td><td>L2范数</td><td>L1范数</td></tr><tr><td><br />第1幅</td><td>2.9</td><td>4.7</td></tr><tr><td><br />第2幅</td><td>4.9</td><td>6.5</td></tr><tr><td><br />第3幅</td><td>10.2</td><td>12.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="179" name="179"><b>4.6 与Pix2pixHR的生成效果对比</b></h4>
                <div class="p1">
                    <p id="180">Pix2pixHR<citation id="330" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation> 是一种能够生成高分辨效果的图像异质转换网络.本文将960对“手绘山水画-布局标签图”作为训练数据,利用Pix2pixHR网络生成仿真山水画.图9为本文方法与Pix2pixHR的对比图.</p>
                </div>
                <div class="area_img" id="259">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909010_25900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 Pix2pixHR和本文方法的生成效果对比" src="Detail/GetImg?filename=images/MSSB201909010_25900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 Pix2pixHR和本文方法的生成效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909010_25900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Generation results comparison between the proposed method and Pix2pixHR</p>

                </div>
                <div class="p1">
                    <p id="188">如图9所示,从视觉效果上看,当布局标签图的色块大部分呈片状且细节信息较少时,本文生成的仿真山水画能够正确完成布局标签图到仿真图的转换过程,而Pix2pixHR会出现失真、模糊不清甚至无意义的图像.其次,本文方法的仿真山水画更具色彩和艺术真实感,线条更流畅、噪声更小.</p>
                </div>
                <div class="p1">
                    <p id="189">本文还对比2种方法的PSNR,如表7所示.由表可看出,本文方法的仿真山水画在三类内容和总体的PSNR值上均高于Pix2pixHR,说明本文方法的仿真效果更好.</p>
                </div>
                <div class="area_img" id="190">
                    <p class="img_tit"><b>表7 Pix2pixHR和本文方法的PSNR对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 7 PSNR comparison of Pix2pixHR and the proposed method</p>
                    <p class="img_note"></p>
                    <table id="190" border="1"><tr><td rowspan="2"><br /></td><td rowspan="2">方法</td><td colspan="4"><br />图9(a)</td></tr><tr><td><br />第1幅</td><td>第2幅</td><td>第3幅</td><td>第4幅</td></tr><tr><td>云水类</td><td>Pix2pixHR<br />本文方法</td><td>23.3<br />26.0</td><td>24.1<br />27.9</td><td>19.2<br />25.8</td><td>19.4<br />27.9</td></tr><tr><td>山石类</td><td>Pix2pixHR<br />本文方法</td><td>25.8<br />27.8</td><td>19.5<br />29.1</td><td>16.7<br />26.8</td><td>17.5<br />29.0</td></tr><tr><td>植被类</td><td>Pix2pixHR<br />本文方法</td><td>25.3<br />28.9</td><td>21.8<br />30.0</td><td>18.2<br />32.4</td><td>19.7<br />32.0</td></tr><tr><td>总体</td><td>Pix2pixHR<br />本文方法</td><td>19.8<br />22.6</td><td>16.0<br />23.3</td><td>13.2<br />22.7</td><td>16.8<br />22.4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="191" name="191"><b>4.7 布局可调的山水画仿真实验结果</b></h4>
                <div class="p1">
                    <p id="192">本文方法的目标是训练可以仿真手绘山水画的网络,实际应用时用户可以输入自构的布局标签图,也可以以数据集中的标签图为底本进行编辑,构造符合自身要求的布局图,输入网络生成仿真图.通过抽象山水画的布局空间结构并编辑,展现一种效果可控的图像编辑方法.</p>
                </div>
                <div class="p1">
                    <p id="193">图10为以非训练集中的标签图为底本,将编辑后的标签图作为输入的仿真结果.其中,红色矩形框标出相对于底本编辑修改的部分.通过对布局标签图采用拉伸、缩放、添加、替换、擦除等方式编辑,实验表明,本文方法能够适应各种编辑后的山水画仿真,在视觉效果上仍具有较好的观赏性和艺术真实感,在实际应用中用户可以编辑布局图,实现绘画布局构思、仿真创作.</p>
                </div>
                <div class="area_img" id="260">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909010_26000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 编辑布局后的仿真效果展示" src="Detail/GetImg?filename=images/MSSB201909010_26000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 编辑布局后的仿真效果展示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909010_26000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 Display of simulation effect after layout editing</p>

                </div>
                <div class="area_img" id="260">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909010_26001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 编辑布局后的仿真效果展示" src="Detail/GetImg?filename=images/MSSB201909010_26001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 编辑布局后的仿真效果展示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909010_26001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 Display of simulation effect after layout editing</p>

                </div>
                <h4 class="anchor-tag" id="197" name="197"><b>4.8 涂鸦式仿真实验结果</b></h4>
                <div class="p1">
                    <p id="198">图11为将涂鸦式布局标签图作为输入的仿真山水画生成结果.由图可得,生成图在风格上仍保持手绘山水画的风格,基本正确地生成与标签对应的景物内容,在视觉效果上仍具有山水画的艺术真实感.但在线条和纹理细节上还不够细腻,主要原因有如下两点:1)布局标签图分类不够细,一个类别里包含多种相近的对象,如植被类里包含苔藓、草、树,使网络在涂鸦类输入的转换效果上难以区分;2)山水画的训练数据集有限,网络学习样本不够.本文将在后续工作中对此加以改进.</p>
                </div>
                <div class="area_img" id="261">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MSSB201909010_26100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 涂鸦式仿真效果" src="Detail/GetImg?filename=images/MSSB201909010_26100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 涂鸦式仿真效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MSSB201909010_26100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.11 Graffiti simulation effect</p>

                </div>
                <h3 id="203" name="203" class="anchor-tag">5 结 束 语</h3>
                <div class="p1">
                    <p id="204">本文基于CGAN设计多尺度特征融合的网络(MSFF-CGAN),实现对中国山水画的仿真.同时提出语义关联的颜色像素聚类方法,解决山水画布局标签图数据稀缺的问题.实验表明,利用MSFF-CGAN对中国山水画仿真是一种切实可行的方案.在此基础上,结合MemNet超分辨网络对仿真图像增强,最终生成一幅完整、具有较好艺术真实感的仿真山水画.同时,本文方法可应对各种不同的布局编辑和简单的手绘涂鸦式草图,具有一定的实际应用前景.</p>
                </div>
                <div class="p1">
                    <p id="205">目前,仿真工作还有很多待改进的地方,下一阶段工作将主要围绕如下4点展开:1)增加标签图的设计种类,研究面向画作,特别是中国山水画的图像分割方法,以适应更细化的标签分类,使仿真画在内容上更丰富、逼真;2)对山水画数据集采用数据增强的方法扩大样本,加强网络的学习效果,使仿真山水画更具艺术真实感;3)进一步优化网络结构,考虑将布局标签图的语义信息通过卷积方式传递到生成器的每层,以加强条件约束,并对初始噪声进行定制处理;4)考虑针对某一画家的单独仿真工作,通过收集画家作品,利用数据增强方法扩充样本数据,训练一个针对画家特定风格和笔触的山水画仿真网络.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="262">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simulating oriental black-ink painting">

                                <b>[1]</b> LEE J.Simulating Oriental Black-Ink Painting.IEEE Computer Graphics and Applications,1999,19(3):74-81.
                            </a>
                        </p>
                        <p id="264">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Synthesis of Rock Textures in Chinese Landscape Painting">

                                <b>[2]</b> WAY D L,SHIH Z C.The Synthesis of Rock Textures in Chinese Landscape Painting.Computer Graphics Forum,2010,20(3):123-131.
                            </a>
                        </p>
                        <p id="266">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201605006&amp;v=MDQ3NTZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeS9rVnJ2QUx6N0JhTEc0SDlmTXFvOUY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 徐添辰,吴恩华.动态模拟中国水墨画中的笔画绘制.计算机辅助设计与图形学学报,2016,28(5):742-749.(XU T C,WU E H.Animating Strokes in Drawing Process of Chinese Ink Painting.Journal of Computer-Aided Design and Computer Graphics,2016,28(5):742-749.)
                            </a>
                        </p>
                        <p id="268">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Animated Construction of Chinese Brush Paintings">

                                <b>[4]</b> TANG F,DONG W M,MENG Y P,et al.Animated Construction of Chinese Brush Paintings.IEEE Transactions on Visualization and Computer Graphics,2018,24(12):3019-3031.
                            </a>
                        </p>
                        <p id="270">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003430836&amp;v=MDM2MDFPZ0pZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDSGxWTDdOSkZZPU5qN0Jhck80SHRIUHE0eEZi&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> YU J H,LUO G M,PENG Q S.Image-Based Synthesis of Chinese Landscape Painting.Journal of Computer Science and Technology,2003,18(1):22-28.
                            </a>
                        </p>
                        <p id="272">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF200411002&amp;v=MDg2MDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tWcnZBTHo3QmFMRzRIdFhOcm85RlpvUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 张海嵩,尹小勤,于金辉.实时绘制3D中国画效果.计算机辅助设计与图形学学报,2004,16(11):1485-1489.(ZHANG H S,YIN X Q,YU J H.Real-Time Rendering of 3D Chinese Painting Effects.Journal of Computer-Aided Design and Computer Graphics,2004,16(11):1485-1489.)
                            </a>
                        </p>
                        <p id="274">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural network-based Chinese ink-painting art style learning">

                                <b>[7]</b> WANG Z,SUN M J,SUN J Z,et al.Neural Network-Based Chinese Ink-Painting Art Style Learning // Proc of the IEEE International Conference on Intelligent Computing and Intelligent Systems.Washington,USA:IEEE,2010:462-466.
                            </a>
                        </p>
                        <p id="276">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                 GATYS L A,ECKER A S,BETHGE M.Image Style Transfer Using Convolutional Neural Networks // Proc of the IEEE Confe-rence on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:2414-2423.
                            </a>
                        </p>
                        <p id="278">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-resolution image synthesis and semantic manipulation with conditional gans">

                                <b>[9]</b> WANG T C,LIU M Y,ZHU J Y,et al.High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs // Proc of the IEEE Conference on Computer Vision and Pattern Re-cognition.Washington,USA:IEEE,2018:8798-8807.
                            </a>
                        </p>
                        <p id="280">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image-to-Image Translation with Conditional Adversarial Networks">

                                <b>[10]</b> ISOLA P,ZHU J Y,ZHOU T H,et al.Image-to-Image Translation with Conditional Adversarial Networks // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2017:5967-5976.
                            </a>
                        </p>
                        <p id="282">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic Image Synthesis with Spatially-Adaptive Normalization[C/OL]">

                                <b>[11]</b> PARK T,LIU M Y,WANG T C,et al.Semantic Image Synthesis with Spatially-Adaptive Normalization[C/OL].[2019-03-21].https://arxiv.org/pdf/1903.07291.pdf.
                            </a>
                        </p>
                        <p id="284">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Memnet A persistent memory network for image restoration">

                                <b>[12]</b> TAI Y,YANG J,LIU X M,et al.MemNet:A Persistent Memory Networks for Image Restoration // Proc of the IEEE International Conference on Computer Vision.Washington,USA:IEEE,2017:4549-4557.
                            </a>
                        </p>
                        <p id="286">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial networks">

                                <b>[13]</b> GOODFELLOW I,POUGET-ABADIE J,MIRZA M,et al.Gene-rative Adversarial Networks // Proc of the 27th International Conference on Neural Information Processing Systems.Cambridge,USA:The MIT Press,2014:2672-2680.
                            </a>
                        </p>
                        <p id="288">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201703001&amp;v=MDE2NTh0R0ZyQ1VSTE9lWmVSbkZ5L2tWcnZBS0NMZlliRzRIOWJNckk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 王坤峰,苟超,段艳杰,等.生成式对抗网络GAN的研究进展与展望.自动化学报,2017,43(3):321-332.(WANG K F,GOU C,DUAN Y J.Generative Adversarial Networks:The State of the Art and Beyond.Acta Automatica Sinica,2017,43(3):321-332.)
                            </a>
                        </p>
                        <p id="290">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201810002&amp;v=MDk1MDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnkva1ZydkFLRDdZYkxHNEg5bk5yNDlGWm9RS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 蔡雨婷,陈昭炯,叶东毅.基于双层级联GAN的草图到真实感图像的异质转换.模式识别与人工智能,2018,31(10):877-886.(CAI Y T,CHEN Z J,YE D Y.Bi-level Cascading GAN-Based Heterogeneous Conversion of Sketch-to-Realistic Images.Pattern Recognition and Artificial Intelligence,2018,31(10):877-886.)
                            </a>
                        </p>
                        <p id="292">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805009&amp;v=MDc3NDJMZlliRzRIOW5NcW85RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5L2tWcnZBS0M=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 唐贤伦,杜一铭,刘雨微,等.基于条件深度卷积生成对抗网络的图像识别方法.自动化学报,2018,44(5):855-864.(TANG X L,DU Y M,LIU Y W,et al.Image Recognition with Conditional Deep Convolutional Generative Adversarial Networks.Acta Automatica Sinica,2018,44(5):855-864.)
                            </a>
                        </p>
                        <p id="294">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceptual losses for real-time style transfer and superresolution">

                                <b>[17]</b> JOHNSON J,ALAHI A,LI F F.Perceptual Losses for Real-Time Style Transfer and Super-Resolution // Proc of the European Conference on Computer Vision.Berlin,Germany:Springer,2016:694-711.
                            </a>
                        </p>
                        <p id="296">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Context Encoders:Feature Learning by Inpainting">

                                <b>[18]</b> PATHAK D,KRAHENBUHL P,DONAHUE J,et al.Context Encoders:Feature Learning by Inpainting // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2016:2536-2544.
                            </a>
                        </p>
                        <p id="298">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pixel-Level Domain Transfer">

                                <b>[19]</b> YOO D,KIM N,PARK S,et al.Pixel-Level Domain Transfer // Proc of the European Conference on Computer Vision.Berlin,Ger-many:Springer,2016:517-532.
                            </a>
                        </p>
                        <p id="300">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">

                                <b>[20]</b> RONNERBERGER O,FISCHER P,BROX T.U-Net:Convolutional Networks for Biomedical Image Segmentation // Proc of the International Conference on Medical Image Computing and Compu-ter-Assisted Intervention.Berlin,Germany:Springer,2015:234-241.
                            </a>
                        </p>
                        <p id="302">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Matconvnet:Convolutional neural networks for MATLAB">

                                <b>[21]</b> VEDALDI A,LENC K.MatConvNet:Convolutional Neural Networks for Matlab // Proc of the 23rd ACM International Conference on Multimedia.New York,USA:ACM,2015:689-692.
                            </a>
                        </p>
                        <p id="304">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet:A largescale hierarchical image database">

                                <b>[22]</b> DENG J,DONG W,SOCHER R,et al.Image-Net:A Large-Scale Hierarchical Image Database // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2009:248-255.
                            </a>
                        </p>
                        <p id="306">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reconstructing pascal voc">

                                <b>[23]</b> VICENTE S,CARREIRA J,AGAPITO L,et al.Reconstructing PASCAL VOC // Proc of the IEEE Conference on Computer Vision and Pattern Recognition.Washington,USA:IEEE,2014:41-48.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MSSB201909010" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201909010&amp;v=MTk5NjNVUkxPZVplUm5GeS9rVnJ2QUtEN1liTEc0SDlqTXBvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY3Z0JzaWFrTHVRSmVlNjhWcnBYcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
