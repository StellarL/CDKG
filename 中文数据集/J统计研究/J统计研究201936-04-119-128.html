

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637140087715131250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dTJYJ201904011%26RESULT%3d1%26SIGN%3dLkCJQSx85RZYpyRIyH%252fgcw9ZGZY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=TJYJ201904011&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=TJYJ201904011&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJYJ201904011&amp;v=MTkxMjU3cWZadVpvRnl6Z1ZML0pNU2ZTWkxHNEg5ak1xNDlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#34" data-title="一、引言及文献综述 ">一、引言及文献综述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#41" data-title="二、模型 ">二、模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#42" data-title=" (&lt;b&gt;一) 一种回归角度下的稀疏主成分分析&lt;/b&gt;"> (<b>一) 一种回归角度下的稀疏主成分分析</b></a></li>
                                                <li><a href="#59" data-title=" (&lt;b&gt;二&lt;/b&gt;) fused&lt;b&gt;稀疏主成分模型&lt;/b&gt;"> (<b>二</b>) fused<b>稀疏主成分模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="三、模拟实验 ">三、模拟实验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title=" (&lt;b&gt;一&lt;/b&gt;) FSPCA&lt;b&gt;模型的模拟验证&lt;/b&gt;"> (<b>一</b>) FSPCA<b>模型的模拟验证</b></a></li>
                                                <li><a href="#89" data-title=" (&lt;b&gt;二) 迭代GSPCA算法的必要性验证&lt;/b&gt;"> (<b>二) 迭代GSPCA算法的必要性验证</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="四、实证研究 ">四、实证研究</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#116" data-title=" (&lt;b&gt;一&lt;/b&gt;) FSPCA&lt;b&gt;模型的整体效果&lt;/b&gt;"> (<b>一</b>) FSPCA<b>模型的整体效果</b></a></li>
                                                <li><a href="#121" data-title=" (&lt;b&gt;二) 主成分的解释性&lt;/b&gt;"> (<b>二) 主成分的解释性</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="五、结论 ">五、结论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#84" data-title="&lt;b&gt;表1 三个模型提取的第一主成分所解释的平均方差百分比&lt;/b&gt;"><b>表1 三个模型提取的第一主成分所解释的平均方差百分比</b></a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;图1&lt;/b&gt;&lt;i&gt;n&lt;/i&gt;=&lt;b&gt;20&lt;/b&gt;、&lt;i&gt;p&lt;/i&gt;=&lt;b&gt;1000&lt;/b&gt;&lt;b&gt;时, 三种模型得到的第一主成分向量及其载荷向量&lt;/b&gt;"><b>图1</b><i>n</i>=<b>20</b>、<i>p</i>=<b>1000</b><b>时, 三种模型得到的第一主成分向量及其载荷向量</b></a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;表2 一步GSPCA算法和迭代GSPCA算法的一次模拟结果对比&lt;/b&gt;"><b>表2 一步GSPCA算法和迭代GSPCA算法的一次模拟结果对比</b></a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;表3 100次模拟得到的平均解释方差百分比的对比结果&lt;/b&gt;"><b>表3 100次模拟得到的平均解释方差百分比的对比结果</b></a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;图2 FSPCA模型对数字0的恢复图像 (提取主成分个数为2&lt;/b&gt;) "><b>图2 FSPCA模型对数字0的恢复图像 (提取主成分个数为2</b>) </a></li>
                                                <li><a href="#123" data-title="&lt;b&gt;图3 FSPCA模型对手写数字0所提取的第一主成分&lt;/b&gt;"><b>图3 FSPCA模型对手写数字0所提取的第一主成分</b></a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;图4 FSPCA模型对手写数字0所提取的第二主成分&lt;/b&gt;"><b>图4 FSPCA模型对手写数字0所提取的第二主成分</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;图5 SPC模型对手写数字0所提取的第一主成分&lt;/b&gt;"><b>图5 SPC模型对手写数字0所提取的第一主成分</b></a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;图6 SPC模型对手写数字0所提取的第二主成分&lt;/b&gt;"><b>图6 SPC模型对手写数字0所提取的第二主成分</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>
                                    <dd class="subnode">
                                        <h6>
                                            <a href="#a_footnote">注释</a>

                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="5">


                                    <a id="bibliography_1" title="Cadima J, Jolliffe I T.Loading and Correlations in the Interpretation of Principle Components [J].Journal of Applied Statistics, 1995, 22 (2) :203-214." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD757584614&amp;v=Mjg3MTZxQnRHRnJDVVI3cWZadVpvRnl6Z1ZML0lOam5CYXJTOUdkVEVxNGxFWUlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        Cadima J, Jolliffe I T.Loading and Correlations in the Interpretation of Principle Components [J].Journal of Applied Statistics, 1995, 22 (2) :203-214.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_2" title="Jolliffe I T, Trendafilov N T, Uddin M.A Modified Principal Component Technique Based on the LASSO [J].Journal of Computational and Graphical Statistics, 2003, 12 (3) :531-547." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800004698&amp;v=MTI3MzhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJuSUpGc1RhaEk9TmpuQmFySzdIdGZPcDQ5RlpPc0xDbg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        Jolliffe I T, Trendafilov N T, Uddin M.A Modified Principal Component Technique Based on the LASSO [J].Journal of Computational and Graphical Statistics, 2003, 12 (3) :531-547.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_3" title="Zou H, Hastie T, Tibshirani R.Sparse Principal Component Analysis [J].Journal of Computational and Graphical Statistics, 2006, 15 (2) :265-286." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800004542&amp;v=Mjg4NjVIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVybklKRnNUYWhJPU5qbkJhcks3SHRmT3A0OUZaT3NMQ1hnN29CTVQ2VDRQUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        Zou H, Hastie T, Tibshirani R.Sparse Principal Component Analysis [J].Journal of Computational and Graphical Statistics, 2006, 15 (2) :265-286.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_4" title="Shen H, Huang J Z.Sparse Principal Component Analysis via Regularized Low Rank Matrix Approximation [J].Journal of Multivariate Analysis, 2008, 99 (6) :1015-1034." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600337495&amp;v=MDcwMjc4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcm5JSkZzVGFoST1OaWZPZmJLN0h0RE5xWTlGWitnSUNIVQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        Shen H, Huang J Z.Sparse Principal Component Analysis via Regularized Low Rank Matrix Approximation [J].Journal of Multivariate Analysis, 2008, 99 (6) :1015-1034.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_5" title="Witten D M, Tibshirani R, Hastie T.A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis [J].Biostatistics, 2009, 10 (3) :515-534." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Penalized Matrix Decomposition with Applications to Sparse Principal Components and Canonical Correlation Analysis">
                                        <b>[5]</b>
                                        Witten D M, Tibshirani R, Hastie T.A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis [J].Biostatistics, 2009, 10 (3) :515-534.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_6" title="Qi X, Luo R, Zhao H.Sparse Principal Component Analysis by Choice of Norm [J].Journal of Multivariate Analysis, 2013, 114 (1) :127-160." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300630182&amp;v=MTkzMjRySTlGWXVnUERYUTdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVybklKRnNUYWhJPU5pZk9mYks5SDlQTw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        Qi X, Luo R, Zhao H.Sparse Principal Component Analysis by Choice of Norm [J].Journal of Multivariate Analysis, 2013, 114 (1) :127-160.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_7" title="Shi J, Song W.Sparse Principal Component Analysis with Measurement Errors [J].Journal of Statistical Planning &amp;amp; Inference, 2016, 175:87-99." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparse principal component analysis with measurement errors">
                                        <b>[7]</b>
                                        Shi J, Song W.Sparse Principal Component Analysis with Measurement Errors [J].Journal of Statistical Planning &amp;amp; Inference, 2016, 175:87-99.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_8" title="Sharifzadeh, et al.Sparse Supervised Principal Component Analysis (SSPCA) for Dimension Reduction and Variable Selection [J].Engineering Applications of Artificial Intelligence, 2017, 65:168-177." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES5D3D5AD33D72BE61D95DED8CB30310FF&amp;v=MTM2ODNIWWZPR1FsZkJyTFUwNXQ1aHdMMjR3S2c9TmlmT2ZiYk1IYVhKM3Z0R1o1OElEZzVNeVJkbjR6b0pQUXZxMzJBMmViR1ZSY3pwQ09OdkZTaVdXcjdKSUZwbWFCdQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        Sharifzadeh, et al.Sparse Supervised Principal Component Analysis (SSPCA) for Dimension Reduction and Variable Selection [J].Engineering Applications of Artificial Intelligence, 2017, 65:168-177.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_9" title="Tibshirani R.Regression Shrinkage and Selection via the Lasso[J].Journal of the Royal Statistical Society, Series B, 1996, 58:267-288." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14120603924782&amp;v=MTA3NjUzUTdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVybklKRnNUYWhJPU5pZlllcks4SDlQTXFZOUdiZWtMQw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        Tibshirani R.Regression Shrinkage and Selection via the Lasso[J].Journal of the Royal Statistical Society, Series B, 1996, 58:267-288.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_10" title="Zou H, Hastie T.Regularization and Variable Selection via the Elastic Net[J].Journal of the Royal Statistical Society, Series B, 2005, 67 (2) :301-320." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00001256895&amp;v=MDQ3ODUrWnVGaTdsVjdyUElGND1OaWZjYXJPNEh0SE5yWXBEYk9JS1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRa&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        Zou H, Hastie T.Regularization and Variable Selection via the Elastic Net[J].Journal of the Royal Statistical Society, Series B, 2005, 67 (2) :301-320.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_11" title="Tibshirani R, et al.Sparsity and Smoothness via the Fused Lasso [J].Journal of the Royal Statistical Society, Series B, 2005, 67 (1) :91-108." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00001256883&amp;v=MTExNjJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZpN2xWN3JQSUY0PU5pZmNhck80SHRITnJZcERiT01NWTNrNXpC&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        Tibshirani R, et al.Sparsity and Smoothness via the Fused Lasso [J].Journal of the Royal Statistical Society, Series B, 2005, 67 (1) :91-108.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_12" title="Friedman J, Hastie T, Tibshirani R.Pathwise Coordinate Optimization [J].Annals of Applied Statistics, 2007, 1 (2) :302-332." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJSTFC7183B4B890081CC97417653E0D4F36&amp;v=MTM3NzlHUWxmQnJMVTA1dDVod0wyNHdLZz1OaWZZZXNYTEdkREVyUDFCRnVNR0RId3h6bVZnNHpoNVNYamtxUkZBZWNhUU03bVpDT052RlNpV1dyN0pJRnBtYUJ1SFlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        Friedman J, Hastie T, Tibshirani R.Pathwise Coordinate Optimization [J].Annals of Applied Statistics, 2007, 1 (2) :302-332.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_13" title="Hoefling H.A Path Algorithm for the Fused Lasso Signal Approximator [J].Journal of Computational and Graphical Statistics, 2010, 19 (4) :984-1006." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800004295&amp;v=Mjg2NzNNbndaZVp0RmlubFVybklKRnNUYWhJPU5qbkJhcks3SHRmT3A0OUZaT3NMRG5VOG9CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        Hoefling H.A Path Algorithm for the Fused Lasso Signal Approximator [J].Journal of Computational and Graphical Statistics, 2010, 19 (4) :984-1006.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_14" title="Petersen A, Witten D, Simon N.Fused Lasso Additive Model [J].Journal of Computational and Graphical Statistics, 2016, 25 (4) :1005-1025." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJDB69BB77BABA6A0079DF287A07F767C74&amp;v=Mjg2MzQwNXp4RWFua2wvUUhpVHJCVkRmclNUTnIyYkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXQ1aHdMMjR3S2c9TmpuQmFzRytGNk8rcUlnM0ZabCtDZw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        Petersen A, Witten D, Simon N.Fused Lasso Additive Model [J].Journal of Computational and Graphical Statistics, 2016, 25 (4) :1005-1025.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=TJYJ" target="_blank">统计研究</a>
                2019,36(04),119-128 DOI:10.19343/j.cnki.11-1302/c.2019.04.011            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于</b>fused<b>惩罚的稀疏主成分分析</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%B3%A2&amp;code=09751940&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张波</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%99%93%E5%80%A9&amp;code=41631336&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘晓倩</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E5%A4%A7%E5%AD%A6%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E4%B8%AD%E5%BF%83&amp;code=0198015&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国人民大学应用统计中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%BE%8E%E5%9B%BD%E5%8C%97%E5%8D%A1%E5%B7%9E%E7%AB%8B%E5%A4%A7%E5%AD%A6&amp;code=0208667&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">美国北卡州立大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>本文旨在研究基于fused惩罚的稀疏主成分分析方法, 以适用于相邻变量之间高度相关甚至完全相等的数据情形。首先, 从回归分析角度出发, 提出一种求解稀疏主成分的简便思路, 给出一种广义的稀疏主成分模型——GSPCA模型及其求解算法, 并证明在惩罚函数取1-范数时, 该模型与现有的稀疏主成分模型——SPC模型的求解结果一致。其次, 提出将fused惩罚与主成分分析相结合, 得到一种fused稀疏主成分分析方法, 并从惩罚性矩阵分解和回归分析两个角度, 给出两种模型形式。在理论上证明了两种模型的求解结果是一致的, 故将其统称为FSPCA模型。模拟实验显示, FSPCA模型在处理相邻变量之间高度相关甚至完全相等的数据集上表现良好。最后, 将FSPCA模型应用于手写数字识别, 发现与SPC模型相比, FSPCA模型所提取的主成分具备更好的解释性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">主成分分析;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%80%E7%96%8F%E5%8C%96%E6%96%B9%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稀疏化方法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fused%E6%83%A9%E7%BD%9A&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fused惩罚;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">手写数字识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">可解释性;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张波, 男, 中国人民大学应用统计中心专职研究员, 统计学院教授、博士生导师。研究方向为高频金融数据分析。;
                                </span>
                                <span>
                                    *刘晓倩 (通讯作者) , 女, 美国北卡州立大学统计学专业在读博士研究生。研究方向为统计学习、高维数据分析。;
                                </span>
                    </p>
                
                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目“基于非结构化数据的个人信用评价” (71873137) ;国家自然科学基金项目“非对称随机波动建模及其在金融风险管理中的应用研究” (71471173) 的资助;</span>
                    </p>
            </div>
                    <h1><b>Sparse Principal Component Analysis with Fused Penalty</b></h1>
                    <h2>
                    <span>Zhang Bo</span>
                    <span>Liu Xiaoqian</span>
            </h2>
                                    <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>This paper mainly studies sparse principal component analysis with fused penalty, so as to solve problems with features which are naturally ordered or variables which are related or even equal to their neighbors. First, we propose a simple approach to obtain sparse PCs from the perspective of regression. A new generalized sparse PCA model is introduced, namely generalized sparse PCA (GSPCA) , and the corresponding algorithm is offered. Also, we prove that the solution of GSPCA is equivalent to that of SPC, an existing sparse PCA model, when the penalty is 1-norm. Next, we propose combining the fused penalty and sparse PCA to get a fused sparse PCA method, and introduce the corresponding model with two forms on the basis of PMD and regression. After theoretical derivation, we find that the solutions of the two model forms are consistent, so we call the model FSPCA without discrimination. The simulation reveals that FSPCA has a good performance on datasets where variables are related or even equal to their neighbors. At last, we apply the FSPCA to handwritten numeral recognition. It turns out that compared with SPC, FSPCA can extract PCs which have better interpretability, and this makes FSPCA of higher practical value.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Principal%20Component%20Analysis&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Principal Component Analysis;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Sparsity%20Method&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Sparsity Method;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Fused%20Penalty&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Fused Penalty;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Handwritten%20Numeral%20Recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Handwritten Numeral Recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Interpretability&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Interpretability;</a>
                </p>
            </div>
        

        <!--brief start-->
                        <h3 id="34" name="34" class="anchor-tag">一、引言及文献综述</h3>
                <div class="p1">
                    <p id="35">在大数据时代, 信息呈现爆发式增长, 数据多是以高维、乃至超高维的形式存在。在对高维数据进行分析时, 通常面临两个问题:一是如何通过特征提取或特征选择对数据进行降维;二是如何提取或选择出更具解释性能的特征。主成分分析通过提取少量主成分以达到降维的目的。稀疏化方法则通过将部分变量系数压缩为零实现特征选择, 同时使得模型具备更好的解释性能。</p>
                </div>
                <div class="p1">
                    <p id="36">稀疏主成分分析是主成分分析和稀疏化方法的完美结合, 其目的是解决传统主成分分析由于载荷向量的非稀疏性而导致的解释性差的问题。通过对载荷向量施加稀疏化惩罚, 使得每个主成分只由部分原始变量线性表出, 大大提高了主成分的解释性。关于该方法的研究可以追溯到上世纪末。<citation id="148" type="reference"><link href="5" rel="bibliography" />Cadima 和 Jolliffe (1995) </citation><citation id="140" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出一种简单的阈值方法, 即将所有绝对值小于某一阈值的载荷直接设为0, 从而得到稀疏的载荷向量。这种方法操作简单, 但不可靠。<citation id="149" type="reference"><link href="7" rel="bibliography" />Jolliffe等 (2003) </citation><citation id="141" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出的SCoTLASS模型在传统主成分模型的基础上对载荷向量施加1-范数惩罚, 从而达到稀疏化载荷向量的目的。<citation id="150" type="reference"><link href="9" rel="bibliography" />Zou等 (2006) </citation><citation id="142" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>从多元回归分析的角度出发, 提出求解稀疏主成分的SPCA方法。将问题转化成一种回归形式的最优化问题, 通过对回归系数施加1-范数和2-范数约束, 得到稀疏的载荷向量, 并通过模拟实验和高维基因数据的应用案例, 证明了模型的有效性。<citation id="151" type="reference"><link href="11" rel="bibliography" />Shen和 Huang (2008) </citation><citation id="143" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>利用主成分分析和SVD之间的关系, 把稀疏主成分的求解转化成对原数据矩阵的低秩近似, 从而建立了一种sPCA-rSVD模型, 通过将模型作为一种基因筛选方法应用于实际数据, 验证了模型的效果。<citation id="152" type="reference"><link href="13" rel="bibliography" />Witten等 (2009) </citation><citation id="144" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>从惩罚性矩阵分解 (PMD) 的角度, 给出基于1-范数惩罚的稀疏性主成分模型——SPC模型。对于高阶主成分的求解, 通过把对载荷向量的正交性约束进行转移, 在保证主成分解释性的同时尽量保证其不相关性。<citation id="153" type="reference"><link href="15" rel="bibliography" />Qi等 (2013) </citation><citation id="145" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>将一种混合范数引入到主成分模型中, 提出了一种新的稀疏主成分分析方法, 并给出了高阶主成分的求解算法, 以保证载荷向量之间的正交性或者各主成分之间的不相关性。近年来的一些文献则主要是在原有理论基础上进行扩充, 如<citation id="154" type="reference"><link href="17" rel="bibliography" />Shi和Song (2016) </citation><citation id="146" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>讨论了当变量带有测量误差时的稀疏主成分分析方法, 并在SPCA模型的基础上给出了一种修正的稀疏主成分模型及其求解算法。<citation id="155" type="reference"><link href="19" rel="bibliography" />Sharifzadeh等 (2017) </citation><citation id="147" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>则是利用PMD算法, 结合SPC方法, 讨论了在有标签数据或有响应变量时的稀疏主成分分析方法, 即有监督的稀疏主成分分析, 并给出相应的模型形式和求解算法。</p>
                </div>
                <div class="p1">
                    <p id="37">稀疏化方法的研究同样开始于上世纪末, 经典的稀疏化模型有Lasso (<citation id="162" type="reference"><link href="21" rel="bibliography" />Tibshirani, 1996</citation>) <citation id="156" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、Elastic Net (<citation id="163" type="reference"><link href="23" rel="bibliography" />Zou和Hastie, 2005</citation>) <citation id="157" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>等。在实际问题中, 存在着这样一类特殊的高维数据集:各变量之间有一种自然排序, 相邻变量之间高度相关甚至完全相等, 例如基因数据、图像数据等, 因此<citation id="164" type="reference"><link href="25" rel="bibliography" />Tibshirani等 (2005) </citation><citation id="158" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出了fused lasso专门用于此类数据的研究。Fused lasso的惩罚函数 (之后统称为fused惩罚) 不仅对参数向量施加1-范数约束, 还对其逐次差分项施加1-范数约束, 这样既保证了参数向量的稀疏性, 又保证了相邻参数之间的连续性, 以契合数据的“相邻变量之间高度相关”这一特征。在fused lasso提出后, 如何优化其求解算法成为了一个研究热点。<citation id="165" type="reference"><link href="27" rel="bibliography" />Friedman等 (2007) </citation><citation id="159" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出了一种坐标下降优化算法来提高计算速率, 并给出其在图像去噪方面的应用。<citation id="166" type="reference"><link href="29" rel="bibliography" />Hoefling (2010) </citation><citation id="160" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了一种求解fused lasso信号近似问题的路径算法, 可以给出调和参数所有取值下的路径解。此外, fused惩罚还可与其他模型相结合, 来解决实际问题。如<citation id="167" type="reference"><link href="31" rel="bibliography" />Petersen等 (2016) </citation><citation id="161" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>将fused惩罚函数应用到广义可加模型中, 提出一种fused lasso可加模型, 并将其应用在肺癌基因数据的预测中。</p>
                </div>
                <div class="p1">
                    <p id="38">综上, 从理论角度来看, 现有的稀疏主成分分析方法已有较完善的理论框架和算法实现, 但各个方法的出发角度不同, 且多采用1-范数来实现稀疏化处理;从应用角度来看, 主成分分析在基因数据分析、图像识别等领域应用广泛, 而fused惩罚在此类问题上非常适用。因此, 在理论层面, 可以考虑是否能给出更具一般性的稀疏主成分模型, 实现对理论模型的扩充;在应用层面, 可以考虑将fused惩罚应用于稀疏主成分模型, 以适用于基因、图像等相邻变量之间高度相关的数据。</p>
                </div>
                <div class="p1">
                    <p id="39">本文针对以上两点对稀疏主成分分析方法进行研究, 主要创新工作如下:</p>
                </div>
                <div class="p1">
                    <p id="40">①从回归分析的角度出发, 提出一种求解稀疏主成分的简便思路, 同时给出一种广义的稀疏主成分模型——GSPCA模型及其求解算法, 并证明其与现有稀疏主成分模型SPC之间的关系;②将fused惩罚与稀疏主成分分析方法结合, 从回归分析和PMD两个角度出发, 给出两种基于fused惩罚的稀疏主成分模型形式及求解算法, 并通过理论推导, 证明两种模型形式的求解结果是一致的;③通过模拟实验和手写数字识别的实证研究, 证明了本文提出的基于fused惩罚的稀疏主成分模型在处理相邻变量之间高度相关甚至完全相等数据时的优越性。</p>
                </div>
                <h3 id="41" name="41" class="anchor-tag">二、模型</h3>
                <h4 class="anchor-tag" id="42" name="42"> (<b>一) 一种回归角度下的稀疏主成分分析</b></h4>
                <div class="p1">
                    <p id="43">设数据矩阵<b><i>X</i></b><sub><i>n</i>×<i>p</i></sub>= (<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>p</i></sub>) 的奇异值分解形式为:</p>
                </div>
                <div class="p1">
                    <p id="44"><b><i>X</i></b>=<b><i>UDV</i></b><sup><i>T</i></sup> (1) </p>
                </div>
                <div class="p1">
                    <p id="45"><b><i>u</i></b><sub><i>k</i></sub>、<b><i>v</i></b><sub><i>k</i></sub>分别为<b><i>U</i></b>和<b><i>V</i></b>的第<i>k</i>个列向量, <i>d</i><sub><i>k</i></sub>为对角矩阵<b><i>D</i></b>的第<i>k</i>个对角元素。对于传统的主成分模型, <b><i>Xv</i></b><sub><i>k</i></sub>=<b><i>u</i></b><sub><i>k</i></sub><i>d</i><sub><i>k</i></sub>即为第<i>k</i>阶主成分, <b><i>v</i></b><sub><i>k</i></sub>为相应的载荷向量。考虑如下情形:设<b><i>u</i></b><sub>1</sub>已知, 则<b><i>u</i></b><sub>1</sub>=<b><i>X</i></b> (<i>d</i><mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml><b><i>v</i></b><sub>1</sub>) ≜<b><i>X</i></b><i>β</i><sub>1</sub>, 用<b><i>u</i></b><sub>1</sub>对<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>p</i></sub>作回归, 即可求得系数向量<i>β</i><sub>1</sub>, 从而得到载荷向量<b><i>v</i></b><sub>1</sub>=<i>β</i><sub>1</sub>/‖<i>β</i><sub>1</sub>‖<sub>2</sub>, 这样就将传统主成分模型转化成线性回归模型来求解。</p>
                </div>
                <div class="p1">
                    <p id="47">将上述思想推广到稀疏主成分的求解中。传统主成分模型得到的一阶主成分能够解释的方差最大, 但载荷向量不稀疏。一个直观的想法就是在保证方差不损失太大的前提下, 将载荷向量稀疏化。从上述回归角度分析, 就是依然用<b><i>u</i></b><sub>1</sub>对<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>p</i></sub>作回归, 但要对<i>β</i><sub>1</sub>施加额外的稀疏性惩罚。于是本文提出如下的稀疏主成分模型:</p>
                </div>
                <div class="p1">
                    <p id="48"><mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>β</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munder><mo stretchy="false">{</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo stretchy="false">}</mo></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="50">为方便书写, 将上述模型称为广义稀疏主成分分析 (Generalized Sparse Principle Component Analysis, GSPCA) 模型。其中, <b><i>u</i></b><sub>1</sub>为原始数据<b><i>X</i></b>的第一左奇异向量, <i>P</i> (<i>β</i><sub>1</sub>) 是一个稀疏性惩罚函数, 稀疏化回归模型中的惩罚函数都可以选用。这样, 本文将稀疏主成分的求解模型转化成稀疏化的回归模型, 并对模型给出了通俗易懂的解释。</p>
                </div>
                <div class="p1">
                    <p id="51">对于GSPCA模型的求解, 直观做法是将其看作简单的稀疏化回归模型, 进行一步式求解, 但这种方式得到的结果往往不是最优的, 这一点本文将在模拟部分给出验证。为了得到最优的载荷向量, 本文采用迭代算法求解GSPCA模型, 具体算法如下:</p>
                </div>
                <div class="area_img" id="52">
                    <p class="img_tit"> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="52" border="1"><tr><td><br />算法1:GSPCA模型一阶主成分的计算</td></tr><tr><td><br /> (1) 初始化<b><i>u</i></b><sub>1</sub>为数据矩阵<b><i>X</i></b>的第一左奇异向量, 满足‖<b><i>u</i></b><sub>1</sub>‖<sub>2</sub>=1;<br /> (2) 迭代 (i) (ii) 至收敛:<br /> (i) 更新<b><i>v</i></b><sub>1</sub>∈<i>R</i><sup><i>p</i></sup>:<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mn>1</mn></msub><mo>←</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>β</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munder><mo stretchy="false">{</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>;</mo></mrow></math><br /><b><i>v</i></b><sub>1</sub>←<i>β</i><sub>1</sub>/‖<i>β</i><sub>1</sub>‖<sub>2</sub>;<br /> (ii) 更新<b><i>u</i></b><sub>1</sub>∈<i>R</i><sup><i>n</i></sup>:<b><i>u</i></b><sub>1</sub>←<b><i>Xv</i></b><sub>1</sub>/‖<b><i>Xv</i></b><sub>1</sub>‖<sub>2</sub>.</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="53">对高阶主成分的求解, 本文采用序贯法, 即在求得第一主成分<b><i>Xv</i></b><sub>1</sub>=<b><i>u</i></b><sub>1</sub><i>d</i><sub>1</sub>后, 再对残差矩阵<b><i>X</i></b><sub>2</sub>=<b><i>X</i></b>-<i>d</i><sub>1</sub><b><i>u</i></b><sub>1</sub><b><i>v</i></b><mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>Τ</mi></msubsup></mrow></math></mathml>做一阶稀疏主成分分析, 以此方式迭代。具体算法如下:</p>
                </div>
                <div class="area_img" id="55">
                    <p class="img_tit"> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="55" border="1"><tr><td><br />算法2:GSPCA模型高阶主成分的计算</td></tr><tr><td><br /> (1) 令<b><i>X</i></b><sub>1</sub>=<b><i>X</i></b>;<br /> (2) 对<i>i</i>=1, 2, …, <i>r</i> (<i>r</i>=<i>rank</i> (<b><i>X</i></b>) ) :<br /> (i) 通过算法1, 对<b><i>X</i></b><sub><i>i</i></sub>求解出相应的<b><i>v</i></b><sub><i>i</i></sub>、<b><i>u</i></b><sub><i>i</i></sub>;<br /> (ii) <b><i>X</i></b><sub><i>i</i>+1</sub>←<b><i>X</i></b><sub><i>i</i></sub>-<i>d</i><sub><i>i</i></sub><b><i>u</i></b><sub><i>i</i></sub><b><i>v</i></b><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>Τ</mi></msubsup></mrow></math>, 其中<i>d</i><sub><i>i</i></sub>=<b><i>u</i></b><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>Τ</mi></msubsup></mrow></math><b><i>Xv</i></b><sub><i>i</i></sub>.</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="56">部分文章会对稀疏主成分模型的高阶主成分作出正交性要求, 但<citation id="169" type="reference"><link href="11" rel="bibliography" />Shen和Huang (2008) </citation><citation id="168" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>指出, 对高阶主成分之间的正交性约束并不能显著提高解释的方差, 反倒会大大增加计算复杂度。因此, 本文舍弃了对高阶主成分的正交性要求。</p>
                </div>
                <div class="p1">
                    <p id="57">此外, 当<i>P</i> (<i>β</i><sub>1</sub>) =‖<i>β</i><sub>1</sub>‖<sub>1</sub>时, GSPCA模型与同样基于1-范数惩罚的SPC模型存在如下关系, 详细证明过程备索。</p>
                </div>
                <div class="p1">
                    <p id="58">定理1:当<i>P</i> (<i>β</i><sub>1</sub>) =‖<i>β</i><sub>1</sub>‖<sub>1</sub>时, GSPCA模型与SPC模型的求解结果相等。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59"> (<b>二</b>) fused<b>稀疏主成分模型</b></h4>
                <div class="p1">
                    <p id="60">根据稀疏化方法中对惩罚函数的研究, 在相邻变量高度相关时应选择fused惩罚。将fused惩罚应用于主成分分析, 就形成了本文提出的fused稀疏主成分分析 (Fused Sparse Principle Component Analysis, FSPCA) 模型。本文从回归分析和PMD两个角度给出两种形式的FSPCA模型。</p>
                </div>
                <div class="p1">
                    <p id="61">首先, 将GSPCA模型中的惩罚函数具体化为fused惩罚, 得到FSPCA-1模型:</p>
                </div>
                <div class="p1">
                    <p id="62"><mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>β</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munder><mo stretchy="false">{</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></msub><mrow><mrow><mo>|</mo><mrow><mi>β</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>, </mo><mi>j</mi></mrow></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow></msub><mrow><mrow><mo>|</mo><mrow><mi>β</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>, </mo><mi>j</mi></mrow></msub><mo>-</mo><mi>β</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>, </mo><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo stretchy="false">}</mo></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="64">其中, <b><i>v</i></b><sub>1</sub>=<i>β</i><sub>1</sub>/‖<i>β</i><sub>1</sub>‖<sub>2</sub>为第一主成分的载荷向量, <b><i>z</i></b><sub>1</sub>=<b><i>Xv</i></b><sub>1</sub>为第一主成分。</p>
                </div>
                <div class="p1">
                    <p id="65">对于FSPCA-1模型的求解, 根据上文对GSPCA模型的研究, 得到如下算法3。</p>
                </div>
                <div class="area_img" id="66">
                    <p class="img_tit"> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="66" border="1"><tr><td><br />算法3:FSPCA-1模型一阶主成分的计算</td></tr><tr><td><br /> (1) 初始化<b><i>u</i></b><sub>1</sub>为数据矩阵<b><i>X</i></b>的第一左奇异向量, 满足‖<b><i>u</i></b><sub>1</sub>‖<sub>2</sub>=1;<br /> (2) 迭代 (i) (ii) 至收敛:<br /> (i) 更新<b><i>v</i></b><sub>1</sub>∈<i>R</i><sup><i>p</i></sup>:<br /><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mn>1</mn></msub><mo>←</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>β</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munder><mo stretchy="false">{</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></msub><mrow><mrow><mo>|</mo><mrow><mi>β</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>, </mo><mi>j</mi></mrow></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow></msub><mrow><mrow><mo>|</mo><mrow><mi>β</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>, </mo><mi>j</mi></mrow></msub><mo>-</mo><mi>β</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>, </mo><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo stretchy="false">}</mo><mo>;</mo></mrow></math><br /><b><i>v</i></b><sub>1</sub>←<i>β</i><sub>1</sub>/‖<i>β</i><sub>1</sub>‖<sub>2</sub>;<br /> (ii) 更新<b><i>u</i></b><sub>1</sub>∈<i>R</i><sup><i>n</i></sup>:<br /><b><i>u</i></b><sub>1</sub>←<b><i>Xv</i></b><sub>1</sub>/‖<b><i>Xv</i></b><sub>1</sub>‖<sub>2</sub></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="67">其次, 从PMD角度出发, 结合SPC模型, 给出FSPCA-2模型:</p>
                </div>
                <div class="p1">
                    <p id="68"><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow></munder><mi mathvariant="bold-italic">u</mi><msup><mrow></mrow><mi>Τ</mi></msup><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">v</mi><mtext> </mtext><mi>s</mi><mo>.</mo><mi>t</mi><mo>.</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></munder><mrow><mrow><mo>|</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow></munder><mo stretchy="false">|</mo></mstyle><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>v</mi><msub><mrow></mrow><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">|</mo><mo>≤</mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>≤</mo><mn>1</mn><mo>, </mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">v</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>≤</mo><mn>1</mn></mrow></math></mathml>      (4) </p>
                </div>
                <div class="p1">
                    <p id="70">求得的最优解<b><i>v</i></b>即为第一主成分的载荷向量, <b><i>z</i></b>=<b><i>Xv</i></b>即为对应的第一主成分。</p>
                </div>
                <div class="p1">
                    <p id="71">FSPCA-2模型一阶主成分的求解算法如下, 详细推导过程备索。</p>
                </div>
                <div class="area_img" id="72">
                    <p class="img_tit"> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="72" border="1"><tr><td><br />算法4:FSPCA-2模型一阶主成分的计算</td></tr><tr><td><br /> (1) 初始化向量<b><i>v</i></b>∈<i>R</i><sup><i>p</i></sup>为数据矩阵<b><i>X</i></b>的第一右奇异向量, 满足‖<b><i>v</i></b>‖<sub>2</sub>=1;<br /> (2) 按如下方式更新<b><i>u</i></b>、<b><i>v</i></b>至收敛:<br /> (i) 更新<b><i>u</i></b>∈<i>R</i><sup><i>n</i></sup>:<br /><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">u</mi><mo>←</mo><mfrac><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">v</mi></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">v</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mo>;</mo></mrow></math><br /> (ii) 更新<b><i>v</i></b>∈<i>R</i><sup><i>p</i></sup>:<br /><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">v</mi><mo>←</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>v</mi></munder><mrow><mo>{</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>Τ</mi></msup><mi mathvariant="bold-italic">u</mi><mo>-</mo><mi mathvariant="bold-italic">v</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></msub><mrow><mrow><mo>|</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow></msub><mrow><mrow><mo>|</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>v</mi><msub><mrow></mrow><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>|</mo></mrow></mrow></mstyle></mrow><mo>}</mo></mrow><mo>;</mo></mrow></math><br /> (3) <i>d</i>←<b><i>u</i></b><sup><i>T</i></sup><b><i>Xv</i></b>.</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="73">通过对比算法3和算法4, 不难发现, 虽然FSPCA-1模型和FSPCA-2模型的出发点完全不同, 但是求解算法非常相似。两种算法都是通过迭代方式进行求解, 对向量<b><i>u</i></b>的更新方式完全相同, 对于<b><i>v</i></b>的更新方式也非常相像。下面给出如下定理, 详细证明备索。</p>
                </div>
                <div class="p1">
                    <p id="74">定理2:算法3和算法4中对向量<i>v</i>的更新方式等价, 即FSPCA-1模型和FSPCA-2模型的求解结果是一致的。</p>
                </div>
                <div class="p1">
                    <p id="75">有鉴于此, 本文不再对上述两种形式的模型加以区分, 直接称带fused惩罚的稀疏主成分模型为FSPCA模型。由于R中有求解PMD模型的R包, 在下文的模拟实验与实证研究部分, 本文将利用FSPCA-2模型的相应算法进行编程求解, 以保证结果的可靠性。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag">三、模拟实验</h3>
                <h4 class="anchor-tag" id="77" name="77"> (<b>一</b>) FSPCA<b>模型的模拟验证</b></h4>
                <div class="p1">
                    <p id="78">对于本文提出的FSPCA模型, 将通过随机模拟实验来验证其在高维模拟数据下的应用表现。设定模拟数据矩阵<i>X</i><sub><i>n</i>×<i>p</i></sub>, <i>n</i>为20, <i>p</i>分别为500、1000、2000。每个元素的产生方式如下:</p>
                </div>
                <div class="p1">
                    <p id="79"> (1) <i>p</i>=500时, 对<i>i</i>∈{5, …, 10}, <i>j</i>∈{200, …, 400}, <i>X</i><sub><i>ij</i></sub>∈<i>N</i> (5, 1) ;</p>
                </div>
                <div class="p1">
                    <p id="80"><i>p</i>=1000时, 对<i>i</i>∈{1, …, 5}, <i>j</i>∈{300, …, 700}, <i>X</i><sub><i>ij</i></sub>∈<i>N</i> (3, 1) ;</p>
                </div>
                <div class="p1">
                    <p id="81"><i>p</i>=2000时, 对<i>i</i>∈{8, …, 15}, <i>j</i>∈{800, …, 1300}, <i>X</i><sub><i>ij</i></sub>∈<i>N</i> (10, 4) ;</p>
                </div>
                <div class="p1">
                    <p id="82"> (2) 三种情形下, 其他所有元素均取<i>X</i><sub><i>ij</i></sub>∈<i>N</i> (0, 1) 。</p>
                </div>
                <div class="p1">
                    <p id="83">分别利用传统PCA模型、SPC模型和FSPCA模型对模拟数据提取第一主成分, 将模拟实验进行100次, 得到解释的平均方差百分比如表1所示。</p>
                </div>
                <div class="area_img" id="84">
                    <p class="img_tit"><b>表1 三个模型提取的第一主成分所解释的平均方差百分比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"> (%) </p>
                    <table id="84" border="1"><tr><td><br /><i>p</i></td><td>PCA</td><td>SPC</td><td>FSPCA</td></tr><tr><td><br />500</td><td>76.4</td><td>43.2</td><td>75.3</td></tr><tr><td><br />1000</td><td>50.1</td><td>17.3</td><td>47.8</td></tr><tr><td><br />2000</td><td>81.9</td><td>23.6</td><td>80.5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:这里采用<citation id="170" type="reference"><link href="11" rel="bibliography" />Shen和Huang (2008) </citation>中对解释的方差百分比的定义方式。</p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="85">可以看出, 在不同的维度下, FSPCA模型提取的第一主成分与非稀疏的主成分在解释的方差百分比上差距很小, 说明该模型能够保证在仅损失小部分信息的前提下, 得到稀疏的载荷向量, 而SPC模型则丢失了过多的信息。这表明FSPCA模型比SPC模型更适合此类数据。</p>
                </div>
                <div class="p1">
                    <p id="86">为了更深入且直观地进行结果对比, 以<i>p</i>=1000为例, 给出三个模型得到的第一主成分<b><i>z</i></b><sub>1</sub>及其对应的载荷向量<b><i>v</i></b><sub>1</sub>, 如图1。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/TJYJ201904011_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1n=20、p=1000时, 三种模型得到的第一主成分向量及其载荷向量" src="Detail/GetImg?filename=images/TJYJ201904011_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1</b><i>n</i>=<b>20</b>、<i>p</i>=<b>1000</b><b>时, 三种模型得到的第一主成分向量及其载荷向量</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJYJ201904011_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="88">由图<b>1</b>可见, 首先, 从载荷向量上看, <b>FSPCA</b>模型得到的载荷向量与非稀疏的载荷向量在结构上更相近, <b>SPC</b>模型则出现了过度稀疏化的现象。其次, 从第一主成分的数值上看, <b>FSPCA</b>模型求得的主成分与真实值更接近, 说明该模型的结果更合理。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89"> (<b>二) 迭代GSPCA算法的必要性验证</b></h4>
                <div class="p1">
                    <p id="90">本文从回归分析角度提出<b>GSPCA</b>模型, 但其求解算法并未根据一步回归的方式得到, 而是采用迭代方式。这里给出一个模拟实验, 以基于<b>1</b>-范数的<b>GSPCA</b>模型为例, 通过对比一步算法和迭代算法的表现, 说明迭代算法的必要性。</p>
                </div>
                <div class="p1">
                    <p id="91">本文参考<citation id="172" type="reference"><link href="9" rel="bibliography" /><b>Zou</b> 等 (<b>2006</b>) </citation><citation id="171" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>给出的一个模拟案例, 设有三个隐含因子:</p>
                </div>
                <div class="p1">
                    <p id="92"><i>V</i><sub><b>1</b></sub>～<i>N</i> (<b>0, 300</b>) , <i>V</i><sub><b>2</b></sub>～<i>N</i> (<b>0, 290</b>) , <i>V</i><sub><b>3</b></sub>=-<b>0.3</b><i>V</i><sub><b>1</b></sub>+<b>0.925</b><i>V</i><sub><b>2</b></sub>+ϵ ϵ～<i>N</i> (<b>0, 1</b>) </p>
                </div>
                <div class="p1">
                    <p id="93">且<i>V</i><sub><b>1</b></sub>、<i>V</i><sub><b>2</b></sub>和ϵ相互独立。<b>10</b>个观测变量按如下方式产生:</p>
                </div>
                <div class="p1">
                    <p id="94"><i>X</i><sub><i>i</i></sub>=<i>V</i><sub><b>1</b></sub>+ϵ<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>1</mn></msubsup></mrow></math></mathml>ϵ<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>1</mn></msubsup></mrow></math></mathml>～<i>N</i> (<b>0, 1</b>) <i>i</i>=<b>1, 2, 3, 4</b></p>
                </div>
                <div class="p1">
                    <p id="97"><i>X</i><sub><i>i</i></sub>=<i>V</i><sub><b>2</b></sub>+ϵ<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></math></mathml>ϵ<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></math></mathml>～<i>N</i> (<b>0, 1</b>) <i>i</i>=<b>5, 6, 7, 8</b></p>
                </div>
                <div class="p1">
                    <p id="100"><i>X</i><sub><i>i</i></sub>=<i>V</i><sub><b>3</b></sub>+ϵ<mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>3</mn></msubsup></mrow></math></mathml>ϵ<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>3</mn></msubsup></mrow></math></mathml>～<i>N</i> (<b>0, 1</b>) <i>i</i>=<b>9, 10</b></p>
                </div>
                <div class="p1">
                    <p id="103">且ϵ<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>相互独立, <i>j</i>=<b>1, 2, 3, 4</b>, <i>i</i>=<b>1, …, 10</b>。</p>
                </div>
                <div class="p1">
                    <p id="105">根据上述数据生成机制, 合理的稀疏主成分模型应提取两个主成分, 第一主成分是变量<i>X</i><sub><b>1</b></sub>、<i>X</i><sub><b>2</b></sub>、<i>X</i><sub><b>3</b></sub>和<i>X</i><sub><b>4</b></sub>的线性组合, 以体现<i>V</i><sub><b>1</b></sub>的潜在影响;第二主成分是变量<i>X</i><sub><b>5</b></sub>、<i>X</i><sub><b>6</b></sub>、<i>X</i><sub><b>7</b></sub>和<i>X</i><sub><b>8</b></sub>的线性组合, 以反映<i>V</i><sub><b>2</b></sub>的影响。这两个主成分就能解释原始数据的绝大部分信息。</p>
                </div>
                <div class="p1">
                    <p id="106">对数据矩阵<i>X</i>= (<i>X</i><sub><b>1</b></sub>, …, <i>X</i><sub><b>10</b></sub>) 进行传统的主成分分析、一步GSPCA分析 (基于<b>1</b>-范数惩罚) 和迭代GSPCA分析 (根据定理<b>1</b>, 这里直接采用SPC算法) , 结果如表<b>2</b>所示。</p>
                </div>
                <div class="area_img" id="107">
                    <p class="img_tit"><b>表2 一步GSPCA算法和迭代GSPCA算法的一次模拟结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="107" border="1"><tr><td rowspan="2"><br />变量</td><td colspan="2"><br />PCA</td><td colspan="2">一步GSPCA</td><td colspan="2">迭代GSPCA</td></tr><tr><td><br />PC1</td><td>PC2</td><td>PC1</td><td>PC2</td><td>PC1</td><td>PC2</td></tr><tr><td><br /><i>X</i><sub>1</sub></td><td>-0.13</td><td>-0.47</td><td>0.49</td><td>0</td><td>-0.50</td><td>0</td></tr><tr><td><br /><i>X</i><sub>2</sub></td><td>-0.13</td><td>-0.47</td><td>-0.01</td><td>0</td><td>-0.50</td><td>0</td></tr><tr><td><br /><i>X</i><sub>3</sub></td><td>-0.13</td><td>-0.47</td><td>0.38</td><td>0</td><td>-0.50</td><td>0</td></tr><tr><td><br /><i>X</i><sub>4</sub></td><td>-0.13</td><td>-0.47</td><td>0</td><td>0</td><td>-0.50</td><td>0</td></tr><tr><td><br /><i>X</i><sub>5</sub></td><td>-0.41</td><td>0.05</td><td>0.78</td><td>-0.84</td><td>0</td><td>0.5</td></tr><tr><td><br /><i>X</i><sub>6</sub></td><td>-0.41</td><td>0.05</td><td>0</td><td>0.26</td><td>0</td><td>0.5</td></tr><tr><td><br /><i>X</i><sub>7</sub></td><td>-0.41</td><td>0.05</td><td>0</td><td>0.45</td><td>0</td><td>0.5</td></tr><tr><td><br /><i>X</i><sub>8</sub></td><td>-0.41</td><td>0.05</td><td>0</td><td>0.13</td><td>0</td><td>0.5</td></tr><tr><td><br /><i>X</i><sub>9</sub></td><td>-0.37</td><td>0.23</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td><br /><i>X</i><sub>10</sub></td><td>-0.37</td><td>0.23</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td><br />解释的方差百分比</td><td>55.3%</td><td>44.6%</td><td>16.7%</td><td>32.7%</td><td>47.9%</td><td>36.5%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="108">由表2看出, 一步GSPCA算法的结果与预期偏差较大。该算法提取的第一主成分中丢失了变量<i>X</i><sub>4</sub>, 且非相关变量<i>X</i><sub>5</sub>起主导作用。第二主成分提取变量各系数之间差距较大。此外, 第二主成分解释的方差百分比高于第一主成分的相应值, 说明模型在第一主成分的提取上出现了偏差。</p>
                </div>
                <div class="p1">
                    <p id="109">相比而言, 迭代GSPCA算法的结果与预期基本一致。该算法得到了稀疏的载荷向量, 前两个主成分所解释的累计方差百分比为84.4%, 并未损失过多信息。第一、第二主成分所含变量及变量的相应系数均与预期一致, 两个主成分都具备良好解释性, 同时反映了数据的潜在模式, 说明迭代GSPCA算法在这一案例中是有效的。</p>
                </div>
                <div class="p1">
                    <p id="110">将上述实验进行100次, 得到的前两个主成分的平均解释方差百分比及其标准差如表3。</p>
                </div>
                <div class="area_img" id="111">
                    <p class="img_tit"><b>表3 100次模拟得到的平均解释方差百分比的对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"> (%) </p>
                    <table id="111" border="1"><tr><td><br /></td><td>PCA</td><td>一步GSPCA</td><td>迭代GSPCA</td></tr><tr><td><br />PC1</td><td>65.9 (0.09) </td><td>15.6 (0.05) </td><td>46.0 (0.07) </td></tr><tr><td><br />PC2</td><td>33.9 (0.09) </td><td>24.3 (0.10) </td><td>34.5 (0.04) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:括号内的值表示解释的方差百分比的标准差。</p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="112">由表3看出, 100次模拟与一次模拟结果基本一致, 迭代算法得到的累计解释方差百分比在80%以上, 而一步算法的值仅为39.9%, 且其第一主成分解释的方差过小, 这样的结果不合理。从标准差来看, 迭代算法的相应值最小, 说明其结果最稳定。综上, 应采用迭代算法求解。</p>
                </div>
                <h3 id="113" name="113" class="anchor-tag">四、实证研究</h3>
                <div class="p1">
                    <p id="114">下文利用FSPCA方法进行手写数字识别, 并将其效果与SPC方法进行对比分析。本文数据来自MNIST手写数字数据库http://yann.lecun.com/exdb/mnist/index.html。该数据库的每张图片包含28×28个像素点, 在数据集中以一个784维的向量形式存储。本文从中抽取0～9的手写数字图片各10张, 组成10个10×784的数据矩阵, 来测试FSPCA模型在手写数字图像恢复上的效果。</p>
                </div>
                <div class="p1">
                    <p id="115">本文采用格点法对FSPCA模型中的λ值进行选择, 且不要求λ的值固定不变, 即可以在求解下一个主成分时, 对λ的值进行更新。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116"> (<b>一</b>) FSPCA<b>模型的整体效果</b></h4>
                <div class="p1">
                    <p id="117">以数字0为例, 给出FSPCA模型对10个手写数字0图像的恢复效果, 如图2。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/TJYJ201904011_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 FSPCA模型对数字0的恢复图像 (提取主成分个数为2)" src="Detail/GetImg?filename=images/TJYJ201904011_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 FSPCA模型对数字0的恢复图像 (提取主成分个数为2</b>)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJYJ201904011_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="119">从图<b>2</b>可以看出, 当仅提取两个主成分时, <b>FSPCA</b>模型即可对手写数字<b>0</b>的图像进行充分的恢复。<b>10</b>个样本中, 前两个主成分所解释的累计方差百分比均达到<b>70</b>%以上。</p>
                </div>
                <div class="p1">
                    <p id="120">此外, 本文还利用<b>FSPCA</b>模型对数字<b>1～9</b>进行了图像恢复, 并将<b>FSPCA</b>模型和<b>SPC</b>模型在提取相同个数的主成分时所解释的累计方差百分比进行了对比<citation id="173" type="note"><link href="3" rel="footnote" /><sup> (1) </sup></citation>。结果表明, <b>FSPCA</b>模型在仅提取少数 (<b>2～4</b>个) 主成分的前提下, 能够很好地对手写数字图像进行恢复。从累计方差百分比来看, <b>FSPCA</b>模型虽然得到的相应值更大, 但与<b>SPC</b>模型之间的差距并不显著。</p>
                </div>
                <h4 class="anchor-tag" id="121" name="121"> (<b>二) 主成分的解释性</b></h4>
                <div class="p1">
                    <p id="122">下文从模型的解释能力上来分析<b>FSPCA</b>模型和<b>SPC</b>模型。仍以数字<b>0</b>为例, 图<b>3</b>和图<b>4</b>分别给出了<b>FSPCA</b>模型所提取的第一主成分和第二主成分。</p>
                </div>
                <div class="area_img" id="123">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/TJYJ201904011_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 FSPCA模型对手写数字0所提取的第一主成分" src="Detail/GetImg?filename=images/TJYJ201904011_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 FSPCA模型对手写数字0所提取的第一主成分</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJYJ201904011_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/TJYJ201904011_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 FSPCA模型对手写数字0所提取的第二主成分" src="Detail/GetImg?filename=images/TJYJ201904011_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 FSPCA模型对手写数字0所提取的第二主成分</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJYJ201904011_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="125">可以看出, <b>10</b>个第一主成分中, 有<b>6</b>个表示左右两条竖线, <b>4</b>个表示上下两条横线。而<b>10</b>个第二主成分所代表的内容和相应的第一主成分恰好实现互补, 有<b>6</b>个表示上下两条横线, <b>4</b>个表示左右两条竖线。即<b>FSPCA</b>模型给出的解释为:数字<b>0</b>主要是由左右两条竖线和上下两条横线构成的。两个主成分给出的解释非常明确, 也符合实际。当然, <b>10</b>个样本所提取的第一主成分不完全相同, 这是因为标准的数字<b>0</b>是一个中心对称的图像, 每个人在书写<b>0</b>时的着重点不同所致。</p>
                </div>
                <div class="p1">
                    <p id="126"><b>SPC</b>模型所提取的第一、第二主成分如图<b>5</b>和图<b>6</b>所示。显然, 整体上<b>SPC</b>模型所提取的主成分的解释能力较差。虽然多数样本的第一主成分的解释性尚好, 但是部分样本 (如样本<b>8</b>和<b>9</b>) 所提取的第一主成分不能给出合理解释。同时, 大部分的第二主成分已经失去了解释性。因此可以看出, <b>FSPCA</b>模型在模型的解释性上表现出了明显的优势。</p>
                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/TJYJ201904011_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 SPC模型对手写数字0所提取的第一主成分" src="Detail/GetImg?filename=images/TJYJ201904011_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 SPC模型对手写数字0所提取的第一主成分</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJYJ201904011_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/TJYJ201904011_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 SPC模型对手写数字0所提取的第二主成分" src="Detail/GetImg?filename=images/TJYJ201904011_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 SPC模型对手写数字0所提取的第二主成分</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJYJ201904011_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="129">为了不以个例的结果来判定模型效果, 本文又以数字<b>3</b>为例, 对比了<b>FSPCA</b>模型和<b>SPC</b>模型所提取的第一、二主成分。发现对于第一主成分而言, 两个模型的结果差距不大, 每个主成分都是在表示数字<b>3</b>的上中下三条横线。但在第二主成分上, 两个模型的结果出现了较大差距。<b>FSPCA</b>模型所提取的第二主成分基本都是展示数字<b>3</b>中右侧的竖线部分, 将第一、二主成分结合, 数字<b>3</b>的形状基本形成。但<b>SPC</b>模型所提取的第二主成分并没有明显规律, 失去了合理的解释。</p>
                </div>
                <div class="p1">
                    <p id="130">从以上两个案例可以看出, 与<b>SPC</b>模型相比, <b>FSPCA</b>模型所提取的主成分具备更好的解释性能, 能够正确的反映原始数据的结构, 体现数据的潜在模式。</p>
                </div>
                <div class="p1">
                    <p id="131">此外, 在<b>FSPCA</b>模型中, 有两个因素会对主成分分析效果产生影响。首先是主成分个数, 与传统<b>PCA</b>一样, 提取的主成分即提取的信息, 以图片数据为例, 每个主成分代表图片中不同位置的信息, 主成分个数决定了提取的信息量大小。其次是参数<i>λ</i>的取值。从模型来看, <i>λ</i>控制着<b>fused</b>惩罚中∑<sub><i>j</i>=<b>2</b></sub>|<i>v</i><sub><i>j</i></sub>-<i>v</i><sub><i>j</i>-<b>1</b></sub>|对于总目标函数的贡献。增大<i>λ</i>的值, 会使更多相邻元素取值相等, 再加上<mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></msub><mrow><mrow><mo>|</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></math></mathml>的作用, 也就有更多的元素被压缩为<b>0</b>。也就是说, 改变<i>λ</i>的值往往会对数据零点和非零点交界处的值产生影响。以图像数据为例, 改变<i>λ</i>往往会改变图像边界的光滑或粗细。</p>
                </div>
                <div class="p1">
                    <p id="133">综上, 本文提出的FSPCA模型在手写数字识别上取得了较好效果。首先, FSPCA模型可以在提取少数主成分的前提下, 对手写数字图像进行很好的恢复, 不会因载荷向量的稀疏性而损失太多信息。其次, FSPCA模型所提取的主成分具有很好的解释性, 这一点相比于SPC模型有非常明显的优势。最后, 主成分的个数和调和参数λ的取值都会对FSPCA方法的结果产生影响。</p>
                </div>
                <h3 id="134" name="134" class="anchor-tag">五、结论</h3>
                <div class="p1">
                    <p id="135">本文主要研究基于fused惩罚的稀疏主成分分析。首先, 从回归角度出发, 提出一种广义的稀疏主成分模型——GSPCA模型, 该模型的惩罚函数可以是任意适当形式。其次, 提出将fused惩罚与稀疏主成分分析结合, 得到一种fused稀疏主成分分析方法——FSPCA方法, 并从回归分析和PMD两个角度, 给出两种模型形式。通过理论证明、模拟实验和实证分析, 得出如下结论:</p>
                </div>
                <div class="p1">
                    <p id="136"><b>1</b>. GSPCA模型在取<b>1</b>-范数惩罚时, 其求解结果与同样基于<b>1</b>-范数惩罚的稀疏主成分模型——SPC模型的结果相同。</p>
                </div>
                <div class="p1">
                    <p id="137"><b>2</b>. 从PMD角度出发与从回归角度出发得到的fused稀疏主成分模型的求解结果是一致的。</p>
                </div>
                <div class="p1">
                    <p id="138"><b>3</b>. 模拟实验显示, 本文提出的FSPCA方法在处理相邻变量之间高度相关甚至完全相等的高维数据时, 表现突出, 能更准确的体现数据的潜在模式。</p>
                </div>
                <div class="p1">
                    <p id="139"><b>4</b>. 实证分析表明, FSPCA模型在手写数字识别上有很好的应用价值。在仅提取少数主成分的前提下, 该模型可以反映数据的大部分信息。虽然在解释的累计方差百分比上, FSPCA模型与SPC模型相比并没有提高很多, 但在主成分的解释性上, FSPCA模型表现出了显著的优势。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="5">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD757584614&amp;v=MzEzMTdML0lOam5CYXJTOUdkVEVxNGxFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnl6Z1Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>Cadima J, Jolliffe I T.Loading and Correlations in the Interpretation of Principle Components [J].Journal of Applied Statistics, 1995, 22 (2) :203-214.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800004698&amp;v=MTU2ODNRVE1ud1plWnRGaW5sVXJuSUpGc1RhaEk9TmpuQmFySzdIdGZPcDQ5RlpPc0xDblV4b0JNVDZUNFBRSC9pclJkR2VycQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>Jolliffe I T, Trendafilov N T, Uddin M.A Modified Principal Component Technique Based on the LASSO [J].Journal of Computational and Graphical Statistics, 2003, 12 (3) :531-547.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800004542&amp;v=MDMwODJnN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJuSUpGc1RhaEk9TmpuQmFySzdIdGZPcDQ5RlpPc0xDWA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>Zou H, Hastie T, Tibshirani R.Sparse Principal Component Analysis [J].Journal of Computational and Graphical Statistics, 2006, 15 (2) :265-286.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600337495&amp;v=MjM3OTZhaEk9TmlmT2ZiSzdIdEROcVk5RlorZ0lDSFU4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcm5JSkZzVA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>Shen H, Huang J Z.Sparse Principal Component Analysis via Regularized Low Rank Matrix Approximation [J].Journal of Multivariate Analysis, 2008, 99 (6) :1015-1034.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Penalized Matrix Decomposition with Applications to Sparse Principal Components and Canonical Correlation Analysis">

                                <b>[5]</b>Witten D M, Tibshirani R, Hastie T.A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis [J].Biostatistics, 2009, 10 (3) :515-534.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300630182&amp;v=MjYyNzg0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVybklKRnNUYWhJPU5pZk9mYks5SDlQT3JJOUZZdWdQRFhRN29CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>Qi X, Luo R, Zhao H.Sparse Principal Component Analysis by Choice of Norm [J].Journal of Multivariate Analysis, 2013, 114 (1) :127-160.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparse principal component analysis with measurement errors">

                                <b>[7]</b>Shi J, Song W.Sparse Principal Component Analysis with Measurement Errors [J].Journal of Statistical Planning &amp; Inference, 2016, 175:87-99.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES5D3D5AD33D72BE61D95DED8CB30310FF&amp;v=MzA2MzU9TmlmT2ZiYk1IYVhKM3Z0R1o1OElEZzVNeVJkbjR6b0pQUXZxMzJBMmViR1ZSY3pwQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dDVod0wyNHdLZw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>Sharifzadeh, et al.Sparse Supervised Principal Component Analysis (SSPCA) for Dimension Reduction and Variable Selection [J].Engineering Applications of Artificial Intelligence, 2017, 65:168-177.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14120603924782&amp;v=MDQwOTVHYmVrTEMzUTdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVybklKRnNUYWhJPU5pZlllcks4SDlQTXFZOQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>Tibshirani R.Regression Shrinkage and Selection via the Lasso[J].Journal of the Royal Statistical Society, Series B, 1996, 58:267-288.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00001256895&amp;v=MDkxNjNOcllwRGJPSUtZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZpN2xWN3JQSUY0PU5pZmNhck80SHRI&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>Zou H, Hastie T.Regularization and Variable Selection via the Elastic Net[J].Journal of the Royal Statistical Society, Series B, 2005, 67 (2) :301-320.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00001256883&amp;v=MjE4MjVpZmNhck80SHRITnJZcERiT01NWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaTdsVjdyUElGND1O&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>Tibshirani R, et al.Sparsity and Smoothness via the Fused Lasso [J].Journal of the Royal Statistical Society, Series B, 2005, 67 (1) :91-108.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJSTFC7183B4B890081CC97417653E0D4F36&amp;v=MTI0NzlKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dDVod0wyNHdLZz1OaWZZZXNYTEdkREVyUDFCRnVNR0RId3h6bVZnNHpoNVNYamtxUkZBZWNhUU03bVpDT052RlNpV1dyNw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>Friedman J, Hastie T, Tibshirani R.Pathwise Coordinate Optimization [J].Annals of Applied Statistics, 2007, 1 (2) :302-332.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800004295&amp;v=Mjc0NzhLN0h0Zk9wNDlGWk9zTERuVThvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVybklKRnNUYWhJPU5qbkJhcg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>Hoefling H.A Path Algorithm for the Fused Lasso Signal Approximator [J].Journal of Computational and Graphical Statistics, 2010, 19 (4) :984-1006.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJDB69BB77BABA6A0079DF287A07F767C74&amp;v=MTU5NDhRSGlUckJWRGZyU1ROcjJiQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dDVod0wyNHdLZz1Oam5CYXNHK0Y2TytxSWczRlpsK0NnMDV6eEVhbmtsLw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>Petersen A, Witten D, Simon N.Fused Lasso Additive Model [J].Journal of Computational and Graphical Statistics, 2016, 25 (4) :1005-1025.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
            <div class="reference anchor-tag" id="a_footnote">
 <h3>注释</h3>
                    <p>
                        <span id="3" href="javascript:void(0)">
                            <b>1</b> 为控制篇幅, 具体结果未在正文展出, 读者可向作者索要。
                        </span>
                    </p>
            </div>
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="TJYJ201904011" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJYJ201904011&amp;v=MTkxMjU3cWZadVpvRnl6Z1ZML0pNU2ZTWkxHNEg5ak1xNDlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVuMWxvWHh6aDZ1UFp4Z2NVUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=vAscMyvIPP9NePnbGPkqJ0A5tHOvnzl65tGuRDsf9xg1" rel="stylesheet"/>

</body>
</html>

