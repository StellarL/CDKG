

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637140718017318750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dTJLT201908005%26RESULT%3d1%26SIGN%3dgtJpaXFSWkXKZPCMycSfsNL83ek%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=TJLT201908005&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=TJLT201908005&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJLT201908005&amp;v=MDI5OTJGaURsVkwzQU1TZkhlckc0SDlqTXA0OUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm4=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="一、引 言 ">一、引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#43" data-title="二、随机森林算法 ">二、随机森林算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#44" data-title=" (&lt;b&gt;一) 随机森林算法简介&lt;/b&gt;"> (<b>一) 随机森林算法简介</b></a></li>
                                                <li><a href="#49" data-title=" (&lt;b&gt;二) 随机森林理论&lt;/b&gt;"> (<b>二) 随机森林理论</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="三、自编码神经网络 ">三、自编码神经网络</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#78" data-title=" (&lt;b&gt;一) 传统自编码器&lt;/b&gt;"> (<b>一) 传统自编码器</b></a></li>
                                                <li><a href="#90" data-title=" (&lt;b&gt;二) 改进自编码器&lt;/b&gt;"> (<b>二) 改进自编码器</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#104" data-title="四、基于自编码特征提取的随机森林模型实验 ">四、基于自编码特征提取的随机森林模型实验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#105" data-title=" (&lt;b&gt;一) 实验目的&lt;/b&gt;"> (<b>一) 实验目的</b></a></li>
                                                <li><a href="#112" data-title=" (&lt;b&gt;二) 实验数据及主要过程&lt;/b&gt;"> (<b>二) 实验数据及主要过程</b></a></li>
                                                <li><a href="#123" data-title=" (&lt;b&gt;三) 实验结果及分析&lt;/b&gt;"> (<b>三) 实验结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#135" data-title="五、结 论 ">五、结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#46" data-title="&lt;b&gt;图&lt;/b&gt;1 &lt;b&gt;随机森林图解&lt;/b&gt;"><b>图</b>1 <b>随机森林图解</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;图&lt;/b&gt;2 &lt;b&gt;自编码网络示意图&lt;/b&gt;"><b>图</b>2 <b>自编码网络示意图</b></a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;实验数据信息&lt;/b&gt;"><b>表</b>1 <b>实验数据信息</b></a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;部分参数信息&lt;/b&gt;"><b>表</b>2 <b>部分参数信息</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;图&lt;/b&gt;3 &lt;b&gt;钞票数据集实验结果&lt;/b&gt;"><b>图</b>3 <b>钞票数据集实验结果</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;图&lt;/b&gt;4 &lt;b&gt;声纳数据集实验结果&lt;/b&gt;"><b>图</b>4 <b>声纳数据集实验结果</b></a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;图&lt;/b&gt;5 MNIST&lt;b&gt;数据集实验结果&lt;/b&gt;"><b>图</b>5 MNIST<b>数据集实验结果</b></a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;测试误差&lt;/b&gt;"><b>表</b>3 <b>测试误差</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>
                                    <dd class="subnode">
                                        <h6>
                                            <a href="#a_footnote">注释</a>

                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="154">


                                    <a id="bibliography_1" title=" Quinlan J R.Induction of Decision Trees[J].Machine Learning, 1986, 1 (1) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001337810&amp;v=MTY3NjlqN0Jhck80SHRITnJJeENiT29QWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaUhrVzcvUElsWT1O&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Quinlan J R.Induction of Decision Trees[J].Machine Learning, 1986, 1 (1) .
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_2" title=" Cortes C, Vapnik V.Support-vector Networks[J].Machine Learning, 1995, 20 (3) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339251&amp;v=MDEwNTQ5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlIa1c3L1BJbFk9Tmo3QmFyTzRIdEhOckl4TVp1NE9ZM2s1ekJkaDRq&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Cortes C, Vapnik V.Support-vector Networks[J].Machine Learning, 1995, 20 (3) .
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_3" title=" Breiman L.Bagging Predictors[J].Machine Learning, 1996, 24 (2) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339482&amp;v=MjM4MThITnJJeE1ZT01OWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaUhrVzcvUElsWT1OajdCYXJPNEh0&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Breiman L.Bagging Predictors[J].Machine Learning, 1996, 24 (2) .
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_4" title=" Freund Y, Schapire R E.A Decision-Theoretic Generalization of On-line Learning and an Application to Boosting[J].Journal of Computer and System Sciences, 1997, 55 (1) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601258112&amp;v=MTg3Mjg2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJiSktGNFRhQm89TmlmT2ZiSzdIdEROcVk5RVp1NEhEWDA3b0JNVA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Freund Y, Schapire R E.A Decision-Theoretic Generalization of On-line Learning and an Application to Boosting[J].Journal of Computer and System Sciences, 1997, 55 (1) .
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_5" title=" Breiman L.Random Forests[J].Machine Learning, 2001, 45 (1) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340271&amp;v=MjE2NTZ6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlIa1c3L1BJbFk9Tmo3QmFyTzRIdEhOckl0Rlp1d09ZM2s1&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Breiman L.Random Forests[J].Machine Learning, 2001, 45 (1) .
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_6" title=" 董师师, 黄哲学.随机森林理论浅析[J].集成技术, 2013, 2 (1) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JCJI201301001&amp;v=MjYwMTMzenFxQnRHRnJDVVI3cWZadVpuRmlEbFZMM0FMeTdCWjdHNEg5TE1ybzlGWllRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         董师师, 黄哲学.随机森林理论浅析[J].集成技术, 2013, 2 (1) .
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_7" title=" 王奕森, 夏树涛.集成学习之随机森林算法综述[J].信息通信技术, 2018, 12 (1) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=OXXT201801009&amp;v=Mjg3OTgzQUtqWFRlckc0SDluTXJvOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm5GaURsVkw=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         王奕森, 夏树涛.集成学习之随机森林算法综述[J].信息通信技术, 2018, 12 (1) .
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_8" title=" 方匡南, 吴见彬, 朱建平, 等.随机森林方法研究综述[J].统计与信息论坛, 2011, 26 (3) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJLT201103007&amp;v=MjAwMjhadVpuRmlEbFZMM0FNU2ZIZXJHNEg5RE1ySTlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         方匡南, 吴见彬, 朱建平, 等.随机森林方法研究综述[J].统计与信息论坛, 2011, 26 (3) .
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_9" title=" Bourlard H, Kamp Y.Auto-association by Multilayer Perceptrons and Singular Value Decomposition[J].Biological Cybernetics, 1988, 59 (4) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003264827&amp;v=MjA4MjEzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlIa1c3L1BJbFk9Tmo3QmFyTzRIdEhQcllsQmJPa0lZ&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Bourlard H, Kamp Y.Auto-association by Multilayer Perceptrons and Singular Value Decomposition[J].Biological Cybernetics, 1988, 59 (4) .
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_10" title=" 殷瑞刚, 魏帅, 李晗, 等.深度学习中的无监督学习方法综述[J].计算机系统应用, 2016, 25 (8) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYY201608001&amp;v=Mjc4NDFUblNkN0c0SDlmTXA0OUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm5GaURsVkwzQVA=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         殷瑞刚, 魏帅, 李晗, 等.深度学习中的无监督学习方法综述[J].计算机系统应用, 2016, 25 (8) .
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_11" title=" Rumelhart D E, Hinton G E, Williams R J.Learning Representations by Back-propagating Errors[J].Nature, 1986, 323 (6088) ." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning representations by back-propagating errors">
                                        <b>[11]</b>
                                         Rumelhart D E, Hinton G E, Williams R J.Learning Representations by Back-propagating Errors[J].Nature, 1986, 323 (6088) .
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_12" title=" Ho T K.The Random Subspace Method for Constructing Decision Forests[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1998, 20 (8) ." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The random subspace method for constructing decision forests">
                                        <b>[12]</b>
                                         Ho T K.The Random Subspace Method for Constructing Decision Forests[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1998, 20 (8) .
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_13" title=" Olshausen B A, Field D J.Sparse Coding of Sensory Inputs[J].Current Opinion in Neurobiology, 2004, 14 (4) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501611081&amp;v=MzI1ODJETnFvOUVZdW9PREhRNG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJiSktGNFRhQm89TmlmT2ZiSzdIdA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Olshausen B A, Field D J.Sparse Coding of Sensory Inputs[J].Current Opinion in Neurobiology, 2004, 14 (4) .
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_14" title=" Vincent P, Larochelle H, Bengio Y, et al.Extracting and Composing Robust Features with Denoising Autoencoders[C].In Proceedings of the 25th International Conference on Machine learning-ICML’08, 2008." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extracting and composing robust features with denoising autoencoders">
                                        <b>[14]</b>
                                         Vincent P, Larochelle H, Bengio Y, et al.Extracting and Composing Robust Features with Denoising Autoencoders[C].In Proceedings of the 25th International Conference on Machine learning-ICML’08, 2008.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_15" title=" Rao Y, Principe J.A Fast, On-line Algorithm for PCA and Its Convergence Characteristics[J].IEEE, Transactions on Neural Network, 2000, 4 (2) ." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Fast On-line Algorithm for PCA and Its Convergence Characteristics">
                                        <b>[15]</b>
                                         Rao Y, Principe J.A Fast, On-line Algorithm for PCA and Its Convergence Characteristics[J].IEEE, Transactions on Neural Network, 2000, 4 (2) .
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=TJLT" target="_blank">统计与信息论坛</a>
                2019,34(08),27-33             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于稀疏降噪自编码器的随机森林模型</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%AD%A6%E5%B3%A5&amp;code=42647892&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">武峥</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%81%E5%86%B2&amp;code=42647893&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">丁冲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%99%AF%E8%8B%B1%E5%B7%9D&amp;code=22448593&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">景英川</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%AA%E5%8E%9F%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E6%95%B0%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0077528&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">太原理工大学数学学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>随机森林算法是一类在机器学习中较为常见的算法, 其在数据的分类以及非参数回归中都有重要的作用。如何更好地处理数据, 进行特征选择是随机森林的重点研究领域。自编码神经网络在深度学习中有着不可替代的作用, 其在数据压缩、特征提取等方面有着优异的性能。结合两者优点, 提出一种基于使用稀疏降噪自编码器对原始数据进行特征提取的随机森林算法。采用多种常用数据集进行实验分析, 对原始数据分别采用不同的特征提取方法, 并利用随机森林将提取后的特征进行分类。实验结果表明, 利用稀疏降噪自编码神经网络进行特征提取所得到的特征, 能够使随机森林的分类精度得到一定程度的提高。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">随机森林;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自编码器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    武峥, 男, 山西太原人, 硕士生, 研究方向:数据挖掘;;
                                </span>
                                <span>
                                    丁冲, 男, 辽宁盘锦人, 硕士生, 研究方向:数据挖掘;;
                                </span>
                                <span>
                                    *景英川, 女, 山西临汾人, 硕士, 副教授, 研究方向:数据挖掘。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目《高维数据变量间非线性交互作用的研究》 (11571009);</span>
                    </p>
            </div>
                    <h1><b>Random Forest Model Based on Sparse Denoising Auto-encode</b></h1>
                    <h2>
                    <span>WU Zheng</span>
                    <span>DING Chong</span>
                    <span>JING Ying-chuan</span>
            </h2>
                    <h2>
                    <span>School of Mathematics, Taiyuan University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Random forest algorithm is a common algorithm in machine learning, which plays an important role in data classification and non-parametric regression.How to better process data and select features have become a key research area of random forest.Auto-Encode neural network plays an irreplaceable role in deep learning and has excellent performance in data compression and feature extraction.Combining the advantages mentioned, a random forest algorithm based on feature extraction of original data using sparse noise reduction auto-encoder is proposed.A variety of commonly used data sets are used for experimental analysis.Different feature extraction methods are adopted for the original data, and the extracted features are classified by random forest.The results show that the classification accuracy of random forests can be improved to a certain extent by using sparse noise reduction auto-encoder neural network to extract features.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=random%20forest&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">random forest;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=auto-encoder&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">auto-encoder;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-11</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag">一、引 言</h3>
                <div class="p1">
                    <p id="36">伴随着新时代的来临, 当今的信息技术迈进了高速发展的时期, 获取与储存数据越发方便, 人们在实际应用中所获得的数据种类也多种多样。在多种不同的数据面前, 随机森林在某些情况下会表现出较差的性能, 对部分数据无法很好地适应。为增强随机森林算法性能, 本文从数据本身入手, 对数据的特征提取方法进行改进, 主要是运用了神经网络之中的稀疏降噪自编码器对数据中隐含的特征进行精炼提取, 并将提取后的特征运用于随机森林。</p>
                </div>
                <div class="p1">
                    <p id="37">本文采用了三个数据集对特征提取后的随机森林算法进行分析比较, 发现该算法对于一般常见的数据集而言, 相比普通的随机森林或者进行主成分数据提取后的随机森林拥有更为优异的性能。</p>
                </div>
                <div class="p1">
                    <p id="38">在机器学习的众多算法之中, 基本的分类器有许多种类, 例如决策树以及支持向量机等算法<citation id="184" type="reference"><link href="154" rel="bibliography" /><link href="156" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。这类算法被称为单分类器, 可能存在过拟合等不足之处。</p>
                </div>
                <div class="p1">
                    <p id="39">运用集成学习的方法, 将多个相同或不同的分类器进行某种组合, 可以获得相较于单分类器而言更为优秀的性能。Bagging以及Boosting是两类基础的集成学习方法<citation id="185" type="reference"><link href="158" rel="bibliography" /><link href="160" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>, 而随机森林是一类相对比较成熟的多分类器, 也是Bagging中运用最为广泛的一类方法<citation id="186" type="reference"><link href="162" rel="bibliography" /><link href="164" rel="bibliography" /><link href="166" rel="bibliography" /><link href="168" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="40">随机森林作为一种多分类器, 其主要思想是将决策树作为基本的单分类器, 将各个独立的单分类器同时运用Bagging算法进行组合, 在变量的选取方面使用了随机属性的选择。随机森林相对于传统的决策树算法, 能够在不进行剪枝的同时防止过拟合的出现, 同时又具备较快的训练速度且参数调整方便, 而且在高维数据处理上, 对其分类也有着良好的并行性。鉴于随机森林所表现出的各种出色的性能, 该方法被应用于医学、生物学等多种学科的分类预测之中。</p>
                </div>
                <div class="p1">
                    <p id="41">自编码神经网络作为深度学习中不可忽视的一个重要内容, 是一类无监督的学习方法, 对原始数据采用一系列非线性变化后重构成与原始数据相近似的数据<citation id="187" type="reference"><link href="170" rel="bibliography" /><link href="172" rel="bibliography" /><link href="174" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>。假如自编码器之中的隐藏层神经元个数比原始数据的维数小, 将会使得自编码器将原始数据维数进行一定的压缩处理。同时, 若原始数据之间隐含有某些有联系的结构, 那么自编码器也能够很快发掘此类数据间的相关性, 在输出层重新构建原始数据。假如使得自编码器之中的隐藏层神经元个数接近或大于原始数据维数, 此时就可以对自编码器之中的隐藏层神经元增加适当的稀疏性限制, 使得数据变得相对稀疏, 可视为对原始数据的一种重构。与此相同的是, 在每次应用样本对神经元进行训练时随机激活部分神经元可以令自编码器更具有鲁棒性。</p>
                </div>
                <div class="p1">
                    <p id="42">本文将稀疏降噪自编码神经网络与随机森林相结合, 提出基于稀疏降噪自编码进行特征提取的随机森林, 旨在应用稀疏降噪自编码器的特殊性质, 对原始数据进行重新构成, 发现原始数据潜在联系, 使得数据更容易被随机森林学习。将变化后的特征作为新的特征集, 运用随机森林对重构后的特征分类。本文使用两类小样本数据集及一类较大样本数据集对该方法进行测试, 选取普通随机森林、主成分随机森林作为参照模型, 与稀疏降噪自编码器随机森林进行分类效果的比较。</p>
                </div>
                <h3 id="43" name="43" class="anchor-tag">二、随机森林算法</h3>
                <h4 class="anchor-tag" id="44" name="44"> (<b>一) 随机森林算法简介</b></h4>
                <div class="p1">
                    <p id="45">Leo Breiman于2001年在决策树及集成学习的基础上进行改进并发表了随机森林算法。如图1所示, 随机森林利用Bagging算法构建了众多与原始数据集的样本个数相同的训练集, 这些训练集全部是由原始数据集有放回地抽取所产生, 且各个训练集互不相同。将每个训练集构建单棵决策树的同时进行随机属性选择<citation id="188" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 对其节点采取随机的属性分裂。</p>
                </div>
                <div class="area_img" id="46">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/TJLT201908005_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 随机森林图解" src="Detail/GetImg?filename=images/TJLT201908005_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 <b>随机森林图解</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJLT201908005_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="47">随机森林的分类是结合了所有决策树的结果进行综合投票后产生的, 将所获票数占总得票数比值最大的类别视为样本最终的分类, 因此随机森林算法在进行数据分析时对于数据的噪声拥有优秀的容忍性, 获得优良的分类效果。对于高维的数据而言, 随机森林在编程中拥有优秀的并行性。不仅如此, 在实际应用中随机森林除了能进行分类、回归以外, 还能对缺失值、异常值进行处理。</p>
                </div>
                <div class="p1">
                    <p id="48">随机森林拥有许多优良之处:对于许多数据集, 其准确率均高于其余算法;在面对较高维的数据时能够进行准确的处理, 而且不必对数据的特征属性进行人为选取;训练速度较为迅速, 能够利用并行化方法加速运算;即使使用不平衡的数据集, 它也能够尽可能地平衡误差;即便缺失较多特征, 仍能够保证准确度不降低;由于引入了部分随机性, 令随机森林具有更加优秀的鲁棒性。</p>
                </div>
                <h4 class="anchor-tag" id="49" name="49"> (<b>二) 随机森林理论</b></h4>
                <div class="p1">
                    <p id="50">设随机森林能够表达为{<i>h</i> (<i>X</i>, <i>θ</i><sub><i>k</i></sub>) , <i>k</i>=1, 2…, <i>K</i>}, <i>K</i>作为随机森林之中拥有的决策树的数量。原始的数据集<i>T</i>为{ (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) , <i>x</i><sub><i>i</i></sub>∈<i>X</i>, <i>y</i><sub><i>i</i></sub>∈<i>Y</i>, <i>i</i>=1, 2, …, <i>N</i>}, 其中<i>N</i>为样本数, <i>X</i>为含有<i>M</i>个特征的<i>N</i>×<i>M</i>矩阵, <i>Y</i>含有<i>J</i>个互不相同的类别。第<i>k</i>次抽取的训练集记为<i>T</i><sub><i>k</i></sub>。</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51">1.泛化误差界。</h4>
                <div class="p1">
                    <p id="153">随机森林的泛化误差PE<sup>*</sup>为:</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>Ρ</mtext><mtext>E</mtext><msup><mrow></mrow><mo>*</mo></msup><mover><mstyle mathsize="140%" displaystyle="true"><mrow><munderover><mo stretchy="true" symmetric="false">=</mo><mrow></mrow><mrow></mrow></munderover></mrow></mstyle><mrow><mtext>d</mtext><mtext>e</mtext><mtext>f</mtext></mrow></mover><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>X</mi><mo>, </mo><mi>Y</mi></mrow></msub><mo stretchy="false"> (</mo><mo stretchy="false"> (</mo><mtext>a</mtext><mtext>v</mtext><msub><mrow></mrow><mi>k</mi></msub><mi>Ι</mi><mo stretchy="false"> (</mo><mi>h</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>-</mo></mtd></mtr><mtr><mtd><mrow><mi>max</mi></mrow><msub><mrow></mrow><mrow><mi>j</mi><mo>≠</mo><mi>Y</mi></mrow></msub><mtext>a</mtext><mtext>v</mtext><msub><mrow></mrow><mi>k</mi></msub><mi>Ι</mi><mo stretchy="false"> (</mo><mi>h</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>j</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>&lt;</mo><mn>0</mn><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="53">PE<sup>*</sup>是随机森林对于分类错误率的一个度量标准。有如下收敛定理<citation id="189" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="54" class="code-formula">
                        <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>Ρ</mtext><mtext>E</mtext><msup><mrow></mrow><mo>*</mo></msup><mover><mo stretchy="true" symmetric="false">→</mo><mrow><mtext>a</mtext><mo>.</mo><mtext>s</mtext><mo>.</mo></mrow></mover><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>X</mi><mo>, </mo><mi>Y</mi></mrow></msub><mo stretchy="false"> (</mo><mo stretchy="false"> (</mo><mi>Ρ</mi><msub><mrow></mrow><mi>θ</mi></msub><mo stretchy="false"> (</mo><mi>h</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>-</mo></mtd></mtr><mtr><mtd><mrow><mi>max</mi></mrow><msub><mrow></mrow><mrow><mi>j</mi><mo>≠</mo><mi>Y</mi></mrow></msub><mi>Ρ</mi><msub><mrow></mrow><mi>θ</mi></msub><mo stretchy="false"> (</mo><mi>h</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mi>j</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>&lt;</mo><mn>0</mn><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="55">定义随机森林的余量函数mr (<i>X</i>, <i>Y</i>) 为公式 (3) 。同时该函数值越大, 标志着随机森林的可信度越大。</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>m</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>Y</mi><mo stretchy="false">) </mo><mover><mstyle mathsize="140%" displaystyle="true"><mo>=</mo></mstyle><mrow><mtext>d</mtext><mtext>e</mtext><mtext>f</mtext></mrow></mover><mi>Ρ</mi><msub><mrow></mrow><mi>θ</mi></msub><mo stretchy="false"> (</mo><mi>h</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>-</mo></mtd></mtr><mtr><mtd><mrow><mi>max</mi></mrow><msub><mrow></mrow><mrow><mi>j</mi><mo>≠</mo><mi>Y</mi></mrow></msub><mi>Ρ</mi><msub><mrow></mrow><mi>θ</mi></msub><mo stretchy="false"> (</mo><mi>h</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mi>j</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">从收敛定理可以得出<mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Ρ</mtext><mtext>E</mtext><msup><mrow></mrow><mo>*</mo></msup><mover><mo stretchy="true" symmetric="false">→</mo><mrow><mtext>a</mtext><mo>.</mo><mtext>s</mtext><mo>.</mo></mrow></mover><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>X</mi><mo>, </mo><mi>Y</mi></mrow></msub><mo stretchy="false"> (</mo><mtext>m</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>Y</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>, 加入需要获取泛化误差的上界, 不妨转化为求<i>P</i><sub><i>X</i>, <i>Y</i></sub> (mr (<i>X</i>, <i>Y</i>) &lt;0) 的上界。运用切比雪夫不等式可得:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>X</mi><mo>, </mo><mi>Y</mi></mrow></msub><mo stretchy="false"> (</mo><mtext>m</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>Y</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>&lt;</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>X</mi><mo>, </mo><mi>Y</mi></mrow></msub><mo stretchy="false"> (</mo><mrow><mo>|</mo><mrow><mtext>m</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>Y</mi><mo stretchy="false">) </mo><mo>-</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>X</mi><mo>, </mo><mi>Y</mi></mrow></msub><mtext>m</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>Y</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><mo>&gt;</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>X</mi><mo>, </mo><mi>Y</mi></mrow></msub><mtext>m</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>Y</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>≤</mo><mfrac><mrow><mi>var</mi><msub><mrow></mrow><mrow><mi>X</mi><mo>, </mo><mi>Y</mi></mrow></msub><mo stretchy="false"> (</mo><mtext>m</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>Y</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>X</mi><mo>, </mo><mi>Y</mi></mrow></msub><mtext>m</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>Y</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">同时定义随机森林中单一决策树分类的强度为<i>s</i>、各个决策树的相关性<mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>ρ</mi><mo>¯</mo></mover></math></mathml>为:</p>
                </div>
                <div class="p1">
                    <p id="62"><i>s</i>=<i>E</i><sub><i>X</i>, <i>Y</i></sub>mr (<i>X</i>, <i>Y</i>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>ρ</mi><mo>¯</mo></mover><mo>=</mo><mfrac><mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>θ</mi><mo>, </mo><mi>θ</mi><msup><mrow></mrow><mo>´</mo></msup></mrow></msub><mo stretchy="false"> (</mo><mi>ρ</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo>, </mo><mi>θ</mi><msup><mrow></mrow><mo>´</mo></msup><mo stretchy="false">) </mo><mtext>s</mtext><mtext>d</mtext><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mtext>s</mtext><mtext>d</mtext><mo stretchy="false"> (</mo><mi>θ</mi><msup><mrow></mrow><mo>´</mo></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>θ</mi><mo>, </mo><mi>θ</mi><msup><mrow></mrow><mo>´</mo></msup></mrow></msub><mo stretchy="false"> (</mo><mtext>s</mtext><mtext>d</mtext><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mtext>s</mtext><mtext>d</mtext><mo stretchy="false"> (</mo><mi>θ</mi><msup><mrow></mrow><mo>´</mo></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">结合式 (5) 、 (6) 可得, var<sub><i>X</i>, <i>Y</i></sub>mr<mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>Y</mi><mo stretchy="false">) </mo><mo>≤</mo><mover accent="true"><mi>ρ</mi><mo stretchy="true">¯</mo></mover><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>s</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="66">结合以上几个公式, 随机森林的泛化误差界可化简为式 (7) :</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Ρ</mtext><mtext>E</mtext><msup><mrow></mrow><mo>*</mo></msup><mo>≤</mo><mfrac><mrow><mover accent="true"><mi>ρ</mi><mo stretchy="true">¯</mo></mover><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>s</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo></mrow><mrow><mi>s</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">从这里能够得出, 随机森林的泛化误差界同单一决策树分类的强度<i>s</i>及各个决策树的相关性<mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>ρ</mi><mo stretchy="true">¯</mo></mover></mrow></math></mathml>有关。并且, <i>s</i>越大、<mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>ρ</mi><mo stretchy="true">¯</mo></mover></mrow></math></mathml>越小, 则相对应的泛化误差界就越小, 因此在需要优化随机森林方法的时候, 也应该从这两个方向入手。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71">2.袋外估计。</h4>
                <div class="p1">
                    <p id="72">随机森林有一个独特的优势, 不必对它使用交叉验证以得到误差的无偏估计。Bagging的主要思想是在包含有<i>m</i>个样本的数据内, 有放回地任意选择<i>m</i>个样本重组为新的数据集。由式 (8) 可知在原始数据集中大约存在36.8%的样本从未在新的数据集之中被抽取到, 这些未出现在采样数据集中的样本可以用来验证学习器的泛化性能, 称之为袋外估计<citation id="190" type="reference"><link href="158" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。Breiman也在论文中指出, 袋外估计是随机森林泛化能力的一个无偏估计。</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>lim</mi></mrow></mstyle><mrow><mi>m</mi><mo>→</mo><mo>+</mo><mi>∞</mi></mrow></munder><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mo stretchy="false">) </mo><msup><mrow></mrow><mi>m</mi></msup><mo>=</mo><mfrac><mn>1</mn><mi>e</mi></mfrac><mo>≈</mo><mn>0</mn><mo>.</mo><mn>3</mn><mn>6</mn><mn>8</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">袋外错误率 (obb error) 是袋外估计一种常见的表达形式, 具体计算方式为:首先对于每一个样本, 使用那些将它作为袋外样本的树 (大约为36.8%的树) 进行计算, 并且统计每棵树的分类情况;其次选择投票最多的类视为该样本的类别;最终使用分类错误的样本数量占所有样本数的比值当作随机森林的袋外错误率。</p>
                </div>
                <div class="p1">
                    <p id="75">为检验三种方法在训练集和测试集上的性能, 本文使用两种标准对三类不同方法的性能进行度量, 以测试误差为主的同时加入袋外错误率作为辅助评价标准。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag">三、自编码神经网络</h3>
                <div class="p1">
                    <p id="77">自编码器作为神经网络之中一类极为常见的算法, 在最近几年内获得普遍关注, 而且成功地运用于众多领域, 其在数据压缩、特征提取等方面有着十分出色的性能。自编码发展至今, 在传统自编码器的基础之上<citation id="191" type="reference"><link href="170" rel="bibliography" /><link href="174" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">11</a>]</sup></citation>, 已经有了很多的变种方法, 比如稀疏自编码器、降噪自编码器等<citation id="192" type="reference"><link href="178" rel="bibliography" /><link href="180" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78"> (<b>一) 传统自编码器</b></h4>
                <div class="p1">
                    <p id="79">传统自编码器一般包括编码阶段和解码阶段两个结构互相对称的阶段, 即在存在多个隐藏层的自编码器中编码阶段和解码阶段隐藏层的数量是相同的。图2是一种存在单隐藏层的自编码器, 这个自编码器输入一个六维向量, 经过编码阶段压缩为三维向量后, 再经过解码阶段还原为与输入向量近似相等的六维向量。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/TJLT201908005_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 自编码网络示意图 (1)" src="Detail/GetImg?filename=images/TJLT201908005_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>2 <b>自编码网络示意图</b><citation id="196" type="note"><link href="194" rel="footnote" /><sup> (1) </sup></citation><sup></sup>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJLT201908005_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="81">按照图2的结构, 传统自编码器的编码解码过程能够表现为以下公式:</p>
                </div>
                <div class="p1">
                    <p id="82"><i>h</i>=<i>σ</i><sub><i>e</i></sub> (<i>W</i><sub>1</sub><i>x</i>+<i>b</i><sub>1</sub>)      (9) </p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>=</mo><mi>σ</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>W</mi><msub><mrow></mrow><mn>2</mn></msub><mi>h</mi><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中, <i>x</i>为含有<i>M</i>个特征的一个样本, <i>W</i><sub>1</sub>, <i>b</i><sub>1</sub>为编码过程的权重项和偏置项, <i>W</i><sub>2</sub>, <i>b</i><sub>2</sub>为解码过程的权重项和偏置项, <i>σ</i>为一类非线性变化, 通常使用的函数是sigmoid、tanh、relu等。解码过程的<i>σ</i>可以是编码过程相同的非线性变化或者仿射变化。</p>
                </div>
                <div class="p1">
                    <p id="85">传统自编码器的损失函数一般表达为<mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>x</mi><mo>^</mo></mover></math></mathml>与<i>x</i>之间的误差, 通常使用式 (11) 给出的均方误差进行度量。一般而言传统自编码器的损失函数能够表达为式 (12) 。最终目的是令损失函数最小化。</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>|</mo><mrow><mi>x</mi><mo>-</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>J</mi><mo stretchy="false"> (</mo><mi>W</mi><mo>, </mo><mi>b</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><mo>∑</mo><mo stretchy="false"> (</mo></mstyle><mi>L</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">由于在编码以及解码的过程当中, 自编码器未涉及关于数据的标签信息, 因此自编码器是一类无监督的神经网络。对于传统的自编码器, 隐藏层有三种互不相同的表现:压缩结构、稀疏结构和等维结构。这三种结构通常是按照隐藏层之中的神经元的个数进行划分的, 当原始数据的维数大于隐藏层神经元个数时称为压缩结构;反之称为稀疏结构;若原始数据的维数等于隐藏层神经元个数时, 则称之为等维结构。</p>
                </div>
                <div class="p1">
                    <p id="89">压缩结构能够对原始数据进行压缩, 若原始数据之间隐含某些有联系的结构, 自编码神经网络就能够发掘出它们之间的相关性, 并在输出层重构原始数据。而稀疏结构的作用是在隐藏层神经元之中加入一定的稀疏性限制, 使得重构之后的数据具有优秀的稀疏性;同时, 能够对原始数据进行干扰, 人为施加噪声使得网络更加具有鲁棒性。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90"> (<b>二) 改进自编码器</b></h4>
                <h4 class="anchor-tag" id="91" name="91">1.稀疏自编码器。</h4>
                <div class="p1">
                    <p id="92">当自编码器之中的隐藏层节点比输入的原始节点数目多时, 应当人为地对隐藏层实施一定的约束。</p>
                </div>
                <div class="p1">
                    <p id="93">Andrew Ng认为高维并且稀疏的表达是优秀的, 因此本文在损失函数中加入惩罚项将隐藏层的节点进行稀疏性限制。为了实现限制效果一般使用KL散度或者L1, L2正则化迫使神经元稀疏化, 并且将其作为惩罚项加入损失函数之中。</p>
                </div>
                <div class="p1">
                    <p id="94">如果通过稀疏化后的神经元仍能完美重建原始数据, 那么说明这些稀疏表达包含原始数据的大部分特征, 能够看作是对原始数据的一种简单表达, 这样使得模型的性能获得优异的改善。</p>
                </div>
                <div class="p1">
                    <p id="95">本文使用加入L1, L2惩罚项的损失函数对数据进行稀疏性处理并防止数据的过拟合。在这里稀疏自编码的损失函数能够近似地改写为以下公式:</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>J</mi><msub><mrow></mrow><mrow><mi>S</mi><mi>C</mi></mrow></msub><mo stretchy="false"> (</mo><mi>W</mi><mo>, </mo><mi>b</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><mo>∑</mo><mo stretchy="false"> (</mo></mstyle><mi>L</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mrow><mo>|</mo><mi>W</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mrow><mo>|</mo><mi>W</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">其中L1范数作为稀疏惩罚项使数据能够稀疏表达, 而L2范数的加入能够避免数据产生过拟合现象。</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98">2.降噪自编码器。</h4>
                <div class="p1">
                    <p id="99">降噪自编码的提出是受到了人类对被遮挡的事物仍然能够进行较准确的判别这一现象启发。假如某个模型拥有优秀的鲁棒性, 那么部分缺失的数据在隐藏层上的表达应该同无缺失的数据几乎相等。</p>
                </div>
                <div class="p1">
                    <p id="100">因此, 降噪自编码的主要思想就是人为地在原始数据上施加部分噪音, 将变化后的数据输入自编码器中, 令其尽可能重构一个与原始数据相同的输出。关键点在于对原始数据施加噪音, 通常具有两类方式。第一类是加入一个较小的随机扰动, 即<mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>x</mi><mo>˜</mo></mover><mo>=</mo><mi>x</mi><mo>+</mo><mi>ε</mi><mo>, </mo><mi>ε</mi><mo>∼</mo><mi>Ν</mi><mo stretchy="false"> (</mo><mn>0</mn><mo>, </mo><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup><mi>Ι</mi><mo stretchy="false">) </mo></mrow></math></mathml>, 另外的一种方法就是随机把输入向量中的一部分分量依概率<i>p</i>赋值为零 (式 (14) , <i>M</i>为原始特征总数, <i>X</i><sub><i>j</i></sub>为<i>X</i>的第<i>j</i>个特征) , 这样同样使原始数据中加入了噪声。</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>r</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∼</mo><mtext>B</mtext><mtext>e</mtext><mtext>r</mtext><mtext>n</mtext><mtext>o</mtext><mtext>u</mtext><mtext>l</mtext><mtext>l</mtext><mtext>i</mtext><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo></mtd><mtd columnalign="left"><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Μ</mi></mtd></mtr><mtr><mtd columnalign="left"><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mi>X</mi><msub><mrow></mrow><mi>j</mi></msub><mo>×</mo><mi>r</mi><msub><mrow></mrow><mi>j</mi></msub></mtd><mtd columnalign="left"><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Μ</mi></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">降噪自编码器通过人为地在原始数据之中施加少量噪音, 避免了隐藏层神经元在学习过程中学到没有意义的恒等函数, 同时得以学习到一个更具有鲁棒性的表达。</p>
                </div>
                <h3 id="104" name="104" class="anchor-tag">四、基于自编码特征提取的随机森林模型实验</h3>
                <h4 class="anchor-tag" id="105" name="105"> (<b>一) 实验目的</b></h4>
                <div class="p1">
                    <p id="106">为改善随机森林的性能, 本文从对原始数据特征的重构入手, 结合深度学习中应用较为广泛的自编码器对数据进行深层次改变。在改变数据的特征结构的同时进行特征提取, 将特征提取后的数据进行随机森林预测。</p>
                </div>
                <div class="p1">
                    <p id="107">因为稀疏自编码器具有良好的泛化性能以及稀疏性, 降噪自编码拥有较为优秀的鲁棒性, 因此本文采用的模型结合了稀疏自编码与降噪自编码的部分优点, 利用稀疏降噪自编码器对原始数据进行重构, 使得重构后的数据具有优秀的性能, 采用了大于或近似于原始数据维数的隐藏层神经元个数。</p>
                </div>
                <div class="p1">
                    <p id="108">当模型运用较大的隐藏层神经元时, 可以令其具有更加优异的表达能力, 然而于此同时会产生过拟合的状况, 因此要进行一定的限制, 通常采用添加约束条件的方法。经过限制的多隐藏层神经元模型和较少隐藏层神经元的模型相比仍然拥有很强的表达能力, 但是又避免了过拟合的情况。并且, 由于不为零的权值较少, 因此神经网络只能够使用较少的神经节点以学习到神经网络的总体信息, 这样会使重构数据变得稀疏并且被学习的特征仍旧拥有很强的代表性。稀疏降噪自编码的损失函数可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>r</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∼</mo><mtext>B</mtext><mtext>e</mtext><mtext>r</mtext><mtext>n</mtext><mtext>o</mtext><mtext>u</mtext><mtext>l</mtext><mtext>l</mtext><mtext>i</mtext><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Μ</mi></mtd></mtr><mtr><mtd><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mi>X</mi><msub><mrow></mrow><mi>j</mi></msub><mo>×</mo><mi>r</mi><msub><mrow></mrow><mi>j</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mspace width="0.25em" /><mtext> </mtext><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Μ</mi></mtd></mtr><mtr><mtd><mi>J</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>D</mtext><mtext>A</mtext><mtext>E</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>W</mi><mo>, </mo><mi>b</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><mo>∑</mo><mo stretchy="false"> (</mo></mstyle><mi>L</mi><mo stretchy="false"> (</mo><mover accent="true"><mi>x</mi><mo>˜</mo></mover><mo>, </mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mrow><mrow><mo>|</mo><mi>W</mi><mo>|</mo></mrow></mrow><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mrow><mrow><mo>|</mo><mi>W</mi><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">为了验证稀疏降噪自编码器处理后的随机森林算法 (以下简称EN-RF) 的可行性和适用性, 本文收集了三组经典的分类数据集。为表现本文算法的优势, 同时构建随机森林算法 (RF) 及主成分分析处理后的随机森林算法 (以下简称PCA-RF) <citation id="193" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 同使用稀疏降噪自编码器处理后的随机森林模型进行对比。</p>
                </div>
                <div class="p1">
                    <p id="111">如果基于稀疏降噪自编码器的随机森林模型在分类预测效果上优于其他两种算法, 则说明此方法在分类问题中是适合并有效的, 能够用于实证研究。</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112"> (<b>二) 实验数据及主要过程</b></h4>
                <h4 class="anchor-tag" id="113" name="113">1.实验数据。</h4>
                <div class="p1">
                    <p id="114">本文选用的三种经典数据集均来自UCI, 分别为banknote authentication Data Set (以下简称BA) 、Connectionist Bench Data Set (以下简称CB) 以及经典的MNIST数据集, 其中MNIST数据集是一组常用的多分类样本量较大的稀疏数据集, 其余两组数据集为小样本量的二分类数据集。</p>
                </div>
                <div class="p1">
                    <p id="115">在实验中将三者的训练集进行随机分割, 每次测试将其中20%的样本数据视为测试集, 其余的视为训练集, 重复进行100次实验。数据集部分重要信息见表1。</p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表</b>1 <b>实验数据信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td><br />数据集</td><td>样本总数</td><td>特征数</td><td>类别数</td></tr><tr><td><br />BA</td><td>1 372</td><td>4</td><td>2</td></tr><tr><td><br />CB</td><td>208</td><td>60</td><td>2</td></tr><tr><td><br />MNIST</td><td>42 000</td><td>784</td><td>10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="117">这三类数据集包括了二分类及多分类问题、普通数据集及稀疏数据集、样本量较少的数据集及样本量较大的数据集。三类数据集包含了常见的分类情况, 本文利用稀疏降噪自编码随机森林对这三类数据集进行分类, 同时加入随机森林以及主成分随机森林进行结果对比。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118">2.主要过程。</h4>
                <div class="p1">
                    <p id="119">本文利用Python语言, 对三类经典数据集分别应用普通随机森林、主成分随机森林作为参照模型, 对稀疏降噪自编码随机森林进行分类效果比较。</p>
                </div>
                <div class="p1">
                    <p id="120">为获得合适的随机森林参数, 本文采取了交叉验证的实验方法对随机森林参数进行选取。在每一个数据集上, 分别对原始数据集、主成分降维后的数据以及自编码重构后的数据进行交叉验证, 在所获取的最佳属性中选取较为合适的属性作为该数据集随机森林的最优属性。</p>
                </div>
                <div class="p1">
                    <p id="121">对于主成分分析中的特征选择, 对所有数据集统一按照累计方差贡献率提取因子。将因子按方差贡献率由大到小排列, 且累计方差贡献率大于98%。对于稀疏降噪自编码的特征选择, 本文选用了稀疏降噪自编码器对原始数据进行一定的重构, 部分参数见表2。</p>
                </div>
                <div class="area_img" id="122">
                    <p class="img_tit"><b>表</b>2 <b>部分参数信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="122" border="1"><tr><td colspan="2"><br /> 参数</td><td colspan="3">数据集</td></tr><tr><td><br />算法</td><td>参数名称</td><td>BA</td><td>CB</td><td>MNIST</td></tr><tr><td><br />自编码 </td><td>重构特征数</td><td>10</td><td>100</td><td>800</td></tr><tr><td><br /></td><td>神经元失活比</td><td>0.1</td><td>0.6</td><td>0.5</td></tr><tr><td><br /></td><td>L1正则化系数</td><td>10<sup>-6</sup></td><td>10<sup>-7</sup></td><td>10<sup>-9</sup></td></tr><tr><td><br /></td><td>L2正则化系数</td><td>10<sup>-4</sup></td><td>10<sup>-7</sup></td><td>10<sup>-9</sup></td></tr><tr><td><br />随机森林</td><td>决策树个数</td><td>70</td><td>30</td><td>800</td></tr><tr><td><br /></td><td>决策树最大深度</td><td>7</td><td>10</td><td>120</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="123" name="123"> (<b>三) 实验结果及分析</b></h4>
                <h4 class="anchor-tag" id="124" name="124">1.实验结果。</h4>
                <div class="p1">
                    <p id="125">本文使用所选的数据集重复100次随机实验, 每次实验过程中均将原始数据集依据固定的比例随机分割成为训练集及测试集。为直观显示结果, 本文将100次实验所得测试误差以及obb error进行图像拟合。将结果进行展示, 见图3～5。</p>
                </div>
                <div class="area_img" id="126">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/TJLT201908005_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 钞票数据集实验结果" src="Detail/GetImg?filename=images/TJLT201908005_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>3 <b>钞票数据集实验结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJLT201908005_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/TJLT201908005_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 声纳数据集实验结果" src="Detail/GetImg?filename=images/TJLT201908005_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>4 <b>声纳数据集实验结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJLT201908005_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/TJLT201908005_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 MNIST数据集实验结果" src="Detail/GetImg?filename=images/TJLT201908005_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>5 MNIST<b>数据集实验结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJLT201908005_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="129">本文将100次实验数据的测试误差取平均值, 分析各方法的优劣。实验结果见表3。</p>
                </div>
                <div class="area_img" id="130">
                    <p class="img_tit"><b>表</b>3 <b>测试误差</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">单位:%</p>
                    <table id="130" border="1"><tr><td><br />数据集</td><td>RF</td><td>PCARF</td><td>ENRF</td></tr><tr><td><br />BA</td><td>0.92</td><td>0.48</td><td>0.11</td></tr><tr><td><br />CB</td><td>19.09</td><td>22.52</td><td>16.16</td></tr><tr><td><br />MNIST</td><td>3.57</td><td>5.76</td><td>3.65</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="131" name="131">2.结果分析。</h4>
                <div class="p1">
                    <p id="132">通过以上实验结果的展示, 能够看出无论选取测试误差还是obb error值作为评判标准, 在维数较少的数据集中, 本文的算法性能优势更为明显, 而在MNIST数据集上, 该算法同未经数据处理后的随机森林算法表现接近。尽管普通随机森林在BA数据集上的分类效果已经足够好了, 但本文采用的算法依旧具有少量的改善效果。并且, 由于本文所用到的参数较多, 导致部分参数未调整至最好, 因此算法依然具有一定的进步空间。</p>
                </div>
                <div class="p1">
                    <p id="133">比较三种方法, 普通随机森林在高维或低维数据中均有不错的表现。运用主成分分析进行特征选择的随机森林在实验中表现不理想, 且由于主成分方法的限制令其无法运用于更高维的数据中。相对而言, 运用自编码进行特征选择的随机森林在高维或低维数据集上均能取得理想的结果且综合优于随机森林算法, 因此认为自编码随机森林是可行的, 同时其适用性较为广泛。</p>
                </div>
                <div class="p1">
                    <p id="134">自编码器作为神经网络之中一类极为常见的深度学习算法, 虽然进行稀疏性处理可以消除部分过拟合的影响, 但不可避免地有时仍然存在过拟合等问题。这主要是由于在进行自编码时数据无监督, 导致了自编码产生较大偏差, 具体表现为在参数调整的过程中出现了过低的准确率, 而这种过低的准确率是能够排除的。随着稀疏自编码调参过程, 准确率过低问题出现频率有着明显降低, 但如何将这种问题解决仍需要做进一步探讨。</p>
                </div>
                <h3 id="135" name="135" class="anchor-tag">五、结 论</h3>
                <div class="p1">
                    <p id="136">面对如今多样的数据集, 本文将随机森林中原始数据的特征选择做为主要思路, 提出了应用稀疏降噪自编码器将原始数据进行重构的随机森林模型。该模型采用了改进的稀疏降噪自编码器对原始数据进行等维或稀疏重构, 由于该自编码器泛化能力较强且拥有优秀的鲁棒性, 重构后的数据相较于原始数据更为稀疏且可以展示原始数据之间的内在联系, 较好地减小了各个属性之间的相关性, 使得数据更容易被随机森林学习。</p>
                </div>
                <div class="p1">
                    <p id="137">本文通过实际的数据集, 说明了稀疏降噪自编码重构的随机森林方法的有效性及优越性。其无论在小样本数据集还是大样本数据集中均存在良好的表现, 证明稀疏降噪自编码重构的随机森林方法具有较强的适用性。</p>
                </div>
                <div class="p1">
                    <p id="138">本文方法仍存在许多应当进一步改良的地方:第一, 自编码器是深度学习中的一类重要分支, 虽然在处理过程中加入惩罚项及噪音防止过拟合, 但仍不可避免地存在部分过拟合的情况。虽然随着稀疏自编码调参过程的进行, 准确率过低的问题出现频率有着明显降低, 但如何将这种问题解决仍需要做进一步探讨。第二, 虽然随机森林的调参工作是由计算机完成, 但自编码调参无法由计算机完成, 需手动调参。这主要是由于自编码作为一类经典的无监督学习方法, 无法对参数好坏做出评价。因此, 应当更加密切结合深度学习中的知识, 设计一个有监督的学习, 使总体的大部分调参工作能够由计算机完成, 并且有监督的学习能够更好地适应数据本身的情况。</p>
                </div>
                <div class="p1">
                    <p id="139">在以后的研究中, 我们不妨考虑以下几个方向的发展:通过改进自编码器的模型, 使其能够对某些特殊的数据具有更加优秀的特征提取能力;随机森林是一类强大的并行算法, 自编码同样能够进行并行运算。为应对当今的大数据挑战, 将随机森林同自编码更有效地结合起来, 能够在算法上大幅度提高效率和性能, 令该算法得以运用到大数据之中;自编码作为一类经典的无监督学习方法, 应当结合深度学习中的内容, 从而构造一个有监督的方法, 令自编码学习到更多有用的数据间的内在联系, 使得算法精度得到提高。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="154">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001337810&amp;v=MjQ3MTh4Q2JPb1BZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZpSGtXNy9QSWxZPU5qN0Jhck80SHRITnJJ&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Quinlan J R.Induction of Decision Trees[J].Machine Learning, 1986, 1 (1) .
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339251&amp;v=MjM3MTA5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlIa1c3L1BJbFk9Tmo3QmFyTzRIdEhOckl4TVp1NE9ZM2s1ekJkaDRq&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Cortes C, Vapnik V.Support-vector Networks[J].Machine Learning, 1995, 20 (3) .
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339482&amp;v=MDI0MDV4b3hjTUg3UjdxZForWnVGaUhrVzcvUElsWT1OajdCYXJPNEh0SE5ySXhNWU9NTlkzazV6QmRoNGo5OVNYcVJy&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Breiman L.Bagging Predictors[J].Machine Learning, 1996, 24 (2) .
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601258112&amp;v=MjQwOTJEWDA3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmJKS0Y0VGFCbz1OaWZPZmJLN0h0RE5xWTlFWnU0SA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Freund Y, Schapire R E.A Decision-Theoretic Generalization of On-line Learning and an Application to Boosting[J].Journal of Computer and System Sciences, 1997, 55 (1) .
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340271&amp;v=MjE3NjNCYXJPNEh0SE5ySXRGWnV3T1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlIa1c3L1BJbFk9Tmo3&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Breiman L.Random Forests[J].Machine Learning, 2001, 45 (1) .
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JCJI201301001&amp;v=MTYwMjU3RzRIOUxNcm85RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVabkZpRGxWTDNBTHk3Qlo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 董师师, 黄哲学.随机森林理论浅析[J].集成技术, 2013, 2 (1) .
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=OXXT201801009&amp;v=MjAxMzdCdEdGckNVUjdxZlp1Wm5GaURsVkwzQUtqWFRlckc0SDluTXJvOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 王奕森, 夏树涛.集成学习之随机森林算法综述[J].信息通信技术, 2018, 12 (1) .
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJLT201103007&amp;v=MjA3MjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVabkZpRGxWTDNBTVNmSGVyRzRIOURNckk5Rlk0UUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 方匡南, 吴见彬, 朱建平, 等.随机森林方法研究综述[J].统计与信息论坛, 2011, 26 (3) .
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003264827&amp;v=MTAxOTRrSVkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlIa1c3L1BJbFk9Tmo3QmFyTzRIdEhQcllsQmJP&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Bourlard H, Kamp Y.Auto-association by Multilayer Perceptrons and Singular Value Decomposition[J].Biological Cybernetics, 1988, 59 (4) .
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYY201608001&amp;v=MjcyNDZHNEg5Zk1wNDlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpuRmlEbFZMM0FQVG5TZDc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 殷瑞刚, 魏帅, 李晗, 等.深度学习中的无监督学习方法综述[J].计算机系统应用, 2016, 25 (8) .
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning representations by back-propagating errors">

                                <b>[11]</b> Rumelhart D E, Hinton G E, Williams R J.Learning Representations by Back-propagating Errors[J].Nature, 1986, 323 (6088) .
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The random subspace method for constructing decision forests">

                                <b>[12]</b> Ho T K.The Random Subspace Method for Constructing Decision Forests[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1998, 20 (8) .
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501611081&amp;v=Mjc1NTVFWXVvT0RIUTRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyYkpLRjRUYUJvPU5pZk9mYks3SHRETnFvOQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Olshausen B A, Field D J.Sparse Coding of Sensory Inputs[J].Current Opinion in Neurobiology, 2004, 14 (4) .
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extracting and composing robust features with denoising autoencoders">

                                <b>[14]</b> Vincent P, Larochelle H, Bengio Y, et al.Extracting and Composing Robust Features with Denoising Autoencoders[C].In Proceedings of the 25th International Conference on Machine learning-ICML’08, 2008.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Fast On-line Algorithm for PCA and Its Convergence Characteristics">

                                <b>[15]</b> Rao Y, Principe J.A Fast, On-line Algorithm for PCA and Its Convergence Characteristics[J].IEEE, Transactions on Neural Network, 2000, 4 (2) .
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
            <div class="reference anchor-tag" id="a_footnote">
 <h3>注释</h3>
                    <p>
                        <span id="194" href="javascript:void(0)">
                            <b>1</b> 图片引用自https://blog.csdn.net/on2way/article/details/50095081。
                        </span>
                    </p>
            </div>
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="TJLT201908005" />
        <input id="dpi" type="hidden" value="800" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJLT201908005&amp;v=MDI5OTJGaURsVkwzQU1TZkhlckc0SDlqTXA0OUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm4=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=vAscMyvIPP9NePnbGPkqJ0A5tHOvnzl65tGuRDsf9xg1" rel="stylesheet"/>

</body>
</html>

