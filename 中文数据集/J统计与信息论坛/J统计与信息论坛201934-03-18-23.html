<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637140137355756250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dTJLT201903003%26RESULT%3d1%26SIGN%3d38QyUwakZu3zAtHx%252bQuBQKoudVM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=TJLT201903003&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=TJLT201903003&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJLT201903003&amp;v=MjU4NDhadVpvRnk3a1dydktNU2ZIZXJHNEg5ak1ySTlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="一、引 言 ">一、引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#39" data-title="二、最大匹配算法和n-gram语言模型 ">二、最大匹配算法和n-gram语言模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title=" (&lt;b&gt;一) 基于规则的最大匹配算法&lt;/b&gt;"> (<b>一) 基于规则的最大匹配算法</b></a></li>
                                                <li><a href="#50" data-title=" (&lt;b&gt;二) 基于统计的&lt;/b&gt;n-gram&lt;b&gt;语言模型&lt;/b&gt;"> (<b>二) 基于统计的</b>n-gram<b>语言模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#59" data-title="三、基于最大匹配算法的似然导向中文分词方法 ">三、基于最大匹配算法的似然导向中文分词方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title=" (&lt;b&gt;一) 预处理阶段&lt;/b&gt;"> (<b>一) 预处理阶段</b></a></li>
                                                <li><a href="#82" data-title=" (&lt;b&gt;二) 分词阶段&lt;/b&gt;"> (<b>二) 分词阶段</b></a></li>
                                                <li><a href="#101" data-title=" (&lt;b&gt;三) 判定阶段&lt;/b&gt;"> (<b>三) 判定阶段</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#111" data-title="四、 实 验 ">四、 实 验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#112" data-title=" (&lt;b&gt;一) 实验语料&lt;/b&gt;"> (<b>一) 实验语料</b></a></li>
                                                <li><a href="#114" data-title=" (&lt;b&gt;二) 实验方法&lt;/b&gt;"> (<b>二) 实验方法</b></a></li>
                                                <li><a href="#124" data-title=" (&lt;b&gt;三) 评价指标&lt;/b&gt;"> (<b>三) 评价指标</b></a></li>
                                                <li><a href="#126" data-title=" (&lt;b&gt;四) 实验结果与分析&lt;/b&gt;"> (<b>四) 实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#136" data-title="五、结 语 ">五、结 语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="&lt;b&gt;图&lt;/b&gt;1 &lt;b&gt;基于最大匹配的似然导向中文分词方法流程图&lt;/b&gt;"><b>图</b>1 <b>基于最大匹配的似然导向中文分词方法流程图</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;正向训练集共现词典&lt;/b&gt;&lt;i&gt;D&lt;/i&gt;&lt;b&gt;表&lt;/b&gt;"><b>表</b>1 <b>正向训练集共现词典</b><i>D</i><b>表</b></a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;逆向训练集共现词典&lt;/b&gt;&lt;i&gt;D&lt;/i&gt;&lt;b&gt;表&lt;/b&gt;"><b>表</b>2 <b>逆向训练集共现词典</b><i>D</i><b>表</b></a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同方法的准确率、召回率&lt;/b&gt;、&lt;i&gt;F&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;b&gt;值比较表&lt;/b&gt;"><b>表</b>3 <b>不同方法的准确率、召回率</b>、<i>F</i><sub>1</sub><b>值比较表</b></a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;图&lt;/b&gt;2 &lt;b&gt;错误分词结果对比词云图&lt;/b&gt;"><b>图</b>2 <b>错误分词结果对比词云图</b></a></li>
                                                <li><a href="#134" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;不同方法的分词效率对比表&lt;/b&gt;"><b>表</b>4 <b>不同方法的分词效率对比表</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 黄昌宁, 赵海.中文分词十年回顾[J].中文信息学报, 2007 (3) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MESS200703001&amp;v=MjYyNDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N2tXcnZOS0NqWWZiRzRIdGJNckk5RlpZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         黄昌宁, 赵海.中文分词十年回顾[J].中文信息学报, 2007 (3) .
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 王克岭, 张甜溪, 段玲.微信公众号软文内部点赞影响因素研究[J].西安财经学院学报, 2018 (2) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SXGY201802010&amp;v=MjA1NDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3a1dydk5OalhNZDdHNEg5bk1yWTlFWklRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         王克岭, 张甜溪, 段玲.微信公众号软文内部点赞影响因素研究[J].西安财经学院学报, 2018 (2) .
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 朱建平.网络舆情分析的统计思维[J].统计与信息论坛, 2016 (11) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJLT201611030&amp;v=MDc3MDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N2tXcnZOTVNmSGVyRzRIOWZOcm85R1pJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         朱建平.网络舆情分析的统计思维[J].统计与信息论坛, 2016 (11) .
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" 刘伟.“一带一路”倡议下国内外新闻舆情及其演化分析[J].统计与信息论坛, 2018 (6) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJLT201806006&amp;v=MjA2ODNVUjdxZlp1Wm9GeTdrV3J2Tk1TZkhlckc0SDluTXFZOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         刘伟.“一带一路”倡议下国内外新闻舆情及其演化分析[J].统计与信息论坛, 2018 (6) .
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 吴春颖, 王士同.基于二元语法的N-最大概率中文粗分模型[J].计算机应用, 2007 (12) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200712007&amp;v=Mjc1NjY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N2tXcnZOTHo3QmQ3RzRIdGJOclk5Rlk0UUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         吴春颖, 王士同.基于二元语法的N-最大概率中文粗分模型[J].计算机应用, 2007 (12) .
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" 张玉茹.中文分词算法之最大匹配算法的研究[J].现代计算机:专业版, 2011 (19) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDJS201119008&amp;v=MTEwMjl1Wm9GeTdrV3J2TlBTbkJmYkc0SDlETnBvOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         张玉茹.中文分词算法之最大匹配算法的研究[J].现代计算机:专业版, 2011 (19) .
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" 莫建文, 郑阳, 首照宇, 等.改进的基于词典的中文分词方法[J].计算机工程与设计, 2013 (5) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201305056&amp;v=MDA0ODBOaWZZWkxHNEg5TE1xbzlBWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3a1dydk4=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         莫建文, 郑阳, 首照宇, 等.改进的基于词典的中文分词方法[J].计算机工程与设计, 2013 (5) .
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" 张劲松, 袁健.回溯正向匹配中文分词算法[J].计算机工程与应用, 2009 (22) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG200922045&amp;v=MTIwMDl0ak9yWTlCWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3a1dydk5MejdNYWJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         张劲松, 袁健.回溯正向匹配中文分词算法[J].计算机工程与应用, 2009 (22) .
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" 周俊, 郑中华, 张炜.基于改进最大匹配算法的中文分词粗分方法[J].计算机工程与应用, 2014 (2) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201402027&amp;v=MDA1NzE0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeTdrV3J2Tkx6N01hYkc0SDlYTXJZOUhZNFFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         周俊, 郑中华, 张炜.基于改进最大匹配算法的中文分词粗分方法[J].计算机工程与应用, 2014 (2) .
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Nianwen Xue.Chinese Word Segmentation as Character Tagging[J].The Association for Computational Linguistics and Chinese Language Processiong, 2003 (1) ." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Chinese Word Segmentation as Character Tagging">
                                        <b>[10]</b>
                                         Nianwen Xue.Chinese Word Segmentation as Character Tagging[J].The Association for Computational Linguistics and Chinese Language Processiong, 2003 (1) .
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" 王昊, 李思舒, 邓三鸿.基于N-Gram的文本语种识别研究[J].现代图书情报技术, 2013 (4) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDTQ201304013&amp;v=Mjc4ODl1Wm9GeTdrV3J2TlBTbmZmN0c0SDlMTXE0OUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         王昊, 李思舒, 邓三鸿.基于N-Gram的文本语种识别研究[J].现代图书情报技术, 2013 (4) .
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 翟凤文, 赫枫龄, 左万利.字典与统计相结合的中文分词方法[J].小型微型计算机系统, 2006 (9) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX200609038&amp;v=MjUzNDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3a1dydk5QVFhjZHJHNEh0Zk1wbzlHYklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         翟凤文, 赫枫龄, 左万利.字典与统计相结合的中文分词方法[J].小型微型计算机系统, 2006 (9) .
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Zhao Hai, Huang Chang Ning, Li Mu.An Improved Chinese Word Segmentation System with Conditional Random Field[C].Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, 2006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Improved Chinese Word Segmentation System with Conditional Random Field">
                                        <b>[13]</b>
                                         Zhao Hai, Huang Chang Ning, Li Mu.An Improved Chinese Word Segmentation System with Conditional Random Field[C].Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, 2006.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" 张梅山, 邓知龙, 车万翔, 等.统计与词典相结合的领域自适应中文分词[J].中文信息学报, 2012 (2) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MESS201202001&amp;v=MjkxMTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeTdrV3J2TktDallmYkc0SDlQTXJZOUZaWVE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         张梅山, 邓知龙, 车万翔, 等.统计与词典相结合的领域自适应中文分词[J].中文信息学报, 2012 (2) .
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" 宋彦, 蔡东风, 张桂平, 等.一种基于字词联合解码的中文分词方法[J].软件学报, 2009 (9) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200909009&amp;v=MzA4MzlOTnlmVGJMRzRIdGpNcG85RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N2tXcnY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         宋彦, 蔡东风, 张桂平, 等.一种基于字词联合解码的中文分词方法[J].软件学报, 2009 (9) .
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=TJLT" target="_blank">统计与信息论坛</a>
                2019,34(03),18-23             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于最大匹配算法的似然导向中文分词方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E8%B4%B5%E5%86%9B&amp;code=08882797&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨贵军</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E9%9B%AA&amp;code=27644960&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐雪</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%87%A4%E4%B8%BD%E6%B4%B2&amp;code=37845375&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">凤丽洲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E7%8E%89%E6%85%A7&amp;code=41262930&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐玉慧</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A9%E6%B4%A5%E8%B4%A2%E7%BB%8F%E5%A4%A7%E5%AD%A6%E7%BB%9F%E8%AE%A1%E5%AD%A6%E9%99%A2&amp;code=0111364&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">天津财经大学统计学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E8%81%94%E5%90%88%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8%E9%9D%92%E5%B2%9B%E5%88%86%E5%85%AC%E5%8F%B8&amp;code=0144735&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国联合网络通信有限公司青岛分公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>综合基于规则的分词方法与基于统计的分词方法在分词效果上的优势, 提出一种基于最大匹配算法的似然导向中文分词方法。新方法在分词阶段, 将训练数据的统计信息融入到基于规则的最大匹配分词算法中, 并根据共现性自动识别后续词;在判定阶段, 利用具有马尔可夫性的n-gram模型对分词阶段获得的多组分词模式进行判定, 并基于最大似然原理确定最优的分词模式以提高分词准确率。实验结果表明, 新方法有效提高了分词准确率和召回率, 适用于中文文本信息挖掘。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中文分词;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=n-gram&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">n-gram;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最大匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BC%BC%E7%84%B6%E5%AF%BC%E5%90%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">似然导向;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    杨贵军, 男, 黑龙江哈尔滨人, 统计学博士, 教授, 研究方向:应用统计;;
                                </span>
                                <span>
                                    徐雪, 女, 山东菏泽人, 博士生, 研究方向:应用统计;;
                                </span>
                                <span>
                                    凤丽洲, 女, 吉林四平人, 博士, 讲师, 研究方向:社会网络、数据挖掘;;
                                </span>
                                <span>
                                    徐玉慧, 女, 山东临沂人, 硕士生, 研究方向:应用统计。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-06-29</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家社会科学基金青年项目《社交媒体中敏感信息可信度评估方法研究》 (18CTJ008);</span>
                                <span>全国统计科学研究一般项目《基于多源数据融合的人民群众获得感和幸福感测度研究》 (2018LY50);</span>
                                <span>全国统计科学研究重点项目《Web社会网络中敏感信息识别及突发事件预测研究》 (2017LZ05);</span>
                    </p>
            </div>
                    <h1><b>Likelihood Oriented Method for Chinese Word Segmentation based on Maximum Match Algorithm</b></h1>
                    <h2>
                    <span>YANG Gui-jun</span>
                    <span>XU Xue</span>
                    <span>FENG Li-zhou</span>
                    <span>XU Yu-hui</span>
            </h2>
                    <h2>
                    <span>School of Statistics, Tianjin University of Finance & Economics</span>
                    <span>QingDao Branch, China United Network Communications Limited</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A likelihood probability oriented Chinese word segmentation method based on maximum match algorithm is proposed in this paper, which combines the advantage of statistical methods and rule methods for Chinese word segmentation.In the word segmentation step, the statistical information is integrated into maximum match algorithm.Then the follow up words can be identified according to the cooccurrence to improve the efficiency of the word segmentation.In the decision step, in order to obtain the optimal word segmentation sequence, n-gram model with Markov property is used for likelihood probability oriented decision.Experimental results indicate that new approach generally obtains higher accuracy and has good adaption in the domain of Chinese information processing.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Chinese%20word%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Chinese word segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=n-gram&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">n-gram;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=maximum%20match&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">maximum match;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=likelihood%20oriented&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">likelihood oriented;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-06-29</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag">一、引 言</h3>
                <div class="p1">
                    <p id="34">近年来, 随着互联网技术的快速发展产生了大量的文本数据, 其中蕴含着丰富的信息为人们日常生活的各个方面提供决策支持。中文分词作为中文信息处理的基础环节, 分词准确率的高低将直接影响中文文本数据挖掘的效果<citation id="139" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。中文分词技术具有非常广阔的应用前景, 已得到广泛关注。因自然语言的模糊性和复杂性加大了中文分词的难度, 而高准确度的中文分词方法对舆情分析<citation id="140" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、信息推荐以及市场营销等领域的中文文本数据挖掘, 具有重要意义<citation id="141" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="35">目前, 常用的中文分词方法可以划分为两类:基于规则的分词方法和基于统计的分词方法<citation id="142" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>, 其中最大匹配算法 (Maximum Match) 是最常用的基于规则分词方法之一。该方法利用词典作为分词依据, 以长词优先为基本原则, 不需要考虑领域自适应性问题, 只需要具有相关领域的高质量词典即可<citation id="143" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>, 因而分词速度较快。也正因为如此, 该类方法歧义处理能力不强, 经常会错误地切分歧义字段, 分词效果仍有待提高, 目前已经出现了一些改进的最大匹配算法。莫建文等在传统分词词典构造的基础上, 结合双字哈希结构, 利用改进的前向最大匹配分词算法进行中文分词<citation id="144" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 而该算法没有针对词汇粘连现象进行特殊处理, 无法避免由于词典颗粒度过大导致的歧义切分问题;张劲松等利用前向匹配、回溯匹配和尾词匹配有效发现了歧义字段<citation id="145" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>;周俊等将最长词条优先原则改为最长广义词条优先原则以解决歧义问题, 得到了比传统最长词条优先原则更好的效果<citation id="146" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。从研究方法可以看出, 传统的基于规则的分词方法单纯依赖词典信息, 并未有效利用分词语料中词与词之间的共现关系, 导致其分词效果不够理想。</p>
                </div>
                <div class="p1">
                    <p id="36">基于统计的分词方法利用已经切分好的分词语料作为主要的分词依据, 根据相邻字或词的紧密结合程度构造统计模型进行分词, 当紧密程度高于某一个阈值时, 可认为相邻的字组成了一个词组<citation id="147" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。n-gram语言模型是基于统计分词方法的常用模型之一<citation id="148" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 该模型采用马尔可夫条件假设, 可以有效地利用训练语料信息以及上下文相邻词之间的共现频率等统计信息, 很好地反映训练语料中词和词之间的转移关系, 歧义处理能力较强。</p>
                </div>
                <div class="p1">
                    <p id="37">针对以上两类分词方法具有的相对优势, 部分学者提出了新的分词方法, 将词典信息融入统计分词模型中以改善分词方法的性能<citation id="150" type="reference"><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>, 但该类方法大多都仅将词典当作一种内部资源引入, 其训练和解码都使用同样的词典<citation id="149" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 同时也很少从统计模型的角度对基于规则的分词序列进行概率判定。</p>
                </div>
                <div class="p1">
                    <p id="38">笔者将两类方法相结合, 提出一种基于最大匹配算法的似然导向分词方法。创新点主要包括:一是在分词阶段, 利用基于词典的最大匹配方法进行初始词识别, 在后续词切分过程中根据训练集数据的统计信息构建训练集共现词典, 并依据共现性自动识别后续词, 既提高了分词效率, 又能够较好消解歧义词, 提高了分词的准确性;二是在判定阶段, 利用基于马尔可夫性的n-gram模型, 提出基于似然导向的判定方法, 并计算在分词阶段获得的不同分词序列的概率值。基于最大似然原理确定最优的分词模式, 既充分利用了已有的统计信息, 又合理避免了每类分词算法的不足。</p>
                </div>
                <h3 id="39" name="39" class="anchor-tag">二、最大匹配算法和n-gram语言模型</h3>
                <h4 class="anchor-tag" id="40" name="40"> (<b>一) 基于规则的最大匹配算法</b></h4>
                <div class="p1">
                    <p id="41">基于规则的最大匹配算法主要包括前向最大匹配、后向最大匹配和双向匹配算法等。假设将包含所有词条的通用词典记为<i>D</i>, 其最长词条所含字数记为MaxLen。不失一般性, 记长度为<i>M</i>的待切分汉字串<i>S</i>=<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>M</i></sub>。为了从语句中切分出词条, 前向最大匹配算法是从左向右取待切分语句的候选字串, 记:</p>
                </div>
                <div class="p1">
                    <p id="42" class="code-formula">
                        <mathml id="42"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>k</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>h</mi></munder><mo stretchy="false">{</mo><mi>h</mi><mi>Ι</mi><mo stretchy="false"> (</mo><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>s</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>s</mi><msub><mrow></mrow><mi>h</mi></msub><mo stretchy="false">) </mo><mo>∈</mo><mi>D</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="43">其中</p>
                </div>
                <div class="p1">
                    <p id="44" class="code-formula">
                        <mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>s</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>s</mi><msub><mrow></mrow><mi>h</mi></msub><mo stretchy="false">) </mo><mo>∈</mo><mi>D</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>, </mo><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>s</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>s</mi><msub><mrow></mrow><mi>h</mi></msub><mo stretchy="false">) </mo><mo>∈</mo><mi>D</mi></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>s</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>s</mi><msub><mrow></mrow><mi>h</mi></msub><mo stretchy="false">) </mo><mo>∉</mo><mi>D</mi></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="45">则<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>k</i></sub>与通用词典匹配成功, 将<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>k</i></sub>作为一个词切分出来;再继续对汉字串<i>s</i><sub><i>k</i>+1</sub>, <i>s</i><sub><i>k</i>+2</sub>, …, <i>s</i><sub><i>M</i></sub>按照式 (1) 进行匹配和切分, 直到切分出所有词为止, 通常情况下<i>k</i>&gt;1。当<i>k</i>=1时, 即<i>s</i><sub>1</sub>∈<i>D</i>, 表示切分的单字作为一个词条;当<i>k</i>=0时, 即<i>s</i><sub>1</sub>∉<i>D</i>, 表示切分的单字作为一个未登录词。后向最大匹配与前向最大匹配算法的基本原理相同, 只是进行语句的反向切分。</p>
                </div>
                <div class="p1">
                    <p id="46">最大匹配分词方法不具备歧义检测和消解能力, 加大了后续歧义消解的难度。由于最大匹配分词方法遵循“长词优先”的规则, 导致每个词语的长度相对较大, 会造成算法的很多无效循环, 并对分词效率产生负面影响。此外, 传统的最大匹配分词方法一般都是利用通用词典按照某种模式进行字串的切分, 会使分词结果受词典的颗粒度影响较大, 容易产生词语粘连现象<citation id="151" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 影响分词准确性。例如:</p>
                </div>
                <div class="p1">
                    <p id="47">逻辑/思维/能力→逻辑思维/能力 (前为正确切分, 后为错误切分, 下同) </p>
                </div>
                <div class="p1">
                    <p id="48">国际/贸易壁垒/的/现状→国际贸易壁垒/的/现状</p>
                </div>
                <div class="p1">
                    <p id="49">在不考虑标准分词结果的情况下, 有些分词结果也可以认为是正确的。但是, 标准分词结果是以训练语料为基础获得的, 因而所提出分词方法产生的分词结果需要符合训练语料中词语的切分模式。</p>
                </div>
                <h4 class="anchor-tag" id="50" name="50"> (<b>二) 基于统计的</b>n-gram<b>语言模型</b></h4>
                <div class="p1">
                    <p id="51">假设<i>S</i>=<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>M</i></sub>为待切分汉字串, 可以切分为<i>N</i>个词条, 记为<i>S</i>=<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, …, <i>w</i><sub><i>N</i></sub>。采用n-gram语言模型, 词条序列<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, …, <i>w</i><sub><i>N</i></sub>出现的概率为:</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>S</mi><mo>=</mo><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>w</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mo>=</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mi>p</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>⋯</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>w</mi><msub><mrow></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="53">其中约定<i>p</i> (<i>w</i><sub>1</sub>|<i>w</i><sub>0</sub>) =<i>p</i> (<i>w</i><sub>1</sub>) , 表示以词条<i>w</i><sub>1</sub>开始的句子在训练集中出现的概率。上式的概率计算在当前的计算机水平下往往是不可行的。因而, n-gram语言模型引入马尔可夫条件假设, 即认为第<i>n</i>个词出现的概率只与其前面的<i>n</i>-1个词相关, 而与其它词都不相关。<i>S</i>出现的概率就是词条序列<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, …, <i>w</i><sub><i>N</i></sub>出现概率的乘积:</p>
                </div>
                <div class="p1">
                    <p id="54" class="code-formula">
                        <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>S</mi><mo>=</mo><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>w</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>=</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>2</mn></mrow><mi>Ν</mi></munderover><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>max</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>, </mo><mi>i</mi><mo>-</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="55">其中max (1, <i>i</i>-<i>n</i>) 表示1和<i>i</i>-<i>n</i>的最大值。当<i>n</i>=1时, 一个词的出现仅依赖于其前面出现的1个词, 分词模型被称为bi-gram, 即:</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>S</mi><mo>=</mo><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>w</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">仍约定<i>p</i> (<i>w</i><sub>1</sub>|<i>w</i><sub>0</sub>) =<i>p</i> (<i>w</i><sub>1</sub>) 。</p>
                </div>
                <div class="p1">
                    <p id="58">本文将基于规则的分词方法和基于统计的分词方法结合起来, 其中将基于规则的最大匹配分词方法视为一种初级分词法, 并利用其对待测试文本进行简单、快速处理;利用统计语言信息处理词语粘连以及后续词/前缀词的自动扩展等, 生成多组候选分词序列;利用基于马尔可夫性的n-gram模型, 计算候选分词序列的概率值, 以最大似然原理作为依据选择最优的分词模式, 得到最终的分词结果。新方法不仅充分利用了训练集的词频规律, 还有效弥补了单一分词算法的不足。</p>
                </div>
                <h3 id="59" name="59" class="anchor-tag">三、基于最大匹配算法的似然导向中文分词方法</h3>
                <div class="p1">
                    <p id="60">基于最大匹配算法的似然导向中文分词方法主要包括三个阶段:预处理阶段、分词阶段、判定阶段, 具体流程如图1所示。预处理阶段的目的是构建共现词典, 为分词阶段的自动识别后续词/前缀词以及为判定阶段的计算多组概率值做准备;分词阶段的任务是将基于规则的前向/后向最大匹配算法与基于统计信息的词共现性方法相结合, 对测试语料中待划分的句子进行分词操作, 生成多组候选分词序列, 而在该阶段中每种候选分词序列的作用应该是均等的;判定阶段的任务是利用基于马尔可夫性的n-gram语言模型, 计算多组候选分词序列对应的概率作为序列子串的似然函数, 并选取似然概率值最大的分词序列作为最终的分词结果。</p>
                </div>
                <div class="area_img" id="61">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/TJLT201903003_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于最大匹配的似然导向中文分词方法流程图" src="Detail/GetImg?filename=images/TJLT201903003_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 <b>基于最大匹配的似然导向中文分词方法流程图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJLT201903003_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="62" name="62"> (<b>一) 预处理阶段</b></h4>
                <div class="p1">
                    <p id="63">利用训练语料中的词共现信息, 构建正向训练集共现词典<i>D</i><sup><i>F</i></sup>和逆向训练集共现词典<i>D</i><sup><i>B</i></sup>。记词条<i>w</i><sub><i>i</i></sub>∈<i>T</i>, 与<i>w</i><sub><i>i</i></sub>共现的后续词条个数记为<i>m</i><mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>B</mi></msubsup></mrow></math></mathml>, 与<i>w</i><sub><i>i</i></sub>共现的后续词条集合记为{<i>w</i><mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>B</mi></msubsup></mrow></math></mathml>, 1≤<i>j</i>≤<i>m</i><mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>B</mi></msubsup></mrow></math></mathml>}, 简记为<i>B</i> (<i>w</i><sub><i>i</i></sub>) ={<i>w</i><mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>B</mi></msubsup></mrow></math></mathml>, 1≤<i>j</i>≤<i>m</i><mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>B</mi></msubsup></mrow></math></mathml>}。在<i>D</i><sup><i>F</i></sup>中, 将<i>w</i><sub><i>i</i></sub>作为词典的键, 记录与<i>w</i><sub><i>i</i></sub>共现的后续词条及共现频数{<i>w</i><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>B</mi></msubsup></mrow></math></mathml>∶<i>c</i><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>B</mi></msubsup></mrow></math></mathml>, 1≤<i>j</i>≤<i>m</i><mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>B</mi></msubsup></mrow></math></mathml>}, 其中<i>c</i><mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>B</mi></msubsup></mrow></math></mathml>为<i>w</i><sub><i>i</i></sub>与<i>w</i><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>B</mi></msubsup></mrow></math></mathml>的共现频数。正向训练集共现词典<i>D</i><sup><i>F</i></sup>的形式如表1所示;逆向训练集共现词典<i>D</i><sup><i>B</i></sup>的存储方式与正向训练集共现词典<i>D</i><sup><i>F</i></sup>相同, 但它是一个逆向词典, 词项<i>w</i><sub><i>i</i></sub>对应存储<i>w</i><sub><i>i</i></sub>的所有前缀词集合, 记为{<i>w</i><mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>Ρ</mi></msubsup></mrow></math></mathml>:<i>c</i><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>Ρ</mi></msubsup></mrow></math></mathml>, 1≤<i>j</i>≤<i>m</i><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>Ρ</mi></msubsup></mrow></math></mathml>}, 其中<i>m</i><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>Ρ</mi></msubsup></mrow></math></mathml>表示<i>w</i><sub><i>i</i></sub>前缀词个数, <i>c</i><mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>Ρ</mi></msubsup></mrow></math></mathml>表示出现在词项<i>w</i><sub><i>i</i></sub>之前的词条<i>w</i><mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>Ρ</mi></msubsup></mrow></math></mathml>与<i>w</i><sub><i>i</i></sub>的共现频数 (见表2) 。</p>
                </div>
                <div class="area_img" id="80">
                    <p class="img_tit"><b>表</b>1 <b>正向训练集共现词典</b><i>D</i><sup><i>F</i></sup><b>表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="80" border="1"><tr><td><br />词条</td><td colspan="6">后续词条</td></tr><tr><td><br /><i>w</i><sub>1</sub></td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow><mi>B</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow><mi>B</mi></msubsup></mrow></math></td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow><mi>B</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow><mi>B</mi></msubsup></mrow></math></td><td>…</td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mi>j</mi></mrow><mi>B</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mi>j</mi></mrow><mi>B</mi></msubsup></mrow></math></td><td>…</td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mi>B</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mi>B</mi></msubsup></mrow></math></td></tr><tr><td><br />…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr><tr><td><br /><i>w</i><sub><i>i</i></sub></td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mn>1</mn></mrow><mi>B</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mn>1</mn></mrow><mi>B</mi></msubsup></mrow></math></td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mn>2</mn></mrow><mi>B</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mn>2</mn></mrow><mi>B</mi></msubsup></mrow></math></td><td>…</td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>B</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>B</mi></msubsup></mrow></math></td><td>…</td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /><mi>m</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>B</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /><mi>m</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>B</mi></msubsup></mrow></math></td></tr><tr><td><br />…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="81">
                    <p class="img_tit"><b>表</b>2 <b>逆向训练集共现词典</b><i>D</i><sup><i>B</i></sup><b>表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="81" border="1"><tr><td><br />词项</td><td colspan="6">前缀词条</td></tr><tr><td><br /><i>w</i><sub>1</sub></td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow><mi>Ρ</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow><mi>Ρ</mi></msubsup></mrow></math></td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow><mi>Ρ</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow><mi>Ρ</mi></msubsup></mrow></math></td><td>…</td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mi>j</mi></mrow><mi>Ρ</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mi>j</mi></mrow><mi>Ρ</mi></msubsup></mrow></math></td><td>…</td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mi>Ρ</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mi>Ρ</mi></msubsup></mrow></math></td></tr><tr><td><br />…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr><tr><td><br /><i>w</i><sub><i>i</i></sub></td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mn>1</mn></mrow><mi>Ρ</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mn>1</mn></mrow><mi>Ρ</mi></msubsup></mrow></math></td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mn>2</mn></mrow><mi>Ρ</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mn>2</mn></mrow><mi>Ρ</mi></msubsup></mrow></math></td><td>…</td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>Ρ</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>Ρ</mi></msubsup></mrow></math></td><td>…</td><td><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /><mi>m</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>Ρ</mi></msubsup></mrow></math>∶<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /><mi>m</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>Ρ</mi></msubsup></mrow></math></td></tr><tr><td><br />…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="82" name="82"> (<b>二) 分词阶段</b></h4>
                <div class="p1">
                    <p id="83">针对前向/后向最大匹配算法, 使用两类词典:一类为不含上下文信息的通用词典<i>D</i>;另一类为在预处理阶段构建的包含上下文信息的共现词典<i>D</i><sup><i>F</i></sup>和<i>D</i><sup><i>B</i></sup>。该阶段的分词策略是利用基于通用词典的最大匹配方法切分初始词, 为了提高分词效率并消解歧义词, 本文在后续词切分过程中, 基于领域训练集语料的统计信息和后续词的共现性自动识别后续词。</p>
                </div>
                <div class="p1">
                    <p id="84">本文利用前向最大匹配与后向最大匹配算法进行分词的实现思想基本相同, 区别在于利用了不同的训练集共现词典<i>D</i><sup><i>F</i></sup>和<i>D</i><sup><i>B</i></sup>。下面, 以前向最大匹配算法为例描述具体分词过程:</p>
                </div>
                <div class="p1">
                    <p id="85">Step1 初始词切分。对于每一个待切分长度为<i>M</i>的句子<i>S</i>, 假设汉字串<i>S</i>=<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>M</i></sub>;在没有相应上下文信息的情况下, 利用传统的前向最大匹配算法, 在通用词典<i>D</i>中进行匹配, 将式 (1) 的计算结果记为<i>k</i><sub>1</sub>, 将<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>k</i><sub>1</sub></sub>作为切分结果序列的初始词<i>w</i><mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>G</mi></msubsup></mrow></math></mathml>=<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>k</i><sub>1</sub></sub>, <i>w</i><mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>G</mi></msubsup></mrow></math></mathml>的长度不大于<i>M</i>和MaxLen;采用最大匹配算法时, 获得的初始词一般具有较大的长度, 但长度较大的词往往会包含词项粘连现象, 容易导致歧义。针对这类情况, 词共现信息往往可以提供较好的切分指导, 避免多个词由于粘连而被切分在一起, 进而检测所切出的初始词是否粘连;如果具有粘连, 对其进行再次拆分, 直到不存在粘连, 以重新获得初始词<i>w</i><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>G</mi></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="89">Step2 后续词自动识别。对于待切分序列<i>S</i>, 当初始词<i>w</i><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>G</mi></msubsup></mrow></math></mathml>生成之后, 在<i>D</i><sup><i>F</i></sup>中遍历<i>w</i><mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>G</mi></msubsup></mrow></math></mathml>所对应的全部可能后续词<i>B</i> (<i>w</i><mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>G</mi></msubsup></mrow></math></mathml>) , 如果搜寻到与<i>S</i>中<i>w</i><mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>G</mi></msubsup></mrow></math></mathml>相连匹配的词, 即<i>s</i><sub><i>k</i><sub>1</sub>+1</sub>…<i>s</i><sub><i>k</i><sub>2</sub></sub>∈<i>B</i> (<i>w</i><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>G</mi></msubsup></mrow></math></mathml>) , 则将<i>s</i><sub><i>k</i><sub>1</sub>+1</sub>…<i>s</i><sub><i>k</i><sub>2</sub></sub>自动识别为初始词<i>w</i><mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>G</mi></msubsup></mrow></math></mathml>的后续词, 作为一个词条, 记为<i>w</i><sub>2</sub>。重复这个过程, 将句子<i>S</i>切分成<i>N</i>个词条, 记为:</p>
                </div>
                <div class="p1">
                    <p id="96"><i>w</i><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>G</mi></msubsup></mrow></math></mathml>=<i>s</i><sub>1</sub>…<i>s</i><sub><i>k</i><sub>1</sub></sub><i>w</i><sub>2</sub>=<i>s</i><sub><i>k</i><sub>1</sub>+1</sub>…<i>s</i><sub><i>k</i><sub>2</sub></sub>, …<i>w</i><sub><i>N</i></sub>=<i>s</i><sub><i>k</i><sub><i>N</i>-1</sub>+1</sub>…<i>s</i><sub><i>M</i></sub>      (6) </p>
                </div>
                <div class="p1">
                    <p id="99">后续词的自动识别, 一方面是基于词语的共现性, 比较符合人们进行文字表述的规律;另一方面是大大减少重复切词和匹配通用词典的循环操作, 可以提高分词方法的效率。实际操作时, 如果在<i>D</i><sup><i>F</i></sup>中找不到可以匹配的后续词, 则转step1, 重新在<i>D</i>中进行查找是否有匹配项, 并执行后续操作。</p>
                </div>
                <div class="p1">
                    <p id="100">经过上述步骤, 最终可以生成多组候选分词序列。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101"> (<b>三) 判定阶段</b></h4>
                <div class="p1">
                    <p id="102">对于候选分词序列, 计算每个分词序列的概率, 将其视为分词序列的似然概率, 将具有最大似然概率值的分词序列作为最终的分词结果。假设长度为<i>M</i>的句子<i>S</i>, 在分词阶段得到的切分词条序列的集合记为<i>W</i>={<i>W</i><sub>1</sub>, <i>W</i><sub>2</sub>, …, <i>W</i><sub><i>Q</i></sub>}, 其中<i>W</i><sub><i>q</i></sub>=<i>w</i><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>q</mi><mn>1</mn></mrow><mi>G</mi></msubsup></mrow></math></mathml>, <i>w</i><sub><i>q</i>2</sub>, …, <i>w</i><sub><i>qN</i><sub><i>q</i></sub></sub>, 1≤<i>q</i>≤<i>Q</i>, <i>N</i><sub><i>q</i></sub>为切分的词条总数。根据n-gram语言模型的马尔可夫条件假设, 对于分词阶段获得的多个切分序列, 当切分词条序列<i>W</i><sub><i>q</i></sub>的似然概率达到最大值时, 即满足式 (7) 时词条序列<i>W</i><sup>*</sup><sub><i>q</i></sub>被认为是似然导向的词条序列:</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><msubsup><mrow></mrow><mi>q</mi><mo>*</mo></msubsup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mrow><mi>W</mi><msub><mrow></mrow><mi>q</mi></msub><mo>∈</mo><mi>W</mi></mrow></munder><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>q</mi></msub></mrow></munderover><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>q</mi><mi>i</mi></mrow></msub><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>q</mi><mo stretchy="false"> (</mo><mi>max</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>, </mo><mi>i</mi><mo>-</mo><mi>n</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></msub><mo>⋯</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>q</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">) </mo></mrow><mo>) </mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105">语料库不可能包含所有可能出现的序列, 某些词条在语料中出现频数会很少。为了降低数据稀疏问题对计算似然概率带来的影响, 仅使用bi-gram语言模型估计似然概率。将式 (7) 简化为式 (8) 的形式, 即得到基于马尔可夫性的bi-gram语言模型:</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><msubsup><mrow></mrow><mi>q</mi><mo>*</mo></msubsup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mrow><mi>W</mi><msub><mrow></mrow><mi>q</mi></msub><mo>∈</mo><mi>W</mi></mrow></munder><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>q</mi></msub></mrow></munderover><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>q</mi><mi>i</mi></mrow></msub><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>q</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">) </mo></mrow><mo>) </mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">满足式 (8) 的词条序列<i>W</i><sup>*</sup><sub><i>q</i></sub>被认为是似然导向的词条序列。使用最大似然估计方法, <i>p</i> (<i>w</i><sub><i>q i</i></sub>|<i>w</i><sub><i>q</i> (<i>i</i>-1) </sub>) =<i>c</i> (<i>w</i><sub><i>q i</i></sub><i>w</i><sub><i>q</i> (<i>i</i>-1) </sub>) /∑<i>c</i> (<i>w</i><sub><i>q i</i></sub><i>w</i><sub><i>q</i> (<i>i</i>-1) </sub>) , 其中<i>c</i> (<i>w</i><sub><i>q</i> (<i>i</i>-1) </sub><i>w</i><sub><i>qi</i></sub>) 表示<i>w</i><sub><i>q i</i></sub>作为后续词与<i>w</i><sub><i>q</i> (<i>i</i>-1) </sub>共现的频数, ∑<i>c</i> (<i>w</i><sub><i>q</i> (<i>i</i>-1) </sub><i>w</i><sub><i>q i</i></sub>) 表示所有与<i>w</i><sub><i>q</i> (<i>i</i>-1) </sub>共现的后续词频数之和, 二者都根据词典<i>D</i><sup><i>F</i></sup>中词共现频率计算得到。</p>
                </div>
                <div class="p1">
                    <p id="108">在训练语料中用未曾共现过的新序列计算似然概率时, 会出现估计值为零的情况。为此, 采用数据平滑策略对其进行处理, 似然概率计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>q</mi><mi>i</mi></mrow></msub><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>q</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>c</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>q</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msub><mi>w</mi><msub><mrow></mrow><mrow><mi>q</mi><mi>i</mi></mrow></msub><mo stretchy="false">) </mo><mo>+</mo><mi>δ</mi></mrow><mrow><mstyle displaystyle="true"><mo>∑</mo><mi>c</mi></mstyle><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>q</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msub><mi>w</mi><msub><mrow></mrow><mrow><mi>q</mi><mi>i</mi></mrow></msub><mo stretchy="false">) </mo><mo>+</mo><mi>δ</mi><mo>×</mo><mtext>D</mtext><mtext>F</mtext></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">其中<i>δ</i> (0≤<i>δ</i>≤1) 为平滑参数, 表示每个词出现的频数比实际统计频数多<i>δ</i>次;DF为<i>D</i><sup><i>F</i></sup>中词共现词条的总频数。</p>
                </div>
                <h3 id="111" name="111" class="anchor-tag">四、 实 验</h3>
                <h4 class="anchor-tag" id="112" name="112"> (<b>一) 实验语料</b></h4>
                <div class="p1">
                    <p id="113">实验语料来源于SIGHAN组织的国际中文自然语言处理竞赛 (Bakeoff) , 实验数据包括标准词典、训练语料、测试语料以及测试语料的标准切分。由于编码方式的不同, 实验仅在中文简体语料库上进行了测试, 训练语料包含44.947万词, 测试语料包含11.35万词。在训练过程中, 实验还使用了通用词典, 包含42.7452万词。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114"> (<b>二) 实验方法</b></h4>
                <div class="p1">
                    <p id="115">本文针对8种算法进行中文分词对比实验:</p>
                </div>
                <div class="p1">
                    <p id="116">1) FMM:传统前向最大匹配分词方法。</p>
                </div>
                <div class="p1">
                    <p id="117">2) BMM:传统后向最大匹配分词方法。</p>
                </div>
                <div class="p1">
                    <p id="118">3) FBMM:将似然导向用于FMM和BMM的分词方法。</p>
                </div>
                <div class="p1">
                    <p id="119">4) IFMM:利用词共现性改进的前向最大匹配分词方法。</p>
                </div>
                <div class="p1">
                    <p id="120">5) IBMM:利用词共现性改进的后向最大匹配分词方法。</p>
                </div>
                <div class="p1">
                    <p id="121">6) POMM:基于最大匹配算法的似然导向中文分词方法。在该方法中, 分词阶段利用词共现性改进FMM和BMM, 判定阶段结合n-gram语言模型实现似然导向功能。</p>
                </div>
                <div class="p1">
                    <p id="122">7) IPOMM1:在POMM的基础上, 为解决分词粘连现象, 针对分词阶段获得的初始词加入再次切分策略。</p>
                </div>
                <div class="p1">
                    <p id="123">8) IPOMM2:在IPOMM1的基础上, 为避免零概率问题, 在判定阶段计算似然概率时采用平滑参数<i>δ</i>=0.000 01的平滑策略。</p>
                </div>
                <h4 class="anchor-tag" id="124" name="124"> (<b>三) 评价指标</b></h4>
                <div class="p1">
                    <p id="125">为了评价本文方法的分词性能, 分别选择准确率、召回率、<i>F</i><sub>1</sub>值和时间开销作为评价指标。准确率是在分词方法生成的分词结果中切分正确的目标词所占的比例, 即分词方法正确分词数/分词方法分词总数, 记为<i>P</i>;召回率是针对标准分词结果中, 分词方法生成的分词结果被正确切分的目标词所占的比例, 即分词方法正确分词数/标准分词总数, 记为<i>R</i>;<i>F</i><sub>1</sub>值为<i>F</i><sub>1</sub>=2<i>PR</i>/ (<i>P</i>+<i>R</i>) , <i>F</i><sub>1</sub>值折中考虑准确率与召回率, 综合反映分词方法的整体效果。</p>
                </div>
                <h4 class="anchor-tag" id="126" name="126"> (<b>四) 实验结果与分析</b></h4>
                <div class="p1">
                    <p id="127">将以上8种方法归纳为3种类型, 其中第1类包括FMM、BMM、FBMM;第2类包括IFMM、IBMM、POMM;第3类包括IPOMM1、IPOMM2。表3给出了3类方法的对比实验结果, 其中“right”列表示分词方法生成的分词结果中切分正确的目标词数量;“split”列表示分词方法生成的全部词数量;“goal”列表示标准分词结果中词的数量。</p>
                </div>
                <div class="area_img" id="128">
                    <p class="img_tit"><b>表</b>3 <b>不同方法的准确率、召回率</b>、<i>F</i><sub>1</sub><b>值比较表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="128" border="1"><tr><td>方法<br />类型</td><td>方法<br />名称</td><td>准确<br />率<i>P</i></td><td>召回<br />率<i>R</i></td><td>调和均<br />值<i>F</i><sub>1</sub></td><td>正确分词<br />数量right</td><td>分词总数<br />split</td><td>标准分词<br />数量goal</td></tr><tr><td>第一类</td><td>FMM</td><td>0.834 8</td><td>0.820 3</td><td>0.827 5</td><td>93 128</td><td>111 554</td><td>113 527</td></tr><tr><td><br /></td><td>BMM</td><td>0.855 2</td><td>0.816 3</td><td>0.835 3</td><td>92 681</td><td>108 370</td><td>113 527</td></tr><tr><td><br /></td><td>FBMM</td><td>0.855 2</td><td>0.816 3</td><td>0.835 3</td><td>92 681</td><td>108 370</td><td>113 527</td></tr><tr><td><br />第二类</td><td>IFMM</td><td>0.856 0</td><td>0.828 1</td><td>0.841 8</td><td>94 007</td><td>109 827</td><td>113 527</td></tr><tr><td><br /></td><td>IBMM</td><td>0.852 2</td><td>0.830 0</td><td>0.841 0</td><td>94 227</td><td>110 562</td><td>113 527</td></tr><tr><td><br /></td><td>POMM</td><td>0.871 1</td><td>0.832 2</td><td>0.851 2</td><td>94 474</td><td>108 459</td><td>113 527</td></tr><tr><td><br />第三类</td><td>IPOMM1</td><td>0.879 4</td><td>0.873 2</td><td>0.876 3</td><td>99 129</td><td>112 723</td><td>113 527</td></tr><tr><td><br /></td><td>IPOMM2</td><td>0.884 3</td><td>0.887 0</td><td>0.885 6</td><td>100 692</td><td>113 864</td><td>113 527</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="129">由表3可知:在第1类方法中, BMM与FBMM方法对应的准确率、召回率及<i>F</i><sub>1</sub>值均为0.855 2、0.816 3、0.835 3, FBMM表现与BMM一致, 这是由于似然导向策略使得FBMM方法对应结果由BMM决定;虽然BMM在准确率上高于FMM方法, 但从正确切分目标词数量上可以看出BMM对应的right值 (92 681) 少于FMM对应结果 (93 128) 。可见, BMM方法在召回正确切分目标词数量上并未获得提高。</p>
                </div>
                <div class="p1">
                    <p id="130">第2类方法中, IFMM、IBMM、POMM在<i>P</i>、<i>R</i>、<i>F</i><sub>1</sub>值方面, 相比于第1类方法均有改善, 尤其在使用基于n-gram的似然导向策略时准确率从0.834 8提高到0.871 1、召回率从0.820 3提高到0.832 2, 这说明在基于规则的最大匹配算法中考虑基于统计的词共现性信息能有效提高算法的准确率和召回率;同时, 对POMM方法判定阶段的似然导向策略起到了积极作用, 从正确切分目标词数量上也可以看出POMM 对应的right值 (94 474) 大于FBMM对应结果 (92 681) , 似然导向策略增加了目标词的召回率, 这说明POMM方法综合考虑了IFMM、IBMM各自的优势, 引入词共现性和似然导向策略能有效提高分词算法的性能, 验证了似然导向相比于单个分词方法的优势。</p>
                </div>
                <div class="p1">
                    <p id="131">第3类方法中, IPOMM1、IPOMM2相对于第1、2类方法, 在<i>P</i>、<i>R</i>、<i>F</i><sub>1</sub>值方面都得到明显提升。例如第1、2类方法准确率的最大值分别为0.855 2和0.871 1, 明显小于IPOMM1和IPOMM2的结果;在召回率方面, 从第1、2类方法的最大值0.820 3、0.832 2, 提高到了0.887 0。通过对比召回目标词数量发现, 第1类方法中FMM方法最优, 正确分词数量为93 128;第2类方法中POMM方法最优, 正确分词数量为94 474;第3类方法中IPOMM2方法最优, 正确分词数量为100 692, 这进一步说明了引入词共现性和似然导向策略能有效提高分词算法性能。</p>
                </div>
                <div class="p1">
                    <p id="132">对具体的错误分词结果进行分析发现:第1类方法基于最大匹配算法, 受到通用词典颗粒度的影响, 易出现粘连现象;第2类方法通过加入基于统计的词共现性信息, 在一定程度上减少了这种粘连现象, 但在初始词划分时依然受到通用词典中词的颗粒度的影响;第3类方法在第2类方法的框架下优化了初始词切分策略, 克服了最大匹配算法受通用词典颗粒度影响较大的问题。此外, 该方法在判定阶段调整了平滑参数, 降低了数据稀疏对概率计算准确性的影响。图2展示了第3类方法IPOMM2与第1类方法FBMM在错误分词结果方面的对比情况, 即图2中的词是采用FBMM方法会被错误切分但采用IPOMM2方法时可以正确切分的词, 例如在FBMM 方法中切分错的词“国有企业”、“非关税壁垒”、“经济运行”等, 通过IPOMM2方法正确切分为“国有/企业”、“非关税/壁垒”、“经济/运行”等;第3类方法IPOMM2有效解决了中文分词中受词典颗粒度影响而产生的词语粘连问题。</p>
                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/TJLT201903003_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 错误分词结果对比词云图" src="Detail/GetImg?filename=images/TJLT201903003_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>2 <b>错误分词结果对比词云图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJLT201903003_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="134">
                    <p class="img_tit"><b>表</b>4 <b>不同方法的分词效率对比表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="134" border="1"><tr><td><br />第1类方法</td><td>第2类方法</td><td colspan="2">第3类方法</td></tr><tr><td><br />FBMM</td><td>POMM</td><td>IPOMM1</td><td>IPOMM2</td></tr><tr><td><br /><i>T</i>= 63.74</td><td><i>T</i>= 56.30</td><td><i>T</i>= 56.51</td><td><i>T</i>=59.84</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="135">表4对比了3类分词方法中采用似然导向策略时所需的时间开销, 即第1类方法中的FBMM、第2类方法中的POMM、第3类方法中的IPOMM1和IPOMM2。由表4知, 第2类方法、第3类方法所产生的时间开销普遍少于第1类方法, 说明分词阶段进行后续词/前缀词的自动扩展有助于提高运算效率;第3类方法为了减少词语粘连现象和避免零概率问题, 在第2类方法的基础上优化了初始词切分方法并调整了平滑参数, 因而导致第3类方法的时间开销稍高于第2类方法, 在提升分词性能的同时稍微损失一些计算效率, 也是在可接受范围内。</p>
                </div>
                <h3 id="136" name="136" class="anchor-tag">五、结 语</h3>
                <div class="p1">
                    <p id="137">本文结合基于规则的分词方法与基于统计的分词方法的优势, 提出了一种基于最大匹配算法的似然导向中文分词方法POMM。新方法在分词阶段引入词共现性, 很好地发挥了基于统计的分词方法处理词与词之间紧密程度的优势;在判定阶段利用n-gram语言模型的马尔可夫性对不同的概率值进行似然导向调整;在POMM的基础上, 针对粒度较大的初始词进一步切分以消解歧义, 并调整平滑参数, 避免数据稀疏所带来的精度下降问题。相比于传统的最大匹配算法或者单一分词方法, 新方法在准确率、召回率、<i>F</i><sub>1</sub>值方面均得到了显著的提升。</p>
                </div>
                <div class="p1">
                    <p id="138">对于某些词语, 似然导向也带来了一定的负效应, 即前向最大匹配/后向最大匹配可以正确切分的词, 使用似然导向方法偶尔会发生切分错误。在下一步研究中, 一方面将考虑引入多种不同的分词策略或者新词识别方法以解决无法正确切分的句子;另一方面将引入集成学习方法以保留更多的正确导向, 降低错误率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MESS200703001&amp;v=MzEyOTlHRnJDVVI3cWZadVpvRnk3a1dydk5LQ2pZZmJHNEh0Yk1ySTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 黄昌宁, 赵海.中文分词十年回顾[J].中文信息学报, 2007 (3) .
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SXGY201802010&amp;v=MjYxNjc0SDluTXJZOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeTdrV3J2Tk5qWE1kN0c=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 王克岭, 张甜溪, 段玲.微信公众号软文内部点赞影响因素研究[J].西安财经学院学报, 2018 (2) .
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJLT201611030&amp;v=Mjk4OTJybzlHWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3a1dydk5NU2ZIZXJHNEg5Zk4=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 朱建平.网络舆情分析的统计思维[J].统计与信息论坛, 2016 (11) .
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJLT201806006&amp;v=MDc4MjBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N2tXcnZOTVNmSGVyRzRIOW5NcVk5RllvUUs=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 刘伟.“一带一路”倡议下国内外新闻舆情及其演化分析[J].统计与信息论坛, 2018 (6) .
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200712007&amp;v=MzE0NTFOclk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N2tXcnZOTHo3QmQ3RzRIdGI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 吴春颖, 王士同.基于二元语法的N-最大概率中文粗分模型[J].计算机应用, 2007 (12) .
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDJS201119008&amp;v=MDI4NTZxQnRHRnJDVVI3cWZadVpvRnk3a1dydk5QU25CZmJHNEg5RE5wbzlGYklRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 张玉茹.中文分词算法之最大匹配算法的研究[J].现代计算机:专业版, 2011 (19) .
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201305056&amp;v=MDUxMThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3a1dydk5OaWZZWkxHNEg5TE1xbzlBWW8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 莫建文, 郑阳, 首照宇, 等.改进的基于词典的中文分词方法[J].计算机工程与设计, 2013 (5) .
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG200922045&amp;v=MDQwODViRzRIdGpPclk5QllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N2tXcnZOTHo3TWE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 张劲松, 袁健.回溯正向匹配中文分词算法[J].计算机工程与应用, 2009 (22) .
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201402027&amp;v=MTk4NjNNYWJHNEg5WE1yWTlIWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3a1dydk5Mejc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 周俊, 郑中华, 张炜.基于改进最大匹配算法的中文分词粗分方法[J].计算机工程与应用, 2014 (2) .
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Chinese Word Segmentation as Character Tagging">

                                <b>[10]</b> Nianwen Xue.Chinese Word Segmentation as Character Tagging[J].The Association for Computational Linguistics and Chinese Language Processiong, 2003 (1) .
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDTQ201304013&amp;v=MjQ5MDk5TE1xNDlFWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3a1dydk5QU25mZjdHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 王昊, 李思舒, 邓三鸿.基于N-Gram的文本语种识别研究[J].现代图书情报技术, 2013 (4) .
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX200609038&amp;v=MjM3NTJGeTdrV3J2TlBUWGNkckc0SHRmTXBvOUdiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 翟凤文, 赫枫龄, 左万利.字典与统计相结合的中文分词方法[J].小型微型计算机系统, 2006 (9) .
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Improved Chinese Word Segmentation System with Conditional Random Field">

                                <b>[13]</b> Zhao Hai, Huang Chang Ning, Li Mu.An Improved Chinese Word Segmentation System with Conditional Random Field[C].Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, 2006.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MESS201202001&amp;v=MTI1OTdydk5LQ2pZZmJHNEg5UE1yWTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3a1c=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 张梅山, 邓知龙, 车万翔, 等.统计与词典相结合的领域自适应中文分词[J].中文信息学报, 2012 (2) .
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200909009&amp;v=MjQzODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3a1dydk5OeWZUYkxHNEh0ak1wbzlGYllRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 宋彦, 蔡东风, 张桂平, 等.一种基于字词联合解码的中文分词方法[J].软件学报, 2009 (9) .
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="TJLT201903003" />
        <input id="dpi" type="hidden" value="800" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJLT201903003&amp;v=MjU4NDhadVpvRnk3a1dydktNU2ZIZXJHNEg5ak1ySTlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNczVwMDZGR0d6Yk0xaUg1WWF1cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
