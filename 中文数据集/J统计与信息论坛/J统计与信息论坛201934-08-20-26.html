

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637140717989975000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dTJLT201908004%26RESULT%3d1%26SIGN%3d7DZnBwWNxP0PX8i22tKtyKbWOBI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=TJLT201908004&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=TJLT201908004&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJLT201908004&amp;v=MTY0NDJmSGVyRzRIOWpNcDQ5RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVabkZpRGxWTDNQTVM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="一、引 言 ">一、引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#60" data-title="二、Sticky HDP模型的相关理论 ">二、Sticky HDP模型的相关理论</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title=" (&lt;b&gt;一) 模型介绍&lt;/b&gt;"> (<b>一) 模型介绍</b></a></li>
                                                <li><a href="#95" data-title=" (&lt;b&gt;二) 参数推断&lt;/b&gt;"> (<b>二) 参数推断</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#122" data-title="三、仿真模拟结果与分析 ">三、仿真模拟结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#126" data-title=" (&lt;b&gt;一) 仿真模拟数据&lt;/b&gt;"> (<b>一) 仿真模拟数据</b></a></li>
                                                <li><a href="#146" data-title=" (&lt;b&gt;二&lt;/b&gt;) IRIS&lt;b&gt;数据集&lt;/b&gt;"> (<b>二</b>) IRIS<b>数据集</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#161" data-title="四、结 论 ">四、结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#136" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;各指标数据中各成分模型对应参数&lt;/b&gt;"><b>表</b>1 <b>各指标数据中各成分模型对应参数</b></a></li>
                                                <li><a href="#180" data-title="表2 各指标数据参数的先验分布">表2 各指标数据参数的先验分布</a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;各超参数的先验分布&lt;/b&gt;"><b>表</b>3 <b>各超参数的先验分布</b></a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;总体数据聚类结果&lt;/b&gt;"><b>表</b>4 <b>总体数据聚类结果</b></a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;不同聚类方法的聚类结果 (基于准确率&lt;/b&gt;) "><b>表</b>5 <b>不同聚类方法的聚类结果 (基于准确率</b>) </a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;表&lt;/b&gt;6 &lt;b&gt;基于&lt;/b&gt;sticky HDP&lt;b&gt;模型的鸢尾花数据聚类结果&lt;/b&gt;"><b>表</b>6 <b>基于</b>sticky HDP<b>模型的鸢尾花数据聚类结果</b></a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;表&lt;/b&gt;7 &lt;b&gt;不同聚类方法的聚类结果&lt;/b&gt;"><b>表</b>7 <b>不同聚类方法的聚类结果</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;表&lt;/b&gt;8 &lt;b&gt;鸢尾花的特征属性数据的一致性与粘性结果&lt;/b&gt;"><b>表</b>8 <b>鸢尾花的特征属性数据的一致性与粘性结果</b></a></li>
                                                <li><a href="#160" data-title="&lt;b&gt;图&lt;/b&gt;1 4&lt;b&gt;个特征属性数据的直方图&lt;/b&gt;"><b>图</b>1 4<b>个特征属性数据的直方图</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="181">


                                    <a id="bibliography_1" title=" 朱绍军.模型自动选择聚类算法的研究与应用[D].宁波:宁波大学, 2014." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1014085485.nh&amp;v=MTMzMjhHOVhFcXBFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVabkZpRGxWTDNQVkYyNkdyT3c=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         朱绍军.模型自动选择聚类算法的研究与应用[D].宁波:宁波大学, 2014.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_2" title=" 何晓群.多元统计分析[M].第4版.北京:中国人民大学出版社, 2015." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787300208480000&amp;v=MjY2MzVNcDR0TlpPc1BEQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWmVadkZ5bnNVcmZJSjF3VFhGcXpHYkM0SHRQ&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         何晓群.多元统计分析[M].第4版.北京:中国人民大学出版社, 2015.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_3" title=" Pang-NingT, Michael S, Vipin K.数据挖掘导论 (完整版) [M].第2版.北京:人民邮电出版社, 2011." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787115241009002&amp;v=MTQ4NjhEaE04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWmVadkZ5bnNVcmZJSjF3VFhGcXpHYks1RzlQSXJvOUZiZXNQ&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Pang-NingT, Michael S, Vipin K.数据挖掘导论 (完整版) [M].第2版.北京:人民邮电出版社, 2011.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_4" title=" Antoniak C E.Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems[J].The Annals of Statistics, 1974 (6) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14112600625536&amp;v=MjU3ODY5Rll1a0tDWDgvb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmJKS0Y0VGFCVT1OaWZZZXJLOEg5RE9xWQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Antoniak C E.Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems[J].The Annals of Statistics, 1974 (6) .
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_5" title=" 徐谦, 周俊生, 陈家骏.Dirichlet过程及其在自然语言处理中的应用[J].中文信息学报, 2009 (5) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MESS200905003&amp;v=MDA2MzMzenFxQnRHRnJDVVI3cWZadVpuRmlEbFZMM1BLQ2pZZmJHNEh0ak1xbzlGWjRRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         徐谦, 周俊生, 陈家骏.Dirichlet过程及其在自然语言处理中的应用[J].中文信息学报, 2009 (5) .
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_6" title=" 李涛, 张景肖.基于BT-SVM模型组合的动态加权多分类算法研究[J].统计与信息论坛, 2019 (1) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJLT201901004&amp;v=MDQxODBGckNVUjdxZlp1Wm5GaURsVkwzUE1TZkhlckc0SDlqTXJvOUZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         李涛, 张景肖.基于BT-SVM模型组合的动态加权多分类算法研究[J].统计与信息论坛, 2019 (1) .
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_7" title=" 李松, 谢新新, 刘东林, 等.基于Dirichlet过程的无线视频码率变化识别算法[J].高技术通讯, 2016 (Z2) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GJSX2016Z2003&amp;v=MTI1MjBGckNVUjdxZlp1Wm5GaURsVkwzUElpZllkckc0SDllbXJZOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         李松, 谢新新, 刘东林, 等.基于Dirichlet过程的无线视频码率变化识别算法[J].高技术通讯, 2016 (Z2) .
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_8" title=" Ferguson T S.A Bayesian Analysis of Some Nonparametric Problems[J].Annals of Statistics, 1973 (2) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14112600625288&amp;v=MTI5MTFKS0Y0VGFCVT1OaWZZZXJLOEg5RE9xWTlGWXVrS0RuUXhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Ferguson T S.A Bayesian Analysis of Some Nonparametric Problems[J].Annals of Statistics, 1973 (2) .
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_9" title=" BlackwellD, MacQueen J B.Ferguson Distributions via P&#243;lya Urn Schemes[J].Annals of Statistics, 1973 (2) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14112600625300&amp;v=MzI1MTVPcVk5Rll1a0tEM3c1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmJKS0Y0VGFCVT1OaWZZZXJLOEg5RA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         BlackwellD, MacQueen J B.Ferguson Distributions via P&#243;lya Urn Schemes[J].Annals of Statistics, 1973 (2) .
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_10" title=" Pitman J.Combinatorial Stochastic Processes[R].Univecsity of California at Berkwell Department of Statistics, 2002." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Combinatorial Stochastic Processes">
                                        <b>[10]</b>
                                         Pitman J.Combinatorial Stochastic Processes[R].Univecsity of California at Berkwell Department of Statistics, 2002.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_11" title=" Sethuraman J.A Constructive Definition of the Dirichlet Prior[J].Statistica Sinica, 1994 (2) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJSTCBB04F73D4F5D24BFFF52D19436F603F&amp;v=Mjk5NDVibnBDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0Rmd6TGk0d3E4PU5pZlllc0RLYk5ISTJZaEdFTzk1Q1FnN3kyUmxuRWw0U2d2anBSWTJmOFNTUg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Sethuraman J.A Constructive Definition of the Dirichlet Prior[J].Statistica Sinica, 1994 (2) .
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_12" title=" Teh Y W, Jordan M I, Beal M J, et al.Hierarchical Dirichlet Processes[J].Journal of the American Statistical Association, 2006 (1) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800006293&amp;v=MjExMDdUNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyYkpLRjRUYUJVPU5qbkJhcks3SHRmT3A0OUZaT3NKRG5VNm9CTQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Teh Y W, Jordan M I, Beal M J, et al.Hierarchical Dirichlet Processes[J].Journal of the American Statistical Association, 2006 (1) .
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_13" title=" Fox EB, Sudderth E B, Jordan M I, et al.A Sticky HDP-HMM with Application to Speaker Diarization[J].The Annals of Applied Statistics, 2011 (2A) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST15012900203933&amp;v=MTc1NzNUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmJKS0Y0VGFCVT1OaWZZZXJLOUh0RE9wbzlGWnVzTUJYODZvQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Fox EB, Sudderth E B, Jordan M I, et al.A Sticky HDP-HMM with Application to Speaker Diarization[J].The Annals of Applied Statistics, 2011 (2A) .
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_14" title=" Andrieu C, Freitas N D, Doucet A.An Introduction to MCMC for Machine Learning[J].Machine Learning, 2003 (1/2) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340541&amp;v=MTMzNDI5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlIa1c3L1BJbGs9Tmo3QmFyTzRIdEhOckl0RlllOE9ZM2s1ekJkaDRq&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Andrieu C, Freitas N D, Doucet A.An Introduction to MCMC for Machine Learning[J].Machine Learning, 2003 (1/2) .
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_15" title=" Ishwaran J, James L.Gibbs Sampling Methods for Stick-breaking Priors[J].Journal of the American Statistical Association, 2001 (4) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800007394&amp;v=MDg2MzRVPU5qbkJhcks3SHRmT3A0OUZaT3NJRDNVOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJiSktGNFRhQg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Ishwaran J, James L.Gibbs Sampling Methods for Stick-breaking Priors[J].Journal of the American Statistical Association, 2001 (4) .
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_16" title=" West M.Hyperparameter Estimation in Dirichlet Process Mixture Models[R].Durham:Duke Univecsity, 1992." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hyperparameter Estimation in Dirichlet Process Mixture Models">
                                        <b>[16]</b>
                                         West M.Hyperparameter Estimation in Dirichlet Process Mixture Models[R].Durham:Duke Univecsity, 1992.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=TJLT" target="_blank">统计与信息论坛</a>
                2019,34(08),20-26             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>带粘性的层次</b>Dirichlet<b>过程聚类方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%B6%B5&amp;code=42647891&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李涵</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%A8%9C&amp;code=24815952&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张娜</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B7%B1%E5%9C%B3%E5%A4%A7%E5%AD%A6%E7%BB%8F%E6%B5%8E%E5%AD%A6%E9%99%A2&amp;code=0147648&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深圳大学经济学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>对由多个指标组成的多元数据进行聚类分析时, 数据维度的增加、各指标与总体聚类的相关性程度不一致以及各指标服从的分布不同会增加聚类的复杂性, 影响聚类结果的准确性, 因此需要通过合适的方法来对多元数据进行聚类分析。针对这一问题, 提出改进的带粘性的层次Dirichlet过程 (sticky Hierarchical Dirichlet Process) 方法来实现对多元数据的降维聚类, 以解决各指标服从不同分布的问题, 并用粘性参数反映各指标与总体聚类之间的相关性。用MCMC方法来估计模型参数。通过对仿真模拟数据和IRIS数据集的聚类分析, 证实了该方法的有效性, 同时发现单个指标与总体聚类的相关性越大, 则相应的粘性参数越大, 从而反映该指标在总体聚类中的重要性程度越高;并且当各指标数据中有粘性较大的指标时, 带粘性的层次Dirichlet过程方法明显优于其他聚类方法, 能够显著提高分类的准确性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">聚类分析;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B8%A6%E7%B2%98%E6%80%A7%E7%9A%84%E5%B1%82%E6%AC%A1Dirichlet%E8%BF%87%E7%A8%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">带粘性的层次Dirichlet过程;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B4%E5%90%88%E5%88%86%E6%9E%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">整合分析;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%85%83%E6%95%B0%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多元数据;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李涵, 女, 广东信宜人, 讲师, 研究方向:统计计算, 数据整合分析;;
                                </span>
                                <span>
                                    张娜, 女, 湖南邵阳人, 硕士, 研究方向:贝叶斯分析与统计计算。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>深圳大学人文社科青年教师扶持项目《我国商业银行的操作风险整合分析》 (00000309);</span>
                    </p>
            </div>
                    <h1><b>A Sticky Hierarchical Dirichlet Process Clustering Method</b></h1>
                    <h2>
                    <span>LI Han</span>
                    <span>ZHANG Na</span>
            </h2>
                    <h2>
                    <span>College of Economics, Shenzhen University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>When clustering analysis is performed on multivariate data, the increasing data dimension, the difference of correlation between each index and the overall clustering, and the different distribution of each index follows will enhance the complexity of clustering and decrease the accuracy of the clustering results.Therefore, it is necessary to cluster the multivariate data while addressing the above problems.A new method named Improved Sticky Hierarchical Dirichlet Process is proposed to cluster multivariate data, solve the problem of varying distribution The MCMC method is applied to estimate the model parameters.Its validity is verified through simulating data and IRIS dataset.It is found that the greater the correlation between the index and the overall clustering, the larger the sticky parameter is, which implies that the index is more important for the overall clustering.When the indices which have larger sticky parameters, the new method is superior that significantly improve the accuracy of classification.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=clustering%20analysis&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">clustering analysis;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sticky%20hierarchical%20dirichlet%20process&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sticky hierarchical dirichlet process;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=integrative%20analysis&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">integrative analysis;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multivariate%20data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multivariate data;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-20</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag">一、引 言</h3>
                <div class="p1">
                    <p id="36">聚类分析是挖掘数据信息的一种方式, 其目的是把一个有限的、未标记的数据集, 根据预定义的相似性度量, 划分成多个自然的类别, 从而使得来自同一类里的数据对象都彼此接近, 不同的类中的数据对象彼此不同<citation id="213" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。聚类分析是模式识别的一个相关工具, 在许多领域都有应用, 如生物信息学、信息检索、市场研究、股票趋势分析等。</p>
                </div>
                <div class="p1">
                    <p id="37">在实际应用中, 进行聚类分析的数据通常是由多个指标构成的多元数据, 我们用<i>n</i>表示观测对象的个数, <i>m</i>表示观测指标的个数, <i>X</i><sub><i>i</i></sub>= (<i>x</i><sub><i>i</i>1</sub>, <i>x</i><sub><i>i</i>2</sub>, …, <i>x</i><sub><i>im</i></sub>) 表示第<i>i</i>个观测对象的<i>m</i>个指标数据, <mathml id="38"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mrow><mn>1</mn><mi>j</mi></mrow></msub><mo>, </mo><mi>x</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>j</mi></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>x</mi><msub><mrow></mrow><mrow><mi>n</mi><mi>j</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mo>´</mo></msup></mrow></math></mathml>表示第<i>j</i>个指标, <i>i</i>=1, 2, …, <i>n</i>, <i>j</i>=1, 2, …, <i>m</i>。那么, 数据维度<i>m</i>的增加会增加聚类的复杂性, 而且每个指标<mathml id="39"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>服从的分布以及与总体聚类的相关性程度也不尽相同, 因此需要对各指标<mathml id="40"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>进行分析, 以便能更准确地对多元数据进行聚类。举例来说, 假设要对我国某些地级市的农业现代化发展水平进行聚类分析, 那么会从多个角度来进行综合的考察, 如单位面积粮食产量<mathml id="41"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>、劳均农业增加值<mathml id="42"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>、劳均肉食产量<mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>、农村居民人均可支配收入<mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>4</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>、国家级现代农业示范区数量<mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>5</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>, 这些指标数据就组成了多元数据<mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>5</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>。其中, 各指标数据<mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>的类型不同, 比如<mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>1</mn></msub></mrow></math></mathml>是连续变量, <mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>5</mn></msub></mrow></math></mathml>是离散变量, 同时各指标数据<mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>包含的农业现代化水平的信息量大小不完全相同, 因此它们与各地级市农业现代化发展水平评价之间的相关性程度可能存在不一致的情况, 那么就需要应用合适的聚类方法来对该多元数据<mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>进行聚类分析。</p>
                </div>
                <div class="p1">
                    <p id="52">传统聚类方法通常假设数据样本独立同分布, 比如<i>K</i>-均值方法<citation id="214" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation><sup>64-65</sup>、朴素贝叶斯<citation id="215" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation><sup>141-145</sup>、系统聚类法<citation id="216" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation><sup>52-55</sup>。然而在真实的样本数据中, 数据样本很有可能来源于多个不相同的分布, 与此同时, 样本数据也不一定是完全独立的, 部分指标之间可能存在一定的相关关系, 导致这些方法的假设条件难以得到满足。其次, 对于聚类分析, 其中一个比较困难的问题是确定数据分类的最佳个数。现有的大多数聚类方法需要通过模型选择准则或者人为观察树状聚类图来确定类别的最佳个数, 不能实现聚类数目根据数据自适应地确定, 且聚类的准确性和泛化性还受到模型选择准则的影响。近年来, Dirichlet过程混合模型由于能够实现数据分类的最佳个数由模型和数据自主计算确定, 且无需数据样本同分布, 因此作为先验分布在许多领域中得到了应用<citation id="217" type="reference"><link href="187" rel="bibliography" /><link href="189" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>, 包括基因表达数据的聚类分析、无线视频码率变化的聚类识别等<citation id="218" type="reference"><link href="191" rel="bibliography" /><link href="193" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>。但是该方法不考虑各指标与总体聚类之间相关性不一致的问题。在实际的样本数据中, 各指标与总体分类的相关性有所差异, 这会影响到该方法的准确性。比如, 在以上的农业现代化发展水平聚类分析的例子中, 总体分类可以分为发展水平较高、发展水平中等以及发展水平较低三类, 而各指标可能出现单位面积粮食产量<mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>的分类与总体分类情况一致, 而劳均肉食产量<mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>的分类与总体分类情况不完全一致的情况, 即对于发展水平较高的地级市, 其单位面积粮食产量也属于较高的水平, 但其劳均肉食产量属于较高、中等以及较低水平的皆有, 也就是说单位面积粮食产量<mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>与总体分类相关性较强, 而劳均肉食产量<mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>与总体分类相关性较弱, 此时在聚类分析中若把这两个指标放在同等的地位考虑, 则会影响聚类结果的准确性。与此同时, 当多元数据中各指标数据分别服从不同的分布族时, 该方法也失去了适用性。</p>
                </div>
                <div class="p1">
                    <p id="57">因此, 我们需要一种新的聚类方法, 该方法能够有效处理多元数据的聚类问题, 能适用于各指标数据<mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>服从不同的分布以及各指标数据<mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>与总体聚类的相关性程度不一致的情况, 并且实现数据分类最佳个数的自适应确定。在已有研究的基础上, 本文提出改进的带粘性层次Dirichlet过程 (sticky Hierarchical Dirichlet Process, 即sticky HDP) 方法, 并将其引入到多元数据的整合聚类分析之中, 对多元数据进行降维聚类分析, 并通过模型参数来反映各指标数据与总体聚类之间的相关性, 从而确定哪些指标与总体密切相关, 进而从中提取有用的信息, 以对各指标作进一步的分析研究。</p>
                </div>
                <h3 id="60" name="60" class="anchor-tag">二、Sticky HDP模型的相关理论</h3>
                <h4 class="anchor-tag" id="61" name="61"> (<b>一) 模型介绍</b></h4>
                <h4 class="anchor-tag" id="62" name="62">1.Dirichlet过程</h4>
                <div class="p1">
                    <p id="63">Dirichlet过程是一个应用在非参数贝叶斯模型中的随机过程, 根据Ferguson的研究成果<citation id="219" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, Dirichlet过程的定义如下:假设<i>H</i>是测度空间Θ上的随机概率分布, 对于测度空间Θ的任意一个有限划分<i>A</i><sub>1</sub>, <i>A</i><sub>2</sub>, …, <i>A</i><sub><i>r</i></sub>, 如果测度空间Θ上的随机概率分布<i>G</i>满足以下条件:</p>
                </div>
                <div class="p1">
                    <p id="64"> (<i>G</i> (<i>A</i><sub>1</sub>) , (<i>G</i> (<i>A</i><sub>2</sub>) , …, <i>G</i> (<i>A</i><sub><i>r</i></sub>) ) ～Dir (<i>αH</i> (<i>A</i><sub>1</sub>) , <i>αH</i> (<i>A</i><sub>2</sub>) , …, <i>αH</i> (<i>A</i><sub><i>r</i></sub>) )      (1) </p>
                </div>
                <div class="p1">
                    <p id="65">则<i>G</i>服从以<i>H</i>和<i>α</i>为参数的Dirichlet过程, 记为<i>G</i>～<i>DP</i> (<i>α</i>, <i>H</i>) 。其中, <i>H</i>为基分布, <i>α</i>&gt;0, 为集中度参数, 表示了基分布<i>H</i>和<i>G</i>的相似程度。<i>α</i>越大, 两者越相似。</p>
                </div>
                <div class="p1">
                    <p id="66">以上为Dirichlet过程的定义, 为了实现Dirichlet过程的抽样, 研究者们提出了三种等价形式来构造Dirichlet过程, 分别为Blackwell-MacQueen urn scheme、Chinese restaurant process和stick-breaking构造<citation id="220" type="reference"><link href="197" rel="bibliography" /><link href="199" rel="bibliography" /><link href="201" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>。本文主要应用的是其中的stick-breaking构造形式, 随机概率分布<i>G</i>的生成过程如下。</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∼</mo><mtext>B</mtext><mtext>e</mtext><mtext>t</mtext><mtext>a</mtext><mo stretchy="false"> (</mo><mn>1</mn><mo>, </mo><mi>α</mi><mo stretchy="false">) </mo><mo>, </mo><mi>π</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mn>1</mn><mo>-</mo><mi>β</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∼</mo><mi>Η</mi><mo>, </mo></mtd></mtr><mtr><mtd><mi>k</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo></mtd></mtr><mtr><mtd><mi>G</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>∞</mi></munderover><mi>π</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mi>δ</mi><msub><mrow></mrow><mrow><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">其中, <i>δ</i>为示性函数, </p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><msub><mrow></mrow><mrow><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>, </mo><mi>θ</mi><mo>=</mo><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">为来自于<i>H</i>分布的不重复抽样。通过式 (2) , 可以发现<mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>∞</mi></munderover><mi>π</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mn>1</mn></mrow></math></mathml>, 因此可以将 (<i>π</i><sub>1</sub>, <i>π</i><sub>2</sub>, …) 看作是一个随机概率分布。Sethuraman证明满足式 (2) 的分布函数<i>G</i>是服从DP (<i>α</i>, <i>H</i>) 的一个随机分布<citation id="221" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72">2.Dirichlet过程混合模型</h4>
                <div class="p1">
                    <p id="73">Dirichlet过程混合模型在Dirichlet过程上进行了扩展<citation id="222" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>, 对模型参数用Dirichlet过程作为其先验分布。假设有独立观测数据{<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>n</i></sub>}, 引入指示因子{<i>z</i><sub>1</sub>, <i>z</i><sub>2</sub>, …, <i>z</i><sub><i>n</i></sub>}, 其中指示因子<i>z</i><sub><i>i</i></sub>表示观测数据<i>y</i><sub><i>i</i></sub>的类别标签, <i>θ</i><sub><i>z</i><sub><i>i</i></sub></sub>表示在类别标签<i>z</i><sub><i>i</i></sub>下, 观测数据<i>y</i><sub><i>i</i></sub>服从分布<i>F</i> (<i>θ</i><sub><i>i</i></sub>) , 则观测数据<i>y</i><sub><i>i</i></sub> (<i>i</i>=1, 2, …, <i>n</i>) 的生成过程如下:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∼</mo><mtext>B</mtext><mtext>e</mtext><mtext>t</mtext><mtext>a</mtext><mo stretchy="false"> (</mo><mn>1</mn><mo>, </mo><mi>α</mi><mo stretchy="false">) </mo><mo>, </mo><mi>π</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mn>1</mn><mo>-</mo><mi>β</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∼</mo><mi>Η</mi><mo>, </mo></mtd></mtr><mtr><mtd><mi>k</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo></mtd></mtr><mtr><mtd><mi>G</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>∞</mi></munderover><mi>π</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mi>δ</mi><msub><mrow></mrow><mrow><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub><mo>, </mo><mtext>即</mtext><mspace width="0.25em" /><mi>p</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo>=</mo><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>π</mi><msub><mrow></mrow><mi>k</mi></msub></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75"><i>y</i><sub><i>i</i></sub>|<i>z</i><sub><i>i</i></sub>, (<i>θ</i><sub><i>k</i></sub>) <sup>∞</sup><sub><i>k</i>=1</sub>～<i>F</i> (<i>θ</i><sub><i>z</i><sub><i>i</i></sub></sub>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="76">Dirichlet过程混合模型是有限混合模型的推广, 区别在于其类别个数是无限的, 把{<i>z</i><sub>1</sub>, <i>z</i><sub>2</sub>, …, <i>z</i><sub><i>n</i></sub>}通过求和的形式去掉, 得到如下形式:</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mo>∣</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub><mo>, </mo><mo>⋯</mo><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">[</mo></mstyle><mi>π</mi><msub><mrow></mrow><mn>1</mn></msub><mi>F</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∣</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>+</mo><mi>π</mi><msub><mrow></mrow><mn>2</mn></msub><mi>F</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∣</mo><mi>θ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>+</mo><mo>⋯</mo></mtd></mtr><mtr><mtd><mo>+</mo><mi>π</mi><msub><mrow></mrow><mi>k</mi></msub><mi>F</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∣</mo><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mo>⋯</mo><mo stretchy="false">]</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="78" name="78">3.Sticky HDP模型及其应用</h4>
                <div class="p1">
                    <p id="79">层次Dirichlet过程是Dirichlet过程的推广<citation id="223" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 其形式为:<i>G</i><sub>0</sub>|<i>γ</i>, <i>H</i>～<i>DP</i> (<i>γ</i>, <i>H</i>) , <i>G</i><sub><i>j</i></sub>|<i>α</i>, <i>G</i><sub>0</sub>～<i>DP</i> (<i>α</i>, <i>G</i><sub>0</sub>) 。其中, <i>H</i>是基分布, <i>γ</i>和<i>α</i>是集中度参数。本文提出的带粘性的层次Dirichlet过程 (sticky HDP) 基于Fox、Sudderth和Jordan等提出的sticky HDP-HMM过程<citation id="224" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。Sticky HDP-HMM是在层次Dirichlet过程隐马尔科夫模型 (hierarchical Dirichlet process hidden Markov model, HDP-HMM) 的基础上, 用粘性来反映自身状态的滞留。本文在此基础上做了如下改进:用sticky粘性反映总体聚类和指标数据聚类的相似性;允许不同的状态取不同的分布类型, 即不同的指标服从不同的分布。</p>
                </div>
                <div class="p1">
                    <p id="80">假设有<i>n</i>个观测对象, 每个观测对象有<i>m</i>个观测指标。用<i>X</i>= (<i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>, …, <i>X</i><sub><i>n</i></sub>) <sup>′</sup>表示总体观测数据, <i>X</i><sub><i>i</i></sub>= (<i>x</i><sub><i>i</i>1</sub>, <i>x</i><sub><i>i</i>2</sub>, …, <i>x</i><sub><i>im</i></sub>) 表示每个观测对象的数据, <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mrow><mn>1</mn><mi>j</mi></mrow></msub><mo>, </mo><mi>x</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>j</mi></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>x</mi><msub><mrow></mrow><mrow><mi>n</mi><mi>j</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mo>´</mo></msup></mrow></math></mathml>表示第<i>j</i>个指标中的观测数据, <i>i</i>=1, 2, …, <i>n</i>, <i>j</i>=1, 2, …, <i>m</i>。引入指示因子<i>z</i><sub><i>i</i></sub>表示第<i>i</i>个观测对象在总体聚类中的分类类别, <i>τ</i><sub><i>ij</i></sub>表示第<i>i</i>个观测对象在第<i>j</i>个指标中的分类类别, <i>F</i><sub><i>j</i></sub>表示第<i>j</i>个指标数据服从的分布, 即<i>x</i><sub><i>ij</i></sub>∣<i>θ</i><sub><i>j</i></sub>, <i>τ</i><sub><i>ij</i></sub>=<i>r</i>～<i>F</i><sub><i>j</i></sub> (<i>θ</i><sub><i>jr</i></sub>) , <i>θ</i><sub><i>j</i></sub>= (<i>θ</i><sub><i>j</i>1</sub>, <i>θ</i><sub><i>j</i>2</sub>, …) , 其中 (<i>θ</i><sub><i>j</i>1</sub>, <i>θ</i><sub><i>j</i>2</sub>, …) 为来自于<i>H</i><sub><i>j</i></sub>的不重复抽样。总体分类<i>z</i><sub><i>i</i></sub>和指标对应的分类<i>τ</i><sub><i>ij</i></sub>的关系可以用sticky HDP的层次结构来表示, 其中:<mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><mo stretchy="false">|</mo><mi>γ</mi><mo>∼</mo><mtext>D</mtext><mtext>i</mtext><mtext>r</mtext><mrow><mo> (</mo><mrow><mfrac><mi>γ</mi><mi>Κ</mi></mfrac><mo>, </mo><mo>⋯</mo><mo>, </mo><mfrac><mi>γ</mi><mi>Κ</mi></mfrac></mrow><mo>) </mo></mrow></mrow></math></mathml>, 即数据<i>X</i><sub><i>i</i></sub>属于第<i>k</i>类的概率为:<i>P</i> (<i>z</i><sub><i>i</i></sub>=<i>k</i>) =<i>β</i><sub><i>k</i></sub>, <i>k</i>=1, 2, …, <i>K</i>。</p>
                </div>
                <div class="p1">
                    <p id="83">这里<i>K</i>设为分类数目的上限, 一般情况下根据观测对象的个数设定, 并不需要知道准确的分类数目, 最终的分类数目依赖于观测数据<citation id="225" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="84"><i>π</i><sub><i>j</i></sub>|<i>α</i><sub><i>j</i></sub>, <i>z</i><sub><i>i</i></sub>=<i>k</i>～Dir (<i>α</i><sub><i>j</i></sub><i>β</i><sub>1</sub>, …, <i>α</i><sub><i>j</i></sub><i>β</i><sub><i>k</i>-1</sub>, <i>α</i><sub><i>j</i></sub><i>β</i><sub><i>k</i></sub>+<i>κ</i><sub><i>j</i></sub>, <i>α</i><sub><i>j</i></sub><i>β</i><sub><i>k</i>+1</sub>, …, <i>α</i><sub><i>j</i></sub><i>β</i><sub><i>K</i></sub>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="85">即数据<i>x</i><sub><i>ij</i></sub>在指标数据<mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>的聚类中属于第<i>r</i>类的概率为<i>P</i> (<i>τ</i><sub><i>ij</i></sub>=<i>r</i>∣<i>z</i><sub><i>i</i></sub>=<i>k</i>) =<i>π</i><sub><i>jr</i></sub>。其中, 参数<i>κ</i><sub><i>j</i></sub>是大于零的实数 (见式 (4) ) 。当<i>κ</i><sub><i>j</i></sub>=0时, 该过程退化为分层Dirichlet过程。</p>
                </div>
                <div class="p1">
                    <p id="87">根据Dirichlet分布的性质, <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>τ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>k</mi><mo>∣</mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>k</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><mo stretchy="false"> (</mo><mi>π</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow></msub><mo>∣</mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>k</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow></math></mathml>, 故当指标数据<i>x</i><sub><i>ij</i></sub>在总体聚类中属于第<i>k</i>类时, 其在指标数据<mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>中属于第<i>k</i>类的概率由<mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow></math></mathml>控制, 相比一般的层次Dirichlet过程, 其概率增加了<mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow></math></mathml>。本文定义<mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow></math></mathml>为第<i>j</i>个指标的分类粘性, 其反映了总体聚类和第<i>j</i>个指标数据聚类的一致性。粘性<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow></math></mathml>越大, 指标数据<i>x</i><sub><i>ij</i></sub>在总体聚类与在指标数据<mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>聚类中所属类别一致的概率就越大, 即总体聚类和第<i>j</i>个指标数据聚类的一致性越高。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95"> (<b>二) 参数推断</b></h4>
                <div class="p1">
                    <p id="96">本文用马尔科夫链蒙特卡罗方法 (MCMC) 中的Gibbs抽样算法来实现参数的推断<citation id="226" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。Gibbs抽样算法主要分为Polya Urn Gibbs抽样和Blocked Gibbs抽样, 但是前者的收敛速度比后者慢<citation id="227" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。所以, 本文对于各指标数据采取的是Blocked Gibbs抽样算法来推断参数及实现聚类。</p>
                </div>
                <div class="p1">
                    <p id="97">本文在分析时不限定<i>F</i><sub><i>j</i></sub> (·) 的分布类型, 以下的标记符号和前文保持一致。</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98">1.各参数的先验分布</h4>
                <div class="p1">
                    <p id="99">参数<i>α</i><sub><i>j</i></sub>+<i>κ</i><sub><i>j</i></sub>和<i>α</i><sub><i>j</i></sub>的先验分布。参数<i>α</i><sub><i>j</i></sub>+<i>κ</i><sub><i>j</i></sub>的先验分布为:<i>α</i><sub><i>j</i></sub>+<i>κ</i><sub><i>j</i></sub>～Gamma (<i>a</i>, <i>b</i>) , 参数<i>α</i><sub><i>j</i></sub>的先验分布与<i>α</i><sub><i>j</i></sub>+<i>κ</i><sub><i>j</i></sub>属于同一分布族<citation id="228" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100">2.各参数的后验分布</h4>
                <div class="p1">
                    <p id="101">给定参数的先验分布和观测数据服从sticky HDP模型, 根据贝叶斯公式, 可得到各参数的后验分布, 分别如下。</p>
                </div>
                <div class="p1">
                    <p id="102">1) 参数<i>τ</i><sub><i>ij</i></sub>的后验分布。参数<i>τ</i><sub><i>ij</i></sub>的后验分布为:<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>τ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>∣</mo><mi>π</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>τ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>k</mi><mo stretchy="false">) </mo><mo>∝</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>π</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow></msub><mi>F</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>∣</mo><mi>τ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>τ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml><i>δ</i> (<i>τ</i><sub><i>ij</i></sub>, <i>k</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="106">2) 参数<i>z</i><sub><i>i</i></sub>的后验分布。参数<i>z</i><sub><i>i</i></sub>的后验分布为:</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>, </mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>, </mo><mi>τ</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>, </mo><mi>τ</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>τ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>, </mo><mi>π</mi><mo>, </mo><mi>β</mi><mo stretchy="false">) </mo><mo>∝</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><msubsup><mo>∏</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mi>π</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>τ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></msub><mi>F</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>∣</mo><mi>τ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>τ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></msub><mo stretchy="false">) </mo><mi>δ</mi><mo stretchy="false"> (</mo><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>k</mi><mo stretchy="false">) </mo><mo>。</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">3) 参数<i>β</i>的后验分布。参数<i>β</i>的后验分布为: (<i>β</i><sub>1</sub>, <i>β</i><sub>2</sub>, …, <i>β</i><sub><i>K</i></sub>) ～Dir (<i>γ</i>/<i>K</i>+<i>n</i><sub>1</sub>, …, <i>γ</i>/<i>K</i>+<i>n</i><sub><i>K</i></sub>) , 其中, <i>n</i><sub><i>k</i></sub>表示总体观测数据中属于类别<i>k</i>的个数。</p>
                </div>
                <div class="p1">
                    <p id="111">4) 参数<i>π</i><sub><i>j</i></sub>的后验分布。参数<i>π</i><sub><i>j</i></sub>的后验分布为:<mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>π</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∣</mo><mi>τ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>k</mi><mo>∼</mo><mtext>D</mtext><mtext>i</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>β</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>m</mi><msub><mrow></mrow><mrow><mi>j</mi><mn>1</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>β</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>m</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>, </mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>m</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow></msub><mo>, </mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>β</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>m</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>β</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo>+</mo><mi>m</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>Κ</mi></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>, 其中, <i>m</i><sub><i>jk</i></sub>表示在总体聚类属于第<i>k</i>类的各数据<i>X</i><sub><i>i</i></sub>中, 其对应的各指标数据<i>x</i><sub><i>ij</i></sub>在指标数据的聚类中属于第<i>k</i>类的个数。</p>
                </div>
                <div class="p1">
                    <p id="114">5) 参数<i>α</i><sub><i>j</i></sub>+<i>κ</i><sub><i>j</i></sub>的后验分布。参数<i>α</i><sub><i>j</i></sub>+<i>κ</i><sub><i>j</i></sub>的后验分布为<citation id="229" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>:<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∣</mo><mi>Κ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>n</mi><mo>, </mo><mi>ω</mi><mo stretchy="false">) </mo><mo>∝</mo><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mi>a</mi><mo>+</mo><mi>Κ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mn>1</mn></mrow></msup><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>b</mi><mo>-</mo><mi>log</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></msup><mo>+</mo><mi>n</mi><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mi>a</mi><mo>+</mo><mi>Κ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mn>2</mn></mrow></msup><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>b</mi><mo>-</mo><mi>log</mi><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></msup></mrow></math></mathml>, 其中, <i>ω</i>为辅助变量, <i>ω</i>～Beta (<i>α</i><sub><i>j</i></sub>+<i>κ</i><sub><i>j</i></sub>+1, <i>n</i>) 。参数<i>γ</i>的推断与<i>α</i><sub><i>j</i></sub>+<i>κ</i><sub><i>j</i></sub>相似, 可以参考<i>α</i><sub><i>j</i></sub>+<i>κ</i><sub><i>j</i></sub>的推断过程。</p>
                </div>
                <div class="p1">
                    <p id="117">6) 参数<i>α</i><sub><i>j</i></sub>的后验分布。参数<i>α</i><sub><i>j</i></sub>的后验分布为:</p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∣</mo><mi>β</mi><mo>, </mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>∝</mo><mi>α</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>a</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mi>b</mi><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msup><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow></mrow></mstyle><msup><mrow></mrow><mrow><mi>n</mi><msubsup><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow><mo>*</mo></msubsup></mrow></msup></mtd></mtr><mtr><mtd><mrow><mo> (</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>κ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mi>n</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi>n</mi><msubsup><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow><mo>*</mo></msubsup></mrow></msup></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">, 其中, <i>n</i><sup>*</sup><sub><i>jk</i></sub>表示数据<i>x</i><sub><i>ij</i></sub>在指标数据<mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>的聚类中所属类别与数据<i>X</i><sub><i>i</i></sub>在总体数据的聚类中所属类别一致 (都为<i>k</i>) 的个数。</p>
                </div>
                <h3 id="122" name="122" class="anchor-tag">三、仿真模拟结果与分析</h3>
                <div class="p1">
                    <p id="123">为了验证本文所描述的用sticky HDP模型对由多个指标组成的多元数据进行聚类的有效性, 分别对仿真模拟数据和IRIS数据集进行了聚类分析。在进行聚类时, 运行Gibbs抽样迭代1 500次, 由于仿真模拟数据的参数在300次左右达到平稳, IRIS数据集的参数在200次左右达到平稳, 故都取前500次作为预热抽样数目, 然后在500～1 500次迭代中, 每间隔5次迭代保留一次参数样本, 选取出现概率最大的类别个数作为最终类别个数, 然后选取该类别个数下极大似然函数值对应的分类结果作为总体数据最终的聚类结果。对于仿真模拟数据, 在模型的初始设置中, 对聚类类别的上限个数尝试了多个数值, 分别有<i>K</i>=20, 25, 30等。结果显示该初始设置足够大时, 对参数估计和最终的总体分类结果没有影响。</p>
                </div>
                <div class="p1">
                    <p id="124">对于仿真模拟数据和IRIS数据集, 真实的类别已知。本文将分别运用不同的聚类方法对数据集进行聚类分析, 以分类的准确率作为标准来判断不同聚类方法的优劣。其中, 对于模拟数据, 所有的运行结果都是基于30次运行结果的平均值, 而结果后面括号里的数值是标准差。</p>
                </div>
                <div class="p1">
                    <p id="125">本文运行算法的计算机硬件配置如下:Inter (R) Core (TM) i5-5200U CPU@2.20 GHz, 8G内存;操作系统为Windows 10;程序运行软件环境:R 3.5.0。</p>
                </div>
                <h4 class="anchor-tag" id="126" name="126"> (<b>一) 仿真模拟数据</b></h4>
                <div class="p1">
                    <p id="127">用sticky HDP的结构生成了12组<i>n</i>=1 000, <i>m</i>=8, 真实的分类数目为10的样本数据。其中, <mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>1</mn></msub></mrow></math></mathml>、<mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>2</mn></msub></mrow></math></mathml>、<mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>3</mn></msub></mrow></math></mathml>服从混合正态分布, <mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>4</mn></msub></mrow></math></mathml>、<mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>5</mn></msub></mrow></math></mathml>服从混合泊松分布, <mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>6</mn></msub></mrow></math></mathml>、<mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>7</mn></msub></mrow></math></mathml>服从混合指数分布, <mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>8</mn></msub></mrow></math></mathml>服从混合伽马分布。各指标数据中各成分模型对应的参数如表1所示。在<i>MCMC</i>算法中, 各参数先验分布的设置如表2和表3所示。</p>
                </div>
                <div class="area_img" id="136">
                    <p class="img_tit"><b>表</b>1 <b>各指标数据中各成分模型对应参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="136" border="1"><tr><td>指标</td><td></td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr><tr><td><br />指标一</td><td>μ<sub>1</sub></td><td>-80</td><td>-60</td><td>-40</td><td>-20</td><td>0</td><td>20</td><td>40</td><td>60</td><td>80</td><td>100</td></tr><tr><td><br /></td><td>σ<sub>1</sub></td><td>7.0</td><td>6.3</td><td>5.6</td><td>4.9</td><td>4.2</td><td>3.5</td><td>2.8</td><td>2.1</td><td>1.4</td><td>0.7</td></tr><tr><td><br />指标二</td><td>μ<sub>2</sub></td><td>-75</td><td>-57</td><td>-39</td><td>-21</td><td>-3</td><td>15</td><td>33</td><td>51</td><td>69</td><td>87</td></tr><tr><td><br /></td><td>σ<sub>2</sub></td><td>6.0</td><td>5.4</td><td>4.8</td><td>4.2</td><td>3.6</td><td>3.0</td><td>2.4</td><td>1.8</td><td>1.2</td><td>0.6</td></tr><tr><td><br />指标三</td><td>μ<sub>3</sub></td><td>-60</td><td>-45</td><td>-30</td><td>-15</td><td>0</td><td>15</td><td>30</td><td>45</td><td>60</td><td>75</td></tr><tr><td><br /></td><td>σ<sub>3</sub></td><td>5.0</td><td>4.5</td><td>4.0</td><td>3.5</td><td>3.0</td><td>2.5</td><td>2.0</td><td>1.5</td><td>1.0</td><td>0.5</td></tr><tr><td><br />指标四</td><td>λ<sub>4</sub></td><td>5</td><td>10</td><td>15</td><td>20</td><td>25</td><td>30</td><td>35</td><td>40</td><td>45</td><td>50</td></tr><tr><td><br />指标五</td><td>λ<sub>5</sub></td><td>10</td><td>20</td><td>30</td><td>40</td><td>50</td><td>60</td><td>70</td><td>80</td><td>90</td><td>100</td></tr><tr><td><br />指标六</td><td>λ<sub>6</sub></td><td>3</td><td>6</td><td>9</td><td>12</td><td>15</td><td>18</td><td>21</td><td>24</td><td>27</td><td>30</td></tr><tr><td><br />指标七</td><td>λ<sub>7</sub></td><td>5</td><td>10</td><td>15</td><td>20</td><td>25</td><td>30</td><td>35</td><td>40</td><td>45</td><td>50</td></tr><tr><td><br />指标八</td><td>a<sub>8</sub></td><td>3</td><td>6</td><td>9</td><td>12</td><td>15</td><td>18</td><td>21</td><td>24</td><td>27</td><td>30</td></tr><tr><td><br /></td><td>b<sub>8</sub></td><td>3.0</td><td>2.7</td><td>2.4</td><td>2.1</td><td>1.8</td><td>1.5</td><td>1.2</td><td>0.9</td><td>0.6</td><td>0.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="180">
                                            <p class="img_tit">
                                                表2 各指标数据参数的先验分布
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJLT201908004_18000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/TJLT201908004_18000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/TJLT201908004_18000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 各指标数据参数的先验分布" src="Detail/GetImg?filename=images/TJLT201908004_18000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="138">
                                            <p class="img_tit">
                                                <b>表</b>3 <b>各超参数的先验分布</b>
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJLT201908004_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/TJLT201908004_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/TJLT201908004_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 各超参数的先验分布" src="Detail/GetImg?filename=images/TJLT201908004_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="140">为了验证指标数据粘性大小对于模型最终聚类结果准确率的影响, 在生成数据时, 设置了多组不同的粘性参数, 如表4所示。其中第四组数据粘性参数的设置, 是为了验证当各个指标数据的粘性大小不同时, 对最终聚类结果准确率的影响, 即检验模型的稳健性。模拟数据聚类结果的准确率见表4。其中, 准确率定义为样本分类结果与其真实分类一致的比例。</p>
                </div>
                <div class="area_img" id="141">
                    <p class="img_tit"><b>表</b>4 <b>总体数据聚类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="141" border="1"><tr><td rowspan="2"><br />模拟<br />组别</td><td colspan="8"><br />粘性</td><td rowspan="2">准确率</td></tr><tr><td><br />指标一</td><td>指标二</td><td>指标三</td><td>指标四</td><td>指标五</td><td>指标六</td><td>指标七</td><td>指标八</td></tr><tr><td>一</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>99.68% (0.42) </td></tr><tr><td><br />二</td><td>0.90</td><td>0.90</td><td>0.90</td><td>0.90</td><td>0.90</td><td>0.90</td><td>0.90</td><td>0.90</td><td>96.38% (0.98) </td></tr><tr><td><br />三</td><td>0.80</td><td>0.80</td><td>0.80</td><td>0.80</td><td>0.80</td><td>0.80</td><td>0.80</td><td>0.80</td><td>92.34% (1.52) </td></tr><tr><td><br />四</td><td>0.80</td><td>0.80</td><td>0.50</td><td>0.70</td><td>0.10</td><td>0.50</td><td>0.20</td><td>0.20</td><td>85.21% (2.09) <sup>*1</sup></td></tr><tr><td><br />五</td><td>0.70</td><td>0.70</td><td>0.70</td><td>0.70</td><td>0.70</td><td>0.70</td><td>0.70</td><td>0.70</td><td>82.59% (1.64) </td></tr><tr><td><br />六</td><td>0.60</td><td>0.60</td><td>0.60</td><td>0.60</td><td>0.60</td><td>0.60</td><td>0.60</td><td>0.60</td><td>71.67% (1.87) </td></tr><tr><td><br />七</td><td>0.50</td><td>0.50</td><td>0.50</td><td>0.50</td><td>0.50</td><td>0.50</td><td>0.50</td><td>0.50</td><td>54.58% (2.06) </td></tr><tr><td><br />八</td><td>0.40</td><td>0.40</td><td>0.40</td><td>0.40</td><td>0.40</td><td>0.40</td><td>0.40</td><td>0.40</td><td>32.51% (1.67) <sup>*2</sup></td></tr><tr><td><br />九</td><td>0.30</td><td>0.30</td><td>0.30</td><td>0.30</td><td>0.30</td><td>0.30</td><td>0.30</td><td>0.30</td><td>20.94% (1.72) </td></tr><tr><td><br />十</td><td>0.20</td><td>0.20</td><td>0.20</td><td>0.20</td><td>0.20</td><td>0.20</td><td>0.20</td><td>0.20</td><td>17.59% (1.62) </td></tr><tr><td><br />十一</td><td>0.10</td><td>0.10</td><td>0.10</td><td>0.10</td><td>0.10</td><td>0.10</td><td>0.10</td><td>0.10</td><td>15.04% (1.51) </td></tr><tr><td><br />十二</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>14.45% (1.57) <sup>*3</sup></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="142">从表4可以看出, 指标数据的粘性越大, 总体数据聚类结果的准确率越高。当指标数据的粘性有高有低, 如表4中的第四组总体数据, 指标数据中粘性最大的有0.8, 最小的仅0.1, 而最终聚类结果的准确率为85.21% (见表4中*1处) , 这说明了sticky HDP模型的稳健性, 即当指标数据中有粘性很小的数据时, 对sticky HDP模型聚类的准确率影响不大。当指标数据的粘性都较小, 仅0.4时, 总体数据聚类结果的准确率骤降, 从粘性0.5时的54.58%下降到32.51% (见表4中*2处) , 说明当指标数据的粘性低于0.5时, 即指标数据与总体数据聚类之间的相关性都较弱时, 总体聚类的精度较差。当指标数据都没有粘性, 即粘性为0时 (见表4中*3处) , 总体聚类和指标数据聚类相互独立, 该准确率为简单随机情况下的结果。</p>
                </div>
                <div class="p1">
                    <p id="143">同时, 分别运用K-均值聚类方法、最短距离法和最长距离法对该仿真模拟数据进行聚类分析, 聚类对比结果见表5。其中各模拟组别的指标数据粘性大小设置与表4一致, K-均值聚类方法和系统聚类法的聚类个数均设为真实值10。</p>
                </div>
                <div class="p1">
                    <p id="144">根据表5的结果, 以准确率为标准, 四种聚类方法的优劣次序依次是sticky HDP聚类方法、K-均值聚类方法、最长距离聚类法以及最短距离聚类法。其中, 当指标数据的粘性较大时, sticky HDP聚类方法的准确率明显高于其他方法, 显著提高了分类质量。同时, sticky HDP聚类方法还不需要事先知道确切的分类数目, 均证实了用该模型对多元数据进行聚类的有效性。</p>
                </div>
                <div class="area_img" id="145">
                    <p class="img_tit"><b>表</b>5 <b>不同聚类方法的聚类结果 (基于准确率</b>)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="145" border="1"><tr><td><br />模拟组别</td><td>K-均值</td><td>最短距离法</td><td>最长距离法</td><td>sticky HDP<br />聚类方法</td></tr><tr><td><br />一</td><td>71.58% (4.17) </td><td>12.90% (0.81) </td><td>48.97% (3.52) </td><td>99.68% (0.42) </td></tr><tr><td><br />二</td><td>52.72% (3.74) </td><td>12.52% (0.89) </td><td>34.52% (2.80) </td><td>96.38% (0.98) </td></tr><tr><td><br />三</td><td>38.99% (3.28) </td><td>12.91% (1.07) </td><td>33.61% (3.32) </td><td>92.34% (1.52) </td></tr><tr><td><br />四</td><td>30.14% (2.82) </td><td>12.40% (0.97) </td><td>26.87% (2.51) </td><td>85.21% (2.09) </td></tr><tr><td><br />五</td><td>32.68% (3.01) </td><td>12.66% (0.95) </td><td>27.76% (2.92) </td><td>82.59% (1.64) </td></tr><tr><td><br />六</td><td>27.58% (2.58) </td><td>12.65% (1.05) </td><td>24.50% (2.43) </td><td>71.67% (1.87) </td></tr><tr><td><br />七</td><td>24.07% (2.29) </td><td>12.58% (0.84) </td><td>22.40% (1.92) </td><td>54.58% (2.06) </td></tr><tr><td><br />八</td><td>20.23% (1.89) </td><td>12.72% (0.93) </td><td>18.97% (1.81) </td><td>32.51% (1.67) </td></tr><tr><td><br />九</td><td>18.51% (2.01) </td><td>12.49% (0.82) </td><td>17.95% (1.74) </td><td>20.94% (1.72) </td></tr><tr><td><br />十</td><td>16.06% (1.73) </td><td>12.69% (0.94) </td><td>15.71% (1.51) </td><td>17.59% (1.62) </td></tr><tr><td><br />十一</td><td>13.96% (1.88) </td><td>12.57% (1.01) </td><td>13.77% (1.62) </td><td>15.04% (1.51) </td></tr><tr><td><br />十二</td><td>13.78% (1.67) </td><td>12.63% (0.87) </td><td>13.50% (1.53) </td><td>14.45% (1.57) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="146" name="146"> (<b>二</b>) IRIS<b>数据集</b></h4>
                <div class="p1">
                    <p id="147">本文以经典的IRIS数据集作为测试数据集。该数据集是从加拿大加斯帕半岛上鸢尾花的花朵中提取的地理变异数据, 其中包含150个鸢尾花花朵样本, 分属于鸢尾花下的3个类别, 分别是山鸢尾、变色鸢尾和维吉尼亚鸢尾。每个类别中有50个样本, 每个样本包含鸢尾花的花萼和花瓣的长度及宽度4个特征属性, 即萼片长度 (Sepal Length) 、萼片宽度 (Sepal Width) 、花瓣长度 (Petal Length) 、花瓣宽度 (Petal Width) 。</p>
                </div>
                <div class="p1">
                    <p id="148">在sticky HDP算法的初始化中, 聚类的类别个数设置为K=15。最终的聚类结果见表6。</p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit"><b>表</b>6 <b>基于</b>sticky HDP<b>模型的鸢尾花数据聚类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="149" border="1"><tr><td><br />实际类别数</td><td>聚类结果类别数</td><td>准确率</td></tr><tr><td><br />3</td><td>3</td><td>92.67%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="150">根据表6的结果, sticky HDP聚类方法能够确定鸢尾花数据的真实类别个数, 且能保证准确率, 准确率达到92.67%。同时, 分别运用基于K-均值聚类方法、最短距离法、最长距离法以及Dirichlet过程聚类方法对该数据集进行聚类分析, 聚类对比结果见表7。其中, K-均值聚类方法、最短距离法和最长距离法的聚类个数均设为真实值3。</p>
                </div>
                <div class="area_img" id="151">
                    <p class="img_tit"><b>表</b>7 <b>不同聚类方法的聚类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="151" border="1"><tr><td>聚类方法</td><td>K-均值聚<br />类方法</td><td>最短距<br />离法</td><td>最长距<br />离法</td><td>Dirichlet过<br />程聚类方法</td><td>sticky HDP<br />聚类方法</td></tr><tr><td><br />准确率</td><td>89.33%</td><td>68.00%</td><td>84.00%</td><td>70.67%</td><td>92.67%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="152">由表7可以看出, 以准确率为标准, 几种聚类方法的优劣次序依次是sticky HDP聚类方法、K-均值聚类方法、最长距离法、Dirichlet过程聚类方法和最短距离法。其中, sticky HDP聚类方法的准确率高达92.67%, 比准确率最低的最短距离法的准确率高出24.67%。由于IRIS数据集中各特征数据处于不同类别时重叠较少, 所以K-均值和最长距离法在这种情况下比较适用, 但sticky HDP方法仍得到更高的聚类准确率, 并且不需要事先知道确切的分类数目, 这都证实了用该模型对IRIS数据集进行聚类的有效性。</p>
                </div>
                <div class="p1">
                    <p id="153">基于表6的聚类结果, 进一步估计4个特征属性数据的一致性与粘性, 结果如表8所示。其中, <mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>的一致性<mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>=</mo><mfrac><mrow><mi>n</mi><msubsup><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow><mo>*</mo></msubsup></mrow><mi>n</mi></mfrac><mo>, </mo><mi>n</mi><msubsup><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow><mo>*</mo></msubsup></mrow></math></mathml>表示数据<i>x</i><sub><i>ij</i></sub>在指标数据<mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>的聚类中所属类别与数据<i>X</i><sub><i>i</i></sub>在总体数据的聚类中所属类别一致 (都为<i>k</i>) 的个数。</p>
                </div>
                <div class="area_img" id="157">
                    <p class="img_tit"><b>表</b>8 <b>鸢尾花的特征属性数据的一致性与粘性结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="157" border="1"><tr><td><br />特征属性</td><td>一致性</td><td>粘性</td></tr><tr><td><br />花瓣宽度 (Petal Width) </td><td>0.817</td><td>0.803</td></tr><tr><td><br />花瓣长度 (Petal Length) </td><td>0.563</td><td>0.526</td></tr><tr><td><br />萼片长度 (Sepal Length) </td><td>0.512</td><td>0.453</td></tr><tr><td><br />萼片宽度 (Sepal Width) </td><td>0.382</td><td>0.329</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="158">从表8的结果可以看出, 特征属性花瓣宽度的粘性最大, 有0.803, 这说明该指标与鸢尾花类别的相关性很高, 即当该指标中的数据在指标数据的聚类中属于某一类时, 其在总体聚类中也属于该类的概率很大 (有0.817) , 根据该指标数据的类别能够在很大程度上判断鸢尾花的类别。而特征属性萼片宽度的粘性最小, 仅0.329, 这说明该指标与鸢尾花类别之间的相关性很弱, 即当该指标中的数据在指标数据的聚类中属于某一类时, 其在总体聚类中也属于该类的概率不大 (仅0.382) , 也就是说根据该指标判断鸢尾花类别的准确率很低。</p>
                </div>
                <div class="p1">
                    <p id="159">最后, 对该数据集下每个特征属性的数据进行直观分析, 来进一步验证该聚类方法的结果, 每个特征属性数据的直方图见图1。从该直方图可以看出, 特征属性中花瓣宽度的数据可能来自三个分布的混合, 即有三个类别, 这与该数据集下鸢尾花类别的数目一致, 则该指标数据的粘性可能较大;而萼片宽度数据可能只来自一个分布, 这与该数据集下鸢尾花类别的数目相差较大, 则该指标数据的粘性可能较小。该直观分析与表8中估计的粘性结果一致, 进一步证实了sticky HDP聚类方法的有效性。</p>
                </div>
                <div class="area_img" id="160">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/TJLT201908004_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 4个特征属性数据的直方图" src="Detail/GetImg?filename=images/TJLT201908004_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 4<b>个特征属性数据的直方图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/TJLT201908004_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="161" name="161" class="anchor-tag">四、结 论</h3>
                <div class="p1">
                    <p id="162">多元数据的各指标服从的分布不同以及相关性的差异会影响聚类方法的适用性、复杂性和准确率。本文提出改进的sticky HDP模型, 旨在解决指标服从不同分布的问题, 实现多元数据的降维聚类, 同时用模型参数来反映指标数据与总体聚类之间的相关性, 最后用仿真模拟数据和实例证明了该方法的有效性。指标数据粘性的大小, 可以直接反映指标数据与总体数据聚类之间的相关性程度高低。指标与总体聚类的相关性越大, 则相应的粘性参数越大, 从而反映该指标在总体聚类中的重要性程度越高, 这为研究多元数据的聚类、各指标与总体聚类之间的相关性提供了一个新的方法和角度。当各指标数据中有粘性较大的指标时, sticky HDP聚类方法明显优于其他聚类方法, 能够显著提高分类质量。而当指标数据与总体数据聚类之间的粘性都很小, 即分类的相关性都很弱时, 对总体数据进行聚类的意义并不大。 对于仿真模拟数据, sticky HDP算法运行的总时间为0.82h, 每次Gibbs抽样过程的平均运行时间为1.96s;对于IRIS数据集, sticky HDP算法运行的总时间为3.78min, 每次Gibbs抽样过程的平均运行时间为0.15s。本文的实例只分析了IRIS数据集, 后续还可以把这个方法应用到其他领域的数据集上, 以进行进一步的挖掘, 得到更多有用的信息。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="181">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1014085485.nh&amp;v=MTQ5MTMzenFxQnRHRnJDVVI3cWZadVpuRmlEbFZMM1BWRjI2R3JPd0c5WEVxcEViUElRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 朱绍军.模型自动选择聚类算法的研究与应用[D].宁波:宁波大学, 2014.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787300208480000&amp;v=MTk1MjhIdFBNcDR0TlpPc1BEQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWmVadkZ5bnNVcmZJSjF3VFhGcXpHYkM0&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 何晓群.多元统计分析[M].第4版.北京:中国人民大学出版社, 2015.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787115241009002&amp;v=MTc0ODllc1BEaE04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWmVadkZ5bnNVcmZJSjF3VFhGcXpHYks1RzlQSXJvOUZi&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Pang-NingT, Michael S, Vipin K.数据挖掘导论 (完整版) [M].第2版.北京:人民邮电出版社, 2011.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14112600625536&amp;v=MjAyMzhxUVRNbndaZVp0RmlubFVyYkpLRjRUYUJVPU5pZlllcks4SDlET3FZOUZZdWtLQ1g4L29CTVQ2VDRQUUgvaXJSZEdlcg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Antoniak C E.Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems[J].The Annals of Statistics, 1974 (6) .
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MESS200905003&amp;v=MDc4NzJxbzlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpuRmlEbFZMM1BLQ2pZZmJHNEh0ak0=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 徐谦, 周俊生, 陈家骏.Dirichlet过程及其在自然语言处理中的应用[J].中文信息学报, 2009 (5) .
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJLT201901004&amp;v=MTgwNzVGWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpuRmlEbFZMM1BNU2ZIZXJHNEg5ak1ybzk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 李涛, 张景肖.基于BT-SVM模型组合的动态加权多分类算法研究[J].统计与信息论坛, 2019 (1) .
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GJSX2016Z2003&amp;v=MDU2NjY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVabkZpRGxWTDNQSWlmWWRyRzRIOWVtclk5Rlo0UUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 李松, 谢新新, 刘东林, 等.基于Dirichlet过程的无线视频码率变化识别算法[J].高技术通讯, 2016 (Z2) .
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14112600625288&amp;v=MjgyNzRubFVyYkpLRjRUYUJVPU5pZlllcks4SDlET3FZOUZZdWtLRG5ReG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Ferguson T S.A Bayesian Analysis of Some Nonparametric Problems[J].Annals of Statistics, 1973 (2) .
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14112600625300&amp;v=MzE4MTVKS0Y0VGFCVT1OaWZZZXJLOEg5RE9xWTlGWXVrS0QzdzVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> BlackwellD, MacQueen J B.Ferguson Distributions via Pólya Urn Schemes[J].Annals of Statistics, 1973 (2) .
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Combinatorial Stochastic Processes">

                                <b>[10]</b> Pitman J.Combinatorial Stochastic Processes[R].Univecsity of California at Berkwell Department of Statistics, 2002.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJSTCBB04F73D4F5D24BFFF52D19436F603F&amp;v=MDMxMTd0Rmd6TGk0d3E4PU5pZlllc0RLYk5ISTJZaEdFTzk1Q1FnN3kyUmxuRWw0U2d2anBSWTJmOFNTUmJucENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Sethuraman J.A Constructive Definition of the Dirichlet Prior[J].Statistica Sinica, 1994 (2) .
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800006293&amp;v=MzA1MjJiSktGNFRhQlU9TmpuQmFySzdIdGZPcDQ5RlpPc0pEblU2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Teh Y W, Jordan M I, Beal M J, et al.Hierarchical Dirichlet Processes[J].Journal of the American Statistical Association, 2006 (1) .
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST15012900203933&amp;v=MjA3MzdCVT1OaWZZZXJLOUh0RE9wbzlGWnVzTUJYODZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyYkpLRjRUYQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Fox EB, Sudderth E B, Jordan M I, et al.A Sticky HDP-HMM with Application to Speaker Diarization[J].The Annals of Applied Statistics, 2011 (2A) .
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340541&amp;v=MjkwMTRrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaUhrVzcvUElsaz1OajdCYXJPNEh0SE5ySXRGWWU4T1kz&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Andrieu C, Freitas N D, Doucet A.An Introduction to MCMC for Machine Learning[J].Machine Learning, 2003 (1/2) .
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800007394&amp;v=MTk0NjZzSUQzVTlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyYkpLRjRUYUJVPU5qbkJhcks3SHRmT3A0OUZaTw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Ishwaran J, James L.Gibbs Sampling Methods for Stick-breaking Priors[J].Journal of the American Statistical Association, 2001 (4) .
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hyperparameter Estimation in Dirichlet Process Mixture Models">

                                <b>[16]</b> West M.Hyperparameter Estimation in Dirichlet Process Mixture Models[R].Durham:Duke Univecsity, 1992.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="TJLT201908004" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TJLT201908004&amp;v=MTY0NDJmSGVyRzRIOWpNcDQ5RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVabkZpRGxWTDNQTVM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZEYVI3MHF6ek1oNHI0dHhJUW1XTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=vAscMyvIPP9NePnbGPkqJ0A5tHOvnzl65tGuRDsf9xg1" rel="stylesheet"/>

</body>
</html>

