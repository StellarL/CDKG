<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136763732315000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201905020%26RESULT%3d1%26SIGN%3dYCVqN4az%252f%252fik%252fAbcwP5HqIaOLgs%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905020&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905020&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905020&amp;v=MTg2ODFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEbVc3L05MejdCZDdHNEg5ak1xbzlIWklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#49" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#50" data-title="1.1 AdaBoost&lt;b&gt;算法&lt;/b&gt;">1.1 AdaBoost<b>算法</b></a></li>
                                                <li><a href="#66" data-title="1.2 &lt;b&gt;间隔理论&lt;/b&gt;">1.2 <b>间隔理论</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#82" data-title="2 基于间隔理论的过采样方法 ">2 基于间隔理论的过采样方法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#105" data-title="3 MOSBoost算法 ">3 MOSBoost算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#112" data-title="4 实验与分析 ">4 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#113" data-title="4.1 &lt;b&gt;分类性能评价准则&lt;/b&gt;">4.1 <b>分类性能评价准则</b></a></li>
                                                <li><a href="#126" data-title="4.2 UCI &lt;b&gt;数据集分析&lt;/b&gt;">4.2 UCI <b>数据集分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#131" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#111" data-title="图1 MOSBoost算法流程">图1 MOSBoost算法流程</a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;二分类问题的混淆矩阵&lt;/b&gt;"><b>表</b>1 <b>二分类问题的混淆矩阵</b></a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;实验中使用的&lt;/b&gt;UCI&lt;b&gt;数据集信息&lt;/b&gt;"><b>表</b>2 <b>实验中使用的</b>UCI<b>数据集信息</b></a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;几种算法性能比较&lt;/b&gt;"><b>表</b>3 <b>几种算法性能比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="150">


                                    <a id="bibliography_1" title=" DAI H L.Class imbalance learning via a fuuzy total margin based support vector machine[J].Applied Soft Computing, 2015, 31 (C) :172-184." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES325859E5EA0E0649383D3D6BB283A39F&amp;v=MDE3OTVwQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekx1M3dLMD1OaWZPZmJDNkc5bkpwdnBBRVpvUGVYdy95eDhRNGp3SlN3dmszbUEzY2JIbFJyUA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         DAI H L.Class imbalance learning via a fuuzy total margin based support vector machine[J].Applied Soft Computing, 2015, 31 (C) :172-184.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_2" title=" 谭洁帆, 朱焱, 陈同孝, 等.基于卷积神经网络和代价敏感的不平衡图像分类方法[J].计算机应用, 2018, 38 (7) :1862-1865, 1871. (TAN J F, ZHU Y, CHEN T X, et al.Imbalanced image classification approach based on convolution network and cost-sensitivity[J].Journal of Computer Applications, 2018, 38 (7) :1862-1865, 1871.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201807007&amp;v=MDY0NTJPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG1XNy9OTHo3QmQ3RzRIOW5NcUk5Rlk0UUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         谭洁帆, 朱焱, 陈同孝, 等.基于卷积神经网络和代价敏感的不平衡图像分类方法[J].计算机应用, 2018, 38 (7) :1862-1865, 1871. (TAN J F, ZHU Y, CHEN T X, et al.Imbalanced image classification approach based on convolution network and cost-sensitivity[J].Journal of Computer Applications, 2018, 38 (7) :1862-1865, 1871.) 
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_3" title=" WANG S, YAO X.Using class imbalance learning for software defect prediction[J].IEEE Transactions on Reliability, 2013, 62 (2) :434-443." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using class imbalance learning for software defect prediction">
                                        <b>[3]</b>
                                         WANG S, YAO X.Using class imbalance learning for software defect prediction[J].IEEE Transactions on Reliability, 2013, 62 (2) :434-443.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_4" title=" OZCIFT A, GULTEN A.Classifer ensemble construction with rotation forest to improve medical diagnosis performance of machine learning algorithms[J].Computer Methods and Programs in Biomedicine, 2011, 104 (3) :443-451." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300239204&amp;v=MDUwOTAzSUtGMGNhaGM9TmlmT2ZiSzdIdERPckk5Rlp1Z0dEbnc5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         OZCIFT A, GULTEN A.Classifer ensemble construction with rotation forest to improve medical diagnosis performance of machine learning algorithms[J].Computer Methods and Programs in Biomedicine, 2011, 104 (3) :443-451.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_5" title=" YU H, NI J, ZHAO J.ACOSampling:an ant colony optimization-based undersampling method for classifying imbalanced DNA microarray data[J].Neurocomputing, 2013, 101:309-318." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600369818&amp;v=MDM0NjFGMGNhaGM9TmlmT2ZiSzhIdERNcVk5RlorMEdCSDB4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         YU H, NI J, ZHAO J.ACOSampling:an ant colony optimization-based undersampling method for classifying imbalanced DNA microarray data[J].Neurocomputing, 2013, 101:309-318.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_6" title=" TOMEK I.Two modifications of CNN[J].IEEE Transactions on Systems, Man and Cybernetics, 1976, SMC-6 (11) :769-772." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Two Modifications of CNN">
                                        <b>[6]</b>
                                         TOMEK I.Two modifications of CNN[J].IEEE Transactions on Systems, Man and Cybernetics, 1976, SMC-6 (11) :769-772.
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_7" title=" KUBAT M, MATWIN S.Addressing the curse of imbalanced training sets:one-sided selection[C]// Proceedings of the 14th International Conference on Machine Learning.San Francisco:Morgan Kaufmann, 1997:179-186." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Addressing the curse of imbalanced training sets: one-sided selection">
                                        <b>[7]</b>
                                         KUBAT M, MATWIN S.Addressing the curse of imbalanced training sets:one-sided selection[C]// Proceedings of the 14th International Conference on Machine Learning.San Francisco:Morgan Kaufmann, 1997:179-186.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_8" title=" LAURIKKALA J.Improving identification of difficult small classes by balancing class distribution[C]// Proceedings of the 8th Conference on Artificial Intelligence in Medicine in Europe.Berlin:Springer, 2001:63-66." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving Identification of Difficult Small Classes by Balancing Class Distribution">
                                        <b>[8]</b>
                                         LAURIKKALA J.Improving identification of difficult small classes by balancing class distribution[C]// Proceedings of the 8th Conference on Artificial Intelligence in Medicine in Europe.Berlin:Springer, 2001:63-66.
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_9" title=" CHAWLA N, BOWYER K, HALL L, et al.SMOTE:synthetic minority over-sampling technique[J].Journal of Artificial Intelligence Research, 2002, 16 (1) :321-357." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SMOTE: synthetic minority over-sampling technique">
                                        <b>[9]</b>
                                         CHAWLA N, BOWYER K, HALL L, et al.SMOTE:synthetic minority over-sampling technique[J].Journal of Artificial Intelligence Research, 2002, 16 (1) :321-357.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_10" title=" RIVERA W A.Noise reduction a priori synthetic over-sampling for class imbalanced data sets[J].Information Sciences, 2017, 408 (C) :146-161." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1D6C57643828D7BD8076944F2A8B743F&amp;v=MTEwNzR2bTJoQkVjY0NUUWJucENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMdTN3SzA9TmlmT2ZiTE1HS0xKcUlsQlorTU5CQWcrdldJYjZqaDdRWA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         RIVERA W A.Noise reduction a priori synthetic over-sampling for class imbalanced data sets[J].Information Sciences, 2017, 408 (C) :146-161.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_11" title=" MA L, FAN S.CURE-SMOTE algorithm and hybrid algorithm for feature selection and parameter optimization based on random forests [J].BMC Bioinformatics, 2017, 18 (1) :169." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDF3C674389B3E4073EF5A41E847F2A9D3&amp;v=MDY5MjR5QlZtbkRvTVRINlhwQll5RDdEbFRNNmNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6THUzd0swPU5qN0Jhc1c3YmRmTHE0eE5iWmtNZVhnNQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         MA L, FAN S.CURE-SMOTE algorithm and hybrid algorithm for feature selection and parameter optimization based on random forests [J].BMC Bioinformatics, 2017, 18 (1) :169.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_12" title=" BOROWSKA, K, STEPANIUK J.Imbalanced data classification:a novel re-sampling approach combining versatile improved SMOTE and rough sets[C]// CISIM 2016:IFIP International Conference on Computer Information Systems and Industrial Management.Berlin:Springer, 2016:31-42." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imbalanced data classification:a novel re-sampling approach combining versatile improved SMOTE and rough sets">
                                        <b>[12]</b>
                                         BOROWSKA, K, STEPANIUK J.Imbalanced data classification:a novel re-sampling approach combining versatile improved SMOTE and rough sets[C]// CISIM 2016:IFIP International Conference on Computer Information Systems and Industrial Management.Berlin:Springer, 2016:31-42.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_13" title=" BAIG M M, AWAIS M M, EL-ALFY E S M.AdaBoost-based artificial neural network learning[J].Neurocomputing, 2017, 248 (C) :120-126." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3C714C207A89FA5758A8C3362FCCFA1F&amp;v=MzAxNzQwPU5pZk9mYkRMR2RESTNJMUZZNW9IQlFwSXloRVc0azUxTzN6aHFoQkRDc0hpTkx2cENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMdTN3Sw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         BAIG M M, AWAIS M M, EL-ALFY E S M.AdaBoost-based artificial neural network learning[J].Neurocomputing, 2017, 248 (C) :120-126.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_14" title=" MINZ A, MAHOBIYA C.MR image classification using Adaboost for brain tumor type[C]// Proceedings of the 2017 IEEE 7th International Advance Computing Conference.Washington, DC:IEEE Computer Society, 2017:701-705." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MR image classification using adaboost for brain tumor type">
                                        <b>[14]</b>
                                         MINZ A, MAHOBIYA C.MR image classification using Adaboost for brain tumor type[C]// Proceedings of the 2017 IEEE 7th International Advance Computing Conference.Washington, DC:IEEE Computer Society, 2017:701-705.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_15" title=" 王军, 费凯, 程勇.基于改进的Adaboost-BP模型在降水中的预测[J].计算机应用, 2017, 37 (9) :2689-2693. (WANG J, FEI K, CHENG Y.Prediction of rainfall based on improved Adaboost-BP model[J].Journal of Computer Applications, 2017, 37 (9) :2689-2693.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201709049&amp;v=MDU5NThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEbVc3L05MejdCZDdHNEg5Yk1wbzlCYlk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         王军, 费凯, 程勇.基于改进的Adaboost-BP模型在降水中的预测[J].计算机应用, 2017, 37 (9) :2689-2693. (WANG J, FEI K, CHENG Y.Prediction of rainfall based on improved Adaboost-BP model[J].Journal of Computer Applications, 2017, 37 (9) :2689-2693.) 
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_16" title=" SCHAPIRE R E, FREUND Y, BARTLETT P, et al.Boosting the margin:a new explanation for the effectiveness of voting methods[J].Annals of Statistics, 1998, 26 (5) :1651-1686." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14112600628791&amp;v=MTk0OTZaZVp0RmlubFVyM0lLRjBjYWhjPU5pZlllcks4SDlET3FZOUZZdWtIQzNVNG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         SCHAPIRE R E, FREUND Y, BARTLETT P, et al.Boosting the margin:a new explanation for the effectiveness of voting methods[J].Annals of Statistics, 1998, 26 (5) :1651-1686.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_17" title=" GAO W, ZHOU Z H.On the doubt about margin explanation of boosting[J].Artificial Intelligence, 2013, 203:1-18." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15120400272742&amp;v=MDU2NjJ3TkMzZzdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lLRjBjYWhjPU5pZk9mYks5SDlQTXE0OUZadQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         GAO W, ZHOU Z H.On the doubt about margin explanation of boosting[J].Artificial Intelligence, 2013, 203:1-18.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_18" title=" BACHE K, LICHMAN M.UCI repository of machine learning databases[DB/OL].[2018- 06- 20].http://www.ics.uci.edu/～mlearn/MLRepository.html." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=UCI repository of machine learning databases">
                                        <b>[18]</b>
                                         BACHE K, LICHMAN M.UCI repository of machine learning databases[DB/OL].[2018- 06- 20].http://www.ics.uci.edu/～mlearn/MLRepository.html.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_19" title=" van HULSE J, KHOSHGOFTAAR T M, NAPOLITANO A.Expertimental perspectives on learning from imbalanced data[C]// Proceedings of the 24th International Conference on Machine Learing.New York:ACM, 2007:935-942." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Expertimental perspectives on learning fromimbalanced data">
                                        <b>[19]</b>
                                         van HULSE J, KHOSHGOFTAAR T M, NAPOLITANO A.Expertimental perspectives on learning from imbalanced data[C]// Proceedings of the 24th International Conference on Machine Learing.New York:ACM, 2007:935-942.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_20" title=" LIU N, WEI L W, AUNG Z.Handling class imbalance in customer behavior prediction[C]// Proceedings of the 2014 International Conference on Collaboration Technologies and Systems.Piscataway, NJ:IEEE, 2014:100-103." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Handling class imbalance in customer behavior prediction">
                                        <b>[20]</b>
                                         LIU N, WEI L W, AUNG Z.Handling class imbalance in customer behavior prediction[C]// Proceedings of the 2014 International Conference on Collaboration Technologies and Systems.Piscataway, NJ:IEEE, 2014:100-103.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-09 13:48</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(05),1364-1367 DOI:10.11772/j.issn.1001-9081.2018112346            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于间隔理论的过采样集成算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%AE%97%E5%A0%82&amp;code=32024130&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张宗堂</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E5%96%86&amp;code=28038946&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈喆</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%88%B4%E5%8D%AB%E5%9B%BD&amp;code=20157658&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">戴卫国</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%B7%E5%86%9B%E6%BD%9C%E8%89%87%E5%AD%A6%E9%99%A2%E8%88%AA%E6%B5%B7%E8%A7%82%E9%80%9A%E7%B3%BB&amp;code=0939167&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">海军潜艇学院航海观通系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对传统集成算法不适用于不平衡数据分类的问题, 提出基于间隔理论的AdaBoost算法 (MOSBoost) 。首先通过预训练得到原始样本的间隔;然后依据间隔排序对少类样本进行启发式复制, 从而形成新的平衡样本集;最后将平衡样本集输入AdaBoost算法进行训练以得到最终集成分类器。在UCI数据集上进行测试实验, 利用<i>F</i>-<i>measure</i>和<i>G</i>-<i>mean</i>两个准则对MOSBoost、AdaBoost、随机过采样AdaBoost (ROSBoost) 和随机降采样AdaBoost (RDSBoost) 四种算法进行评价。实验结果表明, MOSBoost算法分类性能优于其他三种算法, 其中, 相对于AdaBoost算法, MOSBoost算法在<i>F</i>-<i>measure</i>和<i>G</i>-<i>mean</i>准则下分别提升了8.4%和6.2%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">不平衡数据;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%97%B4%E9%9A%94%E7%90%86%E8%AE%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">间隔理论;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%87%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">过采样方法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9B%86%E6%88%90%E5%88%86%E7%B1%BB%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">集成分类器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *张宗堂 (1989—) , 男, 山东青岛人, 博士, 主要研究方向:水声目标识别;电子邮箱qtxy_robin@126.com;
                                </span>
                                <span>
                                    陈喆 (1987—) , 男, 吉林四平人, 讲师, 博士, 主要研究方向:水声目标识别;;
                                </span>
                                <span>
                                    戴卫国 (1968—) , 男, 河北深州人, 教授, 博士, 主要研究方向:水声目标识别。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-26</p>

            </div>
                    <h1><b>Over sampling ensemble algorithm based on margin theory</b></h1>
                    <h2>
                    <span>ZHANG Zongtang</span>
                    <span>CHEN Zhe</span>
                    <span>DAI Weiguo</span>
            </h2>
                    <h2>
                    <span>Navigation and Observation Department, Navy Submarine Academy</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to solve the problem that traditional ensemble algorithms are not suitable for imbalanced data classification, Over Sampling AdaBoost based on Margin theory (MOSBoost) was proposed. Firstly, the margins of original samples were obtained by pre-training. Then, the minority class samples were heuristic duplicated by margin sorting thus forming a new balanced sample set. Finally, the finall ensemble classifier was obtained by the trained AdaBoost with the balanced sample set as the input. In the experiment on UCI dataset, <i>F</i>-<i>measure</i> and <i>G</i>-<i>mean</i> were used to evaluate MOSBoost, AdaBoost, Random OverSampling AdaBoost (ROSBoost) and Random UnderSampling AdaBoost (RDSBoost) . The experimental results show that MOSBoost is superior to other three algorithm. Compared with AdaBoost, MOSBoost improves 8.4% and 6.2% respctively under <i>F</i>-<i>measure</i> and <i>G</i>-<i>mean</i> criteria.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=imbalanced%20data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">imbalanced data;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=margin%20theory&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">margin theory;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=over%20sampling%20method&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">over sampling method;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ensemble%20classifier&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ensemble classifier;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Zongtang, born in 1989, Ph. D. His research interest includes underwater target recognition. ;
                                </span>
                                <span>
                                    CHEN Zhe, born in 1987, Ph. D. , lecturer. His research interest includes underwater target recognition. ;
                                </span>
                                <span>
                                    DAI Weiguo, born in 1968, Ph. D. , professor. His research interest includes underwater target recognition.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-11-26</p>
                            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="46">近些年, 不平衡数据分类问题成为了机器学习的热点问题, 它广泛存在于现实生产生活中, 例如邮件过滤<citation id="190" type="reference"><link href="150" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、图像分类<citation id="191" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、软件缺陷预测<citation id="192" type="reference"><link href="154" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、医疗诊断<citation id="193" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、基因数据分析<citation id="194" type="reference"><link href="158" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等。对于二分类问题, 不平衡数据中多类的样本数量远大于少类。传统的分类方法以总体分类精度为目标, 忽视了类别不平衡性, 从而导致少类样本分类准确率降低, 然而少类样本往往具有较高的价值, 这使得错分代价较大。</p>
                </div>
                <div class="p1">
                    <p id="47">针对不平衡数据的处理方法大致分为算法层面和数据层面: 算法层面指构造新的算法或对原有算法进行改造以偏向少类; 数据层面主要是利用重采样方法获得平衡样本集, 再结合现有分类器进行分类。重采样方法, 包括欠采样法和过采样法, 形式上比较简练, 且不影响分类器设计, 因此得到了广泛的研究。根据采取的策略, 它又可分为随机采样和启发式采样: 随机采样不依据数据信息, 只是简单地随机删除或添加样本; 启发式采样则是在利用数据内部特性的基础上进行采样。典型的启发式欠采样方法如Tomek links<citation id="195" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、One sided selection<citation id="196" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、Neighborhood Cleaning Rule<citation id="197" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等克服了随机欠采样中容易缺失有用信息的缺点, 一定程度上提高了算法性能。而启发式过采样中比较有代表性的是SMOTE (Synthetic Minority Oversampling TEchnique) <citation id="198" type="reference"><link href="166" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>方法及其改进算法<citation id="199" type="reference"><link href="168" rel="bibliography" /><link href="170" rel="bibliography" /><link href="172" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>。SMOTE方法的基本假设是相同类别的邻近数据点所生成的凸集也属于同一类别。启发式重采样方法基本都是在某种准则下对样本进行筛选, 对数据集的依赖性较强, 然而不平衡数据集往往存在类内不平衡、小析取项、高噪声等特点, 使得其难以满足准则要求, 进而降低了算法性能。表面上看, 这是数据集与准则之间的适配性问题, 实际上是这些方法缺乏理论基础, 泛化性较低。</p>
                </div>
                <div class="p1">
                    <p id="48">AdaBoost算法是一种经典的集成分类算法, 在机器学习中有广泛的应用<citation id="200" type="reference"><link href="174" rel="bibliography" /><link href="176" rel="bibliography" /><link href="178" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>。AdaBoost以最小化总体分类误差为目标, 忽视了类别间的不平衡性, 因而不适用于不平衡数据分类。间隔理论是AdaBoost算法的重要理论基础, 成功解释了AdaBoost算法不易过拟合等现象。本文从间隔理论出发, 定义了少类间隔和多类间隔, 对少类间隔样本依据符号正负进行筛选, 对正的少类间隔样本进行启发式复制, 形成新的平衡样本集, 在此样本集上进行AdaBoost训练, 形成了MOSBoost算法, 从而提高了不平衡数据分类性能。</p>
                </div>
                <h3 id="49" name="49" class="anchor-tag">1 相关工作</h3>
                <h4 class="anchor-tag" id="50" name="50">1.1 AdaBoost<b>算法</b></h4>
                <div class="p1">
                    <p id="51">AdaBoost算法将训练样本集{ (<i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>) , (<i>x</i><sub>2</sub>, <i>y</i><sub>2</sub>) , …, (<i>x</i><sub><i>N</i></sub>, <i>y</i><sub><i>N</i></sub>) }作为输入, 其中<i>x</i><sub><i>i</i></sub>是样本, <i>y</i><sub><i>i</i></sub>为其类标, 对于二分类问题, <i>y</i><sub><i>i</i></sub>∈{-1, 1}。然后根据已知的基分类算法在<i>t</i>=1, 2, …, <i>T</i>轮中不断地运算。<i>D</i><sup><i>t</i></sup> (<i>i</i>) 表示第<i>t</i>轮中第<i>i</i>个训练样本的权重。基分类算法的任务是在权重分布<i>D</i><sup><i>t</i></sup>的基础上得到基分类器<i>h</i><sub><i>t</i></sub>来最小化分类误差。当<i>h</i><sub><i>t</i></sub>训练完成, AdaBoost选择一个参数<i>α</i><sub><i>t</i></sub>∈<b>R</b>来衡量<i>h</i><sub><i>t</i></sub>的分类性能。然后更新权重分布<i>D</i><sup><i>t</i></sup>。最终的集成分类器<i>F</i>是<i>T</i>个基分类器的加权输出。具体算法如算法1所示。</p>
                </div>
                <div class="p1">
                    <p id="52">算法1 AdaBoost算法。</p>
                </div>
                <div class="area_img" id="206">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201905020_20600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="66" name="66">1.2 <b>间隔理论</b></h4>
                <div class="p1">
                    <p id="67">本文用<mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>Ρ</mtext><mtext>r</mtext></mrow></mstyle><mi>D</mi></munder><mo stretchy="false">[</mo><mo>⋅</mo><mo stretchy="false">]</mo></mrow></math></mathml>来表示数据集<i>D</i>上的概率, <i>E</i>[·]为数学期望。假设<i>C</i> (<i>H</i>) 是基分类器空间<i>H</i>的凸包, 那么集成分类器 <i>f</i>∈<i>C</i> (<i>H</i>) 可以表示成以下的形式:</p>
                </div>
                <div class="p1">
                    <p id="69"><i>f</i>=∑<i>α</i><sub><i>i</i></sub><i>h</i><sub><i>i</i></sub>; ∑<i>α</i><sub><i>i</i></sub>=1, <i>α</i><sub><i>i</i></sub>≥0       (1) </p>
                </div>
                <div class="p1">
                    <p id="70">样本 (<i>x</i>, <i>y</i>) 关于集成分类器<i>f</i>的间隔定义为:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><mi>g</mi><mo>=</mo><mi>y</mi><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>:</mo><mi>y</mi><mo>=</mo><mi>h</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>:</mo><mi>y</mi><mo>≠</mo><mi>h</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">Schapire等<citation id="201" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>推导出了基于间隔分布的AdaBoost算法泛化误差界, 如定理1所示。</p>
                </div>
                <div class="p1">
                    <p id="73"><b>定理</b>1 从训练样本集上的一个分布<i>Dist</i>中独立随机抽取<i>N</i>个训练样本组成集合<i>D</i><sub><i>tr</i></sub>, 假设基分类器空间<i>H</i>是有限的并且<i>δ</i>&gt;0, 则对所有的<i>θ</i>&gt;0, 每一个集成分类器<i>f</i>∈<i>C</i> (<i>H</i>) 在<i>D</i><sub><i>tr</i></sub>上至少以1-<i>δ</i>的概率满足下面的泛化误差界:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>Ρ</mtext><mtext>r</mtext></mrow></mstyle><mrow><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi></mrow></munder><mo stretchy="false">[</mo><mi>y</mi><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mn>0</mn><mo stretchy="false">]</mo><mo>≤</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>Ρ</mtext><mtext>r</mtext></mrow></mstyle><mrow><mi>D</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>r</mi></mrow></msub></mrow></munder><mo stretchy="false">[</mo><mi>y</mi><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>≤</mo><mi>θ</mi><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>Ο</mi><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mrow><msqrt><mi>Ν</mi></msqrt></mrow></mfrac><mrow><mo> (</mo><mrow><mfrac><mrow><mi>ln</mi><mspace width="0.25em" /><mi>Ν</mi><mspace width="0.25em" /><mi>ln</mi><mspace width="0.25em" /><mrow><mo>|</mo><mi>Η</mi><mo>|</mo></mrow></mrow><mrow><mi>θ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo>+</mo><mi>ln</mi><mfrac><mn>1</mn><mi>δ</mi></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">可以看出, 泛化误差界与间隔分布有关, 但如何获得一个好的间隔分布呢?众所众知, 统计特征是刻画分布的良好工具, 因此, 在过去基于间隔的泛化误差的基础上, 文献<citation id="202" type="reference">[<a class="sup">17</a>]</citation>推导出的泛化误差界揭示了其与间隔统计特征的关系, 如定理2所示。</p>
                </div>
                <div class="p1">
                    <p id="76"><b>定理</b>2 从训练样本集上的一个分布<i>Dist</i>中独立随机抽取<i>N</i> (<i>N</i>&gt;5) 个训练样本组成集合<i>D</i><sub><i>tr</i></sub>, 对任意的<i>θ</i>&gt;0, 每一个集成分类器<i>f</i>∈<i>C</i> (<i>H</i>) 在<i>D</i><sub><i>tr</i></sub>上至少以1-<i>δ</i>的概率满足下面的泛化误差界:</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>Ρ</mtext><mtext>r</mtext></mrow></mstyle><mrow><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi></mrow></munder><mo stretchy="false">[</mo><mi>y</mi><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mn>0</mn><mo stretchy="false">]</mo><mo>≤</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msup><mrow></mrow><mrow><mn>5</mn><mn>0</mn></mrow></msup></mrow></mfrac><mo>+</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>inf</mi></mrow></mstyle><mrow><mi>θ</mi><mo>∈</mo><mo stretchy="false"> (</mo><mn>0</mn><mo>, </mo><mn>1</mn><mo stretchy="false">]</mo></mrow></munder><mo stretchy="false">{</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>Ρ</mtext><mtext>r</mtext></mrow></mstyle><mrow><mi>D</mi><mi>t</mi><mi>r</mi></mrow></munder><mo stretchy="false">[</mo><mi>y</mi><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mi>θ</mi><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>Ν</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>2</mn><mo>/</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>E</mi><msubsup><mrow></mrow><mrow><mi>D</mi><mi>t</mi><mi>r</mi></mrow><mn>2</mn></msubsup><mo stretchy="false">[</mo><mi>y</mi><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo><mi>θ</mi><mo>/</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></msup><mo>+</mo><mfrac><mrow><mn>3</mn><msqrt><mi>μ</mi></msqrt></mrow><mrow><mi>Ν</mi><msup><mrow></mrow><mrow><mn>3</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></mfrac><mo>+</mo><mfrac><mrow><mn>7</mn><mi>μ</mi></mrow><mrow><mn>3</mn><mi>Ν</mi></mrow></mfrac><mo>+</mo><msqrt><mrow><mfrac><mrow><mn>3</mn><mi>μ</mi></mrow><mi>Ν</mi></mfrac><mover accent="true"><mi>Γ</mi><mo stretchy="true">⌢</mo></mover><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo></mrow></msqrt><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">其中:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>μ</mi><mo>=</mo><mn>1</mn><mn>4</mn><mn>4</mn><mspace width="0.25em" /><mi>ln</mi><mspace width="0.25em" /><mi>m</mi><mspace width="0.25em" /><mi>ln</mi><mspace width="0.25em" /><mo stretchy="false"> (</mo><mn>2</mn><mrow><mo>|</mo><mi>Η</mi><mo>|</mo></mrow><mo stretchy="false">) </mo><mo>/</mo><mi>θ</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>ln</mi><mo stretchy="false"> (</mo><mn>2</mn><mrow><mo>|</mo><mi>Η</mi><mo>|</mo></mrow><mo>/</mo><mi>δ</mi><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mover accent="true"><mi>Γ</mi><mo stretchy="true">⌢</mo></mover><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>Ρ</mtext><mtext>r</mtext></mrow></mstyle><mrow><mi>D</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>r</mi></mrow></msub></mrow></munder><mo stretchy="false">[</mo><mi>y</mi><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mi>θ</mi><mo stretchy="false">]</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>Ρ</mtext><mtext>r</mtext></mrow></mstyle><mrow><mi>D</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>r</mi></mrow></msub></mrow></munder><mo stretchy="false">[</mo><mi>y</mi><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>≥</mo><mn>2</mn><mi>θ</mi><mo>/</mo><mn>3</mn><mo stretchy="false">]</mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">定理2中, <i>E</i>[<i>yf</i> (<i>x</i>) ]是间隔均值, <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Γ</mi><mo stretchy="true">⌢</mo></mover><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo></mrow></math></mathml>反映间隔方差的大小。可以看出, 训练样本集的间隔统计特征直接影响泛化误差的大小。当训练样本集规模和集成分类器复杂度固定的情况下, 样本集的间隔均值越大, 间隔方差越小, 则泛化误差越小。</p>
                </div>
                <h3 id="82" name="82" class="anchor-tag">2 基于间隔理论的过采样方法</h3>
                <div class="p1">
                    <p id="83">根据数据的不平衡特性, 对间隔理论进行扩展, 将少类样本的间隔定义为少类间隔<i>mg</i><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup></mrow></math></mathml>, 多类样本的间隔定义为多类间隔<i>mg</i><mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>n</mi></msubsup></mrow></math></mathml>:</p>
                </div>
                <div class="p1">
                    <p id="86"><mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><mi>g</mi><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup><mo>=</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>; <i>x</i><sub><i>i</i></sub>∈<i>L</i><sub><i>p</i></sub>      (6) </p>
                </div>
                <div class="p1">
                    <p id="88"><mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><mi>g</mi><msubsup><mrow></mrow><mi>j</mi><mi>n</mi></msubsup><mo>=</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>; <i>x</i><sub><i>j</i></sub>∈<i>L</i><sub><i>n</i></sub>      (7) </p>
                </div>
                <div class="p1">
                    <p id="90">其中:<i>L</i><sub><i>p</i></sub>为大小是<i>N</i><sub><i>p</i></sub>的少类样本集, <i>L</i><sub><i>n</i></sub>为大小是<i>N</i><sub><i>n</i></sub>的多类样本集。一个样本的间隔有两重含义:间隔的符号表示集成分类器对该样本分类的对错, 间隔大于0, 表示分类正确;间隔小于0, 则表示分类错误。间隔的幅值表示集成分类器对该样本分类的置信度, 间隔幅值越大, 置信度越高;间隔幅值越小, 置信度越低。对于不平衡数据分类, 希望在保证总体分类正确率的情况下, 尽量提高少类的分类正确率, 即有尽可能多的少类间隔大于0且置信度尽可能高。因此, 将正的少类间隔样本进行复制, 即对少类间隔样本进行过采样, 这样不但优化了总体间隔分布, 同时使分类器偏向少类样本。</p>
                </div>
                <div class="p1">
                    <p id="91">从间隔的定义可以看出, 它是样本类标和集成分类器输出的乘积, 而集成分类器是通过AdaBoost算法训练而得, 因此一个样本的间隔只有训练结束后才能得到。为了利用少类间隔进行过采样, 就需要先对原始样本集进行训练, 这个过程被称为预训练。虽然预训练需要耗费一定的时间, 但对于非实时系统而言, 只要完成了分类器设计, 便可以直接对待测样本进行分类, 没有额外的时间消耗, 因此这里没必要过分考虑训练时间问题。预训练结束后, 将少类间隔样本集划分为大小是<i>N</i><sub><i>p</i>+</sub>的正的少类间隔样本集<i>L</i><sub><i>p</i>+</sub>和大小是<i>N</i><sub><i>n</i>-</sub>的负的少类间隔样本集<i>L</i><sub><i>p</i>-</sub>。</p>
                </div>
                <div class="p1">
                    <p id="92">由于间隔的幅值表示集成分类器对样本分类的置信度, 在正的少类间隔样本集中, 多复制低置信度样本, 少复制高置信度样本, 便可以在提升总体间隔的同时避免了高置信度样本的过度依赖。基于此提出一种启发式过采样, 如算法2所示。其中, 采样率<i>SR</i>= (<i>N</i><sub><i>n</i></sub>-<i>N</i><sub><i>p</i>-</sub>) /<i>N</i><sub><i>p</i>+</sub>。</p>
                </div>
                <div class="p1">
                    <p id="93">算法2 启发式过采样算法。</p>
                </div>
                <div class="area_img" id="207">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201905020_20700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="207">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201905020_20701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="105" name="105" class="anchor-tag">3 MOSBoost算法</h3>
                <div class="p1">
                    <p id="106">在上文提到的过采样过程中对正的少类间隔样本进行了处理而忽视了负的少类样本, 因此, 在第二轮<i>AdaBoost</i>算法中对负的少类间隔样本给予一定的初始权重倾斜。结合算法2与<i>AdaBoost</i>算法, 形成<i>MOSBoost</i>算法, 如算法3所示, 流程见图1。</p>
                </div>
                <div class="p1">
                    <p id="107">算法3 <i>MOSBoost</i>算法。</p>
                </div>
                <div class="p1">
                    <p id="108">1) 对原始样本集进行预训练;</p>
                </div>
                <div class="p1">
                    <p id="109">2) 实现算法2;</p>
                </div>
                <div class="p1">
                    <p id="110">3) 将过采样后的训练样本集作为<i>AdaBoost</i>算法输入进行训练, 在初始权重分配时, 给予负的少类间隔样本<i>d</i>倍于预训练AdaBoost的初始样本。</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905020_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 MOSBoost算法流程" src="Detail/GetImg?filename=images/JSJY201905020_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 MOSBoost算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905020_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Flow chart of MOSBoost algorithm</p>

                </div>
                <h3 id="112" name="112" class="anchor-tag">4 实验与分析</h3>
                <h4 class="anchor-tag" id="113" name="113">4.1 <b>分类性能评价准则</b></h4>
                <div class="p1">
                    <p id="114">评价准则对于评估分类性能和指导分类器构建有重要作用。传统的分类器一般采用总体分类精度作为评价准则。但是, 对于不平衡数据分类, 总体分类精度就不再适合了。<i>F</i>-<i>measure</i>和<i>G</i>-<i>mean</i>是两种常用的针对不平衡数据的评价准则, 它们都是建立在混淆矩阵 (如表1所示) 的基础上, TP (True Positive) 表示被识别为少类的少类样本个数, FP (False Negative) 表示被识别为少类的多类样本个数, FN (False Negative) 表示被识别为多类的少类样本个数, TN (True Negative) 表示被识别为多类的多类样本个数。</p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表</b>1 <b>二分类问题的混淆矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Confusion matrix for bi-class problem</p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td><br />类别</td><td>预测少类</td><td>预测多类</td></tr><tr><td><br />实际少类</td><td>TP</td><td>FN</td></tr><tr><td><br />实际多类</td><td>FP</td><td>TN</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="116"><i>F</i>-<i>measure</i>是查全率和查准率的函数, 如下所示:</p>
                </div>
                <div class="p1">
                    <p id="117" class="code-formula">
                        <mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo>-</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>s</mi><mi>u</mi><mi>r</mi><mi>e</mi><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi>β</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo>×</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>×</mo><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow><mrow><mi>β</mi><msup><mrow></mrow><mn>2</mn></msup><mo>×</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>+</mo><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="118"><i>Recall</i>=<i>TP</i>/ (<i>TP</i>+<i>FN</i>)      (9) </p>
                </div>
                <div class="p1">
                    <p id="119"><i>Precision</i>=<i>TP</i>/ (<i>TP</i>+<i>FP</i>)      (10) </p>
                </div>
                <div class="p1">
                    <p id="120">其中:<i>Recall</i>是查全率;<i>Precision</i>是查准率;<i>β</i>为两者的相对重要性, 下文实验中取1。只有在<i>Recall</i>和<i>Precision</i>同时较大时, <i>F</i>-<i>measure</i>的值才会较大。</p>
                </div>
                <div class="p1">
                    <p id="121"><i>G</i>-<i>mean</i>是少类正确率和多类正确率的函数, 如下所示:</p>
                </div>
                <div class="p1">
                    <p id="122" class="code-formula">
                        <mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mo>-</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>=</mo><msqrt><mrow><mi>Μ</mi><mi>i</mi><mi>n</mi><mi>o</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi><mi>A</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>r</mi><mi>y</mi><mo>×</mo><mi>Μ</mi><mi>a</mi><mi>j</mi><mi>o</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi><mi>A</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>r</mi><mi>y</mi></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>i</mi><mi>n</mi><mi>o</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi><mi>A</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>r</mi><mi>y</mi><mo>=</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="124" class="code-formula">
                        <mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>a</mi><mi>j</mi><mi>o</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi><mi>A</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>r</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ν</mi></mrow><mrow><mi>Τ</mi><mi>Ν</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="125">其中:<i>MinorityAccurary</i>是少类正确率, <i>MajorityAccurary</i>是多类正确率。只有当<i>MinorityAccurary</i>和<i>MajorityAccurary</i>同时较大时, <i>G</i>-<i>mean</i>的值才会较大。</p>
                </div>
                <h4 class="anchor-tag" id="126" name="126">4.2 UCI <b>数据集分析</b></h4>
                <div class="p1">
                    <p id="127">为了验证本文方法的有效性, 选取了一些UCI数据集<citation id="203" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>在Matlab软件上进行实验验证。对于其中含多类样本的数据集, 合并其中的几类样本以形成两类样本集。具体数据集信息如表2所示。虽然在平衡数据集与不平衡数据集之间没有明确的界线, 但一般情况下不平衡率大于2就可以认为是不平衡数据集。因此在本文选取的UCI数据集中, heart、sonar和vote是平衡数据集, vehicle、wpbc和segment是不平衡数据集。</p>
                </div>
                <div class="area_img" id="128">
                    <p class="img_tit"><b>表</b>2 <b>实验中使用的</b>UCI<b>数据集信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 UCI dataset used in the experiment</p>
                    <p class="img_note"></p>
                    <table id="128" border="1"><tr><td><br />数据集</td><td>样本容量</td><td>少类样本个数</td><td>多类样本个数</td><td>不平衡率</td></tr><tr><td>heart</td><td>303</td><td>139</td><td>164</td><td>1.18</td></tr><tr><td><br />sonar</td><td>208</td><td>97</td><td>111</td><td>1.14</td></tr><tr><td><br />vote</td><td>435</td><td>168</td><td>267</td><td>1.59</td></tr><tr><td><br />vehicle</td><td>846</td><td>212</td><td>634</td><td>2.99</td></tr><tr><td><br />wpbc</td><td>198</td><td>47</td><td>151</td><td>3.21</td></tr><tr><td><br />segment</td><td>2 310</td><td>330</td><td>1 980</td><td>6.00</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="129">采用10折交叉验证法对分类性能进行验证。单次实验中, 选择决策桩作为基分类器, 训练轮次为50, 比例系数<i>c</i><sub>1</sub>=2, <i>c</i><sub>2</sub>=1, 初始权重系数<i>d</i>=2。在相同样本集上, 采用AdaBoost算法、随机过采样AdaBoost算法 (Random OverSampling AdaBoost, ROSBoost) <citation id="204" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>和随机欠采样AdaBoost算法 (Random UnderSampling AdaBoost, RDSBoost) <citation id="205" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>进行对比, 利用<i>F</i>-<i>measure</i>和<i>G</i>-<i>mean</i>对四种算法进行比较, 结果如表3所示。</p>
                </div>
                <div class="p1">
                    <p id="130">从表3中可以看出, 首先, 相对于平衡数据集, AdaBoost在不平衡数据集上性能较差, <i>F</i>-<i>measure</i>平均值由0.82下降到0.49, <i>G</i>-<i>mean</i>平均值由0.84下降到0.53, 进一步印证了进行算法改进的必要性。其次, ROSBoost和RDSBoost比AdaBoost性能稍有提升, 但提升的幅度不大, 某些情况下RDSBoost性能更低, 可能的原因是降采样过程中丢了一些有用信息。在四种算法中, 在<i>F</i>-<i>measure</i>和<i>G</i>-<i>mean</i>准则下MOSBoost性能均为最优: 相对于AdaBoost, MOSBoost在<i>F</i>-<i>measure</i>准则下提升了8.4%, 在<i>G</i>-<i>mean</i>准则下提升了6.2%。</p>
                </div>
                <h3 id="131" name="131" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="132">本文在间隔理论的基础上, 首先提出了基于间隔理论的过采样方法, 然后结合AdaBoost形成了MOSBoost。在UCI数据集上进行实验验证, 对比了MOSBoost与其他三种算法的分类性能, 发现MOSBoost在<i>F</i>-<i>measure</i>和<i>G</i>-<i>mean</i>准则下性能最优。在未来的工作中可以尝试将本文方法推广到多类不平衡数据分类问题中, 同时考虑基于间隔理论的过采样与其他分类器的结合。</p>
                </div>
                <div class="area_img" id="133">
                    <p class="img_tit"><b>表</b>3 <b>几种算法性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Performance comparison of several algorithms</p>
                    <p class="img_note"></p>
                    <table id="133" border="1"><tr><td><br />数据集</td><td>算法</td><td><i>F</i>-<i>measure</i></td><td><i>G</i>-<i>mean</i></td></tr><tr><td rowspan="4"><br />heart</td><td><br />AdaBoost</td><td>0.78</td><td>0.81</td></tr><tr><td><br />ROSBoost</td><td>0.80</td><td>0.82</td></tr><tr><td><br />RDSBoost</td><td>0.79</td><td>0.80</td></tr><tr><td><br />MOSBoost</td><td>0.82</td><td>0.86</td></tr><tr><td rowspan="4"><br />sonar</td><td><br />AdaBoost</td><td>0.74</td><td>0.76</td></tr><tr><td><br />ROSBoost</td><td>0.74</td><td>0.78</td></tr><tr><td><br />RDSBoost</td><td>0.75</td><td>0.76</td></tr><tr><td><br />MOSBoost</td><td>0.81</td><td>0.79</td></tr><tr><td rowspan="4"><br />vote</td><td><br />AdaBoost</td><td>0.93</td><td>0.95</td></tr><tr><td><br />ROSBoost</td><td>0.93</td><td>0.96</td></tr><tr><td><br />RDSBoost</td><td>0.92</td><td>0.91</td></tr><tr><td><br />MOSBoost</td><td>0.93</td><td>0.96</td></tr><tr><td rowspan="4"><br />vehicle</td><td><br />AdaBoost</td><td>0.26</td><td>0.41</td></tr><tr><td><br />ROSBoost</td><td>0.28</td><td>0.43</td></tr><tr><td><br />RDSBoost</td><td>0.27</td><td>0.40</td></tr><tr><td><br />MOSBoost</td><td>0.29</td><td>0.45</td></tr><tr><td rowspan="4"><br />wpbc</td><td><br />AdaBoost</td><td>0.77</td><td>0.44</td></tr><tr><td><br />ROSBoost</td><td>0.78</td><td>0.47</td></tr><tr><td><br />RDSBoost</td><td>0.78</td><td>0.45</td></tr><tr><td><br />MOSBoost</td><td>0.82</td><td>0.49</td></tr><tr><td rowspan="4"><br />segment</td><td><br />AdaBoost</td><td>0.43</td><td>0.74</td></tr><tr><td><br />ROSBoost</td><td>0.44</td><td>0.74</td></tr><tr><td><br />RDSBoost</td><td>0.42</td><td>0.72</td></tr><tr><td><br />MOSBoost</td><td>0.47</td><td>0.78</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="150">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES325859E5EA0E0649383D3D6BB283A39F&amp;v=MDg4NjA0andKU3d2azNtQTNjYkhsUnJQcENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMdTN3SzA9TmlmT2ZiQzZHOW5KcHZwQUVab1BlWHcveXg4UQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> DAI H L.Class imbalance learning via a fuuzy total margin based support vector machine[J].Applied Soft Computing, 2015, 31 (C) :172-184.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201807007&amp;v=MDQ0NzJPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG1XNy9OTHo3QmQ3RzRIOW5NcUk5Rlk0UUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 谭洁帆, 朱焱, 陈同孝, 等.基于卷积神经网络和代价敏感的不平衡图像分类方法[J].计算机应用, 2018, 38 (7) :1862-1865, 1871. (TAN J F, ZHU Y, CHEN T X, et al.Imbalanced image classification approach based on convolution network and cost-sensitivity[J].Journal of Computer Applications, 2018, 38 (7) :1862-1865, 1871.) 
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using class imbalance learning for software defect prediction">

                                <b>[3]</b> WANG S, YAO X.Using class imbalance learning for software defect prediction[J].IEEE Transactions on Reliability, 2013, 62 (2) :434-443.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300239204&amp;v=MTk4NThmT2ZiSzdIdERPckk5Rlp1Z0dEbnc5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0YwY2FoYz1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> OZCIFT A, GULTEN A.Classifer ensemble construction with rotation forest to improve medical diagnosis performance of machine learning algorithms[J].Computer Methods and Programs in Biomedicine, 2011, 104 (3) :443-451.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600369818&amp;v=MjgxOTdNbndaZVp0RmlubFVyM0lLRjBjYWhjPU5pZk9mYks4SHRETXFZOUZaKzBHQkgweG9CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> YU H, NI J, ZHAO J.ACOSampling:an ant colony optimization-based undersampling method for classifying imbalanced DNA microarray data[J].Neurocomputing, 2013, 101:309-318.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Two Modifications of CNN">

                                <b>[6]</b> TOMEK I.Two modifications of CNN[J].IEEE Transactions on Systems, Man and Cybernetics, 1976, SMC-6 (11) :769-772.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Addressing the curse of imbalanced training sets: one-sided selection">

                                <b>[7]</b> KUBAT M, MATWIN S.Addressing the curse of imbalanced training sets:one-sided selection[C]// Proceedings of the 14th International Conference on Machine Learning.San Francisco:Morgan Kaufmann, 1997:179-186.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving Identification of Difficult Small Classes by Balancing Class Distribution">

                                <b>[8]</b> LAURIKKALA J.Improving identification of difficult small classes by balancing class distribution[C]// Proceedings of the 8th Conference on Artificial Intelligence in Medicine in Europe.Berlin:Springer, 2001:63-66.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SMOTE: synthetic minority over-sampling technique">

                                <b>[9]</b> CHAWLA N, BOWYER K, HALL L, et al.SMOTE:synthetic minority over-sampling technique[J].Journal of Artificial Intelligence Research, 2002, 16 (1) :321-357.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1D6C57643828D7BD8076944F2A8B743F&amp;v=MDAyNDVGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMdTN3SzA9TmlmT2ZiTE1HS0xKcUlsQlorTU5CQWcrdldJYjZqaDdRWHZtMmhCRWNjQ1RRYm5wQ09OdkZTaVdXcjdKSQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> RIVERA W A.Noise reduction a priori synthetic over-sampling for class imbalanced data sets[J].Information Sciences, 2017, 408 (C) :146-161.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDF3C674389B3E4073EF5A41E847F2A9D3&amp;v=MjA4MzM3RGxUTTZjQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekx1M3dLMD1OajdCYXNXN2JkZkxxNHhOYlprTWVYZzV5QlZtbkRvTVRINlhwQll5RA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> MA L, FAN S.CURE-SMOTE algorithm and hybrid algorithm for feature selection and parameter optimization based on random forests [J].BMC Bioinformatics, 2017, 18 (1) :169.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imbalanced data classification:a novel re-sampling approach combining versatile improved SMOTE and rough sets">

                                <b>[12]</b> BOROWSKA, K, STEPANIUK J.Imbalanced data classification:a novel re-sampling approach combining versatile improved SMOTE and rough sets[C]// CISIM 2016:IFIP International Conference on Computer Information Systems and Industrial Management.Berlin:Springer, 2016:31-42.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3C714C207A89FA5758A8C3362FCCFA1F&amp;v=MjEwNTBmQnJMVTA1dHBoekx1M3dLMD1OaWZPZmJETEdkREkzSTFGWTVvSEJRcEl5aEVXNGs1MU8zemhxaEJEQ3NIaU5MdnBDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> BAIG M M, AWAIS M M, EL-ALFY E S M.AdaBoost-based artificial neural network learning[J].Neurocomputing, 2017, 248 (C) :120-126.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MR image classification using adaboost for brain tumor type">

                                <b>[14]</b> MINZ A, MAHOBIYA C.MR image classification using Adaboost for brain tumor type[C]// Proceedings of the 2017 IEEE 7th International Advance Computing Conference.Washington, DC:IEEE Computer Society, 2017:701-705.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201709049&amp;v=MDQ5MzJwbzlCYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEbVc3L05MejdCZDdHNEg5Yk0=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 王军, 费凯, 程勇.基于改进的Adaboost-BP模型在降水中的预测[J].计算机应用, 2017, 37 (9) :2689-2693. (WANG J, FEI K, CHENG Y.Prediction of rainfall based on improved Adaboost-BP model[J].Journal of Computer Applications, 2017, 37 (9) :2689-2693.) 
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14112600628791&amp;v=MzA2MDk5RE9xWTlGWXVrSEMzVTRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lLRjBjYWhjPU5pZlllcks4SA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> SCHAPIRE R E, FREUND Y, BARTLETT P, et al.Boosting the margin:a new explanation for the effectiveness of voting methods[J].Annals of Statistics, 1998, 26 (5) :1651-1686.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15120400272742&amp;v=MTU4MzJVcjNJS0YwY2FoYz1OaWZPZmJLOUg5UE1xNDlGWnV3TkMzZzdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> GAO W, ZHOU Z H.On the doubt about margin explanation of boosting[J].Artificial Intelligence, 2013, 203:1-18.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=UCI repository of machine learning databases">

                                <b>[18]</b> BACHE K, LICHMAN M.UCI repository of machine learning databases[DB/OL].[2018- 06- 20].http://www.ics.uci.edu/～mlearn/MLRepository.html.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Expertimental perspectives on learning fromimbalanced data">

                                <b>[19]</b> van HULSE J, KHOSHGOFTAAR T M, NAPOLITANO A.Expertimental perspectives on learning from imbalanced data[C]// Proceedings of the 24th International Conference on Machine Learing.New York:ACM, 2007:935-942.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Handling class imbalance in customer behavior prediction">

                                <b>[20]</b> LIU N, WEI L W, AUNG Z.Handling class imbalance in customer behavior prediction[C]// Proceedings of the 2014 International Conference on Collaboration Technologies and Systems.Piscataway, NJ:IEEE, 2014:100-103.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201905020" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905020&amp;v=MTg2ODFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEbVc3L05MejdCZDdHNEg5ak1xbzlIWklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
