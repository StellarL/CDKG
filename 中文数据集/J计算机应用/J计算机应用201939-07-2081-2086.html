<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136664816565000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201907036%26RESULT%3d1%26SIGN%3dI5dJ99j%252bJBPx2KjIpaBMKPR%252bUSQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907036&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907036&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907036&amp;v=MDU3MDF6N0JkN0c0SDlqTXFJOUdZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVTd6UEw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#39" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#42" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#46" data-title="2 基于视频分段的空时双通道行为识别 ">2 基于视频分段的空时双通道行为识别</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#47" data-title="2.1 &lt;b&gt;整体框架&lt;/b&gt;">2.1 <b>整体框架</b></a></li>
                                                <li><a href="#54" data-title="2.2 &lt;b&gt;空域网络数据预处理&lt;/b&gt;">2.2 <b>空域网络数据预处理</b></a></li>
                                                <li><a href="#59" data-title="2.3 &lt;b&gt;时域网络数据预处理&lt;/b&gt;">2.3 <b>时域网络数据预处理</b></a></li>
                                                <li><a href="#62" data-title="2.4 &lt;b&gt;迁移学习&lt;/b&gt;">2.4 <b>迁移学习</b></a></li>
                                                <li><a href="#67" data-title="2.5 &lt;b&gt;单通道分段特征融合&lt;/b&gt;">2.5 <b>单通道分段特征融合</b></a></li>
                                                <li><a href="#70" data-title="2.6 &lt;b&gt;双通道特征集成&lt;/b&gt;">2.6 <b>双通道特征集成</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#73" data-title="3 结果分析 ">3 结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#74" data-title="3.1 &lt;b&gt;基本参数设置&lt;/b&gt;">3.1 <b>基本参数设置</b></a></li>
                                                <li><a href="#78" data-title="3.2 &lt;b&gt;不同分段数目下行为识别性能分析&lt;/b&gt;">3.2 <b>不同分段数目下行为识别性能分析</b></a></li>
                                                <li><a href="#81" data-title="3.3 &lt;b&gt;不同网络结构下行为识别性能分析&lt;/b&gt;">3.3 <b>不同网络结构下行为识别性能分析</b></a></li>
                                                <li><a href="#86" data-title="3.4 &lt;b&gt;不同分段融合方案下行为识别性能分析&lt;/b&gt;">3.4 <b>不同分段融合方案下行为识别性能分析</b></a></li>
                                                <li><a href="#90" data-title="3.5 &lt;b&gt;不同集成策略下行为识别性能分析&lt;/b&gt;">3.5 <b>不同集成策略下行为识别性能分析</b></a></li>
                                                <li><a href="#95" data-title="3.6 &lt;b&gt;与现有方法对比&lt;/b&gt;">3.6 <b>与现有方法对比</b></a></li>
                                                <li><a href="#98" data-title="3.7 HMDB51&lt;b&gt;数据集行为识别性能分析&lt;/b&gt;">3.7 HMDB51<b>数据集行为识别性能分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#100" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#49" data-title="图1 基于视频分段的空时双通道卷积神经网络的行为识别框架">图1 基于视频分段的空时双通道卷积神经网络的行为识别框架</a></li>
                                                <li><a href="#56" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;空域通道不同采样方式下数据集规模及行为识别准确率对比&lt;/b&gt;"><b>表</b>1 <b>空域通道不同采样方式下数据集规模及行为识别准确率对比</b></a></li>
                                                <li><a href="#58" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;空域通道不同数据增强技术下行为识别准确率对比&lt;/b&gt; %"><b>表</b>2 <b>空域通道不同数据增强技术下行为识别准确率对比</b> %</a></li>
                                                <li><a href="#64" data-title="图2 连续两帧图像与对应光流图像">图2 连续两帧图像与对应光流图像</a></li>
                                                <li><a href="#66" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;空域通道不同迁移学习方案下行为识别准确率对比&lt;/b&gt;"><b>表</b>3 <b>空域通道不同迁移学习方案下行为识别准确率对比</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;双通道网络基本参数设置&lt;/b&gt;"><b>表</b>4 <b>双通道网络基本参数设置</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;空域通道不同视频分段数目下行为识别准确率对比&lt;/b&gt;"><b>表</b>5 <b>空域通道不同视频分段数目下行为识别准确率对比</b></a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;表&lt;/b&gt;6 &lt;b&gt;空域通道不同网络结构的行为识别准确率对比&lt;/b&gt;"><b>表</b>6 <b>空域通道不同网络结构的行为识别准确率对比</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;表&lt;/b&gt;7 &lt;b&gt;时域通道不同网络结构的行为识别准确率对比&lt;/b&gt;"><b>表</b>7 <b>时域通道不同网络结构的行为识别准确率对比</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;表&lt;/b&gt;8 &lt;b&gt;空域通道不同分段融合方案下行为识别准确率对比&lt;/b&gt;"><b>表</b>8 <b>空域通道不同分段融合方案下行为识别准确率对比</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;表&lt;/b&gt;9 &lt;b&gt;时域通道不同分段融合方案下行为识别准确率对比&lt;/b&gt;"><b>表</b>9 <b>时域通道不同分段融合方案下行为识别准确率对比</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;表&lt;/b&gt;10 &lt;b&gt;基于试凑集成方式的双通道行为识别准确率对比&lt;/b&gt;"><b>表</b>10 <b>基于试凑集成方式的双通道行为识别准确率对比</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;表&lt;/b&gt;11 &lt;b&gt;基于方差集成方式的双通道行为识别准确率对比&lt;/b&gt;"><b>表</b>11 <b>基于方差集成方式的双通道行为识别准确率对比</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;表&lt;/b&gt;12 &lt;b&gt;不同方法在&lt;/b&gt;UCF101&lt;b&gt;数据集上行为识别准确率对比&lt;/b&gt;"><b>表</b>12 <b>不同方法在</b>UCF101<b>数据集上行为识别准确率对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="122">


                                    <a id="bibliography_1" title=" 单言虎, 张彰, 黄凯奇.人的视觉行为识别研究回顾、现状及展望[J].计算机研究与发展, 2016, 53 (1) :93-112. (SHAN Y H, ZHANG Z, HUANG K Q.Review, current situation and prospect of human visual behavior recognition [J].Journal of Computer Research and Development, 2016, 53 (1) :93-112.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201601010&amp;v=MjUwNzBac0Z5L2dVN3pQTHl2U2RMRzRIOWZNcm85RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         单言虎, 张彰, 黄凯奇.人的视觉行为识别研究回顾、现状及展望[J].计算机研究与发展, 2016, 53 (1) :93-112. (SHAN Y H, ZHANG Z, HUANG K Q.Review, current situation and prospect of human visual behavior recognition [J].Journal of Computer Research and Development, 2016, 53 (1) :93-112.) 
                                    </a>
                                </li>
                                <li id="124">


                                    <a id="bibliography_2" title=" FORSYTH D A.Computer Vision:A Modern Approach[M].2nd ed.Englewood Cliffs, NJ:Prentice Hall, 2011:1-2." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Computer Vision:A Modern Approach">
                                        <b>[2]</b>
                                         FORSYTH D A.Computer Vision:A Modern Approach[M].2nd ed.Englewood Cliffs, NJ:Prentice Hall, 2011:1-2.
                                    </a>
                                </li>
                                <li id="126">


                                    <a id="bibliography_3" title=" CAI Z, WANG L, PENG X, et al.Multi-view super vector for action recognition[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:596-603." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-view super vector for action recognition">
                                        <b>[3]</b>
                                         CAI Z, WANG L, PENG X, et al.Multi-view super vector for action recognition[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:596-603.
                                    </a>
                                </li>
                                <li id="128">


                                    <a id="bibliography_4" title=" WANG H, SCHMID C.Action recognition with improved trajectories[C]// Proceedings of the 2013 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2014:3551-3558." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Action recognition with improved trajectories">
                                        <b>[4]</b>
                                         WANG H, SCHMID C.Action recognition with improved trajectories[C]// Proceedings of the 2013 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2014:3551-3558.
                                    </a>
                                </li>
                                <li id="130">


                                    <a id="bibliography_5" title=" PENG X, WANG L, WANG X, et al.Bag of visual words and fusion methods for action recognition:comprehensive study and good practice [J].Computer Vision and Image Understanding, 2016, 150:109-125." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bag of visual words and fusion methods for action recognition:Comprehensive study and good practice">
                                        <b>[5]</b>
                                         PENG X, WANG L, WANG X, et al.Bag of visual words and fusion methods for action recognition:comprehensive study and good practice [J].Computer Vision and Image Understanding, 2016, 150:109-125.
                                    </a>
                                </li>
                                <li id="132">


                                    <a id="bibliography_6" title=" WANG L, QIAO Y, TANG X.MoFAP:a multi-level representation for action recognition[J].International Journal of Computer Vision, 2016, 119 (3) :254-271." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MoFAP:Amulti-level representation for action recognition">
                                        <b>[6]</b>
                                         WANG L, QIAO Y, TANG X.MoFAP:a multi-level representation for action recognition[J].International Journal of Computer Vision, 2016, 119 (3) :254-271.
                                    </a>
                                </li>
                                <li id="134">


                                    <a id="bibliography_7" title=" KARPATHY A, TODERICI G, SHETTY S, et al.Large-scale video classification with convolutional neural networks[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Rec-ognition.Washington, DC:IEEE Computer Society, 2014:1725-1732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-scale video classification with convolutional neural networks">
                                        <b>[7]</b>
                                         KARPATHY A, TODERICI G, SHETTY S, et al.Large-scale video classification with convolutional neural networks[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Rec-ognition.Washington, DC:IEEE Computer Society, 2014:1725-1732.
                                    </a>
                                </li>
                                <li id="136">


                                    <a id="bibliography_8" title=" TRAN D, BOURDEV L, FERGUS R, et al.Learning spatiotemporal features with 3D convolutional networks[C]// Proceedings of the 2014 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:4489-4497." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatiotemporal features with 3d convolutional networks">
                                        <b>[8]</b>
                                         TRAN D, BOURDEV L, FERGUS R, et al.Learning spatiotemporal features with 3D convolutional networks[C]// Proceedings of the 2014 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:4489-4497.
                                    </a>
                                </li>
                                <li id="138">


                                    <a id="bibliography_9" title=" VAROL G, LAPTEV I, SCHMID C.Long-term temporal convolutions for action recognition [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (6) :1510-1517." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long-term temporal convolutions for action recognition">
                                        <b>[9]</b>
                                         VAROL G, LAPTEV I, SCHMID C.Long-term temporal convolutions for action recognition [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (6) :1510-1517.
                                    </a>
                                </li>
                                <li id="140">


                                    <a id="bibliography_10" title=" SIMONYAN K, ZISSERMAN A.Two-stream convolutional net-works for action recognition in videos[C]// Proceedings of the 2014 Conference on Neural Information Processing Systems.New York:Curran Associates, 2014:568-576." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Two-stream convolutional networks for action recognition in videos">
                                        <b>[10]</b>
                                         SIMONYAN K, ZISSERMAN A.Two-stream convolutional net-works for action recognition in videos[C]// Proceedings of the 2014 Conference on Neural Information Processing Systems.New York:Curran Associates, 2014:568-576.
                                    </a>
                                </li>
                                <li id="142">


                                    <a id="bibliography_11" title=" NG Y H, HAUSKNECHT M, VIJAYANARASIMHAN S, et al.Beyond short snippets:deep networks for video classification[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:4694-4702." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond short snippets:Deep networks for video classification">
                                        <b>[11]</b>
                                         NG Y H, HAUSKNECHT M, VIJAYANARASIMHAN S, et al.Beyond short snippets:deep networks for video classification[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:4694-4702.
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_12" title=" WANG L M, XIONG Y J, WANG Z, et al.Temporal segment networks:towards good practices for deep action recognition [C]// Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:22-36." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Temporal segment networks:towards good practices for deep action recognition">
                                        <b>[12]</b>
                                         WANG L M, XIONG Y J, WANG Z, et al.Temporal segment networks:towards good practices for deep action recognition [C]// Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:22-36.
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_13" title=" WANG L, QIAO Y, TANG X.Action recognition with trajectory-pooled deep-convolutional descriptors[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:4305-4314." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Action recognition with trajectory-pooled deep-convolutional descriptors">
                                        <b>[13]</b>
                                         WANG L, QIAO Y, TANG X.Action recognition with trajectory-pooled deep-convolutional descriptors[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:4305-4314.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_14" title=" SZEGEDY C, VANHOUCKE V, IOFFE S, et al.Rethinking the inception architecture for computer vision[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:2818-2826." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rethinking the inception architecture for computer vision">
                                        <b>[14]</b>
                                         SZEGEDY C, VANHOUCKE V, IOFFE S, et al.Rethinking the inception architecture for computer vision[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:2818-2826.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_15" title=" MURPHY K P.Machine Learning:A Probabilistic Perspective [M].Cambridge:MIT Press, 2012:22." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Machine Learning:A Probabilistic Perspective">
                                        <b>[15]</b>
                                         MURPHY K P.Machine Learning:A Probabilistic Perspective [M].Cambridge:MIT Press, 2012:22.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_16" title=" HORN B K P, SCHUNCK B G.Determining optical flow [J].Artificial Intelligence, 1981, 17 (1/2/3) :185-203." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Determining optical flow">
                                        <b>[16]</b>
                                         HORN B K P, SCHUNCK B G.Determining optical flow [J].Artificial Intelligence, 1981, 17 (1/2/3) :185-203.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_17" title=" 周志华.机器学习[M].北京:清华大学出版社, 2016:171-173. (ZHOU Z H.Machine Learning [M].Beijing:Tsinghua University Press, 2016:171-173.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MTg2OTdOWE9ySTFOWStzUERCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2RnlublU3ak5JRjBUWEZxekdiQzRI&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         周志华.机器学习[M].北京:清华大学出版社, 2016:171-173. (ZHOU Z H.Machine Learning [M].Beijing:Tsinghua University Press, 2016:171-173.) 
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_18" title=" JIANG Y G, LIU J, ZAMIR A, et.al.Competition track evaluation setup, the first international workshop on action recognition with a large number of classes [EB/OL].[2018- 05- 20].http://www.crcv.ucf.edu/ICCV13-Action-Workshop/index.files/Competition_Track_Evaluation.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Competition track evaluation setup,the first international workshop on action recognition with a large number of classes">
                                        <b>[18]</b>
                                         JIANG Y G, LIU J, ZAMIR A, et.al.Competition track evaluation setup, the first international workshop on action recognition with a large number of classes [EB/OL].[2018- 05- 20].http://www.crcv.ucf.edu/ICCV13-Action-Workshop/index.files/Competition_Track_Evaluation.pdf.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-15 14:06</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(07),2081-2086 DOI:10.11772/j.issn.1001-9081.2019010156            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于视频分段的空时双通道卷积神经网络的行为识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E8%90%8D&amp;code=10180946&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王萍</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BA%9E%E6%96%87%E6%B5%A9&amp;code=42202201&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">庞文浩</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%AE%89%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0189085&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西安交通大学电子与信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对原始空时双通道卷积神经网络 (CNN) 模型对长时段复杂视频中行为识别率低的问题, 提出了一种基于视频分段的空时双通道卷积神经网络的行为识别方法。首先将视频分成多个等长不重叠的分段, 对每个分段随机采样得到代表视频静态特征的帧图像和代表运动特征的堆叠光流图像;然后将这两种图像分别输入到空域和时域卷积神经网络进行特征提取, 再在两个通道分别融合各视频分段特征得到空域和时域的类别预测特征;最后集成双通道的预测特征得到视频行为识别结果。通过实验讨论了多种数据增强方法和迁移学习方案以解决训练样本不足导致的过拟合问题, 分析了不同分段数、预训练网络、分段特征融合方案和双通道集成策略对行为识别性能的影响。实验结果显示所提模型在UCF101数据集上的行为识别准确率达到91.80%, 比原始的双通道模型提高了3.8个百分点;同时在HMDB51数据集上的行为识别准确率也比原模型提高, 达到61.39%, 这表明所提模型能够更好地学习和表达长时段复杂视频中人体行为特征。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E9%80%9A%E9%81%93%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双通道卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">行为识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E9%A2%91%E5%88%86%E6%AE%B5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视频分段;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">迁移学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征融合;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *王萍 (1976—) , 女, 陕西西安人, 副教授, 博士, 主要研究方向:视频编码、视频分析;电子邮箱ping.fu@xjtu.edu.cn;
                                </span>
                                <span>
                                    庞文浩 (1994—) , 男, 山东临沂人, 硕士研究生, 主要研究方向:视频分类、视频摘要。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-22</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61671365);</span>
                    </p>
            </div>
                    <h1><b>Two-stream CNN for action recognition based on video segmentation</b></h1>
                    <h2>
                    <span>WANG Ping</span>
                    <span>PANG Wenhao</span>
            </h2>
                    <h2>
                    <span>School of Electronic and Information Engineering, Xi'an Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the issue that original spatial-temporal two-stream Convolutional Neural Network (CNN) model has low accuracy for action recognition in long and complex videos, a two-stream CNN for action recognition based on video segmentation was proposed. Firstly, a video was split into multiple non-overlapping segments with same length. For each segment, one frame image was sampled randomly to represent its static features and stacked optical flow images were calculated to represent its motion features. Secondly, these two patterns of images were input into the spatial CNN and temporal CNN for feature extraction, respectively. And the classification prediction features of spatial and temporal domains for action recognition were obtained by merging all segment features in two streams respectively. Finally, the two-steam predictive features were integrated to obtain the action recognition results for the video. In series of experiments, some data augmentation techniques and transfer learning methods were discussed to solve the problem of over-fitting caused by the lack of training samples. The effects of various factors including the number of segments, network architectures, feature fusion schemes based on segmentation and two-stream integration strategy on the performance of action recognition were analyzed. The experimental results show that the accuracy of action recognition of the proposed model on dataset UCF101 reaches 91.80%, which is 3.8% higher than that of original two-stream CNN model; and the accuracy of the proposed model on dataset HMDB51 is improved to 61.39%, which is higher than that of the original model. It shows that the proposed model can better learn and express the action features in long and complex videos.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=two-stream%20Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">two-stream Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=action%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">action recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=video%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">video segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=transfer%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">transfer learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature fusion;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Ping, born in 1976, Ph. D. , associate professor. Her research interests include video coding, video analysis. ;
                                </span>
                                <span>
                                    PANG Wenhao, born in 1994, M. S. candidate. His research interests include video classification, video summarization.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-01-22</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61671365);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="39" name="39" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="40">人类从外界获取信息时, 视觉信息占各种器官获取信息总量的80%<citation id="158" type="reference"><link href="122" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 这些信息对于了解事物本质具有重要的意义。随着移动互联网和电子技术的飞速发展, 手机等视频采集设备大量普及, 互联网短视频应用也如雨后春笋般出现, 极大降低了视频拍摄和分享的成本, 这使得网络视频资源爆炸式增长。这些资源丰富了人们的生活, 但由于其数量庞大、种类繁多、内容庞杂, 如何对这些视频数据进行智能分析、理解、识别成为急需面对的挑战。</p>
                </div>
                <div class="p1">
                    <p id="41">人体行为识别是计算机视觉<citation id="159" type="reference"><link href="124" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>领域一个重要的研究方向, 其主要内容是利用计算机模拟人脑分析和识别视频中的人体行为, 通常包括人的个体动作、人与人之间以及人与外界环境之间的交互行为。空时双通道神经网络可以从空域和时域两个角度表征视频的特征, 相比其他神经网络模型在人体行为识别上更有优势。本文基于视频分段利用空时双通道神经网络提取空域的帧图像特征和时域的运动特征, 并将各分段的空域和时域的识别结果进行融合, 最后得到整段视频的行为识别分类。</p>
                </div>
                <h3 id="42" name="42" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="43">在传统的基于人工设计特征的行为识别方法中, 早期的基于人体几何或者运动信息的特征仅适用于简单场景下的人体简单动作识别, 而在背景相对复杂的情况下基于时空兴趣点的方法效果较好。这些方法首先获取视频中的时空兴趣点或稠密采样点, 并根据这些点周围的时空块计算局部特征, 再利用经典的特征袋 (Bag of Features, BoF) 、VLAD (Vector of Locally Aggregated Descriptors) 或FV (Fisher Vector) 等特征编码方法最终形成描述视频动作的特征向量。目前在基于局部特征的方法中, 基于稠密轨迹 (Dense Trajectory, DT) 的行为识别方法在很多公开的真实场景行为数据库中得到了较好的识别结果, 它们通过跟踪视频每一帧内的稠密采样点获取稠密轨迹, 再计算轨迹特征描述视频中行为。如:Cai等<citation id="160" type="reference"><link href="126" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>用多视角超向量 (Multi-View Super Vector, MVSV) 作为全局描述符来编码稠密轨迹特征;Wang等<citation id="161" type="reference"><link href="128" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>使用FV编码改进的稠密轨迹 (improved Dense Trajectory, iDT) 特征;Peng等<citation id="162" type="reference"><link href="130" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>使用视觉词袋模型 (Bag of Visual Words, BoVW) 编码空时兴趣点或改进的稠密轨迹特征;Wang等<citation id="163" type="reference"><link href="132" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>基于稠密轨迹特征提出了一种视频的多级表示模型MoFAP (Motion Features, Atoms, and Phrases) , 可以分级地表示视觉信息。稠密轨迹能够以更广的覆盖面和更细的颗粒度提取行为特征, 但通常存在大量轨迹冗余而限制了识别效果。</p>
                </div>
                <div class="p1">
                    <p id="44">随着深度学习尤其是卷积神经网络 (Convolutional Neural Network, CNN) 在语音和图像识别等领域的成功运用, 近年来出现了多种基于深度学习框架的人体行为识别方法, 当训练样本足够多时可通过深度网络学习到具有一定语义的特征, 更适合于目标和行为的识别。Karpathy等<citation id="164" type="reference"><link href="134" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>训练深度网络DeepNet, 利用慢融合模型对视频中不同图像帧特征进行融合, 然而该模型无法提取视频的运动信息, 因此效果并不理想。Tran等<citation id="165" type="reference"><link href="136" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>为了利用视频中的时域特性, 将二维卷积推广到三维卷积, 使用3D-CNN (3-Dimensional Convolutional Neural Network) 深度网络学习空时特征, 该网络在避免处理光流的情况下获得了视频的运动特征, 但时域信息提取能力有限, 对长时段复杂的人体行为识别效果提升并不明显。Varol等<citation id="166" type="reference"><link href="138" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>在定长时间的视频块内使用三维空时卷积特征, 进一步提升了行为识别效果。</p>
                </div>
                <div class="p1">
                    <p id="45">Simonyan等<citation id="167" type="reference"><link href="140" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>首先提出了使用两个数据流 (Two-stream) 的卷积神经网络进行视频行为识别, 空域网络的输入数据流是静态帧图像, 时域网络的输入数据流是表征帧间运动的光流, 每个数据流都使用深度卷积神经网络进行特征提取和动作预测, 最后融合两个数据流的结果进行最终动作的识别。该模型取得了与改进稠密轨迹法相似的识别性能。Ng等<citation id="168" type="reference"><link href="142" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>将长短期记忆 (Long-Short Term Memory, LSTM) 网络加入到原始双通道模型中, 用来加强时域信息的联系。最初双通道模型中使用的卷积网络层数较浅, Wang等<citation id="169" type="reference"><link href="144" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出采用在图像分类任务中性能更好的预训练深度网络模型如VGGNet、GoogLeNet, 增强了对视频运动特征的学习和建模能力。将手工特征和深度学习相结合也是一种研究趋势, Wang等<citation id="170" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>利用双通道神经网络学习卷积特征图, 并利用轨迹约束获得深度卷积特征描述子 (Trajectory-pooled Deep-convolutional Descriptors, TDD) , 之后用FV编码得到视频级表示。</p>
                </div>
                <h3 id="46" name="46" class="anchor-tag">2 基于视频分段的空时双通道行为识别</h3>
                <h4 class="anchor-tag" id="47" name="47">2.1 <b>整体框架</b></h4>
                <div class="p1">
                    <p id="48">最初的双通道方法从视频中随机采样单帧进行行为识别, 对于复杂行为或持续时间较长的视频, 视角变换和背景扰动会导致仅利用单帧图像无法有效表达视频的类别信息。为了对长时段复杂视频建立有效的识别模型, 本文基于视频分段应用空时双通道神经网络, 整体框架如图1所示。先将视频分成多个等长不重叠的分段, 对每个分段通过随机采样得到静态帧图像和包含运动信息的堆叠光流图像, 分别输入到空域和时域CNN进行特征提取;然后在各自通道内将各个分段的网络输出预测特征进行融合;最后集成融合两个通道的预测特征得到最终的行为识别结果。</p>
                </div>
                <div class="area_img" id="49">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907036_049.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于视频分段的空时双通道卷积神经网络的行为识别框架" src="Detail/GetImg?filename=images/JSJY201907036_049.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基于视频分段的空时双通道卷积神经网络的行为识别框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907036_049.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Two-stream CNN framework for action recognition based on video segmentation</p>

                </div>
                <div class="p1">
                    <p id="50">根据时长将视频分成<i>K</i>个等长的片段{<i>S</i><sub>1</sub>, <i>S</i><sub>2</sub>, …, <i>S</i><sub><i>K</i></sub>}, 基于分段的空时双通道卷积神经网络<i>Y</i>对行为的识别可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="51"><i>Y</i> (<i>S</i><sub>1</sub>, <i>S</i><sub>2</sub>, …, <i>S</i><sub><i>K</i></sub>) =<i>Η</i> (<i>g</i> (<b><i>F</i></b> (<b><i>T</i></b><sub>1</sub>;<b><i>W</i></b>) , <b><i>F</i></b> (<b><i>T</i></b><sub>2</sub>;<b><i>W</i></b>) , …, <b><i>F</i></b> (<b><i>T</i></b><sub><i>K</i></sub>;<b><i>W</i></b>) ) )      (1) </p>
                </div>
                <div class="p1">
                    <p id="53">其中:<b><i>T</i></b><sub><i>i</i></sub>表示视频第<i>i</i>个分段的随机采样, 空域中是RGB帧图像, 时域中是堆叠光流图像;<b><i>F</i></b> (<b><i>T</i></b><sub><i>i</i></sub>;<b><i>W</i></b>) 表示参数为<b><i>W</i></b>的卷积神经网络对<b><i>T</i></b><sub><i>i</i></sub>的特征提取, 其输出为对应类别数目维度的特征向量;分段融合函数<i>g</i>表示对<i>K</i>个分段特征以某种方法进行融合, 得到空域或者时域的特征;输出函数<i>H</i>表示对识别结果进行类别分类, 一般采用Softmax函数得到每个行为类别的概率值。此外, 每个视频分段的空域网络结构完全相同, 共享网络权值;时域网络结构亦如此。</p>
                </div>
                <h4 class="anchor-tag" id="54" name="54">2.2 <b>空域网络数据预处理</b></h4>
                <div class="p1">
                    <p id="55">空域网络是对视频中采样得到的静态<i>RGB</i>帧图像进行识别, 为了测试不同采样方式对行为识别性能的影响, 使用<i>UCF</i>101数据集的<i>Split</i>1训练/测试分割方案, 测试<i>Top</i>- 1行为识别准确率 (即网络输出中最大概率的类别是正确的识别结果) 。表1列出了三种采样策略的识别性能, 在网络训练过程中, 采用了<i>GoogLeNet</i>卷积神经网络的改进版本<i>InceptionV</i>3模型<citation id="171" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。可以看到, 采样帧数增加并未提升识别性能, 反而增加了数据冗余, 增大了计算复杂度, 因此, 对视频进行密集采样并不可取, 本文实验中对于K个等长的视频分段, 每个分段随机采样1帧图像。</p>
                </div>
                <div class="area_img" id="56">
                    <p class="img_tit"><b>表</b>1 <b>空域通道不同采样方式下数据集规模及行为识别准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Comparison of spatial CNN dataset size and</i><i>action recognition accuracy under different sampling modes</i></p>
                    <p class="img_note"></p>
                    <table id="56" border="1"><tr><td rowspan="2"><br />采样方法</td><td colspan="2"><br />采样帧数</td><td rowspan="2"><i>Top</i>- 1识别<br />准确率/%</td></tr><tr><td><br />训练集</td><td>测试集</td></tr><tr><td><br />全采样</td><td>1 791 290</td><td>695 000</td><td>76.25</td></tr><tr><td><br />60帧采样</td><td>571 430</td><td>226 413</td><td>77.62</td></tr><tr><td><br />10帧采样</td><td>95 238</td><td>33 735</td><td>77.50</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="57">为了防止学习建模中的过拟合问题<citation id="172" type="reference"><link href="150" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 通常会采用数据增强技术, 这不仅能扩增输入数据的规模、增加样本的差异性, 还能增强网络模型的泛化能力。在空域网络中, 本文对视频帧使用水平翻转、角度旋转、平移变换、错切变换等数据增强方法, 并在<i>InceptionV</i>3网络模型上测试了这些方法对行为识别性能的影响。表2列出了5种情况下的<i>Top</i>- 1和<i>Top</i>- 5识别准确率。可以看到, 缺少任一种数据增强技术, 识别准确率均有下降, 这说明了数据增强方法的有效性, 因此本文实验中采用全部4种数据增强技术。</p>
                </div>
                <div class="area_img" id="58">
                    <p class="img_tit"><b>表</b>2 <b>空域通道不同数据增强技术下行为识别准确率对比</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Comparison of spatial CNN action recognition accuracy under</i><i>different data augmentation techniques</i> %</p>
                    <p class="img_note"></p>
                    <table id="58" border="1"><tr><td rowspan="2"><br />数据增强技术</td><td colspan="2"><br />识别准确率</td></tr><tr><td><br /><i>Top</i>- 1</td><td><i>Top</i>- 5</td></tr><tr><td><br />缺少水平翻转</td><td>75.00</td><td>94.06</td></tr><tr><td><br />缺少角度旋转</td><td>73.50</td><td>91.87</td></tr><tr><td><br />缺少水平垂直偏移</td><td>73.12</td><td>89.69</td></tr><tr><td><br />缺少错切变换</td><td>72.19</td><td>90.94</td></tr><tr><td><br />四种均使用</td><td><b>77.62</b></td><td><b>96.32</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="59" name="59">2.3 <b>时域网络数据预处理</b></h4>
                <div class="p1">
                    <p id="60">视频中的运动信息对于行为识别至关重要, 光流是一种简单实用的表达图像序列运动信息的方式, 被广泛用于提取行为运动特征。Horn等<citation id="173" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>基于两个基本假设推导了图像序列光流的计算公式, 本文使用该方法计算水平和垂直两方向的光流。因光流数值接近0且有正有负, 为了能够作为时域网络通道的输入, 需要对其进行线性变换, 最终将两个方向的光流保存为两张灰度图像, 如图2所示。为了有效提取视频的运动信息, 本文采用10个连续帧的水平和垂直光流堆叠形成20个密集光流图像。</p>
                </div>
                <div class="p1">
                    <p id="61">空域和时域中通常会采用预先在ImageNet上训练的CNN, 这些网络的输入是RGB图像, 因此第一个卷积层的通道数为3, 但时域网络输入20个光流图像, 与第一个卷积层的通道数不匹配, 这里采用跨模态交叉预训练的方法, 将第一个卷积层的3个通道的权值取平均, 再将其复制20份作为时域网络第一个卷积层20个通道的权值;而时域网络其他层的权值与空域对应层的权值参数相同。</p>
                </div>
                <h4 class="anchor-tag" id="62" name="62">2.4 <b>迁移学习</b></h4>
                <div class="p1">
                    <p id="63">机器学习方法需要有足够的训练样本才能学习到一个好的分类模型, 但实际中针对目标任务的现有样本往往规模较小, 而人为标注大量样本不仅费时费力, 还会受标注者主观因素的影响。迁移学习方法能够使用预训练模型解决目标任务数据不足的问题, 对于新目标任务, 使用时需要将预训练网络模型中最后一个用于分类的全连接层替换成新的针对目标任务类别数目的全连接层。本文采用在ImageNet上预训练的残差网络模型ResNet50/101对UCF101数据集进行行为识别, 迁移学习时需要将最后一个全连接层设置为对应的101类输出。</p>
                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907036_064.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 连续两帧图像与对应光流图像" src="Detail/GetImg?filename=images/JSJY201907036_064.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 连续两帧图像与对应光流图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907036_064.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Two consecutive video frames and corresponding optical flow images</p>

                </div>
                <div class="p1">
                    <p id="65">实验中对比了两种迁移学习方案:一种是仅对卷积神经网络的最后一个分类层进行权值更新;另一种是微调整个网络更新所有权值。两种方案的识别准确率如表3所示。可以看到, 采用微调整个网络的方案可以获得更好的识别性能, Top- 1及Top- 5准确率均高于仅微调最后一层的方案, 因此本文实验中采用微调整个网络的迁移学习方案。</p>
                </div>
                <div class="area_img" id="66">
                    <p class="img_tit"><b>表</b>3 <b>空域通道不同迁移学习方案下行为识别准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Comparison of spatial CNN action recognition accuracy under different transfer learning schemes </p>
                    <p class="img_note">%</p>
                    <table id="66" border="1"><tr><td rowspan="2"><br />网络模型</td><td rowspan="2">迁移学习方案</td><td colspan="2"><br />识别准确率</td></tr><tr><td><br />Top- 1</td><td>Top5</td></tr><tr><td rowspan="2"><br />ResNet101</td><td><br />微调最后一层</td><td>70.21</td><td>90.40</td></tr><tr><td><br />微调整个网络</td><td><b>80.89</b></td><td><b>95.51</b></td></tr><tr><td rowspan="2"><br />ResNet50</td><td><br />微调最后一层</td><td>70.02</td><td>90.19</td></tr><tr><td><br />微调整个网络</td><td><b>78.91</b></td><td><b>93.97</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="67" name="67">2.5 <b>单通道分段特征融合</b></h4>
                <div class="p1">
                    <p id="68">基于视频分段的空时双通道模型包含独立的空域和时域卷积神经网络, 两个网络在结构上除了第一层的输入通道数不同, 其他层参数完全相同。单通道分段融合是指在单个通道中将各个视频分段的网络输出通过某种方式融合, 得到该通道的行为识别结果。本文设计了基于最大值、均值和方差三种分段特征融合方案。</p>
                </div>
                <div class="p1">
                    <p id="69">迁移学习后网络最后一个全连接层输出的特征向量其维度对应于类别数目, 越大特征值对应的类别可能性越大。最大值分段特征融合指取所有分段对应类别输出特征值中的最大值作为该类别的特征输出, 这是一种对每个类别取最有可能模式的融合策略。均值分段特征融合指取所有分段对应类别输出特征值的平均值, 这种策略平等看待每个分段中的行为信息。基于方差的分段特征融合策略是根据分段输出特征的方差对分段的重要性进行区分, 方差较大, 对应输出特征离散程度较大, 说明有可显著识别的行为类别, 这样的特征对视频的行为识别贡献度应该高, 因此给该分段赋予较大权重;反之, 分段输出特征的方差小, 说明输出特征离散程度小, 其对行为识别辨识度低, 重要性低, 融合时权重也较小。</p>
                </div>
                <h4 class="anchor-tag" id="70" name="70">2.6 <b>双通道特征集成</b></h4>
                <div class="p1">
                    <p id="71">双通道模型中的空域和时域两个CNN彼此独立, 在各自通道对分段特征融合后, 还需融合空域和时域的识别结果。本文基于集成学习<citation id="174" type="reference"><link href="154" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>的思想, 讨论试凑集成和方差集成两种空时特征集成方案, 以实现识别性能的进一步提升。</p>
                </div>
                <div class="p1">
                    <p id="72">试凑集成的方法通过设置加权系数<i>θ</i><sub>spatial</sub>和<i>θ</i><sub>temporal</sub>对分段融合后的空域和时域特征进行加权求和得到双通道输出特征, 最终以最大特征值对应类别为识别结果。一般来说, 时域中的运动信息对行为识别更为重要, 因此可设置较大权重。方差集成的方法以融合后的空域和时域特征向量的方差作为加权系数, 对两个通道的重要性进行区分。</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag">3 结果分析</h3>
                <h4 class="anchor-tag" id="74" name="74">3.1 <b>基本参数设置</b></h4>
                <div class="p1">
                    <p id="75">本文实验在Linux系统下基于PyTorch0.3.0深度学习框架进行。双通道网络的基本参数设置如表4所示, 包括初始学习速率、Batch-size大小以及动量。本文采用预训练的网络模型对UCF101数据集进行行为识别, 使用较小的学习速率将有利于网络的训练。空域网络的初始学习速率设置为0.000 5;时域网络由于其输入数据为光流图像, 与RGB图像存在一定差异, 设置相对较大的初始学习速率将有利于网络的快速收敛, 实验中设置为0.01。优化时学习速率采用自适应方法, 根据学习结果自动更新学习速率。从内存容量、使用率以及收敛速度等方面考虑将Batch-size设置为32。为了有效加速网络的收敛, 动量的设置遵循传统双通道行为识别方法<citation id="175" type="reference"><link href="140" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 设置为0.9。空域和时域网络训练时均采用交叉熵损失函数作为优化目标函数, 优化方法为随机梯度下降算法。</p>
                </div>
                <div class="p1">
                    <p id="76">UCF101数据集中, 训练集包含9 537个视频, 测试集包含3 783个视频。每个轮回的训练共需要300次迭代, 每次迭代时随机选取32个视频作为训练样本, 每个样本采用前述数据增强方法后被裁剪为网络输入的尺寸224×224, 并且进行归一化操作。每个轮回的训练完成后对测试集进行测试, 以检验学习模型的性能, 测试时遵循THUMOS13挑战机制<citation id="176" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="77">
                    <p class="img_tit"><b>表</b>4 <b>双通道网络基本参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Two-stream CNN parameter setting</p>
                    <p class="img_note"></p>
                    <table id="77" border="1"><tr><td><br />通道</td><td>初始学习速率</td><td>Batch-size</td><td>动量</td></tr><tr><td><br />空域</td><td>5E-4</td><td>32</td><td>0.9</td></tr><tr><td><br />时域</td><td>1E-2</td><td>32</td><td>0.9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="78" name="78">3.2 <b>不同分段数目下行为识别性能分析</b></h4>
                <div class="p1">
                    <p id="79">为了对长时段视频进行有效建模, 本文将视频分成<i>K</i>个等长的分段:分段数目较少时, 会导致行为信息提取不足、训练模型过于简单;而分段数目较多又会导致数据冗余, 增加计算量。表5给出了采用ResNet50/101网络时在不同视频分段数目下的空域通道行为识别性能。可以看到, 当视频分成3个分段时, 其行为识别性能较好, 因此后续实验中将视频分段数目设置为3。</p>
                </div>
                <div class="area_img" id="80">
                    <p class="img_tit"><b>表</b>5 <b>空域通道不同视频分段数目下行为识别准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Comparison of spatial CNN action recognition accuracy under different number of video segments </p>
                    <p class="img_note">%</p>
                    <table id="80" border="1"><tr><td rowspan="2"><br />网络结构</td><td colspan="6"><br />识别准确率 (Top- 1) </td></tr><tr><td><br /><i>K</i>=1</td><td><i>K</i>=2</td><td><i>K</i>=3</td><td><i>K</i>=4</td><td><i>K</i>=5</td><td><i>K</i>=6</td></tr><tr><td><br />ResNet50</td><td>77.39</td><td>78.03</td><td><b>78.91</b></td><td>78.25</td><td>77.85</td><td>77.90</td></tr><tr><td><br />ResNet101</td><td>78.98</td><td>79.93</td><td><b>80.89</b></td><td>79.89</td><td>78.90</td><td>79.01</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="81" name="81">3.3 <b>不同网络结构下行为识别性能分析</b></h4>
                <div class="p1">
                    <p id="82">表6给出了ResNet18/50/101、GoogLeNet网络改进版本InceptionV3及Inception_ResNet_v2等5种网络在空域通道的行为识别性能, 表7给出了ResNet18/50/101等3种网络在时域通道的行为识别性能。</p>
                </div>
                <div class="p1">
                    <p id="83">从表6～7中可以看到, 相比其他网络结构, ResNet101在空域通道和时域通道均取得了最高的行为识别准确率, Top- 1准确率分别达到了82.24%和83.48%。此外也看到ResNet18/50/101等3种残差网络的识别性能随着网络深度的增加而提高, 这说明了卷积神经网络的深度对行为识别的重要性。</p>
                </div>
                <div class="area_img" id="84">
                    <p class="img_tit"><b>表</b>6 <b>空域通道不同网络结构的行为识别准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 6 Comparison of spatial CNN action recognition accuracy under different network architectures </p>
                    <p class="img_note">%</p>
                    <table id="84" border="1"><tr><td rowspan="2"><br />网络结构</td><td colspan="2"><br />识别准确率</td></tr><tr><td><br />Top- 1</td><td>Top- 5</td></tr><tr><td><br />ResNet18</td><td>73.88</td><td>93.07</td></tr><tr><td><br />ResNet50</td><td>79.01</td><td>95.53</td></tr><tr><td><br />ResNet101</td><td><b>82.24</b></td><td><b>95.93</b></td></tr><tr><td><br />InceptionV3</td><td>77.50</td><td>93.71</td></tr><tr><td><br />Inception_ResNet_v2</td><td>78.75</td><td>94.06</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="85">
                    <p class="img_tit"><b>表</b>7 <b>时域通道不同网络结构的行为识别准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 7 Comparison of temporal CNN action recognition accuracy under different network architectures </p>
                    <p class="img_note">%</p>
                    <table id="85" border="1"><tr><td rowspan="2"><br />网络结构</td><td colspan="2"><br />识别准确率</td></tr><tr><td><br />Top- 1</td><td>Top- 5</td></tr><tr><td><br />ResNet18</td><td>76.32</td><td>93.39</td></tr><tr><td><br />ResNet50</td><td>77.51</td><td>94.29</td></tr><tr><td><br />ResNet101</td><td><b>83.48</b></td><td><b>96.17</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="86" name="86">3.4 <b>不同分段融合方案下行为识别性能分析</b></h4>
                <div class="p1">
                    <p id="87">实验中将每个视频分为3个等长的分段, 空域通道输出的101维特征向量代表输入分段的空域行为识别结果, 如前所述, 对3个分段的101维特征融合后经过Softmax函数后即可得到整个空域通道的行为识别结果。对时域通道亦如此。表8和表9给出了几种网络结构在不同分段融合方案下的行为识别性能。实验中先对ResNet18残差网络在空时双通道中均采用了基于均值、最大值以及方差的分段融合方案。可以看到, 基于均值的方案都取得了较佳的识别性能, 而基于最大值的方案总体性能较差, 这可能是因为视频分段内容的差异会导致判别误差较大, 因此对ResNet50和ResNet101网络结构不再采用基于最大值的分段融合方案。可以看到, 随着网络深度的增加, 基于均值的融合方案识别性能仍是较好, 而且考虑到均值融合方案的计算更简单, 因此基于各分段输出特征的平均值更适合作为分段融合方案。</p>
                </div>
                <div class="area_img" id="88">
                    <p class="img_tit"><b>表</b>8 <b>空域通道不同分段融合方案下行为识别准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 8 Comparison of spatial CNN action recognition accuracy under different fusion schemes based on segmentation </p>
                    <p class="img_note">%</p>
                    <table id="88" border="1"><tr><td rowspan="2"><br />网络结构</td><td rowspan="2">分段融合方案</td><td colspan="2"><br />识别准确率</td></tr><tr><td><br />Top- 1</td><td>Top- 5</td></tr><tr><td rowspan="3"><br />ResNet18</td><td><br />均值</td><td><b>73.88</b></td><td><b>93.07</b></td></tr><tr><td><br />最大值</td><td>69.73</td><td>88.79</td></tr><tr><td><br />方差</td><td>70.90</td><td>91.20</td></tr><tr><td rowspan="2"><br />ResNet50</td><td><br />均值</td><td><b>79.01</b></td><td><b>95.53</b></td></tr><tr><td><br />方差</td><td>78.91</td><td>94.40</td></tr><tr><td rowspan="2"><br />ResNet101</td><td><br />均值</td><td><b>82.24</b></td><td><b>95.93</b></td></tr><tr><td><br />方差</td><td>80.09</td><td>95.51</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="89">
                    <p class="img_tit"><b>表</b>9 <b>时域通道不同分段融合方案下行为识别准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 9 Comparison of temporal CNN action recognition accuracy under different fusion schemes based on segmentation </p>
                    <p class="img_note">%</p>
                    <table id="89" border="1"><tr><td rowspan="2"><br />网络结构</td><td rowspan="2">分段融合方案</td><td colspan="2"><br />识别准确率</td></tr><tr><td><br />Top- 1</td><td>Top- 5</td></tr><tr><td rowspan="3"><br />ResNet18</td><td><br />均值</td><td><b>76.32</b></td><td><b>93.39</b></td></tr><tr><td><br />最大值</td><td>67.02</td><td>85.04</td></tr><tr><td><br />方差</td><td>65.40</td><td>88.45</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="90" name="90">3.5 <b>不同集成策略下行为识别性能分析</b></h4>
                <div class="p1">
                    <p id="91">试凑集成策略通过设置加权系数<i>θ</i><sub>spatial</sub>和<i>θ</i><sub>temporal</sub>对分段融合后的空域和时域特征进行加权求和, 得到最终的双通道输出特征。本文在ResNet101网络结构上采用多种权重比例进行空时双通道的集成, 行为识别性能如表10所示。可以看到, 当空域与时域的权重比例不断减小时, 识别准确率逐步上升, 这说明了相对于空域通道提取的静态特征, 时域通道提取的运动特征对行为识别有着更重要的作用。当权重比例为1∶3时, 识别性能最好, 此时单独空域通道的Top- 1准确率是82.24%, 单独时域通道的Top- 1准确率是83.48%, 而集成后Top- 1准确率达到了91.72%, 这说明了集成双通道特征可以有效提升行为识别性能。</p>
                </div>
                <div class="area_img" id="92">
                    <p class="img_tit"><b>表</b>10 <b>基于试凑集成方式的双通道行为识别准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 10 Comparison of two-stream action recognition accuracy based on trial and error integration </p>
                    <p class="img_note">%</p>
                    <table id="92" border="1"><tr><td rowspan="2"><br /><i>θ</i><sub>spatial </sub>∶<i>θ</i><sub>temporal</sub></td><td colspan="2"><br />识别准确率</td></tr><tr><td><br />Top- 1</td><td>Top- 5</td></tr><tr><td><br />5∶1</td><td>81.39</td><td>94.91</td></tr><tr><td><br />3∶1</td><td>83.26</td><td>95.58</td></tr><tr><td><br />2∶1</td><td>84.49</td><td>96.60</td></tr><tr><td><br />1∶1</td><td>88.27</td><td>97.96</td></tr><tr><td><br />1∶2</td><td>90.87</td><td>98.82</td></tr><tr><td><br />1∶3</td><td><b>91.72</b></td><td><b>98.90</b></td></tr><tr><td><br />1∶5</td><td>91.24</td><td>98.79</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="93">基于方差的集成方法是使用分段融合后的空域和时域特征向量的方差作为两通道的加权系数, 对空时两个学习器进行集成。表11列出了在ResNet101网络结构上采用基于方差的集成方法的行为识别性能, 其中Top- 1准确率仅为79.81%, 性能出现了下降, 这说明采用所有101个类别输出值的离散程度来对空域或时域进行重要性打分的评价标准不合理, 其结果会受到非预测类别输出值的干扰。为了减少这种干扰, 考虑到通常卷积神经网络输出的较大特征值对分类更具意义, 因此采用空时双通道输出的最大5个特征值的方差作为集成时的加权系数, 可以看到Top- 1识别准确率达到86.93%, 比采用101类方差集成的性能有所提升, 但与前述基于试凑方式获得的最好性能仍有差距。</p>
                </div>
                <div class="area_img" id="94">
                    <p class="img_tit"><b>表</b>11 <b>基于方差集成方式的双通道行为识别准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 11 Comparison of two-stream action recognition accuracy based on variance integration </p>
                    <p class="img_note">%</p>
                    <table id="94" border="1"><tr><td rowspan="2"><br />方差集成策略</td><td colspan="2"><br />识别准确率</td></tr><tr><td><br />Top- 1</td><td>Top- 5</td></tr><tr><td><br />101类方差</td><td>79.81</td><td>93.95</td></tr><tr><td><br />Top- 5方差</td><td>86.93</td><td>96.44</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="95" name="95">3.6 <b>与现有方法对比</b></h4>
                <div class="p1">
                    <p id="96">表12列出了本文方法与一些基于传统手工设计特征以及基于深度学习的方法在UCF101行为识别数据集上的性能对比。表中前4种基于稠密轨迹使用不同的特征编码方法得到视频级表示, 可以看到基于手工特征的方法识别准确率最高达到88.3%。表中后7种方法为基于深度学习的方法, 最早应用深度学习的DeepNet网络识别准确率仅有63.3%, 三维卷积神经网络3D-CNN的准确率是85.2%, 性能都低于最好的手工特征方法。原始双通道模型的识别准确率是88%, 加入LSTM循环神经网络后准确率是88.6%, 使用深层卷积神经网络的准确率达到90.9%。文献<citation id="177" type="reference">[<a class="sup">13</a>]</citation>结合深度特征和轨迹特征, 识别准确率是90.3%。本文在对长时段视频运动信息建模时采用了基于视频分段的空时双通道模型, 取得了91.8%的识别准确率, 相比原始的双通道方法, 准确率提升了3.8个百分点。这说明基于深度学习的方法随着多种网络模型及学习策略的应用, 可以取得比传统方法更好的识别性能。</p>
                </div>
                <div class="area_img" id="97">
                    <p class="img_tit"><b>表</b>12 <b>不同方法在</b>UCF101<b>数据集上行为识别准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 12 Action recognition accuracy comparison of different methods on dataset UCF101 </p>
                    <p class="img_note">%</p>
                    <table id="97" border="1"><tr><td>序号</td><td>方法</td><td>识别<br />准确率<br /> (Top- 1) </td><td></td><td>序号</td><td>方法</td><td>识别<br />准确率<br /> (Top- 1) </td></tr><tr><td>1</td><td>DT+MVSV<sup>[3]</sup></td><td>83.5</td><td></td><td>7</td><td>Two-stream<sup>[10]</sup></td><td>88.0</td></tr><tr><td><br />2</td><td>iDT+FV<sup>[4]</sup></td><td>85.9</td><td></td><td>8</td><td>Two-stream+LSTM<sup>[11]</sup></td><td>88.6</td></tr><tr><td><br />3</td><td>iDT+BoVW<sup>[5]</sup></td><td>87.9</td><td></td><td>9</td><td>Two-stream (GoogLeNet) <sup>[12]</sup></td><td>89.0</td></tr><tr><td><br />4</td><td>iDT+MoFAP<sup>[6]</sup></td><td>88.3</td><td></td><td>10</td><td>Two-stream (VGGNet- 16) <sup>[12]</sup></td><td>90.9</td></tr><tr><td><br />5</td><td>DeepNet<sup>[7]</sup></td><td>63.3</td><td></td><td>11</td><td>TDD+FV<sup>[13]</sup></td><td>90.3</td></tr><tr><td><br />6</td><td>3D-CNN<sup>[8]</sup></td><td>85.2</td><td></td><td>12</td><td>本文方法</td><td>91.8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="98" name="98">3.7 HMDB51<b>数据集行为识别性能分析</b></h4>
                <div class="p1">
                    <p id="99">基于视频分段的空时双通道卷积神经网络的行为识别方法在公开数据集UCF101上取得了不错的性能, 为了进一步检验算法的性能, 基于ResNet101网络模型在HMDB51数据集上进行了实验。该数据集包含51个行为类别共6 766个视频, 每个类别至少包含101个视频。HMDB51是目前数据集里最复杂的, 识别率最低的。使用该数据集学习分类模型时同样有3种训练/测试分割方案, 训练集有3 570个样本, 测试集有1 530个样本, 实验仍然在Split1训练/测试方案上进行。视频分段采用基于均值的融合方式, 空域和时域通道的Top- 1行为识别准确率分别为49.41%和45.22%。当双通道采用试凑方式集成, 空时权重比例系数为1∶2时, 双通道融合后Top- 1准确率达到61.39%, 比最初的空时双通道网络模型的行为识别准确率58%也有提高。HMDB51数据集上识别准确率较低主要是因为与UCF101数据集相比, HMDB51存在大量类间差别较小的行为, 比如面部吃和喝的运动、说话和微笑等等, 此外视频的规模和质量也对模型的学习及表达存在一定限制。</p>
                </div>
                <h3 id="100" name="100" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="101">本文实现了一种基于视频分段的空时双通道卷积神经网络的人体行为识别方法, 主要基于残差网络模型在UCF101数据集上进行了识别分类的训练和测试。为了解决因数据集样本不足造成的过拟合问题, 实验讨论分析了多种数据增强方法对空域网络识别准确率的影响;同时因为在采用ImageNet上预训练网络模型对目标数据集分类识别时需要调整网络, 从而讨论分析了两种迁移学习方案, 实验显示全局微调网络比仅微调最后一层可获得较大性能的提升。对基于分段的空时双通道模型, 通过实验讨论分析了不同视频分段数目、预训练网络结构、分段特征融合方法、空时特征集成策略等环节对识别性能的影响, 证明了融合双通道内各个视频分段的卷积神经网络输出特征的方法能够捕获视频中的行为运动特征, 提高了行为识别准确率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="122">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201601010&amp;v=MTAxNDc0SDlmTXJvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVTd6UEx5dlNkTEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 单言虎, 张彰, 黄凯奇.人的视觉行为识别研究回顾、现状及展望[J].计算机研究与发展, 2016, 53 (1) :93-112. (SHAN Y H, ZHANG Z, HUANG K Q.Review, current situation and prospect of human visual behavior recognition [J].Journal of Computer Research and Development, 2016, 53 (1) :93-112.) 
                            </a>
                        </p>
                        <p id="124">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Computer Vision:A Modern Approach">

                                <b>[2]</b> FORSYTH D A.Computer Vision:A Modern Approach[M].2nd ed.Englewood Cliffs, NJ:Prentice Hall, 2011:1-2.
                            </a>
                        </p>
                        <p id="126">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-view super vector for action recognition">

                                <b>[3]</b> CAI Z, WANG L, PENG X, et al.Multi-view super vector for action recognition[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:596-603.
                            </a>
                        </p>
                        <p id="128">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Action recognition with improved trajectories">

                                <b>[4]</b> WANG H, SCHMID C.Action recognition with improved trajectories[C]// Proceedings of the 2013 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2014:3551-3558.
                            </a>
                        </p>
                        <p id="130">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bag of visual words and fusion methods for action recognition:Comprehensive study and good practice">

                                <b>[5]</b> PENG X, WANG L, WANG X, et al.Bag of visual words and fusion methods for action recognition:comprehensive study and good practice [J].Computer Vision and Image Understanding, 2016, 150:109-125.
                            </a>
                        </p>
                        <p id="132">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MoFAP:Amulti-level representation for action recognition">

                                <b>[6]</b> WANG L, QIAO Y, TANG X.MoFAP:a multi-level representation for action recognition[J].International Journal of Computer Vision, 2016, 119 (3) :254-271.
                            </a>
                        </p>
                        <p id="134">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-scale video classification with convolutional neural networks">

                                <b>[7]</b> KARPATHY A, TODERICI G, SHETTY S, et al.Large-scale video classification with convolutional neural networks[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Rec-ognition.Washington, DC:IEEE Computer Society, 2014:1725-1732.
                            </a>
                        </p>
                        <p id="136">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatiotemporal features with 3d convolutional networks">

                                <b>[8]</b> TRAN D, BOURDEV L, FERGUS R, et al.Learning spatiotemporal features with 3D convolutional networks[C]// Proceedings of the 2014 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:4489-4497.
                            </a>
                        </p>
                        <p id="138">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long-term temporal convolutions for action recognition">

                                <b>[9]</b> VAROL G, LAPTEV I, SCHMID C.Long-term temporal convolutions for action recognition [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (6) :1510-1517.
                            </a>
                        </p>
                        <p id="140">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Two-stream convolutional networks for action recognition in videos">

                                <b>[10]</b> SIMONYAN K, ZISSERMAN A.Two-stream convolutional net-works for action recognition in videos[C]// Proceedings of the 2014 Conference on Neural Information Processing Systems.New York:Curran Associates, 2014:568-576.
                            </a>
                        </p>
                        <p id="142">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond short snippets:Deep networks for video classification">

                                <b>[11]</b> NG Y H, HAUSKNECHT M, VIJAYANARASIMHAN S, et al.Beyond short snippets:deep networks for video classification[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:4694-4702.
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Temporal segment networks:towards good practices for deep action recognition">

                                <b>[12]</b> WANG L M, XIONG Y J, WANG Z, et al.Temporal segment networks:towards good practices for deep action recognition [C]// Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:22-36.
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Action recognition with trajectory-pooled deep-convolutional descriptors">

                                <b>[13]</b> WANG L, QIAO Y, TANG X.Action recognition with trajectory-pooled deep-convolutional descriptors[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:4305-4314.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rethinking the inception architecture for computer vision">

                                <b>[14]</b> SZEGEDY C, VANHOUCKE V, IOFFE S, et al.Rethinking the inception architecture for computer vision[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:2818-2826.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Machine Learning:A Probabilistic Perspective">

                                <b>[15]</b> MURPHY K P.Machine Learning:A Probabilistic Perspective [M].Cambridge:MIT Press, 2012:22.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Determining optical flow">

                                <b>[16]</b> HORN B K P, SCHUNCK B G.Determining optical flow [J].Artificial Intelligence, 1981, 17 (1/2/3) :185-203.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MTQ3MjRHYkM0SE5YT3JJMU5ZK3NQREJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlplWnZGeW5uVTdqTklGMFRYRnF6&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> 周志华.机器学习[M].北京:清华大学出版社, 2016:171-173. (ZHOU Z H.Machine Learning [M].Beijing:Tsinghua University Press, 2016:171-173.) 
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Competition track evaluation setup,the first international workshop on action recognition with a large number of classes">

                                <b>[18]</b> JIANG Y G, LIU J, ZAMIR A, et.al.Competition track evaluation setup, the first international workshop on action recognition with a large number of classes [EB/OL].[2018- 05- 20].http://www.crcv.ucf.edu/ICCV13-Action-Workshop/index.files/Competition_Track_Evaluation.pdf.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201907036" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907036&amp;v=MDU3MDF6N0JkN0c0SDlqTXFJOUdZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVTd6UEw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
