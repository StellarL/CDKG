<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136466221065000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201909005%26RESULT%3d1%26SIGN%3d%252b52cLCXyUuC9SE4HlRdRUYcAyfo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909005&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909005&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909005&amp;v=MzAyNjlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amxVci9PTHo3QmQ3RzRIOWpNcG85RllZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#43" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#50" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="2 本文主要方法 ">2 本文主要方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="2.1 &lt;b&gt;结合语义边界信息的语义分割模型&lt;/b&gt;">2.1 <b>结合语义边界信息的语义分割模型</b></a></li>
                                                <li><a href="#64" data-title="2.2 &lt;b&gt;结合语义边界检测的语义分割模型训练方法&lt;/b&gt;">2.2 <b>结合语义边界检测的语义分割模型训练方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="3 实验及分析 ">3 实验及分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="3.1 &lt;b&gt;实验方法&lt;/b&gt;">3.1 <b>实验方法</b></a></li>
                                                <li><a href="#83" data-title="3.2 &lt;b&gt;语义边界检测子网络的结果及分析&lt;/b&gt;">3.2 <b>语义边界检测子网络的结果及分析</b></a></li>
                                                <li><a href="#85" data-title="3.3 &lt;b&gt;语义分割结果及分析&lt;/b&gt;">3.3 <b>语义分割结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#96" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="图1 本文提出的方法">图1 本文提出的方法</a></li>
                                                <li><a href="#60" data-title="图2 本文所设计的模型结构">图2 本文所设计的模型结构</a></li>
                                                <li><a href="#82" data-title="图3 语义分割和语义边界检测收敛过程">图3 语义分割和语义边界检测收敛过程</a></li>
                                                <li><a href="#89" data-title="图4 语义边界检测子网络的输出结果比对">图4 语义边界检测子网络的输出结果比对</a></li>
                                                <li><a href="#91" data-title="图5 加入语义边界子网络后的语义分割结果比对">图5 加入语义边界子网络后的语义分割结果比对</a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;本文方法与其他方法的语义分割精度对比&lt;/b&gt;"><b>表</b>1 <b>本文方法与其他方法的语义分割精度对比</b></a></li>
                                                <li><a href="#95" data-title="图6 本文方法与其他方法语义分割结果对比">图6 本文方法与其他方法语义分割结果对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="114">


                                    <a id="bibliography_1" title=" ALVAREZ J M,GEVERS T,LeCUN Y,et al.Road scene segmentation from a single image [C]// ECCV &#39;12:Proceedings of the 12th European Conference on Computer Vision,LNCS 7578.Berlin:Springer,2012:376-389." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Road scene segmentation from a single image">
                                        <b>[1]</b>
                                         ALVAREZ J M,GEVERS T,LeCUN Y,et al.Road scene segmentation from a single image [C]// ECCV &#39;12:Proceedings of the 12th European Conference on Computer Vision,LNCS 7578.Berlin:Springer,2012:376-389.
                                    </a>
                                </li>
                                <li id="116">


                                    <a id="bibliography_2" title=" BRUST C,SICKERT S,SIMON M,et al.Convolutional patch networks with spatial prior for road detection and urban scene understanding [EB/OL].[2019- 01- 21].https://arxiv.org/pdf/1502.06344.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional patch networks with spatial prior for road detection and urban scene understanding">
                                        <b>[2]</b>
                                         BRUST C,SICKERT S,SIMON M,et al.Convolutional patch networks with spatial prior for road detection and urban scene understanding [EB/OL].[2019- 01- 21].https://arxiv.org/pdf/1502.06344.pdf.
                                    </a>
                                </li>
                                <li id="118">


                                    <a id="bibliography_3" title=" HARIHARAN B,ARBEL&#193;EZ P,GIRSHICK R,et al.Simultaneous detection and segmentation [C]// Proceedings of the 13th European Conference on Computer Vision,LNCS 8695.Berlin:Springer,2014:297-312." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simultaneous detection and segmentation">
                                        <b>[3]</b>
                                         HARIHARAN B,ARBEL&#193;EZ P,GIRSHICK R,et al.Simultaneous detection and segmentation [C]// Proceedings of the 13th European Conference on Computer Vision,LNCS 8695.Berlin:Springer,2014:297-312.
                                    </a>
                                </li>
                                <li id="120">


                                    <a id="bibliography_4" title=" 高凯珺,孙韶媛,姚广顺.基于深度学习的无人车夜视图像语义分割[J].应用光学,2017,38(3):421-428.(GAO K J,SUN S Y,YAO G S.Semantic segmentation of night vision images for unmanned vehicles based on deep learning [J].Journal of Applied Optics,2017,38(3):421-428.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YYGX201703013&amp;v=MzI1MjJUTWRyRzRIOWJNckk5RVo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amxVci9PUEQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         高凯珺,孙韶媛,姚广顺.基于深度学习的无人车夜视图像语义分割[J].应用光学,2017,38(3):421-428.(GAO K J,SUN S Y,YAO G S.Semantic segmentation of night vision images for unmanned vehicles based on deep learning [J].Journal of Applied Optics,2017,38(3):421-428.)
                                    </a>
                                </li>
                                <li id="122">


                                    <a id="bibliography_5" title=" 吴宗胜,傅卫平,韩改宁.基于深度卷积神经网络的道路场景理解[J].计算机工程与应用,2017,53(22):8-15.(WU Z S,FU W P,HAN G N.Road scene understanding based on deep convolutional neural network [J].Computer Engineering and Applications,2017,53(22):8-15.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201722002&amp;v=MjMxNDFyQ1VSN3FmWnVac0Z5amxVci9PTHo3TWFiRzRIOWJPclk5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         吴宗胜,傅卫平,韩改宁.基于深度卷积神经网络的道路场景理解[J].计算机工程与应用,2017,53(22):8-15.(WU Z S,FU W P,HAN G N.Road scene understanding based on deep convolutional neural network [J].Computer Engineering and Applications,2017,53(22):8-15.)
                                    </a>
                                </li>
                                <li id="124">


                                    <a id="bibliography_6" title=" 张军阳,王慧丽,郭阳,等.深度学习相关研究综述[J].计算机应用研究,2018,35(7):1921-1928.(ZHANG J Y,WANG H L,GUO Y,et al.Review of deep learning [J].Application Research of Computers,2018,35(7):1921-1928.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201807002&amp;v=MjMxMDF6N1NaTEc0SDluTXFJOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVXIvT0w=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         张军阳,王慧丽,郭阳,等.深度学习相关研究综述[J].计算机应用研究,2018,35(7):1921-1928.(ZHANG J Y,WANG H L,GUO Y,et al.Review of deep learning [J].Application Research of Computers,2018,35(7):1921-1928.)
                                    </a>
                                </li>
                                <li id="126">


                                    <a id="bibliography_7" title=" YE L,LIU Z,WANG Y.Learning semantic segmentation with diverse supervision [C]// Proceedings of the 2018 IEEE Winter Conference on Applications of Computer Vision.Washington,DC:IEEE Computer Society,2018:1461-1469." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning semantic segmentation with diverse supervision">
                                        <b>[7]</b>
                                         YE L,LIU Z,WANG Y.Learning semantic segmentation with diverse supervision [C]// Proceedings of the 2018 IEEE Winter Conference on Applications of Computer Vision.Washington,DC:IEEE Computer Society,2018:1461-1469.
                                    </a>
                                </li>
                                <li id="128">


                                    <a id="bibliography_8" title=" CHEN Y,ROHRBACH M,YAN Z,et al.Graph-based global reasoning networks [EB/OL].[2018- 12- 10].https://arxiv.org/pdf/1811.12814v1.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Graph-based global reasoning networks">
                                        <b>[8]</b>
                                         CHEN Y,ROHRBACH M,YAN Z,et al.Graph-based global reasoning networks [EB/OL].[2018- 12- 10].https://arxiv.org/pdf/1811.12814v1.pdf.
                                    </a>
                                </li>
                                <li id="130">


                                    <a id="bibliography_9" title=" PASZKE ADAM,CHAURASIA A,KIM S,et al.ENet:a deep neural network architecture for real-time semantic segmentation [EB/OL].[2018- 12- 10].https://arxiv.org/pdf/1606.02147.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ENet:a deep neural network architecture for real-time semantic segmentation">
                                        <b>[9]</b>
                                         PASZKE ADAM,CHAURASIA A,KIM S,et al.ENet:a deep neural network architecture for real-time semantic segmentation [EB/OL].[2018- 12- 10].https://arxiv.org/pdf/1606.02147.pdf.
                                    </a>
                                </li>
                                <li id="132">


                                    <a id="bibliography_10" title=" ROMERA E,&#193;LVAREZ J M,BERGASA L M,et al.ERFNet:efficient residual factorized convnet for real-time semantic segmentation [J].IEEE Transactions on Intelligent Transportation Systems,2018,19(1):263-272." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ERFNet:Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation">
                                        <b>[10]</b>
                                         ROMERA E,&#193;LVAREZ J M,BERGASA L M,et al.ERFNet:efficient residual factorized convnet for real-time semantic segmentation [J].IEEE Transactions on Intelligent Transportation Systems,2018,19(1):263-272.
                                    </a>
                                </li>
                                <li id="134">


                                    <a id="bibliography_11" title=" CORDTS M,OMRAN M,RAMOS S,et al.The cityscapes dataset for semantic urban scene understanding [C]// Proceedings of the 29th IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:3213-3223." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Cityscapes Dataset for Semantic Urban Scene Understanding">
                                        <b>[11]</b>
                                         CORDTS M,OMRAN M,RAMOS S,et al.The cityscapes dataset for semantic urban scene understanding [C]// Proceedings of the 29th IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:3213-3223.
                                    </a>
                                </li>
                                <li id="136">


                                    <a id="bibliography_12" title=" LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2015:3431-3440 ." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[12]</b>
                                         LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2015:3431-3440 .
                                    </a>
                                </li>
                                <li id="138">


                                    <a id="bibliography_13" title=" BADRINARAYANAN V,KENDALL A,CIPOLLA R.SegNet:a deep convolutional encoder-decoder architecture for image segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(12):2481-2495." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SegNet:A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation">
                                        <b>[13]</b>
                                         BADRINARAYANAN V,KENDALL A,CIPOLLA R.SegNet:a deep convolutional encoder-decoder architecture for image segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(12):2481-2495.
                                    </a>
                                </li>
                                <li id="140">


                                    <a id="bibliography_14" title=" CHEN L,PAPANDREOU G,KOKKINOS I,et al.Semantic image segmentation with deep convolutional nets and fully connected CRFs [EB/OL].[2018- 05- 10].https://arxiv.org/pdf/1412.7062.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs[C/OL]">
                                        <b>[14]</b>
                                         CHEN L,PAPANDREOU G,KOKKINOS I,et al.Semantic image segmentation with deep convolutional nets and fully connected CRFs [EB/OL].[2018- 05- 10].https://arxiv.org/pdf/1412.7062.pdf.
                                    </a>
                                </li>
                                <li id="142">


                                    <a id="bibliography_15" title=" CHEN L,PAPANDREOU G,KOKKINOS I,et al.DeepLab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(4):834-848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Lab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs">
                                        <b>[15]</b>
                                         CHEN L,PAPANDREOU G,KOKKINOS I,et al.DeepLab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(4):834-848.
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_16" title=" MARMANIS D,SCHINDLER K,WEGNER J D,et al.Classification with an edge:improving semantic image segmentation with boundary detection [J].ISPRS Journal of Photogrammetry and Remote Sensing,2018,135:158-172." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6BE46128E728C32B7A2EA6DC8830CD47&amp;v=MDI5MjhyTFUwNXRwaHhMaSt3SzQ9TmlmT2ZiWEthOVhLcm8xTkVld05CQTg2eldRVW16MElPWG1XM3hvOWVyTG5NYjZZQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         MARMANIS D,SCHINDLER K,WEGNER J D,et al.Classification with an edge:improving semantic image segmentation with boundary detection [J].ISPRS Journal of Photogrammetry and Remote Sensing,2018,135:158-172.
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_17" title=" HUANG Q,XIA C,ZHENG W,et al.Object boundary guided semantic segmentation [C]// Proceedings of the 13th Asian Conference on Computer Vision,LNCS 10111.Berlin:Springer,2016:197-212." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object boundary guided semantic segmentation">
                                        <b>[17]</b>
                                         HUANG Q,XIA C,ZHENG W,et al.Object boundary guided semantic segmentation [C]// Proceedings of the 13th Asian Conference on Computer Vision,LNCS 10111.Berlin:Springer,2016:197-212.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_18" title=" BERTASIUS GEDAS,SHI J,TORRESANI L.Semantic segmentation with boundary neural fields [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:3602-3610." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic segmentation with boundary neural fields">
                                        <b>[18]</b>
                                         BERTASIUS GEDAS,SHI J,TORRESANI L.Semantic segmentation with boundary neural fields [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:3602-3610.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_19" title=" LIU Y,CHENG M,HU X,et al.Richer convolutional features for edge detection [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017:5872-5881." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Richer Convolutional Features for Edge Detection">
                                        <b>[19]</b>
                                         LIU Y,CHENG M,HU X,et al.Richer convolutional features for edge detection [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017:5872-5881.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_20" title=" YANG J,PRICE B,COHEN S,et al.Object contour detection with a fully convolutional encoder-decoder network [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:193-202." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object contour detection with a fully convolutional encoder-decoder network">
                                        <b>[20]</b>
                                         YANG J,PRICE B,COHEN S,et al.Object contour detection with a fully convolutional encoder-decoder network [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:193-202.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-05-17 09:47</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(09),2505-2510 DOI:10.11772/j.issn.1001-9081.2019030488            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>结合语义边界信息的道路环境语义分割方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AE%8B%E5%B0%8F%E5%A8%9C&amp;code=40224749&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">宋小娜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%8A%AE%E6%8C%BA&amp;code=40224748&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">芮挺</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%96%B0%E6%99%B4&amp;code=38740062&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王新晴</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E8%A7%A3%E6%94%BE%E5%86%9B%E9%99%86%E5%86%9B%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E9%87%8E%E6%88%98%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=1701801&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国人民解放军陆军工程大学野战工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E5%8C%97%E6%B0%B4%E5%88%A9%E6%B0%B4%E7%94%B5%E5%A4%A7%E5%AD%A6%E6%9C%BA%E6%A2%B0%E5%AD%A6%E9%99%A2&amp;code=1699454&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华北水利水电大学机械学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>语义分割是实现道路语义环境解释的重要方法,深度学习语义分割由于卷积、池化及反卷积的作用使分割边界模糊、不连续以及小目标漏分错分,影响了分割效果,降低了分割精度。针对上述问题,提出了一种结合语义边界信息的新的语义分割方法,首先在语义分割深度模型中构建了一个语义边界检测子网,利用网络中的特征共享层将语义边界检测子网络学习到的语义边界信息传递给语义分割网络;然后结合语义边界检测任务和语义分割任务定义了新的模型代价函数,同时完成语义边界检测和语义分割两个任务,提升语义分割网络对物体边界的描述能力,提高语义分割质量。最后在Cityscapes数据集上进行一系列实验证明,结合语义边界信息的语义分割方法在准确率上比已有的语义分割网络SegNet提升了2.9%,比ENet提升了1.3%。所提方法可以改善语义分割中出现的分割不连续、物体边界不清晰、小目标错分漏分、分割精度不高等问题。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%93%E8%B7%AF%E7%8E%AF%E5%A2%83%E6%84%9F%E7%9F%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">道路环境感知;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">边缘检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%E8%BD%A6%E8%BE%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">无人驾驶车辆;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    宋小娜(1982—),女,河南南阳人,讲师,博士研究生,主要研究方向:图像处理、模式识别、深度学习;;
                                </span>
                                <span>
                                    *芮挺(1972—),男,江苏南京人,教授,博士,主要研究方向:人工智能、模式识别;电子邮箱354442511@qq.com;
                                </span>
                                <span>
                                    王新晴(1963—),男,江苏泰州人,教授,博士,主要研究方向:信号处理、智能算法、无人化智能车辆。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-25</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目(2016YFC0802904);</span>
                                <span>国家自然科学基金资助项目(61472444,61671470);</span>
                                <span>江苏省自然科学基金资助项目(BK20161470);</span>
                    </p>
            </div>
                    <h1><b>Semantic segmentation method of road environment combined semantic boundary information</b></h1>
                    <h2>
                    <span>SONG Xiaona</span>
                    <span>RUI Ting</span>
                    <span>WANG Xinqing</span>
            </h2>
                    <h2>
                    <span>College of Field Engineering, Army Engineering University of People&apos;s Liberation Army</span>
                    <span>College of Mechanical Engineering, North China University of Water Resources and Electric Power</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Semantic segmentation is an important method to interpret the road semantic environment. The convolution, pooling and deconvolution in semantic segmentation of deep learning result in blur and discontinuous segmentation boundary, missing and wrong segmentation of small objects. These influence the outcome of segmentation and reduce the accuracy of segmentation. To deal with the problems above, a new semantic segmentation method combined semantic boundary information was proposed. Firstly, a subnet of semantic boundary detection was built in the deep model of semantic segmentation, and the feature sharing layers in the network were used to transfer the semantic boundary information learned in the semantic boundary detection subnet to the semantic segmentation network. Then, a new cost function of the model was defined according to the tasks of semantic boundary detection and semantic segmentation. The model was able to accomplish two tasks simultaneously and improve the descriptive ability of object boundary and the quality of semantic segmentation. Finally, the method was verified on the Cityscapes dataset. The experimental results demonstrate that the accuracy of the method proposed is improved by 2.9 % compared to SegNet and is improved by 1.3% compared to ENet. It can overcome the problems in semantic segmentation such as discontinous segmentation, blur boundary of object, missing and wrong segmentation of small objects and low accuracy of segmentation.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semantic%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semantic segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Fully%20Convolutional%20Network(FCN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Fully Convolutional Network(FCN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=road%20environment%20perception&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">road environment perception;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=boundary%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">boundary detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=unmanned%20vehicle&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">unmanned vehicle;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    SONG Xiaona, born in 1982, Ph. D. candidate, lecturer. Her research interests include image processing, pattern recognition, deep learning.;
                                </span>
                                <span>
                                    RUI Ting, born in 1972, Ph. D. , professor. His research interests include artificial intelligence, pattern recognition.;
                                </span>
                                <span>
                                    WANG Xinqing, born in 1963, Ph. D. , professor. His research interests include signal processing, intelligence algorithms, unmanned intelligent vehicles.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-25</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Key Research and Development Program of China(2016YFC0802904);</span>
                                <span>the National Natural Science Foundation of China(61472444,61671470);</span>
                                <span>the Natural Science Foundation of Jiangsu Province(BK20161470);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="43" name="43" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="44">道路环境感知问题一直以来是智能车辆的关键技术,其中基于计算机视觉的环境感知方法已成为主要研究热点。它通过车辆前方的摄像头获取道路环境信息,利用相关算法对道路场景进行语义分割,获取车辆周边环境情况,为智能车辆的安全行驶提供保障。</p>
                </div>
                <div class="p1">
                    <p id="45">目前,现有的道路环境语义分割方法<citation id="154" type="reference"><link href="114" rel="bibliography" /><link href="116" rel="bibliography" /><link href="118" rel="bibliography" /><link href="120" rel="bibliography" /><link href="122" rel="bibliography" /><link href="124" rel="bibliography" /><link href="126" rel="bibliography" /><link href="128" rel="bibliography" /><link href="130" rel="bibliography" /><link href="132" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>大多基于深度学习网络框架,全卷积神经网络的引入,更是有效提升了语义分割任务的精度和速度。但是卷积层中大的感受野和池化层的存在使得道路环境中大量物体边界细节缺失,造成语义分割结果边界模糊、分割精度不高。针对此类问题,许多研究人员采取对语义分割结果进行后处理,如采用条件随机场(Conditional Random Field, CRF)等方法来改善粗糙的语义分割结果。然而这种方法大多数是基于颜色等低维特征进行修正,不能学习场景中不同物体之间的语义关系,因此很难获得很好的语义分割结果。</p>
                </div>
                <div class="p1">
                    <p id="46">本文提出一种结合语义边界信息的道路环境语义分割方法,通过在语义分割网络中引入语义边界检测子网络来学习语义边界信息,在同一深度网络框架下,完成语义边界检测和语义分割两个任务,并定义了新的模型代价函数,有效提升语义分割边界精度。本文的主要工作如下:</p>
                </div>
                <div class="p1">
                    <p id="47">1)在语义分割网络中构建了语义边界检测子网络,通过融合语义分割模型中不同卷积层的特征,学习从低层到物体层次的多尺度边缘信息,为语义分割提供丰富准确的物体边界信息。</p>
                </div>
                <div class="p1">
                    <p id="48">2)在同一深度神经网络框架下,完成物体语义边界检测和语义分割两个任务,通过定义新的模型代价函数,使网络更好地学习语义边界信息,改善语义分割过程中出现的边界不连续或模糊现象。同时,学习的语义边界信息还有效地改善了语义分割中小目标的漏分或错分现象,提升了语义分割的精度。</p>
                </div>
                <div class="p1">
                    <p id="49">3)在道路语义分割公有数据集Cityscapes<citation id="155" type="reference"><link href="134" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>上验证了方法的有效性。</p>
                </div>
                <h3 id="50" name="50" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="51">语义分割是为图像中每个像素分配一个预先定义的表示其语义目标类别的标签。深度学习在各种视觉任务上取得了巨大的成功,尤其是卷积神经网络(Convolutional Neural Network, CNN)被广泛用于图像分类或物体检测。在此基础上,一些学者将原用于分类的网络转化为适合分割的形式,提出一种更为直接的以像素直接分类为基础的语义分割方法。其基本思想是:以大量带有像素级标注的图像为样本,训练神经网络等分类器,对图像中每个像素分类,输入是原图像,输出是源图像对应的语义分割图,形成一种端到端的方法。</p>
                </div>
                <div class="p1">
                    <p id="52">2015年,Long等<citation id="156" type="reference"><link href="136" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出了全卷积神经网络(Fully Convolutional Network, FCN)方法,设计了一种针对任意大小的输入图像,训练端到端的全卷积网络的框架,实现逐像素分类,奠定了使用深度网络解决图像语义分割问题的基础框架。为了避免网络提取特征时丢失空间位置信息,FCN通过双线性插值上采样和组合中间层输出的特征图,将粗糙分割结果转换为密集分割结果,并且成功将原本用于分类的网络转换为生成图像分割的网络,实现了像素级预测。但是FCN得到的结果还不够精细,目标物体的细节结构可能会丢失或边界模糊。在此基础上,Badrinarayanan等<citation id="157" type="reference"><link href="138" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了SegNet用于图像分割的一种深度卷积编码器-解码器架构,比FCN效率更高并且占用更少的内存,但划分精度不高。Chen等<citation id="158" type="reference"><link href="140" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>在FCN 框架的末端增加全连接条件随机场,提出了 DeepLab-v1模型,首先使用双线性插值法对 FCN的输出结果上采样得到粗糙分割结果,以该结果图中每个像素为一个节点构造 CRF 模型并求解,对分割结果求精。该CRF模型以颜色此类低维特征作为像素之间关联函数,存在一定弊端,容易形成物体分割空间上的不连续。之后,Chen等<citation id="159" type="reference"><link href="142" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>又提出了DeepLab-v2,主要使用了带孔卷积,并且提出了在空间维度上实现金字塔型的空洞池化(Atrous Spatial Pyramid Pooling, ASPP)。卷积核之间引入了“孔洞”,可以增大空间分辨率同时不改变感受受野和参数数量的效果,ASPP使用多种孔洞率的卷积核对相同特征图同时处理,由于不同的孔洞率对应着不同的感受野大小,可以提取不同尺度的目标特征,网络最后通过CRF精细化边缘信息,更好地分割物体边界;但带孔卷积需要大量高分辨率特征图,因此其计算成本高昂,且占用大量内存,同时容易丢失重要的细节信息。</p>
                </div>
                <div class="p1">
                    <p id="53">纵观上述语义分割方法,大都是通过对深度构架中卷积和池化层修改、低层特征和高级语义特征融合等方法来改善语义分割过程中存在的物体边界粗糙或模糊问题。事实上,语义边界信息是语义分割任务中有效区分一个物体与另一个物体的非常重要的信息。在网络训练中有效学习物体语义边界信息或利用物体语义边界信息指导语义分割同样可以提升语义分割的精度。Marmanis等<citation id="160" type="reference"><link href="144" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>将边缘检测融入语义分割,图像首先进入边缘检测模块,然后再进入后续语义分割模块,通过这种方法学习图像中边缘信息,可以提升图像的语义分割中边界精度。Huang等<citation id="161" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>构建一个物体边界检测FCN来获取物体精确的定位信息和物体形状细节信息,通过物体边界检测FCN与原有的语义分割FCN的信息融合,获取语义分割中的边界细节。Bertasius等<citation id="162" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>设计了边界神经场(Boundary Neural Field, BNF)这个能量模型,通过引入一个全局能量函数,将粗糙的语义分割结果与语义边界信息结合来改善语义分割结果。</p>
                </div>
                <div class="p1">
                    <p id="54">本文所设计的结合语义边界信息的语义分割网络,在语义分割网络中通过整合网络结构中各阶段卷积层提取的信息,构建了语义边界检测子网络。通过定义了新的模型代价函数,在同一框架下完成了语义边界检测和语义分割两个任务,并利用语义边界检测任务中所学习到的语义边界信息,来有效提升语义分割效果。在文献<citation id="163" type="reference">[<a class="sup">16</a>]</citation>中,边缘检测网络与语义分割网络是并行存在的,网络结构存在冗余。本文所设计的网络不同于文献<citation id="164" type="reference">[<a class="sup">16</a>]</citation>,语义边界检测子网络与语义分割网络存在特征共享层,通过共享层将语义边界检测子网络学习到的语义边界信息传递给语义分割网络,其结构更为简洁,执行效率更高。在文献<citation id="165" type="reference">[<a class="sup">17</a>]</citation>中,输入图像先后进入边缘检测模块与语义分割模块,若边缘检测模块学习的边缘信息不完整,直接影响了语义分割模块的分割效果。而文献<citation id="166" type="reference">[<a class="sup">18</a>]</citation>中能量函数的构建方法复杂,网络训练周期长。</p>
                </div>
                <h3 id="55" name="55" class="anchor-tag">2 本文主要方法</h3>
                <div class="p1">
                    <p id="56">针对语义分割中存在的边界模糊或不连续、小目标漏分误分、分割精度不高等问题,本文提出了一种结合物体语义边界信息的语义分割方法。在原语义分割网络框架中构建了语义边界检测子网络来学习物体边界信息,并通过共享特征层将语义边界检测子网络学习到的语义边界信息传递给语义分割网络,有效提升了语义分割网络的分割精度。本方法在同一深度神经网络框架下同时完成语义边界检测和语义分割两个任务,通过定义新的模型代价函数,来完成网络训练,整个训练过程是端对端的,具体方法如图1所示。</p>
                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909005_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文提出的方法" src="Detail/GetImg?filename=images/JSJY201909005_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文提出的方法  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909005_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 The proposed method</p>

                </div>
                <h4 class="anchor-tag" id="58" name="58">2.1 <b>结合语义边界信息的语义分割模型</b></h4>
                <div class="p1">
                    <p id="59">本文所设计的结合语义边界信息的语义分割模型如图2所示。</p>
                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909005_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 本文所设计的模型结构" src="Detail/GetImg?filename=images/JSJY201909005_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 本文所设计的模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909005_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Model structure designed in this paper</p>

                </div>
                <div class="p1">
                    <p id="61">图2中实线框部分构成了语义分割网络,而虚线框部分构成了语义边界检测子网络,由此可知,整个网络模型存在共有部分,将其称为网络特征共享层。通过特征共享层将语义边界子网络学习的信息传送给语义分割网络,提升网络的语义边界学习能力。整个网络架构是基于VGG19改进的,VGG19网络包含有16个卷积层和3个全连接层,该网络在图像分类、目标检测等领域取得巨大成功。它的卷积层分为5个阶段,每个阶段后面紧跟一个池化层。随着感受野的增加,每个卷积层所学习到的有用信息逐渐变得粗糙,这种多层次的结构可以学习更加丰富的特征信息。</p>
                </div>
                <div class="p1">
                    <p id="62">如图2所示虚线框内为语义边界检测子网络,它与现有的基于深度学习的边缘检测方法<citation id="167" type="reference"><link href="150" rel="bibliography" /><link href="152" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>类似,由编码和解码两部分构成。其中编码部分充分利用语义分割网络中编码部分信息,通过对每个阶段的卷积层的结果融合来获取不同尺度的物体边界信息。解码部分则通过反卷积将不同尺度的物体边界放大后融合,得到最终的语义边界检测预测结果。在语义边界检测子网络中,VGG19每个阶段的卷积层取出后紧跟1×1的卷积层,然后将该阶段的所有输出相加以获得融合特征后再次进入一个1×1的卷积层,然后通过一个反卷积层来对输出特征图上采样,以获得与输入图像大小相同的输出。每个阶段的上采样层后紧跟一个交叉熵损失层。所有的上采样层输出结果级联并通过1×1的卷积融合,最后再进入交叉熵损失层来获得最终的语义边界检测结果。在该网络结构下,由于卷积层的感受野大小不同,因此可以学习到从低层到物体层次的多尺度信息。这种网络结构,有助于捕获物体边界的细微细节,为精准的语义分割提供有用信息。</p>
                </div>
                <div class="p1">
                    <p id="63">如图2实线框所示的语义分割网络框架,其结构仍然是基于编码-解码形式的语义分割框架。由于本文的网络结构是对无人驾驶车辆的道路环境进行语义分割,为了提高实时性,减少了FCN、SegNet等传统的语义分割网络中特征图的个数,同时采用不对称的结构。编码部分与语义边界检测子网络共享,各阶段的特征图的个数分别为16,32,64,128,256。由于解码部分只是为了上采样编码的结果输出语义分割结果,与SegNet网络(对称结构,编码和解码层数相等)不同,只设计了5个反卷积层,其特征图的个数分别为64,64,32,16,20。在上采样过程中,没有使用最大-反池化方式,而是采用了步幅为2的反卷积层(也称为转置卷积),这样不需要与编码层共享最大池化时的位置索引信息,从而降低内存和计算成本,这种做法并没有降低准确度。</p>
                </div>
                <h4 class="anchor-tag" id="64" name="64">2.2 <b>结合语义边界检测的语义分割模型训练方法</b></h4>
                <div class="p1">
                    <p id="65">为了训练引入语义边界检测子网络的语义分割模型,本文首先将该模型的两个任务定义如下:</p>
                </div>
                <div class="p1">
                    <p id="66">给定图像<i>X</i>,指定输入图像<i>X</i>中每个像素点<i>X</i>(<i>i</i>, <i>j</i>)的语义目标类别的标签,将该结果记为语义分割结果<i>Y</i><sub>seg</sub>,同时检测出输入图像<i>X</i>中的每个物体的语义边界,将该结果记为语义边界<i>Y</i><sub>edge</sub>。</p>
                </div>
                <div class="p1">
                    <p id="67">通过图2可以看到,本文所构建的网络是通过网络中特征共享层将语义边界检测子网络学习到的物体边界信息传递给语义分割网络,因此语义边界检测任务的目的是为了促使语义分割网络模型学习语义边界信息。</p>
                </div>
                <div class="p1">
                    <p id="68">为了完成多任务的需求,本文定义了新的目标代价函数,通过调配语义边界检测子网络与语义分割网络的目标代价函数的比重,以其获得更好的语义分割效果,其目标代价函数为</p>
                </div>
                <div class="p1">
                    <p id="69">min(<i>L</i><sub>seg</sub>+<i>λL</i><sub>edge</sub>)      (1)</p>
                </div>
                <div class="p1">
                    <p id="70">而<i>L</i><sub>seg</sub>和<i>L</i><sub>edge</sub>分别为语义分割网络和语义边界检测子网络的目标代价函数,<i>λ</i>为控制语义分割和语义边界检测网络的目标函数的比例系数。语义分割网络的目标代价函数采用常用的交叉熵损失函数,其表达式如下:</p>
                </div>
                <div class="area_img" id="170">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201909005_17000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="72">其中:<i>Y</i>^ <sub>seg</sub>为语义分割网络的输出,<i>Y</i><sub>seg</sub>为语义分割标签,<i>n</i>为图像的总像素点数量。</p>
                </div>
                <div class="p1">
                    <p id="73">而语义边界检测子网络和语义分割原理是相同的,是一个对像素点进行是否为边界的二分类问题,因此目标代价函数同样采用交叉熵,其表达式如下:</p>
                </div>
                <div class="area_img" id="171">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201909005_17100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="75">其中:<i>Y</i>^ <sub>edge</sub>为语义边界检测子网络的输出,为<i>Y</i><sub>edge</sub>语义边界检测标签,<i>n</i>为图像的总像素点数量。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag">3 实验及分析</h3>
                <h4 class="anchor-tag" id="77" name="77">3.1 <b>实验方法</b></h4>
                <div class="p1">
                    <p id="78">在道路环境语义分割数据库Cityscapes上对本文提出的方法进行一系列实验。Cityscapes是最近在无人驾驶环境的语义分割领域中广泛使用的一个数据库。它包含了50个城市不同场景、不同背景、不同季节的街景,提供5 000张精细标注的图像、20 000张粗略标注的图像、35类标注物体。在实验过程中,只使用了5 000张精细标注图像,将其划分为训练集2 975张、验证集500张和测试集1 525张,选择了常用的19类物体进行分类。为了实验方便,将原有图像的分辨率由2 048×1 024改为512×256。</p>
                </div>
                <div class="p1">
                    <p id="79">整个实验是基于tensorflow框架的,在模型的训练过程中,使用Adam优化算法来获得最快的收敛速度。初始学习率设置为10<sup>-3</sup>,然后每迭代1 000次,降低为原有的1/10。将样本块的大小设置为5,momentum设置为0.9,权重衰减系数设置为5×10<sup>-4</sup>,最大迭代次数设置为10<sup>5</sup>。</p>
                </div>
                <div class="p1">
                    <p id="80">在网络训练时,运用迁移学习的方法,将在图像分类数据集上训练的性能良好的VGG19模型学习到的特征迁移到结合边界信息的语义分割模型中去,获得了较快的收敛速度和较好的分割效果。整个网络结构中,特征共享层的初始参数使用已训练的VGG19对应各层参数,其他参数则为随机初始化的参数。</p>
                </div>
                <div class="p1">
                    <p id="81">由于模型要在同一个深度神经网络中同时进行语义边界检测和语义分割两个任务,通过实验,本方法首先验证了同时进行两种任务的可行性,通过两个任务的收敛过程,如图3所示,两个任务都能很好地收敛,说明语义边界检测可以共用语义分割模型的编码层,并不会对语义分割造成干扰,两个任务可以同时训练。接下来,在现有的数据集上,调整两个任务的目标函数的比例系数<i>λ</i>,通过全局目标函数的收敛速度和最终的语义分割效果,确定最佳比例系数<i>λ</i>=50。在后续的实验结果部分,将看到语义边界检测子网络的加入可以有效改善物体边界不清晰问题、提升语义分割精度。</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909005_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 语义分割和语义边界检测收敛过程" src="Detail/GetImg?filename=images/JSJY201909005_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 语义分割和语义边界检测收敛过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909005_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Convergence process of semantic segmentation and semantic boundary detection</p>

                </div>
                <h4 class="anchor-tag" id="83" name="83">3.2 <b>语义边界检测子网络的结果及分析</b></h4>
                <div class="p1">
                    <p id="84">在实验中,并不需要额外对物体的边界进行标注,而是利用已有的语义分割标签,对其进行语义边界检测后,生成语义边界标注。本文所设计的语义边界检测子网络融合了模型中不同卷积层的特征,学习到从低层到物体层次的多尺度边缘信息,为语义分割提供丰富准确的物体边界信息。图4为语义边界检测子网络的输出结果,生成的物体边界清晰准确。由于本文所设计的语义边界检测子网络融合了不同尺度的卷积核,因此可以获得更丰富的特征,有效提升了语义边界准确率。</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">3.3 <b>语义分割结果及分析</b></h4>
                <div class="p1">
                    <p id="86">在训练数据集上,利用全局目标函数对模型进行训练,并在验证集和测试集上进行测试。语义分割评价方法采用通常使用的交并比方法:</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mi>Ο</mi><mi>U</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">其中:<i>TP</i>、<i>FP</i>和<i>FN</i>分别代表真正、假正和假负的像素点的个数。在Cityscapes数据集对无人驾驶道路环境更为重要的19类目标分别计算出各类的交并比数值。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909005_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 语义边界检测子网络的输出结果比对" src="Detail/GetImg?filename=images/JSJY201909005_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 语义边界检测子网络的输出结果比对  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909005_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Results of semantic boundary detection</p>

                </div>
                <div class="p1">
                    <p id="90">为了验证加入语义边界检测子网络对语义分割精度的提升,本研究分别构建了两个网络Seg_edge和Seg_noedge,分别对应加入语义边界检测子网络和不加语义边界检测子网络的语义分割模型。两个模型的参数设置完全相同,训练方法完全一样。通过图5的语义分割结果可见,Seg_edge网络检出的道路、汽车、人行道等类别的边界更为精准,分割精度更高。例1中,由于光照不均匀,Seg_noedge网络检出的道路区域边界不清晰,而Seg_edge能准确找到道路的边界,有效区分道路和人行道区域。例2中Seg_noedge所分割的人行道区域不连续、汽车区域不连续、存在部分区域错分现象,而Seg_edge分割的人行道区域连续,边界清晰准确,在该图中的两辆汽车也能被精准地分割。例3中Seg_edge则成功区分了汽车和卡车,将原本错误分割在一起的汽车和卡车区分开来,有效改善分类中可能出现的错分现象。例4中Seg_edge检测小目标的能力有所提升,比如对图中的行人分割得比较准确。这些例子充分说明了语义边界检测子网络的引入,有效改善了物体边界不清晰、分割不连续、小目标漏分错分、分割精度不高等问题,有效地提升了语义分割的精度。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909005_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 加入语义边界子网络后的语义分割结果比对" src="Detail/GetImg?filename=images/JSJY201909005_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 加入语义边界子网络后的语义分割结果比对  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909005_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Results of semantic segmentation with and without semantic boundary subnet</p>

                </div>
                <div class="p1">
                    <p id="92">同时,将本文的方法Seg_edge与现有的语义分割方法SegNet<citation id="168" type="reference"><link href="138" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、ENet<citation id="169" type="reference"><link href="130" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>(最近新提出的应用于道路环境语义分割方法)进行比对,表1为各种方法在Cityscapes数据集上19类的准确率。其中平均准确率为表中19类目标准确率的平均值,而Cityscapes数据集同时将19类目标分为7大类,分别为平地、自然、物体、天空、建筑物、人和车辆,平均类准确率是基于此分类准则给出的。由表1可知,本文提出的方法在所有类都获得高的准确率,在道路、天空、车辆等常见类别中,准确率都超过了90%,相比其他方法,在行人、人行道、自行车、卡车、公交车等类别准确率都得到了提高。本文提出的方法在平均准确率上相比已有的语义分割网络SegNet提升了2.9%,相比ENet提升了1.3%。图6给出了以上各种方法的语义分割结果,证明了本文提出的方法在Cityscapes数据集上所有类上都取得很好的分割结果。SegNet和ENet方法在道路、人行道、车辆等类别可能出现分割不均匀、不连续、边界不准确等现象,本文的方法有力改善此类问题,道路和人行道的边界清晰准确。而针对小目标(行人、交通标志、路灯和信号灯)可能存在的漏检或不准确问题,本文的方法也有所改善。这充分说明了语义边界检测子网络能促使网络结构学习物体边界信息,本文提出的方法在复杂道路场景下能获取准确的物体边界,提升语义分割精度。但是,针对分类中样本较少的类别如火车、树木等,分割精度有待提高,主要原因是缺乏有效的训练样本。</p>
                </div>
                <div class="area_img" id="94">
                    <p class="img_tit"><b>表</b>1 <b>本文方法与其他方法的语义分割精度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Comparison of semantic segmentation accurancy </p>
                    <p class="img_note">单位:%</p>
                    <table id="94" border="1"><tr><td><br />类別</td><td>SegNet</td><td>ENet</td><td>Seg_edge</td></tr><tr><td><br />道路</td><td>96.4</td><td>96.3</td><td>97.8</td></tr><tr><td><br />人行道</td><td>73.2</td><td>74.2</td><td>76.4</td></tr><tr><td><br />建筑物</td><td>84.0</td><td>75.0</td><td>83.1</td></tr><tr><td><br />墙</td><td>28.4</td><td>32.2</td><td>34.5</td></tr><tr><td><br />栅栏</td><td>29.0</td><td>33.2</td><td>38.9</td></tr><tr><td><br />灯杆</td><td>35.7</td><td>43.4</td><td>48.9</td></tr><tr><td><br />交通灯</td><td>39.8</td><td>34.1</td><td>50.2</td></tr><tr><td><br />交通信号</td><td>45.1</td><td>44.0</td><td>53.4</td></tr><tr><td><br />树木</td><td>87.0</td><td>88.6</td><td>86.7</td></tr><tr><td><br />土地</td><td>63.8</td><td>61.4</td><td>64.5</td></tr><tr><td><br />天空</td><td>91.8</td><td>90.6</td><td>93.4</td></tr><tr><td><br />人</td><td>62.8</td><td>65.5</td><td>67.4</td></tr><tr><td><br />骑手</td><td>42.8</td><td>38.4</td><td>45.7</td></tr><tr><td><br />汽车</td><td>89.3</td><td>90.6</td><td>92.5</td></tr><tr><td><br />卡车</td><td>38.1</td><td>36.9</td><td>42.8</td></tr><tr><td><br />公共汽车</td><td>43.1</td><td>50.5</td><td>54.3</td></tr><tr><td><br />火车</td><td>44.1</td><td>48.1</td><td>43.2</td></tr><tr><td><br />摩托车</td><td>35.8</td><td>38.8</td><td>40.3</td></tr><tr><td><br />自行车</td><td>51.9</td><td>55.4</td><td>56.4</td></tr><tr><td><br />平均准确率</td><td>56.95</td><td>58.28</td><td>61.60</td></tr><tr><td><br />平均类准确率</td><td>79.13</td><td>80.39</td><td>81.42</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909005_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 本文方法与其他方法语义分割结果对比" src="Detail/GetImg?filename=images/JSJY201909005_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 本文方法与其他方法语义分割结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909005_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Comparison of semantic segmentation results by different methods</p>

                </div>
                <h3 id="96" name="96" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="97">本文针对深度学习语义分割中由于卷积、池化及反卷积的作用所带来的分割边界模糊或不连续、小目标漏分错分等现象提出了结合物体边界信息的语义分割方法,并在道路环境数据集Cityscapes上验证了方法的有效性。在同一深度神经网络框架下,完成了语义边界检测和语义分割两个任务,利用网络特征共享层,将语义边界检测子网络学习到的物体边界信息传递到语义分割网络,有效改善了语义分割中出现的边界模糊、分割不连续、小目标漏分错分、分割精度不高等问题,提高了语义分割的准确率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="114">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Road scene segmentation from a single image">

                                <b>[1]</b> ALVAREZ J M,GEVERS T,LeCUN Y,et al.Road scene segmentation from a single image [C]// ECCV '12:Proceedings of the 12th European Conference on Computer Vision,LNCS 7578.Berlin:Springer,2012:376-389.
                            </a>
                        </p>
                        <p id="116">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional patch networks with spatial prior for road detection and urban scene understanding">

                                <b>[2]</b> BRUST C,SICKERT S,SIMON M,et al.Convolutional patch networks with spatial prior for road detection and urban scene understanding [EB/OL].[2019- 01- 21].https://arxiv.org/pdf/1502.06344.pdf.
                            </a>
                        </p>
                        <p id="118">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simultaneous detection and segmentation">

                                <b>[3]</b> HARIHARAN B,ARBELÁEZ P,GIRSHICK R,et al.Simultaneous detection and segmentation [C]// Proceedings of the 13th European Conference on Computer Vision,LNCS 8695.Berlin:Springer,2014:297-312.
                            </a>
                        </p>
                        <p id="120">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YYGX201703013&amp;v=MjkyNjl1WnNGeWpsVXIvT1BEVE1kckc0SDliTXJJOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 高凯珺,孙韶媛,姚广顺.基于深度学习的无人车夜视图像语义分割[J].应用光学,2017,38(3):421-428.(GAO K J,SUN S Y,YAO G S.Semantic segmentation of night vision images for unmanned vehicles based on deep learning [J].Journal of Applied Optics,2017,38(3):421-428.)
                            </a>
                        </p>
                        <p id="122">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201722002&amp;v=MjY2NTFPclk5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amxVci9PTHo3TWFiRzRIOWI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 吴宗胜,傅卫平,韩改宁.基于深度卷积神经网络的道路场景理解[J].计算机工程与应用,2017,53(22):8-15.(WU Z S,FU W P,HAN G N.Road scene understanding based on deep convolutional neural network [J].Computer Engineering and Applications,2017,53(22):8-15.)
                            </a>
                        </p>
                        <p id="124">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201807002&amp;v=MzExMjZxZlp1WnNGeWpsVXIvT0x6N1NaTEc0SDluTXFJOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 张军阳,王慧丽,郭阳,等.深度学习相关研究综述[J].计算机应用研究,2018,35(7):1921-1928.(ZHANG J Y,WANG H L,GUO Y,et al.Review of deep learning [J].Application Research of Computers,2018,35(7):1921-1928.)
                            </a>
                        </p>
                        <p id="126">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning semantic segmentation with diverse supervision">

                                <b>[7]</b> YE L,LIU Z,WANG Y.Learning semantic segmentation with diverse supervision [C]// Proceedings of the 2018 IEEE Winter Conference on Applications of Computer Vision.Washington,DC:IEEE Computer Society,2018:1461-1469.
                            </a>
                        </p>
                        <p id="128">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Graph-based global reasoning networks">

                                <b>[8]</b> CHEN Y,ROHRBACH M,YAN Z,et al.Graph-based global reasoning networks [EB/OL].[2018- 12- 10].https://arxiv.org/pdf/1811.12814v1.pdf.
                            </a>
                        </p>
                        <p id="130">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ENet:a deep neural network architecture for real-time semantic segmentation">

                                <b>[9]</b> PASZKE ADAM,CHAURASIA A,KIM S,et al.ENet:a deep neural network architecture for real-time semantic segmentation [EB/OL].[2018- 12- 10].https://arxiv.org/pdf/1606.02147.pdf.
                            </a>
                        </p>
                        <p id="132">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ERFNet:Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation">

                                <b>[10]</b> ROMERA E,ÁLVAREZ J M,BERGASA L M,et al.ERFNet:efficient residual factorized convnet for real-time semantic segmentation [J].IEEE Transactions on Intelligent Transportation Systems,2018,19(1):263-272.
                            </a>
                        </p>
                        <p id="134">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Cityscapes Dataset for Semantic Urban Scene Understanding">

                                <b>[11]</b> CORDTS M,OMRAN M,RAMOS S,et al.The cityscapes dataset for semantic urban scene understanding [C]// Proceedings of the 29th IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:3213-3223.
                            </a>
                        </p>
                        <p id="136">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[12]</b> LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2015:3431-3440 .
                            </a>
                        </p>
                        <p id="138">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SegNet:A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation">

                                <b>[13]</b> BADRINARAYANAN V,KENDALL A,CIPOLLA R.SegNet:a deep convolutional encoder-decoder architecture for image segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(12):2481-2495.
                            </a>
                        </p>
                        <p id="140">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs[C/OL]">

                                <b>[14]</b> CHEN L,PAPANDREOU G,KOKKINOS I,et al.Semantic image segmentation with deep convolutional nets and fully connected CRFs [EB/OL].[2018- 05- 10].https://arxiv.org/pdf/1412.7062.pdf.
                            </a>
                        </p>
                        <p id="142">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Lab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs">

                                <b>[15]</b> CHEN L,PAPANDREOU G,KOKKINOS I,et al.DeepLab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(4):834-848.
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6BE46128E728C32B7A2EA6DC8830CD47&amp;v=MjIwNjcwNXRwaHhMaSt3SzQ9TmlmT2ZiWEthOVhLcm8xTkVld05CQTg2eldRVW16MElPWG1XM3hvOWVyTG5NYjZZQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> MARMANIS D,SCHINDLER K,WEGNER J D,et al.Classification with an edge:improving semantic image segmentation with boundary detection [J].ISPRS Journal of Photogrammetry and Remote Sensing,2018,135:158-172.
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object boundary guided semantic segmentation">

                                <b>[17]</b> HUANG Q,XIA C,ZHENG W,et al.Object boundary guided semantic segmentation [C]// Proceedings of the 13th Asian Conference on Computer Vision,LNCS 10111.Berlin:Springer,2016:197-212.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic segmentation with boundary neural fields">

                                <b>[18]</b> BERTASIUS GEDAS,SHI J,TORRESANI L.Semantic segmentation with boundary neural fields [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:3602-3610.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Richer Convolutional Features for Edge Detection">

                                <b>[19]</b> LIU Y,CHENG M,HU X,et al.Richer convolutional features for edge detection [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017:5872-5881.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object contour detection with a fully convolutional encoder-decoder network">

                                <b>[20]</b> YANG J,PRICE B,COHEN S,et al.Object contour detection with a fully convolutional encoder-decoder network [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:193-202.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201909005" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909005&amp;v=MzAyNjlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amxVci9PTHo3QmQ3RzRIOWpNcG85RllZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
