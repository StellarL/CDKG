<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637139023811041250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201903046%26RESULT%3d1%26SIGN%3dzyBgXg%252bQD%252f9M0dF2lEaRSelV9VI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903046&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903046&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903046&amp;v=MDQ3MjU3cWZadVpwRnlubFViN0lMejdCZDdHNEg5ak1ySTlCWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#42" data-title="1 NMF语音增强算法 ">1 NMF语音增强算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#49" data-title="2 NMF算法改进 ">2 NMF算法改进</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="2.1 &lt;b&gt;感知增益函数计算&lt;/b&gt;">2.1 <b>感知增益函数计算</b></a></li>
                                                <li><a href="#89" data-title="2.2 &lt;b&gt;结合&lt;/b&gt;SPP&lt;b&gt;的感知增益函数计算&lt;/b&gt;">2.2 <b>结合</b>SPP<b>的感知增益函数计算</b></a></li>
                                                <li><a href="#107" data-title="2.3 PM-RNMF&lt;b&gt;算法框架&lt;/b&gt;">2.3 PM-RNMF<b>算法框架</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#121" data-title="3 仿真与结果 ">3 仿真与结果</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#123" data-title="3.1 &lt;b&gt;实验数据与评价指标&lt;/b&gt;">3.1 <b>实验数据与评价指标</b></a></li>
                                                <li><a href="#127" data-title="3.2 &lt;b&gt;实验结果及分析&lt;/b&gt;">3.2 <b>实验结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="图1 监督学习的NMF语音增强框架">图1 监督学习的NMF语音增强框架</a></li>
                                                <li><a href="#109" data-title="图2 PM-RNMF语音增强流程">图2 PM-RNMF语音增强流程</a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同方法在&lt;/b&gt;4&lt;b&gt;种&lt;/b&gt;SNR&lt;b&gt;下对&lt;/b&gt;3&lt;b&gt;种噪声的&lt;/b&gt;PESQ&lt;b&gt;值和&lt;/b&gt;SDR&lt;b&gt;值&lt;/b&gt;"><b>表</b>1 <b>不同方法在</b>4<b>种</b>SNR<b>下对</b>3<b>种噪声的</b>PESQ<b>值和</b>SDR<b>值</b></a></li>
                                                <li><a href="#133" data-title="图3 四种语音增强算法对带噪语音增强的语谱图">图3 四种语音增强算法对带噪语音增强的语谱图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" VENKATESWARLU S C, PRASAD K S, REDDY A S. Improve speech enhancement using Wiener filtering [J]. Global Journal of Computer Science and Technology, 2011, 11 (7) : 30-38." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improve speech enhancement using Wiener filtering">
                                        <b>[1]</b>
                                         VENKATESWARLU S C, PRASAD K S, REDDY A S. Improve speech enhancement using Wiener filtering [J]. Global Journal of Computer Science and Technology, 2011, 11 (7) : 30-38.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" MARTIN R. Speech enhancement using MMSE short time spectral estimation with gamma distributed speech priors [C]// ICASSP 2002: Proceedings of the 2002 IEEE International Conference on Acoustics, Speech, and Signal Processing. Piscataway, NJ: IEEE, 2002, 1: 253-256." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech enhancement using MMSE short time spectral estimation with gamma distributed speech priors">
                                        <b>[2]</b>
                                         MARTIN R. Speech enhancement using MMSE short time spectral estimation with gamma distributed speech priors [C]// ICASSP 2002: Proceedings of the 2002 IEEE International Conference on Acoustics, Speech, and Signal Processing. Piscataway, NJ: IEEE, 2002, 1: 253-256.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" XU Y, DU J, DAI L, et al. A regression approach to speech enhancement based on deep neural networks [J]. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2015, 23 (1) : 7-19." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A regression approach to speech enhancement based on deep neural networks">
                                        <b>[3]</b>
                                         XU Y, DU J, DAI L, et al. A regression approach to speech enhancement based on deep neural networks [J]. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2015, 23 (1) : 7-19.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" 韩伟, 张雄伟, 闵刚, 等.基于感知掩蔽深度神经网络的单通道语音增强方法[J].自动化学报, 2017, 43 (2) :248-258. (HAN W, ZHANG X W, MIN G, et al. A single-channel speech enhancement approach based on perceptual masking deep neural network [J]. Acta Automatica Sinica, 2017, 43 (2) : 248-258.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201702007&amp;v=MDEwNjZxZlp1WnBGeW5sVWI3SUtDTGZZYkc0SDliTXJZOUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         韩伟, 张雄伟, 闵刚, 等.基于感知掩蔽深度神经网络的单通道语音增强方法[J].自动化学报, 2017, 43 (2) :248-258. (HAN W, ZHANG X W, MIN G, et al. A single-channel speech enhancement approach based on perceptual masking deep neural network [J]. Acta Automatica Sinica, 2017, 43 (2) : 248-258.) 
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" MOHAMMADIHA N, SMARAGDIS P, LEIJON A. Supervised and unsupervised speech enhancement using nonnegative matrix factorization [J]. IEEE Transactions on Audio, Speech, and Language Processing, 2013, 21 (10) : 2140-2151." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised and unsupervised speech enhancement using nonnegative matrix factorization">
                                        <b>[5]</b>
                                         MOHAMMADIHA N, SMARAGDIS P, LEIJON A. Supervised and unsupervised speech enhancement using nonnegative matrix factorization [J]. IEEE Transactions on Audio, Speech, and Language Processing, 2013, 21 (10) : 2140-2151.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" 蒋茂松, 王冬霞, 牛芳琳, 等. 稀疏正则非负矩阵分解的语音增强算法[J].计算机应用, 2018, 38 (4) :1176-1180. (JIANG M S, WANG D X, NIU F L, et al. Speech enhancement method based on sparsity-regularized non-negative matrix factorization [J]. Journal of Computer Applications, 2018, 38 (4) : 1176-1180.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201804044&amp;v=MjY2NDF6N0JkN0c0SDluTXE0OUJZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeW5sVWI3SUw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         蒋茂松, 王冬霞, 牛芳琳, 等. 稀疏正则非负矩阵分解的语音增强算法[J].计算机应用, 2018, 38 (4) :1176-1180. (JIANG M S, WANG D X, NIU F L, et al. Speech enhancement method based on sparsity-regularized non-negative matrix factorization [J]. Journal of Computer Applications, 2018, 38 (4) : 1176-1180.) 
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" WILSON K W, RAJ B, SMARAGDIS P, et al. Speech denoising using non-negative matrix factorization with priors [C]// ICASSP 2008: Proceedings of the 2008 IEEE International Conference on Acoustics, Speech and Signal Processing. Piscataway, NJ: IEEE, 2008: 4029-4032." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech denoising using nonnegative matrix factorization with priors">
                                        <b>[7]</b>
                                         WILSON K W, RAJ B, SMARAGDIS P, et al. Speech denoising using non-negative matrix factorization with priors [C]// ICASSP 2008: Proceedings of the 2008 IEEE International Conference on Acoustics, Speech and Signal Processing. Piscataway, NJ: IEEE, 2008: 4029-4032.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" HOYER P O. Non-negative matrix factorization with sparseness constraints [J]. Journal of Machine Learning Research, 2004, 5 (9) : 1457-1469." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Non-negative Matrix Factorization withSparseness Constraints">
                                        <b>[8]</b>
                                         HOYER P O. Non-negative matrix factorization with sparseness constraints [J]. Journal of Machine Learning Research, 2004, 5 (9) : 1457-1469.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" 路成, 田猛, 周健, 等.L1/2稀疏约束卷积非负矩阵分解的单通道语音增强方法[J]. 声学学报, 2017, 42 (3) :377-384. (LU C, TIAN M, ZHOU J, et al. A single-channel speech enhancement approach using convolution non-negative matrix factorization with L1/2 sparse constraint[J]. Acta Acustica, 2017, 42 (3) : 377-384.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XIBA201703016&amp;v=MjMyODk5Yk1ySTlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnlubFViN0lQU1RKYjdHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         路成, 田猛, 周健, 等.L1/2稀疏约束卷积非负矩阵分解的单通道语音增强方法[J]. 声学学报, 2017, 42 (3) :377-384. (LU C, TIAN M, ZHOU J, et al. A single-channel speech enhancement approach using convolution non-negative matrix factorization with L1/2 sparse constraint[J]. Acta Acustica, 2017, 42 (3) : 377-384.) 
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" KWON K, SHIN J W, KIM N S. NMF-based speech enhancement using bases update [J]. IEEE Signal Processing Letters, 2015, 22 (4) : 450-454." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=NMF-based speech enhancement using bases update">
                                        <b>[10]</b>
                                         KWON K, SHIN J W, KIM N S. NMF-based speech enhancement using bases update [J]. IEEE Signal Processing Letters, 2015, 22 (4) : 450-454.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" CHUNG H, PLOURDE E, CHAMPAGNE B. Basis compensation in non-negative matrix factorization model for speech enhancement [C]// ICASSP 2016: Proceedings of the 2016 IEEE International Conference on Acoustics, Speech and Signal Processing. Piscataway, NJ: IEEE, 2016: 2249-2253." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Basis compensation in non-negative matrix factorization model for speech enhancement">
                                        <b>[11]</b>
                                         CHUNG H, PLOURDE E, CHAMPAGNE B. Basis compensation in non-negative matrix factorization model for speech enhancement [C]// ICASSP 2016: Proceedings of the 2016 IEEE International Conference on Acoustics, Speech and Signal Processing. Piscataway, NJ: IEEE, 2016: 2249-2253.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" HU Y, LOIZOU P C. Incorporating a psychoacoustical model in frequency domain speech enhancement [J]. IEEE Signal Processing Letters, 2004, 11 (2) : 270-273." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Incorporating a psychoacoustical model in frequency domain speech enhancement">
                                        <b>[12]</b>
                                         HU Y, LOIZOU P C. Incorporating a psychoacoustical model in frequency domain speech enhancement [J]. IEEE Signal Processing Letters, 2004, 11 (2) : 270-273.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" 张毅, 王可佳, 席兵, 等.基于子带能熵比的语音端点检测算法[J].计算机科学, 2017, 44 (5) :304-307. (ZHANG Y, WANG K J, XI B, et al. Speech endpoint detection algorithm based on sub-band energy-entropy-ratio [J]. Computer Science, 2017, 44 (5) : 304-307.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201705056&amp;v=MDIzOTZVYjdJTHo3QmI3RzRIOWJNcW85QVlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5bmw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         张毅, 王可佳, 席兵, 等.基于子带能熵比的语音端点检测算法[J].计算机科学, 2017, 44 (5) :304-307. (ZHANG Y, WANG K J, XI B, et al. Speech endpoint detection algorithm based on sub-band energy-entropy-ratio [J]. Computer Science, 2017, 44 (5) : 304-307.) 
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" LEE S, HAN D K, KO H. Single-channel speech enhancement method using reconstructive NMF with spectrotemporal speech presence probabilities [J]. Applied Acoustics, 2017, 117: 257-262." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-channel speech enhancement method using reconstructive NMF with spectrotemporal speech presence probabilities">
                                        <b>[14]</b>
                                         LEE S, HAN D K, KO H. Single-channel speech enhancement method using reconstructive NMF with spectrotemporal speech presence probabilities [J]. Applied Acoustics, 2017, 117: 257-262.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" SUNNYDAYAL V, KUMAR T K. Speech enhancement using posterior regularized NMF with bases update [J]. Computers and Electrical Engineering, 2017, 62: 663-675." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech enhancement using posterior regularized NMF with bases update">
                                        <b>[15]</b>
                                         SUNNYDAYAL V, KUMAR T K. Speech enhancement using posterior regularized NMF with bases update [J]. Computers and Electrical Engineering, 2017, 62: 663-675.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" RIX A W, BEERENDS J G, HOLLIER M P, et al. Perceptual Evaluation of Speech Quality (PESQ) —a new method for speech quality assessment of telephone networks and codecs [C]// ICASSP 2001: Proceedings of the 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Piscataway, NJ: IEEE, 2001, 2:749-752." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceptual evaluation of speech quality(PESQ)-a new method for speech quality assessment of telephone networks and codecs">
                                        <b>[16]</b>
                                         RIX A W, BEERENDS J G, HOLLIER M P, et al. Perceptual Evaluation of Speech Quality (PESQ) —a new method for speech quality assessment of telephone networks and codecs [C]// ICASSP 2001: Proceedings of the 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Piscataway, NJ: IEEE, 2001, 2:749-752.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" HUANG P S, KIM M, HASEGAWA-JOHNSON M, et al. Deep learning for monaural speech separation [C]// ICASSP 2014: Proceedings of the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing. Piscataway, NJ: IEEE, 2014: 1562-1566.This work is partially supported by the Chongqing Science and Technology Commission Project (cstc2015jcyjBX0066) .LI Yansheng, born in 1983, Ph. D., lecturer. His research interests include human-computer interaction, intelligent robot.LIU Yuan, born in 1993, M. S. candidate. Her research interests include human-computer interaction, speech enhancement.ZHANG Yi, born in 1966, Ph. D., professor. His research interests include human-computer interaction, intelligent system, mobile robot." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning for monaural speech separation">
                                        <b>[17]</b>
                                         HUANG P S, KIM M, HASEGAWA-JOHNSON M, et al. Deep learning for monaural speech separation [C]// ICASSP 2014: Proceedings of the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing. Piscataway, NJ: IEEE, 2014: 1562-1566.This work is partially supported by the Chongqing Science and Technology Commission Project (cstc2015jcyjBX0066) .LI Yansheng, born in 1983, Ph. D., lecturer. His research interests include human-computer interaction, intelligent robot.LIU Yuan, born in 1993, M. S. candidate. Her research interests include human-computer interaction, speech enhancement.ZHANG Yi, born in 1966, Ph. D., professor. His research interests include human-computer interaction, intelligent system, mobile robot.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-10-17 15:32</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(03),894-898 DOI:10.11772/j.issn.1001-9081.2018071489            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于感知掩蔽的重构非负矩阵分解单通道语音增强算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E8%89%B3%E7%94%9F&amp;code=36234029&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李艳生</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%9B%AD&amp;code=41276911&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘园</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%AF%85&amp;code=10570313&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张毅</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E5%9B%BD%E5%AE%B6%E4%BF%A1%E6%81%AF%E6%97%A0%E9%9A%9C%E7%A2%8D%E4%B8%8E%E6%9C%8D%E5%8A%A1%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%B7%A5%E7%A8%8B%E7%A0%94%E5%8F%91%E4%B8%AD%E5%BF%83&amp;code=0174747&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学国家信息无障碍与服务机器人工程研发中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对非负矩阵分解 (NMF) 语音增强算法在低信噪比 (SNR) 非稳定环境下存在噪声残留的问题, 提出一种基于感知掩蔽的重构NMF (PM-RNMF) 单通道语音增强算法。首先, 将心理声学掩蔽特性应用于NMF语音增强算法中;其次, 对不同频率位采用不同的掩蔽阈值, 建立自适应感知掩蔽增益函数, 通过阈值约束残余噪声能量和语音失真能量;最后, 结合语音存在概率 (SPP) 进行感知增益修正, 重构NMF算法, 以此建立新的目标函数。仿真结果表明, 在不同SNR的3种非稳定噪声环境下, 与NMF、重构NMF (RNMF) 、感知掩蔽深度神经网络 (PM-DNN) 算法相比, PM-RNMF算法的感知语音质量评估 (PESQ) 平均值分别提高了0.767、0.474、0.162, 信源失真比 (SDR) 平均值分别提高了2.785、1.197、0.948。实验结果表明, 无论是在低频还是高频PM-RNMF有更好的降噪效果。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">非负矩阵分解;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%84%9F%E7%9F%A5%E6%8E%A9%E8%94%BD&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">感知掩蔽;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E9%9F%B3%E5%A2%9E%E5%BC%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语音增强;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E9%9F%B3%E5%AD%98%E5%9C%A8%E6%A6%82%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语音存在概率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%95%E9%80%9A%E9%81%93&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">单通道;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李艳生 (1983—) , 男, 黑龙江哈尔滨人, 讲师, 博士, 主要研究方向:人机交互、智能机器人;;
                                </span>
                                <span>
                                    *刘园 (1993—) , 女, 河南三门峡人, 硕士研究生, 主要研究方向:人机交互、语音增强;电子邮箱1767237325@qq.com;
                                </span>
                                <span>
                                    张毅 (1966—) , 男, 四川成都人, 教授, 博士, 主要研究方向:人机交互、智能系统、移动机器人。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>重庆市基础与前沿研究计划重点项目 (cstc2015jcyjBX0066);</span>
                    </p>
            </div>
                    <h1><b>Reconstructed NMF single channel speech enhancement algorithm based on perceptual masking</b></h1>
                    <h2>
                    <span>LI Yansheng</span>
                    <span>LIU Yuan</span>
                    <span>ZHANG Yi</span>
            </h2>
                    <h2>
                    <span>National Information Accessibility and Service Robot Engineering R&D Center, Chongqing University of Posts and Telecommunications</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problem of noise residual in Non-negative Matrix Factorization (NMF) speech enhancement algorithm in low Signal-to-Noise Ratio (SNR) unsteady environment, a Perceptual Masking-based reconstructed NMF (PM-RNMF) single-channel speech enhancement algorithm was proposed. Firstly, psychoacoustic masking features were applied to NMF speech enhancement algorithms. Secondly, different masking thresholds were used for different frequencies to establish an adaptive perceptual masking gain function, and the residual noise energy and speech distortion energy were constrained by the thresholds. Finally, Speech Presence Probability (SPP) was combined to realize perceptual gain correction, the NMF algorithm was reconstructed and a new objective function was established. The simulation results show that under three kinds of unsteady noise environments with different SNR, the average Perceptual Evaluation of Speech Quality (PESQ) of PM-RNMF algorithm is improved by 0.767, 0.474 and 0.162 respectively and the average Signal-to-Distortion Ratio (SDR) is increased by 2.785, 1.197 and 0.948 respectively compared with NMF, RNMF (Reconstructive NMF) and PM-DNN (Perceptual Masking-Deep Neural Network) algorithms. Experimental results show that PM-RNMF has better noise reduction effect in both low frequency and high frequency.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Non-negative%20Matrix%20Factorization%20(NMF)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Non-negative Matrix Factorization (NMF) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=perceived%20masking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">perceived masking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=speech%20enhancement&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">speech enhancement;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Speech%20Presence%20Probability%20(SPP)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Speech Presence Probability (SPP) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=single-channel&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">single-channel;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LI Yansheng, born in 1983, Ph. D. , lecturer. His research interests include human-computer interaction, intelligent robot.;
                                </span>
                                <span>
                                    LIU Yuan, born in 1993, M. S. candidate. Her research interests include human-computer interaction, speech enhancement.;
                                </span>
                                <span>
                                    ZHANG Yi, born in 1966, Ph. D. , professor. His research interests include human-computer interaction, intelligent system, mobile robot.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-07-19</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the Chongqing Science and Technology Commission Project (cstc2015jcyjBX0066);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="38">语音增强是从带噪语音中提取纯净的语音信号, 一方面可提高语音信号的可懂度和话音质量, 另一方面可增强语音识别和声纹识别的鲁棒性。传统语音增强方法如维纳滤波法<citation id="136" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、最小均方误差 (Minimum Mean Squared Error, MMSE) <citation id="137" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>等均属于无监督的语音增强方法, 这类方法不需要预先的任何训练阶段。然而这类方法通常假定噪声是平稳的, 在真实环境下难以有效抑制非平稳噪声的影响。随着互联网数字处理能力的增强, 有监督的语音增强算法发展迅速, 如基于深度神经网络的算法<citation id="138" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>、基于字典学习和稀疏表示的算法<citation id="139" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。相比无监督方法, 有监督学习的方法需要大量先验信息, 但是有很好的语音增强效果, 尤其是在非平稳噪声环境下。</p>
                </div>
                <div class="p1">
                    <p id="39">近年来, 非负矩阵分解 (Non-negative Matrix Factorization, NMF) <citation id="140" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>的算法由于成功地找到了能够最好地描述未处理数据的潜在特征的高质量字典原子而备受关注, 其基本思想是通过一组基函数和它们的激活系数来表示源的特征, 每个源一组, 然后使用基本函数的级联集合分析信号的混合状态, 并且使用相应的激活系数和基本集合重建每个源。Wilson等<citation id="141" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>将NMF与基于统计模型结合, 提出一种NMF语音增强算法。该算法克服了基于统计模型不能有效处理非平稳噪声的不足, 取得了不错的增强效果。Hoyer<citation id="142" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>根据NMF能产生对数据稀疏表示的特性, 在NMF上增加L1正则项作为稀疏性限制来平衡重构误差和稀疏程度。在此基础上, 路成等<citation id="143" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出L1/2稀疏约束NMF单通道语音增强算法。由于语音信号随时间变化的双状态特性即存在与不存在特性, Kwon等<citation id="144" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出将语音存在概率 (Speech Presence Probability, SPP) 与NMF算法结合的无监督在线语音增强算法。该算法能够很好地抑制噪声同时可以保留弱语音成分, 但由于语音信号的基矢量是从多个源 (如Babble噪声) 的混合中进行调整的, 因此仍然可以表现出不同源的特征, 导致增强语音中可能包含残余噪声。Chung等<sup></sup><citation id="145" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>针对该问题提出NMF模型的基础补偿算法, 用于监督单通道语音增强。</p>
                </div>
                <div class="p1">
                    <p id="40">但是, 语音增强效果是由人耳主观感受评价的, 以上增强算法很少考虑人类的心理声学掩蔽特性对语音增强效果的影响, 会影响增强语音的试听效果。韩伟等<citation id="146" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>将心理声学的掩蔽特性应用于基于深度神经网络的语音增强算法中, 利用深度神经网络 (Deep Neural Network, DNN) 对带噪语音特征进行训练得到具有心理声学掩蔽特性的增益函数, 然后将该感知增益函数与带噪语音幅度谱进行计算得到纯净语音的幅度谱估计, 实验证明该算法能提高语音增强的效果。</p>
                </div>
                <div class="p1">
                    <p id="41">基于上述分析, 本文利用NMF在语音增强方面的优势, 提出一种基于感知掩蔽的重构NMF (Perceptual Masking-Reconstructive NMF, PM-RNMF) 单通道语音增强算法。首先通过NMF获得语音信号的先验信息, 然后将心理声学掩蔽特性融合于NMF增益函数估计中, 通过阈值来约束增强语音的失真和残余噪声的能量, 建立感知掩蔽的增益函数, 最后结合SPP进行增益修正。</p>
                </div>
                <h3 id="42" name="42" class="anchor-tag">1 NMF语音增强算法</h3>
                <div class="p1">
                    <p id="43">NMF语音增强算法是通过对语音和 (或) 噪声信号进行建模, 然后用语音或噪声样本对所建模型进行训练估计出模型具体参数, 最后利用所得参数从带噪语音中估计出纯净语音。NMF语音增强算法分为两个阶段:训练阶段和增强阶段, 如图1所示。</p>
                </div>
                <div class="p1">
                    <p id="44">假设带噪语音信号帧为<i>y</i>=<i>s</i>+<i>n</i>, <i>s</i>、<i>n</i>分别为纯净语音信号帧和噪声信号帧, 且两者互不相关。在训练阶段通过短时傅里叶变换 (Short-Time Fourier Transform, STFT) 得到纯净语音和噪声的幅度谱<b><i>S</i></b> (<i>ω</i>) 、<b><i>N</i></b> (<i>ω</i>) (<i>ω</i>为采样频率) , 然后用NMF算法<b><i>X</i></b>≈<b><i>WH</i></b>分别将<b><i>S</i></b> (<i>ω</i>) 和<b><i>N</i></b> (<i>ω</i>) ) 分解为特征字典矩阵<b><i>W</i></b><sub><i>s</i></sub>、<b><i>W</i></b><sub><i>n</i></sub>和对应的激活矩阵<b><i>H</i></b><sub><i>s</i></sub>、<b><i>H</i></b><sub><i>n</i></sub>。最后将特征字典矩阵<b><i>W</i></b><sub><i>s</i></sub>、<b><i>W</i></b><sub><i>n</i></sub>作为增强阶段的先验信息保存下来。</p>
                </div>
                <div class="p1">
                    <p id="45">在增强阶段通过STFT得到带噪语音的幅度谱<b><i>Y</i></b> (<i>ω</i>) , 同样用NMF算法对带噪语音幅度谱进行分解, 得到带噪语音激活矩阵<b><i>H</i></b><sub><i>s</i></sub>′、<b><i>H</i></b><sub><i>n</i></sub>′后, 与训练阶段保存的特征字典矩阵<b><i>W</i></b><sub><i>s</i></sub>、<b><i>W</i></b><sub><i>n</i></sub>重构出纯净语音的幅度谱<b><i>S</i></b>^ (<i>ω</i>) , 如式 (1) 所示。最后通过逆短时傅里叶变换 (Inverse Short-Time Fourier Transform, ISTFT) 得到增强后的语音信号。</p>
                </div>
                <div class="p1">
                    <p id="46" class="code-formula">
                        <mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi mathvariant="bold-italic">S</mi><mo>^</mo></mrow><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo><mi mathvariant="bold-italic">G</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>⨂</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>s</mi></msub><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><msup><mi>s</mi><mo>′</mo></msup></msub></mrow><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>s</mi></msub><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><msup><mi>s</mi><mo>′</mo></msup></msub><mo>+</mo><mi>μ</mi><mo>⋅</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>n</mi></msub><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><msup><mi>n</mi><mo>′</mo></msup></msub></mrow></mfrac><mo>⨂</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="47">其中, <b><i>G</i></b> (<i>ω</i>) 为增益函数; <i>μ</i>为语音信号的谱衰减约束因子, 通常选择<i>μ</i>=1;⨂表示对应元素相乘。</p>
                </div>
                <div class="area_img" id="48">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903046_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 监督学习的NMF语音增强框架" src="Detail/GetImg?filename=images/JSJY201903046_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 监督学习的NMF语音增强框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903046_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Framework of NMF speech enhancement based on supervised learning</p>

                </div>
                <h3 id="49" name="49" class="anchor-tag">2 NMF算法改进</h3>
                <div class="p1">
                    <p id="50">使用NMF进行语音增强通常由于谱衰减因子的取值不同, 获得不同的增强效果<citation id="147" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 如当式 (1) 中<i>μ</i>=1时为对所有频率位谱衰减因子均相同的维纳滤波增益函数, 获得的增强语音对不同的频率位的响应是不变的。该增益函数的缺点是对不同频率位的不同语音分布特性, 只有一个阈值约束, 即对所有的频率响应是固定不变的, 没有考虑人耳对语音的频率感知特性。人类听觉感知特性描述了人类听觉系统对语音及噪声的感知和掩蔽能力, 感知掩蔽是指当一些噪声处于人类听觉掩蔽阈值之下时, 听觉系统就无法感知这些噪声存在的现象, 通过掩蔽阈值的约束能够提高语音的清晰度, 减小畸变度, 因此, 本文提出通过将人耳对不同频率位的掩蔽阈值作为增强语音和纯净语音误差的约束, 以此得到不同频率位不同的增益函数, 解决NMF算法对频率响应单一的问题。</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51">2.1 <b>感知增益函数计算</b></h4>
                <div class="p1">
                    <p id="52">本文通过人耳听觉系统的感知掩蔽特性对不同频率位建立不同增益函数<b><i>G</i></b> (<i>ω</i>) , 具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="53">步骤1 语音信号频域估计误差<i>ε</i> (<i>ω</i>) 计算。增强后的语音幅度谱为<b><i>S</i></b>^ (<i>ω</i>) ) , 纯净语音幅度谱为<b><i>S</i></b> (<i>ω</i>) , 则语音信号频域估计误差为:</p>
                </div>
                <div class="p1">
                    <p id="54"><i>ε</i> (<i>ω</i>) =<b><i>S</i></b>^ (<i>ω</i>) -<b><i>S</i></b> (<i>ω</i>) = (<b><i>G</i></b>-<b><i>I</i></b>) ⨂<b><i>S</i></b> (<i>ω</i>) +</p>
                </div>
                <div class="p1">
                    <p id="55"><b><i>G</i></b>⨂<b><i>N</i></b> (<i>ω</i>) =<i>ε</i><sub><i>S</i>′</sub> (<i>ω</i>) +<i>ε</i><sub><i>N</i>′</sub> (<i>ω</i>)      (2) </p>
                </div>
                <div class="p1">
                    <p id="56">其中:<i>ε</i><sub><i>S</i>′</sub> (<i>ω</i>) 表示频率为<i>ω</i>时的语音失真频谱;<i>ε</i><sub><i>N</i>′</sub> (<i>ω</i>) 表示频率为<i>ω</i>时的残余噪声频谱。</p>
                </div>
                <div class="p1">
                    <p id="57">步骤2 建立最优增益函数<b><i>G</i></b> (<i>ω</i>) 的约束函数。最优增益函数<b><i>G</i></b> (<i>ω</i>) 应满足在残余噪声期望值低于掩蔽阈值时语音失真最小, 表示如下:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">G</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow></munder><mspace width="0.25em" /><mi mathvariant="bold-italic">E</mi><mo stretchy="false">[</mo><mi mathvariant="bold-italic">ε</mi><msubsup><mrow></mrow><msup><mi>S</mi><mo>′</mo></msup><mn>2</mn></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mspace width="0.25em" /><mspace width="0.25em" /><mi mathvariant="bold-italic">E</mi><mo stretchy="false">[</mo><mi mathvariant="bold-italic">ε</mi><msubsup><mrow></mrow><msup><mi>Ν</mi><mo>′</mo></msup><mn>2</mn></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>≤</mo><mi>Τ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">其中:<b><i>E</i></b>[<i>ε</i><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><msup><mi>S</mi><mo>′</mo></msup><mn>2</mn></msubsup></mrow></math></mathml> (<i>ω</i>) ]=<b><i>E</i></b> (<i>ε</i><sup>H</sup><sub><i>S</i>′</sub> (<i>ω</i>) ×<i>ε</i><sub><i>S</i>′</sub> (<i>ω</i>) ) 为语音失真能量;<b><i>E</i></b>[<i>ε</i><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><msup><mi>Ν</mi><mo>′</mo></msup><mn>2</mn></msubsup></mrow></math></mathml> (<i>ω</i>) ]=<b><i>E</i></b> (<i>ε</i><sup>H</sup><sub><i>N</i>′</sub> (<i>ω</i>) ×<i>ε</i><sub><i>N</i>′</sub> (<i>ω</i>) ) 为残余噪声能量;<b><i>T</i></b> (<i>ω</i>) 为短时幅度谱分量的听觉掩蔽阈值估计值。</p>
                </div>
                <div class="p1">
                    <p id="62">步骤3 通过Lagrange乘子法优化步骤2中的约束函数。由于式 (3) 是一个凸规划问题, 所以可用Lagrange乘子法优化。最优增益函数<b><i>G</i></b> (<i>ω</i>) 为满足目标函数梯度方程的一个平稳可行点, 构造Lagrange代价函数为:</p>
                </div>
                <div class="p1">
                    <p id="63"><b><i>J</i></b> (<b><i>G</i></b>, <i>λ</i>) =<b><i>E</i></b>[<i>ε</i><mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><msup><mi>S</mi><mo>′</mo></msup><mn>2</mn></msubsup></mrow></math></mathml> (<i>ω</i>) ]+<i>λ</i>· (<b><i>E</i></b>[<i>ε</i><mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><msup><mi>Ν</mi><mo>′</mo></msup><mn>2</mn></msubsup></mrow></math></mathml> (<i>ω</i>) ]-<b><i>T</i></b> (<i>ω</i>) ) =</p>
                </div>
                <div class="p1">
                    <p id="66"><b><i>E</i></b>{[ (<b><i>G</i></b>-<b><i>I</i></b>) ⨂<b><i>S</i></b> (<i>ω</i>) ]<sup>H</sup>[ (<b><i>G</i></b>-<b><i>I</i></b>) ⨂<b><i>S</i></b> (<i>ω</i>) ]}+<i>λ</i>·</p>
                </div>
                <div class="p1">
                    <p id="67"> (<b><i>E</i></b>{[<b><i>G</i></b>⨂<b><i>N</i></b> (<i>ω</i>) ]<sup>H</sup>[<b><i>G</i></b>⨂<b><i>N</i></b> (<i>ω</i>) ]}-<b><i>T</i></b> (<i>ω</i>) )      (4) </p>
                </div>
                <div class="p1">
                    <p id="68">其中, <i>λ</i>为大于等于0的Lagrange乘数因子矩阵。</p>
                </div>
                <div class="p1">
                    <p id="69">式 (4) 对<b><i>G</i></b> (<i>ω</i>) 求偏导即ᐁ<sub><b><i>G</i></b></sub><i>J</i> (<b><i>G</i></b>, <i>λ</i>) =0可得 (详细求解过程见文献<citation id="148" type="reference">[<a class="sup">12</a>]</citation>) :</p>
                </div>
                <div class="p1">
                    <p id="70"><b><i>G</i></b> (<i>ω</i>) · (<b><i>P</i></b><sub><b><i>S</i></b></sub> (<i>ω</i>) +<i>λ</i>·<b><i>P</i></b><sub><b><i>N</i></b></sub> (<i>ω</i>) ) =<b><i>P</i></b><sub><b><i>S</i></b></sub> (<i>ω</i>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="71">其中, <b><i>P</i></b><sub><b><i>S</i></b></sub> (<i>ω</i>) 和<b><i>P</i></b><sub><b><i>N</i></b></sub> (<i>ω</i>) ) 分别为纯净语音和噪声的功率谱分量。</p>
                </div>
                <div class="p1">
                    <p id="72">语音和噪声频谱时域平滑表示为:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>S</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>=</mo><mi>τ</mi><msub><mrow></mrow><mi>s</mi></msub><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>S</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>τ</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">) </mo><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>S</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>Ν</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>=</mo><mi>τ</mi><msub><mrow></mrow><mi>n</mi></msub><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>Ν</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>τ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>Ν</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">其中, <i>τ</i><sub><i>s</i></sub>、<i>τ</i><sub><i>n</i></sub> (0≤<i>τ</i><sub><i>s</i></sub>, <i>τ</i><sub><i>n</i></sub>≤1) 分别为语音和噪声的平滑常数, <i>t</i>为帧索引。</p>
                </div>
                <div class="p1">
                    <p id="75">由式 (5) 、式 (6) 可得增益函数<b><i>G</i></b> (<i>ω</i>) 可表示为:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">G</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>S</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow><mrow><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>S</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">λ</mi><mo>×</mo><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>Ν</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>s</mi></msub><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mi>s</mi></msub><mo>´</mo></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>s</mi></msub><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mi>s</mi></msub><mo>´</mo></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi mathvariant="bold-italic">λ</mi><mo>×</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>n</mi></msub><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mi>n</mi></msub><mo>´</mo></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">ξ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow><mrow><mi mathvariant="bold-italic">ξ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">λ</mi></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">其中, <i>ξ</i> (<i>ω</i>) 为不同频率位<i>ω</i>时的先验信噪比 (Signal-to-Noise Ratio, SNR) , 表示如下:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">ξ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>S</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow><mrow><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>Ν</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>s</mi></msub><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mi>s</mi></msub><mo>´</mo></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>n</mi></msub><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mi>n</mi></msub><mo>´</mo></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">步骤4 求解感知增益函数<b><i>G</i></b> (<i>ω</i>) 。结合听觉系统的掩蔽特性, 当约束函数 (3) 中的等号成立即<b><i>G</i></b><sup>2</sup> (<i>ω</i>) ·<b><i>P</i></b><mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ν</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml> (<i>ω</i>) =<b><i>T</i></b> (<i>ω</i>) 时, 将式 (7) 得到的<b><i>G</i></b> (<i>ω</i>) 代入该等式即:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mfrac><mrow><mi mathvariant="bold-italic">ξ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow><mrow><mi mathvariant="bold-italic">ξ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">λ</mi></mrow></mfrac><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>⋅</mo><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>Ν</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>=</mo><mi mathvariant="bold-italic">Τ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">由于<i>λ</i>中为大于等于0的数, 同时在考虑先验信噪比 (SNR) 的条件下, 对式 (9) 化简可得Lagrange乘数因子为:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">λ</mi><mo>=</mo><mrow><mi>max</mi></mrow><mo stretchy="false"> (</mo><msqrt><mrow><mfrac><mrow><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>S</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow><mrow><mi mathvariant="bold-italic">Τ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></msqrt><mo>⋅</mo><msqrt><mrow><mfrac><mi mathvariant="bold-italic">Ι</mi><mrow><mi mathvariant="bold-italic">ξ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></msqrt><mo>-</mo><mi mathvariant="bold-italic">Ι</mi><mo>, </mo><mn>0</mn><mo stretchy="false">) </mo><mo>⋅</mo><mi mathvariant="bold-italic">ξ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">将<i>λ</i>代入式 (7) 得<b><i>G</i></b> (<i>ω</i>) 为:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">G</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mi mathvariant="bold-italic">Ι</mi><mrow><mi mathvariant="bold-italic">Ι</mi><mo>+</mo><mi>max</mi><mo stretchy="false"> (</mo><msqrt><mrow><mfrac><mrow><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>S</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow><mrow><mi mathvariant="bold-italic">Τ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></msqrt><mo>⋅</mo><msqrt><mrow><mfrac><mi mathvariant="bold-italic">Ι</mi><mrow><mi mathvariant="bold-italic">ξ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></msqrt><mo>-</mo><mi mathvariant="bold-italic">Ι</mi><mo>, </mo><mn>0</mn><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mrow><mi mathvariant="bold-italic">Ι</mi><mo>+</mo><mi>max</mi><mo stretchy="false"> (</mo><msqrt><mrow><mfrac><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>s</mi></msub><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mi>s</mi></msub><mo>´</mo></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi mathvariant="bold-italic">Τ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></msqrt><mo>⋅</mo><msqrt><mrow><mfrac><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>n</mi></msub><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mi>n</mi></msub><mo>´</mo></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>s</mi></msub><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mi>s</mi></msub><mo>´</mo></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></msqrt><mo>-</mo><mi mathvariant="bold-italic">Ι</mi><mo>, </mo><mn>0</mn><mo stretchy="false">) </mo></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">式 (11) 表明, 如果<b><i>P</i></b><mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ν</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml> (<i>ω</i>) &lt;<b><i>T</i></b> (<i>ω</i>) , 则增益为1, 即噪声在频率<i>ω</i>时已被掩蔽, 不进行抑制;如果<b><i>P</i></b><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ν</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml> (<i>ω</i>) &gt;<b><i>T</i></b> (<i>ω</i>) , 即噪声在频率<i>ω</i>时能被感知, 则对其进行抑制。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89">2.2 <b>结合</b>SPP<b>的感知增益函数计算</b></h4>
                <div class="p1">
                    <p id="90">实际情况下语音会包含大量的停顿, 即使在语音活动期间也是如此。例如塞音闭合段, 即闭塞辅音脉冲发出之前的短暂静音时段, 通常会在语句的中间阶段。另外, 即使在浊音段, 在某些特定的频率上可能也没有语音。SPP为频率位上语音存在的状态模型, 能够对带噪语音谱的语音和噪声进行估计, 避免低SNR时端点检测不准确问题。对于语音帧存在的先验概率, 本文选用低SNR仍具有很强鲁棒性的基于子带能熵比的语音端点检测算法<citation id="149" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>获得。该算法将子带能量和子带谱熵的比值作为端点检测的重要参数, 以此设定阈值进行语音端点的检测。</p>
                </div>
                <div class="p1">
                    <p id="91">在得到语音帧存在的先验概率后采用文献<citation id="150" type="reference">[<a class="sup">14</a>]</citation>中的复高斯分布模型来估计语音的条件存在状态。假设语音和噪声分别是均值为零, 方差为<i>λ</i><sub><i>s</i></sub> (<i>ω</i>) 和<i>λ</i><sub><i>n</i></sub> (<i>ω</i>) 的复高斯分布, 语音存在的后验概率<i>p</i> (<b><i>H</i></b><sub>1</sub>|<b><i>Y</i></b> (<i>ω</i>) ) 利用贝叶斯定理计算表达式为:</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mi mathvariant="bold-italic">Ι</mi><mo>-</mo><mi mathvariant="bold-italic">q</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow><mrow><mi mathvariant="bold-italic">Ι</mi><mo>-</mo><mi mathvariant="bold-italic">q</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">q</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><mo>+</mo><msup><mi mathvariant="bold-italic">ξ</mi><mo>′</mo></msup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mi>exp</mi><mo stretchy="false"> (</mo><mo>-</mo><msup><mi>υ</mi><mo>′</mo></msup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">其中:<mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">q</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mover><mstyle mathsize="140%" displaystyle="true"><mo>=</mo></mstyle><mtext>Δ</mtext></mover><mi mathvariant="bold-italic">p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>表示频率<i>ω</i>时语音不存在的先验概率, 则<b><i>I</i></b>-<b><i>q</i></b> (<i>ω</i>) =<b><i>p</i></b> (<b><i>H</i></b><sub>1</sub>) 表示频率<i>ω</i>时语音存在的先验概率。<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi mathvariant="bold-italic">ξ</mi><mo>′</mo></msup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">E</mi><mo stretchy="false">[</mo><mi mathvariant="bold-italic">S</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">]</mo></mrow><mrow><mi mathvariant="bold-italic">λ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>表示条件先验SNR, <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi mathvariant="bold-italic">υ</mi><mo>′</mo></msup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><msup><mi mathvariant="bold-italic">ξ</mi><mo>′</mo></msup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow><mrow><msup><mi mathvariant="bold-italic">ξ</mi><mo>′</mo></msup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">Ι</mi></mrow></mfrac><mspace width="0.25em" /><mi mathvariant="bold-italic">γ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow></math></mathml> (其中<i>γ</i> (<i>ω</i>) 为后验SNR) 。但是<i>ξ</i>′ (<i>ω</i>) 不易直接估计, 可用无条件信噪比<i>ξ</i> (<i>ω</i>) 表示即<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi mathvariant="bold-italic">ξ</mi><mo>′</mo></msup><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">ξ</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow><mrow><mi mathvariant="bold-italic">Ι</mi><mo>-</mo><mi mathvariant="bold-italic">q</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="98">将<b><i>p</i></b> (<b><i>H</i></b><sub>1</sub>|<b><i>Y</i></b> (<i>ω</i>) ) 集成到式 (11) 的感知增益估计器中, 对其进行增益修正, 提出一种基于感知掩蔽的重构NMF语音增强算法, 表示形式如下:</p>
                </div>
                <div class="p1">
                    <p id="99"><b><i>S</i></b>^ (<i>ω</i>) ) =<b><i>G</i></b> (<i>ω</i>, <b><i>H</i></b><sub>1</sub>) <b><i>p</i></b> (<b><i>H</i></b><sub>1</sub>|<b><i>Y</i></b> (<i>ω</i>) ) <b><i>Y</i></b> (<i>ω</i>) +</p>
                </div>
                <div class="p1">
                    <p id="100"><b><i>G</i></b> (<i>ω</i>, <b><i>H</i></b><sub>0</sub>) <b><i>p</i></b> (<b><i>H</i></b><sub>0</sub>|<b><i>Y</i></b> (<i>ω</i>) ) <b><i>Y</i></b> (<i>ω</i>)      (13) </p>
                </div>
                <div class="p1">
                    <p id="101">其中:<b><i>p</i></b> (<b><i>H</i></b><sub>1</sub>|<b><i>Y</i></b> (<i>ω</i>) ) 表示在给定带噪语音谱<b><i>Y</i></b> (<i>ω</i>) 的条件下, 语音存在的条件概率;<b><i>p</i></b> (<b><i>H</i></b><sub>0</sub>|<b><i>Y</i></b> (<i>ω</i>) ) 表示在给定带噪语音谱<b><i>Y</i></b> (<i>ω</i>) 的条件下, 语音不存在的条件概率。</p>
                </div>
                <div class="p1">
                    <p id="102">由于在语音不存在时<b><i>G</i></b> (<i>ω</i>, <b><i>H</i></b><sub>0</sub>) =0, 所以频率<i>ω</i>的感知增益估计器被该频率的语音存在概率加权的简化表示为:</p>
                </div>
                <div class="p1">
                    <p id="103"><b><i>S</i></b>^ (<i>ω</i>) ) =<b><i>G</i></b> (<i>ω</i>, <b><i>H</i></b><sub>1</sub>) <b><i>p</i></b> (<b><i>H</i></b><sub>1</sub>|<b><i>Y</i></b> (<i>ω</i>) ) <b><i>Y</i></b> (<i>ω</i>)      (14) </p>
                </div>
                <div class="p1">
                    <p id="104">最终结合SPP的感知增益语音表达式为:</p>
                </div>
                <div class="p1">
                    <p id="105"><b><i>S</i></b>^ (<i>ω</i>) ) =<b><i>G</i></b> (<i>ω</i>, <i>ξ</i>) |<sub><i>ξ</i>=<i>ξ</i>′</sub><b><i>p</i></b> (<b><i>H</i></b><sub>1</sub>|<b><i>Y</i></b> (<i>ω</i>) ) <b><i>Y</i></b> (<i>ω</i>) =</p>
                </div>
                <div class="p1">
                    <p id="106"><b><i>G</i></b> (<i>ω</i>, <i>ξ</i>′) <b><i>p</i></b> (<b><i>H</i></b><sub>1</sub>|<b><i>Y</i></b> (<i>ω</i>) ) <b><i>Y</i></b> (<i>ω</i>)      (15) </p>
                </div>
                <h4 class="anchor-tag" id="107" name="107">2.3 PM-RNMF<b>算法框架</b></h4>
                <div class="p1">
                    <p id="108">本文提出的PM-RNMF语音增强框架, 如图2所示。</p>
                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903046_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 PM-RNMF语音增强流程" src="Detail/GetImg?filename=images/JSJY201903046_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 PM-RNMF语音增强流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903046_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Flow chart of PM-RNMF speech enhancement</p>

                </div>
                <div class="p1">
                    <p id="110">PM-RNMF语音增强算法将感知增益函数与SPP结合建立增益函数模型, 然后与有监督的NMF语音增强算法进行集成, 具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="111">步骤1 通过NMF对纯净语音和噪声进行训练, 得到字典矩阵<b><i>W</i></b><sub><i>s</i></sub>、<b><i>W</i></b><sub><i>n</i></sub>作为增强阶段先验信息;</p>
                </div>
                <div class="p1">
                    <p id="112">步骤2 选择<i>μ</i>=1时NMF增强算法, 得到增强的语音和噪声的初始幅度谱<mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">S</mi><mo>˜</mo></mover></math></mathml>、<mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Ν</mi><mo>˜</mo></mover></math></mathml>, 以及先验SNR <i>ξ</i> (<i>ω</i>) 和后验SNR <i>γ</i> (<i>ω</i>) ;</p>
                </div>
                <div class="p1">
                    <p id="115">步骤3 利用增强后的语音幅度谱<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">S</mi><mo>˜</mo></mover></math></mathml>计算掩蔽阈值<b><i>T</i></b> (<i>ω</i>) , 用得到的<b><i>T</i></b> (<i>ω</i>) 和噪声幅度谱<mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Ν</mi><mo>˜</mo></mover></math></mathml>计算感知增益函数<b><i>G</i></b> (<i>ω</i>, <i>ξ</i>) ;</p>
                </div>
                <div class="p1">
                    <p id="118">步骤4 利用<i>ξ</i> (<i>ω</i>) 计算条件先验SNR <i>ξ</i>′ (<i>ω</i>) , 由此计算<b><i>G</i></b> (<i>ω</i>, <i>ξ</i>′) , 再结合<i>γ</i> (<i>ω</i>) 计算语音存在概率<i>p</i> (<b><i>H</i></b><sub>1</sub>|<b><i>Y</i></b> (<i>ω</i>) ) ;</p>
                </div>
                <div class="p1">
                    <p id="119">步骤5 结合SPP集成新的感知增益函数, 通过式 (15) 得到纯净语音幅度谱<b><i>S</i></b>^ (<i>ω</i>) ) ;</p>
                </div>
                <div class="p1">
                    <p id="120">步骤6 结合带噪语音的相位信息进行语音重构得到增强语音的频谱, 最后通过ISTFT得到时域的增强语音信号。</p>
                </div>
                <h3 id="121" name="121" class="anchor-tag">3 仿真与结果</h3>
                <div class="p1">
                    <p id="122">本章对提出的PM-RNMF语音增强算法进行Matlab实验仿真, 并对其性能进行评估。</p>
                </div>
                <h4 class="anchor-tag" id="123" name="123">3.1 <b>实验数据与评价指标</b></h4>
                <div class="p1">
                    <p id="124">实验中纯净语音是从TIMIT (The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus) 数据库中随机选择的男声50句和女声50句, 每句约长3 s;噪声选自Noisex-92标准噪声库中的Babble、Factory1、F-16这3种非稳定噪声作为训练噪声;带噪语音集为SNR为-5 dB、0 dB、5 dB、10 dB的纯净语音和噪声的随机混合。在实验中使用的所有音频数据以WAV格式文件被采样, 采样率为16 kHz, 用STFT计算语音和噪声幅度谱时对信号进行分帧处理, 帧长为512, 帧移为128。训练算法经500次迭代得到字典矩阵和激活矩阵, 语音字典基和噪声字典基分别设为60和30。式 (6) 中的语音平滑系数<i>τ</i><sub><i>s</i></sub>=0.4, 噪声平滑系数<i>τ</i><sub><i>n</i></sub>=0.9<citation id="151" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="125">为了验证PM-RNMF算法的性能, 本文选择NMF<citation id="152" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、重构NMF (Reconstructive NMF, RNMF) <citation id="153" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、感知掩蔽深度神经网络 (Perceptual Masking-Deep Neural Network, PM-DNN) <citation id="154" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>作对比实验。其中, NMF是代价函数为IS (Itakura-Saito) 距离的增强算法;RNMF是将基于统计模型和NMF与在线语音和噪声基更新相结合的增强算法;PM-DNN是将感知掩蔽特性与深度神经网络结合的增强算法, 隐藏层设为3层, 每层2 048个节点, 激活函数为修正线形单元 (Rectified Linear Unit, ReLU) 函数。为了更合理地比较各个算法性能, 实验中3种对比算法均采用与PM-RNMF相同类型和相同数量的输入信号。</p>
                </div>
                <div class="p1">
                    <p id="126">评价指标采用感知语音质量评估 (Perceptual Evaluation of Speech Quality, PESQ) 方法<citation id="155" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>和信源失真比 (Signal-to-Distortion Ratio, SDR) <citation id="156" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>来分别评估增强语音的质量和增强方法的性能。其中PESQ是评价语音主观试听效果的客观评价方法, 取值范围为-0.5～4.5;SDR是在考虑语音失真和噪声失真时以分贝 (dB) 为单位的评估语音和噪声平均分离效果的值。两个评价指标的得分越高表示增强效果越好。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127">3.2 <b>实验结果及分析</b></h4>
                <div class="p1">
                    <p id="128">从表1的PESQ测量值中看出, 本文所提出的PM-RNMF增强算法在不同SNR下均有优于NMF、RNMF、PM-DNN的增强效果, 尤其是在低SNR时。PM-RNMF与NMF、RNMF对比, 由实验结果可知增强语音质量提高, 这是由于感知掩蔽自适应的阈值约束虽然使得相邻共振峰之间波谷处的能量失真误差增大, 但共振峰处的能量得到了很好的保留, 而波谷处的失真并不影响主观的试听效果;PM-RNMF与PM-DNN对比, 由于SPP的增益修正, 使得在低SNR时, PM-RNMF的增强效果优于PM-DNN, 随着SNR的提高存在PM-DNN的PESQ值高于PM-RNMF, 这是由于PM-DNN算法的训练对增强语音的幅度谱描述更加准确, 使得以此计算出掩蔽阈值也更加准确, 增强效果也更好。</p>
                </div>
                <div class="p1">
                    <p id="129">从表1的SDR值中显示, PM-RNMF的SDR值在不同SNR下均优于NMF、RNMF、PM-DNN, 这与PESQ测量值所得出的结论一致。PM-RNMF与NMF、RNMF对比, 由于感知掩蔽的阈值约束, 语音失真减少, 使得增强后的语音听起来更加自然, SDR的值得到提高;PM-RNMF与PM-DNN对比, SDR的值提高明显, 这是由于相比DNN方法NMF利用掩蔽更能提高SDR的值。</p>
                </div>
                <div class="area_img" id="130">
                    <p class="img_tit"><b>表</b>1 <b>不同方法在</b>4<b>种</b>SNR<b>下对</b>3<b>种噪声的</b>PESQ<b>值和</b>SDR<b>值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 PESQ values and SDR values of three kinds of noise under four SNR values</p>
                    <p class="img_note"></p>
                    <table id="130" border="1"><tr><td rowspan="2"><br />噪声环境</td><td rowspan="2">信噪比/dB</td><td colspan="4"><br />PESQ</td><td rowspan="2"></td><td colspan="4"><br />SDR</td></tr><tr><td><br />NMF</td><td>RNMF</td><td>PM-DNN</td><td>PM-RNMF</td><td><br />NMF</td><td>RNMF</td><td>PM-DNN</td><td>PM-RNMF</td></tr><tr><td rowspan="4"><br />Babble<br />noise</td><td>-5</td><td>0.48</td><td>0.78</td><td>0.96</td><td>1.34</td><td></td><td>1.01</td><td>1.52</td><td>1.63</td><td>2.04</td><td rowspan="12"></td><td rowspan="12"></td><td rowspan="12"></td><td rowspan="12"></td><td rowspan="12"></td><td rowspan="12"></td><td rowspan="12"></td><td rowspan="12"></td><td rowspan="12"></td></tr><tr><td><br />0</td><td>1.21</td><td>1.62</td><td>1.78</td><td>2.29</td><td></td><td>1.43</td><td>3.73</td><td>3.77</td><td>5.68</td></tr><tr><td><br />5</td><td>1.98</td><td>2.26</td><td>2.41</td><td>2.51</td><td></td><td>5.26</td><td>8.28</td><td>8.25</td><td>9.06</td></tr><tr><td><br />10</td><td>2.12</td><td>2.51</td><td>2.59</td><td>2.61</td><td></td><td>8.98</td><td>9.57</td><td>9.63</td><td>10.98</td></tr><tr><td rowspan="4"><br />Factory1</td><td><br />-5</td><td>0.12</td><td>0.31</td><td>0.79</td><td>1.23</td><td></td><td>1.11</td><td>1.51</td><td>1.49</td><td>2.06</td></tr><tr><td><br />0</td><td>0.65</td><td>0.88</td><td>1.69</td><td>1.75</td><td></td><td>1.44</td><td>4.12</td><td>4.18</td><td>6.32</td></tr><tr><td><br />5</td><td>1.59</td><td>1.79</td><td>2.09</td><td>2.11</td><td></td><td>5.52</td><td>8.40</td><td>8.63</td><td>8.90</td></tr><tr><td><br />10</td><td>1.71</td><td>2.11</td><td>2.48</td><td>2.43</td><td></td><td>9.27</td><td>9.69</td><td>10.57</td><td>11.39</td></tr><tr><td rowspan="4"><br />F-16</td><td><br />-5</td><td>0.24</td><td>0.49</td><td>0.98</td><td>1.26</td><td></td><td>1.29</td><td>1.68</td><td>1.88</td><td>2.13</td></tr><tr><td><br />0</td><td>0.98</td><td>1.23</td><td>1.66</td><td>1.74</td><td></td><td>1.62</td><td>4.29</td><td>4.67</td><td>6.47</td></tr><tr><td><br />5</td><td>1.97</td><td>2.28</td><td>2.23</td><td>2.37</td><td></td><td>5.74</td><td>8.63</td><td>8.74</td><td>8.91</td></tr><tr><td><br />10</td><td>1.99</td><td>2.29</td><td>2.64</td><td>2.60</td><td></td><td>9.42</td><td>9.73</td><td>10.69</td><td>11.57</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="131">表1通过PESQ和SDR值证明了PM-RNMF算法的优良性能, 但是为了更直观地看出PM-RNMF算法的增强性能, 本文给出了NMF、RNMF、PM-DNN、PM-RNMF这4种增强算法在噪声为Babble噪声、输入SNR为5dB时带噪语音的增强前后的语谱图, 如图3所示。</p>
                </div>
                <div class="p1">
                    <p id="132">由图3中 (a) 和 (c) 对比可知, 使用NMF进行增强的结果在高频段降噪效果明显, 但存在语音失真, 并且在低频段存在较多的噪声残留, 试听效果不佳; (d) 和 (c) 相比, RNMF在低频段降噪效果优于NMF, 这是由于SPP对噪声和语音的估计, 提高了语音质量且减少了噪声残留, 但由于语音与噪声特性的相似性, 使得增强语音中存在残余噪声; (e) 和 (c) 、 (d) 相比, PM-DNN的增强效果要好很多, 在有效去除噪声的前提下, 很好地保存了语音固有的谐波结构, 但低频段存在少量的噪声残留; (f) 与 (d) 相比, PM-RNMF由于感知掩蔽的约束, 语音失真减少, 提高了语音的可懂度; (f) 与 (e) 相比, PM-RNMF提高了低频的去噪效果。综上所述, 本文所提的PM-RNMF算法增强效果明显优于NMF、RNMF、PM-DNN算法。</p>
                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903046_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 四种语音增强算法对带噪语音增强的语谱图" src="Detail/GetImg?filename=images/JSJY201903046_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 四种语音增强算法对带噪语音增强的语谱图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903046_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Spectrograms of four speech enhancement algorithms to a noisy speech</p>

                </div>
                <h3 id="134" name="134" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="135">针对低SNR非稳定噪声环境下NMF语音增强算法存在噪声残留的问题, 本文提出PM-RNMF语音增强算法。该算法将感知增益函数与SPP结合建立增益函数模型, 然后与有监督的NMF语音增强算法进行集成。通过Matlab仿真, 与NMF、RNMF、PM-DNN算法相比, PM-RNMF算法提高了在低SNR非稳定噪声环境下的语音增强效果。然而在实际环境中, 随着人工智能领域应用范围的逐渐扩大, 如服务机器人通常会工作在室内环境中, 由于室内环境通常会存在混响噪声, 这将影响该算法的性能, 因此如何在混响环境下提高语音增强性能将成为下一步的研究重点。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improve speech enhancement using Wiener filtering">

                                <b>[1]</b> VENKATESWARLU S C, PRASAD K S, REDDY A S. Improve speech enhancement using Wiener filtering [J]. Global Journal of Computer Science and Technology, 2011, 11 (7) : 30-38.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech enhancement using MMSE short time spectral estimation with gamma distributed speech priors">

                                <b>[2]</b> MARTIN R. Speech enhancement using MMSE short time spectral estimation with gamma distributed speech priors [C]// ICASSP 2002: Proceedings of the 2002 IEEE International Conference on Acoustics, Speech, and Signal Processing. Piscataway, NJ: IEEE, 2002, 1: 253-256.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A regression approach to speech enhancement based on deep neural networks">

                                <b>[3]</b> XU Y, DU J, DAI L, et al. A regression approach to speech enhancement based on deep neural networks [J]. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2015, 23 (1) : 7-19.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201702007&amp;v=MDAwMTNZOUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeW5sVWI3SUtDTGZZYkc0SDliTXI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 韩伟, 张雄伟, 闵刚, 等.基于感知掩蔽深度神经网络的单通道语音增强方法[J].自动化学报, 2017, 43 (2) :248-258. (HAN W, ZHANG X W, MIN G, et al. A single-channel speech enhancement approach based on perceptual masking deep neural network [J]. Acta Automatica Sinica, 2017, 43 (2) : 248-258.) 
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised and unsupervised speech enhancement using nonnegative matrix factorization">

                                <b>[5]</b> MOHAMMADIHA N, SMARAGDIS P, LEIJON A. Supervised and unsupervised speech enhancement using nonnegative matrix factorization [J]. IEEE Transactions on Audio, Speech, and Language Processing, 2013, 21 (10) : 2140-2151.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201804044&amp;v=MDQzMTFwRnlubFViN0lMejdCZDdHNEg5bk1xNDlCWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 蒋茂松, 王冬霞, 牛芳琳, 等. 稀疏正则非负矩阵分解的语音增强算法[J].计算机应用, 2018, 38 (4) :1176-1180. (JIANG M S, WANG D X, NIU F L, et al. Speech enhancement method based on sparsity-regularized non-negative matrix factorization [J]. Journal of Computer Applications, 2018, 38 (4) : 1176-1180.) 
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech denoising using nonnegative matrix factorization with priors">

                                <b>[7]</b> WILSON K W, RAJ B, SMARAGDIS P, et al. Speech denoising using non-negative matrix factorization with priors [C]// ICASSP 2008: Proceedings of the 2008 IEEE International Conference on Acoustics, Speech and Signal Processing. Piscataway, NJ: IEEE, 2008: 4029-4032.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Non-negative Matrix Factorization withSparseness Constraints">

                                <b>[8]</b> HOYER P O. Non-negative matrix factorization with sparseness constraints [J]. Journal of Machine Learning Research, 2004, 5 (9) : 1457-1469.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XIBA201703016&amp;v=MjA2MjhadVpwRnlubFViN0lQU1RKYjdHNEg5Yk1ySTlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 路成, 田猛, 周健, 等.L1/2稀疏约束卷积非负矩阵分解的单通道语音增强方法[J]. 声学学报, 2017, 42 (3) :377-384. (LU C, TIAN M, ZHOU J, et al. A single-channel speech enhancement approach using convolution non-negative matrix factorization with L1/2 sparse constraint[J]. Acta Acustica, 2017, 42 (3) : 377-384.) 
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=NMF-based speech enhancement using bases update">

                                <b>[10]</b> KWON K, SHIN J W, KIM N S. NMF-based speech enhancement using bases update [J]. IEEE Signal Processing Letters, 2015, 22 (4) : 450-454.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Basis compensation in non-negative matrix factorization model for speech enhancement">

                                <b>[11]</b> CHUNG H, PLOURDE E, CHAMPAGNE B. Basis compensation in non-negative matrix factorization model for speech enhancement [C]// ICASSP 2016: Proceedings of the 2016 IEEE International Conference on Acoustics, Speech and Signal Processing. Piscataway, NJ: IEEE, 2016: 2249-2253.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Incorporating a psychoacoustical model in frequency domain speech enhancement">

                                <b>[12]</b> HU Y, LOIZOU P C. Incorporating a psychoacoustical model in frequency domain speech enhancement [J]. IEEE Signal Processing Letters, 2004, 11 (2) : 270-273.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201705056&amp;v=MDIzMDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnlubFViN0lMejdCYjdHNEg5Yk1xbzlBWW9RS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 张毅, 王可佳, 席兵, 等.基于子带能熵比的语音端点检测算法[J].计算机科学, 2017, 44 (5) :304-307. (ZHANG Y, WANG K J, XI B, et al. Speech endpoint detection algorithm based on sub-band energy-entropy-ratio [J]. Computer Science, 2017, 44 (5) : 304-307.) 
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-channel speech enhancement method using reconstructive NMF with spectrotemporal speech presence probabilities">

                                <b>[14]</b> LEE S, HAN D K, KO H. Single-channel speech enhancement method using reconstructive NMF with spectrotemporal speech presence probabilities [J]. Applied Acoustics, 2017, 117: 257-262.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech enhancement using posterior regularized NMF with bases update">

                                <b>[15]</b> SUNNYDAYAL V, KUMAR T K. Speech enhancement using posterior regularized NMF with bases update [J]. Computers and Electrical Engineering, 2017, 62: 663-675.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceptual evaluation of speech quality(PESQ)-a new method for speech quality assessment of telephone networks and codecs">

                                <b>[16]</b> RIX A W, BEERENDS J G, HOLLIER M P, et al. Perceptual Evaluation of Speech Quality (PESQ) —a new method for speech quality assessment of telephone networks and codecs [C]// ICASSP 2001: Proceedings of the 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Piscataway, NJ: IEEE, 2001, 2:749-752.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning for monaural speech separation">

                                <b>[17]</b> HUANG P S, KIM M, HASEGAWA-JOHNSON M, et al. Deep learning for monaural speech separation [C]// ICASSP 2014: Proceedings of the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing. Piscataway, NJ: IEEE, 2014: 1562-1566.This work is partially supported by the Chongqing Science and Technology Commission Project (cstc2015jcyjBX0066) .LI Yansheng, born in 1983, Ph. D., lecturer. His research interests include human-computer interaction, intelligent robot.LIU Yuan, born in 1993, M. S. candidate. Her research interests include human-computer interaction, speech enhancement.ZHANG Yi, born in 1966, Ph. D., professor. His research interests include human-computer interaction, intelligent system, mobile robot.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201903046" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903046&amp;v=MDQ3MjU3cWZadVpwRnlubFViN0lMejdCZDdHNEg5ak1ySTlCWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpnREMzODZiNTI3ZHZ6cGl0Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=P0ea_aWnQfx5aXr8bS4u3GGmKX_ORbqAf_1HJ_4Trvg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
