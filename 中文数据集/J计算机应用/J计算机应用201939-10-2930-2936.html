<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136459368721250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910023%26RESULT%3d1%26SIGN%3dp9%252b0mvq3IBnGu7GiTmvXs1BniCI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910023&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910023&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910023&amp;v=Mjc5Mzh0R0ZyQ1VSN3FmWnVac0Z5bmhXcnZMTHo3QmQ3RzRIOWpOcjQ5SFo0UUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="1 材料与方法 ">1 材料与方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="1.1 &lt;b&gt;数据集概况&lt;/b&gt;">1.1 <b>数据集概况</b></a></li>
                                                <li><a href="#64" data-title="1.2 &lt;b&gt;实验方法&lt;/b&gt;">1.2 <b>实验方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#94" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#95" data-title="2.1 &lt;b&gt;支持向量机与迁移学习结果分析&lt;/b&gt;">2.1 <b>支持向量机与迁移学习结果分析</b></a></li>
                                                <li><a href="#99" data-title="2.2 &lt;b&gt;卷积神经网络结果分析&lt;/b&gt;">2.2 <b>卷积神经网络结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="3 结语 ">3 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="&lt;b&gt;表&lt;/b&gt;1 15&lt;b&gt;种葡萄数据特性&lt;/b&gt;"><b>表</b>1 15<b>种葡萄数据特性</b></a></li>
                                                <li><a href="#61" data-title="图1 类内和类间的差异性">图1 类内和类间的差异性</a></li>
                                                <li><a href="#86" data-title="图2 迁移学习网络模型">图2 迁移学习网络模型</a></li>
                                                <li><a href="#93" data-title="图3 &lt;i&gt;MS&lt;/i&gt;-&lt;i&gt;EAlexNet&lt;/i&gt;网络模型">图3 <i>MS</i>-<i>EAlexNet</i>网络模型</a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;典型模型及其预处理方法在葡萄数据集&lt;/b&gt;&lt;i&gt;Vitis&lt;/i&gt;-15&lt;b&gt;上的识别准确率比较&lt;/b&gt;"><b>表</b>2 <b>典型模型及其预处理方法在葡萄数据集</b><i>Vitis</i>-15<b>上的识别准确率比较</b></a></li>
                                                <li><a href="#105" data-title="图4 残差网络测试准确率">图4 残差网络测试准确率</a></li>
                                                <li><a href="#109" data-title="图5 三种数据预处理方法在AlexNet和 MS-EAlexNet网络上的测试准确率">图5 三种数据预处理方法在AlexNet和 MS-EAlexNet网络上的测试准确率</a></li>
                                                <li><a href="#112" data-title="图6 BN在MS-EAlexNet上的训练准确率和损失值的比较">图6 BN在MS-EAlexNet上的训练准确率和损失值的比较</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="146">


                                    <a id="bibliography_1" title="晁无疾.调整提高转型升级促进我国葡萄产业稳步发展[J].中国果菜,2015(9):12-14.(CHAO W J.Adjustment,improvement,transformation and upgrading to promote the steady development of China&#39;s grape industry[J].China Fruit Vegetable,2015(9):12-14.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGGP201509008&amp;v=MDY5MDBQeXJNZnJHNEg5VE1wbzlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFdydkw=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        晁无疾.调整提高转型升级促进我国葡萄产业稳步发展[J].中国果菜,2015(9):12-14.(CHAO W J.Adjustment,improvement,transformation and upgrading to promote the steady development of China&#39;s grape industry[J].China Fruit Vegetable,2015(9):12-14.)
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_2" title="ZHAO B,FENG J,WU X,et al.A Survey on deep learning-based fine-grained object classification and semantic segmentation[J].International Journal of Automation and Computing,2017,14(2):119-135." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDA035BEBF235BB5578932F35423CDDF4F&amp;v=MDM3ODhadWdLZmc0OHloRWI0engvUG56bnFCQTJDc2JnTTc3cENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhieTJ4S3M9Tmo3QmFzSzRIZFMrMnYweg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        ZHAO B,FENG J,WU X,et al.A Survey on deep learning-based fine-grained object classification and semantic segmentation[J].International Journal of Automation and Computing,2017,14(2):119-135.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_3" title="LUO L,TANG Y,ZOU X,et al.Vision-based extraction of spatial information in grape clusters for harvesting robots[J].Biosystems Engineering,2016,151:90-104." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES5B3552F04FEC2D2C1CB6AD72411A5C30&amp;v=MTgxMjdPZmJiS0hkVEpyZmxGWUoxNmYzNU56V1VTbVUxN09RdmxyaFkwZU1PUk5ybWZDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4YnkyeEtzPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        LUO L,TANG Y,ZOU X,et al.Vision-based extraction of spatial information in grape clusters for harvesting robots[J].Biosystems Engineering,2016,151:90-104.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_4" title="FAN J,GAO Y,LUO H.Multi-level annotation of natural scenes using dominant image components and semantic concepts[C]//Proceedings of the 12th Annual ACM International Conference on Multimedia.New York:ACM,2004:540-547." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-Level Annotation of Natural Scenes Using Dominant Image Components and Semantic Image Concepts">
                                        <b>[4]</b>
                                        FAN J,GAO Y,LUO H.Multi-level annotation of natural scenes using dominant image components and semantic concepts[C]//Proceedings of the 12th Annual ACM International Conference on Multimedia.New York:ACM,2004:540-547.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_5" title="NIXON M S,AGUADO A S.特征提取与图像处理[M].李实英,杨高波,译.北京:电子工业出版社,2010:147-289.(NIXON M S,AGUADO A S.Feature Extraction and Image Processing[M].LI S Y,YANG G B,translated.Beijing:Publishing House of Electronics Industry,2010:147-289.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787121118784001&amp;v=MDA0NjdQRFJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlplWnZGeW5uVTc3TUtWb1hYRnF6R2JLNkg5RE5wNGhOWU9z&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        NIXON M S,AGUADO A S.特征提取与图像处理[M].李实英,杨高波,译.北京:电子工业出版社,2010:147-289.(NIXON M S,AGUADO A S.Feature Extraction and Image Processing[M].LI S Y,YANG G B,translated.Beijing:Publishing House of Electronics Industry,2010:147-289.)
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_6" title="HINTON G E,OSINDERO S,TEH Y W.A fast learning algorithm for deep belief nets[J].Neural Computation,2006,18(7):1527-1554." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012538&amp;v=MDM3MjhVcjNJSVZvZGJoRT1OaWZKWmJLOUh0ak1xbzlGWk9vTkNYOHhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        HINTON G E,OSINDERO S,TEH Y W.A fast learning algorithm for deep belief nets[J].Neural Computation,2006,18(7):1527-1554.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_7" title="YU C,WANG J,PENG C,et al.Learning a discriminative feature network for semantic segmentation[C]//Proceedings of the 2018IEEE/CVF Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:1857-1866." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a discriminative feature network for semantic segmentation">
                                        <b>[7]</b>
                                        YU C,WANG J,PENG C,et al.Learning a discriminative feature network for semantic segmentation[C]//Proceedings of the 2018IEEE/CVF Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:1857-1866.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_8" title="SCHUSTER M,PALIWAL K K.Bidirectional recurrent neural networks[J].IEEE Transactions on Signal Processing,1997,45(11):2673-2681." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bidirectional recurrent neural networks">
                                        <b>[8]</b>
                                        SCHUSTER M,PALIWAL K K.Bidirectional recurrent neural networks[J].IEEE Transactions on Signal Processing,1997,45(11):2673-2681.
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_9" title="CARNEIRO G,VASCONCELOS N.Formulating semantic image annotation as a supervised learning problem[C]//Proceedings of the 2005 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2005:163-168." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Formulating semantic image annotation as a supervised learning problem">
                                        <b>[9]</b>
                                        CARNEIRO G,VASCONCELOS N.Formulating semantic image annotation as a supervised learning problem[C]//Proceedings of the 2005 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2005:163-168.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                    Le CUN Y,BOTTOU L,BENGIO Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.</a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_11" title="KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.La Jolla,CA:Neural Information Processing Systems Foundation,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net classification with deep convolutional neural networks">
                                        <b>[11]</b>
                                        KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.La Jolla,CA:Neural Information Processing Systems Foundation,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_12" title="SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-02-10].https://arxiv.org/pdf/1409.1556.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[12]</b>
                                        SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-02-10].https://arxiv.org/pdf/1409.1556.pdf.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_13" title="SZEGEDY C,LIU W,JIA Y,et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Patten Recognition.Piscataway:IEEE,2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going Deepe Rwith Convolutions">
                                        <b>[13]</b>
                                        SZEGEDY C,LIU W,JIA Y,et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Patten Recognition.Piscataway:IEEE,2015:1-9.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_14" title="HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[14]</b>
                                        HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:770-778.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_15" title="SZEGEDY C,IOFFE S,van HOUCKE V,et al.Inception-V4,inception-Res Net and the impact of residual connections on learning[C]//Proceedings of the 2016 31st AAAI Conference on Artificial Intelligence.Pola Alto,CA:AAAI,2016:4278-4284." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-ResNet and the impact of residual connections on learning">
                                        <b>[15]</b>
                                        SZEGEDY C,IOFFE S,van HOUCKE V,et al.Inception-V4,inception-Res Net and the impact of residual connections on learning[C]//Proceedings of the 2016 31st AAAI Conference on Artificial Intelligence.Pola Alto,CA:AAAI,2016:4278-4284.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_16" title="GEHLER P,NOWOZIN S.On feature combination for multiclass object classification[C]//Proceedings of the 12th IEEE International Conference on Computer Vision.Piscataway:IEEE,2009:221-228." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On feature combination for multiclass object classification">
                                        <b>[16]</b>
                                        GEHLER P,NOWOZIN S.On feature combination for multiclass object classification[C]//Proceedings of the 12th IEEE International Conference on Computer Vision.Piscataway:IEEE,2009:221-228.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_17" title="JARRETT K,KAVUKCUOGLU K,RANZATO M,et al.What is the best multi-stage architecture for object recognition?[C]//Proceedings of the 12th IEEE International Conference on Computer Vision.Piscataway:IEEE,2009:2146-2153." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=What is the best multi-stage architecture for object recognition?">
                                        <b>[17]</b>
                                        JARRETT K,KAVUKCUOGLU K,RANZATO M,et al.What is the best multi-stage architecture for object recognition?[C]//Proceedings of the 12th IEEE International Conference on Computer Vision.Piscataway:IEEE,2009:2146-2153.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_18" title="CHEN P H,LIN C J,SCHOLKOPF,BERNHARD.A tutorial onν-support vector machines[J].Applied Stochastic Models in Business and Industry,2005,21(2):111-136." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A tutorial on v-support vector machines">
                                        <b>[18]</b>
                                        CHEN P H,LIN C J,SCHOLKOPF,BERNHARD.A tutorial onν-support vector machines[J].Applied Stochastic Models in Business and Industry,2005,21(2):111-136.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_19" title="WEISS K,KHOSHGOFTAAR T M,WANG D D.A survey of transfer learning[J].Journal of Big Data,2016,3:9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A survey of transfer learning">
                                        <b>[19]</b>
                                        WEISS K,KHOSHGOFTAAR T M,WANG D D.A survey of transfer learning[J].Journal of Big Data,2016,3:9.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_20" title="WOLD S.Principal component analysis[J].Chemometrics&amp;amp;Intelligent Laboratory Systems,1987,2(1):37-52." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Principal component analysis">
                                        <b>[20]</b>
                                        WOLD S.Principal component analysis[J].Chemometrics&amp;amp;Intelligent Laboratory Systems,1987,2(1):37-52.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_21" >
                                        <b>[21]</b>
                                    SZEGEDY C,van HOUCKE V,IOFFE S,et al.Rethinking the Inception architecture for computer vision[C]//Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:2818-2826.</a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_22" title="LECUN Y,BENGIO Y,HINTON G.Deep learning[J].Nature,2015,521(7553):436-444." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning">
                                        <b>[22]</b>
                                        LECUN Y,BENGIO Y,HINTON G.Deep learning[J].Nature,2015,521(7553):436-444.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_23" title="IOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[EB/OL].[2019-01-10].https://arxiv.org/pdf/1502.03167.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:accelerating deep network training by reducing internal covariate shift">
                                        <b>[23]</b>
                                        IOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[EB/OL].[2019-01-10].https://arxiv.org/pdf/1502.03167.pdf.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-08-19 08:55</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),2930-2936 DOI:10.11772/j.issn.1001-9081.2019040594            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于卷积神经网络的多尺度葡萄图像识别方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B1%E6%B4%A5%E6%80%A1&amp;code=42899630&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邱津怡</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BD%97%E4%BF%8A&amp;code=15134018&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">罗俊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E7%A7%80&amp;code=42899631&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李秀</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B4%BE%E4%BC%9F&amp;code=42899632&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贾伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%80%AA%E7%A6%8F%E5%B7%9D&amp;code=22222698&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">倪福川</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%AF%E6%85%A7&amp;code=33883583&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冯慧</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E4%B8%AD%E5%86%9C%E4%B8%9A%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0234297&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华中农业大学信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B9%96%E5%8C%97%E7%9C%81%E5%86%9C%E4%B8%9A%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">湖北省农业大数据工程技术研究中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E4%B8%AD%E5%86%9C%E4%B8%9A%E5%A4%A7%E5%AD%A6%E5%B7%A5%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华中农业大学工学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>葡萄品种质量检测需要识别多类别的葡萄,而葡萄图片中存在多种景深变化、多串等多种场景,单一预处理方法存在局限导致葡萄识别的效果不佳。实验的研究对象是大棚中采集的15个类别的自然场景葡萄图像,并建立相应图像数据集Vitis-15。针对葡萄图像中同一类别的差异较大而不同类别的差异较小的问题,提出一种基于卷积神经网络(CNN)的多尺度葡萄图像识别方法。首先,对Vitis-15数据集中的数据通过三种方法进行预处理:旋转图像的数据扩增方法、中心裁剪的多尺度图像方法以及前两种方法的数据融合方法;然后,采用迁移学习方法和卷积神经网络方法来进行分类识别,迁移学习选取ImageNet上预训练的Inception V3网络模型,卷积神经网络采用AlexNet、ResNet、Inception V3这三类模型;最后,提出适合Vitis-15的多尺度图像数据融合的分类模型MS-EAlexNet。实验结果表明,在同样的学习率和同样的测试集上,数据融合方法在MS-EAlexNet上的测试准确率达到了99.92%,相较扩增和多尺度图像方法提升了近1个百分点,并且所提方法在分类小样本数据集上具有较高的效率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%84%B6%E5%9C%BA%E6%99%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自然场景;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">迁移学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E6%8D%AE%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据融合;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    邱津怡(1995—),女,天津人,硕士研究生,CCF会员,主要研究方向:计算机视觉、图像处理;;
                                </span>
                                <span>
                                    *罗俊(1981—),男,湖北武汉人,副教授,博士,CCF会员,主要研究方向:机器学习、大数据;电子邮箱luojun@mail.hzau.edu.cn;
                                </span>
                                <span>
                                    李秀(1995—),女,山东济南人,硕士研究生,CCF会员,主要研究方向:计算机视觉、图像处理;;
                                </span>
                                <span>
                                    贾伟(1994—),男,四川德阳人,硕士研究生,主要研究方向:计算机视觉、深度学习;;
                                </span>
                                <span>
                                    倪福川(1974—),男,湖北黄冈人,讲师,博士,主要研究方向:机器学习、大数据;;
                                </span>
                                <span>
                                    冯慧(1987—),女,湖北浠水人,讲师,博士,主要研究方向:计算机视觉、植物表型检测。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-10</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(21800305);</span>
                                <span>国家重点研发计划项目(2018YFC1604000);</span>
                                <span>中央高校基本科研业务费专项资金资助项目(2662017PY059);</span>
                    </p>
            </div>
                    <h1><b>Multi-scale grape image recognition method based on convolutional neural network</b></h1>
                    <h2>
                    <span>QIU Jinyi</span>
                    <span>LUO Jun</span>
                    <span>LI Xiu</span>
                    <span>JIA Wei</span>
                    <span>NI Fuchuan</span>
                    <span>FENG Hui</span>
            </h2>
                    <h2>
                    <span>College of Informatics, Huazhong Agricultural University</span>
                    <span>Hubei Engineering Technology Research Center of Agricultural Big Data</span>
                    <span>College of Engineering, Huazhong Agricultural University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Grape quality inspection needs the identification of multiple categories of grapes, and there are many scenes such as depth of field changes and multiple strings in the grape images. Grape recognition is ineffective due to the limitations of single pretreatment method. The research objects were 15 kinds of natural scene grape images collected in the greenhouse, and the corresponding image dataset Vitis-15 was established. Aiming at the large intra-class differences and small inter-class of differences grape images, a multi-scale grape image recognition method based on Convolutional Neural Network(CNN) was proposed. Firstly, the data in Vitis-15 dataset were pre-processed by three methods, including the image rotating based data augmentation method, central cropping based multi-scale image method and data fusion method of the above two. Then, transfer learning method and convolution neural network method were adopted to realiize the classification and recognition. The Inception V3 network model pre-trained on ImageNet was selected for transfer learning, and three types of models — AlexNet, ResNet and Inception V3 were selected for convolution neural network. The multi-scale image data fusion classification model MS-EAlexNet was proposed, which was suitable for Vitis-15. Experimental results show that with the same learning rate on the same test dataset, compared with the augmentation and multi-scale image method, the data fusion method improves nearly 1% testing accuracy on MS-EAlexNet model with 99.92% accuracy, meanwhile the proposed method has higher efficiency in classifying small sample datasets.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=natural%20scene&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">natural scene;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=transfer%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">transfer learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network(CNN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network(CNN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=data%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">data fusion;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    QIU Jinyi,born in 1995,M.S.candidate.Her research interests include computer vision,image processing.;
                                </span>
                                <span>
                                    LUO Jun,born in 1981,Ph.D.,associate professor.His research interests include machine learning,big data.;
                                </span>
                                <span>
                                    LI Xiu,born in 1995,M.S.candidate.Her research interests include computer vision,image processing.;
                                </span>
                                <span>
                                    JIA Wei,born in 1994,M.S.candidate.His research interests include computer vision,deep learning.;
                                </span>
                                <span>
                                    NI Fuchuan,born in 1974,Ph.D.,lecturer.His research interests include machine learning,big data.;
                                </span>
                                <span>
                                    FENG Hui,born in 1987,Ph.D.,lecturer.Her research interests include computer vision,plant phenotype detection.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-10</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China(21800305);</span>
                                <span>the National Key R&amp;D Program of China(2018YFC1604000);</span>
                                <span>the Fundamental Research Funds for the Central Universities(2662017PY059);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="50">近年来,我国葡萄产量逐年增加,截止到2014年,我国葡萄种植面积已达767.2千公顷(7672 km<sup>2</sup>),产量已跃居世界第一<citation id="192" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。目前果园中葡萄分类识别需要大量的人力来完成,然而,由于人力分辨能力和速度的限制,果园中葡萄的分类识别效果不佳;并且由于果园环境的复杂性和不确定性,加之葡萄是簇生水果且其轮廓不规则,导致同一类间差别较大而不同类别间差异较小<citation id="193" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>,使得葡萄串的识别和定位成为难题<citation id="194" type="reference"><link href="150" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="51">多年来,图像识别对于智能果园管理、智能农业目标检测、定位与识别等问题至关重要,然而图像识别的关键在于图像特征的提取,使用显著对象作为图像内容表示和特征提取的主要图像组件<citation id="195" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。按照特征提取方法的不同分为传统图像特征提取方法与深度卷积神经网络自动提取特征两种。通过文献<citation id="196" type="reference">[<a class="sup">5</a>]</citation>总结出:传统图像特征提取方法主要考虑图像的颜色特征、纹理特征、形状特征和空间关系特征。基于颜色直方图特征匹配方法主要有直方图相交法、距离法、中心距法、参考颜色表法、累加颜色直方图法等,由于颜色无法衡量图像的方向和大小,所以不能很好提取图像的局部特征。基于纹理特征提取常用的统计方法是灰度共生矩阵(Gray-Level Co-occurrence Matrix, GLCM)法和半方差图,常用的模型有随机场模型和分形模型,而纹理是居于区域的概念,所以会导致过度区域化而忽略了全局特征。基于形状特征提取的方法主要有边界特征法、傅里叶形状描述符法、几何参数法和形状不变矩法等,而对于形变目标识别效果不佳。基于空间关系特征提取常用两种方法:一种是对图像进行自动分割,划分图像中包含的对象区域,根据这些区域提取图像特征并建立索引关系;另一种是将图像划分为若干子块,对子块进行特征提取并建立索引关系,而对于图像的旋转、尺度变化不敏感。由于传统特征提取方法具有较强的局限性,针对图像分类问题,本文主要采用卷积神经网络来提取图像特征。</p>
                </div>
                <div class="p1">
                    <p id="52">在图像识别技术发展过程中,许多深度学习模型被提出,如:深度置信网络(Deep Belief Network,DBN)<citation id="197" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、判别特征网络(Discriminative Feature Network, DFN)<citation id="198" type="reference"><link href="158" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、卷积神经网络(Convolutional Neural Network, CNN)、循环神经网络(Recurrent Neural Network, RNN)<citation id="199" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等。相较于浅层学习模型的特征提取依靠手工来进行选择,深度学习的深层网络结构可以逐层对数据进行特征提取,使得特征更明显且更容易被用于图像分类和识别。</p>
                </div>
                <div class="p1">
                    <p id="53">深度学习根据监督方式的不同,主要分为监督、半监督和无监督方式将图像语义标注信息表示为监督学习的问题<citation id="200" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。早在20世纪80年代,LeCun等<citation id="201" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出了基于卷积神经网络的手写数字识别网络LeNet-5。2012年Krizhevsky等<citation id="202" type="reference"><link href="166" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出AlexNet模型,并在ImageNet竞赛中取得了冠军,该模型采用两块GPU,大幅提升了网络的运算效率。2014年Simonyan等<citation id="203" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出层数更深、分类效果更好的模型VGG(Visual Geometry Group)。同年GoogLeNet<citation id="204" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>被提出,它采用一种网中网的结构,加大了整个模型的宽度和深度;但随着模型深度加深的同时,会出现模型难以训练的情况。He等<citation id="205" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>在2015年提出残差网络ResNet解决了这个问题;2016年Inception-v4<citation id="206" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>被提出,它是基于Inception-v3的改进,模型更简洁,计算量也更小。在分类识别算法上,通过组合不同的描述符可以获得更好的分类结果<citation id="207" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>,使用监督学习或者无监督学习要比使用随机滤波器和人工特征识别效果更好<citation id="208" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="54">在本文的实验中,采用了支持向量机(Support Vector Machine, SVM)方法<citation id="209" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、迁移学习<citation id="210" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>和深度卷积神经网络来分类葡萄图像,并且提出适合Vitis-15数据集的分类识别模型MS-EAlexNet,最后通过调节参数及激活函数来优化网络,使得在测试集上的效果得以提升。</p>
                </div>
                <h3 id="55" name="55" class="anchor-tag">1 材料与方法</h3>
                <h4 class="anchor-tag" id="56" name="56">1.1 <b>数据集概况</b></h4>
                <h4 class="anchor-tag" id="57" name="57">1.1.1 葡萄数据集Vitis-15</h4>
                <div class="p1">
                    <p id="58">本实验数据于2017年和2018年通过采集大棚中15个类别的自然场景葡萄图像并建立相应小样本数据集,数据集命名为Vitis-15。在拍摄过程中,拍摄条件没有任何限制,在自然光的照射下用安卓和苹果手机对悬挂的葡萄进行拍摄,葡萄品种分别为:比昂扣、夏黑、金手指、美人指、水晶葡萄、摩尔多瓦、甬优一号、克伦生、阳光玫瑰、巨玫瑰、香玉、红提、红地球、黑珍珠、赤霞珠,如表1所示。</p>
                </div>
                <div class="area_img" id="59">
                    <p class="img_tit"><b>表</b>1 15<b>种葡萄数据特性</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.1 Data features of 15 categories of grapes</p>
                    <p class="img_note"></p>
                    <table id="59" border="1"><tr><td>种类</td><td>拍摄<br />数量</td><td>扩增后<br />数量</td><td>拍摄时间</td><td>拍摄地点</td></tr><tr><td>比昂扣</td><td>286</td><td>1 430</td><td>2017- 08</td><td>武汉(江夏)</td></tr><tr><td><br />夏黑</td><td>704</td><td>1 412</td><td>2017- 08<br />2018- 08</td><td>武汉(江夏)<br />武汉(蔡甸)</td></tr><tr><td><br />金手指</td><td>590</td><td>1 180</td><td>2017- 08<br />2018- 08</td><td>武汉(江夏)<br />武汉(蔡甸)</td></tr><tr><td><br />美人指</td><td>481</td><td>1 443</td><td>2018- 08</td><td>武汉(蔡甸)</td></tr><tr><td><br />水晶葡萄</td><td>102</td><td>1 122</td><td>2017- 08</td><td>云南(施甸)</td></tr><tr><td><br />摩尔多瓦</td><td>61</td><td>1 098</td><td>2017- 08</td><td>云南(施甸)</td></tr><tr><td><br />甬优一号</td><td>922</td><td>1 226</td><td>2017- 08<br />2018- 08</td><td>武汉(江夏)<br />武汉(蔡甸)</td></tr><tr><td><br />克伦生</td><td>153</td><td>1 377</td><td>2017- 08</td><td>云南(施甸)</td></tr><tr><td><br />阳光玫瑰</td><td>523</td><td>1 046</td><td>2017- 08<br />2018- 08</td><td>武汉(江夏)<br />武汉(蔡甸)</td></tr><tr><td><br />巨玫瑰</td><td>313</td><td>1 252</td><td>2017- 08</td><td>武汉(江夏)</td></tr><tr><td><br />香玉</td><td>498</td><td>1 494</td><td>2018- 08</td><td>武汉(东西湖)</td></tr><tr><td><br />红提</td><td>280</td><td>1 400</td><td>2018- 08</td><td>武汉(蔡甸)</td></tr><tr><td><br />红地球</td><td>397</td><td>1 588</td><td>2018- 08</td><td>武汉(蔡甸)</td></tr><tr><td><br />黑珍珠</td><td>531</td><td>1 062</td><td>2018- 08</td><td>武汉(东西湖)</td></tr><tr><td><br />赤霞珠</td><td>548</td><td>1 096</td><td>2018- 08</td><td>武汉(蔡甸)</td></tr><tr><td><br /> 总计</td><td>6 389</td><td>19 226</td><td>—</td><td>—</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="60">本数据集的复杂度在于:1)同一品种的葡萄由于年份及生长环境不同,成熟度、色泽、葡萄串的形状会有差别,导致同一类别间差异较大。如图1(a)所示,同一品种的甬优一号有黄绿色、紫红色、黄绿色与浅紫色相间。同一品种的夏黑,由于栽培方式和种植环境的不同,串形和大小有明显差异。2)不同品种之间形状大小及颜色也较为相近,导致不同类别间差异较小。如图1(b)所示,阳光玫瑰、比昂扣和水晶葡萄三个类别,黑珍珠、甬优一号和夏黑三个类别,摩尔多瓦和赤霞珠两个类别,在外观上肉眼难以区分。3)拍摄的图片中既有单串的又有多串的;自然背景较为复杂,有逆光和背光拍摄;有的葡萄支架会与葡萄本身颜色接近形成干扰,葡萄叶片与果粒本身颜色也很接近形成较强的干扰。</p>
                </div>
                <div class="area_img" id="61">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910023_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 类内和类间的差异性" src="Detail/GetImg?filename=images/JSJY201910023_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 类内和类间的差异性  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910023_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Differences within and between classes</p>

                </div>
                <h4 class="anchor-tag" id="62" name="62">1.1.2 Vitis-15数据集预处理</h4>
                <div class="p1">
                    <p id="63">本实验中采取的数据预处理为数据缩放,预先将图片缩放至卷积神经网络要求大小(224×224和299×299)。由于原始拍摄图片宽高比并不是1∶1, 直接缩放到1∶1会使葡萄发生形变,丢失物体本身的特征信息,对葡萄图像分类识别的准确率会有所影响,所以在缩放时保留原始图像的宽高比,空白信息填充像素“0”(即为黑色)。</p>
                </div>
                <h4 class="anchor-tag" id="64" name="64">1.2 <b>实验方法</b></h4>
                <h4 class="anchor-tag" id="65" name="65">1.2.1 支持向量机</h4>
                <div class="p1">
                    <p id="66">训练机器学习分类算法,需要先进行数据预处理;再进行特征选择,最后选择分类器。在数据预处理过程中,先将特征值缩放到相同的区间,称为特征缩放。特征缩放有两个常用的方法:归一化和标准化。本文实验采用标准化。通过标准化,可以将特征列的均值设为0,方差设为1,使得特征列的值呈现标准正态分布,这更易于权重的更新。相较于归一化方法,标准化方法保持了异常值所蕴含的有用信息,并且使得算法受到这些值的影响较小。标准化的过程可用式(1)表示:</p>
                </div>
                <div class="p1">
                    <p id="67"><i><b>x</b></i><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>d</mtext></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>=(<i><b>x</b></i><sup>(</sup><sup><i>i</i></sup><sup>)</sup>-<i>μ</i><sub><i><b>x</b></i></sub>)/<i>σ</i><sub><i><b>x</b></i></sub>      (1)</p>
                </div>
                <div class="p1">
                    <p id="68">其中: <i>μ</i><sub><i><b>x</b></i></sub>和<i>σ</i><sub><i><b>x</b></i></sub>分别为某个特征列的均值和方差。</p>
                </div>
                <div class="p1">
                    <p id="69">由于本文的特征是整幅图片的像素信息,特征列数是相当大的,所以在特征选择过程中,采用特征抽取方法。在尽可能多地保持相关信息的情况下,对数据进行维度压缩。本文采用主成分分析(Principal Component Analysis, PCA)方法<citation id="211" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>来降维。PCA可以基于特征之间的关系识别出数据内在的模式,在高维数据中找到最大方差的方向,并将数据映射到一个维度不大于原始数据的新的子空间上。实验流程为:</p>
                </div>
                <div class="p1">
                    <p id="70">1)构造样本的协方差矩阵;</p>
                </div>
                <div class="p1">
                    <p id="71">2)计算协方差矩阵的特征值和相应的特征向量;</p>
                </div>
                <div class="p1">
                    <p id="72">3)通过第2)步计算得到前9个主成分占总体方差的60%多,选择前9个最大特征值对应的特征向量;</p>
                </div>
                <div class="p1">
                    <p id="73">4)通过前9个特征向量构建映射矩阵;</p>
                </div>
                <div class="p1">
                    <p id="74">5)通过映射矩阵将原始维度的输入数据集转换到新的9维特征子空间。</p>
                </div>
                <div class="p1">
                    <p id="75">在分类器算法中,本文采用支持向量机(SVM)<citation id="212" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>分类算法。为了使用SVM解决非线性问题,通过一个映射函数<i>φ</i>(·)将训练数据映射到更高维的特征空间,并在新的特征空间上训练一个线性SVM模型。然后将同样的映射函数<i>φ</i>(·)应用于新的、未知数据上,进而使用新特征空间上的线性SVM模型对其进行分类。本文映射函数采用径向基函数核(Radical Basis Function kernel, RBF kernel)<citation id="213" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,如式(2)所示。</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>k</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>exp</mi></mrow><mrow><mo>(</mo><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">1.2.2 迁移学习</h4>
                <div class="p1">
                    <p id="78">实验中研究对象是葡萄小样本数据集,若用大型网络自行训练会造成过拟合效果,所以采用迁移学习<citation id="214" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>训练样本集进行分类。</p>
                </div>
                <div class="p1">
                    <p id="79">迁移学习是指源数据域到目标数据域的迁移。如式(3)和(4)所示:</p>
                </div>
                <div class="p1">
                    <p id="80"><i><b>D</b></i><sub><i>s</i></sub>={(<i><b>x</b></i><sub>1</sub>, <i><b>y</b></i><sub>1</sub>),(<i><b>x</b></i><sub>2</sub>, <i><b>y</b></i><sub>2</sub>),…,(<i><b>x</b></i><sub><i>n</i></sub>, <i><b>y</b></i><sub><i>n</i></sub>)}      (3)</p>
                </div>
                <div class="p1">
                    <p id="81"><i><b>D</b></i><sub><i>t</i></sub>={(<i><b>x</b></i>′<sub>1</sub>, <i><b>y</b></i>′<sub>1</sub>),(<i><b>x</b></i>′<sub>2</sub>, <i><b>y</b></i>′<sub>2</sub>),…,(<i><b>x</b></i>′<sub><i>n</i></sub>, <i><b>y</b></i>′<sub><i>n</i></sub>)}      (4)</p>
                </div>
                <div class="p1">
                    <p id="82">其中:<i><b>D</b></i><sub><i>s</i></sub>是源数据域;<i><b>D</b></i><sub><i>t</i></sub>是目标数据域;<i><b>x</b></i>是特征空间;<i><b>y</b></i>是标签空间。</p>
                </div>
                <div class="p1">
                    <p id="83">迁移学习在分类问题中应用广泛,是把已经学好的模型参数迁移到新的模型来帮助新模型训练。实验中迁移学习的模型是Inception V3<citation id="215" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>模型在ImageNet数据集上预训练得到,由于ImageNet数据集里有葡萄图像,所以通过迁移学习可以将已经学到的模型参数分享给新模型,从而提高并优化模型的学习效率,而不用像大多数网络那样从零学习。</p>
                </div>
                <div class="p1">
                    <p id="84">实验中的迁移学习研究过程分为两个部分:一个是迁移学习加SVM分类器对数据集进行分类;另一个是迁移学习加全连接层对数据集进行分类。如图2所示。通过预训练好的深度卷积神经网络的模型,提取图片的特征向量,并将图片的标签传给SVM分类器。使用SVM分类器进行分类,即将迁移学习训练过程中的全连接层改成SVM分类器,把迁移学习提取到的图片特征作为特征向量传送给SVM分类器进行分类。实验过程种将特征向量和训练数据的标签这两个参数传给SVM分类器进行模型训练。再将测试数据的特征向量和真实标签送给SVM分类器,最终对测试数据的特征向量进行预测,返回测试结果,可将测试结果与真实标签进行比对,得到测试准确率。</p>
                </div>
                <div class="p1">
                    <p id="85">在迁移学习提取的特征向量以后连接全连接层,即为第二个实验。在实验过程中,学习率设置为0.01,迭代5 000步,训练批次和测试批次与SVM分类器设置一致。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910023_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 迁移学习网络模型" src="Detail/GetImg?filename=images/JSJY201910023_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 迁移学习网络模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910023_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Transfer learning network model</p>

                </div>
                <h4 class="anchor-tag" id="87" name="87">1.2.3 卷积神经网络</h4>
                <div class="p1">
                    <p id="88">小样本数据集除了用迁移学习的方法外,还可以通过卷积神经网络直接进行训练。卷积神经网络最大的特点是采用了权值共享的策略,其权值共享的网络结构使之更类似于生物神经网络,不仅降低了网络模型的复杂度,而且减少了权值的数量。用卷积神经网络进行重新训练时,本文采用了三种方法对输入图像进行预处理:</p>
                </div>
                <div class="p1">
                    <p id="89">1)为了解决自然场景下的葡萄图像类内差异较大而类间差异较小的问题,提出第一种方法——多尺度(<i>Multi</i>-<i>Scale</i>, <i>MS</i>)方法。将原始图像放大到不同尺度,这样图像中葡萄的边缘特征、形状特征会更加明显,有利于图像特征提取,提高分类精度。实验中采用中心裁剪原始图像的50%、60%、70%、80%四个尺度,再放大到跟原始图像一样的大小(即在原始图像的数量上扩大了5倍)。</p>
                </div>
                <div class="p1">
                    <p id="90">2)为了解决样本数据间的不均衡性,提出了第二种方法——数据扩增(<i>Augment</i>)。通过表1可以看到,拍摄的每个品种的葡萄数量相差较大,会对学习过程造成困扰,为了样本的均衡性,避免在训练过程中,某个类别的样本训练数目太少,而没有学到东西,所以采用过采样方式来进行数据扩增。数据扩增方法有:图像平移、图像旋转、图像镜像、图像亮度变化、图像裁剪、图像缩放、图像模糊等。本文采用的数据扩增方法为图像旋转,有原始拍摄图像分别顺时针旋转5°、10°、20°;逆时针旋转5°、10°、20°。数据扩增后样本数目如表1第3列所示。</p>
                </div>
                <div class="p1">
                    <p id="91">3)前两种方法旨在针对性的解决某一种问题,将两者结合,提出第三种方法——多尺度与数据扩增融合(后文采用<i>Mix</i>)。将前两个方法的训练集融合在一起训练网络,最后进行网络优化,将最后一层全连接层的激活函数由空改为线性整流函数(<i>Rectified Linear Unit</i>, <i>ReLU</i>)<citation id="216" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、归一化函数改为批归一化函数(<i>Batch Normalization</i>, <i>BN</i>)<citation id="217" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="92">实验过程中用三种数据输入方法重新训练了<i>AlexNet</i>、<i>ResNet</i>、<i>Inception V</i>3这三类卷积网络模型,并且提出了增强型多尺度<i>MS</i>-<i>EAlexNet</i>网络,网络模型结构如图3所示,在原始的<i>AlexNet</i>网络的第六层和第七层之间增加一层全连接层,全连接层对于分类问题的表现要优于卷积层。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910023_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 MS-EAlexNet网络模型" src="Detail/GetImg?filename=images/JSJY201910023_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 <i>MS</i>-<i>EAlexNet</i>网络模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910023_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.3 <i>MS</i>-<i>EAlexNet network model</i></p>

                </div>
                <h3 id="94" name="94" class="anchor-tag">2 实验结果与分析</h3>
                <h4 class="anchor-tag" id="95" name="95">2.1 <b>支持向量机与迁移学习结果分析</b></h4>
                <div class="p1">
                    <p id="96"><i>SVM</i>作为机器学习分类算法之一,在学术界和工业界应用广泛,并且是一种较为高效的分类算法。<i>SVM</i>相较于迁移学习,能够极大地缩短训练时间,但是在测试集上的准确率却明显低于迁移学习两个模型,相差近20%。在分类准确率上,迁移学习的两种方法相较于<i>SVM</i>分类性能较好,接近人类分类水准。本文迁移学习加全连接层的方法是端到端模型,可以让网络自己学习如何提取关键特征。</p>
                </div>
                <div class="p1">
                    <p id="97">传统机器学习与深度迁移学习算法各有利弊,迁移学习存在对于数据和硬件的依赖问题而机器学习并不存在,最后的效果如表2前三行所示,迁移学习要优于机器学习,所以在机器硬件性能较好、数据量较大并且对结果要求较高时,可以选用迁移学习方法。</p>
                </div>
                <div class="p1">
                    <p id="98">迁移学习的两种方法为迁移学习+<i>SVM</i>分类器(<i>Inception V</i>3-<i>SVM</i>)和迁移学习+全连接层(<i>Inception V</i>3-<i>FC</i>),虽然这两种方法在测试集上的准确率相差较小,但是在训练时间上<i>Inception V</i>3-<i>FC</i>却是<i>Inception V</i>3-<i>SVM</i>的10倍。所以两种方法在现实应用中根据速度和精度要求不同各有所长。例如,在刑事案件追踪的研究中,对于准确率的要求较低,而更要求时效性时,可以采用迁移学习加<i>SVM</i>方法;在医学影像这种需要高精度的研究中,可以采用迁移学习加全连接层这种方法。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99">2.2 <b>卷积神经网络结果分析</b></h4>
                <div class="p1">
                    <p id="100">模型训练与测试均是在<i>TensorFlow</i>框架下完成的。硬件环境:<i>Intel Xeon E</i>5-2620<i>v</i>4 @2.10 <i>GHz CPU</i>,128 <i>GB</i>内存;<i>NVIDIA GTX</i> 1080 <i>Ti GPU</i>,11 <i>GB</i>显存。软件环境:<i>CUDA Toolkit</i> 9.0,<i>CUDNN V</i>7.0;<i>Python</i> 3.5.2;<i>TensorFlow</i>-<i>GPU</i> 1.7.0;<i>Ubuntu</i>16.04操作系统。模型训练和测试均是通过<i>GPU</i>加速。</p>
                </div>
                <div class="p1">
                    <p id="101">在划分训练集和测试集时,采用8∶2的比例来划分,不是整个所有扩增后的数据直接随机抽取20%作为测试集,而是采用分层抽样的思想,即每个种类都随机采样20%作为测试集,包括1 277张图片,且所有模型的测试准确率均是在同一测试集上所得。</p>
                </div>
                <div class="area_img" id="102">
                    <p class="img_tit"><b>表</b>2 <b>典型模型及其预处理方法在葡萄数据集</b><i>Vitis</i>-15<b>上的识别准确率比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.2 <i>Comparison of recognition accuracy on grape dataset Vitis</i>-15 <i>with typical models and their preprocessing methods</i></p>
                    <p class="img_note"></p>
                    <table id="102" border="1"><tr><td>方法</td><td>网络模型</td><td>迭代步数/10<sup>3</sup></td><td>运行时间</td><td><i>BN</i></td><td><i>ReLU</i></td><td>测试准确率/%</td></tr><tr><td><br /><i>SVM</i><sup>[18]</sup></td><td><br />—</td><td>—</td><td>00:01:30</td><td></td><td></td><td>68.00</td></tr><tr><td><br /><i>Inception V</i>3-<i>SVM</i></td><td><br />—</td><td><br />—</td><td>00:03:45</td><td></td><td></td><td>91.30</td></tr><tr><td><br /><i>Inception V</i>3-<i>FC</i></td><td><br />—</td><td><br />5</td><td>00:30:25</td><td></td><td></td><td>94.70</td></tr><tr><td rowspan="11"><br /><i>MS</i></td><td><br /><i>ResNet</i>50<sup>[14]</sup></td><td><br />25</td><td>02:56:11</td><td></td><td></td><td>90.59</td></tr><tr><td><br /><i>ResNet</i>101<sup>[14]</sup></td><td><br />25</td><td>04:16:46</td><td></td><td></td><td>74.37</td></tr><tr><td><br /><i>ResNet</i>152<sup>[14]</sup></td><td><br />25</td><td>05:53:39</td><td></td><td></td><td>73.40</td></tr><tr><td><br /><i>ResNet</i>200<sup>[14]</sup></td><td><br />25</td><td>08:13:57</td><td></td><td></td><td>61.23</td></tr><tr><td><br /><i>Inception V</i>3<sup>[21]</sup></td><td><br />30</td><td>06:57:23</td><td></td><td></td><td>94.38</td></tr><tr><td rowspan="4"><br /><i>AlexNet</i><sup>[11]</sup></td><td><br />20</td><td>01:54:11</td><td></td><td></td><td>97.81</td></tr><tr><td><br />20</td><td>01:54:47</td><td></td><td>√</td><td>97.73</td></tr><tr><td><br />20</td><td>01:54:22</td><td>√</td><td></td><td>98.59</td></tr><tr><td><br />20</td><td>01:53:34</td><td>√</td><td>√</td><td>98.04</td></tr><tr><td rowspan="2"><br /><i>MS</i>-<i>EAlexNet</i></td><td><br />20</td><td>04:28:39</td><td>√</td><td></td><td>98.28</td></tr><tr><td><br />20</td><td>03:21:56</td><td>√</td><td>√</td><td>98.67</td></tr><tr><td rowspan="9"><br /><i>Augment</i></td><td><br /><i>ResNet</i>50<sup>[14]</sup></td><td><br />25</td><td>04:37:08</td><td></td><td></td><td>98.67</td></tr><tr><td><br /><i>ResNet</i>101<sup>[14]</sup></td><td><br />25</td><td>06:47:26</td><td></td><td></td><td>88.05</td></tr><tr><td><br /><i>Inception V</i>3<sup>[21]</sup></td><td><br />30</td><td>06:54:58</td><td></td><td></td><td>77.61</td></tr><tr><td rowspan="4"><br /><i>AlexNet</i><sup>[11]</sup></td><td><br />20</td><td>01:54:08</td><td></td><td></td><td>98.67</td></tr><tr><td><br />20</td><td>01:54:46</td><td></td><td>√</td><td>98.59</td></tr><tr><td><br />20</td><td>01:55:17</td><td>√</td><td></td><td>98.90</td></tr><tr><td><br />20</td><td>02:19:58</td><td>√</td><td>√</td><td>98.67</td></tr><tr><td rowspan="2"><br /><i>MS</i>-<i>EAlexNet</i></td><td><br />20</td><td>03:21:05</td><td>√</td><td></td><td>98.90</td></tr><tr><td><br />20</td><td>03:31:16</td><td>√</td><td>√</td><td>98.75</td></tr><tr><td rowspan="2"><br /><i>Mix</i></td><td><br /><i>AlexNet</i><sup>[11]</sup></td><td><br />20</td><td>02:00:30</td><td>√</td><td>√</td><td>99.84</td></tr><tr><td><br /><i>MS</i>-<i>EAlexNet</i></td><td><br />20</td><td>03:19:53</td><td>√</td><td></td><td><b>99.92</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="103" name="103">2.2.1 深度卷积网络模型的结果分析</h4>
                <div class="p1">
                    <p id="104">本文实验分别采用AlexNet、ResNet50、ResNet101、ResNet152、ResNet200、Inception V3和MS-EAlexNet七种卷积神经网络进行训练,在测试集上的准确率如表2所示。通过表2和图4可以看出,在MS方法下,四种残差网络(ResNet50、ResNet101、ResNet152和ResNet200)的准确率在采用同样参数训练时,随着网络层数的加深而单调递减。</p>
                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910023_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 残差网络测试准确率" src="Detail/GetImg?filename=images/JSJY201910023_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 残差网络测试准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910023_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 ResNets test accuracy</p>

                </div>
                <div class="p1">
                    <p id="106">在实验过程中当采用与ResNet50和ResNet101这两个网络同样的参数时,ResNet152和ResNet200这两个网络会出现无法收敛现象,将学习率(learning rate, lr)由0.001降低到0.000 1和0.000 06后,网络能够收敛,也验证随着网络模型的加深,Vitis-15分类识别精度降低,所以在后续实验中不考虑ResNet152和ResNet200这两种深层网络。AlexNet网络相较残差和Inception V3网络,网络层数较少,迭代步数较少,训练时间最短且性能较好。对于Vitis-15数据集而言,可以采用AlexNet这类轻量型的卷积网络进行训练。因此可以得出对于小样本数据集分类问题,网络层数越深,模型反而更加难以训练,分类识别效果不佳,所以浅层的网络相较于深层的网络表现更好一些,并且小型网络能够极大地缩短训练时间且性能最好。本文提出的MS-EAlexNet网络,在同种方法对比时比AlexNet表现更好,在测试集上的准确率能够提升约0.1个百分点。</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107">2.2.2 数据预处理三种方法对模型性能的影响</h4>
                <div class="p1">
                    <p id="108">表2中展现了卷积神经网络三种数据预处理方法(MS、Augment、Mix)在七种网络模型上的测试准确率。通过实验发现,1、1/2、1/4这三个多尺度最终会使图片中全部为葡萄图像,不利于整串葡萄图像的识别,而连续多尺度的优点在于:1)基本保持了原始图像中葡萄串形的大小;2)还保留了原始葡萄图像中局部特征的完整性,所以选择了连续多尺度图像。从图5中可以得出,Augment与MS两种方法在AlexNet和MS-EAlexNet网络上测试准确率较为接近,而Mix方法在测试集上的准确率明显高于前两种方法,提升近1个百分点。最终本实验采用Mix方法。</p>
                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910023_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 三种数据预处理方法在AlexNet和 MS-EAlexNet网络上的测试准确率" src="Detail/GetImg?filename=images/JSJY201910023_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 三种数据预处理方法在AlexNet和 MS-EAlexNet网络上的测试准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910023_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Test accuracy of three data pre-processing methods on AlexNet and MS-EAlexnet</p>

                </div>
                <h4 class="anchor-tag" id="110" name="110">2.2.3 BN和ReLU对模型性能的影响</h4>
                <div class="p1">
                    <p id="111">本文采用批归一化处理和ReLU激活函数,对比分析两者对模型性能的影响。在网络的最后一个全连接层中修改激活函数(ReLU)和批归一化函数(Batch Normalization, BN),实验结果发现,修改ReLU激活函数,网络的性能没有明显变化,但是采用BN后网络提升了近1个百分点,达到99.92%的准确率。采用BN可以使得梯度更加可靠和可预测,改善梯度性质使得在计算梯度的方向采用更大的步长而能够保持对实际梯度方向的精确估计。不采用BN时,损失函数不仅非凸而且趋向平坦区域和尖锐极小值。这使得梯度下降算法更加不稳定。并且通过图6可以得出,在采用BN时,网络的训练准确率(图6(a))和训练损失值(图6(b))的波动程度降低,趋于平缓的上升和下降,虽然训练的损失函数最后收敛值高于没有采用BN的,但是在测试集上,采用BN的网络表现更好;而没有采用BN时,网络训练的准确率和损失值波动明显。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910023_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 BN在MS-EAlexNet上的训练准确率和损失值的比较" src="Detail/GetImg?filename=images/JSJY201910023_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 BN在MS-EAlexNet上的训练准确率和损失值的比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910023_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Comparison of training accuracy and loss value of BN in MS-EAlexNet</p>

                </div>
                <h3 id="113" name="113" class="anchor-tag">3 结语</h3>
                <div class="p1">
                    <p id="114">本文基于卷积网络的多尺度葡萄图像品种识别研究,以融合多尺度数据和数据扩增两种方法训练卷积神经网络,对葡萄图像进行分类,并对网络模型、学习方法、批归一化参数等因素对模型性能的影响进行了对比分析,得到如下结论:</p>
                </div>
                <div class="p1">
                    <p id="115">1)深度学习相较于迁移学习和支持向量机的方法,可以较好地自动提取葡萄特征,具有较高的分类性能。在深度学习网络模型中,较浅层的网络在Vitis-15数据集上分类效果要优于较深层的网络,网络层数越深,模型反而更加难以训练,本文提出的MS-EAlexNet网络模型在AlexNet网络基础上修改了网络参数并且增加了一层全连接层,在测试集上的准确率要高于AlexNet网络模型,也表明全连接层更适合分类识别问题。</p>
                </div>
                <div class="p1">
                    <p id="116">2)相较于数据扩增和多尺度图像方法,两者在Vitis-15数据集分类结果都取得了较高的准确率,当将两种方法融合到一起时,测试集的准确率提升了近1个百分点,这也说明了多尺度图像数据融合在小样本数据集分类识别问题中丰富了数据的多样性,减轻了模型的过拟合现象,并且提升了网络的性能。</p>
                </div>
                <div class="p1">
                    <p id="117">3)BN的使用使得网络在训练时更加稳定、波动性小、训练的准确率和训练的损失值更加平滑,并且网络的测试准确率也有明显提升。</p>
                </div>
                <div class="p1">
                    <p id="118">通过本文的实验与分析,卷积神经网络对于特征提取要优于传统的特征提取算法,可以看出数据在预处理过程中数据融合方法是可行的,该方法可以有效提高分类管理和生产效率,可以应用于葡萄分类采摘机器人,降低葡萄人工分类的工作量和劳动力,可为果园智能化的识别提供帮助。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="146">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGGP201509008&amp;v=MTQ4OTNvOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oV3J2TFB5ck1mckc0SDlUTXA=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>晁无疾.调整提高转型升级促进我国葡萄产业稳步发展[J].中国果菜,2015(9):12-14.(CHAO W J.Adjustment,improvement,transformation and upgrading to promote the steady development of China's grape industry[J].China Fruit Vegetable,2015(9):12-14.)
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDA035BEBF235BB5578932F35423CDDF4F&amp;v=MjQ2NDh4YnkyeEtzPU5qN0Jhc0s0SGRTKzJ2MHpadWdLZmc0OHloRWI0engvUG56bnFCQTJDc2JnTTc3cENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>ZHAO B,FENG J,WU X,et al.A Survey on deep learning-based fine-grained object classification and semantic segmentation[J].International Journal of Automation and Computing,2017,14(2):119-135.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES5B3552F04FEC2D2C1CB6AD72411A5C30&amp;v=MjI0ODNscmhZMGVNT1JOcm1mQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeGJ5MnhLcz1OaWZPZmJiS0hkVEpyZmxGWUoxNmYzNU56V1VTbVUxN09Rdg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>LUO L,TANG Y,ZOU X,et al.Vision-based extraction of spatial information in grape clusters for harvesting robots[J].Biosystems Engineering,2016,151:90-104.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-Level Annotation of Natural Scenes Using Dominant Image Components and Semantic Image Concepts">

                                <b>[4]</b>FAN J,GAO Y,LUO H.Multi-level annotation of natural scenes using dominant image components and semantic concepts[C]//Proceedings of the 12th Annual ACM International Conference on Multimedia.New York:ACM,2004:540-547.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787121118784001&amp;v=MTc4MDdOWU9zUERSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2RnlublU3N01LVm9YWEZxekdiSzZIOUROcDRo&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>NIXON M S,AGUADO A S.特征提取与图像处理[M].李实英,杨高波,译.北京:电子工业出版社,2010:147-289.(NIXON M S,AGUADO A S.Feature Extraction and Image Processing[M].LI S Y,YANG G B,translated.Beijing:Publishing House of Electronics Industry,2010:147-289.)
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012538&amp;v=MDI0MjFycVFUTW53WmVadEZpbmxVcjNJSVZvZGJoRT1OaWZKWmJLOUh0ak1xbzlGWk9vTkNYOHhvQk1UNlQ0UFFIL2lyUmRHZQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>HINTON G E,OSINDERO S,TEH Y W.A fast learning algorithm for deep belief nets[J].Neural Computation,2006,18(7):1527-1554.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a discriminative feature network for semantic segmentation">

                                <b>[7]</b>YU C,WANG J,PENG C,et al.Learning a discriminative feature network for semantic segmentation[C]//Proceedings of the 2018IEEE/CVF Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:1857-1866.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bidirectional recurrent neural networks">

                                <b>[8]</b>SCHUSTER M,PALIWAL K K.Bidirectional recurrent neural networks[J].IEEE Transactions on Signal Processing,1997,45(11):2673-2681.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Formulating semantic image annotation as a supervised learning problem">

                                <b>[9]</b>CARNEIRO G,VASCONCELOS N.Formulating semantic image annotation as a supervised learning problem[C]//Proceedings of the 2005 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2005:163-168.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                Le CUN Y,BOTTOU L,BENGIO Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net classification with deep convolutional neural networks">

                                <b>[11]</b>KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.La Jolla,CA:Neural Information Processing Systems Foundation,2012:1097-1105.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[12]</b>SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-02-10].https://arxiv.org/pdf/1409.1556.pdf.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going Deepe Rwith Convolutions">

                                <b>[13]</b>SZEGEDY C,LIU W,JIA Y,et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Patten Recognition.Piscataway:IEEE,2015:1-9.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[14]</b>HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:770-778.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-ResNet and the impact of residual connections on learning">

                                <b>[15]</b>SZEGEDY C,IOFFE S,van HOUCKE V,et al.Inception-V4,inception-Res Net and the impact of residual connections on learning[C]//Proceedings of the 2016 31st AAAI Conference on Artificial Intelligence.Pola Alto,CA:AAAI,2016:4278-4284.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On feature combination for multiclass object classification">

                                <b>[16]</b>GEHLER P,NOWOZIN S.On feature combination for multiclass object classification[C]//Proceedings of the 12th IEEE International Conference on Computer Vision.Piscataway:IEEE,2009:221-228.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=What is the best multi-stage architecture for object recognition?">

                                <b>[17]</b>JARRETT K,KAVUKCUOGLU K,RANZATO M,et al.What is the best multi-stage architecture for object recognition?[C]//Proceedings of the 12th IEEE International Conference on Computer Vision.Piscataway:IEEE,2009:2146-2153.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A tutorial on v-support vector machines">

                                <b>[18]</b>CHEN P H,LIN C J,SCHOLKOPF,BERNHARD.A tutorial onν-support vector machines[J].Applied Stochastic Models in Business and Industry,2005,21(2):111-136.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A survey of transfer learning">

                                <b>[19]</b>WEISS K,KHOSHGOFTAAR T M,WANG D D.A survey of transfer learning[J].Journal of Big Data,2016,3:9.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Principal component analysis">

                                <b>[20]</b>WOLD S.Principal component analysis[J].Chemometrics&amp;Intelligent Laboratory Systems,1987,2(1):37-52.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_21" >
                                    <b>[21]</b>
                                SZEGEDY C,van HOUCKE V,IOFFE S,et al.Rethinking the Inception architecture for computer vision[C]//Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:2818-2826.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning">

                                <b>[22]</b>LECUN Y,BENGIO Y,HINTON G.Deep learning[J].Nature,2015,521(7553):436-444.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:accelerating deep network training by reducing internal covariate shift">

                                <b>[23]</b>IOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[EB/OL].[2019-01-10].https://arxiv.org/pdf/1502.03167.pdf.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910023" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910023&amp;v=Mjc5Mzh0R0ZyQ1VSN3FmWnVac0Z5bmhXcnZMTHo3QmQ3RzRIOWpOcjQ5SFo0UUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
