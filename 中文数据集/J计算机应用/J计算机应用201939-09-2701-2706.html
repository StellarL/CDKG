<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136481909190000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201909037%26RESULT%3d1%26SIGN%3dlk2SNmk0vnXpmQA8Q8gxw3oPkuo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909037&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909037&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909037&amp;v=MDQxMDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqblZMelBMejdCZDdHNEg5ak1wbzlHWTRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#57" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="1 客观评价方法 ">1 客观评价方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="1.1 TV-L&lt;b&gt;多级分解&lt;/b&gt;">1.1 TV-L<b>多级分解</b></a></li>
                                                <li><a href="#68" data-title="1.2 &lt;i&gt;λ&lt;/i&gt;&lt;b&gt;参数对分解图像的影响&lt;/b&gt;">1.2 <i>λ</i><b>参数对分解图像的影响</b></a></li>
                                                <li><a href="#72" data-title="1.3 &lt;b&gt;结构纹理分解评价指标构造&lt;/b&gt;">1.3 <b>结构纹理分解评价指标构造</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#95" data-title="2.1 &lt;b&gt;实验设置&lt;/b&gt;">2.1 <b>实验设置</b></a></li>
                                                <li><a href="#100" data-title="2.2 &lt;i&gt;λ&lt;/i&gt;&lt;b&gt;参数选择&lt;/b&gt;">2.2 <i>λ</i><b>参数选择</b></a></li>
                                                <li><a href="#111" data-title="2.3 &lt;b&gt;对比主观评价&lt;/b&gt;">2.3 <b>对比主观评价</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#117" data-title="3 结语 ">3 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#67" data-title="图1 TV-L分解原理">图1 TV-L分解原理</a></li>
                                                <li><a href="#71" data-title="图2 不同参数&lt;i&gt;λ&lt;/i&gt;调节分解图像的效果">图2 不同参数<i>λ</i>调节分解图像的效果</a></li>
                                                <li><a href="#74" data-title="图3 评价方法整体框图">图3 评价方法整体框图</a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;波达系数算法举例&lt;/b&gt;"><b>表</b>1 <b>波达系数算法举例</b></a></li>
                                                <li><a href="#104" data-title="图4 一组不同类型图像且不同参数分解示例">图4 一组不同类型图像且不同参数分解示例</a></li>
                                                <li><a href="#105" data-title="图5 肯德尔系数结果">图5 肯德尔系数结果</a></li>
                                                <li><a href="#110" data-title="图6 一组红外可见光图像0.2系数分解图示例">图6 一组红外可见光图像0.2系数分解图示例</a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;主观评价的对比&lt;/b&gt;"><b>表</b>2 <b>主观评价的对比</b></a></li>
                                                <li><a href="#116" data-title="图7 部分图像的融合结果比较">图7 部分图像的融合结果比较</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="155">


                                    <a id="bibliography_1" title=" HASSEN R,WANG Z,SALAMA M M A.Objective quality assessment for multiexposure multi-focus image fusion [J].IEEE Transactions on Image Processing,2015,24(9):2712-2724." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Objective quality assessment for multiexposure multifocus image fusion">
                                        <b>[1]</b>
                                         HASSEN R,WANG Z,SALAMA M M A.Objective quality assessment for multiexposure multi-focus image fusion [J].IEEE Transactions on Image Processing,2015,24(9):2712-2724.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_2" title=" MALVIYA A,BHIRUD S G.Objective criterion for performance evaluation of image fusion techniques [J].International Journal of Computer Applications,2010,1(25):57-60." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Objective Criterion for Performance Evaluation of Image Fusion Techniques">
                                        <b>[2]</b>
                                         MALVIYA A,BHIRUD S G.Objective criterion for performance evaluation of image fusion techniques [J].International Journal of Computer Applications,2010,1(25):57-60.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_3" title=" PETROVIC V.Subjective tests for image fusion evaluation and objective metric validation [J].Information Fusion,2007,8(2):208-216." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329191&amp;v=MjIxMjg2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUlGd1RhUlU9TmlmT2ZiSzdIdEROckk5Rlora0dEWFU0b0JNVA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         PETROVIC V.Subjective tests for image fusion evaluation and objective metric validation [J].Information Fusion,2007,8(2):208-216.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_4" title=" 张小利,李雄飞,李军.融合图像质量评价指标的相关性分析及性能评估[J].自动化学报,2014,40(2):306-315.(ZHANG X L,LI X F,LI J.Validation and correlation analysis of metrics for evaluating performance of image fusion [J].Acta Automatica Sinica,2014,40(2):306-315.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201402014&amp;v=MTY4MzdCdEdGckNVUjdxZlp1WnNGeWpuVkx6UEtDTGZZYkc0SDlYTXJZOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         张小利,李雄飞,李军.融合图像质量评价指标的相关性分析及性能评估[J].自动化学报,2014,40(2):306-315.(ZHANG X L,LI X F,LI J.Validation and correlation analysis of metrics for evaluating performance of image fusion [J].Acta Automatica Sinica,2014,40(2):306-315.)
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_5" title=" TIRUPAL T,MOHAN B C,KUMAR S S.Multimodal medical image fusion based on Yager&#39;s intuitionistic fuzzy sets [J].Iranian Journal of Fuzzy Systems,2019,16(1):33-48." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multimodal medical image fusion based on Yager&amp;#39;&amp;#39;s intuitionistic fuzzy sets">
                                        <b>[5]</b>
                                         TIRUPAL T,MOHAN B C,KUMAR S S.Multimodal medical image fusion based on Yager&#39;s intuitionistic fuzzy sets [J].Iranian Journal of Fuzzy Systems,2019,16(1):33-48.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_6" title=" ZHANG X,FENG X,WANG W.et al.Edge strength similarity for image quality assessment [J].IEEE Signal Processing Letters,2013,20(4):319-322." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Edge Strength Similarity for Image Quality Assessment">
                                        <b>[6]</b>
                                         ZHANG X,FENG X,WANG W.et al.Edge strength similarity for image quality assessment [J].IEEE Signal Processing Letters,2013,20(4):319-322.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_7" title=" SHAH P,MERCHANT S N,DESAI U B.Multifocus and multispectral image fusion based on pixel significance using multiresolution decomposition [J].Signal,Image and Video Processing,2013,7(1):95-109." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130122018163&amp;v=MTgzODI5RWJPb0pEeE04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWmVadkZ5bm5VNy9LSjEwVE5qN0Jhcks3SHRET3JZ&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         SHAH P,MERCHANT S N,DESAI U B.Multifocus and multispectral image fusion based on pixel significance using multiresolution decomposition [J].Signal,Image and Video Processing,2013,7(1):95-109.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_8" title=" HOSSNY M,NAHAVANDI S,CREIGHTON D.Comments on ‘Information measure for performance of image fusion’ [J].Electronics Letters,2008,44(18):1066-1067." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Comments on &amp;#39;Information measure for performance of image fusion&amp;#39;">
                                        <b>[8]</b>
                                         HOSSNY M,NAHAVANDI S,CREIGHTON D.Comments on ‘Information measure for performance of image fusion’ [J].Electronics Letters,2008,44(18):1066-1067.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_9" title=" LIU Z,BLASCH E,XUE Z,et al.Objective assessment of multiresolution image fusion algorithms for context enhancement in night vision:a comparative study [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(1):94-109." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Objective Assessment of Multiresolution Image Fusion Algorithms for Context Enhancement in Night Vision: A Comparative Study">
                                        <b>[9]</b>
                                         LIU Z,BLASCH E,XUE Z,et al.Objective assessment of multiresolution image fusion algorithms for context enhancement in night vision:a comparative study [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(1):94-109.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_10" title=" CVEJIC N,CANAGARAJAH C N,BULL D R.Image fusion metric based on mutual information and tsallis entropy [J].Electronics Letters,2006,42(11):626-627." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image fusion metric based on mutual information and Tsallis entropy">
                                        <b>[10]</b>
                                         CVEJIC N,CANAGARAJAH C N,BULL D R.Image fusion metric based on mutual information and tsallis entropy [J].Electronics Letters,2006,42(11):626-627.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_11" title=" WANG Z,BOVIK A C,SHEIKH H R.et al.Image quality assessment:from error visibility to structural similarity [J].IEEE Transactions on Image Processing,2004,13(4):600-612." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image quality assessment: from error visibility to structural similarity">
                                        <b>[11]</b>
                                         WANG Z,BOVIK A C,SHEIKH H R.et al.Image quality assessment:from error visibility to structural similarity [J].IEEE Transactions on Image Processing,2004,13(4):600-612.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_12" title=" PIELLA G,HEIJMANS H.A new quality metric for image fusion [C]// Proceedings of the 2003 International Conference on Image Processing.Piscataway,NJ:IEEE,2003:173-176." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A new quality metric for imagefusion">
                                        <b>[12]</b>
                                         PIELLA G,HEIJMANS H.A new quality metric for image fusion [C]// Proceedings of the 2003 International Conference on Image Processing.Piscataway,NJ:IEEE,2003:173-176.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_13" title=" PISTONESI S,MARTINEZ J,OJEDA S M,et al.Structural similarity metrics for quality image fusion assessment:algorithms [J].Image Processing on Line,2018,8:345-368." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structural similarity metrics for quality image fusion assessment:algorithms">
                                        <b>[13]</b>
                                         PISTONESI S,MARTINEZ J,OJEDA S M,et al.Structural similarity metrics for quality image fusion assessment:algorithms [J].Image Processing on Line,2018,8:345-368.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     CHEN H,VARSHNEY P K.A human perception inspired quality metric for image fusion based on regional information [J].Information Fusion,2007,8(2):193-207.</a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_15" title=" LI S,HONG R,WU X,et al.A noval similarity based quality metric for image fusion [C]// Proceeding of the 2008 International Conference on Audio,Language and Image Processing.Piscataway,NJ:IEEE,2008:167-172." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel similarity based quality metric for image fusion">
                                        <b>[15]</b>
                                         LI S,HONG R,WU X,et al.A noval similarity based quality metric for image fusion [C]// Proceeding of the 2008 International Conference on Audio,Language and Image Processing.Piscataway,NJ:IEEE,2008:167-172.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_16" title=" CHEN H,VARSHENEY P K.A human perception inspired quality metric for image fusion based on regional information [J].Image Fusion,2007,8(2):193-207." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329187&amp;v=MDQ3NDVpbmxVcjNJSUZ3VGFSVT1OaWZPZmJLN0h0RE5ySTlGWitrR0RYUStvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0Rg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         CHEN H,VARSHENEY P K.A human perception inspired quality metric for image fusion based on regional information [J].Image Fusion,2007,8(2):193-207.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_17" title=" CHEN Y.BLUM R S.A new automated quality assessment algorithm for image fusion [J].Image and Vision Computing,2009,27(10):1421-1432." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349038&amp;v=MTU2MjFlWnRGaW5sVXIzSUlGd1RhUlU9TmlmT2ZiSzdIdERPclk5RVorOEdESDh4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         CHEN Y.BLUM R S.A new automated quality assessment algorithm for image fusion [J].Image and Vision Computing,2009,27(10):1421-1432.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_18" title=" 程刚,王春恒.基于结构和纹理特征融合的场景图像分类[J].计算机工程,2011,37(5):227-229.(CHENG G,WANG C H.Scene image categorization based on structure and texture feature fusion [J].Computer Engineering,2011,37(5):227-229.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201105079&amp;v=MzEyMjdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqblZMelBMejdCYmJHNEg5RE1xbzlDYllRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         程刚,王春恒.基于结构和纹理特征融合的场景图像分类[J].计算机工程,2011,37(5):227-229.(CHENG G,WANG C H.Scene image categorization based on structure and texture feature fusion [J].Computer Engineering,2011,37(5):227-229.)
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_19" title=" FANG Y,CHEN Q,SUN L,et al.Decomposition and extraction:a new framework for visual classification [J].IEEE Transactions on Image Processing,2014,23(8):3412-3427." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Decomposition and extraction:a new framework for visual classification">
                                        <b>[19]</b>
                                         FANG Y,CHEN Q,SUN L,et al.Decomposition and extraction:a new framework for visual classification [J].IEEE Transactions on Image Processing,2014,23(8):3412-3427.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_20" title=" WANG Z,WANG W,SU B.Multi-sensor image fusion algorithm based on multiresolution analysis [J].International Journal of Online Engineering,2018,14(6):44-57." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-sensor image fusion algorithm based on multiresolution analysis">
                                        <b>[20]</b>
                                         WANG Z,WANG W,SU B.Multi-sensor image fusion algorithm based on multiresolution analysis [J].International Journal of Online Engineering,2018,14(6):44-57.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_21" title=" BEHRMANN M,KIMCHI R.What does visual agnosia tell us about perceptual organization and its relationship to object perception?[J].Journal of Experimental Psychology:Human Perception and Performance,2003,29(1):19-42." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=What does visual agnosia tell us about perceptual organization and its relationship to object perception?">
                                        <b>[21]</b>
                                         BEHRMANN M,KIMCHI R.What does visual agnosia tell us about perceptual organization and its relationship to object perception?[J].Journal of Experimental Psychology:Human Perception and Performance,2003,29(1):19-42.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_22" title=" 陈浩,王延杰.基于拉普拉斯金字塔变换的图像融合算法研究[J].激光与红外,2009,39(4):439-442.(CHEN H,WANG Y J.Research on image fusion algorithm based on Laplacian pyramid transform [J].Laser and Infrared,2009,39(4):439-442.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGHW200904029&amp;v=MTQ0NzM0OUhiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpuVkx6UEx5ckRlYkc0SHRqTXE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         陈浩,王延杰.基于拉普拉斯金字塔变换的图像融合算法研究[J].激光与红外,2009,39(4):439-442.(CHEN H,WANG Y J.Research on image fusion algorithm based on Laplacian pyramid transform [J].Laser and Infrared,2009,39(4):439-442.)
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_23" title=" LIU Y,LIU S.WANG Z.A general framework for image fusion based on multi-scale transform and sparse representation [J].Information Fusion,2015,24:147-164." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700451440&amp;v=MjM2ODFPNE9DSGc1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSUZ3VGFSVT1OaWZPZmJLOEg5RE1xSTlGWQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         LIU Y,LIU S.WANG Z.A general framework for image fusion based on multi-scale transform and sparse representation [J].Information Fusion,2015,24:147-164.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_24" title=" BHATNAGAR G,WU Q M J,LIU Z.Directive contrast based multimodal medical image fusion in NSCT domain [J].IEEE Transactions on Multimedia,2013,15(5):1014-1024." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Directive contrast based multimodal medical image fusion in NSCT domain">
                                        <b>[24]</b>
                                         BHATNAGAR G,WU Q M J,LIU Z.Directive contrast based multimodal medical image fusion in NSCT domain [J].IEEE Transactions on Multimedia,2013,15(5):1014-1024.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_25" title=" LI S,KANG X,HU J.Image fusion with guided filtering [J].IEEE Transactions on Image Processing,2013,22(7):2864-2875." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image fusion with guided filtering">
                                        <b>[25]</b>
                                         LI S,KANG X,HU J.Image fusion with guided filtering [J].IEEE Transactions on Image Processing,2013,22(7):2864-2875.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_26" title=" 屈小波,闫敬文,肖弘智,等.非降采样Contourlet域内空间频率激励的PCNN图像融合算法[J].自动化学报,2008,34(12):1508-1514.(QU X B,YAN J W,XIAO H Z,et al.Image fusion algorithm based on spatial frequency-motivated pulse coupled neural networks in nonsubsampled Contourlet transform domain [J].Acta Automatica Sinica,2008,34(12):1508-1514.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO200812010&amp;v=MjA5NjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpuVkx6UEtDTGZZYkc0SHRuTnJZOUVaSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         屈小波,闫敬文,肖弘智,等.非降采样Contourlet域内空间频率激励的PCNN图像融合算法[J].自动化学报,2008,34(12):1508-1514.(QU X B,YAN J W,XIAO H Z,et al.Image fusion algorithm based on spatial frequency-motivated pulse coupled neural networks in nonsubsampled Contourlet transform domain [J].Acta Automatica Sinica,2008,34(12):1508-1514.)
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_27" title=" LIU Y,CHEN X,WANG Z,et al.Deep learning for pixel-level image fusion recent:advances and future prospects [J].Information Fusion,2018,42:158-173." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFB5835143E660843FA8E042889D49001&amp;v=MDM0MTJHOW5QcW81Qlo1NEpDbnd4eXhWbG16Y0lTSHZncEJvOERiYWRSYnFlQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeExxNHc2OD1OaWZPZmNYSw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         LIU Y,CHEN X,WANG Z,et al.Deep learning for pixel-level image fusion recent:advances and future prospects [J].Information Fusion,2018,42:158-173.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-05-31 11:44</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(09),2701-2706 DOI:10.11772/j.issn.1001-9081.2019020302            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于TV-L</b><sup>1</sup><b>结构纹理分解的图像融合质量评价算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%96%8C&amp;code=07760150&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张斌</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BD%97%E6%99%93%E6%B8%85&amp;code=11159107&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">罗晓清</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%88%98%E6%88%90&amp;code=40233368&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张战成</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E5%8D%97%E5%A4%A7%E5%AD%A6%E7%89%A9%E8%81%94%E7%BD%91%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0074200&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江南大学物联网工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%8B%8F%E5%B7%9E%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0228583&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏州科技大学电子与信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为对图像融合算法进行客观准确的综合评价,提出一种基于总变差正则化(TV-L<sup>1</sup>)结构纹理分解的评价算法。根据对人类视觉系统的研究,可知人们对图像质量的感知主要来自图像底层视觉特征,而结构特征以及纹理特征是最重要的图像底层视觉特征,但目前的图像融合质量评价算法并没有利用这两种特征来进行评价。鉴于此,将图像进行二级结构和纹理分解,根据结构和纹理图像蕴含图像特征的不同,从结构图像和纹理图像两方面分别进行相似度评价,综合各级得分得到最终的评价总得分。基于30幅图像的数据集和8种主流融合算法,参照已有的11种客观评价指标,用波达计数法和肯德尔系数检验了该评价指标的一致性,另外在主观评价图像集上验证了该客观评价指标与主观评价的一致性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%9E%8D%E5%90%88%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">融合质量评价;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%80%BB%E5%8F%98%E5%B7%AE%E6%AD%A3%E5%88%99%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">总变差正则化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%93%E6%9E%84%E7%9B%B8%E4%BC%BC%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">结构相似度;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%93%E6%9E%84%E5%92%8C%E7%BA%B9%E7%90%86%E5%88%86%E8%A7%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">结构和纹理分解;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张斌(1994—),男,江西景德镇人,硕士研究生,主要研究方向:图像融合;;
                                </span>
                                <span>
                                    *罗晓清(1980—),女,江西南昌人,副教授,博士,CCF会员,主要研究方向:图像融合、医学图像分析;电子邮箱xqluo@jiangnan.edu.cn;
                                </span>
                                <span>
                                    张战成(1977—),男,山西平遥人,副教授,博士,主要研究方向:图像融合、医学图像分析。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-26</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61772237);</span>
                                <span>中央高校基本科研业务费资助项目(JUSRP51618B);</span>
                                <span>苏州市重点产业技术创新项目(SYG201702);</span>
                    </p>
            </div>
                    <h1><b>Image fusion quality evaluation algorithm based on TV-L</b><sup>1</sup><b>structure and texture decomposition</b></h1>
                    <h2>
                    <span>ZHANG Bin</span>
                    <span>LUO Xiaoqing</span>
                    <span>ZHANG Zhancheng</span>
            </h2>
                    <h2>
                    <span>School of Internet of Things Engineering, Jiangnan University</span>
                    <span>School of Electronics and Information Engineering, Suzhou University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to objectively and accurately evaluate the image fusion algorithms, an evaluation algorithm based on TV-L<sup>1</sup>(Total Variation regularization) structure and texture decomposition was proposed. According to the studies on human visual system, human's perception to image quality mainly comes from the underlying visual features of image, and structure features and texture features are the most important features of underlying visual feature of image. However, the existed image fusion quality evaluation algorithms ignore this fact and lead to inaccurate evaluation. To address this problem, a pair of source images and their corresponding fusion results were individually decomposed into structure and texture images with a two-level TV-L<sup>1</sup> decomposition. Then, According to the difference of image features between the structure and texture images, the similarity evaluation was carried out from the decomposed structure image and the texture image respectively, and the final evaluation score was obtained by integrating the scores at all levels. Based on the dataset with 30 images and 8 mainstream fusion algorithms, compared with the 11 existing objective evaluation indexes, the Borda counting method and Kendall coefficient were employed to verify the consistency of the proposed evaluation algorithm. Moreover, the consistency between the proposed objective evaluation index and the subjective evaluation is verified on the subjective evaluation image set.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fusion%20quality%20evaluation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fusion quality evaluation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=total%20variation%20regularization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">total variation regularization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=structural%20similarity&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">structural similarity;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=structure%20and%20texture%20decomposition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">structure and texture decomposition;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Bin, born in 1994, M. S. candidate. His research interests include image fusion. ;
                                </span>
                                <span>
                                    LUO Xiaoqing, born in 1980, Ph. D. , association professor. Her research interests include image fusion, medical image analysis. ;
                                </span>
                                <span>
                                    ZHANG Zhancheng, born in 1977, Ph. D. , association professor. His research interests include image fusion, medical image analysis.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-02-26</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China(61772237);</span>
                                <span>the Fundamental Research Funds for the Central Universities(JUSRP51618B);</span>
                                <span>the Technological Innovation Projects of Major Industries in Suzhou City(SYG201702);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="57" name="57" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="58">图像融合是指通过传感器针对不同的场景捕获不同内容的图片,经过整合形成一幅质量更高图像的技术<citation id="209" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。如何丰富融合图像的信息量和改善图像质量是图像融合的关键技术之一,也是衡量各种融合方法效果的基本准则。采用科学合理、客观公正的方法对融合图像进行评价,对在实际应用中选择融合算法以及现有融合算法的改进和研究新的融合算法等具有十分重要的指导意义。因此,融合质量评价是图像融合技术中不可或缺的核心内容之一<citation id="210" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。图像融合质量评价方法分为两类:一类是将融合图像交给人眼的进行主观感觉度量的方法。最经典的是曼彻斯特大学的Petrovic<citation id="211" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>2007年提出的主观测量方法,通过多个测试者对不同融合方法得到的融合图像中的特定目标进行识别,依据识别时间和识别的正确率,判断图像融合方法的优劣。由于在大部分情况下人是融合图像的直接使用者,所以该方法具有一定的可靠性以及直接性。但是该方法也存在一定的局限性:1)由于人脑的感知图像的机制非常复杂,从而导致人的主观感觉有可能是不一样的。2)评价成本高。该方法非常耗时耗力,并且效率也比较低。3)受环境的影响比较大,比如观察距离、光照等都会影响人的判断。另一类是通过计算相关指标来定量地模拟人类的视觉系统(Human-Visual-System, HVS)的客观法<citation id="212" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。该方法通过定量的测量使得融合效果具有较高的客观性,是如今流行的图像融合质量评价方法。从原理上可以把客观法分为三类<citation id="213" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="59">1)基于统计特性的评价指标<citation id="214" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>:文献<citation id="215" type="reference">[<a class="sup">6</a>]</citation>提到的平均梯度,可以敏感地反映出图像的微小细节反差和纹理变换特征,用来评价图像的相对清晰度,平均梯度越大,图像的层次越多,纹理也就更加清晰;文献<citation id="216" type="reference">[<a class="sup">7</a>]</citation>中的边缘强度,是指边缘与邻近像素的对比强度,边缘越丰富,融合图像越清晰;文献<citation id="217" type="reference">[<a class="sup">8</a>]</citation>使用的空间频率,反映的是空间域内图像的总体活跃度。但是上述评价指标无法反映源图像和融合图像之间的依赖程度,而这种依赖程度可以体现融合图像的质量,因此一般不能将上述评价指标单独用于图像融合质量评价。</p>
                </div>
                <div class="p1">
                    <p id="60">2)基于信息量的客观指标<citation id="218" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>:文献<citation id="219" type="reference">[<a class="sup">9</a>]</citation>提到的互信息,它是衡量两个域之间变量的依懒性,用来比较融合图像与源图像之间灰度分布方面的相似情况;文献<citation id="220" type="reference">[<a class="sup">10</a>]</citation>设计的非线性的信息熵,它和互信息的定义是类似的。但是该类指标没有相应的参数可以调节,适应程度比较差,需要与其他的指标进行综合评价。</p>
                </div>
                <div class="p1">
                    <p id="61">3)基于人类感知的评价指标<citation id="221" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>:该指标是基于人类的视觉系统对图像的感知原理来判断融合图像的优劣。其中人眼对图像的边缘信息、结构化信息以及空间频率信息是非常敏感的。</p>
                </div>
                <div class="p1">
                    <p id="62">Xydeas和Petrovic认为人类视觉系统对图像的边缘十分敏感<citation id="222" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>,提出了通过融合图像保留源图像中边缘信息的完整程度,从而达到评价融合图像的质量。该方法首先用Sobel算子检测出图像的梯度和边缘角用来计算边缘强度和方向信息,然后求和归一化。由于人眼对图像边缘较为敏感,该方法与人眼主观评价具有较高的一致性。同样利用边缘信息的还有Liu等<citation id="223" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出的基于双层的哈尔小波变换指标。该方法从小波分解的高阶和带通分量中提取边缘信息。Wang等<citation id="224" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>认为人类视觉系统对图像的失真程度也较为敏感,据此他们提出了通用图像质量评价(Universal Image Quality Index, UIQI)。该指标通过计算源图像和融合图像之间的结构损失、亮度失真、以及对比度相似性,进而作出相应的评价。该指标有效地对源图像和融合图像的结构失真进行计算,评价效果得到了学术界的认可。在Wang的UIQI的基础上,Piella等<citation id="225" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>作出了一定的改进,分别提出了图像融合质量指数<i>Q</i><sub>0</sub>、加权融合质量指数<i>Q</i><sub><i>w</i></sub>、以及边缘相关融合质量指数<i>Q</i><sub><i>E</i></sub>。图像融合质量指数<i>Q</i><sub>0</sub>在UIQI的基础上引入了局部窗体,利用局部窗体对图像进行分割。然后计算每个窗体的相似程度。由于<i>Q</i><sub>0</sub>对图像的每一区域没有区别对待,而人眼对图像每一块区域感知程度是不同的,所以随后提出了加权融合质量指数<i>Q</i><sub><i>w</i></sub>,该评价指标对图像中信息丰富的窗体给予较大的权重,而具有较少信息的窗体采用较少的权重,使得该指标取得了较好的效果;由于人眼对图像边缘也比较敏感,将边缘图像引入到<i>Q</i><sub><i>w</i></sub>中,据此提出了<i>Q</i><sub><i>E</i></sub>边缘相关融合质量指数。视觉研究人员发现人类视觉系统对图像的感知是高度结构化的<citation id="226" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>,所以Wang在UIQI的基础上提出了结构相似性(Structural SIMilarity index, SSIM)的客观评价,使得图像质量评价领域从像素阶段上升到了结构阶段。但是对模糊图像评价时效果不太理想。将结构性引入评价的还有两种基于结构相似矩阵的评价指标,分别是Cvejic指标<citation id="227" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>和Yang指标<citation id="228" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。Cvejic指标是通过计算源图像与融合图像的协方差来定出加权系数。Yang指标与最后主观结果具有一定的差异性,因为没有考虑到源图像与源图像之间的差异性导致了评价效果不太好。最后根据对比敏感度函数(Contrast Sensitivity Function, CSF)人类视觉系统对图像空间频率的感知程度,据此,Chen等<citation id="229" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出了利用CSF来用于评价指标。由于CSF无法捕获非线性的局部信息,所以首先将图片进行分块,然后计算区域的显著度。同样利用CSF方法的还有Chen-Blum指标<citation id="230" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>,它是利用该函数得到最后的显著度特征图来计算指标。</p>
                </div>
                <div class="p1">
                    <p id="63">根据对人类视觉系统的研究,人们对图像质量感知主要来自于图像底层视觉特征,其中纹理特征是最重要的特征之一<citation id="231" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。纹理特征产生的位置与图像在人眼视觉系统中产生的区域是一样的,此外纹理特征还有比较好的表征图像质量感知的能力。图像的结构特征是整张图像的轮廓<citation id="232" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,人眼对轮廓也有着较高的辨识度,所以结构特征也是一个比较重要的特征。但是上述指标并没有对图像的纹理特征和结构特征进行单独评价。目前在结构纹理分解的研究领域中总变差正则化(Total Variation regularization, TV-L<sup>1</sup>)<citation id="233" type="reference"><link href="193" rel="bibliography" /><link href="195" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>的表现是最好的,并且在2014年Fang等发表的文章的中就已经将图像的TV-L<sup>1</sup>分解的结构和纹理作为重要的特征用在了图像分类上,并取得了较好的效果。受此启发,本文提出基于TV-L<sup>1</sup>结构纹理分解的图像融合质量评价方法。鉴于纹理特征和结构特征在实际图像中所占的比例不同,而在TV-L<sup>1</sup>结构纹理分解中的系数<i>λ</i>恰好可以调节纹理特征和结构特征的权重,所以本文算法能够准确地评价融合图像的质量。</p>
                </div>
                <h3 id="64" name="64" class="anchor-tag">1 客观评价方法</h3>
                <h4 class="anchor-tag" id="65" name="65">1.1 TV-L<sup>1</sup><b>多级分解</b></h4>
                <div class="p1">
                    <p id="66">TV-L<sup>1</sup>分解<citation id="234" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>是将一幅图像多级分解为纹理图像和结构图像的方法。分解原理如图1所示。首先将原图像<i>f</i>分解为结构图像<i>u</i><sub><i>λ</i></sub><sub>1</sub>和纹理图像<i>v</i><sub><i>λ</i></sub><sub>1</sub>,有<i>f</i>= <i>u</i><sub><i>λ</i></sub><sub>1</sub>+<i>v</i><sub><i>λ</i></sub><sub>1</sub> ,即结构图像和纹理图像合并就是原图像,参数<i>λ</i><sub>1</sub>调节结构和纹理比例。接着保留结构图像,继续将刚刚分解的纹理图像用系数<i>λ</i><sub>2</sub>进行结构纹理分解,得到第二级的结构图像<i>u</i><sub><i>λ</i></sub><sub>2</sub>和纹理图像<i>v</i><sub><i>λ</i></sub><sub>2</sub>。此后不断地如此分解,最后<i>v</i><sub><i>λk</i></sub>=<i>u</i><sub><i>λk</i></sub>+1+ <i>v</i><sub><i>λk</i></sub>+1,也就是上一级的纹理图像被分解为下一级的结构图像和纹理图像。</p>
                </div>
                <div class="area_img" id="67">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909037_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 TV-L1分解原理" src="Detail/GetImg?filename=images/JSJY201909037_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 TV-L<sup>1</sup>分解原理  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909037_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Illustration of TV-L<sup>1 </sup>decomposition</p>

                </div>
                <h4 class="anchor-tag" id="68" name="68">1.2 <i>λ</i><b>参数对分解图像的影响</b></h4>
                <div class="p1">
                    <p id="69">分解系数<i>λ</i>调节纹理图像和结构图像的权重在TV-L<sup>1</sup>分解中有重要的意义。以一幅用不同参数<i>λ</i>分解的多聚焦图像为例。如图2所示,第一行为两张源图像和一张融合图像,下面三行左边两列是对融合图像(c图)的一级分解图像,右边两列是二级分解图像。</p>
                </div>
                <div class="p1">
                    <p id="70">从图2中可以看到随着系数<i>λ</i>不断地增大,结构图像越来越清晰,纹理图像会逐渐模糊。不同场景的融合图像都有类似的效果,这将在2.2节中详细讨论。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909037_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同参数λ调节分解图像的效果" src="Detail/GetImg?filename=images/JSJY201909037_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 不同参数<i>λ</i>调节分解图像的效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909037_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Effect of different parameter <i>λ</i> on image decomposition</p>

                </div>
                <h4 class="anchor-tag" id="72" name="72">1.3 <b>结构纹理分解评价指标构造</b></h4>
                <div class="p1">
                    <p id="73">假设A、B分别是针对同一场景但是由不同传感器获得的图像,F表示图像A和B的融合图像。本文具体构造流程如图3所示。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909037_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 评价方法整体框图" src="Detail/GetImg?filename=images/JSJY201909037_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 评价方法整体框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909037_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Diagram of the proposed evaluation method</p>

                </div>
                <div class="p1">
                    <p id="75">本文的创新点是将融合图像质量评价与人类视觉感知中的视觉信息分离和整合理论结合起来,即人眼分别通过图像的纹理信息和结构信息感知图像,然后再将两种信息结合作出综合的评价。另外图像可以分解为结构信息和纹理信息,且在特定尺度下,纹理信息包含着重要的结构信息,所以为了将图像分解得更加细致,将一级分解的纹理图像继续分解得到二级的纹理图像和结构图像。</p>
                </div>
                <div class="p1">
                    <p id="76">1.2节介绍到,随着系数<i>λ</i>不断地增大,结构图像越来越清晰,纹理图像会逐渐模糊,而当系数为0.8时,纹理图像已经非常模糊,所以把系数设置在[0,1]。由于设置的初始权重较小,且纹理信息总是包含着重要的结构信息,即纹理图像的权重要大于结构图像的权重,所以把初始分解系数<i>λ</i>分配为结构图像,而纹理图像的权重为1-<i>λ</i>。二级分解也是如此。</p>
                </div>
                <div class="p1">
                    <p id="77">由于本文的纹理信息包含着大量的结构信息,而结构信息中有着丰富的边缘信息,所以本文采用基于边缘信息的客观指标(<i>Q</i><sub><i>G</i></sub>)来计算融合得分,因为该指标是通过评估从源图像到融合图像的边缘信息转移量来衡量图像融合质量。该指标的计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Q</mi><msub><mrow></mrow><mi>G</mi></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mo stretchy="false">[</mo></mstyle></mrow></mstyle><mi>Q</mi><msup><mrow></mrow><mrow><mtext>A</mtext><mtext>F</mtext></mrow></msup><mi>w</mi><msup><mrow></mrow><mtext>A</mtext></msup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo><mo>+</mo><mi>Q</mi><msup><mrow></mrow><mrow><mtext>B</mtext><mtext>F</mtext></mrow></msup><mi>w</mi><msup><mrow></mrow><mtext>B</mtext></msup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mo stretchy="false">[</mo></mstyle></mrow></mstyle><mi>w</mi><msup><mrow></mrow><mtext>A</mtext></msup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo><mo>+</mo><mi>w</mi><msup><mrow></mrow><mtext>B</mtext></msup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">其中:<i>Q</i><sup>AF</sup>和<i>Q</i><sup>BF</sup>分别是源图像A与源图像B分别与融合图像F边缘宽度与方向的相似度;<i>w</i><sup>A</sup>(<i>i</i>, <i>j</i>),<i>w</i><sup>B</sup>(<i>i</i>, <i>j</i>)分别是<i>Q</i><sup>AF</sup>和<i>Q</i><sup>BF</sup>的权重。</p>
                </div>
                <div class="p1">
                    <p id="80">构造的步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="81">步骤1 分别将源图像A、B和融合图像F用系数<i>λ</i>进行分解,分别得到结构图像<i>S</i><sup>A</sup><sub>1</sub>、<i>S</i><sup>B</sup><sub>1</sub>、<i>S</i><sup>F</sup><sub>1</sub>以及得到纹理图像<i>D</i><sup>A</sup><sub>1</sub>、<i>D</i><sup>B</sup><sub>1</sub>、<i>D</i><sup>F</sup><sub>1</sub>。将1-<i>λ</i>作为纹理图像的权重。</p>
                </div>
                <div class="p1">
                    <p id="82">步骤2 对三幅纹理图像用相同系数<i>λ</i>继续分别将<i>D</i><sup>A</sup><sub>1</sub>、<i>D</i><sup>B</sup><sub>1</sub>、<i>D</i><sup>F</sup><sub>1</sub>分解,得到结构图像<i>S</i><sup>A</sup><sub>2</sub>、<i>S</i><sup>B</sup><sub>2</sub>、<i>S</i><sup>F</sup><sub>2 </sub>以及三幅纹理图像<i>D</i><sup>A</sup><sub>2</sub>、<i>D</i><sup>B</sup><sub>2</sub>、<i>D</i><sup>F</sup><sub>2</sub>。三幅二级结构图像<i>S</i><sup>A</sup><sub>2</sub>、<i>S</i><sup>B</sup><sub>2</sub>、<i>S</i><sup>F</sup><sub>2</sub>的权重系数为<i>λ</i>(1-<i>λ</i>),三幅二级纹理图像<i>D</i><sup>A</sup><sub>2</sub>、<i>D</i><sup>B</sup><sub>2</sub>、<i>D</i><sup>F</sup><sub>2</sub>的权重为(1-<i>λ</i>)<sup>2</sup>。此时,可看出(用源图像A举例):</p>
                </div>
                <div class="p1">
                    <p id="83"><i>D</i><sup>A</sup><sub>1</sub>=<i>D</i><sup>A</sup><sub>2</sub>+<i>S</i><sup>A</sup><sub>2</sub>      (2)</p>
                </div>
                <div class="p1">
                    <p id="84">所以可以得出:</p>
                </div>
                <div class="p1">
                    <p id="85">1-<i>λ</i>=<i>λ</i>(1-<i>λ</i>)+(1-<i>λ</i>)<sup>2</sup>      (3)</p>
                </div>
                <div class="p1">
                    <p id="86">步骤3 最后分别用边缘梯度指标<i>Q</i><sub><i>G</i></sub>指标计算<i>D</i><sup>A</sup><sub>2</sub>、<i>D</i><sup>B</sup><sub>2</sub>、<i>D</i><sup>F</sup><sub>2</sub>的融合分数<i>Score</i>3,接着算出<i>S</i><sup>A</sup><sub>2</sub>、<i>S</i><sup>B</sup><sub>2</sub>、<i>S</i><sup>F</sup><sub>2</sub>的融合得分<i>Score</i>2,最后计算一级分解的结构图像<i>S</i><sup>A</sup><sub>1</sub>、<i>S</i><sup>B</sup><sub>1</sub>、<i>S</i><sup>F</sup><sub>1</sub>的相似度<i>Score</i>1。</p>
                </div>
                <div class="p1">
                    <p id="87">此时,可得出(以图像A为例):</p>
                </div>
                <div class="p1">
                    <p id="88"><i>f</i><sub>A</sub>=<i>S</i><sup>A</sup><sub>1</sub>+<i>D</i><sup>A</sup><sub>2</sub>+<i>S</i><sup>A</sup><sub>2</sub>      (4)</p>
                </div>
                <div class="p1">
                    <p id="89">步骤4 计算出最终融合分数:</p>
                </div>
                <div class="p1">
                    <p id="90"><i>Score</i>=<i>Score</i>1*<i>λ</i>+[<i>Score</i>2*<i>λ</i>*(1-<i>λ</i>)+</p>
                </div>
                <div class="p1">
                    <p id="91"><i>Score</i>3*(1-<i>λ</i>)<sup>2</sup>]      (5)</p>
                </div>
                <h3 id="92" name="92" class="anchor-tag">2 实验结果与分析</h3>
                <div class="p1">
                    <p id="93">在本文中,将采用拉普拉斯金字塔融合(Laplacian Pyramid fusion, LP)<citation id="235" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、双树复小波变换融合(DualTree Complex Wavelet Transform fusion, DTCWT)<citation id="236" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、曲波变换融合(CurVelet Transform  fusion, CVT)<citation id="237" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、非下采样轮廓波变换融合(NonSubsampling Contour wave Transform fusion, NSCT)<citation id="238" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、对比度的图像融合(Directive Contrast fusion, DC)<citation id="239" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、导向滤波融合(Guided Filter Fusion, GFF)<citation id="240" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、脉冲耦合神经网络融合(Pulse Coupled Neural Network fusion, PCNN)<citation id="241" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、卷积神经网络融合(Convolutional Neural Network fusion, CNN)<citation id="242" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>等8种常用的融合方法进行评价实验。</p>
                </div>
                <div class="p1">
                    <p id="94">实验图像集涵盖3类图像融合类型:多聚焦图像融合、医学图像融合和红外可见光图像融合。每一类图像包含10组测试数据。这3种类型已经包括了图像融合在现实应用中的典型情况,从而有效验证融合算法的客观指标。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95">2.1 <b>实验设置</b></h4>
                <div class="p1">
                    <p id="96">步骤1 引入波达计数法<citation id="243" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>,其中选取了引言中提及的11种经典评价指标来做测试。11种融合指标排列成11行,8个融合算法排成8列,这样就组成了11行8列的矩阵。然后利用每种融合指标对8种融合算法进行评价,根据8种融合算法的分数得出排名。这样每一行就对应着8种融合算法排名。每一列就是每种算法在11个评价指标中的各自排名。最后在每一列上采用波达计数法来对每种融合算法根据融合指标排名作出综合的排名。采用的波达计数规则为:排名最高的权重为1,排名第二高的权重为1/2,依此类推,排名11位的权重为1/11。根据此规则对这8种融合算法得出最终的排名序列a。如表1所示,以3种算法、3个指标为例,这个例子里面:方法1最好,方法2次之,方法3最差。</p>
                </div>
                <div class="area_img" id="97">
                    <p class="img_tit"><b>表</b>1 <b>波达系数算法举例</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Examples of algorithm Borda coefficient</p>
                    <p class="img_note"></p>
                    <table id="97" border="1"><tr><td><br />方法</td><td>融合<br />指标1</td><td>融合<br />指标2</td><td>融合<br />指标3</td><td>本文Borda系数算法</td><td>Borda<br />排名</td></tr><tr><td>融合方法1</td><td>1</td><td>2</td><td>1</td><td>1×2+1×1/2=2.5</td><td>1</td></tr><tr><td><br />融合方法2</td><td>2</td><td>1</td><td>2</td><td>1×1+2×1/2=2</td><td>2</td></tr><tr><td><br />融合方法3</td><td>3</td><td>3</td><td>3</td><td>3×1/3=1</td><td>3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="98">步骤2 用1.3节中的算法分别对上述的8种融合方法进行计算融合分数,根据融合的分数高低,得到8种融合方法的排名序列b。例如,第一步中的方法1、2、3在此算法中,排名序列b是1、2、3。</p>
                </div>
                <div class="p1">
                    <p id="99">步骤3 使用肯德尔系数来测试排名序列a和排名序列b的相关性<citation id="244" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。例如:根据上两步例子中的排名序列a和排名序列b,得出肯德尔系数的结果为1。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100">2.2 <i>λ</i><b>参数选择</b></h4>
                <div class="p1">
                    <p id="101">在TV-L<sup>1</sup>结构纹理分解算法中,如何选择合适的参数<i>λ</i>是一个重要的问题。鉴于纹理信息包含着大量的结构信息,所以本文将系数<i>λ</i>设为 0.2、0.3、0.4开展实验,使得纹理特征权重始终大于结构特征的权重。对于多聚焦图像而言,人眼一般会更加专注于图像纹理,据此将多聚焦图像的<i>λ</i>调为0.2。对于医学图像来说,医生在关注图像纹理特征的同时,也会比较关注结构特征。例如,医生会依据结构特征来判断病灶的边界,以此来判断病灶是否侵入其他的器官,所以将医学图像的<i>λ</i>设计为0.4。对于红外可见光图像而言,把系数设置为0.2。如军人会缜密地分析融合图像每个位置的细节,从而作出对应的行动方案。每类图像中挑选一幅来显示分解结果。由于篇幅有限,只显示融合图像、融合图像一级分解结构图像以及二级分解纹理图像。</p>
                </div>
                <div class="p1">
                    <p id="102">从图4的多聚焦图像和红外可见光图像可以看出:<i>λ</i>=0.2时分解的图像中纹理比较清晰,结构相对模糊。多聚焦图像中的字迹依然比较清晰以及红外可见光图像中的森林和草地的细节也比较清晰。而对于医学图像,<i>λ</i>=0.4分解得出的结构特征是相对明显的,可以清晰地观察到相关结构的边缘。</p>
                </div>
                <div class="p1">
                    <p id="103">据此,将分解医学图像的系数设置为0.4,将红外可见光图像和多聚焦图像的分解系数设置为0.2进行实验测试。然后对3类图像用相对应的系数分解,将分解的图像按照1.3节的算法计算出8种融合分数,得出2.1节的排名序列b。再用波达计数法得出排名系列a,最后利用肯德尔算法计算两种数列的肯德尔系数。最后的结果如图5所示,横坐标表示数据集序号,纵坐标表示肯德尔系数。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909037_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 一组不同类型图像且不同参数分解示例" src="Detail/GetImg?filename=images/JSJY201909037_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 一组不同类型图像且不同参数分解示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909037_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Different types of images and decomposition with different parameters</p>

                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909037_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 肯德尔系数结果" src="Detail/GetImg?filename=images/JSJY201909037_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 肯德尔系数结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909037_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Kendall coefficient results</p>

                </div>
                <div class="p1">
                    <p id="106">根据肯德尔系数的定义,如果实验的相关系数值在50%以上,就表明测试的两个变量线性相关性较好。从图5中可看出,实验所测试的3类图像实验的肯德尔系数都在50%以上,其中3类图片的肯德尔系数大部分达到了70%左右。对于红外可见光图像而言,系数甚至达到了80%以上,而医学图像的肯德尔系数达到了90%以上。所以客观实验结果验证了本文的融合指标与其他主流的客观指标有比较高的关联度,从而证明了此评价算法具有较高的可靠性。</p>
                </div>
                <div class="p1">
                    <p id="107">但是从图5也可以看出,对于同一类图像的肯德尔系数大小差距有点大,例如医学图像的肯德尔系数在50%～100%,红外可见光的图像在50%～90%。肯德尔系数之所以有较大的差别,是由于不同图像包含的纹理信息以及结构信息的内容不一样导致的。例如:以在红外可见光图像总体表现最好的CNN算法为来说明肯德尔系数的表现,如图6所示,每行是一组融合原图像及其一级结构和纹理分解图像。</p>
                </div>
                <div class="p1">
                    <p id="108">因为将红外可见光图像分解的系数设为0.2,对应的分解图像为结构信息较少,而纹理信息较多。图6(b)的第一行结构图像中依然有大量的树叶边缘,说明结构信息较多;图6 中第二行显然是含有结构特征较多、纹理特征较少的图像。所以这两行图像的肯德尔系数是较低的;而对于图6(c)的第三行和第四行可以观察到,图像中树枝和草地占据了图像的大部分区域,说明纹理特征是较多的,含有的结构特征是比较少的,所以这两幅图像的肯德尔系数表现得很好。医学图像和多聚焦图像的肯德尔系数也有类似的表现。</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909037_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 一组红外可见光图像0.2系数分解图示例" src="Detail/GetImg?filename=images/JSJY201909037_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 一组红外可见光图像0.2系数分解图示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909037_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Decomposed infrared visible light image with 0.2 coefficient</p>

                </div>
                <h4 class="anchor-tag" id="111" name="111">2.3 <b>对比主观评价</b></h4>
                <div class="p1">
                    <p id="112">为了进一步验证本文算法的正确性,对每类图像中的3幅图片作了小范围的主观评价。按照国际标准ITU-R的规定,邀请了20位同学来作主观评价(其中包括10位方向为图像质量评价的同学,10位研究方向为其他方向的同学,其中包括4位为江南大学医学院的同学主要评价医学图像)。评价前对研究其他方向同学讲清楚主观评价目的、实验过程、评价指标的定义等。最后每位在规定的相同时间,且在相同环境下作出相应的投票,表2是图4中3类图像(多聚焦图像为子图1、医学图像为子图2、红外可见光图像为子图3)的主观评价与本文的实验排名结果。</p>
                </div>
                <div class="area_img" id="113">
                                            <p class="img_tit">
                                                <b>表</b>2 <b>主观评价的对比</b>
                                                    <br />
                                                Tab. 2 Comparison of subjective evaluation
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909037_11300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201909037_11300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909037_11300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 主观评价的对比" src="Detail/GetImg?filename=images/JSJY201909037_11300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="114">由于篇幅有限,将每幅融合图像分数最高的和得分最低的以及与图像2中与主观排名不一致的图像用图7展示:图7中(a)和(b)是多聚焦图像表现最好和最差的算法图像;(c)和(d)属于红外可见光图像效果最好和最差算法图像;(e)和(f)图是医学图像表现最好与最坏的图像;(g)与(h)是医学图像与主观评价不一致的两幅图像。</p>
                </div>
                <div class="p1">
                    <p id="115">从表2可以看出本文评价指标和主观评价指标是非常吻合的:从图7中观察到得分最高的融合方法比最低的融合方法效果明显要好。虽然图像2中DTCWT算法和CVT算法主观排名与本文指标不一致,但是从图7中可以看到这两种融合图像丢失信息的程度差不多,难以判断哪个较好。其他的对比的几组图像与这3组图像的效果类似,据此说明本文的评价指标可以比较客观地评价各种融合效果。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909037_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 部分图像的融合结果比较" src="Detail/GetImg?filename=images/JSJY201909037_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 部分图像的融合结果比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909037_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Comparison of partial image fusion results</p>

                </div>
                <h3 id="117" name="117" class="anchor-tag">3 结语</h3>
                <div class="p1">
                    <p id="118">针对传统图像融合评价评价未考虑图像结构和纹理内在的区别的问题,本文提出了一种基于TV-L<sup>1</sup>结构和纹理分解的客观评价指标。该算法利用人们对图像质量感知主要来自于图像底层视觉特征的特点,并且依据TV-L<sup>1</sup>中分解系数的作用,将图像二级分解为结构图像和纹理图像,在结构图像和纹理图像上根据结构相似性分别设计融合评价指标,最后将各级的分解图像得出的相似性综合起来得出最终的相似性。结合已有11种客观指标和8种图像融合算法,基于波达计数法和肯德尔系数进行了30组图像的客观评价实验,实验表明本文所提融合评价算法能够比较客观地评价图像融合的质量。但本文只是将图像进行了二级分解,未将图像进行更加多级的分解,下一步将进行更加多级的分解实验,来观察是否随着分解级数的增加,算法的可靠性更高。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="155">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Objective quality assessment for multiexposure multifocus image fusion">

                                <b>[1]</b> HASSEN R,WANG Z,SALAMA M M A.Objective quality assessment for multiexposure multi-focus image fusion [J].IEEE Transactions on Image Processing,2015,24(9):2712-2724.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Objective Criterion for Performance Evaluation of Image Fusion Techniques">

                                <b>[2]</b> MALVIYA A,BHIRUD S G.Objective criterion for performance evaluation of image fusion techniques [J].International Journal of Computer Applications,2010,1(25):57-60.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329191&amp;v=MDI1NDJNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSUZ3VGFSVT1OaWZPZmJLN0h0RE5ySTlGWitrR0RYVTRvQg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> PETROVIC V.Subjective tests for image fusion evaluation and objective metric validation [J].Information Fusion,2007,8(2):208-216.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201402014&amp;v=MTY5NTVxcUJ0R0ZyQ1VSN3FmWnVac0Z5am5WTHpQS0NMZlliRzRIOVhNclk5RVlJUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 张小利,李雄飞,李军.融合图像质量评价指标的相关性分析及性能评估[J].自动化学报,2014,40(2):306-315.(ZHANG X L,LI X F,LI J.Validation and correlation analysis of metrics for evaluating performance of image fusion [J].Acta Automatica Sinica,2014,40(2):306-315.)
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multimodal medical image fusion based on Yager&amp;#39;&amp;#39;s intuitionistic fuzzy sets">

                                <b>[5]</b> TIRUPAL T,MOHAN B C,KUMAR S S.Multimodal medical image fusion based on Yager's intuitionistic fuzzy sets [J].Iranian Journal of Fuzzy Systems,2019,16(1):33-48.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Edge Strength Similarity for Image Quality Assessment">

                                <b>[6]</b> ZHANG X,FENG X,WANG W.et al.Edge strength similarity for image quality assessment [J].IEEE Signal Processing Letters,2013,20(4):319-322.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130122018163&amp;v=MDc3MzVuVTcvS0oxMFROajdCYXJLN0h0RE9yWTlFYk9vSkR4TTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2Rnlu&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> SHAH P,MERCHANT S N,DESAI U B.Multifocus and multispectral image fusion based on pixel significance using multiresolution decomposition [J].Signal,Image and Video Processing,2013,7(1):95-109.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Comments on &amp;#39;Information measure for performance of image fusion&amp;#39;">

                                <b>[8]</b> HOSSNY M,NAHAVANDI S,CREIGHTON D.Comments on ‘Information measure for performance of image fusion’ [J].Electronics Letters,2008,44(18):1066-1067.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Objective Assessment of Multiresolution Image Fusion Algorithms for Context Enhancement in Night Vision: A Comparative Study">

                                <b>[9]</b> LIU Z,BLASCH E,XUE Z,et al.Objective assessment of multiresolution image fusion algorithms for context enhancement in night vision:a comparative study [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(1):94-109.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image fusion metric based on mutual information and Tsallis entropy">

                                <b>[10]</b> CVEJIC N,CANAGARAJAH C N,BULL D R.Image fusion metric based on mutual information and tsallis entropy [J].Electronics Letters,2006,42(11):626-627.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image quality assessment: from error visibility to structural similarity">

                                <b>[11]</b> WANG Z,BOVIK A C,SHEIKH H R.et al.Image quality assessment:from error visibility to structural similarity [J].IEEE Transactions on Image Processing,2004,13(4):600-612.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A new quality metric for imagefusion">

                                <b>[12]</b> PIELLA G,HEIJMANS H.A new quality metric for image fusion [C]// Proceedings of the 2003 International Conference on Image Processing.Piscataway,NJ:IEEE,2003:173-176.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structural similarity metrics for quality image fusion assessment:algorithms">

                                <b>[13]</b> PISTONESI S,MARTINEZ J,OJEDA S M,et al.Structural similarity metrics for quality image fusion assessment:algorithms [J].Image Processing on Line,2018,8:345-368.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 CHEN H,VARSHNEY P K.A human perception inspired quality metric for image fusion based on regional information [J].Information Fusion,2007,8(2):193-207.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel similarity based quality metric for image fusion">

                                <b>[15]</b> LI S,HONG R,WU X,et al.A noval similarity based quality metric for image fusion [C]// Proceeding of the 2008 International Conference on Audio,Language and Image Processing.Piscataway,NJ:IEEE,2008:167-172.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329187&amp;v=MDM1OTh3VGFSVT1OaWZPZmJLN0h0RE5ySTlGWitrR0RYUStvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJRg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> CHEN H,VARSHENEY P K.A human perception inspired quality metric for image fusion based on regional information [J].Image Fusion,2007,8(2):193-207.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349038&amp;v=MTUzMjFIOHhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJRndUYVJVPU5pZk9mYks3SHRET3JZOUVaKzhHRA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> CHEN Y.BLUM R S.A new automated quality assessment algorithm for image fusion [J].Image and Vision Computing,2009,27(10):1421-1432.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201105079&amp;v=MjQ2OTR6cXFCdEdGckNVUjdxZlp1WnNGeWpuVkx6UEx6N0JiYkc0SDlETXFvOUNiWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 程刚,王春恒.基于结构和纹理特征融合的场景图像分类[J].计算机工程,2011,37(5):227-229.(CHENG G,WANG C H.Scene image categorization based on structure and texture feature fusion [J].Computer Engineering,2011,37(5):227-229.)
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Decomposition and extraction:a new framework for visual classification">

                                <b>[19]</b> FANG Y,CHEN Q,SUN L,et al.Decomposition and extraction:a new framework for visual classification [J].IEEE Transactions on Image Processing,2014,23(8):3412-3427.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-sensor image fusion algorithm based on multiresolution analysis">

                                <b>[20]</b> WANG Z,WANG W,SU B.Multi-sensor image fusion algorithm based on multiresolution analysis [J].International Journal of Online Engineering,2018,14(6):44-57.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=What does visual agnosia tell us about perceptual organization and its relationship to object perception?">

                                <b>[21]</b> BEHRMANN M,KIMCHI R.What does visual agnosia tell us about perceptual organization and its relationship to object perception?[J].Journal of Experimental Psychology:Human Perception and Performance,2003,29(1):19-42.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGHW200904029&amp;v=MTQwMzdMelBMeXJEZWJHNEh0ak1xNDlIYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqblY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> 陈浩,王延杰.基于拉普拉斯金字塔变换的图像融合算法研究[J].激光与红外,2009,39(4):439-442.(CHEN H,WANG Y J.Research on image fusion algorithm based on Laplacian pyramid transform [J].Laser and Infrared,2009,39(4):439-442.)
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700451440&amp;v=MTU0OTFsVXIzSUlGd1RhUlU9TmlmT2ZiSzhIOURNcUk5RllPNE9DSGc1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> LIU Y,LIU S.WANG Z.A general framework for image fusion based on multi-scale transform and sparse representation [J].Information Fusion,2015,24:147-164.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Directive contrast based multimodal medical image fusion in NSCT domain">

                                <b>[24]</b> BHATNAGAR G,WU Q M J,LIU Z.Directive contrast based multimodal medical image fusion in NSCT domain [J].IEEE Transactions on Multimedia,2013,15(5):1014-1024.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image fusion with guided filtering">

                                <b>[25]</b> LI S,KANG X,HU J.Image fusion with guided filtering [J].IEEE Transactions on Image Processing,2013,22(7):2864-2875.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO200812010&amp;v=MjA2Nzh0R0ZyQ1VSN3FmWnVac0Z5am5WTHpQS0NMZlliRzRIdG5Oclk5RVpJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> 屈小波,闫敬文,肖弘智,等.非降采样Contourlet域内空间频率激励的PCNN图像融合算法[J].自动化学报,2008,34(12):1508-1514.(QU X B,YAN J W,XIAO H Z,et al.Image fusion algorithm based on spatial frequency-motivated pulse coupled neural networks in nonsubsampled Contourlet transform domain [J].Acta Automatica Sinica,2008,34(12):1508-1514.)
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFB5835143E660843FA8E042889D49001&amp;v=MDk0ODNLRzluUHFvNUJaNTRKQ253eHl4VmxtemNJU0h2Z3BCbzhEYmFkUmJxZUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhMcTR3Njg9TmlmT2ZjWA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> LIU Y,CHEN X,WANG Z,et al.Deep learning for pixel-level image fusion recent:advances and future prospects [J].Information Fusion,2018,42:158-173.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201909037" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909037&amp;v=MDQxMDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqblZMelBMejdCZDdHNEg5ak1wbzlHWTRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
