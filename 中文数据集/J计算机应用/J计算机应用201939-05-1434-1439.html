<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136767478252500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201905034%26RESULT%3d1%26SIGN%3deDlMxDQhYPyoxtJWjz78%252fNAB6dE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905034&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905034&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905034&amp;v=MTA4NzJxbzlHWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVMektMejdCZDdHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#69" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#75" data-title="1 三维重定向图像质量评价数据库 ">1 三维重定向图像质量评价数据库</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="1.1 &lt;b&gt;原始图像&lt;/b&gt;">1.1 <b>原始图像</b></a></li>
                                                <li><a href="#80" data-title="1.2 &lt;b&gt;重定向方法&lt;/b&gt;">1.2 <b>重定向方法</b></a></li>
                                                <li><a href="#93" data-title="1.3 &lt;b&gt;主观评价&lt;/b&gt;">1.3 <b>主观评价</b></a></li>
                                                <li><a href="#95" data-title="1.4 &lt;b&gt;主观分数处理&lt;/b&gt;">1.4 <b>主观分数处理</b></a></li>
                                                <li><a href="#104" data-title="1.5 &lt;b&gt;主观质量分析&lt;/b&gt;">1.5 <b>主观质量分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#115" data-title="2 三维重定向图像客观质量评价方法 ">2 三维重定向图像客观质量评价方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#118" data-title="2.1 &lt;b&gt;深度感&lt;/b&gt;">2.1 <b>深度感</b></a></li>
                                                <li><a href="#128" data-title="2.2 &lt;b&gt;舒适度&lt;/b&gt;">2.2 <b>舒适度</b></a></li>
                                                <li><a href="#137" data-title="2.3 &lt;b&gt;图像质量&lt;/b&gt;">2.3 <b>图像质量</b></a></li>
                                                <li><a href="#143" data-title="2.4 &lt;b&gt;特征融合&lt;/b&gt;">2.4 <b>特征融合</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#146" data-title="3 实验结果及分析 ">3 实验结果及分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#150" data-title="3.1 &lt;b&gt;总体性能分析&lt;/b&gt;">3.1 <b>总体性能分析</b></a></li>
                                                <li><a href="#154" data-title="3.2 &lt;b&gt;特征性能分析&lt;/b&gt;">3.2 <b>特征性能分析</b></a></li>
                                                <li><a href="#158" data-title="3.3 &lt;b&gt;回归方式性能分析&lt;/b&gt;">3.3 <b>回归方式性能分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#161" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#79" data-title="图1 原始S3D图像的左图像">图1 原始S3D图像的左图像</a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;八种重定向方法性能分析&lt;/b&gt;"><b>表</b>1 <b>八种重定向方法性能分析</b></a></li>
                                                <li><a href="#106" data-title="图2 MOS直方图">图2 MOS直方图</a></li>
                                                <li><a href="#109" data-title="图3 三维重定向结果示例">图3 三维重定向结果示例</a></li>
                                                <li><a href="#112" data-title="图4 不同三维重定向方法的平均MOS值">图4 不同三维重定向方法的平均MOS值</a></li>
                                                <li><a href="#117" data-title="图5 S3D重定向图像质量评价整体框图">图5 S3D重定向图像质量评价整体框图</a></li>
                                                <li><a href="#130" data-title="图6 S3D图像视觉重要性检测">图6 S3D图像视觉重要性检测</a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同评价方法总体性能比较&lt;/b&gt;"><b>表</b>2 <b>不同评价方法总体性能比较</b></a></li>
                                                <li><a href="#156" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;特征性能分析&lt;/b&gt;"><b>表</b>3 <b>特征性能分析</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;表&lt;/b&gt;4 DP、VC&lt;b&gt;和&lt;/b&gt;IQ&lt;b&gt;的性能分析&lt;/b&gt;"><b>表</b>4 DP、VC<b>和</b>IQ<b>的性能分析</b></a></li>
                                                <li><a href="#160" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;不同回归方式性能分析&lt;/b&gt;"><b>表</b>5 <b>不同回归方式性能分析</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="199">


                                    <a id="bibliography_1" title=" LUO S J, SUN Y T, SHEN I C, et al.Geometrically consistent stereoscopic image editing using patch-based synthesis[J].IEEE Transactions on Visualization and Computer Graphics, 2015, 21 (1) :56-67." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Geometrically Consistent Stereoscopic Image Editing Using Patch-Based Synthesis">
                                        <b>[1]</b>
                                         LUO S J, SUN Y T, SHEN I C, et al.Geometrically consistent stereoscopic image editing using patch-based synthesis[J].IEEE Transactions on Visualization and Computer Graphics, 2015, 21 (1) :56-67.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_2" title=" SHAMIR A, SHAMIR A, AVIDAN S.Improved seam carving for video retargeting[J].ACM Transactions on Graphics, 2008, 27 (3) :Article No.16." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098584&amp;v=MDgzNDJud1plWnRGaW5sVXIzSUtGd1hhUkE9TmlmSVk3SzdIdGpOcjQ5RlpPSUhDWFE5b0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         SHAMIR A, SHAMIR A, AVIDAN S.Improved seam carving for video retargeting[J].ACM Transactions on Graphics, 2008, 27 (3) :Article No.16.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_3" title=" WOLF L, GUTTMANN M, COHEN-OR D.Non-homogeneous content-driven video-retargeting[C]// Proceedings of the 2007 International Conference on Computer Vision.Piscataway, NJ:IEEE, 2007:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Non-homogeneous content-driven video-retargeting">
                                        <b>[3]</b>
                                         WOLF L, GUTTMANN M, COHEN-OR D.Non-homogeneous content-driven video-retargeting[C]// Proceedings of the 2007 International Conference on Computer Vision.Piscataway, NJ:IEEE, 2007:1-6.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_4" title=" LEE K, MOORTHY A K, LEE S, et al.S3D visual activity assessment based on natural scene statistics[J].IEEE Transactions on Image Processing, 2014, 23 (1) :450-465." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D Visual ActivityAssessment Based on Natural Scene Statistics">
                                        <b>[4]</b>
                                         LEE K, MOORTHY A K, LEE S, et al.S3D visual activity assessment based on natural scene statistics[J].IEEE Transactions on Image Processing, 2014, 23 (1) :450-465.
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_5" title=" KOOI F L, TOET A.Visual comfort of binocular and S3D displays[J].Displays, 2004, 25 (2/3) :99-108." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600010009&amp;v=MTQzNjVPb1BESHd3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0Z3WGFSQT1OaWZPZmJLN0h0RE5xWTlGWg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         KOOI F L, TOET A.Visual comfort of binocular and S3D displays[J].Displays, 2004, 25 (2/3) :99-108.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_6" title=" BASHA DEKEL T, MOSES Y, AVIDAN S.Stereo seam carving a geometrically consistent approach[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (10) :2513-2525." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stereo seam carving a geometrically consistent approach">
                                        <b>[6]</b>
                                         BASHA DEKEL T, MOSES Y, AVIDAN S.Stereo seam carving a geometrically consistent approach[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (10) :2513-2525.
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_7" title=" SHAO F, LIN W C, LIN W S, et al.Stereoscopic visual attention guided seam carving for stereoscopic image retargeting[J].Journal of Display Technology, 2015, 12 (1) :22-30." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stereoscopic visual attention guided seam carving for stereoscopic image retargeting">
                                        <b>[7]</b>
                                         SHAO F, LIN W C, LIN W S, et al.Stereoscopic visual attention guided seam carving for stereoscopic image retargeting[J].Journal of Display Technology, 2015, 12 (1) :22-30.
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_8" title=" LEI J J, WU M, ZHANG C Q, et al.Depth-preserving stereo image retargeting based on pixel fusion[J].IEEE Transactions on Multimedia, 2017, 79 (7) :1442-1453." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Depth-preserving stereo image retargeting based on pixel fusion">
                                        <b>[8]</b>
                                         LEI J J, WU M, ZHANG C Q, et al.Depth-preserving stereo image retargeting based on pixel fusion[J].IEEE Transactions on Multimedia, 2017, 79 (7) :1442-1453.
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_9" title=" CHANG C H, LIANG C K, CHUANG Y Y.Content-aware display adaptation and editing for stereoscopic images[J].IEEE Transactions on Multimedia, 2011, 13 (4) :589-601." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Content-aware display adaptation and interactive editing for stereoscopic images">
                                        <b>[9]</b>
                                         CHANG C H, LIANG C K, CHUANG Y Y.Content-aware display adaptation and editing for stereoscopic images[J].IEEE Transactions on Multimedia, 2011, 13 (4) :589-601.
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_10" title=" LIN S S, LIN C H, CHANG S H, et al.Object-coherence warping for stereoscopic image retargeting[J].IEEE Transactions on Circuits and Systems for Video Technology, 2014, 24 (5) :759-768." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object-coherence warping for stereoscopic image retargeting">
                                        <b>[10]</b>
                                         LIN S S, LIN C H, CHANG S H, et al.Object-coherence warping for stereoscopic image retargeting[J].IEEE Transactions on Circuits and Systems for Video Technology, 2014, 24 (5) :759-768.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_11" title=" LI B, DUAN L Y, LIN C W, et al.Depth-preserving warping for stereo image retargeting[J].IEEE Transactions on Image Processing, 2015, 24 (9) :2811-2826." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Depth-preserving warping for stereo image retargeting">
                                        <b>[11]</b>
                                         LI B, DUAN L Y, LIN C W, et al.Depth-preserving warping for stereo image retargeting[J].IEEE Transactions on Image Processing, 2015, 24 (9) :2811-2826.
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_12" title=" SHAO F, LIN W C, LIN W S, et al.QoE-guided warping for stereoscopic image retargeting[J].IEEE Transactions on Image Processing, 2017, 26 (10) :790-4805." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Qo E-guided warping for stereoscopic image retargeting">
                                        <b>[12]</b>
                                         SHAO F, LIN W C, LIN W S, et al.QoE-guided warping for stereoscopic image retargeting[J].IEEE Transactions on Image Processing, 2017, 26 (10) :790-4805.
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_13" title=" LIN Y H, WU J L.Quality assessment of stereoscopic S3D image compression by binocular integration behaviors[J].IEEE Transactions on Image Processing, 2014, 23 (4) :1527-1542." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Quality assessment of stereoscopic3D image compression by binocular integrationbehaviors">
                                        <b>[13]</b>
                                         LIN Y H, WU J L.Quality assessment of stereoscopic S3D image compression by binocular integration behaviors[J].IEEE Transactions on Image Processing, 2014, 23 (4) :1527-1542.
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_14" title=" SHAO F, LI K M, LIN W S, et al.Full-reference quality assessment of stereoscopic images by learning binocular receptive field properties[J].IEEE Transactions on Image Processing, 2015, 24 (10) :2971-2983." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Full-reference quality assessment of stereoscopic images by learning binocular receptive field properties">
                                        <b>[14]</b>
                                         SHAO F, LI K M, LIN W S, et al.Full-reference quality assessment of stereoscopic images by learning binocular receptive field properties[J].IEEE Transactions on Image Processing, 2015, 24 (10) :2971-2983.
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_15" title=" WANG J H, REHMAN A, ZENG K, et al.Quality prediction of asymmetrically distorted stereoscopic S3D images[J].IEEE Transactions on Image Processing, 2015, 24 (11) :3400-3414." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Quality prediction of asymmetrically distorted stereoscopic 3D images">
                                        <b>[15]</b>
                                         WANG J H, REHMAN A, ZENG K, et al.Quality prediction of asymmetrically distorted stereoscopic S3D images[J].IEEE Transactions on Image Processing, 2015, 24 (11) :3400-3414.
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_16" title=" FANG Y M, ZENG K, WANG Z, et al.Objective quality assessment for image retargeting based on structural similarity[J].IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 2014, 4 (1) :95-105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Objective Qualit Assessment for Image Retargeting Based on Structural Simi larity">
                                        <b>[16]</b>
                                         FANG Y M, ZENG K, WANG Z, et al.Objective quality assessment for image retargeting based on structural similarity[J].IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 2014, 4 (1) :95-105.
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_17" title=" HSU C C, LIN C W, FANG Y M, et al.Objective quality assessment for image retargeting based on perceptual geometric distortion and information loss[J].IEEE Journal of Selected Topics in Signal Processing, 2014, 8 (3) :377-389." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Objective quality assessment for image retargeting based on perceptual geometric distortion and information loss">
                                        <b>[17]</b>
                                         HSU C C, LIN C W, FANG Y M, et al.Objective quality assessment for image retargeting based on perceptual geometric distortion and information loss[J].IEEE Journal of Selected Topics in Signal Processing, 2014, 8 (3) :377-389.
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_18" title=" ZHANG Y B, FANG Y M, LIN W S, et al.Backward registration based aspect ratio similarity for image retargeting quality assessment[J].IEEE Transactions on Image Processing, 2016, 25 (9) :4286-4297." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Backward registrationbased aspect ratio similarity for image retargeting quality assessment">
                                        <b>[18]</b>
                                         ZHANG Y B, FANG Y M, LIN W S, et al.Backward registration based aspect ratio similarity for image retargeting quality assessment[J].IEEE Transactions on Image Processing, 2016, 25 (9) :4286-4297.
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_19" title=" LIANG Y, LIU Y J, GUTIERREZ D.Objective quality prediction of image retargeting algorithms[J].IEEE Transactions on Visualization and Computer Graphics, 2017, 23 (2) :1099-1110." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Objective quality prediction of image retargeting algorithms">
                                        <b>[19]</b>
                                         LIANG Y, LIU Y J, GUTIERREZ D.Objective quality prediction of image retargeting algorithms[J].IEEE Transactions on Visualization and Computer Graphics, 2017, 23 (2) :1099-1110.
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_20" title=" JIANG Q P, SHAO F, LIN W S, et al.Learning sparse representation for objective image retargeting quality assessment[J].IEEE Transactions on Cybernetics, 2018, 48 (4) :1276-1289." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning sparse representation for objective image retargeting quality assessment">
                                        <b>[20]</b>
                                         JIANG Q P, SHAO F, LIN W S, et al.Learning sparse representation for objective image retargeting quality assessment[J].IEEE Transactions on Cybernetics, 2018, 48 (4) :1276-1289.
                                    </a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_21" title=" CHEN Z B, LIN J X, NING L, et al.Full reference quality assessment for image retargeting based on natural scene statistics modeling and bi-directional saliency similarity[J].IEEE Transactions on Image Processing, 2017, 26 (11) :5138-5148." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Full reference quality assessment for image retargeting based on natural scene statistics modeling and bi-directional saliency similarity">
                                        <b>[21]</b>
                                         CHEN Z B, LIN J X, NING L, et al.Full reference quality assessment for image retargeting based on natural scene statistics modeling and bi-directional saliency similarity[J].IEEE Transactions on Image Processing, 2017, 26 (11) :5138-5148.
                                    </a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_22" title=" ZHANG Y C, NGAN K N, MA L, et al.Objective quality assessment of image retargeting by incorporating fidelity measures and inconsistency detection[J].IEEE Transactions on Image Processing, 2017, 26 (12) :5980-5993." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Objective quality assessment of image retargeting by incorporating fidelity measures and inconsistency detection">
                                        <b>[22]</b>
                                         ZHANG Y C, NGAN K N, MA L, et al.Objective quality assessment of image retargeting by incorporating fidelity measures and inconsistency detection[J].IEEE Transactions on Image Processing, 2017, 26 (12) :5980-5993.
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_23" title=" GUO Y C, HAO Y T, YU M.Image retargeting quality assessment based on content deformation measurement[J].Signal Processing:Image Communication, 2018, 67:171-181." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESAA90B2CD79D1A23DDAD375FBD795FF5F&amp;v=MTcxMDR6THE4dzZvPU5pZk9mY0xKRjlHK3Jmd3hZK0o3RFEwN3pHSm5tMHQrVDNxVTNtWXljTGZpTTcvcENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         GUO Y C, HAO Y T, YU M.Image retargeting quality assessment based on content deformation measurement[J].Signal Processing:Image Communication, 2018, 67:171-181.
                                    </a>
                                </li>
                                <li id="245">


                                    <a id="bibliography_24" title=" WANG W G, SHEN J B, YU Y Z, et al.Stereoscopic thumbnail creation via efficient stereo saliency detection[J].IEEE Transactions on Visualization and Computer Graphics, 2017, 23 (8) :2014-2027." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stereoscopic thumbnail creation via efficient stereo saliency detection">
                                        <b>[24]</b>
                                         WANG W G, SHEN J B, YU Y Z, et al.Stereoscopic thumbnail creation via efficient stereo saliency detection[J].IEEE Transactions on Visualization and Computer Graphics, 2017, 23 (8) :2014-2027.
                                    </a>
                                </li>
                                <li id="247">


                                    <a id="bibliography_25" title=" WANG Y S, TAI C L, SORKINE O, et al.Optimized scale-and-stretch for image resizing [J].ACM Transactions on Graphics, 2008, 27 (5) :118." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098682&amp;v=MTI4MTgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUtGd1hhUkE9TmlmSVk3SzdIdGpOcjQ5RlpPSUhDblE3b0JNVDZUNFBRSA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         WANG Y S, TAI C L, SORKINE O, et al.Optimized scale-and-stretch for image resizing [J].ACM Transactions on Graphics, 2008, 27 (5) :118.
                                    </a>
                                </li>
                                <li id="249">


                                    <a id="bibliography_26" title=" SESHADRINATHAN K, SOUNDARARAJAN R, BOVIK A C, et al.Study of subjective and objective quality assessment of video[J].IEEE Transactions on Image Process.2010, 19 (6) :1427-1441." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Study of Subjective and Objective Quality Assessment of Video">
                                        <b>[26]</b>
                                         SESHADRINATHAN K, SOUNDARARAJAN R, BOVIK A C, et al.Study of subjective and objective quality assessment of video[J].IEEE Transactions on Image Process.2010, 19 (6) :1427-1441.
                                    </a>
                                </li>
                                <li id="251">


                                    <a id="bibliography_27" title=" SUN D, ROTH S, BLACK M J.Secrets of optical flow estimation and their principles[C]// Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2010:2432-2439." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Secrets of optical flow estimation and their principles">
                                        <b>[27]</b>
                                         SUN D, ROTH S, BLACK M J.Secrets of optical flow estimation and their principles[C]// Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2010:2432-2439.
                                    </a>
                                </li>
                                <li id="253">


                                    <a id="bibliography_28" title=" JUNG Y J, SOHN H, LEE S I, et al.Predicting visual discomfort of stereoscopic images using human attention model[J].IEEE Transactions on Circuits and Systems for Video Technology, 2013, 23 (12) :2077-2082." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Predicting visual discomfort of stereoscopic images using human attention model">
                                        <b>[28]</b>
                                         JUNG Y J, SOHN H, LEE S I, et al.Predicting visual discomfort of stereoscopic images using human attention model[J].IEEE Transactions on Circuits and Systems for Video Technology, 2013, 23 (12) :2077-2082.
                                    </a>
                                </li>
                                <li id="255">


                                    <a id="bibliography_29" title=" HAREL J, KOCH C, PERONA P.Graph-based visual saliency[C]// Proceedings of the 19th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2006:545-552." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Graph-based visual saliency">
                                        <b>[29]</b>
                                         HAREL J, KOCH C, PERONA P.Graph-based visual saliency[C]// Proceedings of the 19th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2006:545-552.
                                    </a>
                                </li>
                                <li id="257">


                                    <a id="bibliography_30" title=" LIU C, YUEN J, TORRALBA A.SIFT flow:dense correspondence across scenes and its applications[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) :978-994." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SIFT flow: Dense correspondence across scenes and its applications">
                                        <b>[30]</b>
                                         LIU C, YUEN J, TORRALBA A.SIFT flow:dense correspondence across scenes and its applications[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) :978-994.
                                    </a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_31" title=" SIMAKOV D, CASPI Y, SHECHTMAN E, et al.Summarizing visual data using bidirectional similarity[C]// Proceedings of the 2008 Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2008:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Summarizing visual data using bidirectional similarity">
                                        <b>[31]</b>
                                         SIMAKOV D, CASPI Y, SHECHTMAN E, et al.Summarizing visual data using bidirectional similarity[C]// Proceedings of the 2008 Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2008:1-8.
                                    </a>
                                </li>
                                <li id="261">


                                    <a id="bibliography_32" title=" PELE O, WERMAN M.Fast and robust earth mover&#39;s distances[C]// Proceedings of the 2009 International Conference on Computer Vision.Piscataway, NJ:IEEE, 2009:460-467." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast and robust earth movers distances">
                                        <b>[32]</b>
                                         PELE O, WERMAN M.Fast and robust earth mover&#39;s distances[C]// Proceedings of the 2009 International Conference on Computer Vision.Piscataway, NJ:IEEE, 2009:460-467.
                                    </a>
                                </li>
                                <li id="263">


                                    <a id="bibliography_33" title=" SHEIKH H R, SABIR M F, BOVIK A C.A statistical evaluation of recent full reference image quality assessment algorithms[J].IEEE Transactions on Image Processing, 2006, 15 (11) :3440-3451." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Statistical Evaluation of Recent Full Reference Image Quality Assessment Algorithms">
                                        <b>[33]</b>
                                         SHEIKH H R, SABIR M F, BOVIK A C.A statistical evaluation of recent full reference image quality assessment algorithms[J].IEEE Transactions on Image Processing, 2006, 15 (11) :3440-3451.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-21 09:33</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(05),1434-1439 DOI:10.11772/j.issn.1001-9081.2018102054            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>三维重定向图像主观和客观质量评价方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AF%8C%E6%8C%AF%E5%A5%87&amp;code=38592155&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">富振奇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B5%E6%9E%AB&amp;code=22467651&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邵枫</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%AE%81%E6%B3%A2%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0160135&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">宁波大学信息科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>三维 (S3D) 图像重定向技术的作用是调整S3D图像的宽高比。为准确和客观地衡量三维重定向图像的视觉质量, 建立了一个S3D重定向图像质量评价数据库。首先, 使用八种具有代表性的三维重定向算法对45幅原始图像按两种重定向尺度进行分辨率调整, 共生成720幅三维重定向图像;然后, 每幅重定向图像通过主观测试, 得到相应的主观打分值;最后, 对主观分数进行处理, 得到平均主观意见分 (MOS) 值。在此基础上, 提出一种三维重定向图像客观质量评价方法, 即通过提取S3D重定向图像的深度感特征、视觉舒适度特征和左右视点的图像质量特征, 使用支持向量回归预测得到S3D重定向图像的视觉质量。在提出的数据库上进行测试可以得知, 所提方法的Pearson线性相关系数高于0.82, Spearman等级系数高于0.81, 表明其能有效预测S3D重定向图像的视觉质量。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">质量评价;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E5%BA%93&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像数据库;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E7%BB%B4%E5%9B%BE%E5%83%8F%E9%87%8D%E5%AE%9A%E5%90%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三维图像重定向;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E6%84%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度感;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%88%92%E9%80%82%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">舒适度;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    富振奇 (1993—) , 男, 浙江嘉兴人, 硕士研究生, 主要研究方向:图像处理、图像质量评价;;
                                </span>
                                <span>
                                    *邵枫 (1980—) , 男, 浙江杭州人, 教授, 博士生导师, 博士, CCF会员, 主要研究方向:图像处理、视频编码与质量评价。电子邮箱shaofeng@nbu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61622109);</span>
                    </p>
            </div>
                    <h1><b>Subjective and objective quality assessment for stereoscopic</b> 3<b>D retargeted images</b></h1>
                    <h2>
                    <span>FU Zhenqi</span>
                    <span>SHAO Feng</span>
            </h2>
                    <h2>
                    <span>Faculty of Information Science and Engineering, Ningbo University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Stereoscopic 3 D (S3 D) image retargeting aims to adjust aspect ratio of S3 D images. To objectively and accurately assess the quality of different retargeted S3 D images, a retargeted S3 D image quality assessment database was constructed. Firstly, 45 original images were retargeted by eight representative retargeting algorithms with two retargeting scales to generate 720 retargeted S3 D images. Then, the subjective quality evaluation score of each retargeted image was obtained via subjective testing. Finally, the subjective scores were converted to MOS (Mean Opinion Score) values. Based on all above, an objective quality assessment method was proposed for retargeted S3 D images. In this method, three types of features including depth perception, visual comfort and image quality of left and right views were extracted to calculate the retargeted S3 D image quality with the use of support vector regression prediction. Experimental results on the proposed database show that the proposed method has the Pearson linear correlation coefficient and the Spearman rank-order correlation coefficient higher than 0.82 and 0.81 respectively, demonstrating its superiority in retargeted S3 D image visual quality assessment.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=quality%20assessment&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">quality assessment;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20database&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image database;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=stereoscopic%203D%20image%20retargeting&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">stereoscopic 3D image retargeting;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=depth%20perception&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">depth perception;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visual%20comfort&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visual comfort;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    FU Zhenqi, born in 1993, M. S. candidate. His research interests include image processing, image quality assessment. ;
                                </span>
                                <span>
                                    SHAO Feng, born in 1980, Ph. D. , professor. His research interests include image processing, video coding and quality assessment.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-11</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61622109);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="69" name="69" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="70">随着三维 (Stereoscopic 3D, S3D) 图像显示技术的快速发展, 越来越多的S3D显示设备融入人们的生活。各种类型的S3D图像和视频极大地丰富了观众的视觉体验, 然而, 显示设备分辨率的多样化导致S3D图像无法在不同宽高比的显示器上自适应显示, 因此, 需要S3D图像重定向技术对S3D图像的宽高比进行调整<citation id="265" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="71">传统平面重定向方法如CR (CRopping) 、SCL (uniform scaling) 、SC (Seam Carving) <citation id="266" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>和WARP (WARPing) <citation id="267" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等, 在调整图像分辨率的同时, 将图像重要区域的几何形变和内容缺失最小化; 然而, S3D图像的质量不仅包含图像内容本身, 过大的双目视差、双目不对称以及双眼调节和辐辏冲突等都会严重影响S3D图像的观看舒适度<citation id="268" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。另外, S3D图像的深度信息使观看者能够获得更加逼真的临场体验。深度失真将会严重影响S3D图像的观看效果<citation id="269" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>, 因此, 与平面重定向方法相比, S3D图像重定向技术需要考虑更多的图像失真因素, 这也极大地增加了S3D图像重定向技术的复杂度。</p>
                </div>
                <div class="p1">
                    <p id="72">近年来, 人们提出了许多S3D图像重定向方法, 大致可分为离散方法和连续方法两类。离散方法通过删除或者增加左右图像对应位置的像素点或图像块, 调整S3D图像分辨率<citation id="271" type="reference"><link href="209" rel="bibliography" /><link href="211" rel="bibliography" /><link href="213" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>。离散方法的优点是直接对像素点进行插入或删除操作, 能够较好地保持S3D图像的几何一致性; 然而, 高耗时、对象形状难以保持等问题是这一类方法的主要瓶颈。连续方法的本质是非均匀伸缩变换。这类方法通过设定约束条件, 将原始图像映射至目标尺寸<citation id="272" type="reference"><link href="215" rel="bibliography" /><link href="217" rel="bibliography" /><link href="219" rel="bibliography" /><link href="221" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>。连续方法的优点是能够根据图像内容的重要性程度, 将图像的形变处理分布到人眼不关注的区域, 相对于离散方法能够更好地保持对象的形状, 尤其是当图像分辨率压缩较大时, 但连续方法较难保持重定向图像的几何一致性<citation id="270" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。由于不同的S3D图像重定向方法对图像的处理结果差别很大, 一些S3D重定向方法在某一类图像上处理效果较好, 但在其他类别上处理结果却很差, 因此, 提出一种有效的S3D重定向图像客观质量评价方法, 对不同S3D图像重定向方法的处理结果进行准确和客观的衡量, 进而指导和优化S3D图像重定向算法就显得尤为重要。</p>
                </div>
                <div class="p1">
                    <p id="73">然而, S3D重定向图像的分辨率与原始图像不同, 传统的S3D图像质量评价方法如文献<citation id="281" type="reference">[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</citation>等无法直接运用到S3D重定向图像质量评价中。目前, 针对图像重定向技术的评价方法大多集中在平面领域, 例如, Fang等<citation id="273" type="reference"><link href="229" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出了一种IR-SSIM (Image Retargeting Structural SIMilarity) 算法, 采用SIFT (Scale-Invariant Feature Transform) 特征建立原始图像和重定向图像间稠密匹配, 并根据SSIM算法计算匹配后图像的局部质量, 最后对局部质量进行加权得到重定向图像的质量; Hsu等<citation id="274" type="reference"><link href="231" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出了一种PGDIL (Perceptual Geometric Distortion and Information Loss) 算法, 将重定向图像的质量分为几何失真和内容损失两部分; Zhang等<citation id="275" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出了一种ARS (Aspect Ratio Similarity) 算法, 将图像重定向过程转换为后向重采样问题。Liang等<citation id="276" type="reference"><link href="235" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>将重定向图像失真分为显著区域保留、形变失真、全局结构保留、美学特性和对称性五个因素; Jiang等<citation id="277" type="reference"><link href="237" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>通过构建学习字典, 预测重定向图像质量; Chen等<citation id="278" type="reference"><link href="239" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提取自然场景统计特征、全局结构特征和双向内容缺失特征, 评价重定向图像质量; Zhang等<citation id="279" type="reference"><link href="241" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>分别从区域失真、图像块失真和像素失真三个层次评价重定向图像质量; Guo等<citation id="280" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>从全局失真和局部失真两个方面计算重定向图像的几何失真和内容缺失。然而, 平面重定向图像质量评价方法未考虑S3D图像的舒适度和深度等信息, 不适用于S3D重定向图像质量评价。</p>
                </div>
                <div class="p1">
                    <p id="74">为准确和客观地评价S3D重定向图像的视觉质量, 本文首先建立了一个S3D重定向图像质量评价数据库, 并对数据库中的S3D重定向图像进行主观打分。其次, 提出了一种S3D重定向图像客观质量评价方法。该方法从深度感 (Depth Perception, DP) 、视觉舒适度 (Visual Comfort, VC) 以及图像质量 (Image Quality, IQ) 三个方面评价S3D重定向图像的视觉质量。主要贡献包括:1) 本文建立了一个S3D重定向图像质量评价数据库。包含45幅原始图像、8种重定向方法、2种压缩尺度, 整个数据库共720幅S3D重定向图像。2) 本文从遮挡区域面积和对象深度差两个方面评价S3D重定向图像的深度感。提取视差幅值特征和视差梯度特征评价S3D重定向图像的视觉舒适度。最后, 结合深度感、舒适度以及S3D重定向图像左右视点图像的质量, 得到S3D重定向图像的质量, 评价结果符合人眼主观感知。</p>
                </div>
                <h3 id="75" name="75" class="anchor-tag">1 三维重定向图像质量评价数据库</h3>
                <div class="p1">
                    <p id="76">当前, 重定向图像的质量评价研究主要集中在平面领域, 对三维重定向图像的视觉质量评价研究较少, 且缺少相应的质量评价数据库。为此, 本文首先建立了一个三维重定向图像质量评价数据库, 对三维重定向图像进行主观质量评价。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">1.1 <b>原始图像</b></h4>
                <div class="p1">
                    <p id="78">三维图像重定向方法根据S3D图像的内容差异, 压缩人眼不感兴趣区域, 保留图像的重要内容, 保持或提升S3D图像的深度感和视觉舒适度。为建立可靠的S3D重定向图像据库, 更加有效地反映不同重定向方法的处理效果, 本文筛选了45幅原始S3D图像, 包含自然场景、显著前景对象、几何结构、人物等室内与室外场景。45幅原始S3D图像的左图像如图1所示。另外, 本文筛选的原始S3D图像包含不同的视差范围, 以验证不同重定向方法的深度和舒适度优化性能。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905034_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 原始S3D图像的左图像" src="Detail/GetImg?filename=images/JSJY201905034_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 原始S3D图像的左图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905034_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Left images of original S3D images</p>

                </div>
                <h4 class="anchor-tag" id="80" name="80">1.2 <b>重定向方法</b></h4>
                <div class="p1">
                    <p id="81">本文选取了8种具有代表性的三维图像重定向方法:CPC (Content Persistent Cropping) <citation id="282" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、GCSSC (Geometrically Consistent Stereo Seam Carving) <citation id="283" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、QOE (QoE-guided warping) <citation id="284" type="reference"><link href="221" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、MSC (Monocular Seam Carving) <citation id="285" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、SSCL (Stereo Scaling) 、SLWAP (Single-Layer Warping) <citation id="286" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、MSNS (Monocular Scale and Stretch) <citation id="287" type="reference"><link href="247" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>和VASSC (Visual Attention guided Seam Carving) <citation id="288" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。其中:CPC、GCSSC、MSC和VASSC是离散方法;QOE、SSCL、SLWAP和MSNS是连续方法。需要说明的是, MSC和MSNS是将平面重定向方法直接运用于S3D图像。下文将对选取的S3D图像重定向方法作简要介绍:</p>
                </div>
                <div class="p1">
                    <p id="82">CPC 通过S3D图像显著信息确定裁剪窗口位置, 对S3D图像进行自动裁剪。</p>
                </div>
                <div class="p1">
                    <p id="83">GCSSC 根据几何一致性原理, 同时移除左右图像对应的像素点。</p>
                </div>
                <div class="p1">
                    <p id="84">QOE 从视觉质量出发, 设置形状保持, 深度、舒适度优化能量项, 约束网格形变。</p>
                </div>
                <div class="p1">
                    <p id="85">MSC 左右图像分别使用SC方法处理。</p>
                </div>
                <div class="p1">
                    <p id="86">SSCL S3D图像均匀缩放。</p>
                </div>
                <div class="p1">
                    <p id="87">SLWAP 利用SIFT特征建立稀疏匹配, 通过约束匹配点视差关系, 在重定向的同时对图像的深度进行调整。</p>
                </div>
                <div class="p1">
                    <p id="88">MSNS 左右图像分别使用SNS (scale and stretch) 方法处理。</p>
                </div>
                <div class="p1">
                    <p id="89">VASSC 在保持S3D图像几何一致性的同时, 通过显著信息, 保持对象形状。</p>
                </div>
                <div class="p1">
                    <p id="90">与多数S3D图像重定向方法相同, 数据库中, 采用两种常见的缩放尺度:75%和50% (压缩至原始的75%和50%) , 对所有S3D图像在水平方向进行分辨率调整。八种S3D图像重定向方法的性能分析如表1所示。</p>
                </div>
                <div class="area_img" id="91">
                    <p class="img_tit"><b>表</b>1 <b>八种重定向方法性能分析</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Performance analysis of eight S3D image retargeting algorithms</p>
                    <p class="img_note"></p>
                    <table id="91" border="1"><tr><td>方法</td><td>形状</td><td>内容</td><td>深度</td><td>舒适度</td><td>几何一致性</td></tr><tr><td><br />CPC</td><td>√</td><td>√</td><td>√</td><td>×</td><td>√</td></tr><tr><td><br />GCSSC</td><td>√</td><td>√</td><td>√</td><td>×</td><td>√</td></tr><tr><td><br />QOE</td><td>√</td><td>√</td><td>√</td><td>√</td><td>×</td></tr><tr><td><br />MSC</td><td>√</td><td>√</td><td>×</td><td>×</td><td>×</td></tr><tr><td><br />SSCL</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td></tr><tr><td><br />SLWAP</td><td>√</td><td>√</td><td>√</td><td>√</td><td>×</td></tr><tr><td><br />MSNS</td><td>√</td><td>√</td><td>×</td><td>×</td><td>×</td></tr><tr><td><br />VASSC</td><td>√</td><td>√</td><td>√</td><td>×</td><td>√</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:“√”表示该种方法考虑了这项因素, “×”表示没有考虑该因素。</p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="92">由表1可知, 每种重定向方法有各自的重定向特性。本文选取这8种S3D重定向方法建立数据库, 使数据库能够全面地反映立体重定向图像的失真因素。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93">1.3 <b>主观评价</b></h4>
                <div class="p1">
                    <p id="94">本文根据ITU-R BT.500-11和ITU-R 1438两个图像主观质量测试标准<citation id="289" type="reference"><link href="249" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>, 对S3D重定向图像进行主观打分。主观测试采用Samsung UA65F89000 65英寸S3D-LED超高清显示器, 屏幕分辨率为3 840×2 160。主观实验中使用S3D Shutter眼镜, 观看距离为屏幕高度的3倍。显示器串扰水平 (左:0.38%和右: 0.15%) , 峰值亮度调整为50 CD/m<sup>2</sup>。共30个从事图像和视频处理的专家参与打分 (其中20人为男性, 10人为女性) 。主观打分采用5分制 (其中, 1分表示质量最差;2分表示质量较差;3分表示质量中等;4分表示质量较好;5分表示质量最好) 。在主观打分过程中同时给出原始图像和重定向图像, 并将所有重定向图像顺序打乱, 避免连续出现属于同一原始图像的重定向图像, 使观看者产生上下文记忆, 影响主观实验可靠性。在主观测试过程中, 每幅S3D重定向图像放映10 s, 观看者有5 s打分时间。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95">1.4 <b>主观分数处理</b></h4>
                <div class="p1">
                    <p id="96">本文按文献<citation id="290" type="reference">[<a class="sup">26</a>]</citation>中方法对主观打分值进行处理。首先将主观值转为z-scores:</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>u</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>Τ</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>s</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mrow><mi>Τ</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mo stretchy="false"> (</mo></mstyle><mi>s</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow></msub><mo>-</mo><mi>u</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99"><i>z</i><sub><i>i</i>, <i>j</i></sub>= (<i>s</i><sub><i>i</i>, <i>j</i></sub>-<i>u</i><sub><i>i</i></sub>) /<i>σ</i><sub><i>i</i></sub>      (3) </p>
                </div>
                <div class="p1">
                    <p id="100">其中:<i>s</i><sub><i>i</i>, <i>j</i></sub>表示第<i>i</i>个测试者对第<i>j</i>幅S3D重定向图像的主观打分值, <i>T</i>表示测试图像数量。根据置信水平, 剔除不可靠数据。实验中共剔除5人, 剩余25人的主观打分值用于MOS (Mean Opinion Score) 计算。最后, 对归一化的z-scores进行缩放, 将数值调整至[0, 100]区间内, 得到MOS值:</p>
                </div>
                <div class="p1">
                    <p id="101"><i>z</i><sub><i>i</i>, <i>j</i></sub>′=[100 (<i>z</i><sub><i>i</i>, <i>j</i></sub>+3) ]/6      (4) </p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>Ο</mi><mi>S</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>U</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>z</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow></msub><mo>´</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">其中<i>U</i>表示有效的主观打分人数。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">1.5 <b>主观质量分析</b></h4>
                <div class="p1">
                    <p id="105">S3D重定向图像数据库主观值分布如图2所示。主观质量分数的分布范围在20～80, MOS越大表示该S3D重定向图像的主观质量越好。由于提出的S3D重定向图像数据库包含45幅原始图像、8种重定向方法和2种压缩尺度。下文就原始图像、重定向方法和压缩尺度这三个因素对主观质量的影响进行分析。</p>
                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905034_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 MOS直方图" src="Detail/GetImg?filename=images/JSJY201905034_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 MOS直方图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905034_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Histogram of MOS values</p>

                </div>
                <h4 class="anchor-tag" id="107" name="107">1.5.1 原始图像对主观质量的影响</h4>
                <div class="p1">
                    <p id="108">一般而言, 原始图像的内容越复杂, 就越难保持图像的原始结构和内容。另外, 对于视差较大的原始图像, 一些S3D重定向方法虽然较好地保持了图像的结构和内容, 但是未能有效优化重定向图像的深度感和观看舒适度, 得到的重定向图像的主观质量较差。如图3所示, 原始图像的前景与背景复杂度, 视差大小等都会对重定向结果产生一定的影响。第一组原始图像 (第一行) 的背景复杂, 对背景的结构和内容保持较困难。第二组和第三组原始图像 (第二和第三行) 分别是室外和室内场景, 拥有不同的视差范围, 不同的重定向方法对重定向图像的深度和舒适度的优化效果也不相同。</p>
                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905034_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 三维重定向结果示例" src="Detail/GetImg?filename=images/JSJY201905034_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 三维重定向结果示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905034_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Examples of S3D retargeted images</p>

                </div>
                <h4 class="anchor-tag" id="110" name="110">1.5.2 重定向方法对主观质量的影响</h4>
                <div class="p1">
                    <p id="111">不同重定向方法的平均MOS值如图4所示。离散方法直接移除S3D图像的像素点或图像块, 重定向结果易产生对象形变, 尤其是在分辨率调整为50%的情况下, 因此, 离散方法的整体性能劣于连续方法。特别地, CPC通过显著信息建立裁剪窗口, 对S3D图像进行裁剪。当S3D图像的显著信息大部分在裁剪窗口内时, CPC可以得到很好的重定向结果。因此, CPC的整体重定向效果由优于其他三种离散方法。八种重定向方法中QOE和SLWAP两种连续方法的整体重定向效果最好。连续方法MSNS对左右图像分别进行重定向处理, 由于未考虑S3D图像左右视点间的相关性, 其重定向效果劣于QOE和SLWAP。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905034_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同三维重定向方法的平均MOS值" src="Detail/GetImg?filename=images/JSJY201905034_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同三维重定向方法的平均MOS值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905034_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Mean MOS values of different S3D retargeting methods</p>

                </div>
                <h4 class="anchor-tag" id="113" name="113">1.5.3 压缩尺度对主观质量的影响</h4>
                <div class="p1">
                    <p id="114">对于同一原始图像, 压缩尺度越大, 图像的内容、结构及深度等信息的保持也越困难。数据库中, 压缩至50%的S3D重定向图像的平均MOS为39.47, 压缩至75%的S3D重定向图像的平均MOS为60.53。虽然压缩至50%的S3D重定向图像平均质量劣于75%的S3D重定向图像, 但某些情况下, 一些图像压缩至50%的结果仍优于另一些图像压缩至75%的处理结果。本文将所有原始图像的分辨率调整至50%和75%两个尺度, 使数据库内容更丰富。</p>
                </div>
                <h3 id="115" name="115" class="anchor-tag">2 三维重定向图像客观质量评价方法</h3>
                <div class="p1">
                    <p id="116">本文提出了一种三维重定向图像客观质量评价方法。将S3D重定向图像的视觉质量分为深度感、舒适度和图像质量三个部分。提出的S3D重定向图像质量评价整体框图如图5所示。</p>
                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905034_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 S3D重定向图像质量评价整体框图" src="Detail/GetImg?filename=images/JSJY201905034_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 S3D重定向图像质量评价整体框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905034_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Whole framework for the proposed method</p>

                </div>
                <h4 class="anchor-tag" id="118" name="118">2.1 <b>深度感</b></h4>
                <div class="p1">
                    <p id="119">深度感, 即观看S3D图像时不同对象的深度信息。一般而言, S3D图像的主要对象会突出屏幕, 而背景则远离观看者。深度信息给观看者带来真实的临场感, 是S3D图像区别于平面图像的一个重要特征。因此, 在重定向过程中, 需保持或提高S3D图像的深度感, 以获得更好的视觉效果。</p>
                </div>
                <div class="p1">
                    <p id="120">为获取S3D图像的深度信息, 首先计算左图像视差图<citation id="291" type="reference"><link href="251" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>;然后, 根据几何一致性原理, 由左图像视差图获取左图像中被遮挡区域。S3D重定向左图像遮挡区域判定规则<citation id="292" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>如下, 其中<i>D</i>表示视差:</p>
                </div>
                <div class="p1">
                    <p id="121">若∃<i>n</i>′&lt;<i>n</i>, 使得<i>n</i>+<i>D</i><sub><i>m</i>, <i>n</i></sub>=<i>n</i>′+<i>D</i><sub><i>m</i>, <i>n</i>′</sub>, 则像素点 (<i>m</i>, <i>n</i>′) 被 (<i>m</i>, <i>n</i>) 遮挡;</p>
                </div>
                <div class="p1">
                    <p id="122">若∀<i>n</i>≠<i>n</i>′, 使得<i>n</i>+<i>D</i><sub><i>m</i>, <i>n</i></sub>≠<i>n</i>′+<i>D</i><sub><i>m</i>, <i>n</i>′</sub>, 则像素点 (<i>m</i>, <i>n</i>) 与 (<i>m</i>, <i>n</i>′) 不存在遮挡关系。</p>
                </div>
                <div class="p1">
                    <p id="123">如果左图像中有多个像素点同时映射到右图像中某一像素点, 则深度值最小 (距离观看者最近) 的像素点为遮挡像素点, 其余为被遮挡像素点。易知, 不同深度范围的对象, 遮挡背景对象的区域也不相同。S3D图像的深度感越强则被遮挡的背景也越多, 因此, 本文提取重定向图像的左图像被遮挡面积, 来衡量重定向图像的深度感。S3D重定向图像的遮挡特征定义为:</p>
                </div>
                <div class="p1">
                    <p id="124"><i>f</i><sub>1</sub>=<i>S</i><sub><i>o</i></sub>/<i>S</i>      (6) </p>
                </div>
                <div class="p1">
                    <p id="125">其中:<i>S</i><sub><i>o</i></sub>表示立体重定向图像中左图像被遮挡的面积, <i>S</i>表示左图像面积。此外, 本文通过对视差图进行<i>k</i>-means聚类, 将S3D重定向图像分割为前景对象和背景对象。显然, 前景对象与背景对象的深度差异越大, 则S3D重定向图像的深度感越强。因此, 本文定义S3D重定向图像深度差特征:</p>
                </div>
                <div class="p1">
                    <p id="126" class="code-formula">
                        <mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mrow><mo>|</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>m</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>n</mi></munder><mi>Η</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow><mtext>F</mtext></msubsup><mo>⋅</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mo>/</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>m</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>n</mi></munder><mi>Η</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow><mtext>F</mtext></msubsup><mo>-</mo></mrow></mrow></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>m</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>n</mi></munder><mi>Η</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow><mtext>B</mtext></msubsup><mo>⋅</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mo>/</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>m</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>n</mi></munder><mi>Η</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow><mtext>B</mtext></msubsup></mrow><mo>|</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="127">其中:<i>H</i><sup>F</sup>表示前景对象掩膜, <i>H</i><sup>B</sup>表示背景对象掩膜, <i>D</i>为重定向图像视差。</p>
                </div>
                <h4 class="anchor-tag" id="128" name="128">2.2 <b>舒适度</b></h4>
                <div class="p1">
                    <p id="129">S3D重定向图像视觉舒适度是指观看S3D重定向图像时, 观看者的整体视觉舒适感。在S3D图像重定向过程中, 左右图像处理结果不一致、视差过大等因素都会导致观看者双眼焦点调节和辐辏的严重冲突, 使观看者产生视觉疲劳, 因此, S3D重定向图像视觉舒适度是S3D图像重定向过程中需要考虑的一个因素。参考现有S3D图像舒适度评价方法<citation id="293" type="reference"><link href="253" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>, 本文提取了视差幅值与视差梯度两种特征, 来衡量S3D重定向图像的视觉舒适度。</p>
                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905034_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 S3D图像视觉重要性检测" src="Detail/GetImg?filename=images/JSJY201905034_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 S3D图像视觉重要性检测  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905034_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 S3D image visual importance detection</p>

                </div>
                <div class="p1">
                    <p id="131">由于S3D重定向图像不同区域的视觉重要性不同, 在计算舒适度前, 本文首先由左图像显著图<citation id="294" type="reference"><link href="255" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>与左图像视差图, 获取S3D重定向图像的视觉重要性图, 计算方式如下:</p>
                </div>
                <div class="p1">
                    <p id="132"><i>S</i>=<i>w</i><sub><i>I</i></sub><i>S</i><sub><i>I</i></sub>+<i>w</i><sub><i>D</i></sub><i>S</i><sub><i>D</i></sub>      (8) </p>
                </div>
                <div class="p1">
                    <p id="133">其中:<i>S</i>表示S3D图像视觉重要性图, <i>S</i><sub><i>I</i></sub>表示左图像显著图, <i>S</i><sub><i>D</i></sub>表示左图像视差图, <i>w</i><sub><i>I</i></sub>和<i>w</i><sub><i>D</i></sub>分别是<i>S</i><sub><i>I</i></sub>和<i>S</i><sub><i>D</i></sub>的权重, 实验中取<i>w</i><sub><i>I</i></sub>=<i>w</i><sub><i>D</i></sub>=0.5。S3D图像视觉重要性图如图6所示。本文通过视觉重要性加权, 获取S3D重定向图像的整体视觉舒适度。视差幅值与视差梯度特征计算如下:</p>
                </div>
                <div class="p1">
                    <p id="134" class="code-formula">
                        <mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mn>3</mn></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>m</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>n</mi></munder><mi>S</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mo>⋅</mo><mrow><mo>|</mo><mrow><mi>D</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub></mrow><mo>|</mo></mrow><mo>/</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>m</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>n</mi></munder><mi>S</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="135" class="code-formula">
                        <mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mn>4</mn></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>m</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>n</mi></munder><mi>S</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mo>⋅</mo><mrow><mo>|</mo><mrow><mtext>Δ</mtext><mi>D</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub></mrow><mo>|</mo></mrow><mo>/</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>m</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>n</mi></munder><mi>S</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="136" class="code-formula">
                        <mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Δ</mtext><mi>D</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><mo stretchy="false"> (</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn><mo>, </mo><mi>n</mi></mrow></msub><mo>+</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn><mo>, </mo><mi>n</mi></mrow></msub><mo>+</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>-</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="137" name="137">2.3 <b>图像质量</b></h4>
                <div class="p1">
                    <p id="138">与平面重定向类似, S3D重定向方法在调整图像宽高比的同时, 常常会造成S3D图像内容失真, 主要表现为局部几何失真和内容缺失。由于这些失真与传统图像失真 (如噪声、模糊等) 不同, 且重定向图像分辨率与原始图像不同, 导致传统图像质量评价方法, 例如结构相似性指数 (Structural SIMilarity index, SSIM) 、峰值信噪比 (Peak Signal-to-Noise Ratio, PSNR) 等无法直接用于计算重定向图像的质量。</p>
                </div>
                <div class="p1">
                    <p id="139">近些年, 平面重定向图像质量评价得到了较快发展, 提出了一系列平面重定向图像质量评价方法。现有的平面重定向图像质量评价方法大多通过建立重定向图像和原始图像间的匹配关系或转换关系, 预测重定向图像质量。本文选取了四种公开的平面图像重定向评价方法 (SIFT flow<citation id="295" type="reference"><link href="257" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>、BDS (Bidirectional Similarity) <citation id="296" type="reference"><link href="259" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>、EMD (Earth Mover's Distance) <citation id="297" type="reference"><link href="261" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>和ARS<citation id="298" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>) 计算S3D重定向图像的IQ特征:</p>
                </div>
                <div class="p1">
                    <p id="140"><i>f</i><sub>5</sub>=<i>G</i> (<i>I</i><sub>L</sub>, <i>I</i><sub>L</sub>′)      (12) </p>
                </div>
                <div class="p1">
                    <p id="141"><i>f</i><sub>6</sub>=<i>G</i> (<i>I</i><sub>R</sub>, <i>I</i><sub>R</sub>′)      (13) </p>
                </div>
                <div class="p1">
                    <p id="142">其中:<i>G</i>表示平面重定向评价方法; <i>I</i><sub>L</sub>和<i>I</i><sub>R</sub>分别表示原始左右视点图像; <i>I</i><sub>L</sub>′和<i>I</i><sub>R</sub>′分别表示重定向左右视点图像。</p>
                </div>
                <h4 class="anchor-tag" id="143" name="143">2.4 <b>特征融合</b></h4>
                <div class="p1">
                    <p id="144">本文提取S3D重定向图像遮挡特征 (<i>f</i><sub>1</sub>) 、深度差特征 (<i>f</i><sub>2</sub>) 、视差幅值 (<i>f</i><sub>3</sub>) 、视差梯度 (<i>f</i><sub>4</sub>) 、左图像质量 (<i>f</i><sub>5</sub>) 和右图像质量 (<i>f</i><sub>6</sub>) 六维特征。实验中, 选取支持向量回归 (Support Vector Regression, SVR) 作为S3D重定向图像的质量预测器。SVR是一种广泛使用的机器学习算法, 在很多中小样本的分类和回归任务中取得了很好的效果<citation id="299" type="reference"><link href="237" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。实验中, SVR核函数采用径向基函数 (Radial Basis Function, RBF) , 核函数的<i>gamma</i>值设置为4, 惩罚因子<i>cost</i>设置为20。为便于SVR学习, 在训练之前, 本文将所有特征归一化, 特征归一化方式如下:</p>
                </div>
                <div class="p1">
                    <p id="145"><i>y</i>= (<i>y</i><sub>max</sub>-<i>y</i><sub>min</sub>) · (<i>x</i>-<i>x</i><sub>min</sub>) / (<i>x</i><sub>max</sub>-<i>x</i><sub>min</sub>) +<i>y</i><sub>min</sub>      (14) </p>
                </div>
                <h3 id="146" name="146" class="anchor-tag">3 实验结果及分析</h3>
                <div class="p1">
                    <p id="147">在S3D重定向图像数据库上进行测试。随机选择数据库中图像总数的80%作为训练集, 剩余20%作为测试集。实验结果由1 000次交叉验证取平均值得到。采用PLCC (Pearson Linear Correlation Coefficient) 、SRCC (Spearman Rank-order Correlation Coefficient) 和RMSE (Root Mean Squared Error) 作为评价指标, 来衡量客观质量的准确性与单调性。PLCC和SRCC越大, 表明评价结果越准确;RMSE越小, 则预测结果与主观值越接近。在计算PLCC、RMSE时, 采用五参数拟合<citation id="300" type="reference"><link href="263" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="148" class="code-formula">
                        <mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mi>β</mi><msub><mrow></mrow><mn>1</mn></msub><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>-</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mi>β</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>-</mo><mi>β</mi><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">) </mo></mrow></msup></mrow></mfrac></mrow><mo>) </mo></mrow><mo>+</mo><mi>β</mi><msub><mrow></mrow><mn>4</mn></msub><mi>x</mi><mo>+</mo><mi>β</mi><msub><mrow></mrow><mn>5</mn></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="149">其中: <i>β</i><sub>1</sub>、 <i>β</i><sub>2</sub>、 <i>β</i><sub>3</sub>、 <i>β</i><sub>4</sub>、 <i>β</i><sub>5</sub>是待拟合参数。</p>
                </div>
                <h4 class="anchor-tag" id="150" name="150">3.1 <b>总体性能分析</b></h4>
                <div class="p1">
                    <p id="151">本文从DP、VC和IQ三个方面评价S3D重定向图像质量, 其中IQ特征由四种公开的平面重定向图像质量评价方法得到。多种评价方法总体性能比较如表2所示。表2中, 平面质量评价方法SIFT flow、BDS、EMD和ARS取左右视点图像平均质量作为该种方法的总体评价性能。</p>
                </div>
                <div class="area_img" id="152">
                    <p class="img_tit"><b>表</b>2 <b>不同评价方法总体性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Overall performance comparison of different evaluation methods</p>
                    <p class="img_note"></p>
                    <table id="152" border="1"><tr><td><br />方法</td><td>PLCC</td><td>SRCC</td><td>RMSE</td></tr><tr><td>SIFT flow</td><td>0.101 9</td><td>0.054 2</td><td>14.873 1</td></tr><tr><td><br />BDS</td><td>0.300 4</td><td>0.275 1</td><td>14.260 7</td></tr><tr><td><br />EMD</td><td>0.348 0</td><td>0.385 8</td><td>14.016 7</td></tr><tr><td><br />ARS</td><td>0.761 2</td><td>0.774 5</td><td>9.695 8</td></tr><tr><td><br />SIFT flow+DP+VC</td><td>0.386 8</td><td>0.321 8</td><td>13.769 2</td></tr><tr><td><br />BDS+DP+VC</td><td>0.504 2</td><td>0.480 8</td><td>12.898 4</td></tr><tr><td><br />EMD+DP+VC</td><td>0.704 0</td><td>0.698 9</td><td>10.620 6</td></tr><tr><td><br />ARS+DP+VC</td><td>0.823 7</td><td>0.815 3</td><td>8.433 8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="153">如表2所示, 由于平面重定向方法没有考虑S3D重定向图像的DP与VC, 因此评价结果较差。平面评价方法与本文提出的DP和VC特征结合后, 评价结果都有较大提升, 证明了本文提出的S3D重定向图像质量评价方法的有效性。特别地, ARS在S3D重定向图像质量评价上也取得了较好的结果, 这是因为:1) 与平面重定向图像相同, 几何失真和信息损失对S3D重定向图像质量有很大影响;2) 由于ARS能更加有效地提取平面重定向图像的结构信息, 其评价性能与SIFT flow、BDS和EMD相比提升较大。因此, 即使SIFT flow、BDS和EMD结合了DP和VC特征, 其评价结果仍然低于ARS。</p>
                </div>
                <h4 class="anchor-tag" id="154" name="154">3.2 <b>特征性能分析</b></h4>
                <div class="p1">
                    <p id="155">特征的有效性分析如表3和表4所示。表3中IQ特征 (<i>f</i><sub>5</sub>与<i>f</i><sub>6</sub>) 的整体有效性高于DP (<i>f</i><sub>1</sub>与<i>f</i><sub>2</sub>) 与VC (<i>f</i><sub>3</sub>与<i>f</i><sub>4</sub>) 。但是, 单一的IQ特征, 没有充分考虑S3D重定向图像的失真因素, 无法准确预测S3D重定向图像的整体质量。由表4可知, DP和VC都会影响S3D重定向视觉质量评价的准确性。因此, 本文联合DP、VC和IQ特征评价S3D重定向图像质量, 评价结果更加符合人眼主观感知。考虑到不同的特征结合方式会影响客观评价的结果, 下一节中, 将讨论几种常用的特征结合方式的性能。</p>
                </div>
                <div class="area_img" id="156">
                    <p class="img_tit"><b>表</b>3 <b>特征性能分析</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Performance of individual features</p>
                    <p class="img_note"></p>
                    <table id="156" border="1"><tr><td><br />特征</td><td>方法</td><td>PLCC</td><td>SRCC</td><td>RMSE</td></tr><tr><td><br /><i>f</i><sub>1</sub></td><td></td><td>0.170 3</td><td>0.148 9</td><td>14.732 6</td></tr><tr><td><br /><i>f</i><sub>2</sub></td><td>本文</td><td>0.182 6</td><td>0.014 2</td><td>14.699 7</td></tr><tr><td><br /><i>f</i><sub>3</sub></td><td>方法</td><td>0.098 2</td><td>0.000 2</td><td>14.878 3</td></tr><tr><td><br /><i>f</i><sub>4</sub></td><td></td><td>0.171 7</td><td>0.143 7</td><td>14.728 8</td></tr><tr><td rowspan="4"><br /><i>f</i><sub>5</sub></td><td><br />SIFT flow</td><td>0.113 2</td><td>0.069 9</td><td>14.854 8</td></tr><tr><td><br />BDS</td><td>0.250 1</td><td>0.255 9</td><td>14.477 4</td></tr><tr><td><br />EMD</td><td>0.365 5</td><td>0.390 5</td><td>13.916 5</td></tr><tr><td><br />ARS</td><td>0.785 1</td><td>0.768 7</td><td>9.259 7</td></tr><tr><td rowspan="4"><br /><i>f</i><sub>6</sub></td><td><br />SIFT flow</td><td>0.084 7</td><td>0.045 4</td><td>14.897 2</td></tr><tr><td><br />BDS</td><td>0.260 7</td><td>0.273 3</td><td>14.433 8</td></tr><tr><td><br />EMD</td><td>0.329 7</td><td>0.380 2</td><td>14.114 8</td></tr><tr><td><br />ARS</td><td>0.721 1</td><td>0.723 1</td><td>10.362 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="157">
                    <p class="img_tit"><b>表</b>4 DP、VC<b>和</b>IQ<b>的性能分析</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Impact of DP, VC and IQ on performance</p>
                    <p class="img_note"></p>
                    <table id="157" border="1"><tr><td><br />方法</td><td>PLCC</td><td>SRCC</td><td>RMSE</td></tr><tr><td><br />DP</td><td>0.248 5</td><td>0.209 1</td><td>14.487 5</td></tr><tr><td><br />VC</td><td>0.294 1</td><td>0.230 7</td><td>14.216 2</td></tr><tr><td><br />DP+VC</td><td>0.358 8</td><td>0.307 9</td><td>13.923 5</td></tr><tr><td><br />ARS+DP</td><td>0.808 2</td><td>0.800 8</td><td>8.785 3</td></tr><tr><td><br />ARS+VC</td><td>0.817 8</td><td>0.809 3</td><td>8.594 7</td></tr><tr><td><br />ARS+DP+VC</td><td>0.823 7</td><td>0.815 3</td><td>8.433 8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="158" name="158">3.3 <b>回归方式性能分析</b></h4>
                <div class="p1">
                    <p id="159">回归方式性能如表5所示, 其中, Linear-SVR、Poly-SVR和RBF-SVR分别表示使用线性核函数、多项式核函数以及径向基函数 (RBF) 核函数的SVR回归。由表5可知, 不同的回归方式的评价结果有一定差别。非线性回归方法的性能优于线性回归及采用线性核函数的SVR方法的性能, 这是因为线性回归假设各个特征分量对图像整体质量的影响是独立的, 而非线性回归则考虑了特征间的相互作用。实验中, 核函数采用RBF的SVR回归取得了最好的评价结果。</p>
                </div>
                <div class="area_img" id="160">
                    <p class="img_tit"><b>表</b>5 <b>不同回归方式性能分析</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Performance analysis of different regression modes</p>
                    <p class="img_note"></p>
                    <table id="160" border="1"><tr><td><br />方法</td><td>PLCC</td><td>SRCC</td><td>RMSE</td></tr><tr><td><br />线性回归</td><td>0.789 9</td><td>0.786 9</td><td>9.134 8</td></tr><tr><td><br />多项式回归</td><td>0.807 1</td><td>0.800 9</td><td>8.895 9</td></tr><tr><td><br />Linear-SVR</td><td>0.784 5</td><td>0.781 5</td><td>9.252 8</td></tr><tr><td><br />Poly-SVR</td><td>0.805 7</td><td>0.803 9</td><td>8.824 0</td></tr><tr><td><br />RBF-SVR</td><td>0.823 7</td><td>0.815 3</td><td>8.433 8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="161" name="161" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="162">本文构建了一个S3D重定向图像标准数据库, 选取八种代表性的S3D重定向方法, 生成720幅S3D重定向图像, 并对每一幅图像进行主观打分。在此基础上, 本文提出了一种结合深度感、舒适度和图像质量的S3D重定向图像客观质量评价方法。在数据库上的测试结果表明, 提出的方法的评价结果符合人眼感知。本文提出的图像数据库可用于S3D重定向图像的视觉质量研究, 提出的S3D重定向图像质量评价方法能客观地预测S3D重定向图像的视觉质量, 对S3D图像重定向算法优化有一定的指导意义。未来的研究重点是提取更多高层次的语义和结构信息, 建立更加准确的评价模型。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="199">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Geometrically Consistent Stereoscopic Image Editing Using Patch-Based Synthesis">

                                <b>[1]</b> LUO S J, SUN Y T, SHEN I C, et al.Geometrically consistent stereoscopic image editing using patch-based synthesis[J].IEEE Transactions on Visualization and Computer Graphics, 2015, 21 (1) :56-67.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098584&amp;v=Mjc4MjBIdGpOcjQ5RlpPSUhDWFE5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0Z3WGFSQT1OaWZJWTdLNw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> SHAMIR A, SHAMIR A, AVIDAN S.Improved seam carving for video retargeting[J].ACM Transactions on Graphics, 2008, 27 (3) :Article No.16.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Non-homogeneous content-driven video-retargeting">

                                <b>[3]</b> WOLF L, GUTTMANN M, COHEN-OR D.Non-homogeneous content-driven video-retargeting[C]// Proceedings of the 2007 International Conference on Computer Vision.Piscataway, NJ:IEEE, 2007:1-6.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D Visual ActivityAssessment Based on Natural Scene Statistics">

                                <b>[4]</b> LEE K, MOORTHY A K, LEE S, et al.S3D visual activity assessment based on natural scene statistics[J].IEEE Transactions on Image Processing, 2014, 23 (1) :450-465.
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600010009&amp;v=MTY0OTZlcnFRVE1ud1plWnRGaW5sVXIzSUtGd1hhUkE9TmlmT2ZiSzdIdEROcVk5RlpPb1BESHd3b0JNVDZUNFBRSC9pclJkRw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> KOOI F L, TOET A.Visual comfort of binocular and S3D displays[J].Displays, 2004, 25 (2/3) :99-108.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stereo seam carving a geometrically consistent approach">

                                <b>[6]</b> BASHA DEKEL T, MOSES Y, AVIDAN S.Stereo seam carving a geometrically consistent approach[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (10) :2513-2525.
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stereoscopic visual attention guided seam carving for stereoscopic image retargeting">

                                <b>[7]</b> SHAO F, LIN W C, LIN W S, et al.Stereoscopic visual attention guided seam carving for stereoscopic image retargeting[J].Journal of Display Technology, 2015, 12 (1) :22-30.
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Depth-preserving stereo image retargeting based on pixel fusion">

                                <b>[8]</b> LEI J J, WU M, ZHANG C Q, et al.Depth-preserving stereo image retargeting based on pixel fusion[J].IEEE Transactions on Multimedia, 2017, 79 (7) :1442-1453.
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Content-aware display adaptation and interactive editing for stereoscopic images">

                                <b>[9]</b> CHANG C H, LIANG C K, CHUANG Y Y.Content-aware display adaptation and editing for stereoscopic images[J].IEEE Transactions on Multimedia, 2011, 13 (4) :589-601.
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object-coherence warping for stereoscopic image retargeting">

                                <b>[10]</b> LIN S S, LIN C H, CHANG S H, et al.Object-coherence warping for stereoscopic image retargeting[J].IEEE Transactions on Circuits and Systems for Video Technology, 2014, 24 (5) :759-768.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Depth-preserving warping for stereo image retargeting">

                                <b>[11]</b> LI B, DUAN L Y, LIN C W, et al.Depth-preserving warping for stereo image retargeting[J].IEEE Transactions on Image Processing, 2015, 24 (9) :2811-2826.
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Qo E-guided warping for stereoscopic image retargeting">

                                <b>[12]</b> SHAO F, LIN W C, LIN W S, et al.QoE-guided warping for stereoscopic image retargeting[J].IEEE Transactions on Image Processing, 2017, 26 (10) :790-4805.
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Quality assessment of stereoscopic3D image compression by binocular integrationbehaviors">

                                <b>[13]</b> LIN Y H, WU J L.Quality assessment of stereoscopic S3D image compression by binocular integration behaviors[J].IEEE Transactions on Image Processing, 2014, 23 (4) :1527-1542.
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Full-reference quality assessment of stereoscopic images by learning binocular receptive field properties">

                                <b>[14]</b> SHAO F, LI K M, LIN W S, et al.Full-reference quality assessment of stereoscopic images by learning binocular receptive field properties[J].IEEE Transactions on Image Processing, 2015, 24 (10) :2971-2983.
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Quality prediction of asymmetrically distorted stereoscopic 3D images">

                                <b>[15]</b> WANG J H, REHMAN A, ZENG K, et al.Quality prediction of asymmetrically distorted stereoscopic S3D images[J].IEEE Transactions on Image Processing, 2015, 24 (11) :3400-3414.
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Objective Qualit Assessment for Image Retargeting Based on Structural Simi larity">

                                <b>[16]</b> FANG Y M, ZENG K, WANG Z, et al.Objective quality assessment for image retargeting based on structural similarity[J].IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 2014, 4 (1) :95-105.
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Objective quality assessment for image retargeting based on perceptual geometric distortion and information loss">

                                <b>[17]</b> HSU C C, LIN C W, FANG Y M, et al.Objective quality assessment for image retargeting based on perceptual geometric distortion and information loss[J].IEEE Journal of Selected Topics in Signal Processing, 2014, 8 (3) :377-389.
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Backward registrationbased aspect ratio similarity for image retargeting quality assessment">

                                <b>[18]</b> ZHANG Y B, FANG Y M, LIN W S, et al.Backward registration based aspect ratio similarity for image retargeting quality assessment[J].IEEE Transactions on Image Processing, 2016, 25 (9) :4286-4297.
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Objective quality prediction of image retargeting algorithms">

                                <b>[19]</b> LIANG Y, LIU Y J, GUTIERREZ D.Objective quality prediction of image retargeting algorithms[J].IEEE Transactions on Visualization and Computer Graphics, 2017, 23 (2) :1099-1110.
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning sparse representation for objective image retargeting quality assessment">

                                <b>[20]</b> JIANG Q P, SHAO F, LIN W S, et al.Learning sparse representation for objective image retargeting quality assessment[J].IEEE Transactions on Cybernetics, 2018, 48 (4) :1276-1289.
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Full reference quality assessment for image retargeting based on natural scene statistics modeling and bi-directional saliency similarity">

                                <b>[21]</b> CHEN Z B, LIN J X, NING L, et al.Full reference quality assessment for image retargeting based on natural scene statistics modeling and bi-directional saliency similarity[J].IEEE Transactions on Image Processing, 2017, 26 (11) :5138-5148.
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Objective quality assessment of image retargeting by incorporating fidelity measures and inconsistency detection">

                                <b>[22]</b> ZHANG Y C, NGAN K N, MA L, et al.Objective quality assessment of image retargeting by incorporating fidelity measures and inconsistency detection[J].IEEE Transactions on Image Processing, 2017, 26 (12) :5980-5993.
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESAA90B2CD79D1A23DDAD375FBD795FF5F&amp;v=MDkzMTBKN0RRMDd6R0pubTB0K1QzcVUzbVl5Y0xmaU03L3BDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6THE4dzZvPU5pZk9mY0xKRjlHK3Jmd3hZKw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> GUO Y C, HAO Y T, YU M.Image retargeting quality assessment based on content deformation measurement[J].Signal Processing:Image Communication, 2018, 67:171-181.
                            </a>
                        </p>
                        <p id="245">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stereoscopic thumbnail creation via efficient stereo saliency detection">

                                <b>[24]</b> WANG W G, SHEN J B, YU Y Z, et al.Stereoscopic thumbnail creation via efficient stereo saliency detection[J].IEEE Transactions on Visualization and Computer Graphics, 2017, 23 (8) :2014-2027.
                            </a>
                        </p>
                        <p id="247">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098682&amp;v=MjM4MjZNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0Z3WGFSQT1OaWZJWTdLN0h0ak5yNDlGWk9JSENuUTdvQg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> WANG Y S, TAI C L, SORKINE O, et al.Optimized scale-and-stretch for image resizing [J].ACM Transactions on Graphics, 2008, 27 (5) :118.
                            </a>
                        </p>
                        <p id="249">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Study of Subjective and Objective Quality Assessment of Video">

                                <b>[26]</b> SESHADRINATHAN K, SOUNDARARAJAN R, BOVIK A C, et al.Study of subjective and objective quality assessment of video[J].IEEE Transactions on Image Process.2010, 19 (6) :1427-1441.
                            </a>
                        </p>
                        <p id="251">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Secrets of optical flow estimation and their principles">

                                <b>[27]</b> SUN D, ROTH S, BLACK M J.Secrets of optical flow estimation and their principles[C]// Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2010:2432-2439.
                            </a>
                        </p>
                        <p id="253">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Predicting visual discomfort of stereoscopic images using human attention model">

                                <b>[28]</b> JUNG Y J, SOHN H, LEE S I, et al.Predicting visual discomfort of stereoscopic images using human attention model[J].IEEE Transactions on Circuits and Systems for Video Technology, 2013, 23 (12) :2077-2082.
                            </a>
                        </p>
                        <p id="255">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Graph-based visual saliency">

                                <b>[29]</b> HAREL J, KOCH C, PERONA P.Graph-based visual saliency[C]// Proceedings of the 19th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2006:545-552.
                            </a>
                        </p>
                        <p id="257">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SIFT flow: Dense correspondence across scenes and its applications">

                                <b>[30]</b> LIU C, YUEN J, TORRALBA A.SIFT flow:dense correspondence across scenes and its applications[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) :978-994.
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Summarizing visual data using bidirectional similarity">

                                <b>[31]</b> SIMAKOV D, CASPI Y, SHECHTMAN E, et al.Summarizing visual data using bidirectional similarity[C]// Proceedings of the 2008 Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2008:1-8.
                            </a>
                        </p>
                        <p id="261">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast and robust earth movers distances">

                                <b>[32]</b> PELE O, WERMAN M.Fast and robust earth mover's distances[C]// Proceedings of the 2009 International Conference on Computer Vision.Piscataway, NJ:IEEE, 2009:460-467.
                            </a>
                        </p>
                        <p id="263">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Statistical Evaluation of Recent Full Reference Image Quality Assessment Algorithms">

                                <b>[33]</b> SHEIKH H R, SABIR M F, BOVIK A C.A statistical evaluation of recent full reference image quality assessment algorithms[J].IEEE Transactions on Image Processing, 2006, 15 (11) :3440-3451.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201905034" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905034&amp;v=MTA4NzJxbzlHWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVMektMejdCZDdHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
