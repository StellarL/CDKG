<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136466677002500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201909018%26RESULT%3d1%26SIGN%3dH5CzW201ri9LnZbKdcmayiRk7lo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909018&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909018&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909018&amp;v=MzI0MDhIOWpNcG85RWJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amxVNzdLTHo3QmQ3RzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#40" data-title="1 聚合距离参数 ">1 聚合距离参数</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#60" data-title="2 改进&lt;i&gt;K&lt;/i&gt;-means算法 ">2 改进<i>K</i>-means算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="2.1 &lt;b&gt;改进初始聚类中心&lt;/b&gt;">2.1 <b>改进初始聚类中心</b></a></li>
                                                <li><a href="#68" data-title="2.2 &lt;b&gt;改进准则函数&lt;/b&gt;">2.2 <b>改进准则函数</b></a></li>
                                                <li><a href="#79" data-title="2.3 &lt;b&gt;改进算法总流程&lt;/b&gt;">2.3 <b>改进算法总流程</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#127" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#128" data-title="3.1 &lt;b&gt;可视化数据集&lt;/b&gt;">3.1 <b>可视化数据集</b></a></li>
                                                <li><a href="#131" data-title="3.2 &lt;b&gt;标准数据集&lt;/b&gt;">3.2 <b>标准数据集</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#144" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#130" data-title="图1 仿真数据集聚类结果">图1 仿真数据集聚类结果</a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;表&lt;/b&gt;1 UCI &lt;b&gt;数据集&lt;/b&gt;"><b>表</b>1 UCI <b>数据集</b></a></li>
                                                <li><a href="#142" data-title="图2 调整兰德系数、互信息和及Fowlkes-Mallows指标对比结果">图2 调整兰德系数、互信息和及Fowlkes-Mallows指标对比结果</a></li>
                                                <li><a href="#143" data-title="图3 同质性和完整性指标比较结果">图3 同质性和完整性指标比较结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="156">


                                    <a id="bibliography_1" title=" 王治和,黄梦莹,杜辉,等.基于密度峰值与密度聚类的集成算法[J].计算机应用,2019,39(2):398-402.(WANG Z H,HUANG M Y,DU H,et al.Integrated algorithm based on density peaks and density-based clustering J].Journal of Computer Applications,2019,39(2):398-402.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201902017&amp;v=MTk5NDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amxVNzdLTHo3QmQ3RzRIOWpNclk5RVk0UUs=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         王治和,黄梦莹,杜辉,等.基于密度峰值与密度聚类的集成算法[J].计算机应用,2019,39(2):398-402.(WANG Z H,HUANG M Y,DU H,et al.Integrated algorithm based on density peaks and density-based clustering J].Journal of Computer Applications,2019,39(2):398-402.)
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_2" title=" McLOUGHLIN F,DUFFY A,CONLON M.A clustering approach to domestic electricity load profile characterisation using smart metering data [J].Applied Energy,2015,141:190-199." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122600202871&amp;v=MDUzOThNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSUY0VWF4QT1OaWZPZmJLOUg5UE9xWTlGWnVzTkJIczRvQg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         McLOUGHLIN F,DUFFY A,CONLON M.A clustering approach to domestic electricity load profile characterisation using smart metering data [J].Applied Energy,2015,141:190-199.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_3" title=" ALI A-W,WU J,JENKINS N.K-means based load estimation of domestic smart meter measurements [J].Applied Energy,2016,194:333-342." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=K-means based load estimation of domestic smart meter measurements">
                                        <b>[3]</b>
                                         ALI A-W,WU J,JENKINS N.K-means based load estimation of domestic smart meter measurements [J].Applied Energy,2016,194:333-342.
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_4" title=" 杨辉华,王克,李灵巧,等.基于自适应布谷鸟搜索算法的K-means聚类算法及其应用[J].计算机应用,2016,36(8):2066-2070.(YANG H H,WANG K,LI L Q,et al.K-means clustering algorithm based on adaptive cuckoo search and its application [J].Journal of Computer Applications,2016,36(8):2066-2070.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201608003&amp;v=MDYwMDNCZDdHNEg5Zk1wNDlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqbFU3N0tMejc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         杨辉华,王克,李灵巧,等.基于自适应布谷鸟搜索算法的K-means聚类算法及其应用[J].计算机应用,2016,36(8):2066-2070.(YANG H H,WANG K,LI L Q,et al.K-means clustering algorithm based on adaptive cuckoo search and its application [J].Journal of Computer Applications,2016,36(8):2066-2070.)
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_5" title=" 黄韬,刘胜辉,谭艳娜.基于K-means聚类算法的研究[J].计算机技术与发展,2011,21(7):54-57.(HUANG T,LIU S H,TAN Y N.Research of clustering algorithm based on K-means [J].Computer Technology and Development,2011,21(7):54-57.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WJFZ201107016&amp;v=MjUzMjk5RE1xSTlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqbFU3N0tNaWZOZExHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         黄韬,刘胜辉,谭艳娜.基于K-means聚类算法的研究[J].计算机技术与发展,2011,21(7):54-57.(HUANG T,LIU S H,TAN Y N.Research of clustering algorithm based on K-means [J].Computer Technology and Development,2011,21(7):54-57.)
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_6" title=" 王骏,王士同,邓赵红.特征加权距离与软子空间学习相结合的文本聚类新方法[J].计算机学报,2012,35(8):1655-1665.(WANG J,WANG S T,DENG Z H.A novel text clustering algorithm based on feature weighting distance and soft subspace learning [J].Chinese Journal of Computers,2012,35(8):1655-1665.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201208009&amp;v=MTQ2OTg3S0x6N0Jkckc0SDlQTXA0OUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVTc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         王骏,王士同,邓赵红.特征加权距离与软子空间学习相结合的文本聚类新方法[J].计算机学报,2012,35(8):1655-1665.(WANG J,WANG S T,DENG Z H.A novel text clustering algorithm based on feature weighting distance and soft subspace learning [J].Chinese Journal of Computers,2012,35(8):1655-1665.)
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_7" title=" 郁启麟.K-means算法初始聚类中心选择的优化[J].计算机系统应用,2017,26(5):170-174.(YU Q L.Optimization of initial clustering centers selection method for K-means algorithm [J].Computer Systems &amp;amp; Applications,2017,26(5):170-174.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYY201705027&amp;v=MTQ2MTR6cXFCdEdGckNVUjdxZlp1WnNGeWpsVTc3S1BUblNkN0c0SDliTXFvOUhZNFFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         郁启麟.K-means算法初始聚类中心选择的优化[J].计算机系统应用,2017,26(5):170-174.(YU Q L.Optimization of initial clustering centers selection method for K-means algorithm [J].Computer Systems &amp;amp; Applications,2017,26(5):170-174.)
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_8" title=" 周润物,李智勇,陈少淼,等.面向大数据处理的并行优化抽样聚类K-means算法[J].计算机应用,2016,36(2):311-315.(ZHOU R W,LI Z Y,CHEN S M,et al.Parallel optimization sampling clustering K-means algorithm for big data processing [J].Journal of Computer Applications,2016,36(2):311-315.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201602007&amp;v=MjYzNjR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqbFU3N0tMejdCZDdHNEg5Zk1yWTlGWTRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         周润物,李智勇,陈少淼,等.面向大数据处理的并行优化抽样聚类K-means算法[J].计算机应用,2016,36(2):311-315.(ZHOU R W,LI Z Y,CHEN S M,et al.Parallel optimization sampling clustering K-means algorithm for big data processing [J].Journal of Computer Applications,2016,36(2):311-315.)
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_9" title=" 王丽娟,郝志峰,蔡瑞初,等.基于随机取样的选择性K-means聚类融合算法[J].计算机应用,2013,33(7):1969-1972.(WANG L J,HAO Z F,CAI R C,et al.Selective K-means clustering ensemble based on random sampling [J].Journal of Computer Applications,2013,33(7):1969-1972.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201307046&amp;v=Mjk2OTZVNzdLTHo3QmQ3RzRIOUxNcUk5QllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amw=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         王丽娟,郝志峰,蔡瑞初,等.基于随机取样的选择性K-means聚类融合算法[J].计算机应用,2013,33(7):1969-1972.(WANG L J,HAO Z F,CAI R C,et al.Selective K-means clustering ensemble based on random sampling [J].Journal of Computer Applications,2013,33(7):1969-1972.)
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_10" title=" 毛典辉.基于MapReduce的Canopy-Kmeans改进算法[J].计算机工程与应用,2012,48(27):22-26.(MAO D H.Improved Canopy-Kmeans algorithm based on MapReduce [J].Computer Engineering and Applications,2012,48(27):22-26.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201227007&amp;v=Mjg4MDF6N01hYkc0SDlQT3FJOUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVTc3S0w=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         毛典辉.基于MapReduce的Canopy-Kmeans改进算法[J].计算机工程与应用,2012,48(27):22-26.(MAO D H.Improved Canopy-Kmeans algorithm based on MapReduce [J].Computer Engineering and Applications,2012,48(27):22-26.)
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_11" title=" 赵昱,陈琴,苏一丹,等.基于邻域相似度的近邻传播聚类算法[J].计算机工程与设计,2018,39(7):1883-1888.(ZHAO Y,CHEN Q,SU Y D,et al.Affinity propagation clustering algorithm based on neighborhood similarity [J].Computer Engineering and Design,2018,39(7):1883-1888.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201807016&amp;v=MDY0Njl1WnNGeWpsVTc3S05pZllaTEc0SDluTXFJOUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         赵昱,陈琴,苏一丹,等.基于邻域相似度的近邻传播聚类算法[J].计算机工程与设计,2018,39(7):1883-1888.(ZHAO Y,CHEN Q,SU Y D,et al.Affinity propagation clustering algorithm based on neighborhood similarity [J].Computer Engineering and Design,2018,39(7):1883-1888.)
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_12" title=" 刘鹏,王明阳,王焱.基于自适应动态球半径的K邻域搜索算法[J].机械设计与制造工程,2016,45(6):83-86.(LIU P,WANG M Y,WANG Y.K domain search algorithm based on adaptive dynamic sphere radius [J].Machine Design and Manufacturing Engineering,2016,45(6):83-86.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JXZZ201606018&amp;v=MjY5NjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqbFU3N0tMelhSZExHNEg5Zk1xWTlFYklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         刘鹏,王明阳,王焱.基于自适应动态球半径的K邻域搜索算法[J].机械设计与制造工程,2016,45(6):83-86.(LIU P,WANG M Y,WANG Y.K domain search algorithm based on adaptive dynamic sphere radius [J].Machine Design and Manufacturing Engineering,2016,45(6):83-86.)
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_13" title=" NGUYEN D,LE T,NGUYEN S.An algorithmic method of calculating neighborhood radius for clustering in-home activities within smart home environment [C]// Proceedings of the 7th International Conference on Intelligent Systems,Modelling and Simulation.Piscataway,NJ:IEEE,2016:42-47." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An algorithmic method of calculating neighborhood radius for clustering in-home activities within smart home environment">
                                        <b>[13]</b>
                                         NGUYEN D,LE T,NGUYEN S.An algorithmic method of calculating neighborhood radius for clustering in-home activities within smart home environment [C]// Proceedings of the 7th International Conference on Intelligent Systems,Modelling and Simulation.Piscataway,NJ:IEEE,2016:42-47.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_14" title=" COELHO G P,BARBANTE C C,BOCCATO L,et al.Automatic feature selection for BCI:an analysis using the Davies-Bouldin index and extreme learning machines [C]// Proceedings of the 2012 International Joint Conference on Neural Networks.Piscataway,NJ:IEEE,2012:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic Feature Selection for BCI:an Analysis Using the Davies-Bouldin Index and Extreme Learning Machines">
                                        <b>[14]</b>
                                         COELHO G P,BARBANTE C C,BOCCATO L,et al.Automatic feature selection for BCI:an analysis using the Davies-Bouldin index and extreme learning machines [C]// Proceedings of the 2012 International Joint Conference on Neural Networks.Piscataway,NJ:IEEE,2012:1-8.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_15" title=" THOMAS J C R,PE&#209;AS M S,MORA M.New version of Davies-Bouldin index for clustering validation based on cylindrical distance [C]// Proceedings of the 32nd International Conference of the Chilean Computer Science Society.Piscataway,NJ:IEEE,2013:49-53." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=New version of Davies-Bouldin index for clustering validation based on cylindrical distance">
                                        <b>[15]</b>
                                         THOMAS J C R,PE&#209;AS M S,MORA M.New version of Davies-Bouldin index for clustering validation based on cylindrical distance [C]// Proceedings of the 32nd International Conference of the Chilean Computer Science Society.Piscataway,NJ:IEEE,2013:49-53.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-05-20 09:47</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(09),2586-2590 DOI:10.11772/j.issn.1001-9081.2019030485            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于聚合距离参数的改进<i>K</i>-means算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%B7%A7%E7%8E%B2&amp;code=42779645&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王巧玲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B9%94%E9%9D%9E&amp;code=00022958&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">乔非</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%92%8B%E5%8F%8B%E5%A5%BD&amp;code=35567224&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蒋友好</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%90%8C%E6%B5%8E%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0118734&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">同济大学电子与信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对传统<i>K</i>均值聚类(<i>K</i>-means)算法随机选择初始中心及<i>K</i>值导致的聚类结果不确定且精度不高问题,提出了一种基于聚合距离的改进<i>K</i>-means算法。首先,基于聚合距离参数筛选出优质的初始聚类中心,并将其作用于<i>K</i>-means算法。然后,引入戴维森堡丁指数(DBI)作为算法的准则函数,循环更新聚类直到准则函数收敛,最后完成聚类。改进算法提供了优质的初始聚类中心及<i>K</i>值,避免了聚类结果的随机性。二维数值型仿真数据的聚类结果表明,改进算法在数据样本数达到10 000时仍能保持较好的聚类效果。针对Iris和Seg这两个UCI标准数据集的调整兰德系数,改进算法比传统算法性能分别提高了83.7%和71.0%,最终验证了改进算法比传统算法聚类结果的准确性更高。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%9A%E5%90%88%E8%B7%9D%E7%A6%BB%E5%8F%82%E6%95%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">聚合距离参数;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%9A%E7%B1%BB%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">聚类中心;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%9A%E7%B1%BB%E8%AF%84%E5%88%A4%E6%8C%87%E6%A0%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">聚类评判指标;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%88%B4%E7%BB%B4%E6%A3%AE%E5%A0%A1%E4%B8%81%E6%8C%87%E6%95%B0(DBI)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">戴维森堡丁指数(DBI);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E6%8D%AE%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据聚类;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王巧玲(1994—),女,浙江宁波人,硕士研究生,主要研究方向:聚类算法、大数据分析;;
                                </span>
                                <span>
                                    *乔非(1967—),女,陕西西安人,教授,博士,主要研究方向:大数据分析、复杂制造调度、智能生产系统;电子邮箱fqiao@tongji.edu.cn;
                                </span>
                                <span>
                                    蒋友好(1976—),男,山东枣庄人,博士研究生,主要研究方向:大数据分析、智能生产系统。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-25</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金重大项目(71690230,71690234);</span>
                    </p>
            </div>
                    <h1><b>Improved <i>K</i>-means algorithm with aggregation distance coefficient</b></h1>
                    <h2>
                    <span>WANG Qiaoling</span>
                    <span>QIAO Fei</span>
                    <span>JIANG Youhao</span>
            </h2>
                    <h2>
                    <span>School of Electronics and Information Engineering, Tongji University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Initial centers and <i>K</i> value are determined randomly in the traditional <i>K</i>-means algorithm, which makes clustering results uncertain and with low precision. Therefore, an improved <i>K</i>-means algorithm based on aggregation distance was proposed. Firstly, high-quality cluster centers were filtered out based on the aggregation distance coefficient as the initial centers of the <i>K</i>-means algorithm. Secondly, Davies-Bouldin Index(DBI) was introduced as the criterion function of the algorithm, and the clustering was cyclically updated until the criterion function converged. Finally, the clustering was completed. The proposed algorithm provides good initial clustering centers and <i>K</i> value, avoiding the randomness of clustering results. The clustering results of two-dimensional numerical simulation data show that the improved algorithm can still maintain a good clustering effect when the number of data samples reaches 10 000. For the adjusted Rand coefficients of the two UCI standard datasets named Iris and Seg, the improved algorithm respectively improves the performance of clustering by 83.7% and 71.0% compared to the traditional algorithm. It can be seen that the improved algorithm can increase the accuracy of the clustering result compared with the traditional algorithm.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=aggregation%20distance%20coefficient&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">aggregation distance coefficient;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=cluster%20center&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">cluster center;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=clustering%20evaluation%20index&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">clustering evaluation index;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Davies-Bouldin%20Index(DBI)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Davies-Bouldin Index(DBI);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=data%20clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">data clustering;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Qiaoling, born in 1994, M. S. candidate. Her research interests include clustering algorithm, big data analysis. ;
                                </span>
                                <span>
                                    QIAO Fei, born in 1967, Ph. D. , professor. Her research interests include big data analysis, complex manufacturing planning and scheduling, intelligent production systems. ;
                                </span>
                                <span>
                                    JIANG Youhao, born in 1976, Ph. D. candidate. His research interests include big data analysis, intelligent production systems.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-25</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the Major Program of National Natural Science Foundation of China(71690230,71690234);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="36">随着云计算、物联网等技术的出现,企业日趋信息化,应用系统中的数据量呈指数级增长。数据挖掘技术能够帮助用户从大量数据中分析出其所蕴涵的有价值的信息。聚类算法就是一种典型的数据挖掘方法,它是一种无监督的机器学习算法,适合于将不含训练集的大数据以相似度为依据进行聚类<citation id="186" type="reference"><link href="156" rel="bibliography" /><link href="158" rel="bibliography" /><link href="160" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="37"><i>K</i>均值聚类(<i>K</i>-means)是一种常见的聚类算法,它在处理大规模数据集时可保持较好的可伸缩性和高效性<citation id="187" type="reference"><link href="162" rel="bibliography" /><link href="164" rel="bibliography" /><link href="166" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。然而,传统的<i>K</i>-means算法也存在一些缺陷:1)初始聚类中心的选择是随机的,这使得聚类结果不稳定,准确性较低。2)聚类个数<i>K</i>值的选择是随机的,若<i>K</i>值过大,则类与类之间的相似度太小;若<i>K</i>值过小,则类与类之间的相似度过大,这两种情况都会导致聚类结果不准确。</p>
                </div>
                <div class="p1">
                    <p id="38">很多学者针对<i>K</i>-means算法的缺点提出了相应的改进。郁启麟<citation id="188" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>采用关系矩阵和度中心性选择<i>K</i>个初始聚类中心,以此来改进<i>K</i>-means算法。文献<citation id="189" type="reference">[<a class="sup">8</a>]</citation>中提出了一种基于优化抽样聚类的方法,一定程度上解决了<i>K</i>-means算法聚类精度不足和收敛速度慢的问题。文献<citation id="190" type="reference">[<a class="sup">9</a>]</citation>中提出了用随机取样策略来避免决策选取陷入局部最小。为了解决<i>K</i>-means算法初始聚类中心和<i>K</i>值随机性的问题,文献<citation id="191" type="reference">[<a class="sup">10</a>]</citation>应用了Canopy-Kmeans算法,先由Canopy算法对数据集进行粗聚类,得到一定数量的类,每个类的中心作为<i>K</i>-means算法的初始聚类中心,类的个数决定<i>K</i>值的大小。然而Canopy算法的两个阈值是随机选择的,这导致了获得的初始中心和<i>K</i>值存在随机性,从而影响聚类效果。</p>
                </div>
                <div class="p1">
                    <p id="39">针对<i>K</i>-means算法初始聚类中心和<i>K</i>值选择问题,本文提出了一种基于聚合距离参数的改进<i>K</i>-means算法。对给定数据集中的每一个数据点进行聚合度及其所属类的距离分析,筛选出符合条件的数据点作为初始聚类中心,符合条件的数据点的个数即为<i>K</i>值。改进的算法能够确定最优的初始聚类中心及聚类个数,从而避免了聚类结果的不确定性。最后,采用可视化数据及UCI标准数据集,验证了改进算法聚类结果的准确性。</p>
                </div>
                <h3 id="40" name="40" class="anchor-tag">1 聚合距离参数</h3>
                <div class="p1">
                    <p id="41">初始聚类中心及<i>K</i>值选择不准确,会导致<i>K</i>-means算法聚类结果准确性不高,因此,本文提出了聚合距离参数,以筛选出一定量的优质的初始聚类中心。聚合距离参数中涉及到的一些相关定义和概念如下:</p>
                </div>
                <div class="p1">
                    <p id="42">欧氏距离 设每一个数据点包含<i>m</i>个属性,即<i>x</i><sub><i>i</i></sub>={<i>x</i><sub><i>i</i>1</sub>,<i>x</i><sub><i>i</i>2</sub>,…,<i>x</i><sub><i>im</i></sub>},则<i>x</i><sub><i>i</i></sub>、<i>x</i><sub><i>j</i></sub>之间的距离可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="43" class="code-formula">
                        <mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>s</mi></mrow></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>s</mi></mrow></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="44">数据集平均距离 即为一个数据集合中所有数据点之间的平均欧氏距离。</p>
                </div>
                <div class="p1">
                    <p id="45" class="code-formula">
                        <mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>v</mi><mi>g</mi><mi>d</mi><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>2</mn><mrow><mi>n</mi><mo stretchy="false">(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>d</mi></mstyle></mrow></mstyle><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="46">邻域半径<mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo>=</mo><mfrac><mrow><mi>A</mi><mi>v</mi><mi>g</mi><mi>d</mi><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><mrow><mi>n</mi><msup><mrow></mrow><mrow><mi>r</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>R</mi></mrow></msup></mrow></mfrac></mrow></math></mathml>,其中<i>n</i>表示数据点的个数,<i>releR</i>为邻域半径调节系数,范围在0～1,根据经验,<i>releR</i>取0.13时,聚类效果最佳<citation id="192" type="reference"><link href="176" rel="bibliography" /><link href="178" rel="bibliography" /><link href="180" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="48">聚合度 点<i>x</i><sub><i>i</i></sub>的聚合度<i>Deg</i>(<i>x</i><sub><i>i</i></sub>)表示为与其距离小于半径的点的个数,即:</p>
                </div>
                <div class="p1">
                    <p id="49"><mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>f</mi></mstyle><mo stretchy="false">(</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>-</mo><mi>R</mi><mo stretchy="false">)</mo></mrow></math></mathml>;</p>
                </div>
                <div class="p1">
                    <p id="51" class="code-formula">
                        <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>,</mo><mtext> </mtext><mi>x</mi><mo>≤</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>,</mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>x</mi><mo>&gt;</mo><mn>0</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="52">集合平均距离 与点<i>x</i><sub><i>i</i></sub>的距离小于邻域半径的所有点组成一个集合,那么点<i>x</i><sub><i>i</i></sub>所在集合的平均距离可以定义为:</p>
                </div>
                <div class="p1">
                    <p id="53" class="code-formula">
                        <mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>a</mi><mi>v</mi><mi>g</mi><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>2</mn><mrow><mi>D</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>D</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>D</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mn>1</mn></mrow></munderover><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>D</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></munderover><mi>d</mi></mstyle><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="54">集合平均距离可以衡量一个数据点所在集合的紧密度, 值越小,表示<i>Cavgd</i>(<i>x</i><sub><i>i</i></sub>)所在集合越紧密。</p>
                </div>
                <div class="p1">
                    <p id="55">聚合度距离 聚合度距离表示点<i>x</i><sub><i>i</i></sub>与其他具有较高聚合度点<i>x</i><sub><i>j</i></sub>之间的距离。若所有点中<i>x</i><sub><i>i</i></sub>的聚合度最大,则其聚合度距离为<i>x</i><sub><i>i</i></sub>与其余任何点的最大距离。若<i>x</i><sub><i>i</i></sub>的聚合度不是所有点中最大的,则其聚合度距离为<i>x</i><sub><i>i</i></sub>与其余任何点的最小距离。即:</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>G</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>min</mi><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mtext>存</mtext><mtext>在</mtext><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mtext>满</mtext><mtext>足</mtext><mi>D</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>&gt;</mo><mi>D</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>max</mi><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>不</mtext><mtext>存</mtext><mtext>在</mtext><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mtext>满</mtext><mtext>足</mtext><mi>D</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>&gt;</mo><mi>D</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">聚合距离参数 聚合距离参数由聚合度、集合平均距离及聚合度距离3个参数决定。即:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ω</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>D</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>⋅</mo><mi>G</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>C</mi><mi>a</mi><mi>v</mi><mi>g</mi><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">聚合度<i>Deg</i>(<i>x</i><sub><i>i</i></sub>)越大,表明点<i>x</i><sub><i>i</i></sub>周围的数据点越密集。聚合度距离<i>G</i>(<i>x</i><sub><i>i</i></sub>)越大,则两个簇之间的相异程度越高。集合平均距离<i>Cavgd</i>(<i>x</i><sub><i>i</i></sub>)越小,则其倒数越大,表明由<i>x</i><sub><i>i</i></sub>组成的集合中的元素越紧密。由此可见,聚合距离参数值越大的点,越适合作为聚类中心。</p>
                </div>
                <h3 id="60" name="60" class="anchor-tag">2 改进<i>K</i>-means算法</h3>
                <h4 class="anchor-tag" id="61" name="61">2.1 <b>改进初始聚类中心</b></h4>
                <div class="p1">
                    <p id="62">由聚合距离参数筛选最优初始聚类中心步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="63">1)根据式(2)～(6),计算出数据集中所有数据点相关参数,从而得到每一个数据点的聚合距离参数。</p>
                </div>
                <div class="p1">
                    <p id="64">2)筛选出聚合距离值最大的点,作为第一个初始聚类中心,并依次计算其他点到该点的欧氏距离:若距离小于邻域半径<i>R</i>,将该点从数据集中移除;若距离大于<i>R</i>,则不作处理。</p>
                </div>
                <div class="p1">
                    <p id="65">3)从剩余点中筛选出聚合距离值最大的点,作为第二个初始聚类中心,并依次计算其他点到该点的欧氏距离,若距离小于邻域半径<i>R</i>,将该点从数据集中移除。</p>
                </div>
                <div class="p1">
                    <p id="66">4)重复步骤3)和步骤4),直到数据集为空。</p>
                </div>
                <div class="p1">
                    <p id="67">5)输出一系列符合条件的优质初始聚类中心。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68">2.2 <b>改进准则函数</b></h4>
                <div class="p1">
                    <p id="69">传统<i>K</i>-means的准则函数一般为平方误差和函数,该函数计算了每个聚类样本与其聚类中心的平方距离之和,但仅片面地衡量了一个类之内数据是否紧凑,没有考虑到类与类之间的相异性,因此,本文采用戴维森堡丁指数(Davies-Bouldin Index, DBI)指标函数作为<i>K</i>-means算法的准则函数<citation id="193" type="reference"><link href="182" rel="bibliography" /><link href="184" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="70">类间距离 类间距离 <i>Dis</i>(<i>i</i>, <i>j</i>)表示为第<i>i</i>个类与第<i>j</i>个类之间的距离,即第<i>i</i>个聚类中心<i>v</i><sub><i>i</i></sub>与第<i>j</i>个聚类中心<i>v</i><sub><i>j</i></sub>的欧氏距离。</p>
                </div>
                <div class="p1">
                    <p id="71"><i>Dis</i>(<i>i</i>, <i>j</i>)=‖<i>v</i><sub><i>i</i></sub>-<i>v</i><sub><i>j</i></sub>‖      (7)</p>
                </div>
                <div class="p1">
                    <p id="72">类内标准误差 类内标准误差<i>S</i><sub><i>i</i></sub>表示为第<i>i</i>个聚类<i>C</i><sub><i>i</i></sub>中每一个数据点<i>x</i>与该类的中心点<i>v</i><sub><i>i</i></sub>之间的欧氏距离标准误差和,即:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mo stretchy="false">∥</mo></mstyle><mi>x</mi><mo>-</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">其中<i>N</i><sub><i>i</i></sub>表示第<i>i</i>个聚类<i>C</i><sub><i>i</i></sub>包含的数据对象个数。</p>
                </div>
                <div class="p1">
                    <p id="75">DBI指标</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>B</mi><mi>Ι</mi><mo>=</mo><mfrac><mn>1</mn><mi>Κ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Κ</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mi>max</mi></mrow></mstyle></mrow></mstyle><mrow><mo>{</mo><mrow><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mi>D</mi><mi>i</mi><mi>s</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><mo>}</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">其中<i>K</i>表示为数据集的所有聚类个数。</p>
                </div>
                <div class="p1">
                    <p id="78">DBI指标由类之间的距离和类内的距离决定。好的聚类结果应该满足同一个类中数据之间的相似程度大,而类与类之间的相似程度小。DBI指标不仅考虑了类内的相似性,还考虑了类与类之间的相异性:如果一个类的类内距离较小,则DBI的分子较小,表明类中数据点的相似程度大;如果类与类之间的距离较大,则DBI的分母较大,表明类之间的相异性较大。因此,DBI指标数值越小,表明聚类效果越好。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79">2.3 <b>改进算法总流程</b></h4>
                <div class="p1">
                    <p id="80">改进算法根据聚合距离参数选取一定数量的最优中心,作为<i>K</i>-means的初始聚类中心,用DBI指标作为准则函数,若准则函数收敛,则说明聚类效果达到最优,聚类完成,输出聚类结果。整体的算法流程如下所示。</p>
                </div>
                <div class="p1">
                    <p id="81">算法1 基于聚合距离参数的改进<i>K</i>-means算法。</p>
                </div>
                <div class="p1">
                    <p id="82">输入:数据集;</p>
                </div>
                <div class="p1">
                    <p id="83">输出:聚类结果。</p>
                </div>
                <div class="p1">
                    <p id="84"><i>M</i>=数据点个数</p>
                </div>
                <div class="p1">
                    <p id="85"><i>R</i>=邻域半径</p>
                </div>
                <div class="p1">
                    <p id="86">1)for <i>i</i> in <i>range</i>(<i>M</i>)</p>
                </div>
                <div class="p1">
                    <p id="88">2)计算两两数据点之间的欧氏距离</p>
                </div>
                <div class="p1">
                    <p id="90">3)计算每个数据点<i>x</i><sub><i>i</i></sub>的聚合距离参数<i>ω</i>(<i>x</i><sub><i>i</i></sub>)</p>
                </div>
                <div class="p1">
                    <p id="92">4)选出<i>ω</i>(<i>x</i><sub><i>i</i></sub>)数值最大的数据点,作为第一个初始聚聚类中心,并将其从数据集中移除</p>
                </div>
                <div class="p1">
                    <p id="94">5)计算剩余各点<i>x</i><sub><i>j</i></sub>到该聚类中心<i>x</i><sub><i>i</i></sub>的距离<i>d</i><sub><i>i</i></sub>, <i>j</i></p>
                </div>
                <div class="p1">
                    <p id="96">6)if <i>d</i><sub><i>i</i></sub>, <i>j</i> &lt; <i>R</i></p>
                </div>
                <div class="p1">
                    <p id="98">7)则将点<i>x</i><sub><i>j</i></sub>从数据集中移除</p>
                </div>
                <div class="p1">
                    <p id="100">8)else if <i>d</i><sub><i>i</i></sub>, <i>j</i>≥<i>R</i></p>
                </div>
                <div class="p1">
                    <p id="102">9)不作处理</p>
                </div>
                <div class="p1">
                    <p id="104">10)end</p>
                </div>
                <div class="p1">
                    <p id="106">11)重复步骤4)～10),直到数据集为空。</p>
                </div>
                <div class="p1">
                    <p id="108">12)输出一定个数的最优中心作为<i>K</i>-means算法的初始聚类中心</p>
                </div>
                <div class="p1">
                    <p id="110">13)输入中心个数<i>N</i></p>
                </div>
                <div class="p1">
                    <p id="112">14)执行改进<i>K</i>-means算法,将数据集分为<i>N</i>个聚类</p>
                </div>
                <div class="p1">
                    <p id="114">15)if 准则函数<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>B</mi><mi>Ι</mi><mo>=</mo><mfrac><mn>1</mn><mi>Κ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Κ</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mi>max</mi></mrow></mstyle></mrow></mstyle><mrow><mo>{</mo><mrow><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mi>D</mi><mi>i</mi><mi>s</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><mo>}</mo></mrow></mrow></math></mathml>不收敛</p>
                </div>
                <div class="p1">
                    <p id="117">16)继续循环执行改进<i>K</i>-means算法</p>
                </div>
                <div class="p1">
                    <p id="119">17)else if 准则函数收敛</p>
                </div>
                <div class="p1">
                    <p id="121">18)改进<i>K</i>-means算法执行完毕</p>
                </div>
                <div class="p1">
                    <p id="123">19)end</p>
                </div>
                <div class="p1">
                    <p id="125">20)输出聚类结果</p>
                </div>
                <h3 id="127" name="127" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="128" name="128">3.1 <b>可视化数据集</b></h4>
                <div class="p1">
                    <p id="129">为了更好地说明改进算法的优异性,本节使用Python中的make_blobs模块生成二维数值型仿真数据。首先,为了便于可视化处理,生成两组均包含200个样本点的数据。图1(a)和图1(b)的结果表明,改进算法找到了优质的初始聚类中心,成功将数据分成了2类和3类,聚类结果准确,从而改进算法的有效性得以验证。然后,为了检验算法面对较大样本时的效果,再生成一组达到10 000样本数的数据。图1(c)的结果表明,改进算法仍能很好地完成聚类。</p>
                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909018_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 仿真数据集聚类结果" src="Detail/GetImg?filename=images/JSJY201909018_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 仿真数据集聚类结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909018_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Clustering results of simulated data</p>

                </div>
                <h4 class="anchor-tag" id="131" name="131">3.2 <b>标准数据集</b></h4>
                <h4 class="anchor-tag" id="132" name="132">3.2.1 评估指标</h4>
                <div class="p1">
                    <p id="133">标准数据集指明了每一个数据点真实所属的类,即true_label,而实际的聚类结果也会有一个相对应的标签,即pred_label,用于表示每个数据点实际被分到的类。对于标准数据集,本文采用以下指标进行评估:调整兰德系数、互信息、Fowlkes-Mallows 指标、同质性和完整性,这些指标都用于衡量true_label和pred_label的相似程度。</p>
                </div>
                <h4 class="anchor-tag" id="134" name="134">3.2.2 实验结果</h4>
                <div class="p1">
                    <p id="135">本文采用了UCI网站上的标准数据集,数据集名称及其属性如表1所示。</p>
                </div>
                <div class="area_img" id="136">
                    <p class="img_tit"><b>表</b>1 UCI <b>数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Database UCI</p>
                    <p class="img_note"></p>
                    <table id="136" border="1"><tr><td><br />数据集名称</td><td>类数</td><td>实例数</td><td>维数</td></tr><tr><td>Iris</td><td>3</td><td>150</td><td>4</td></tr><tr><td><br />Wine</td><td>3</td><td>178</td><td>13</td></tr><tr><td><br />Seg</td><td>7</td><td>2 310</td><td>19</td></tr><tr><td><br />Soybean</td><td>4</td><td>47</td><td>35</td></tr><tr><td><br />Iono</td><td>2</td><td>351</td><td>34</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="137">为了更深入地进行对比,本文采用了传统<i>K</i>-means算法,Canopy算法及改进<i>K</i>-means算法对标准数据集进行聚类。聚类结果的评估指标如图2所示。</p>
                </div>
                <div class="p1">
                    <p id="138">图2(a)显示调整兰德系数指标结果,调整兰德系数表示两个数据分布之间的相似度,其范围从-1到1。值越大,表明聚类结果与实际情况越一致;若值为负,表明两个数据分布相互独立,匹配程度很低。改进<i>K</i>-means算法相比于传统<i>K</i>-means,调整兰德系数指标均可有效提高。其中Iris和Seg 这两个数据集,改进算法比传统算法的调整兰德系数指标分别提高了83.7%和71.0%。</p>
                </div>
                <div class="p1">
                    <p id="139">图2(b)和图2(c)分别显示互信息及Fowlkes-Mallows指标结果。这两个指标用于表示两个变量true_label和pred_label是否有关系,以及关系的接近程度。互信息和Fowlkes-Mallows指标的范围都为0～1,如果值较大,则表示true_label和pred_label之间的关系更接近。实验结果表明,改进算法在各个数据集上,互信息及Fowlkes-Mallows指标的数值都明显比传统<i>K</i>-means算法有提高。即改进<i>K</i>-means的实际聚类结果与标准结果更接近,聚类效果更好。</p>
                </div>
                <div class="p1">
                    <p id="140">图3表示同质性和完整性的实验结果。同质性表示每个群集只包含单个类的成员,完整性表示给定类的所有成员都分配给同一个群集,这两个指标通常一起使用,范围为0到1之间,值越大表明聚类效果越好。</p>
                </div>
                <div class="p1">
                    <p id="141">通过比较图2和图3的整体结果可知:1)改进<i>K</i>-means算法的评估指标数值均高于其他两个算法,即改进算法的实际聚类结果与标准结果更一致,这说明了其性能是优于传统算法的。2)对于不同的数据集来说,同一个算法聚类结果的评估指标数值也不一样,这说明聚类效果会因不同的数据集而波动。3)Canopy的聚类结果大多比传统<i>K</i>-means算法差,有时比传统<i>K</i>-means更好,这是因为Canopy的聚类结果很大程度上取决于两个阈值,且阈值在实验中是随机选择的。如果阈值更准确,最终的聚类结果将更好,指标将更好。而改进的<i>K</i>-means算法避免了随机性,并且始终具有更好的聚类结果。</p>
                </div>
                <div class="area_img" id="142">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909018_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 调整兰德系数、互信息和及Fowlkes-Mallows指标对比结果" src="Detail/GetImg?filename=images/JSJY201909018_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 调整兰德系数、互信息和及Fowlkes-Mallows指标对比结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909018_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Comparison results of adjusted Rand coefficient, mutual information and Fowlkes-Mallows index</p>

                </div>
                <div class="area_img" id="143">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909018_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 同质性和完整性指标比较结果" src="Detail/GetImg?filename=images/JSJY201909018_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 同质性和完整性指标比较结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909018_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Comparison results of homogeneity and completeness</p>

                </div>
                <h3 id="144" name="144" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="145"><i>K</i>-means算法是聚类算法中的重要方法。本文针对传统<i>K</i>-means算法的不足,提出了基于聚合距离参数的改进<i>K</i>-means算法。首先,通过使用聚合距离参数获取一定的最优聚类中心;然后,将最优聚类中心应用于改进的<i>K</i>-means,并且改进的<i>K</i>-means算法将根据新的准则函数DBI收敛,当准则函数达到最小值时,聚类结束,输出聚类结果。</p>
                </div>
                <div class="p1">
                    <p id="146">改进的<i>K</i>-means算法有效地解决了初始聚类中心和<i>K</i>值选择不确定的问题,实验结果表明,改进的<i>K</i>-means算法比传统<i>K</i>-means算法,在聚类效果上有很大的提升。</p>
                </div>
                <div class="p1">
                    <p id="147">未来工作中将会采用改进的<i>K</i>-means算法来对工业大数据进行聚类。由于工业大数据大都具有时效性,因此,将考虑对大数据进行降维,从而减少聚类算法的计算时间。同时,会在Hadoop平台上并行化实现大数据的聚类,提高时间效能。最后基于改进算法得出的聚类结果,提出针对工业大数据的异常值检测方法,从而将改进算法应用于工业大数据领域,有效检测工业设备运行的健康程度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="156">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201902017&amp;v=MjcyODNVUjdxZlp1WnNGeWpsVTc3S0x6N0JkN0c0SDlqTXJZOUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 王治和,黄梦莹,杜辉,等.基于密度峰值与密度聚类的集成算法[J].计算机应用,2019,39(2):398-402.(WANG Z H,HUANG M Y,DU H,et al.Integrated algorithm based on density peaks and density-based clustering J].Journal of Computer Applications,2019,39(2):398-402.)
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122600202871&amp;v=MjQ5MTNCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUlGNFVheEE9TmlmT2ZiSzlIOVBPcVk5Rlp1c05CSHM0bw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> McLOUGHLIN F,DUFFY A,CONLON M.A clustering approach to domestic electricity load profile characterisation using smart metering data [J].Applied Energy,2015,141:190-199.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=K-means based load estimation of domestic smart meter measurements">

                                <b>[3]</b> ALI A-W,WU J,JENKINS N.K-means based load estimation of domestic smart meter measurements [J].Applied Energy,2016,194:333-342.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201608003&amp;v=MjQ5NTN5amxVNzdLTHo3QmQ3RzRIOWZNcDQ5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 杨辉华,王克,李灵巧,等.基于自适应布谷鸟搜索算法的K-means聚类算法及其应用[J].计算机应用,2016,36(8):2066-2070.(YANG H H,WANG K,LI L Q,et al.K-means clustering algorithm based on adaptive cuckoo search and its application [J].Journal of Computer Applications,2016,36(8):2066-2070.)
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WJFZ201107016&amp;v=MzA3MjZHNEg5RE1xSTlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqbFU3N0tNaWZOZEw=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 黄韬,刘胜辉,谭艳娜.基于K-means聚类算法的研究[J].计算机技术与发展,2011,21(7):54-57.(HUANG T,LIU S H,TAN Y N.Research of clustering algorithm based on K-means [J].Computer Technology and Development,2011,21(7):54-57.)
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201208009&amp;v=MjUyNzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqbFU3N0tMejdCZHJHNEg5UE1wNDlGYlk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 王骏,王士同,邓赵红.特征加权距离与软子空间学习相结合的文本聚类新方法[J].计算机学报,2012,35(8):1655-1665.(WANG J,WANG S T,DENG Z H.A novel text clustering algorithm based on feature weighting distance and soft subspace learning [J].Chinese Journal of Computers,2012,35(8):1655-1665.)
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYY201705027&amp;v=MzIwNDZHNEg5Yk1xbzlIWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqbFU3N0tQVG5TZDc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 郁启麟.K-means算法初始聚类中心选择的优化[J].计算机系统应用,2017,26(5):170-174.(YU Q L.Optimization of initial clustering centers selection method for K-means algorithm [J].Computer Systems &amp; Applications,2017,26(5):170-174.)
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201602007&amp;v=MTE1Mjc0SDlmTXJZOUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVTc3S0x6N0JkN0c=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 周润物,李智勇,陈少淼,等.面向大数据处理的并行优化抽样聚类K-means算法[J].计算机应用,2016,36(2):311-315.(ZHOU R W,LI Z Y,CHEN S M,et al.Parallel optimization sampling clustering K-means algorithm for big data processing [J].Journal of Computer Applications,2016,36(2):311-315.)
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201307046&amp;v=MTMxMDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amxVNzdLTHo3QmQ3RzRIOUxNcUk5QllvUUs=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 王丽娟,郝志峰,蔡瑞初,等.基于随机取样的选择性K-means聚类融合算法[J].计算机应用,2013,33(7):1969-1972.(WANG L J,HAO Z F,CAI R C,et al.Selective K-means clustering ensemble based on random sampling [J].Journal of Computer Applications,2013,33(7):1969-1972.)
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201227007&amp;v=MjYxNDRSN3FmWnVac0Z5amxVNzdLTHo3TWFiRzRIOVBPcUk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 毛典辉.基于MapReduce的Canopy-Kmeans改进算法[J].计算机工程与应用,2012,48(27):22-26.(MAO D H.Improved Canopy-Kmeans algorithm based on MapReduce [J].Computer Engineering and Applications,2012,48(27):22-26.)
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201807016&amp;v=MTA2OTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVTc3S05pZllaTEc0SDluTXFJOUVZb1E=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 赵昱,陈琴,苏一丹,等.基于邻域相似度的近邻传播聚类算法[J].计算机工程与设计,2018,39(7):1883-1888.(ZHAO Y,CHEN Q,SU Y D,et al.Affinity propagation clustering algorithm based on neighborhood similarity [J].Computer Engineering and Design,2018,39(7):1883-1888.)
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JXZZ201606018&amp;v=MDA4MDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVTc3S0x6WFJkTEc0SDlmTXFZOUViSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 刘鹏,王明阳,王焱.基于自适应动态球半径的K邻域搜索算法[J].机械设计与制造工程,2016,45(6):83-86.(LIU P,WANG M Y,WANG Y.K domain search algorithm based on adaptive dynamic sphere radius [J].Machine Design and Manufacturing Engineering,2016,45(6):83-86.)
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An algorithmic method of calculating neighborhood radius for clustering in-home activities within smart home environment">

                                <b>[13]</b> NGUYEN D,LE T,NGUYEN S.An algorithmic method of calculating neighborhood radius for clustering in-home activities within smart home environment [C]// Proceedings of the 7th International Conference on Intelligent Systems,Modelling and Simulation.Piscataway,NJ:IEEE,2016:42-47.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic Feature Selection for BCI:an Analysis Using the Davies-Bouldin Index and Extreme Learning Machines">

                                <b>[14]</b> COELHO G P,BARBANTE C C,BOCCATO L,et al.Automatic feature selection for BCI:an analysis using the Davies-Bouldin index and extreme learning machines [C]// Proceedings of the 2012 International Joint Conference on Neural Networks.Piscataway,NJ:IEEE,2012:1-8.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=New version of Davies-Bouldin index for clustering validation based on cylindrical distance">

                                <b>[15]</b> THOMAS J C R,PEÑAS M S,MORA M.New version of Davies-Bouldin index for clustering validation based on cylindrical distance [C]// Proceedings of the 32nd International Conference of the Chilean Computer Science Society.Piscataway,NJ:IEEE,2013:49-53.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201909018" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909018&amp;v=MzI0MDhIOWpNcG85RWJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amxVNzdLTHo3QmQ3RzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
